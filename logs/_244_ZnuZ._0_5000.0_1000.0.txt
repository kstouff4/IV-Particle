Thu Feb 2 11:52:01 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 48578734
numbers of Z: 22461
shape of features
(22461,)
shape of features
(22461,)
ZX	Vol	Parts	Cubes	Eps
Z	0.016870080987923083	22461	22.461	0.09089975092677936
X	0.015587751161167002	1584	1.584	0.21429371260135763
X	0.016376260613898918	20342	20.342	0.09302657473047868
X	0.01666224609296201	3985	3.985	0.16110229957554367
X	0.015835407081880394	2685	2.685	0.1806737175664726
X	0.016352883620543695	53453	53.453	0.0673815122697156
X	0.015731198121472417	68240	68.24	0.06131606286864734
X	0.015749431930607814	70094	70.094	0.06079408828603592
X	0.015759344663259296	59581	59.581	0.06419139126866291
X	0.01611634811659805	6648	6.648	0.13433583412177963
X	0.015635278139512623	61066	61.066	0.06349926708895784
X	0.015698132364186608	11395	11.395	0.11126994816083653
X	0.015746622245112838	84251	84.251	0.05717472901560212
X	0.016151050734078316	7107	7.107	0.1314734790262025
X	0.015738647469349203	304501	304.501	0.037249995803756486
X	0.015705718039737245	26768	26.768	0.08371705794632989
X	0.015726182931231666	90378	90.378	0.05582818799152203
X	0.01586026049628423	74704	74.704	0.05965618632222962
X	0.015705329693256404	45046	45.046	0.07038232487888288
X	0.01659022192041885	265531	265.531	0.03968066923385194
X	0.01634665954650368	147517	147.517	0.04803181324589809
X	0.015903081462339614	36925	36.925	0.07551854139932906
X	0.016341528806034854	467370	467.37	0.03269971372690379
X	0.015692360333102413	21838	21.838	0.0895691535629788
X	0.01615690713483076	16204	16.204	0.09990303102078686
X	0.01564257539526297	22007	22.007	0.08924470763771938
X	0.01573641268718653	82713	82.713	0.05751449636214911
X	0.015952522431798784	41771	41.771	0.0725523522069588
X	0.01573322525551106	9958	9.958	0.11647030503328334
X	0.01571670742276942	77966	77.966	0.05863434863888884
X	0.016612554083601145	2069062	2069.062	0.02002415973704361
X	0.015674049969112685	8504	8.504	0.12260814594004746
X	0.016453886083587006	546083	546.083	0.031117398130161532
X	0.015683823089148	13713	13.713	0.10457787664076451
X	0.015705165945567104	12282	12.282	0.10854033197018159
X	0.015879449643890736	17665	17.665	0.09651035983591291
X	0.015742251350288384	168972	168.972	0.04533330329983891
X	0.016203818070116744	113489	113.489	0.05226623254335501
X	0.015201875381651726	2070	2.07	0.1943758359762654
X	0.015664046005096077	5860	5.86	0.13878273587761752
X	0.015669744704662238	3998	3.998	0.1576666028993999
X	0.01571493612259231	5692	5.692	0.14028650949002472
X	0.015318294143387316	1454	1.454	0.21922092169489793
X	0.015544090855018133	1332	1.332	0.22682297051195996
X	0.015948890707063068	2911	2.911	0.1762907094495986
X	0.015393534155645644	1349	1.349	0.225134540360363
X	0.01563545683399606	1864	1.864	0.2031833994919018
X	0.015713118062401152	7975	7.975	0.12536526623376215
X	0.01572486626293844	5594	5.594	0.1411307012633286
X	0.015562922290243947	2168	2.168	0.19290520988942042
X	0.015664240992861334	14091	14.091	0.10359111645942667
X	0.016150994334013746	16062	16.062	0.10018434913703232
X	0.015658880658703488	2406	2.406	0.18670460526396288
X	0.015666424285451196	6578	6.578	0.1335442945680746
X	0.015620043918333581	3394	3.394	0.16633748761469475
X	0.015724372431994867	8150	8.15	0.12449117641055445
X	0.01566509771559552	6357	6.357	0.13507043512897396
X	0.01569274280749296	2962	2.962	0.1743296537681817
X	0.015540468536980555	4482	4.482	0.15135522524978004
X	0.015635214448713488	4034	4.034	0.15708063544528383
X	0.015468909746131151	2607	2.607	0.18103931239782733
X	0.016246823693782934	6813	6.813	0.1336010976414949
X	0.015618493947983139	2472	2.472	0.18486877455357364
X	0.01671563944881643	11171	11.171	0.11437830272933602
X	0.015651979782309824	9215	9.215	0.11931397451402206
X	0.015593896145332768	3567	3.567	0.1635123308462834
X	0.015623157088137883	3478	3.478	0.16499840295279217
X	0.015618745118974073	3495	3.495	0.16471493752995686
X	0.01564076931068492	9570	9.57	0.11779189019747972
X	0.01568069987023219	5860	5.86	0.13883190261999542
X	0.015676509624731615	2754	2.754	0.17855080497758655
X	0.01563095653474925	3916	3.916	0.15862840499585462
X	0.01583442582832374	3776	3.776	0.16125880784073227
X	0.01564636661574576	4455	4.455	0.15200408679819066
X	0.015678106151010226	3651	3.651	0.16254012492599762
X	0.015453790050305376	4013	4.013	0.15674316648117184
X	0.015644523062617596	4412	4.412	0.15249031961310835
X	0.015289135183706198	1571	1.571	0.21350214626347683
X	0.015541579867782098	1115	1.115	0.24066158117248687
X	0.015619609583539145	5086	5.086	0.14535543422437094
X	0.015659205562333398	15572	15.572	0.10018632434024698
X	0.015572216125218	3537	3.537	0.16389729563053365
X	0.015621911686751446	1737	1.737	0.20795919132814844
X	0.016348051166167554	7438	7.438	0.13001793719019158
X	0.015467838367154497	1313	1.313	0.227538539615095
X	0.015653737715434508	2661	2.661	0.18051961255755453
X	0.015538959037135876	2018	2.018	0.19746960311819892
X	0.01546882941626132	2845	2.845	0.17584299001794595
X	0.015462693871545998	1842	1.842	0.20323500413567136
X	0.015662263483592738	6550	6.55	0.13372247575785404
X	0.015669579141107337	3313	3.313	0.16785925789652742
X	0.015646450858300062	4739	4.739	0.1489051473117584
X	0.015696234835374957	1873	1.873	0.20311994492084914
X	0.01559474972700767	1995	1.995	0.1984625207101143
X	0.015606799742672145	4482	4.482	0.15157026219017355
X	0.015621080897204249	4517	4.517	0.1512238638455585
X	0.015694374829480072	5419	5.419	0.14254159798669033
X	0.015855792943704435	4062	4.062	0.15745242323303177
X	0.015667429701494425	5450	5.45	0.14218935499306654
X	0.015684411161930364	5411	5.411	0.1425816255018704
X	0.01566024445101428	4441	4.441	0.15220862215662478
X	0.015659125264667905	13019	13.019	0.10634815431791117
X	0.015643217812585535	4491	4.491	0.1515866699271972
X	0.015578399370176733	6440	6.44	0.13423908666400602
X	0.01549549500779808	1785	1.785	0.20552078244006944
X	0.015638017239408164	5134	5.134	0.1449579174586173
X	0.016156256200047618	3038	3.038	0.1745491161160382
X	0.015701681466735756	9133	9.133	0.11979652902986553
X	0.015621877389985155	1110	1.11	0.24143677166222308
X	0.015610488442818224	2428	2.428	0.18594704019980637
X	0.015533603172886152	1883	1.883	0.20205702861155422
X	0.015652497927686203	1825	1.825	0.20469485589702616
X	0.01551680346780366	1247	1.247	0.23172804731166197
X	0.01551005387437414	2266	2.266	0.1898677192762707
X	0.01566732357993528	4391	4.391	0.1528071908854754
X	0.01552272192711134	6342	6.342	0.13476606467390218
X	0.015431406327901092	3293	3.293	0.16734186008552118
X	0.0157387021539177	3252	3.252	0.16915030813011003
X	0.015481183819212329	1334	1.334	0.2264033138282731
X	0.016326925696130676	8076	8.076	0.12644530315754698
X	0.015436428519275196	1878	1.878	0.20181359220842537
X	0.015715044091023974	19935	19.935	0.09237755313360164
X	0.015569465222705636	2625	2.625	0.18101509643570685
X	0.01533111532325982	2279	2.279	0.18877441877268322
X	0.015526033005408871	2878	2.878	0.17538397346844065
X	0.015597540554137217	2486	2.486	0.1844385360379339
X	0.015404951674184979	2311	2.311	0.18820022209121717
X	0.015602211252497	2319	2.319	0.1887825298681622
X	0.015620784158018595	3890	3.89	0.1589465393566994
X	0.015715632280808443	17125	17.125	0.0971778066898216
X	0.015642110078909633	3961	3.961	0.158062979856462
X	0.01572400045299622	10753	10.753	0.1135039914172473
X	0.015449191916660202	1925	1.925	0.2002127256788825
X	0.015540841682561589	4576	4.576	0.15031287221360837
X	0.015576207743760602	2151	2.151	0.1934670933295416
X	0.015555558215186671	4169	4.169	0.15510217993379866
X	0.015427486918343431	2268	2.268	0.18947447491959798
X	0.015477182122244998	1759	1.759	0.2064470514579296
X	0.015396407628505623	2820	2.82	0.17608543348364647
X	0.015727343394103213	2403	2.403	0.18705408607602847
X	0.01570837878577074	3006	3.006	0.17353248912904842
X	0.015444212213556985	3955	3.955	0.15747311710209427
X	0.015494442863569657	1176	1.176	0.23618712208987058
X	0.01600233072354064	11703	11.703	0.11099293754209959
X	0.015838541223166327	11042	11.042	0.11277750407891125
X	0.01623312368154903	7260	7.26	0.13076408892922886
X	0.015504242144905768	2285	2.285	0.18931634563875885
X	0.01560562516812752	2505	2.505	0.1840028205919863
X	0.01587640815202385	3573	3.573	0.16440174124586857
X	0.015665596774709193	2456	2.456	0.1854554186565759
X	0.015666382494939263	2940	2.94	0.174665500124961
X	0.01567129256246659	2336	2.336	0.18860100095335416
X	0.015632211446401055	5794	5.794	0.13921327273259695
X	0.01563332067536394	4404	4.404	0.15254617016445823
X	0.015602770640460322	9990	9.99	0.11602335298133078
X	0.0156379774012159	4085	4.085	0.15643340799569377
X	0.015679812719603965	13084	13.084	0.10621848723450802
X	0.01569774821782645	4540	4.54	0.15121463843733518
X	0.01572134432291776	2937	2.937	0.17492903929355227
X	0.0156750032632739	4564	4.564	0.15087617594513655
X	0.015493801555266633	744	0.744	0.2751239203068173
X	0.016645371205049535	11631	11.631	0.11269175515243864
X	0.01562885833238097	2424	2.424	0.1861222152903351
X	0.01564691705849353	4524	4.524	0.15122910945630688
X	0.015639127179911422	1684	1.684	0.21019552933628585
X	0.015548980734896141	3730	3.73	0.1609401362108966
X	0.015566056749640097	1988	1.988	0.19857324966481216
X	0.015680097863571142	3381	3.381	0.16676357600657965
X	0.015659209698589548	4187	4.187	0.15522284043260415
X	0.01563691803407227	4819	4.819	0.1480464772275575
X	0.015477489727827908	864	0.864	0.26165493323176076
X	0.015417398984535194	1378	1.378	0.22365947338826037
X	0.01551383487708207	1795	1.795	0.2052193167044471
X	0.01567923285215061	8694	8.694	0.1217218071006379
X	0.01565304363366588	2820	2.82	0.17705841106638426
X	0.01572345556084209	7359	7.359	0.1287981815820373
X	0.016007853659393396	4377	4.377	0.1540702732945929
X	0.015728115164528655	6850	6.85	0.13192548607655424
X	0.015518854811961948	3606	3.606	0.16265895676233436
X	0.015627435418960778	5091	5.091	0.14533209651759627
X	0.015564115346189577	3371	3.371	0.16651571610140978
X	0.015710586171174223	3967	3.967	0.15821344830883402
X	0.015588085886958787	2918	2.918	0.17481116515147396
X	0.015697444482198345	6836	6.836	0.13192961666892067
X	0.015594416552510985	2964	2.964	0.17392565349484676
X	0.01606210549818809	7823	7.823	0.12709926759280213
X	0.015725738386916144	5098	5.098	0.14556950496615495
X	0.015496541944022817	3093	3.093	0.17111376955367977
X	0.015574312695237506	4792	4.792	0.14812579476514398
X	0.015711098396665218	6733	6.733	0.13263739325618337
X	0.01577200046382489	14135	14.135	0.10372027417487632
X	0.015580791259285937	2833	2.833	0.17651474036636525
X	0.0155505570334257	942	0.942	0.2546234212278782
X	0.015708083270553067	4376	4.376	0.15311413804510376
X	0.015779889665655765	2972	2.972	0.17445565527026546
X	0.015585016840747581	3719	3.719	0.16122301275960696
X	0.016292667078049765	3133	3.133	0.17325160348592866
X	0.01567433150450167	2850	2.85	0.1765149074037088
X	0.015596006249144881	1978	1.978	0.19903480962997672
X	0.015538528039747266	2710	2.71	0.17898371921468897
X	0.015503362299514718	2360	2.36	0.18728570540536504
X	0.015713241636703203	5571	5.571	0.14128982163663023
X	0.015670459638627697	2375	2.375	0.18755963276701565
X	0.015734579155530073	13477	13.477	0.10529812413793213
X	0.01579777356440738	6696	6.696	0.13312516116320122
X	0.01567950865264509	4396	4.396	0.1527888240935741
X	0.01558835643339802	1849	1.849	0.20352658182131314
X	0.016613221424833517	6423	6.423	0.13726884426563307
X	0.015687730752091945	5200	5.2	0.14449481426741786
X	0.015504133143120727	3244	3.244	0.16844400075601387
X	0.015678502351825968	12051	12.051	0.10916761684952123
X	0.015713113577039296	5494	5.494	0.14194644868904052
X	0.01552822875814734	3941	3.941	0.1579447380225131
X	0.01565984171793614	3401	3.401	0.16636434152113913
X	0.015643695239487127	3855	3.855	0.15950402795719898
X	0.015450644953432777	1386	1.386	0.22338866231492183
X	0.015598036924851816	1795	1.795	0.20558992608445908
X	0.015641351826971395	1440	1.44	0.2214644034730704
X	0.015532901615945525	1095	1.095	0.24207289131930976
X	0.015430889185949596	4590	4.59	0.14980492412896534
X	0.015690070308345995	3487	3.487	0.16509134817882318
X	0.015415187681599697	4245	4.245	0.15370584189395212
X	0.015800522425434803	1983	1.983	0.19973288607824716
X	0.01563540330147849	5371	5.371	0.14278567705427808
X	0.015596978700956731	4250	4.25	0.15424716229678692
X	0.015868701507823692	4128	4.128	0.15665126303374258
X	0.015688767029966372	4419	4.419	0.15255329912309193
X	0.015567443271494015	2502	2.502	0.18392608590792983
X	0.015364262882827023	1230	1.23	0.23202540494464757
X	0.015607542125883654	6143	6.143	0.1364534959551446
X	0.015497697777759077	907	0.907	0.2575648731884143
X	0.015659528888934633	2075	2.075	0.19614934569074807
X	0.015403098626652663	2833	2.833	0.17584114670634823
X	0.016131677464679	3636	3.636	0.16431802961829248
X	0.0166372192666986	5127	5.127	0.14804911227374087
X	0.015614274755667087	2847	2.847	0.17635107714416792
X	0.015635356425007636	3076	3.076	0.17193877105422123
X	0.015770419655514914	3993	3.993	0.1580694637662968
X	0.016301427973218322	7100	7.1	0.13192358162637793
X	0.015717413199857047	2359	2.359	0.18817028524166549
X	0.015599220256448552	10506	10.506	0.11408323592052343
X	0.01574681487045441	41989	41.989	0.07211391328351283
X	0.016158623793372653	19024	19.024	0.09470382297743027
X	0.01560544030391847	6910	6.91	0.13119964495343106
X	0.015326623751640574	1291	1.291	0.2281252816379041
X	0.015608638351387325	7436	7.436	0.12803888367837168
X	0.01567908263517079	160414	160.414	0.046063768002878834
X	0.01552793019360267	382696	382.696	0.03436264211790535
X	0.015676310929100417	4920	4.92	0.1471497703620185
X	0.01581640564239472	107571	107.571	0.05278017950864438
X	0.015625324330239786	42739	42.739	0.07150475035742702
X	0.015720804889488394	59560	59.56	0.06414655875005619
X	0.01644606778457561	427976	427.976	0.033745372010533375
X	0.015665046235124973	2889	2.889	0.1756823153259079
X	0.01565070545267906	5539	5.539	0.14137333972169175
X	0.015748432442001675	244503	244.503	0.04008523000207945
X	0.015773498039506978	583477	583.477	0.030012448299856137
X	0.01571441044498818	6570	6.57	0.13373474098761504
X	0.015754836935491973	36302	36.302	0.07571137264953541
X	0.01582685840150492	134327	134.327	0.04902419455236462
X	0.016335571865721262	76486	76.486	0.059774708167949796
X	0.015729578304682634	72884	72.884	0.059983015273920734
X	0.01581322117229475	40478	40.478	0.07310275870557643
X	0.016202678594377174	35239	35.239	0.07718289789886142
X	0.015730181256883687	12631	12.631	0.10758837068245777
X	0.015537422727656458	4921	4.921	0.14670397191606008
X	0.016206270159962892	483162	483.162	0.032250028394645386
X	0.015718654315263418	8369	8.369	0.12338070999435849
X	0.015625627879042765	5245	5.245	0.14389001625304174
X	0.015742903779107236	56209	56.209	0.06542739975282066
X	0.01568224735780068	47968	47.968	0.06888937260488985
X	0.016234769063149142	293755	293.755	0.03809081654062001
X	0.015762451240579418	122416	122.416	0.050496567405020516
X	0.015588347571566512	1913	1.913	0.20123107018957515
X	0.015739735346313982	56300	56.3	0.06538774248982109
X	0.015548994558521476	9092	9.092	0.11958617126588274
X	0.015700206396190325	45562	45.562	0.07010799182264388
X	0.015746525018678348	78351	78.351	0.05857514799996
X	0.01565785017440869	5557	5.557	0.14124201740638934
X	0.016081379978681757	61084	61.084	0.06409123128675852
X	0.015690544025062335	1251	1.251	0.232341558056365
X	0.015779340119043927	7245	7.245	0.12962342202117189
X	0.015721600229784433	95762	95.762	0.054756352881539565
X	0.015741883828224133	58389	58.389	0.0646013938597125
X	0.015739174249822397	38683	38.683	0.07410040789049133
X	0.015661766523832672	42563	42.563	0.07165879564691885
X	0.016285136254332085	282863	282.863	0.038613433240584895
X	0.01569277115793056	15461	15.461	0.10049721357117214
X	0.01567669159058636	25399	25.399	0.0851424086660916
X	0.015794647881779308	80629	80.629	0.05807727419752745
X	0.0157017256636878	6580	6.58	0.1336309832285148
X	0.015756882173078697	157807	157.807	0.046392524728431174
X	0.016644745950805607	269448	269.448	0.03953066031899806
X	0.016632431592923467	355956	355.956	0.03601801788333324
X	0.015742402974332066	31965	31.965	0.07897060020808301
X	0.015702805069808503	34718	34.718	0.07676103942344553
X	0.015550170511676469	16235	16.235	0.09857367637876886
X	0.01571250449759334	6489	6.489	0.1342834674296006
X	0.015838168231259647	31970	31.97	0.07912628424168706
X	0.01621525031901527	12079	12.079	0.11031406593733432
X	0.015645642357541848	24635	24.635	0.08595680754074081
X	0.01635313216879302	82395	82.395	0.05833108162628613
X	0.01589077230950851	48785	48.785	0.06880493124350605
X	0.015628765276963715	9973	9.973	0.1161536736198224
X	0.015858543399805785	10154	10.154	0.11602245354699742
X	0.015771147194860367	69622	69.622	0.06095915384458151
X	0.015738607845459256	15400	15.4	0.1007276104050898
X	0.015665405089280677	9879	9.879	0.11661190265882299
X	0.01634068535509075	13461	13.461	0.10667540846036655
X	0.016569609257022708	483306	483.306	0.03248603392037591
X	0.015723696973645934	15851	15.851	0.0997315724453062
X	0.015733484301006736	109034	109.034	0.05245106636343351
X	0.015670304631801142	8858	8.858	0.12094295841216313
X	0.0157794853720035	55061	55.061	0.06592997536956541
X	0.016066963417368973	22128	22.128	0.0898801690725009
X	0.016007659110679688	321030	321.03	0.0368067441915694
X	0.015715152129940537	44622	44.622	0.07061926382030832
X	0.01571140070541173	13620	13.62	0.10487676091109879
X	0.015746051924380854	150076	150.076	0.04716503319770276
X	0.015753349340358717	112336	112.336	0.05195387949807342
X	0.01572332699131215	9445	9.445	0.11851704865849261
X	0.015743541657490222	86475	86.475	0.05667662171493187
X	0.016484001473041424	131962	131.962	0.04998863199525882
X	0.01660982544251436	371657	371.657	0.0354874056156468
X	0.016594697405119492	34633	34.633	0.07825157273276881
X	0.015424267746676728	3280	3.28	0.16753680818138006
X	0.015705836415653242	17698	17.698	0.09609754501860494
X	0.016162894029811664	178720	178.72	0.04488645663284602
X	0.015686885949255554	138628	138.628	0.04836842291144641
X	0.015717237084386573	19468	19.468	0.09311470868725431
X	0.015588129284834284	2543	2.543	0.18301325543304306
X	0.015757976653632513	53698	53.698	0.06645292689632815
X	0.01574339428942683	127939	127.939	0.049739163083630494
X	0.015542303500407058	111661	111.661	0.05182483850926681
X	0.015618303209562407	6290	6.29	0.13541322087480476
X	0.01570380273952386	37273	37.273	0.07496700498850918
X	0.016230081581339952	18765	18.765	0.09527757859522572
X	0.01570182287382721	8685	8.685	0.12182228756855608
X	0.016195908551310863	14462	14.462	0.10384660897291755
X	0.015682457571586567	9534	9.534	0.11804465427046192
X	0.015775508766325868	42832	42.832	0.0716811570326454
X	0.015762748166736694	50846	50.846	0.06767969320033791
X	0.015849326944221573	21894	21.894	0.08979012376118763
X	0.016136871378920756	16197	16.197	0.09987610219857337
X	0.01563400020461336	10296	10.296	0.11493894145795285
X	0.01599620633599784	512179	512.179	0.03149186709153048
X	0.015516814414499485	13630	13.63	0.10441644508850241
X	0.015752466102179594	162167	162.167	0.045968679009527544
X	0.015543431316212481	2011	2.011	0.19771742358296862
X	0.015646669539229656	6672	6.672	0.13285827954237414
X	0.015609735626960207	4627	4.627	0.1499795395956207
X	0.01585197283422396	27694	27.694	0.08302952317457642
X	0.016602994750990967	5963	5.963	0.14068265000395855
X	0.015842495996284272	103620	103.62	0.053472030101789976
X	0.01577062471702731	8221	8.221	0.1242533432531356
X	0.015624603414238034	6185	6.185	0.13619351634357546
X	0.01678208744608277	290533	290.533	0.03865599327491301
X	0.016155634742510167	33081	33.081	0.0787496197613854
X	0.01618715760252109	21987	21.987	0.09029593642142641
X	0.016608771087988628	121858	121.858	0.05146292712132682
X	0.01662963100631589	191979	191.979	0.0442462323773288
X	0.01647209716616142	5160	5.16	0.14724245998450095
X	0.015721376682117226	7866	7.866	0.1259637391201996
X	0.0156767938162495	21826	21.826	0.08955593350383856
X	0.015048424082563716	1596	1.596	0.21126102287704474
X	0.0156967987998361	8466	8.466	0.12285069256590272
X	0.015730355204475504	17579	17.579	0.0963639930825104
X	0.015562542865766667	2666	2.666	0.18005567133848818
X	0.015722321070551812	7094	7.094	0.1303792541426512
X	0.015656285983145998	17778	17.778	0.09585216969400745
X	0.015507701058464211	6200	6.2	0.13574335440795712
X	0.016240846851295045	385796	385.796	0.03478700050664575
X	0.015643467714537937	8995	8.995	0.12025707070838718
X	0.01598406227761308	91432	91.432	0.05591516975958055
X	0.01566059442526909	12280	12.28	0.1084434415565873
X	0.015656247296127437	27880	27.88	0.08250209326145563
X	0.015688589571831002	71391	71.391	0.06034576707093987
X	0.016616072175819858	682674	682.674	0.028980441832295017
X	0.015970529422051395	748155	748.155	0.027740178001301068
X	0.01580906884702352	44196	44.196	0.07098628395036673
X	0.016043043599715437	128489	128.489	0.0499812298434907
X	0.015685316204160117	3316	3.316	0.16786477992742505
X	0.01569929406278456	35323	35.323	0.07631457928194267
X	0.015679720440140286	35480	35.48	0.07617016587455983
X	0.016516449272085364	75687	75.687	0.06020489437319098
X	0.015639750443976117	6728	6.728	0.1324691080351335
X	0.015655964882941405	83019	83.019	0.057345689679128904
X	0.01631995611061427	434498	434.498	0.03348963402411443
X	0.015749282110469904	66871	66.871	0.061755311243850905
X	0.0156020789537156	9810	9.81	0.11672695451642011
X	0.016848394604440885	41494	41.494	0.07404988828110642
X	0.015592075036447228	10985	10.985	0.11238317177418701
X	0.01615863398583884	4859	4.859	0.14926314277062494
X	0.016839914587632596	138655	138.655	0.04952237332276135
X	0.015519806116741643	31321	31.321	0.07913165046345662
X	0.015507273799891606	2705	2.705	0.1789737736126422
X	0.016111646624456407	133249	133.249	0.049449130161085916
X	0.016302359813820028	12280	12.28	0.109904979396227
X	0.01524129990430339	1228	1.228	0.231530324615244
X	0.015791240552707975	141848	141.848	0.048105826157082913
X	0.015763860705452885	93131	93.131	0.05531668009693076
X	0.01583735140334642	58861	58.861	0.06455823359740696
X	0.015969415316725678	137559	137.559	0.048782793772782086
X	0.015812697509082756	122218	122.218	0.05057745008946822
X	0.01569409806892513	9900	9.9	0.11660049366899461
X	0.015675515440504127	53884	53.884	0.06626039200543142
X	0.01574458889451065	174888	174.888	0.04481847805513033
X	0.01587447814238009	152354	152.354	0.04705602075839318
X	0.015718930174068314	17372	17.372	0.09672180397289924
X	0.016202503562734588	124282	124.282	0.050705837939006085
X	0.016739035834076144	197808	197.808	0.04390313459134049
X	0.015753079486998035	63351	63.351	0.06288359035730409
X	0.016184055221652373	21226	21.226	0.09135655843846127
X	0.015458662462687204	22136	22.136	0.08872055626942835
X	0.015663885589458946	13185	13.185	0.10591068803984356
X	0.015709377382760077	11008	11.008	0.11258577385996509
X	0.015583754067906304	5774	5.774	0.1392296619811624
X	0.016348769281278628	179998	179.998	0.044950976274974096
X	0.016012459462187285	144293	144.293	0.048054890661673094
X	0.015902299511342373	356086	356.086	0.03547875269949335
X	0.01671885496457946	217471	217.471	0.042520829312060054
X	0.015694681340646166	93720	93.72	0.05511968675496163
X	0.01636442813889673	17176	17.176	0.09839950958875339
X	0.016203861473837095	108155	108.155	0.05311175350273903
X	0.01545036784259777	3161	3.161	0.1697089615083139
X	0.015734625038948215	18115	18.115	0.09541266356038228
X	0.01612826460818316	255093	255.093	0.03983789349942465
X	0.015718071280722504	43614	43.614	0.07116357842390772
X	0.01556825886632658	3627	3.627	0.1625165133178587
X	0.015698714104411586	14299	14.299	0.10316193306445601
X	0.01607187741688633	73897	73.897	0.060137670500429896
X	0.01569654029976881	28769	28.769	0.08171335792081907
X	0.01657008801797758	124566	124.566	0.05104757299642221
X	0.01572333621933011	28811	28.811	0.08172008121960293
X	0.016251894781738118	24830	24.83	0.0868245272821451
X	0.015517260156758856	5558	5.558	0.1408095680770503
X	0.015938237705744752	3448	3.448	0.16658057142079485
X	0.01641182905212018	69333	69.333	0.06185939442161003
X	0.01564122100800007	66099	66.099	0.061852689398186556
X	0.015583843705076672	8466	8.466	0.12255530259183114
X	0.015762121419409705	128210	128.21	0.04972379372559733
X	0.015732162049056617	7742	7.742	0.12666163255919327
X	0.01644430180741979	25332	25.332	0.08658619461037141
X	0.01567231584940032	6212	6.212	0.13613419977887858
X	0.01593381701979741	169886	169.886	0.04543467350392771
X	0.015850849224803277	7285	7.285	0.12958090028141162
X	0.015694733577787355	53398	53.398	0.06648795656799084
X	0.01564571528151003	22096	22.096	0.08913068697036619
X	0.01568876980629147	25878	25.878	0.08463553180825788
X	0.015759742723231298	53408	53.408	0.0665754743488596
X	0.015772896342240883	369299	369.299	0.0349551448267753
X	0.015720597863476352	18798	18.798	0.09421480156295618
X	0.01563562331780497	16034	16.034	0.0991648532002904
X	0.015765052110192947	106661	106.661	0.052872509004235055
X	0.01566749503500717	15851	15.851	0.099612605529405
X	0.016333105899528317	563255	563.255	0.030722360017708075
X	0.015644110926242533	8702	8.702	0.12159356804063057
X	0.01582378455526159	129518	129.518	0.04962037902017261
X	0.016822940400672795	435754	435.754	0.03379767623485856
X	0.015563750193085086	4787	4.787	0.14814384351692803
X	0.015649299406068382	222484	222.484	0.041279272861058904
X	0.015702068147906412	33257	33.257	0.0778678043768256
X	0.01680464286007772	491995	491.995	0.03244562910224823
X	0.015682920486020227	10553	10.553	0.11411699686916961
X	0.016204835727417266	46039	46.039	0.07060567083103758
X	0.015578037876627744	5721	5.721	0.13964120974752395
X	0.016226090802142518	6925	6.925	0.1328203768374179
X	0.015689727007609955	7321	7.321	0.12892832093373416
X	0.01574243083662088	66071	66.071	0.06199456799208712
X	0.015714283032463618	45217	45.217	0.07030684486464202
X	0.0167915840673776	165914	165.914	0.046601820002736764
X	0.01571120730979659	19253	19.253	0.09344808001444482
X	0.015632158909037887	6851	6.851	0.1316502428501316
X	0.01557628624563757	5020	5.02	0.14585457881737987
X	0.016094205320572035	170939	170.939	0.04549281310868432
X	0.016475539301502334	41507	41.507	0.07349189256384996
X	0.01625095965582876	389656	389.656	0.034678945355586785
X	0.015951087581675714	45164	45.164	0.0706858742265101
X	0.015647131736725006	10551	10.551	0.11403732952813145
X	0.015674620989806348	5175	5.175	0.14468679551437783
X	0.015438272799421442	14789	14.789	0.10144250552217529
X	0.01657683041453406	1034265	1034.265	0.025212924429518703
X	0.015662891743927706	13152	13.152	0.1059969532083102
X	0.01571733107708544	17127	17.127	0.09717752515352046
X	0.01573657112201209	116750	116.75	0.05127248970486806
X	0.015639570814523402	61852	61.852	0.06323492792414041
X	0.01566670992122932	12019	12.019	0.10923701455328093
X	0.01568119334970214	4359	4.359	0.153225394049497
X	0.01644739896274275	92780	92.78	0.0561755870574623
X	0.015718452257715645	51526	51.526	0.06731746972705326
X	0.015646699799374178	66137	66.137	0.06184806067343524
X	0.015730120862744796	76809	76.809	0.05894405139723947
X	0.016624755815409238	93132	93.132	0.0563056679970312
X	0.016174815534857296	99473	99.473	0.05458140449887442
X	0.016751532039523013	338483	338.483	0.03671463572613454
X	0.015935297130547046	153187	153.187	0.047030480757069094
X	0.01564170776526737	25252	25.252	0.0852437988642471
X	0.015769976353905993	89006	89.006	0.0561656237163892
X	0.015758834326399525	38977	38.977	0.07394438936886795
X	0.015643130806764313	16846	16.846	0.09756086154226203
X	0.01566244420531326	16673	16.673	0.0979374056843613
X	0.015688533704278723	10583	10.583	0.11402266472655309
X	0.01575030987730541	85339	85.339	0.056935156365503496
X	0.015696018639013876	32703	32.703	0.07829498954711998
X	0.015609326161390163	7194	7.194	0.12946068699524732
X	0.015728543565120625	24875	24.875	0.08583053245900757
X	0.01575576945493424	11375	11.375	0.11147121565075781
X	0.01565440498172243	46762	46.762	0.06943543529130981
X	0.015656800067355488	9302	9.302	0.11895304312195007
X	0.015898544379154392	20254	20.254	0.09224627812112127
X	0.015703077200069106	13712	13.712	0.10462319707683891
X	0.016191098853611093	28859	28.859	0.0824767771261774
X	0.015690092291703598	4357	4.357	0.15327781920326547
X	0.015631088798288444	8917	8.917	0.12057487523518835
X	0.01573949600864962	27618	27.618	0.08290859054785334
X	0.015728543605414237	242910	242.91	0.04015574665426147
X	0.016039778442133345	8671	8.671	0.122756093586194
X	0.015640576454874346	1861	1.861	0.20331470661471535
X	0.015575255807644881	7908	7.908	0.1253495682798209
X	0.01572853879069718	241171	241.171	0.04025202785306374
X	0.01660801100602691	287672	287.672	0.038649155639695426
X	0.015716749720813182	59573	59.573	0.06413637680674401
X	0.015771710655022916	8945	8.945	0.12080905449519068
X	0.01619428212908688	176435	176.435	0.04510856239809132
X	0.01676820739428634	179710	179.71	0.04535635343968025
X	0.015655986560477866	3414	3.414	0.16613927374972695
X	0.015463779554630842	12230	12.23	0.10813419818571378
X	0.015736440474459674	46100	46.1	0.0698878788171837
X	0.015772904024158877	34443	34.443	0.0770791463749284
X	0.015726421558424932	47582	47.582	0.0691399519637675
X	0.015919974705452953	108526	108.526	0.05273952237361192
X	0.015716651204116586	27992	27.992	0.0824977356661272
X	0.01569895418244729	32476	32.476	0.07848187974264893
X	0.016167793213251565	4809	4.809	0.1498069594981983
X	0.01566396684173496	35473	35.473	0.07614965593363683
X	0.01594373414971254	26310	26.31	0.08462325612194116
X	0.015738350875460095	33139	33.139	0.07802011895090287
X	0.01607197558759534	69315	69.315	0.061434738238038314
X	0.015921281642887895	355655	355.655	0.03550719528991924
X	0.016015021128789584	10386	10.386	0.11552954579200642
X	0.015592191280016494	3283	3.283	0.1680913726660644
X	0.015716676111391412	17161	17.161	0.0971119565296248
X	0.0161794645921222	57659	57.659	0.06546849337700032
X	0.015682864986911117	33805	33.805	0.07741316453401625
X	0.015665572144145656	7034	7.034	0.13059141641630437
X	0.01562383949958069	4868	4.868	0.1475069216656207
X	0.01574398320818767	103978	103.978	0.05329965354939671
X	0.015544671647165815	5471	5.471	0.14163533453594299
X	0.01596066775900412	51737	51.737	0.06756938068092576
X	0.015633456807246673	5113	5.113	0.14514199061782182
X	0.015910559162522514	17763	17.763	0.09639541461746248
X	0.015818491181488578	23432	23.432	0.08772402823772875
X	0.01562510684876055	8761	8.761	0.12127085472180227
X	0.015675850992615065	6300	6.3	0.13550756013768675
X	0.015745081843944127	139890	139.89	0.04828209253946766
X	0.015955732153904886	295133	295.133	0.03781229071788447
X	0.015752852865279665	156290	156.29	0.046538174297363184
X	0.016316944517882633	126580	126.58	0.050515491375544834
X	0.015675404952913353	34429	34.429	0.07693042295109717
X	0.015564721896553986	9888	9.888	0.1163262253181691
X	0.01602745817741089	16533	16.533	0.09897017287165273
X	0.01673708010673404	168092	168.092	0.04634941069582844
X	0.016727490725338738	18853	18.853	0.096091161603512
X	0.015740373826834398	28673	28.673	0.08188052826396094
X	0.015700277679875942	10095	10.095	0.11586003892928312
X	0.016818293369749524	411194	411.194	0.03445442858000047
X	0.015697101573768593	38635	38.635	0.07406496989633828
X	0.016611383596537597	42361	42.361	0.07319473922199991
X	0.016264725212535105	197198	197.198	0.043529272424214435
X	0.01590485226121328	50161	50.161	0.06819007233526049
X	0.015741682280791626	61444	61.444	0.0635122093872516
X	0.016807976411788418	227924	227.924	0.041934852902785924
X	0.01573273764413019	164003	164.003	0.04577736979566917
X	0.016058957264602176	57782	57.782	0.06525917674817926
X	0.015692748915402847	17361	17.361	0.09668848669613873
X	0.01574568437870682	51919	51.919	0.06718594151918014
X	0.015851127260119533	13881	13.881	0.10452330482555536
X	0.015743955860487123	119260	119.26	0.05091819729300462
X	0.01639970514024302	736664	736.664	0.028131239247060277
X	0.01655517673236196	153015	153.015	0.047650405952261
X	0.015958955820298874	111020	111.02	0.052384292551909524
X	0.015708780026882776	11631	11.631	0.11053720207686465
X	0.015682533504478013	54466	54.466	0.06603338806366052
X	0.016702835484903304	29342	29.342	0.08287689980004193
X	0.015579967243789235	20010	20.01	0.09199689094962162
X	0.015919989380293303	42742	42.742	0.07194975364230641
X	0.016543814088401415	411785	411.785	0.034249563386274405
X	0.015572723399791741	11736	11.736	0.10988746540095493
X	0.015718704541105962	180638	180.638	0.0443134821497486
X	0.015723792031163208	73435	73.435	0.05982527871820428
X	0.015682360505049196	49760	49.76	0.06805243778172565
X	0.015619881124849928	9419	9.419	0.11836527437582897
X	0.01603373364153237	113634	113.634	0.05206055456971998
X	0.016285741395746428	168495	168.495	0.04589233679315226
X	0.015618152975738242	4211	4.211	0.15479186884538468
X	0.016356927691544672	811873	811.873	0.0272105849778803
X	0.015731086429717777	56403	56.403	0.06533594410010939
X	0.01660880799983628	90496	90.496	0.05682895809298602
X	0.015958520083751378	15648	15.648	0.10065714135589642
X	0.01569952148007809	26680	26.68	0.0837979761808787
X	0.015519810189048822	3168	3.168	0.16983756091245636
X	0.015319896911292166	4476	4.476	0.15070300829735367
X	0.015644818608531702	25020	25.02	0.0855121335370923
X	0.016165590209648712	188543	188.543	0.04409544375149851
X	0.01652584578839673	2055548	2055.548	0.02003297078723127
X	0.01558512605189972	6997	6.997	0.13059688404685468
X	0.01614464785767017	188150	188.15	0.044107060773903514
X	0.015662886823088702	7454	7.454	0.1280837813259207
X	0.01571849682715044	23133	23.133	0.08791433282738617
X	0.01592773610658846	15714	15.714	0.10045134765155443
X	0.01615375753316573	191224	191.224	0.043877686461509566
X	0.015688069772181384	11556	11.556	0.11072711716727746
X	0.016832890455166796	1162169	1162.169	0.02437603904105541
X	0.015640469834041437	8155	8.155	0.12424395754500525
X	0.016144400908701055	158573	158.573	0.046694338830944364
X	0.01570319690673897	58504	58.504	0.06450610789843403
X	0.016024782572654665	87688	87.688	0.0567480130975771
X	0.015735526393757755	213886	213.886	0.041901828248931655
X	0.01572014870405406	23173	23.173	0.08786679714142802
X	0.015688213451298137	8849	8.849	0.12103001808400546
X	0.01601255936741762	88254	88.254	0.056612037393507966
X	0.015854296480925005	9223	9.223	0.11979120187598158
X	0.015477038533495547	22747	22.747	0.08795379414216327
X	0.015757290008006086	170668	170.668	0.04519702088250187
X	0.015984525980893716	49876	49.876	0.06843359919363048
X	0.016340144789311534	40631	40.631	0.07381298973444686
X	0.015729856020974535	40164	40.164	0.07316374538206206
X	0.016233687434102216	256789	256.789	0.03983641559947431
X	0.016492656017926145	448276	448.276	0.033259448049585645
X	0.015747862634678405	60171	60.171	0.06396535230554949
X	0.01617165480313198	9481	9.481	0.11948134567870207
X	0.015705066678755413	166980	166.98	0.045476996679336204
X	0.01569935281778014	44128	44.128	0.0708580454187793
X	0.015881525172471146	51921	51.921	0.06737773171814403
X	0.015875951189928208	75995	75.995	0.05933599716122038
X	0.016709800489154845	63885	63.885	0.06395248951057045
X	0.01578457510312144	112153	112.153	0.05201644492367107
X	0.015644737606901285	245780	245.78	0.03992766513718729
X	0.015870343616610664	32943	32.943	0.07839247832185707
X	0.01587001992160366	24245	24.245	0.08682639330218836
X	0.015981214552586834	4830	4.83	0.14901187649126693
X	0.016029336962290346	18745	18.745	0.09491686188711186
X	0.015728743422908698	85741	85.741	0.05682007830148545
X	0.015624416774213336	116915	116.915	0.0511263191835586
X	0.01612231601756097	192004	192.004	0.0437897417957214
X	0.01567248370622126	9458	9.458	0.1183348975464383
X	0.0157116767957838	25593	25.593	0.08498985989992024
X	0.016361370510344614	77564	77.564	0.05952779929872774
X	0.015814890537242384	177929	177.929	0.044627903376590956
X	0.01613302503698789	54226	54.226	0.06675791220169357
X	0.016158658298281302	67871	67.871	0.06197839845289721
X	0.016194322173798608	295135	295.135	0.037999745371870994
X	0.015748942199216402	37809	37.809	0.07468248025066204
X	0.015533599622759887	2958	2.958	0.17381662479950966
X	0.016156799846323497	8159	8.159	0.12557585310838806
X	0.015903264476852957	198260	198.26	0.04312711364350733
X	0.015723037309941595	36907	36.907	0.07524469740871709
X	0.01575154866822253	78945	78.945	0.058434079907535436
X	0.015615123598656503	3038	3.038	0.1725781803625324
X	0.015720929039471098	3746	3.746	0.1613009077455454
X	0.015723450853065916	180672	180.672	0.04431516171440679
X	0.015693397282307297	108296	108.296	0.05252525680439828
X	0.01569615424863627	60992	60.992	0.06360727559736438
X	0.016084920311728316	6001	6.001	0.13890944057397303
X	0.015726886722186317	6961	6.961	0.13121708177618369
X	0.0157250286012165	44423	44.423	0.07073936982078087
X	0.01571705422424444	51358	51.358	0.0673887937837619
X	0.015719599175436298	100976	100.976	0.05379489965296234
X	0.015719379316899436	8445	8.445	0.1230113675354476
X	0.0161864436239828	54963	54.963	0.06653145222616694
X	0.01566584582538481	25256	25.256	0.08528312252753756
X	0.015502069399718878	28191	28.191	0.08192687667237533
X	0.01661561617063254	269315	269.315	0.039514088611329636
X	0.015707664108475218	9694	9.694	0.11745446476044602
X	0.01572812556190965	47009	47.009	0.06942224463092084
X	0.016088679733960786	142300	142.3	0.0483546787293445
X	0.015764558056499994	39243	39.243	0.07378586959188276
X	0.015788175191314865	64663	64.663	0.06250172865317902
X	0.015742879560289955	166721	166.721	0.04553702164579798
X	0.015680452532434188	2953	2.953	0.17446100972971346
X	0.015727229396283328	56437	56.437	0.06531748192820495
X	0.015768733728226413	25586	25.586	0.0851003751737682
X	0.015789064228752286	113799	113.799	0.051769342419737005
X	0.016315486737081313	99095	99.095	0.0548086911186718
X	0.015603470446941613	46946	46.946	0.06926931124986303
X	0.015839805191557085	89304	89.304	0.05618576625569488
X	0.01571969563641909	17143	17.143	0.09715215444596921
X	0.015561084091462521	22614	22.614	0.08828511504125809
X	0.015726175889031475	15840	15.84	0.09975989510474899
X	0.01562790160125801	14842	14.842	0.1017347714137552
X	0.0157417029365471	127153	127.153	0.04983965573209064
X	0.016673762798246405	293530	293.53	0.03844091425507046
X	0.016055262265220777	10352	10.352	0.11575267787396834
X	0.015725112464386206	23151	23.151	0.08790387126736106
X	0.015719000459487695	15180	15.18	0.10116983732959092
X	0.016216603825481336	299986	299.986	0.03781112854719667
X	0.015738409884407712	49906	49.906	0.06806690909312291
X	0.015751963749548542	17039	17.039	0.09741597918959026
X	0.015839816630560534	92924	92.924	0.055446493300995436
X	0.015750229757852344	100238	100.238	0.053961601891797245
X	0.01568975143710489	13101	13.101	0.10619495182746351
X	0.016196273080375852	70773	70.773	0.06116680956146876
X	0.0157134331021911	11095	11.095	0.11230038835197817
time for making epsilon is 2.5370635986328125
epsilons are
[0.21429371260135763, 0.09302657473047868, 0.16110229957554367, 0.1806737175664726, 0.0673815122697156, 0.06131606286864734, 0.06079408828603592, 0.06419139126866291, 0.13433583412177963, 0.06349926708895784, 0.11126994816083653, 0.05717472901560212, 0.1314734790262025, 0.037249995803756486, 0.08371705794632989, 0.05582818799152203, 0.05965618632222962, 0.07038232487888288, 0.03968066923385194, 0.04803181324589809, 0.07551854139932906, 0.03269971372690379, 0.0895691535629788, 0.09990303102078686, 0.08924470763771938, 0.05751449636214911, 0.0725523522069588, 0.11647030503328334, 0.05863434863888884, 0.02002415973704361, 0.12260814594004746, 0.031117398130161532, 0.10457787664076451, 0.10854033197018159, 0.09651035983591291, 0.04533330329983891, 0.05226623254335501, 0.1943758359762654, 0.13878273587761752, 0.1576666028993999, 0.14028650949002472, 0.21922092169489793, 0.22682297051195996, 0.1762907094495986, 0.225134540360363, 0.2031833994919018, 0.12536526623376215, 0.1411307012633286, 0.19290520988942042, 0.10359111645942667, 0.10018434913703232, 0.18670460526396288, 0.1335442945680746, 0.16633748761469475, 0.12449117641055445, 0.13507043512897396, 0.1743296537681817, 0.15135522524978004, 0.15708063544528383, 0.18103931239782733, 0.1336010976414949, 0.18486877455357364, 0.11437830272933602, 0.11931397451402206, 0.1635123308462834, 0.16499840295279217, 0.16471493752995686, 0.11779189019747972, 0.13883190261999542, 0.17855080497758655, 0.15862840499585462, 0.16125880784073227, 0.15200408679819066, 0.16254012492599762, 0.15674316648117184, 0.15249031961310835, 0.21350214626347683, 0.24066158117248687, 0.14535543422437094, 0.10018632434024698, 0.16389729563053365, 0.20795919132814844, 0.13001793719019158, 0.227538539615095, 0.18051961255755453, 0.19746960311819892, 0.17584299001794595, 0.20323500413567136, 0.13372247575785404, 0.16785925789652742, 0.1489051473117584, 0.20311994492084914, 0.1984625207101143, 0.15157026219017355, 0.1512238638455585, 0.14254159798669033, 0.15745242323303177, 0.14218935499306654, 0.1425816255018704, 0.15220862215662478, 0.10634815431791117, 0.1515866699271972, 0.13423908666400602, 0.20552078244006944, 0.1449579174586173, 0.1745491161160382, 0.11979652902986553, 0.24143677166222308, 0.18594704019980637, 0.20205702861155422, 0.20469485589702616, 0.23172804731166197, 0.1898677192762707, 0.1528071908854754, 0.13476606467390218, 0.16734186008552118, 0.16915030813011003, 0.2264033138282731, 0.12644530315754698, 0.20181359220842537, 0.09237755313360164, 0.18101509643570685, 0.18877441877268322, 0.17538397346844065, 0.1844385360379339, 0.18820022209121717, 0.1887825298681622, 0.1589465393566994, 0.0971778066898216, 0.158062979856462, 0.1135039914172473, 0.2002127256788825, 0.15031287221360837, 0.1934670933295416, 0.15510217993379866, 0.18947447491959798, 0.2064470514579296, 0.17608543348364647, 0.18705408607602847, 0.17353248912904842, 0.15747311710209427, 0.23618712208987058, 0.11099293754209959, 0.11277750407891125, 0.13076408892922886, 0.18931634563875885, 0.1840028205919863, 0.16440174124586857, 0.1854554186565759, 0.174665500124961, 0.18860100095335416, 0.13921327273259695, 0.15254617016445823, 0.11602335298133078, 0.15643340799569377, 0.10621848723450802, 0.15121463843733518, 0.17492903929355227, 0.15087617594513655, 0.2751239203068173, 0.11269175515243864, 0.1861222152903351, 0.15122910945630688, 0.21019552933628585, 0.1609401362108966, 0.19857324966481216, 0.16676357600657965, 0.15522284043260415, 0.1480464772275575, 0.26165493323176076, 0.22365947338826037, 0.2052193167044471, 0.1217218071006379, 0.17705841106638426, 0.1287981815820373, 0.1540702732945929, 0.13192548607655424, 0.16265895676233436, 0.14533209651759627, 0.16651571610140978, 0.15821344830883402, 0.17481116515147396, 0.13192961666892067, 0.17392565349484676, 0.12709926759280213, 0.14556950496615495, 0.17111376955367977, 0.14812579476514398, 0.13263739325618337, 0.10372027417487632, 0.17651474036636525, 0.2546234212278782, 0.15311413804510376, 0.17445565527026546, 0.16122301275960696, 0.17325160348592866, 0.1765149074037088, 0.19903480962997672, 0.17898371921468897, 0.18728570540536504, 0.14128982163663023, 0.18755963276701565, 0.10529812413793213, 0.13312516116320122, 0.1527888240935741, 0.20352658182131314, 0.13726884426563307, 0.14449481426741786, 0.16844400075601387, 0.10916761684952123, 0.14194644868904052, 0.1579447380225131, 0.16636434152113913, 0.15950402795719898, 0.22338866231492183, 0.20558992608445908, 0.2214644034730704, 0.24207289131930976, 0.14980492412896534, 0.16509134817882318, 0.15370584189395212, 0.19973288607824716, 0.14278567705427808, 0.15424716229678692, 0.15665126303374258, 0.15255329912309193, 0.18392608590792983, 0.23202540494464757, 0.1364534959551446, 0.2575648731884143, 0.19614934569074807, 0.17584114670634823, 0.16431802961829248, 0.14804911227374087, 0.17635107714416792, 0.17193877105422123, 0.1580694637662968, 0.13192358162637793, 0.18817028524166549, 0.11408323592052343, 0.07211391328351283, 0.09470382297743027, 0.13119964495343106, 0.2281252816379041, 0.12803888367837168, 0.046063768002878834, 0.03436264211790535, 0.1471497703620185, 0.05278017950864438, 0.07150475035742702, 0.06414655875005619, 0.033745372010533375, 0.1756823153259079, 0.14137333972169175, 0.04008523000207945, 0.030012448299856137, 0.13373474098761504, 0.07571137264953541, 0.04902419455236462, 0.059774708167949796, 0.059983015273920734, 0.07310275870557643, 0.07718289789886142, 0.10758837068245777, 0.14670397191606008, 0.032250028394645386, 0.12338070999435849, 0.14389001625304174, 0.06542739975282066, 0.06888937260488985, 0.03809081654062001, 0.050496567405020516, 0.20123107018957515, 0.06538774248982109, 0.11958617126588274, 0.07010799182264388, 0.05857514799996, 0.14124201740638934, 0.06409123128675852, 0.232341558056365, 0.12962342202117189, 0.054756352881539565, 0.0646013938597125, 0.07410040789049133, 0.07165879564691885, 0.038613433240584895, 0.10049721357117214, 0.0851424086660916, 0.05807727419752745, 0.1336309832285148, 0.046392524728431174, 0.03953066031899806, 0.03601801788333324, 0.07897060020808301, 0.07676103942344553, 0.09857367637876886, 0.1342834674296006, 0.07912628424168706, 0.11031406593733432, 0.08595680754074081, 0.05833108162628613, 0.06880493124350605, 0.1161536736198224, 0.11602245354699742, 0.06095915384458151, 0.1007276104050898, 0.11661190265882299, 0.10667540846036655, 0.03248603392037591, 0.0997315724453062, 0.05245106636343351, 0.12094295841216313, 0.06592997536956541, 0.0898801690725009, 0.0368067441915694, 0.07061926382030832, 0.10487676091109879, 0.04716503319770276, 0.05195387949807342, 0.11851704865849261, 0.05667662171493187, 0.04998863199525882, 0.0354874056156468, 0.07825157273276881, 0.16753680818138006, 0.09609754501860494, 0.04488645663284602, 0.04836842291144641, 0.09311470868725431, 0.18301325543304306, 0.06645292689632815, 0.049739163083630494, 0.05182483850926681, 0.13541322087480476, 0.07496700498850918, 0.09527757859522572, 0.12182228756855608, 0.10384660897291755, 0.11804465427046192, 0.0716811570326454, 0.06767969320033791, 0.08979012376118763, 0.09987610219857337, 0.11493894145795285, 0.03149186709153048, 0.10441644508850241, 0.045968679009527544, 0.19771742358296862, 0.13285827954237414, 0.1499795395956207, 0.08302952317457642, 0.14068265000395855, 0.053472030101789976, 0.1242533432531356, 0.13619351634357546, 0.03865599327491301, 0.0787496197613854, 0.09029593642142641, 0.05146292712132682, 0.0442462323773288, 0.14724245998450095, 0.1259637391201996, 0.08955593350383856, 0.21126102287704474, 0.12285069256590272, 0.0963639930825104, 0.18005567133848818, 0.1303792541426512, 0.09585216969400745, 0.13574335440795712, 0.03478700050664575, 0.12025707070838718, 0.05591516975958055, 0.1084434415565873, 0.08250209326145563, 0.06034576707093987, 0.028980441832295017, 0.027740178001301068, 0.07098628395036673, 0.0499812298434907, 0.16786477992742505, 0.07631457928194267, 0.07617016587455983, 0.06020489437319098, 0.1324691080351335, 0.057345689679128904, 0.03348963402411443, 0.061755311243850905, 0.11672695451642011, 0.07404988828110642, 0.11238317177418701, 0.14926314277062494, 0.04952237332276135, 0.07913165046345662, 0.1789737736126422, 0.049449130161085916, 0.109904979396227, 0.231530324615244, 0.048105826157082913, 0.05531668009693076, 0.06455823359740696, 0.048782793772782086, 0.05057745008946822, 0.11660049366899461, 0.06626039200543142, 0.04481847805513033, 0.04705602075839318, 0.09672180397289924, 0.050705837939006085, 0.04390313459134049, 0.06288359035730409, 0.09135655843846127, 0.08872055626942835, 0.10591068803984356, 0.11258577385996509, 0.1392296619811624, 0.044950976274974096, 0.048054890661673094, 0.03547875269949335, 0.042520829312060054, 0.05511968675496163, 0.09839950958875339, 0.05311175350273903, 0.1697089615083139, 0.09541266356038228, 0.03983789349942465, 0.07116357842390772, 0.1625165133178587, 0.10316193306445601, 0.060137670500429896, 0.08171335792081907, 0.05104757299642221, 0.08172008121960293, 0.0868245272821451, 0.1408095680770503, 0.16658057142079485, 0.06185939442161003, 0.061852689398186556, 0.12255530259183114, 0.04972379372559733, 0.12666163255919327, 0.08658619461037141, 0.13613419977887858, 0.04543467350392771, 0.12958090028141162, 0.06648795656799084, 0.08913068697036619, 0.08463553180825788, 0.0665754743488596, 0.0349551448267753, 0.09421480156295618, 0.0991648532002904, 0.052872509004235055, 0.099612605529405, 0.030722360017708075, 0.12159356804063057, 0.04962037902017261, 0.03379767623485856, 0.14814384351692803, 0.041279272861058904, 0.0778678043768256, 0.03244562910224823, 0.11411699686916961, 0.07060567083103758, 0.13964120974752395, 0.1328203768374179, 0.12892832093373416, 0.06199456799208712, 0.07030684486464202, 0.046601820002736764, 0.09344808001444482, 0.1316502428501316, 0.14585457881737987, 0.04549281310868432, 0.07349189256384996, 0.034678945355586785, 0.0706858742265101, 0.11403732952813145, 0.14468679551437783, 0.10144250552217529, 0.025212924429518703, 0.1059969532083102, 0.09717752515352046, 0.05127248970486806, 0.06323492792414041, 0.10923701455328093, 0.153225394049497, 0.0561755870574623, 0.06731746972705326, 0.06184806067343524, 0.05894405139723947, 0.0563056679970312, 0.05458140449887442, 0.03671463572613454, 0.047030480757069094, 0.0852437988642471, 0.0561656237163892, 0.07394438936886795, 0.09756086154226203, 0.0979374056843613, 0.11402266472655309, 0.056935156365503496, 0.07829498954711998, 0.12946068699524732, 0.08583053245900757, 0.11147121565075781, 0.06943543529130981, 0.11895304312195007, 0.09224627812112127, 0.10462319707683891, 0.0824767771261774, 0.15327781920326547, 0.12057487523518835, 0.08290859054785334, 0.04015574665426147, 0.122756093586194, 0.20331470661471535, 0.1253495682798209, 0.04025202785306374, 0.038649155639695426, 0.06413637680674401, 0.12080905449519068, 0.04510856239809132, 0.04535635343968025, 0.16613927374972695, 0.10813419818571378, 0.0698878788171837, 0.0770791463749284, 0.0691399519637675, 0.05273952237361192, 0.0824977356661272, 0.07848187974264893, 0.1498069594981983, 0.07614965593363683, 0.08462325612194116, 0.07802011895090287, 0.061434738238038314, 0.03550719528991924, 0.11552954579200642, 0.1680913726660644, 0.0971119565296248, 0.06546849337700032, 0.07741316453401625, 0.13059141641630437, 0.1475069216656207, 0.05329965354939671, 0.14163533453594299, 0.06756938068092576, 0.14514199061782182, 0.09639541461746248, 0.08772402823772875, 0.12127085472180227, 0.13550756013768675, 0.04828209253946766, 0.03781229071788447, 0.046538174297363184, 0.050515491375544834, 0.07693042295109717, 0.1163262253181691, 0.09897017287165273, 0.04634941069582844, 0.096091161603512, 0.08188052826396094, 0.11586003892928312, 0.03445442858000047, 0.07406496989633828, 0.07319473922199991, 0.043529272424214435, 0.06819007233526049, 0.0635122093872516, 0.041934852902785924, 0.04577736979566917, 0.06525917674817926, 0.09668848669613873, 0.06718594151918014, 0.10452330482555536, 0.05091819729300462, 0.028131239247060277, 0.047650405952261, 0.052384292551909524, 0.11053720207686465, 0.06603338806366052, 0.08287689980004193, 0.09199689094962162, 0.07194975364230641, 0.034249563386274405, 0.10988746540095493, 0.0443134821497486, 0.05982527871820428, 0.06805243778172565, 0.11836527437582897, 0.05206055456971998, 0.04589233679315226, 0.15479186884538468, 0.0272105849778803, 0.06533594410010939, 0.05682895809298602, 0.10065714135589642, 0.0837979761808787, 0.16983756091245636, 0.15070300829735367, 0.0855121335370923, 0.04409544375149851, 0.02003297078723127, 0.13059688404685468, 0.044107060773903514, 0.1280837813259207, 0.08791433282738617, 0.10045134765155443, 0.043877686461509566, 0.11072711716727746, 0.02437603904105541, 0.12424395754500525, 0.046694338830944364, 0.06450610789843403, 0.0567480130975771, 0.041901828248931655, 0.08786679714142802, 0.12103001808400546, 0.056612037393507966, 0.11979120187598158, 0.08795379414216327, 0.04519702088250187, 0.06843359919363048, 0.07381298973444686, 0.07316374538206206, 0.03983641559947431, 0.033259448049585645, 0.06396535230554949, 0.11948134567870207, 0.045476996679336204, 0.0708580454187793, 0.06737773171814403, 0.05933599716122038, 0.06395248951057045, 0.05201644492367107, 0.03992766513718729, 0.07839247832185707, 0.08682639330218836, 0.14901187649126693, 0.09491686188711186, 0.05682007830148545, 0.0511263191835586, 0.0437897417957214, 0.1183348975464383, 0.08498985989992024, 0.05952779929872774, 0.044627903376590956, 0.06675791220169357, 0.06197839845289721, 0.037999745371870994, 0.07468248025066204, 0.17381662479950966, 0.12557585310838806, 0.04312711364350733, 0.07524469740871709, 0.058434079907535436, 0.1725781803625324, 0.1613009077455454, 0.04431516171440679, 0.05252525680439828, 0.06360727559736438, 0.13890944057397303, 0.13121708177618369, 0.07073936982078087, 0.0673887937837619, 0.05379489965296234, 0.1230113675354476, 0.06653145222616694, 0.08528312252753756, 0.08192687667237533, 0.039514088611329636, 0.11745446476044602, 0.06942224463092084, 0.0483546787293445, 0.07378586959188276, 0.06250172865317902, 0.04553702164579798, 0.17446100972971346, 0.06531748192820495, 0.0851003751737682, 0.051769342419737005, 0.0548086911186718, 0.06926931124986303, 0.05618576625569488, 0.09715215444596921, 0.08828511504125809, 0.09975989510474899, 0.1017347714137552, 0.04983965573209064, 0.03844091425507046, 0.11575267787396834, 0.08790387126736106, 0.10116983732959092, 0.03781112854719667, 0.06806690909312291, 0.09741597918959026, 0.055446493300995436, 0.053961601891797245, 0.10619495182746351, 0.06116680956146876, 0.11230038835197817]
0.09089975092677936
Making ranges
torch.Size([39478, 2])
We keep 6.45e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([4109, 2])
We keep 1.34e+05/2.51e+06 =  5% of the original kernel matrix.

torch.Size([14151, 2])
We keep 9.91e+05/3.56e+07 =  2% of the original kernel matrix.

torch.Size([33547, 2])
We keep 7.80e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([37965, 2])
We keep 6.58e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([8972, 2])
We keep 6.16e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([19546, 2])
We keep 1.92e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([6575, 2])
We keep 3.40e+05/7.21e+06 =  4% of the original kernel matrix.

torch.Size([17058, 2])
We keep 1.44e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([87279, 2])
We keep 6.79e+07/2.86e+09 =  2% of the original kernel matrix.

torch.Size([58909, 2])
We keep 1.44e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([123100, 2])
We keep 6.63e+07/4.66e+09 =  1% of the original kernel matrix.

torch.Size([70579, 2])
We keep 1.79e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([120293, 2])
We keep 6.38e+07/4.91e+09 =  1% of the original kernel matrix.

torch.Size([69901, 2])
We keep 1.83e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([108889, 2])
We keep 4.27e+07/3.55e+09 =  1% of the original kernel matrix.

torch.Size([65908, 2])
We keep 1.58e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([14038, 2])
We keep 1.45e+06/4.42e+07 =  3% of the original kernel matrix.

torch.Size([24069, 2])
We keep 2.73e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([72975, 2])
We keep 2.49e+08/3.73e+09 =  6% of the original kernel matrix.

torch.Size([53749, 2])
We keep 1.58e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([21187, 2])
We keep 4.12e+06/1.30e+08 =  3% of the original kernel matrix.

torch.Size([29633, 2])
We keep 4.19e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([125570, 2])
We keep 1.20e+08/7.10e+09 =  1% of the original kernel matrix.

torch.Size([70728, 2])
We keep 2.13e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([15119, 2])
We keep 1.42e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([24895, 2])
We keep 2.92e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([573874, 2])
We keep 8.60e+08/9.27e+10 =  0% of the original kernel matrix.

torch.Size([157511, 2])
We keep 6.60e+07/6.84e+09 =  0% of the original kernel matrix.

torch.Size([45865, 2])
We keep 1.09e+07/7.17e+08 =  1% of the original kernel matrix.

torch.Size([43513, 2])
We keep 8.10e+06/6.01e+08 =  1% of the original kernel matrix.

torch.Size([159698, 2])
We keep 1.06e+08/8.17e+09 =  1% of the original kernel matrix.

torch.Size([81597, 2])
We keep 2.27e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([131735, 2])
We keep 9.76e+07/5.58e+09 =  1% of the original kernel matrix.

torch.Size([73365, 2])
We keep 1.94e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([74250, 2])
We keep 3.87e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([54542, 2])
We keep 1.25e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([448551, 2])
We keep 8.38e+08/7.05e+10 =  1% of the original kernel matrix.

torch.Size([138884, 2])
We keep 5.82e+07/5.96e+09 =  0% of the original kernel matrix.

torch.Size([244048, 2])
We keep 2.87e+08/2.18e+10 =  1% of the original kernel matrix.

torch.Size([102616, 2])
We keep 3.49e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([51162, 2])
We keep 6.57e+07/1.36e+09 =  4% of the original kernel matrix.

torch.Size([44577, 2])
We keep 1.09e+07/8.29e+08 =  1% of the original kernel matrix.

torch.Size([836580, 2])
We keep 1.80e+09/2.18e+11 =  0% of the original kernel matrix.

torch.Size([197455, 2])
We keep 9.61e+07/1.05e+10 =  0% of the original kernel matrix.

torch.Size([38468, 2])
We keep 7.76e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([41343, 2])
We keep 7.06e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([28922, 2])
We keep 7.00e+06/2.63e+08 =  2% of the original kernel matrix.

torch.Size([35250, 2])
We keep 5.45e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([26364, 2])
We keep 2.42e+07/4.84e+08 =  4% of the original kernel matrix.

torch.Size([32176, 2])
We keep 6.90e+06/4.94e+08 =  1% of the original kernel matrix.

torch.Size([147148, 2])
We keep 6.59e+07/6.84e+09 =  0% of the original kernel matrix.

torch.Size([77792, 2])
We keep 2.08e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([52753, 2])
We keep 4.71e+07/1.74e+09 =  2% of the original kernel matrix.

torch.Size([44588, 2])
We keep 1.20e+07/9.38e+08 =  1% of the original kernel matrix.

torch.Size([19666, 2])
We keep 3.82e+06/9.92e+07 =  3% of the original kernel matrix.

torch.Size([28487, 2])
We keep 3.71e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([120985, 2])
We keep 1.06e+08/6.08e+09 =  1% of the original kernel matrix.

torch.Size([70124, 2])
We keep 2.01e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([4452560, 2])
We keep 2.25e+10/4.28e+12 =  0% of the original kernel matrix.

torch.Size([468723, 2])
We keep 3.86e+08/4.65e+10 =  0% of the original kernel matrix.

torch.Size([16114, 2])
We keep 4.17e+06/7.23e+07 =  5% of the original kernel matrix.

torch.Size([25652, 2])
We keep 3.31e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([900235, 2])
We keep 4.48e+09/2.98e+11 =  1% of the original kernel matrix.

torch.Size([204932, 2])
We keep 1.14e+08/1.23e+10 =  0% of the original kernel matrix.

torch.Size([24561, 2])
We keep 5.09e+06/1.88e+08 =  2% of the original kernel matrix.

torch.Size([31985, 2])
We keep 4.82e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([23461, 2])
We keep 3.10e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([31486, 2])
We keep 4.36e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([28073, 2])
We keep 8.45e+06/3.12e+08 =  2% of the original kernel matrix.

torch.Size([33975, 2])
We keep 5.93e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([251774, 2])
We keep 5.71e+08/2.86e+10 =  2% of the original kernel matrix.

torch.Size([104853, 2])
We keep 4.01e+07/3.80e+09 =  1% of the original kernel matrix.

torch.Size([194411, 2])
We keep 1.94e+08/1.29e+10 =  1% of the original kernel matrix.

torch.Size([91182, 2])
We keep 2.79e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([5055, 2])
We keep 2.17e+05/4.28e+06 =  5% of the original kernel matrix.

torch.Size([15066, 2])
We keep 1.19e+06/4.65e+07 =  2% of the original kernel matrix.

torch.Size([12102, 2])
We keep 1.53e+06/3.43e+07 =  4% of the original kernel matrix.

torch.Size([22203, 2])
We keep 2.52e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([8636, 2])
We keep 8.01e+05/1.60e+07 =  5% of the original kernel matrix.

torch.Size([19045, 2])
We keep 1.92e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([12680, 2])
We keep 1.00e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([22936, 2])
We keep 2.46e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([2922, 2])
We keep 1.83e+05/2.11e+06 =  8% of the original kernel matrix.

torch.Size([11714, 2])
We keep 9.41e+05/3.27e+07 =  2% of the original kernel matrix.

torch.Size([3284, 2])
We keep 1.06e+05/1.77e+06 =  5% of the original kernel matrix.

torch.Size([12685, 2])
We keep 8.87e+05/2.99e+07 =  2% of the original kernel matrix.

torch.Size([6421, 2])
We keep 4.02e+05/8.47e+06 =  4% of the original kernel matrix.

torch.Size([16507, 2])
We keep 1.52e+06/6.54e+07 =  2% of the original kernel matrix.

torch.Size([3371, 2])
We keep 1.17e+05/1.82e+06 =  6% of the original kernel matrix.

torch.Size([12905, 2])
We keep 8.83e+05/3.03e+07 =  2% of the original kernel matrix.

torch.Size([4718, 2])
We keep 1.89e+05/3.47e+06 =  5% of the original kernel matrix.

torch.Size([14835, 2])
We keep 1.11e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([14153, 2])
We keep 2.84e+06/6.36e+07 =  4% of the original kernel matrix.

torch.Size([24000, 2])
We keep 3.23e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([11708, 2])
We keep 1.03e+06/3.13e+07 =  3% of the original kernel matrix.

torch.Size([21707, 2])
We keep 2.44e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([5421, 2])
We keep 2.20e+05/4.70e+06 =  4% of the original kernel matrix.

torch.Size([15564, 2])
We keep 1.23e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([24310, 2])
We keep 5.73e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([31750, 2])
We keep 4.96e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([27844, 2])
We keep 5.87e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([34416, 2])
We keep 5.52e+06/3.61e+08 =  1% of the original kernel matrix.

torch.Size([5681, 2])
We keep 2.86e+05/5.79e+06 =  4% of the original kernel matrix.

torch.Size([15877, 2])
We keep 1.34e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([12897, 2])
We keep 1.73e+06/4.33e+07 =  4% of the original kernel matrix.

torch.Size([22827, 2])
We keep 2.78e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([7434, 2])
We keep 5.65e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([17689, 2])
We keep 1.72e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([16667, 2])
We keep 1.95e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([26016, 2])
We keep 3.21e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([12237, 2])
We keep 1.74e+06/4.04e+07 =  4% of the original kernel matrix.

torch.Size([22410, 2])
We keep 2.70e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([6968, 2])
We keep 3.82e+05/8.77e+06 =  4% of the original kernel matrix.

torch.Size([17282, 2])
We keep 1.53e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([10690, 2])
We keep 7.51e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([21055, 2])
We keep 2.07e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([9034, 2])
We keep 6.13e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([19426, 2])
We keep 1.92e+06/9.06e+07 =  2% of the original kernel matrix.

torch.Size([5893, 2])
We keep 3.38e+05/6.80e+06 =  4% of the original kernel matrix.

torch.Size([16057, 2])
We keep 1.43e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([13781, 2])
We keep 1.40e+06/4.64e+07 =  3% of the original kernel matrix.

torch.Size([23660, 2])
We keep 2.83e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([5815, 2])
We keep 3.12e+05/6.11e+06 =  5% of the original kernel matrix.

torch.Size([16101, 2])
We keep 1.35e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([20673, 2])
We keep 3.20e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([29671, 2])
We keep 4.18e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([18826, 2])
We keep 2.01e+06/8.49e+07 =  2% of the original kernel matrix.

torch.Size([27809, 2])
We keep 3.53e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([8019, 2])
We keep 5.22e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([18292, 2])
We keep 1.76e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([8316, 2])
We keep 4.89e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([18756, 2])
We keep 1.73e+06/7.81e+07 =  2% of the original kernel matrix.

torch.Size([8150, 2])
We keep 4.93e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([18608, 2])
We keep 1.71e+06/7.85e+07 =  2% of the original kernel matrix.

torch.Size([17286, 2])
We keep 2.34e+06/9.16e+07 =  2% of the original kernel matrix.

torch.Size([26387, 2])
We keep 3.64e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([12940, 2])
We keep 1.11e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([23003, 2])
We keep 2.52e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([6556, 2])
We keep 3.68e+05/7.58e+06 =  4% of the original kernel matrix.

torch.Size([16766, 2])
We keep 1.46e+06/6.19e+07 =  2% of the original kernel matrix.

torch.Size([7988, 2])
We keep 7.95e+05/1.53e+07 =  5% of the original kernel matrix.

torch.Size([17981, 2])
We keep 1.88e+06/8.80e+07 =  2% of the original kernel matrix.

torch.Size([8550, 2])
We keep 5.24e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([18851, 2])
We keep 1.81e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([10380, 2])
We keep 6.84e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([20663, 2])
We keep 2.06e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([7312, 2])
We keep 6.58e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([17593, 2])
We keep 1.80e+06/8.20e+07 =  2% of the original kernel matrix.

torch.Size([8970, 2])
We keep 6.07e+05/1.61e+07 =  3% of the original kernel matrix.

torch.Size([19146, 2])
We keep 1.90e+06/9.01e+07 =  2% of the original kernel matrix.

torch.Size([9799, 2])
We keep 7.99e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([20167, 2])
We keep 2.05e+06/9.91e+07 =  2% of the original kernel matrix.

torch.Size([4098, 2])
We keep 1.49e+05/2.47e+06 =  6% of the original kernel matrix.

torch.Size([14039, 2])
We keep 9.89e+05/3.53e+07 =  2% of the original kernel matrix.

torch.Size([3102, 2])
We keep 7.72e+04/1.24e+06 =  6% of the original kernel matrix.

torch.Size([12668, 2])
We keep 7.87e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([11204, 2])
We keep 1.10e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([21387, 2])
We keep 2.25e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([26922, 2])
We keep 4.59e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([33700, 2])
We keep 5.31e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([8363, 2])
We keep 4.91e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([18782, 2])
We keep 1.73e+06/7.94e+07 =  2% of the original kernel matrix.

torch.Size([4257, 2])
We keep 1.82e+05/3.02e+06 =  6% of the original kernel matrix.

torch.Size([14092, 2])
We keep 1.06e+06/3.90e+07 =  2% of the original kernel matrix.

torch.Size([14640, 2])
We keep 1.76e+06/5.53e+07 =  3% of the original kernel matrix.

torch.Size([24523, 2])
We keep 3.04e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([3435, 2])
We keep 1.01e+05/1.72e+06 =  5% of the original kernel matrix.

torch.Size([13113, 2])
We keep 8.72e+05/2.95e+07 =  2% of the original kernel matrix.

torch.Size([6586, 2])
We keep 3.33e+05/7.08e+06 =  4% of the original kernel matrix.

torch.Size([16967, 2])
We keep 1.43e+06/5.98e+07 =  2% of the original kernel matrix.

torch.Size([5208, 2])
We keep 2.07e+05/4.07e+06 =  5% of the original kernel matrix.

torch.Size([15467, 2])
We keep 1.18e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([6248, 2])
We keep 4.13e+05/8.09e+06 =  5% of the original kernel matrix.

torch.Size([16409, 2])
We keep 1.50e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([4532, 2])
We keep 2.11e+05/3.39e+06 =  6% of the original kernel matrix.

torch.Size([14577, 2])
We keep 1.11e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([13475, 2])
We keep 1.54e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([23308, 2])
We keep 2.75e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([7968, 2])
We keep 4.39e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([18443, 2])
We keep 1.66e+06/7.44e+07 =  2% of the original kernel matrix.

torch.Size([9988, 2])
We keep 9.15e+05/2.25e+07 =  4% of the original kernel matrix.

torch.Size([20256, 2])
We keep 2.16e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([4800, 2])
We keep 1.74e+05/3.51e+06 =  4% of the original kernel matrix.

torch.Size([14999, 2])
We keep 1.12e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([4880, 2])
We keep 2.12e+05/3.98e+06 =  5% of the original kernel matrix.

torch.Size([15075, 2])
We keep 1.18e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([9617, 2])
We keep 9.16e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([19768, 2])
We keep 2.09e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([9622, 2])
We keep 8.01e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([19767, 2])
We keep 2.09e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([11900, 2])
We keep 9.25e+05/2.94e+07 =  3% of the original kernel matrix.

torch.Size([21980, 2])
We keep 2.37e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([8985, 2])
We keep 6.59e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([19313, 2])
We keep 1.94e+06/9.12e+07 =  2% of the original kernel matrix.

torch.Size([11290, 2])
We keep 1.28e+06/2.97e+07 =  4% of the original kernel matrix.

torch.Size([21391, 2])
We keep 2.41e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([11687, 2])
We keep 1.03e+06/2.93e+07 =  3% of the original kernel matrix.

torch.Size([21828, 2])
We keep 2.38e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([10619, 2])
We keep 6.70e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([21018, 2])
We keep 2.05e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([24420, 2])
We keep 3.47e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([32005, 2])
We keep 4.62e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([10246, 2])
We keep 7.47e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([20563, 2])
We keep 2.07e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([13946, 2])
We keep 1.29e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([23859, 2])
We keep 2.72e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([4233, 2])
We keep 1.74e+05/3.19e+06 =  5% of the original kernel matrix.

torch.Size([14031, 2])
We keep 1.08e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([10677, 2])
We keep 9.45e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([20837, 2])
We keep 2.30e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([6876, 2])
We keep 4.42e+05/9.23e+06 =  4% of the original kernel matrix.

torch.Size([17186, 2])
We keep 1.59e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([18554, 2])
We keep 2.42e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([27635, 2])
We keep 3.51e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([2727, 2])
We keep 1.00e+05/1.23e+06 =  8% of the original kernel matrix.

torch.Size([11901, 2])
We keep 7.87e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([6084, 2])
We keep 2.62e+05/5.90e+06 =  4% of the original kernel matrix.

torch.Size([16362, 2])
We keep 1.32e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([4427, 2])
We keep 2.52e+05/3.55e+06 =  7% of the original kernel matrix.

torch.Size([14439, 2])
We keep 1.12e+06/4.23e+07 =  2% of the original kernel matrix.

torch.Size([4503, 2])
We keep 1.74e+05/3.33e+06 =  5% of the original kernel matrix.

torch.Size([14477, 2])
We keep 1.10e+06/4.10e+07 =  2% of the original kernel matrix.

torch.Size([3175, 2])
We keep 9.59e+04/1.56e+06 =  6% of the original kernel matrix.

torch.Size([12561, 2])
We keep 8.43e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([4994, 2])
We keep 2.80e+05/5.13e+06 =  5% of the original kernel matrix.

torch.Size([14936, 2])
We keep 1.27e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([9585, 2])
We keep 8.07e+05/1.93e+07 =  4% of the original kernel matrix.

torch.Size([19866, 2])
We keep 2.03e+06/9.86e+07 =  2% of the original kernel matrix.

torch.Size([12865, 2])
We keep 1.51e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([22852, 2])
We keep 2.67e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([6859, 2])
We keep 6.20e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([17051, 2])
We keep 1.67e+06/7.40e+07 =  2% of the original kernel matrix.

torch.Size([7755, 2])
We keep 4.33e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([18240, 2])
We keep 1.61e+06/7.30e+07 =  2% of the original kernel matrix.

torch.Size([3404, 2])
We keep 1.04e+05/1.78e+06 =  5% of the original kernel matrix.

torch.Size([12992, 2])
We keep 8.77e+05/3.00e+07 =  2% of the original kernel matrix.

torch.Size([15814, 2])
We keep 1.81e+06/6.52e+07 =  2% of the original kernel matrix.

torch.Size([25429, 2])
We keep 3.23e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([4636, 2])
We keep 1.97e+05/3.53e+06 =  5% of the original kernel matrix.

torch.Size([14646, 2])
We keep 1.13e+06/4.22e+07 =  2% of the original kernel matrix.

torch.Size([35617, 2])
We keep 7.26e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([39928, 2])
We keep 6.59e+06/4.48e+08 =  1% of the original kernel matrix.

torch.Size([5986, 2])
We keep 3.46e+05/6.89e+06 =  5% of the original kernel matrix.

torch.Size([16218, 2])
We keep 1.42e+06/5.90e+07 =  2% of the original kernel matrix.

torch.Size([5370, 2])
We keep 2.80e+05/5.19e+06 =  5% of the original kernel matrix.

torch.Size([15525, 2])
We keep 1.28e+06/5.12e+07 =  2% of the original kernel matrix.

torch.Size([6934, 2])
We keep 3.65e+05/8.28e+06 =  4% of the original kernel matrix.

torch.Size([17313, 2])
We keep 1.49e+06/6.46e+07 =  2% of the original kernel matrix.

torch.Size([6060, 2])
We keep 2.93e+05/6.18e+06 =  4% of the original kernel matrix.

torch.Size([16229, 2])
We keep 1.35e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([5825, 2])
We keep 2.58e+05/5.34e+06 =  4% of the original kernel matrix.

torch.Size([16043, 2])
We keep 1.30e+06/5.19e+07 =  2% of the original kernel matrix.

torch.Size([6011, 2])
We keep 2.43e+05/5.38e+06 =  4% of the original kernel matrix.

torch.Size([16416, 2])
We keep 1.29e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([8861, 2])
We keep 6.48e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([19306, 2])
We keep 1.88e+06/8.74e+07 =  2% of the original kernel matrix.

torch.Size([30879, 2])
We keep 5.91e+06/2.93e+08 =  2% of the original kernel matrix.

torch.Size([36387, 2])
We keep 5.76e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([9387, 2])
We keep 7.48e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([19838, 2])
We keep 1.88e+06/8.90e+07 =  2% of the original kernel matrix.

torch.Size([20772, 2])
We keep 2.79e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([29314, 2])
We keep 3.96e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([4583, 2])
We keep 2.04e+05/3.71e+06 =  5% of the original kernel matrix.

torch.Size([14567, 2])
We keep 1.14e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([9692, 2])
We keep 9.08e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([20056, 2])
We keep 2.09e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([4157, 2])
We keep 3.01e+05/4.63e+06 =  6% of the original kernel matrix.

torch.Size([13599, 2])
We keep 1.21e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([9191, 2])
We keep 7.01e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([19433, 2])
We keep 1.95e+06/9.36e+07 =  2% of the original kernel matrix.

torch.Size([5374, 2])
We keep 2.51e+05/5.14e+06 =  4% of the original kernel matrix.

torch.Size([15331, 2])
We keep 1.25e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([4233, 2])
We keep 1.77e+05/3.09e+06 =  5% of the original kernel matrix.

torch.Size([14127, 2])
We keep 1.07e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([6445, 2])
We keep 3.80e+05/7.95e+06 =  4% of the original kernel matrix.

torch.Size([16717, 2])
We keep 1.49e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([5935, 2])
We keep 2.70e+05/5.77e+06 =  4% of the original kernel matrix.

torch.Size([16293, 2])
We keep 1.33e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([6876, 2])
We keep 4.30e+05/9.04e+06 =  4% of the original kernel matrix.

torch.Size([17192, 2])
We keep 1.57e+06/6.75e+07 =  2% of the original kernel matrix.

torch.Size([8928, 2])
We keep 5.86e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([19239, 2])
We keep 1.88e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([2957, 2])
We keep 8.80e+04/1.38e+06 =  6% of the original kernel matrix.

torch.Size([12327, 2])
We keep 8.13e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([21509, 2])
We keep 3.55e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([29793, 2])
We keep 4.29e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([19908, 2])
We keep 3.52e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([28609, 2])
We keep 4.09e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([14716, 2])
We keep 1.55e+06/5.27e+07 =  2% of the original kernel matrix.

torch.Size([24657, 2])
We keep 2.99e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([5722, 2])
We keep 2.45e+05/5.22e+06 =  4% of the original kernel matrix.

torch.Size([16001, 2])
We keep 1.27e+06/5.13e+07 =  2% of the original kernel matrix.

torch.Size([6266, 2])
We keep 2.91e+05/6.28e+06 =  4% of the original kernel matrix.

torch.Size([16604, 2])
We keep 1.34e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([7604, 2])
We keep 6.82e+05/1.28e+07 =  5% of the original kernel matrix.

torch.Size([18055, 2])
We keep 1.77e+06/8.03e+07 =  2% of the original kernel matrix.

torch.Size([6055, 2])
We keep 2.74e+05/6.03e+06 =  4% of the original kernel matrix.

torch.Size([16410, 2])
We keep 1.33e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([6805, 2])
We keep 4.20e+05/8.64e+06 =  4% of the original kernel matrix.

torch.Size([17191, 2])
We keep 1.54e+06/6.60e+07 =  2% of the original kernel matrix.

torch.Size([5445, 2])
We keep 2.91e+05/5.46e+06 =  5% of the original kernel matrix.

torch.Size([15544, 2])
We keep 1.30e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([11923, 2])
We keep 1.36e+06/3.36e+07 =  4% of the original kernel matrix.

torch.Size([22192, 2])
We keep 2.50e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([9845, 2])
We keep 7.85e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([20166, 2])
We keep 2.05e+06/9.89e+07 =  2% of the original kernel matrix.

torch.Size([15474, 2])
We keep 3.80e+06/9.98e+07 =  3% of the original kernel matrix.

torch.Size([24859, 2])
We keep 3.80e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([9096, 2])
We keep 6.22e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([19352, 2])
We keep 1.93e+06/9.18e+07 =  2% of the original kernel matrix.

torch.Size([23543, 2])
We keep 4.42e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([31240, 2])
We keep 4.66e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([10878, 2])
We keep 6.93e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([21094, 2])
We keep 2.06e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([6754, 2])
We keep 4.02e+05/8.63e+06 =  4% of the original kernel matrix.

torch.Size([17121, 2])
We keep 1.52e+06/6.60e+07 =  2% of the original kernel matrix.

torch.Size([10429, 2])
We keep 8.51e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([20841, 2])
We keep 2.11e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([2039, 2])
We keep 3.93e+04/5.54e+05 =  7% of the original kernel matrix.

torch.Size([10845, 2])
We keep 5.96e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([21690, 2])
We keep 2.85e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([30371, 2])
We keep 4.25e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([5822, 2])
We keep 3.01e+05/5.88e+06 =  5% of the original kernel matrix.

torch.Size([16017, 2])
We keep 1.34e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([9823, 2])
We keep 7.88e+05/2.05e+07 =  3% of the original kernel matrix.

torch.Size([20145, 2])
We keep 2.09e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([4063, 2])
We keep 1.62e+05/2.84e+06 =  5% of the original kernel matrix.

torch.Size([13922, 2])
We keep 1.05e+06/3.78e+07 =  2% of the original kernel matrix.

torch.Size([8269, 2])
We keep 6.03e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([18601, 2])
We keep 1.81e+06/8.38e+07 =  2% of the original kernel matrix.

torch.Size([5154, 2])
We keep 1.83e+05/3.95e+06 =  4% of the original kernel matrix.

torch.Size([15350, 2])
We keep 1.16e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([7771, 2])
We keep 4.64e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([18350, 2])
We keep 1.68e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([9289, 2])
We keep 6.69e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([19479, 2])
We keep 1.97e+06/9.40e+07 =  2% of the original kernel matrix.

torch.Size([10673, 2])
We keep 7.71e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([20880, 2])
We keep 2.18e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([2356, 2])
We keep 5.21e+04/7.46e+05 =  6% of the original kernel matrix.

torch.Size([11190, 2])
We keep 6.53e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([3319, 2])
We keep 1.19e+05/1.90e+06 =  6% of the original kernel matrix.

torch.Size([12734, 2])
We keep 9.06e+05/3.10e+07 =  2% of the original kernel matrix.

torch.Size([3911, 2])
We keep 2.56e+05/3.22e+06 =  7% of the original kernel matrix.

torch.Size([13444, 2])
We keep 1.09e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([17729, 2])
We keep 1.82e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([26905, 2])
We keep 3.36e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([6523, 2])
We keep 3.98e+05/7.95e+06 =  5% of the original kernel matrix.

torch.Size([16936, 2])
We keep 1.48e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([13969, 2])
We keep 2.23e+06/5.42e+07 =  4% of the original kernel matrix.

torch.Size([23937, 2])
We keep 3.01e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([9477, 2])
We keep 7.01e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([19895, 2])
We keep 2.03e+06/9.83e+07 =  2% of the original kernel matrix.

torch.Size([14054, 2])
We keep 1.57e+06/4.69e+07 =  3% of the original kernel matrix.

torch.Size([23851, 2])
We keep 2.83e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([8501, 2])
We keep 5.58e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([18877, 2])
We keep 1.76e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([10641, 2])
We keep 1.19e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([20863, 2])
We keep 2.28e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([7905, 2])
We keep 4.48e+05/1.14e+07 =  3% of the original kernel matrix.

torch.Size([18279, 2])
We keep 1.68e+06/7.57e+07 =  2% of the original kernel matrix.

torch.Size([9331, 2])
We keep 5.94e+05/1.57e+07 =  3% of the original kernel matrix.

torch.Size([19751, 2])
We keep 1.88e+06/8.91e+07 =  2% of the original kernel matrix.

torch.Size([6821, 2])
We keep 4.01e+05/8.51e+06 =  4% of the original kernel matrix.

torch.Size([17163, 2])
We keep 1.53e+06/6.55e+07 =  2% of the original kernel matrix.

torch.Size([12602, 2])
We keep 2.15e+06/4.67e+07 =  4% of the original kernel matrix.

torch.Size([22633, 2])
We keep 2.87e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([6893, 2])
We keep 4.01e+05/8.79e+06 =  4% of the original kernel matrix.

torch.Size([17416, 2])
We keep 1.55e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([15802, 2])
We keep 1.72e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([25499, 2])
We keep 3.15e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([11316, 2])
We keep 8.59e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([21414, 2])
We keep 2.24e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([7240, 2])
We keep 3.93e+05/9.57e+06 =  4% of the original kernel matrix.

torch.Size([17565, 2])
We keep 1.57e+06/6.95e+07 =  2% of the original kernel matrix.

torch.Size([10301, 2])
We keep 8.70e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([20471, 2])
We keep 2.16e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([14402, 2])
We keep 1.38e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([24183, 2])
We keep 2.78e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([25062, 2])
We keep 5.84e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([32382, 2])
We keep 5.00e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([6650, 2])
We keep 3.81e+05/8.03e+06 =  4% of the original kernel matrix.

torch.Size([16945, 2])
We keep 1.49e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([2665, 2])
We keep 5.99e+04/8.87e+05 =  6% of the original kernel matrix.

torch.Size([11934, 2])
We keep 7.02e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([9802, 2])
We keep 7.31e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([20213, 2])
We keep 2.03e+06/9.83e+07 =  2% of the original kernel matrix.

torch.Size([7128, 2])
We keep 3.65e+05/8.83e+06 =  4% of the original kernel matrix.

torch.Size([17604, 2])
We keep 1.54e+06/6.68e+07 =  2% of the original kernel matrix.

torch.Size([8177, 2])
We keep 6.83e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([18512, 2])
We keep 1.80e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([7406, 2])
We keep 4.02e+05/9.82e+06 =  4% of the original kernel matrix.

torch.Size([17966, 2])
We keep 1.62e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([6728, 2])
We keep 3.52e+05/8.12e+06 =  4% of the original kernel matrix.

torch.Size([17134, 2])
We keep 1.50e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([4503, 2])
We keep 2.38e+05/3.91e+06 =  6% of the original kernel matrix.

torch.Size([14539, 2])
We keep 1.17e+06/4.44e+07 =  2% of the original kernel matrix.

torch.Size([6654, 2])
We keep 3.35e+05/7.34e+06 =  4% of the original kernel matrix.

torch.Size([16948, 2])
We keep 1.44e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([5680, 2])
We keep 3.29e+05/5.57e+06 =  5% of the original kernel matrix.

torch.Size([16118, 2])
We keep 1.33e+06/5.30e+07 =  2% of the original kernel matrix.

torch.Size([12515, 2])
We keep 9.89e+05/3.10e+07 =  3% of the original kernel matrix.

torch.Size([22510, 2])
We keep 2.40e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([6045, 2])
We keep 2.44e+05/5.64e+06 =  4% of the original kernel matrix.

torch.Size([16452, 2])
We keep 1.31e+06/5.33e+07 =  2% of the original kernel matrix.

torch.Size([24580, 2])
We keep 3.53e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([32344, 2])
We keep 4.75e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([13718, 2])
We keep 1.70e+06/4.48e+07 =  3% of the original kernel matrix.

torch.Size([23651, 2])
We keep 2.81e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([10247, 2])
We keep 6.69e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([20560, 2])
We keep 2.04e+06/9.87e+07 =  2% of the original kernel matrix.

torch.Size([4939, 2])
We keep 1.70e+05/3.42e+06 =  4% of the original kernel matrix.

torch.Size([15129, 2])
We keep 1.09e+06/4.15e+07 =  2% of the original kernel matrix.

torch.Size([12288, 2])
We keep 1.65e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([22514, 2])
We keep 2.77e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([11219, 2])
We keep 1.12e+06/2.70e+07 =  4% of the original kernel matrix.

torch.Size([21596, 2])
We keep 2.31e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([7394, 2])
We keep 5.24e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([17866, 2])
We keep 1.65e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([22911, 2])
We keep 3.14e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([31052, 2])
We keep 4.31e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([12081, 2])
We keep 9.59e+05/3.02e+07 =  3% of the original kernel matrix.

torch.Size([22065, 2])
We keep 2.40e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([8536, 2])
We keep 7.97e+05/1.55e+07 =  5% of the original kernel matrix.

torch.Size([18974, 2])
We keep 1.90e+06/8.85e+07 =  2% of the original kernel matrix.

torch.Size([8018, 2])
We keep 4.53e+05/1.16e+07 =  3% of the original kernel matrix.

torch.Size([18364, 2])
We keep 1.69e+06/7.64e+07 =  2% of the original kernel matrix.

torch.Size([8945, 2])
We keep 5.58e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([19283, 2])
We keep 1.86e+06/8.66e+07 =  2% of the original kernel matrix.

torch.Size([3263, 2])
We keep 1.28e+05/1.92e+06 =  6% of the original kernel matrix.

torch.Size([12771, 2])
We keep 9.13e+05/3.11e+07 =  2% of the original kernel matrix.

torch.Size([4338, 2])
We keep 1.74e+05/3.22e+06 =  5% of the original kernel matrix.

torch.Size([14175, 2])
We keep 1.09e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([3721, 2])
We keep 1.21e+05/2.07e+06 =  5% of the original kernel matrix.

torch.Size([13611, 2])
We keep 9.35e+05/3.23e+07 =  2% of the original kernel matrix.

torch.Size([2984, 2])
We keep 8.10e+04/1.20e+06 =  6% of the original kernel matrix.

torch.Size([12532, 2])
We keep 7.48e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([10182, 2])
We keep 7.67e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([20555, 2])
We keep 2.10e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([8419, 2])
We keep 4.63e+05/1.22e+07 =  3% of the original kernel matrix.

torch.Size([18940, 2])
We keep 1.71e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([8797, 2])
We keep 8.03e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([19073, 2])
We keep 2.00e+06/9.53e+07 =  2% of the original kernel matrix.

torch.Size([4550, 2])
We keep 2.44e+05/3.93e+06 =  6% of the original kernel matrix.

torch.Size([14445, 2])
We keep 1.16e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([11481, 2])
We keep 9.55e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([21631, 2])
We keep 2.36e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([8485, 2])
We keep 8.76e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([18750, 2])
We keep 2.00e+06/9.55e+07 =  2% of the original kernel matrix.

torch.Size([9007, 2])
We keep 7.30e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([19330, 2])
We keep 1.96e+06/9.27e+07 =  2% of the original kernel matrix.

torch.Size([10219, 2])
We keep 7.00e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([20503, 2])
We keep 2.03e+06/9.93e+07 =  2% of the original kernel matrix.

torch.Size([5924, 2])
We keep 2.91e+05/6.26e+06 =  4% of the original kernel matrix.

torch.Size([16116, 2])
We keep 1.34e+06/5.62e+07 =  2% of the original kernel matrix.

torch.Size([3257, 2])
We keep 9.96e+04/1.51e+06 =  6% of the original kernel matrix.

torch.Size([12680, 2])
We keep 8.36e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([12934, 2])
We keep 1.17e+06/3.77e+07 =  3% of the original kernel matrix.

torch.Size([22764, 2])
We keep 2.59e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([2432, 2])
We keep 5.46e+04/8.23e+05 =  6% of the original kernel matrix.

torch.Size([11499, 2])
We keep 6.84e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([5058, 2])
We keep 2.14e+05/4.31e+06 =  4% of the original kernel matrix.

torch.Size([15208, 2])
We keep 1.20e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([6455, 2])
We keep 3.86e+05/8.03e+06 =  4% of the original kernel matrix.

torch.Size([16635, 2])
We keep 1.48e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([8215, 2])
We keep 5.42e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([18619, 2])
We keep 1.80e+06/8.17e+07 =  2% of the original kernel matrix.

torch.Size([10979, 2])
We keep 9.24e+05/2.63e+07 =  3% of the original kernel matrix.

torch.Size([21243, 2])
We keep 2.32e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([6247, 2])
We keep 3.79e+05/8.11e+06 =  4% of the original kernel matrix.

torch.Size([16361, 2])
We keep 1.51e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([7162, 2])
We keep 5.20e+05/9.46e+06 =  5% of the original kernel matrix.

torch.Size([17508, 2])
We keep 1.59e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([9312, 2])
We keep 5.96e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([19705, 2])
We keep 1.90e+06/8.97e+07 =  2% of the original kernel matrix.

torch.Size([15021, 2])
We keep 1.57e+06/5.04e+07 =  3% of the original kernel matrix.

torch.Size([24895, 2])
We keep 2.90e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([5788, 2])
We keep 3.12e+05/5.56e+06 =  5% of the original kernel matrix.

torch.Size([16000, 2])
We keep 1.32e+06/5.30e+07 =  2% of the original kernel matrix.

torch.Size([20106, 2])
We keep 2.71e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([28752, 2])
We keep 3.90e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([49806, 2])
We keep 6.76e+07/1.76e+09 =  3% of the original kernel matrix.

torch.Size([43889, 2])
We keep 1.19e+07/9.43e+08 =  1% of the original kernel matrix.

torch.Size([28447, 2])
We keep 1.62e+07/3.62e+08 =  4% of the original kernel matrix.

torch.Size([34088, 2])
We keep 6.00e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([14349, 2])
We keep 1.50e+06/4.77e+07 =  3% of the original kernel matrix.

torch.Size([24193, 2])
We keep 2.79e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([3222, 2])
We keep 1.51e+05/1.67e+06 =  9% of the original kernel matrix.

torch.Size([12590, 2])
We keep 8.80e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([15746, 2])
We keep 1.64e+06/5.53e+07 =  2% of the original kernel matrix.

torch.Size([25246, 2])
We keep 2.98e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([177920, 2])
We keep 1.38e+09/2.57e+10 =  5% of the original kernel matrix.

torch.Size([85189, 2])
We keep 3.87e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([466923, 2])
We keep 3.65e+09/1.46e+11 =  2% of the original kernel matrix.

torch.Size([137797, 2])
We keep 8.28e+07/8.60e+09 =  0% of the original kernel matrix.

torch.Size([10919, 2])
We keep 9.52e+05/2.42e+07 =  3% of the original kernel matrix.

torch.Size([21213, 2])
We keep 2.18e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([167586, 2])
We keep 2.13e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([83418, 2])
We keep 2.67e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([65144, 2])
We keep 6.14e+07/1.83e+09 =  3% of the original kernel matrix.

torch.Size([50889, 2])
We keep 1.18e+07/9.60e+08 =  1% of the original kernel matrix.

torch.Size([82745, 2])
We keep 8.91e+07/3.55e+09 =  2% of the original kernel matrix.

torch.Size([56439, 2])
We keep 1.60e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([589904, 2])
We keep 5.63e+09/1.83e+11 =  3% of the original kernel matrix.

torch.Size([162539, 2])
We keep 9.00e+07/9.61e+09 =  0% of the original kernel matrix.

torch.Size([6888, 2])
We keep 4.29e+05/8.35e+06 =  5% of the original kernel matrix.

torch.Size([17419, 2])
We keep 1.50e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([11390, 2])
We keep 1.16e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([21518, 2])
We keep 2.42e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([427146, 2])
We keep 6.55e+08/5.98e+10 =  1% of the original kernel matrix.

torch.Size([134698, 2])
We keep 5.47e+07/5.49e+09 =  0% of the original kernel matrix.

torch.Size([1131432, 2])
We keep 2.50e+09/3.40e+11 =  0% of the original kernel matrix.

torch.Size([228247, 2])
We keep 1.19e+08/1.31e+10 =  0% of the original kernel matrix.

torch.Size([12796, 2])
We keep 1.62e+06/4.32e+07 =  3% of the original kernel matrix.

torch.Size([22793, 2])
We keep 2.77e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([64208, 2])
We keep 2.04e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([51340, 2])
We keep 1.05e+07/8.15e+08 =  1% of the original kernel matrix.

torch.Size([146501, 2])
We keep 3.43e+09/1.80e+10 = 19% of the original kernel matrix.

torch.Size([76219, 2])
We keep 2.61e+07/3.02e+09 =  0% of the original kernel matrix.

torch.Size([112607, 2])
We keep 1.72e+08/5.85e+09 =  2% of the original kernel matrix.

torch.Size([67149, 2])
We keep 1.92e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([98325, 2])
We keep 4.15e+08/5.31e+09 =  7% of the original kernel matrix.

torch.Size([62236, 2])
We keep 1.87e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([59272, 2])
We keep 4.13e+07/1.64e+09 =  2% of the original kernel matrix.

torch.Size([48042, 2])
We keep 1.15e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([52657, 2])
We keep 3.54e+07/1.24e+09 =  2% of the original kernel matrix.

torch.Size([46263, 2])
We keep 1.02e+07/7.92e+08 =  1% of the original kernel matrix.

torch.Size([24029, 2])
We keep 3.36e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([31763, 2])
We keep 4.41e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([10846, 2])
We keep 1.32e+06/2.42e+07 =  5% of the original kernel matrix.

torch.Size([21117, 2])
We keep 2.15e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([399242, 2])
We keep 5.41e+09/2.33e+11 =  2% of the original kernel matrix.

torch.Size([116785, 2])
We keep 1.02e+08/1.09e+10 =  0% of the original kernel matrix.

torch.Size([16014, 2])
We keep 2.42e+06/7.00e+07 =  3% of the original kernel matrix.

torch.Size([25524, 2])
We keep 3.29e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([10234, 2])
We keep 1.40e+06/2.75e+07 =  5% of the original kernel matrix.

torch.Size([20611, 2])
We keep 2.31e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([95563, 2])
We keep 7.32e+07/3.16e+09 =  2% of the original kernel matrix.

torch.Size([61506, 2])
We keep 1.47e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([70866, 2])
We keep 4.91e+07/2.30e+09 =  2% of the original kernel matrix.

torch.Size([52313, 2])
We keep 1.33e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([493346, 2])
We keep 9.52e+08/8.63e+10 =  1% of the original kernel matrix.

torch.Size([146231, 2])
We keep 6.43e+07/6.60e+09 =  0% of the original kernel matrix.

torch.Size([214221, 2])
We keep 1.46e+08/1.50e+10 =  0% of the original kernel matrix.

torch.Size([95394, 2])
We keep 2.93e+07/2.75e+09 =  1% of the original kernel matrix.

torch.Size([4718, 2])
We keep 1.89e+05/3.66e+06 =  5% of the original kernel matrix.

torch.Size([14793, 2])
We keep 1.12e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([94327, 2])
We keep 1.42e+08/3.17e+09 =  4% of the original kernel matrix.

torch.Size([61454, 2])
We keep 1.54e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([16542, 2])
We keep 3.18e+06/8.27e+07 =  3% of the original kernel matrix.

torch.Size([25936, 2])
We keep 3.54e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([65024, 2])
We keep 7.70e+07/2.08e+09 =  3% of the original kernel matrix.

torch.Size([50874, 2])
We keep 1.29e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([135921, 2])
We keep 7.57e+07/6.14e+09 =  1% of the original kernel matrix.

torch.Size([74853, 2])
We keep 1.97e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([12196, 2])
We keep 9.67e+05/3.09e+07 =  3% of the original kernel matrix.

torch.Size([22140, 2])
We keep 2.40e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([103266, 2])
We keep 5.65e+07/3.73e+09 =  1% of the original kernel matrix.

torch.Size([64248, 2])
We keep 1.62e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([3218, 2])
We keep 9.35e+04/1.57e+06 =  5% of the original kernel matrix.

torch.Size([12671, 2])
We keep 8.49e+05/2.81e+07 =  3% of the original kernel matrix.

torch.Size([15247, 2])
We keep 1.53e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([24991, 2])
We keep 2.94e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([120663, 2])
We keep 2.14e+08/9.17e+09 =  2% of the original kernel matrix.

torch.Size([68807, 2])
We keep 2.42e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([89268, 2])
We keep 8.34e+07/3.41e+09 =  2% of the original kernel matrix.

torch.Size([59429, 2])
We keep 1.55e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([56659, 2])
We keep 3.43e+07/1.50e+09 =  2% of the original kernel matrix.

torch.Size([47072, 2])
We keep 1.11e+07/8.69e+08 =  1% of the original kernel matrix.

torch.Size([41565, 2])
We keep 2.24e+08/1.81e+09 = 12% of the original kernel matrix.

torch.Size([38969, 2])
We keep 1.22e+07/9.56e+08 =  1% of the original kernel matrix.

torch.Size([459813, 2])
We keep 5.93e+08/8.00e+10 =  0% of the original kernel matrix.

torch.Size([141802, 2])
We keep 6.13e+07/6.35e+09 =  0% of the original kernel matrix.

torch.Size([28480, 2])
We keep 4.03e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([34909, 2])
We keep 5.18e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([43372, 2])
We keep 9.55e+06/6.45e+08 =  1% of the original kernel matrix.

torch.Size([42547, 2])
We keep 7.73e+06/5.70e+08 =  1% of the original kernel matrix.

torch.Size([127622, 2])
We keep 9.38e+07/6.50e+09 =  1% of the original kernel matrix.

torch.Size([72053, 2])
We keep 2.05e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([12859, 2])
We keep 2.27e+06/4.33e+07 =  5% of the original kernel matrix.

torch.Size([23005, 2])
We keep 2.77e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([264028, 2])
We keep 4.21e+08/2.49e+10 =  1% of the original kernel matrix.

torch.Size([106652, 2])
We keep 3.72e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([364778, 2])
We keep 1.19e+09/7.26e+10 =  1% of the original kernel matrix.

torch.Size([123794, 2])
We keep 6.00e+07/6.05e+09 =  0% of the original kernel matrix.

torch.Size([562642, 2])
We keep 1.68e+09/1.27e+11 =  1% of the original kernel matrix.

torch.Size([157387, 2])
We keep 7.74e+07/8.00e+09 =  0% of the original kernel matrix.

torch.Size([53088, 2])
We keep 2.05e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([46504, 2])
We keep 9.33e+06/7.18e+08 =  1% of the original kernel matrix.

torch.Size([57948, 2])
We keep 2.54e+07/1.21e+09 =  2% of the original kernel matrix.

torch.Size([48376, 2])
We keep 9.98e+06/7.80e+08 =  1% of the original kernel matrix.

torch.Size([26374, 2])
We keep 6.16e+06/2.64e+08 =  2% of the original kernel matrix.

torch.Size([32660, 2])
We keep 5.45e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([13884, 2])
We keep 1.16e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([23892, 2])
We keep 2.69e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([46116, 2])
We keep 5.77e+07/1.02e+09 =  5% of the original kernel matrix.

torch.Size([42908, 2])
We keep 9.47e+06/7.18e+08 =  1% of the original kernel matrix.

torch.Size([22713, 2])
We keep 3.50e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([30911, 2])
We keep 4.39e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([41061, 2])
We keep 1.46e+07/6.07e+08 =  2% of the original kernel matrix.

torch.Size([41015, 2])
We keep 7.51e+06/5.53e+08 =  1% of the original kernel matrix.

torch.Size([115328, 2])
We keep 2.83e+08/6.79e+09 =  4% of the original kernel matrix.

torch.Size([69157, 2])
We keep 2.00e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([75818, 2])
We keep 7.96e+07/2.38e+09 =  3% of the original kernel matrix.

torch.Size([55420, 2])
We keep 1.36e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([16474, 2])
We keep 4.05e+06/9.95e+07 =  4% of the original kernel matrix.

torch.Size([25689, 2])
We keep 3.79e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([19906, 2])
We keep 2.51e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([28858, 2])
We keep 3.80e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([48843, 2])
We keep 1.66e+08/4.85e+09 =  3% of the original kernel matrix.

torch.Size([41877, 2])
We keep 1.81e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([25234, 2])
We keep 6.82e+06/2.37e+08 =  2% of the original kernel matrix.

torch.Size([32196, 2])
We keep 5.11e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([16429, 2])
We keep 1.21e+07/9.76e+07 = 12% of the original kernel matrix.

torch.Size([25861, 2])
We keep 3.72e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([24632, 2])
We keep 4.48e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([32415, 2])
We keep 4.80e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([645781, 2])
We keep 4.34e+09/2.34e+11 =  1% of the original kernel matrix.

torch.Size([167657, 2])
We keep 1.02e+08/1.09e+10 =  0% of the original kernel matrix.

torch.Size([29367, 2])
We keep 6.06e+06/2.51e+08 =  2% of the original kernel matrix.

torch.Size([35426, 2])
We keep 5.40e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([114980, 2])
We keep 1.07e+09/1.19e+10 =  8% of the original kernel matrix.

torch.Size([65154, 2])
We keep 2.52e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([16009, 2])
We keep 6.70e+06/7.85e+07 =  8% of the original kernel matrix.

torch.Size([25529, 2])
We keep 3.41e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([55362, 2])
We keep 9.93e+07/3.03e+09 =  3% of the original kernel matrix.

torch.Size([43947, 2])
We keep 1.48e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([36115, 2])
We keep 1.99e+07/4.90e+08 =  4% of the original kernel matrix.

torch.Size([39196, 2])
We keep 7.02e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([329913, 2])
We keep 2.13e+09/1.03e+11 =  2% of the original kernel matrix.

torch.Size([106918, 2])
We keep 7.10e+07/7.21e+09 =  0% of the original kernel matrix.

torch.Size([70464, 2])
We keep 2.43e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([53465, 2])
We keep 1.23e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([25813, 2])
We keep 3.65e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([33017, 2])
We keep 4.72e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([259566, 2])
We keep 1.97e+08/2.25e+10 =  0% of the original kernel matrix.

torch.Size([106154, 2])
We keep 3.52e+07/3.37e+09 =  1% of the original kernel matrix.

torch.Size([196365, 2])
We keep 1.13e+08/1.26e+10 =  0% of the original kernel matrix.

torch.Size([91059, 2])
We keep 2.70e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([15644, 2])
We keep 7.85e+06/8.92e+07 =  8% of the original kernel matrix.

torch.Size([25221, 2])
We keep 3.50e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([151303, 2])
We keep 1.13e+08/7.48e+09 =  1% of the original kernel matrix.

torch.Size([79195, 2])
We keep 2.16e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([208258, 2])
We keep 1.61e+08/1.74e+10 =  0% of the original kernel matrix.

torch.Size([94062, 2])
We keep 3.15e+07/2.96e+09 =  1% of the original kernel matrix.

torch.Size([666481, 2])
We keep 9.46e+08/1.38e+11 =  0% of the original kernel matrix.

torch.Size([171932, 2])
We keep 7.91e+07/8.35e+09 =  0% of the original kernel matrix.

torch.Size([52100, 2])
We keep 4.42e+07/1.20e+09 =  3% of the original kernel matrix.

torch.Size([46070, 2])
We keep 1.02e+07/7.78e+08 =  1% of the original kernel matrix.

torch.Size([6702, 2])
We keep 6.37e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([16857, 2])
We keep 1.61e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([31483, 2])
We keep 5.55e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([36746, 2])
We keep 5.84e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([168551, 2])
We keep 1.34e+09/3.19e+10 =  4% of the original kernel matrix.

torch.Size([80970, 2])
We keep 4.12e+07/4.01e+09 =  1% of the original kernel matrix.

torch.Size([189265, 2])
We keep 1.36e+09/1.92e+10 =  7% of the original kernel matrix.

torch.Size([88755, 2])
We keep 3.32e+07/3.11e+09 =  1% of the original kernel matrix.

torch.Size([33763, 2])
We keep 6.38e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([38130, 2])
We keep 6.33e+06/4.37e+08 =  1% of the original kernel matrix.

torch.Size([6015, 2])
We keep 2.93e+05/6.47e+06 =  4% of the original kernel matrix.

torch.Size([16247, 2])
We keep 1.37e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([98067, 2])
We keep 4.25e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([62702, 2])
We keep 1.45e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([220157, 2])
We keep 1.58e+08/1.64e+10 =  0% of the original kernel matrix.

torch.Size([96870, 2])
We keep 3.06e+07/2.87e+09 =  1% of the original kernel matrix.

torch.Size([114955, 2])
We keep 5.21e+08/1.25e+10 =  4% of the original kernel matrix.

torch.Size([64299, 2])
We keep 2.72e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([13022, 2])
We keep 1.44e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([23123, 2])
We keep 2.67e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([60053, 2])
We keep 3.28e+07/1.39e+09 =  2% of the original kernel matrix.

torch.Size([49176, 2])
We keep 1.08e+07/8.37e+08 =  1% of the original kernel matrix.

torch.Size([31546, 2])
We keep 6.85e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([36761, 2])
We keep 6.14e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([17389, 2])
We keep 2.02e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([26694, 2])
We keep 3.38e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([26100, 2])
We keep 3.78e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([33275, 2])
We keep 4.96e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([18479, 2])
We keep 3.82e+06/9.09e+07 =  4% of the original kernel matrix.

torch.Size([27635, 2])
We keep 3.46e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([64623, 2])
We keep 4.54e+07/1.83e+09 =  2% of the original kernel matrix.

torch.Size([50406, 2])
We keep 1.18e+07/9.62e+08 =  1% of the original kernel matrix.

torch.Size([93244, 2])
We keep 3.04e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([60993, 2])
We keep 1.38e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([38057, 2])
We keep 1.15e+07/4.79e+08 =  2% of the original kernel matrix.

torch.Size([40930, 2])
We keep 7.07e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([26948, 2])
We keep 1.10e+07/2.62e+08 =  4% of the original kernel matrix.

torch.Size([33649, 2])
We keep 5.53e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([19171, 2])
We keep 4.28e+06/1.06e+08 =  4% of the original kernel matrix.

torch.Size([28158, 2])
We keep 3.82e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([849329, 2])
We keep 2.40e+09/2.62e+11 =  0% of the original kernel matrix.

torch.Size([195874, 2])
We keep 1.05e+08/1.15e+10 =  0% of the original kernel matrix.

torch.Size([25382, 2])
We keep 3.77e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([32745, 2])
We keep 4.74e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([275026, 2])
We keep 2.54e+08/2.63e+10 =  0% of the original kernel matrix.

torch.Size([109152, 2])
We keep 3.75e+07/3.64e+09 =  1% of the original kernel matrix.

torch.Size([4932, 2])
We keep 1.93e+05/4.04e+06 =  4% of the original kernel matrix.

torch.Size([15019, 2])
We keep 1.15e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([13903, 2])
We keep 1.97e+06/4.45e+07 =  4% of the original kernel matrix.

torch.Size([23791, 2])
We keep 2.79e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([10633, 2])
We keep 7.73e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([20838, 2])
We keep 2.12e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([49021, 2])
We keep 2.39e+07/7.67e+08 =  3% of the original kernel matrix.

torch.Size([45339, 2])
We keep 8.40e+06/6.22e+08 =  1% of the original kernel matrix.

torch.Size([12677, 2])
We keep 1.17e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([22889, 2])
We keep 2.61e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([113697, 2])
We keep 4.65e+08/1.07e+10 =  4% of the original kernel matrix.

torch.Size([65363, 2])
We keep 2.58e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([16235, 2])
We keep 1.87e+06/6.76e+07 =  2% of the original kernel matrix.

torch.Size([25632, 2])
We keep 3.24e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([13079, 2])
We keep 1.31e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([22986, 2])
We keep 2.62e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([471690, 2])
We keep 1.02e+09/8.44e+10 =  1% of the original kernel matrix.

torch.Size([142312, 2])
We keep 6.35e+07/6.53e+09 =  0% of the original kernel matrix.

torch.Size([53618, 2])
We keep 2.06e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([46887, 2])
We keep 9.62e+06/7.43e+08 =  1% of the original kernel matrix.

torch.Size([37017, 2])
We keep 1.43e+07/4.83e+08 =  2% of the original kernel matrix.

torch.Size([38817, 2])
We keep 6.79e+06/4.94e+08 =  1% of the original kernel matrix.

torch.Size([193130, 2])
We keep 3.09e+08/1.48e+10 =  2% of the original kernel matrix.

torch.Size([91192, 2])
We keep 2.99e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([197766, 2])
We keep 6.06e+08/3.69e+10 =  1% of the original kernel matrix.

torch.Size([87047, 2])
We keep 4.47e+07/4.31e+09 =  1% of the original kernel matrix.

torch.Size([10847, 2])
We keep 1.05e+06/2.66e+07 =  3% of the original kernel matrix.

torch.Size([21039, 2])
We keep 2.32e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([16412, 2])
We keep 1.67e+06/6.19e+07 =  2% of the original kernel matrix.

torch.Size([25816, 2])
We keep 3.10e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([35376, 2])
We keep 3.03e+07/4.76e+08 =  6% of the original kernel matrix.

torch.Size([38634, 2])
We keep 7.27e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([3897, 2])
We keep 1.52e+05/2.55e+06 =  5% of the original kernel matrix.

torch.Size([13598, 2])
We keep 9.87e+05/3.58e+07 =  2% of the original kernel matrix.

torch.Size([16501, 2])
We keep 1.89e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([25872, 2])
We keep 3.28e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([28279, 2])
We keep 8.01e+06/3.09e+08 =  2% of the original kernel matrix.

torch.Size([34201, 2])
We keep 5.83e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([6287, 2])
We keep 3.47e+05/7.11e+06 =  4% of the original kernel matrix.

torch.Size([16656, 2])
We keep 1.42e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([14722, 2])
We keep 1.40e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([24528, 2])
We keep 2.90e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([31310, 2])
We keep 5.45e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([36467, 2])
We keep 5.80e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([13575, 2])
We keep 1.19e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([23491, 2])
We keep 2.63e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([630464, 2])
We keep 1.61e+09/1.49e+11 =  1% of the original kernel matrix.

torch.Size([165966, 2])
We keep 8.20e+07/8.67e+09 =  0% of the original kernel matrix.

torch.Size([15598, 2])
We keep 3.06e+06/8.09e+07 =  3% of the original kernel matrix.

torch.Size([25013, 2])
We keep 3.50e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([141044, 2])
We keep 1.70e+08/8.36e+09 =  2% of the original kernel matrix.

torch.Size([75933, 2])
We keep 2.33e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([18720, 2])
We keep 6.65e+06/1.51e+08 =  4% of the original kernel matrix.

torch.Size([27654, 2])
We keep 4.47e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([38503, 2])
We keep 3.08e+07/7.77e+08 =  3% of the original kernel matrix.

torch.Size([38542, 2])
We keep 8.49e+06/6.26e+08 =  1% of the original kernel matrix.

torch.Size([90844, 2])
We keep 1.24e+08/5.10e+09 =  2% of the original kernel matrix.

torch.Size([59400, 2])
We keep 1.88e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([1133127, 2])
We keep 7.82e+09/4.66e+11 =  1% of the original kernel matrix.

torch.Size([223060, 2])
We keep 1.35e+08/1.53e+10 =  0% of the original kernel matrix.

torch.Size([1491702, 2])
We keep 3.37e+09/5.60e+11 =  0% of the original kernel matrix.

torch.Size([261788, 2])
We keep 1.48e+08/1.68e+10 =  0% of the original kernel matrix.

torch.Size([75770, 2])
We keep 5.76e+07/1.95e+09 =  2% of the original kernel matrix.

torch.Size([55479, 2])
We keep 1.24e+07/9.93e+08 =  1% of the original kernel matrix.

torch.Size([201087, 2])
We keep 5.85e+08/1.65e+10 =  3% of the original kernel matrix.

torch.Size([92460, 2])
We keep 3.15e+07/2.89e+09 =  1% of the original kernel matrix.

torch.Size([8101, 2])
We keep 4.40e+05/1.10e+07 =  3% of the original kernel matrix.

torch.Size([18503, 2])
We keep 1.65e+06/7.45e+07 =  2% of the original kernel matrix.

torch.Size([42891, 2])
We keep 1.50e+08/1.25e+09 = 12% of the original kernel matrix.

torch.Size([40498, 2])
We keep 1.04e+07/7.93e+08 =  1% of the original kernel matrix.

torch.Size([48233, 2])
We keep 6.49e+07/1.26e+09 =  5% of the original kernel matrix.

torch.Size([43668, 2])
We keep 1.02e+07/7.97e+08 =  1% of the original kernel matrix.

torch.Size([113450, 2])
We keep 2.69e+08/5.73e+09 =  4% of the original kernel matrix.

torch.Size([68105, 2])
We keep 1.87e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([14658, 2])
We keep 1.26e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([24417, 2])
We keep 2.75e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([136024, 2])
We keep 8.91e+07/6.89e+09 =  1% of the original kernel matrix.

torch.Size([74721, 2])
We keep 2.09e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([710685, 2])
We keep 2.73e+09/1.89e+11 =  1% of the original kernel matrix.

torch.Size([178317, 2])
We keep 9.28e+07/9.76e+09 =  0% of the original kernel matrix.

torch.Size([101758, 2])
We keep 8.31e+07/4.47e+09 =  1% of the original kernel matrix.

torch.Size([62766, 2])
We keep 1.74e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([17000, 2])
We keep 2.79e+06/9.62e+07 =  2% of the original kernel matrix.

torch.Size([25884, 2])
We keep 3.64e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([65028, 2])
We keep 5.78e+07/1.72e+09 =  3% of the original kernel matrix.

torch.Size([52277, 2])
We keep 1.21e+07/9.32e+08 =  1% of the original kernel matrix.

torch.Size([20915, 2])
We keep 2.69e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([29307, 2])
We keep 4.03e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([10707, 2])
We keep 8.15e+05/2.36e+07 =  3% of the original kernel matrix.

torch.Size([20922, 2])
We keep 2.19e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([221659, 2])
We keep 2.58e+08/1.92e+10 =  1% of the original kernel matrix.

torch.Size([97482, 2])
We keep 3.31e+07/3.11e+09 =  1% of the original kernel matrix.

torch.Size([48976, 2])
We keep 2.44e+07/9.81e+08 =  2% of the original kernel matrix.

torch.Size([43965, 2])
We keep 9.19e+06/7.04e+08 =  1% of the original kernel matrix.

torch.Size([6456, 2])
We keep 3.25e+05/7.32e+06 =  4% of the original kernel matrix.

torch.Size([16727, 2])
We keep 1.42e+06/6.08e+07 =  2% of the original kernel matrix.

torch.Size([224029, 2])
We keep 2.36e+08/1.78e+10 =  1% of the original kernel matrix.

torch.Size([97753, 2])
We keep 3.15e+07/2.99e+09 =  1% of the original kernel matrix.

torch.Size([23003, 2])
We keep 3.18e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([31142, 2])
We keep 4.41e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([3157, 2])
We keep 1.06e+05/1.51e+06 =  7% of the original kernel matrix.

torch.Size([12485, 2])
We keep 8.32e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([200495, 2])
We keep 3.65e+08/2.01e+10 =  1% of the original kernel matrix.

torch.Size([91109, 2])
We keep 3.36e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([159363, 2])
We keep 1.05e+08/8.67e+09 =  1% of the original kernel matrix.

torch.Size([81438, 2])
We keep 2.31e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([97278, 2])
We keep 9.04e+07/3.46e+09 =  2% of the original kernel matrix.

torch.Size([62368, 2])
We keep 1.57e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([223550, 2])
We keep 3.43e+08/1.89e+10 =  1% of the original kernel matrix.

torch.Size([97859, 2])
We keep 3.27e+07/3.09e+09 =  1% of the original kernel matrix.

torch.Size([190625, 2])
We keep 2.36e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([88961, 2])
We keep 2.96e+07/2.75e+09 =  1% of the original kernel matrix.

torch.Size([18296, 2])
We keep 2.81e+06/9.80e+07 =  2% of the original kernel matrix.

torch.Size([27211, 2])
We keep 3.71e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([88122, 2])
We keep 5.31e+07/2.90e+09 =  1% of the original kernel matrix.

torch.Size([59094, 2])
We keep 1.44e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([305992, 2])
We keep 2.31e+08/3.06e+10 =  0% of the original kernel matrix.

torch.Size([116255, 2])
We keep 3.99e+07/3.93e+09 =  1% of the original kernel matrix.

torch.Size([260535, 2])
We keep 2.09e+08/2.32e+10 =  0% of the original kernel matrix.

torch.Size([106594, 2])
We keep 3.52e+07/3.42e+09 =  1% of the original kernel matrix.

torch.Size([31167, 2])
We keep 6.72e+06/3.02e+08 =  2% of the original kernel matrix.

torch.Size([36298, 2])
We keep 5.79e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([167763, 2])
We keep 6.57e+08/1.54e+10 =  4% of the original kernel matrix.

torch.Size([84481, 2])
We keep 2.89e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([304010, 2])
We keep 8.19e+08/3.91e+10 =  2% of the original kernel matrix.

torch.Size([114952, 2])
We keep 4.45e+07/4.44e+09 =  1% of the original kernel matrix.

torch.Size([110452, 2])
We keep 6.17e+07/4.01e+09 =  1% of the original kernel matrix.

torch.Size([66908, 2])
We keep 1.66e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([33169, 2])
We keep 1.56e+07/4.51e+08 =  3% of the original kernel matrix.

torch.Size([36102, 2])
We keep 6.37e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([26363, 2])
We keep 4.72e+07/4.90e+08 =  9% of the original kernel matrix.

torch.Size([31911, 2])
We keep 7.04e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([22958, 2])
We keep 4.52e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([30561, 2])
We keep 4.65e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([21573, 2])
We keep 2.56e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([29950, 2])
We keep 4.02e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([11705, 2])
We keep 1.37e+06/3.33e+07 =  4% of the original kernel matrix.

torch.Size([21919, 2])
We keep 2.49e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([296807, 2])
We keep 2.71e+08/3.24e+10 =  0% of the original kernel matrix.

torch.Size([114797, 2])
We keep 4.13e+07/4.04e+09 =  1% of the original kernel matrix.

torch.Size([247005, 2])
We keep 1.85e+08/2.08e+10 =  0% of the original kernel matrix.

torch.Size([103088, 2])
We keep 3.36e+07/3.24e+09 =  1% of the original kernel matrix.

torch.Size([580186, 2])
We keep 1.41e+09/1.27e+11 =  1% of the original kernel matrix.

torch.Size([158600, 2])
We keep 7.65e+07/8.00e+09 =  0% of the original kernel matrix.

torch.Size([297260, 2])
We keep 9.97e+08/4.73e+10 =  2% of the original kernel matrix.

torch.Size([112556, 2])
We keep 5.03e+07/4.88e+09 =  1% of the original kernel matrix.

torch.Size([155827, 2])
We keep 1.18e+08/8.78e+09 =  1% of the original kernel matrix.

torch.Size([80309, 2])
We keep 2.31e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([27060, 2])
We keep 5.36e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([33184, 2])
We keep 5.65e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([186323, 2])
We keep 1.39e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([88414, 2])
We keep 2.64e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([6687, 2])
We keep 4.85e+05/9.99e+06 =  4% of the original kernel matrix.

torch.Size([16724, 2])
We keep 1.61e+06/7.10e+07 =  2% of the original kernel matrix.

torch.Size([28839, 2])
We keep 7.48e+06/3.28e+08 =  2% of the original kernel matrix.

torch.Size([34582, 2])
We keep 5.93e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([444065, 2])
We keep 7.60e+08/6.51e+10 =  1% of the original kernel matrix.

torch.Size([138422, 2])
We keep 5.71e+07/5.73e+09 =  0% of the original kernel matrix.

torch.Size([77010, 2])
We keep 3.27e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([55610, 2])
We keep 1.22e+07/9.80e+08 =  1% of the original kernel matrix.

torch.Size([8254, 2])
We keep 5.17e+05/1.32e+07 =  3% of the original kernel matrix.

torch.Size([18576, 2])
We keep 1.77e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([24855, 2])
We keep 4.35e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([32104, 2])
We keep 4.93e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([98771, 2])
We keep 1.55e+08/5.46e+09 =  2% of the original kernel matrix.

torch.Size([62115, 2])
We keep 1.93e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([47831, 2])
We keep 1.53e+07/8.28e+08 =  1% of the original kernel matrix.

torch.Size([44387, 2])
We keep 8.73e+06/6.46e+08 =  1% of the original kernel matrix.

torch.Size([199264, 2])
We keep 3.04e+08/1.55e+10 =  1% of the original kernel matrix.

torch.Size([92120, 2])
We keep 3.01e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([31385, 2])
We keep 4.67e+07/8.30e+08 =  5% of the original kernel matrix.

torch.Size([34115, 2])
We keep 8.34e+06/6.47e+08 =  1% of the original kernel matrix.

torch.Size([31910, 2])
We keep 2.34e+07/6.17e+08 =  3% of the original kernel matrix.

torch.Size([35637, 2])
We keep 7.53e+06/5.58e+08 =  1% of the original kernel matrix.

torch.Size([11475, 2])
We keep 1.25e+06/3.09e+07 =  4% of the original kernel matrix.

torch.Size([21622, 2])
We keep 2.45e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([7803, 2])
We keep 4.86e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([18157, 2])
We keep 1.72e+06/7.74e+07 =  2% of the original kernel matrix.

torch.Size([118251, 2])
We keep 7.24e+07/4.81e+09 =  1% of the original kernel matrix.

torch.Size([69467, 2])
We keep 1.83e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([108291, 2])
We keep 9.37e+07/4.37e+09 =  2% of the original kernel matrix.

torch.Size([65466, 2])
We keep 1.75e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([14557, 2])
We keep 7.14e+06/7.17e+07 =  9% of the original kernel matrix.

torch.Size([24264, 2])
We keep 3.16e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([193464, 2])
We keep 2.27e+08/1.64e+10 =  1% of the original kernel matrix.

torch.Size([89498, 2])
We keep 3.05e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([14607, 2])
We keep 3.26e+06/5.99e+07 =  5% of the original kernel matrix.

torch.Size([24428, 2])
We keep 3.14e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([42963, 2])
We keep 1.68e+07/6.42e+08 =  2% of the original kernel matrix.

torch.Size([42792, 2])
We keep 7.83e+06/5.69e+08 =  1% of the original kernel matrix.

torch.Size([11823, 2])
We keep 1.95e+06/3.86e+07 =  5% of the original kernel matrix.

torch.Size([22089, 2])
We keep 2.62e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([287572, 2])
We keep 2.29e+08/2.89e+10 =  0% of the original kernel matrix.

torch.Size([114962, 2])
We keep 3.97e+07/3.82e+09 =  1% of the original kernel matrix.

torch.Size([14005, 2])
We keep 2.64e+06/5.31e+07 =  4% of the original kernel matrix.

torch.Size([23934, 2])
We keep 2.99e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([86847, 2])
We keep 8.53e+07/2.85e+09 =  2% of the original kernel matrix.

torch.Size([58447, 2])
We keep 1.45e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([38228, 2])
We keep 9.76e+06/4.88e+08 =  1% of the original kernel matrix.

torch.Size([40723, 2])
We keep 7.05e+06/4.96e+08 =  1% of the original kernel matrix.

torch.Size([42748, 2])
We keep 1.47e+07/6.70e+08 =  2% of the original kernel matrix.

torch.Size([41999, 2])
We keep 7.88e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([93598, 2])
We keep 3.89e+07/2.85e+09 =  1% of the original kernel matrix.

torch.Size([61664, 2])
We keep 1.44e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([535554, 2])
We keep 1.55e+09/1.36e+11 =  1% of the original kernel matrix.

torch.Size([150760, 2])
We keep 7.82e+07/8.29e+09 =  0% of the original kernel matrix.

torch.Size([33733, 2])
We keep 5.87e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([38394, 2])
We keep 6.20e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([24090, 2])
We keep 8.59e+06/2.57e+08 =  3% of the original kernel matrix.

torch.Size([31079, 2])
We keep 5.45e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([180246, 2])
We keep 1.60e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([86882, 2])
We keep 2.64e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([28082, 2])
We keep 6.37e+06/2.51e+08 =  2% of the original kernel matrix.

torch.Size([34149, 2])
We keep 5.40e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([1028083, 2])
We keep 2.31e+09/3.17e+11 =  0% of the original kernel matrix.

torch.Size([218420, 2])
We keep 1.15e+08/1.27e+10 =  0% of the original kernel matrix.

torch.Size([16412, 2])
We keep 2.27e+06/7.57e+07 =  2% of the original kernel matrix.

torch.Size([25865, 2])
We keep 3.34e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([213185, 2])
We keep 1.76e+08/1.68e+10 =  1% of the original kernel matrix.

torch.Size([95280, 2])
We keep 3.08e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([763836, 2])
We keep 1.22e+09/1.90e+11 =  0% of the original kernel matrix.

torch.Size([186857, 2])
We keep 9.14e+07/9.79e+09 =  0% of the original kernel matrix.

torch.Size([10399, 2])
We keep 8.01e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([20572, 2])
We keep 2.14e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([261187, 2])
We keep 7.00e+08/4.95e+10 =  1% of the original kernel matrix.

torch.Size([100757, 2])
We keep 4.96e+07/5.00e+09 =  0% of the original kernel matrix.

torch.Size([33284, 2])
We keep 4.73e+07/1.11e+09 =  4% of the original kernel matrix.

torch.Size([34169, 2])
We keep 9.78e+06/7.47e+08 =  1% of the original kernel matrix.

torch.Size([794221, 2])
We keep 3.67e+09/2.42e+11 =  1% of the original kernel matrix.

torch.Size([190535, 2])
We keep 1.04e+08/1.11e+10 =  0% of the original kernel matrix.

torch.Size([20542, 2])
We keep 2.59e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([29183, 2])
We keep 3.90e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([71378, 2])
We keep 3.84e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([53221, 2])
We keep 1.26e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([8190, 2])
We keep 3.46e+06/3.27e+07 = 10% of the original kernel matrix.

torch.Size([18045, 2])
We keep 2.50e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([13713, 2])
We keep 1.70e+06/4.80e+07 =  3% of the original kernel matrix.

torch.Size([23610, 2])
We keep 2.89e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([13605, 2])
We keep 2.08e+06/5.36e+07 =  3% of the original kernel matrix.

torch.Size([23395, 2])
We keep 3.00e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([111095, 2])
We keep 8.52e+07/4.37e+09 =  1% of the original kernel matrix.

torch.Size([67425, 2])
We keep 1.72e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([72404, 2])
We keep 4.96e+07/2.04e+09 =  2% of the original kernel matrix.

torch.Size([53475, 2])
We keep 1.26e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([248290, 2])
We keep 4.22e+08/2.75e+10 =  1% of the original kernel matrix.

torch.Size([104172, 2])
We keep 3.87e+07/3.73e+09 =  1% of the original kernel matrix.

torch.Size([32880, 2])
We keep 7.00e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([37017, 2])
We keep 6.27e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([14506, 2])
We keep 1.44e+06/4.69e+07 =  3% of the original kernel matrix.

torch.Size([24215, 2])
We keep 2.82e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([10988, 2])
We keep 9.97e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([21179, 2])
We keep 2.21e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([224382, 2])
We keep 6.38e+08/2.92e+10 =  2% of the original kernel matrix.

torch.Size([98767, 2])
We keep 4.10e+07/3.84e+09 =  1% of the original kernel matrix.

torch.Size([50172, 2])
We keep 6.99e+07/1.72e+09 =  4% of the original kernel matrix.

torch.Size([44787, 2])
We keep 1.19e+07/9.32e+08 =  1% of the original kernel matrix.

torch.Size([719278, 2])
We keep 1.76e+09/1.52e+11 =  1% of the original kernel matrix.

torch.Size([180815, 2])
We keep 8.35e+07/8.75e+09 =  0% of the original kernel matrix.

torch.Size([76920, 2])
We keep 5.10e+07/2.04e+09 =  2% of the original kernel matrix.

torch.Size([55705, 2])
We keep 1.24e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([17810, 2])
We keep 4.09e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([26970, 2])
We keep 3.96e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([11084, 2])
We keep 1.35e+06/2.68e+07 =  5% of the original kernel matrix.

torch.Size([21322, 2])
We keep 2.24e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([21876, 2])
We keep 1.27e+07/2.19e+08 =  5% of the original kernel matrix.

torch.Size([29538, 2])
We keep 5.18e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([1959217, 2])
We keep 8.80e+09/1.07e+12 =  0% of the original kernel matrix.

torch.Size([307980, 2])
We keep 2.02e+08/2.32e+10 =  0% of the original kernel matrix.

torch.Size([23924, 2])
We keep 4.70e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([31584, 2])
We keep 4.67e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([30506, 2])
We keep 6.11e+06/2.93e+08 =  2% of the original kernel matrix.

torch.Size([35922, 2])
We keep 5.70e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([178344, 2])
We keep 2.24e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([85544, 2])
We keep 2.83e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([87620, 2])
We keep 8.51e+07/3.83e+09 =  2% of the original kernel matrix.

torch.Size([57680, 2])
We keep 1.66e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([21477, 2])
We keep 1.08e+07/1.44e+08 =  7% of the original kernel matrix.

torch.Size([29684, 2])
We keep 4.36e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([9438, 2])
We keep 7.67e+05/1.90e+07 =  4% of the original kernel matrix.

torch.Size([19629, 2])
We keep 2.03e+06/9.79e+07 =  2% of the original kernel matrix.

torch.Size([120407, 2])
We keep 2.46e+08/8.61e+09 =  2% of the original kernel matrix.

torch.Size([69445, 2])
We keep 2.32e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([87601, 2])
We keep 5.32e+07/2.65e+09 =  2% of the original kernel matrix.

torch.Size([59293, 2])
We keep 1.41e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([66542, 2])
We keep 2.55e+08/4.37e+09 =  5% of the original kernel matrix.

torch.Size([50306, 2])
We keep 1.78e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([133783, 2])
We keep 8.61e+07/5.90e+09 =  1% of the original kernel matrix.

torch.Size([74274, 2])
We keep 1.97e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([141827, 2])
We keep 2.07e+08/8.67e+09 =  2% of the original kernel matrix.

torch.Size([77212, 2])
We keep 2.35e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([145176, 2])
We keep 2.39e+08/9.89e+09 =  2% of the original kernel matrix.

torch.Size([77326, 2])
We keep 2.51e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([541054, 2])
We keep 1.33e+09/1.15e+11 =  1% of the original kernel matrix.

torch.Size([151990, 2])
We keep 7.35e+07/7.60e+09 =  0% of the original kernel matrix.

torch.Size([258981, 2])
We keep 4.17e+08/2.35e+10 =  1% of the original kernel matrix.

torch.Size([105996, 2])
We keep 3.59e+07/3.44e+09 =  1% of the original kernel matrix.

torch.Size([32822, 2])
We keep 3.32e+07/6.38e+08 =  5% of the original kernel matrix.

torch.Size([36065, 2])
We keep 8.02e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([145134, 2])
We keep 2.11e+08/7.92e+09 =  2% of the original kernel matrix.

torch.Size([77468, 2])
We keep 2.21e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([52375, 2])
We keep 1.25e+08/1.52e+09 =  8% of the original kernel matrix.

torch.Size([45491, 2])
We keep 1.14e+07/8.75e+08 =  1% of the original kernel matrix.

torch.Size([28688, 2])
We keep 2.66e+07/2.84e+08 =  9% of the original kernel matrix.

torch.Size([34586, 2])
We keep 5.67e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([28382, 2])
We keep 5.71e+06/2.78e+08 =  2% of the original kernel matrix.

torch.Size([34306, 2])
We keep 5.59e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([20624, 2])
We keep 2.62e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([29272, 2])
We keep 3.93e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([99576, 2])
We keep 4.64e+08/7.28e+09 =  6% of the original kernel matrix.

torch.Size([62589, 2])
We keep 1.99e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([53120, 2])
We keep 2.06e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([46176, 2])
We keep 9.51e+06/7.35e+08 =  1% of the original kernel matrix.

torch.Size([12091, 2])
We keep 4.57e+06/5.18e+07 =  8% of the original kernel matrix.

torch.Size([21912, 2])
We keep 2.94e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([41622, 2])
We keep 1.11e+07/6.19e+08 =  1% of the original kernel matrix.

torch.Size([41482, 2])
We keep 7.63e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([13784, 2])
We keep 5.78e+06/1.29e+08 =  4% of the original kernel matrix.

torch.Size([22972, 2])
We keep 4.06e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([61073, 2])
We keep 7.74e+07/2.19e+09 =  3% of the original kernel matrix.

torch.Size([48977, 2])
We keep 1.27e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([15329, 2])
We keep 1.08e+07/8.65e+07 = 12% of the original kernel matrix.

torch.Size([24782, 2])
We keep 3.43e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([36149, 2])
We keep 7.78e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([39978, 2])
We keep 6.69e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([18602, 2])
We keep 6.74e+06/1.88e+08 =  3% of the original kernel matrix.

torch.Size([26924, 2])
We keep 4.69e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([45338, 2])
We keep 2.41e+07/8.33e+08 =  2% of the original kernel matrix.

torch.Size([43036, 2])
We keep 8.90e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([9944, 2])
We keep 7.25e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([20328, 2])
We keep 2.02e+06/9.79e+07 =  2% of the original kernel matrix.

torch.Size([18240, 2])
We keep 2.05e+06/7.95e+07 =  2% of the original kernel matrix.

torch.Size([27227, 2])
We keep 3.43e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([49323, 2])
We keep 1.16e+07/7.63e+08 =  1% of the original kernel matrix.

torch.Size([45684, 2])
We keep 8.28e+06/6.20e+08 =  1% of the original kernel matrix.

torch.Size([398393, 2])
We keep 7.87e+08/5.90e+10 =  1% of the original kernel matrix.

torch.Size([130938, 2])
We keep 5.46e+07/5.46e+09 =  1% of the original kernel matrix.

torch.Size([16621, 2])
We keep 2.39e+06/7.52e+07 =  3% of the original kernel matrix.

torch.Size([26178, 2])
We keep 3.42e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([4839, 2])
We keep 1.70e+05/3.46e+06 =  4% of the original kernel matrix.

torch.Size([14961, 2])
We keep 1.11e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([15286, 2])
We keep 2.62e+06/6.25e+07 =  4% of the original kernel matrix.

torch.Size([24883, 2])
We keep 3.16e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([415416, 2])
We keep 9.85e+08/5.82e+10 =  1% of the original kernel matrix.

torch.Size([133867, 2])
We keep 5.44e+07/5.42e+09 =  1% of the original kernel matrix.

torch.Size([489720, 2])
We keep 9.47e+08/8.28e+10 =  1% of the original kernel matrix.

torch.Size([146345, 2])
We keep 6.29e+07/6.46e+09 =  0% of the original kernel matrix.

torch.Size([86634, 2])
We keep 5.66e+07/3.55e+09 =  1% of the original kernel matrix.

torch.Size([57451, 2])
We keep 1.57e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([16446, 2])
We keep 2.83e+06/8.00e+07 =  3% of the original kernel matrix.

torch.Size([25919, 2])
We keep 3.49e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([247037, 2])
We keep 5.85e+08/3.11e+10 =  1% of the original kernel matrix.

torch.Size([101797, 2])
We keep 4.08e+07/3.96e+09 =  1% of the original kernel matrix.

torch.Size([291327, 2])
We keep 2.57e+08/3.23e+10 =  0% of the original kernel matrix.

torch.Size([115609, 2])
We keep 4.21e+07/4.04e+09 =  1% of the original kernel matrix.

torch.Size([7821, 2])
We keep 5.26e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([18169, 2])
We keep 1.64e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([21317, 2])
We keep 5.27e+06/1.50e+08 =  3% of the original kernel matrix.

torch.Size([29517, 2])
We keep 4.34e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([85795, 2])
We keep 2.61e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([58469, 2])
We keep 1.26e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([38418, 2])
We keep 9.22e+07/1.19e+09 =  7% of the original kernel matrix.

torch.Size([38513, 2])
We keep 1.03e+07/7.74e+08 =  1% of the original kernel matrix.

torch.Size([71203, 2])
We keep 5.33e+07/2.26e+09 =  2% of the original kernel matrix.

torch.Size([53412, 2])
We keep 1.30e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([185902, 2])
We keep 1.44e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([88544, 2])
We keep 2.67e+07/2.44e+09 =  1% of the original kernel matrix.

torch.Size([49263, 2])
We keep 1.21e+07/7.84e+08 =  1% of the original kernel matrix.

torch.Size([45555, 2])
We keep 8.41e+06/6.29e+08 =  1% of the original kernel matrix.

torch.Size([41287, 2])
We keep 1.69e+08/1.05e+09 = 15% of the original kernel matrix.

torch.Size([40173, 2])
We keep 8.85e+06/7.29e+08 =  1% of the original kernel matrix.

torch.Size([10451, 2])
We keep 8.24e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([20650, 2])
We keep 2.20e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([58969, 2])
We keep 2.81e+07/1.26e+09 =  2% of the original kernel matrix.

torch.Size([48777, 2])
We keep 1.02e+07/7.97e+08 =  1% of the original kernel matrix.

torch.Size([45781, 2])
We keep 1.31e+07/6.92e+08 =  1% of the original kernel matrix.

torch.Size([43734, 2])
We keep 8.01e+06/5.91e+08 =  1% of the original kernel matrix.

torch.Size([52282, 2])
We keep 2.35e+07/1.10e+09 =  2% of the original kernel matrix.

torch.Size([45851, 2])
We keep 9.71e+06/7.44e+08 =  1% of the original kernel matrix.

torch.Size([111921, 2])
We keep 1.46e+08/4.80e+09 =  3% of the original kernel matrix.

torch.Size([66737, 2])
We keep 1.82e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([664594, 2])
We keep 8.66e+08/1.26e+11 =  0% of the original kernel matrix.

torch.Size([170379, 2])
We keep 7.57e+07/7.99e+09 =  0% of the original kernel matrix.

torch.Size([18234, 2])
We keep 3.75e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([27285, 2])
We keep 3.93e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([7373, 2])
We keep 4.86e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([17657, 2])
We keep 1.63e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([30754, 2])
We keep 5.45e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([36286, 2])
We keep 5.73e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([100200, 2])
We keep 2.00e+08/3.32e+09 =  6% of the original kernel matrix.

torch.Size([63655, 2])
We keep 1.53e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([58946, 2])
We keep 1.72e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([49183, 2])
We keep 9.83e+06/7.59e+08 =  1% of the original kernel matrix.

torch.Size([13293, 2])
We keep 2.20e+06/4.95e+07 =  4% of the original kernel matrix.

torch.Size([23332, 2])
We keep 2.82e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([9428, 2])
We keep 1.17e+06/2.37e+07 =  4% of the original kernel matrix.

torch.Size([19521, 2])
We keep 2.22e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([158871, 2])
We keep 6.91e+08/1.08e+10 =  6% of the original kernel matrix.

torch.Size([81044, 2])
We keep 2.61e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([12279, 2])
We keep 1.07e+06/2.99e+07 =  3% of the original kernel matrix.

torch.Size([22369, 2])
We keep 2.41e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([71330, 2])
We keep 9.22e+07/2.68e+09 =  3% of the original kernel matrix.

torch.Size([52533, 2])
We keep 1.42e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([11300, 2])
We keep 8.57e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([21457, 2])
We keep 2.26e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([32101, 2])
We keep 5.18e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([37116, 2])
We keep 5.75e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([35932, 2])
We keep 6.38e+07/5.49e+08 = 11% of the original kernel matrix.

torch.Size([38311, 2])
We keep 7.41e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([17532, 2])
We keep 1.79e+06/7.68e+07 =  2% of the original kernel matrix.

torch.Size([26839, 2])
We keep 3.40e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([13361, 2])
We keep 1.26e+06/3.97e+07 =  3% of the original kernel matrix.

torch.Size([23186, 2])
We keep 2.65e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([242959, 2])
We keep 1.76e+08/1.96e+10 =  0% of the original kernel matrix.

torch.Size([102595, 2])
We keep 3.28e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([534800, 2])
We keep 8.59e+08/8.71e+10 =  0% of the original kernel matrix.

torch.Size([152418, 2])
We keep 6.41e+07/6.63e+09 =  0% of the original kernel matrix.

torch.Size([253646, 2])
We keep 3.04e+08/2.44e+10 =  1% of the original kernel matrix.

torch.Size([104594, 2])
We keep 3.65e+07/3.51e+09 =  1% of the original kernel matrix.

torch.Size([176226, 2])
We keep 4.23e+08/1.60e+10 =  2% of the original kernel matrix.

torch.Size([84470, 2])
We keep 3.07e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([60419, 2])
We keep 2.08e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([49899, 2])
We keep 1.01e+07/7.73e+08 =  1% of the original kernel matrix.

torch.Size([18136, 2])
We keep 3.36e+06/9.78e+07 =  3% of the original kernel matrix.

torch.Size([27057, 2])
We keep 3.75e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([28505, 2])
We keep 7.17e+06/2.73e+08 =  2% of the original kernel matrix.

torch.Size([34614, 2])
We keep 5.58e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([273192, 2])
We keep 2.38e+08/2.83e+10 =  0% of the original kernel matrix.

torch.Size([110166, 2])
We keep 3.93e+07/3.78e+09 =  1% of the original kernel matrix.

torch.Size([32428, 2])
We keep 2.01e+07/3.55e+08 =  5% of the original kernel matrix.

torch.Size([37497, 2])
We keep 6.18e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([48761, 2])
We keep 2.50e+07/8.22e+08 =  3% of the original kernel matrix.

torch.Size([44893, 2])
We keep 8.60e+06/6.44e+08 =  1% of the original kernel matrix.

torch.Size([18199, 2])
We keep 3.63e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([27273, 2])
We keep 3.70e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([550207, 2])
We keep 3.04e+09/1.69e+11 =  1% of the original kernel matrix.

torch.Size([156107, 2])
We keep 8.77e+07/9.24e+09 =  0% of the original kernel matrix.

torch.Size([62226, 2])
We keep 4.75e+07/1.49e+09 =  3% of the original kernel matrix.

torch.Size([50182, 2])
We keep 1.10e+07/8.68e+08 =  1% of the original kernel matrix.

torch.Size([69573, 2])
We keep 3.74e+07/1.79e+09 =  2% of the original kernel matrix.

torch.Size([53388, 2])
We keep 1.20e+07/9.51e+08 =  1% of the original kernel matrix.

torch.Size([324098, 2])
We keep 7.05e+08/3.89e+10 =  1% of the original kernel matrix.

torch.Size([119528, 2])
We keep 4.49e+07/4.43e+09 =  1% of the original kernel matrix.

torch.Size([91623, 2])
We keep 3.05e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([60392, 2])
We keep 1.36e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([85255, 2])
We keep 7.55e+08/3.78e+09 = 19% of the original kernel matrix.

torch.Size([58481, 2])
We keep 1.59e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([361127, 2])
We keep 7.19e+08/5.19e+10 =  1% of the original kernel matrix.

torch.Size([125762, 2])
We keep 5.15e+07/5.12e+09 =  1% of the original kernel matrix.

torch.Size([283108, 2])
We keep 2.22e+08/2.69e+10 =  0% of the original kernel matrix.

torch.Size([110015, 2])
We keep 3.75e+07/3.68e+09 =  1% of the original kernel matrix.

torch.Size([98095, 2])
We keep 5.58e+07/3.34e+09 =  1% of the original kernel matrix.

torch.Size([62528, 2])
We keep 1.53e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([28180, 2])
We keep 9.74e+06/3.01e+08 =  3% of the original kernel matrix.

torch.Size([33900, 2])
We keep 5.49e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([87617, 2])
We keep 6.13e+07/2.70e+09 =  2% of the original kernel matrix.

torch.Size([59020, 2])
We keep 1.39e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([24541, 2])
We keep 7.33e+06/1.93e+08 =  3% of the original kernel matrix.

torch.Size([31841, 2])
We keep 4.87e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([209211, 2])
We keep 1.56e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([94016, 2])
We keep 2.84e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([1431502, 2])
We keep 3.85e+09/5.43e+11 =  0% of the original kernel matrix.

torch.Size([255654, 2])
We keep 1.48e+08/1.65e+10 =  0% of the original kernel matrix.

torch.Size([217803, 2])
We keep 9.41e+08/2.34e+10 =  4% of the original kernel matrix.

torch.Size([97182, 2])
We keep 3.49e+07/3.44e+09 =  1% of the original kernel matrix.

torch.Size([121300, 2])
We keep 2.72e+08/1.23e+10 =  2% of the original kernel matrix.

torch.Size([66865, 2])
We keep 2.75e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([21622, 2])
We keep 4.34e+06/1.35e+08 =  3% of the original kernel matrix.

torch.Size([29877, 2])
We keep 4.15e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([66715, 2])
We keep 1.12e+08/2.97e+09 =  3% of the original kernel matrix.

torch.Size([50563, 2])
We keep 1.49e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([47363, 2])
We keep 2.35e+07/8.61e+08 =  2% of the original kernel matrix.

torch.Size([44595, 2])
We keep 9.01e+06/6.59e+08 =  1% of the original kernel matrix.

torch.Size([30608, 2])
We keep 1.20e+07/4.00e+08 =  3% of the original kernel matrix.

torch.Size([35382, 2])
We keep 6.58e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([70641, 2])
We keep 4.06e+07/1.83e+09 =  2% of the original kernel matrix.

torch.Size([53365, 2])
We keep 1.22e+07/9.60e+08 =  1% of the original kernel matrix.

torch.Size([732722, 2])
We keep 1.05e+09/1.70e+11 =  0% of the original kernel matrix.

torch.Size([181850, 2])
We keep 8.64e+07/9.25e+09 =  0% of the original kernel matrix.

torch.Size([16996, 2])
We keep 8.88e+06/1.38e+08 =  6% of the original kernel matrix.

torch.Size([26008, 2])
We keep 4.18e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([185785, 2])
We keep 9.86e+08/3.26e+10 =  3% of the original kernel matrix.

torch.Size([84282, 2])
We keep 4.16e+07/4.06e+09 =  1% of the original kernel matrix.

torch.Size([115551, 2])
We keep 1.14e+08/5.39e+09 =  2% of the original kernel matrix.

torch.Size([68250, 2])
We keep 1.91e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([85738, 2])
We keep 3.82e+07/2.48e+09 =  1% of the original kernel matrix.

torch.Size([58764, 2])
We keep 1.36e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([17074, 2])
We keep 2.97e+06/8.87e+07 =  3% of the original kernel matrix.

torch.Size([26440, 2])
We keep 3.59e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([183727, 2])
We keep 3.76e+08/1.29e+10 =  2% of the original kernel matrix.

torch.Size([88240, 2])
We keep 2.79e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([283374, 2])
We keep 2.43e+08/2.84e+10 =  0% of the original kernel matrix.

torch.Size([111000, 2])
We keep 3.87e+07/3.78e+09 =  1% of the original kernel matrix.

torch.Size([8556, 2])
We keep 9.39e+05/1.77e+07 =  5% of the original kernel matrix.

torch.Size([18694, 2])
We keep 1.97e+06/9.46e+07 =  2% of the original kernel matrix.

torch.Size([1608695, 2])
We keep 3.95e+09/6.59e+11 =  0% of the original kernel matrix.

torch.Size([272572, 2])
We keep 1.61e+08/1.82e+10 =  0% of the original kernel matrix.

torch.Size([87592, 2])
We keep 5.93e+07/3.18e+09 =  1% of the original kernel matrix.

torch.Size([59694, 2])
We keep 1.49e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([137971, 2])
We keep 1.78e+08/8.19e+09 =  2% of the original kernel matrix.

torch.Size([75112, 2])
We keep 2.33e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([23239, 2])
We keep 3.73e+07/2.45e+08 = 15% of the original kernel matrix.

torch.Size([30883, 2])
We keep 4.90e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([44861, 2])
We keep 1.30e+07/7.12e+08 =  1% of the original kernel matrix.

torch.Size([43201, 2])
We keep 8.11e+06/5.99e+08 =  1% of the original kernel matrix.

torch.Size([7411, 2])
We keep 4.59e+05/1.00e+07 =  4% of the original kernel matrix.

torch.Size([17735, 2])
We keep 1.60e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([9190, 2])
We keep 1.13e+06/2.00e+07 =  5% of the original kernel matrix.

torch.Size([19319, 2])
We keep 2.07e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([43073, 2])
We keep 1.69e+07/6.26e+08 =  2% of the original kernel matrix.

torch.Size([42449, 2])
We keep 7.75e+06/5.62e+08 =  1% of the original kernel matrix.

torch.Size([236673, 2])
We keep 7.99e+08/3.55e+10 =  2% of the original kernel matrix.

torch.Size([98865, 2])
We keep 4.17e+07/4.23e+09 =  0% of the original kernel matrix.

torch.Size([2411837, 2])
We keep 4.81e+10/4.23e+12 =  1% of the original kernel matrix.

torch.Size([277086, 2])
We keep 3.91e+08/4.62e+10 =  0% of the original kernel matrix.

torch.Size([13911, 2])
We keep 2.45e+06/4.90e+07 =  5% of the original kernel matrix.

torch.Size([23914, 2])
We keep 2.88e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([313065, 2])
We keep 3.25e+08/3.54e+10 =  0% of the original kernel matrix.

torch.Size([117898, 2])
We keep 4.27e+07/4.23e+09 =  1% of the original kernel matrix.

torch.Size([15254, 2])
We keep 2.29e+06/5.56e+07 =  4% of the original kernel matrix.

torch.Size([24996, 2])
We keep 2.98e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([39714, 2])
We keep 9.55e+06/5.35e+08 =  1% of the original kernel matrix.

torch.Size([41122, 2])
We keep 7.30e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([23554, 2])
We keep 9.46e+06/2.47e+08 =  3% of the original kernel matrix.

torch.Size([30999, 2])
We keep 5.46e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([248158, 2])
We keep 1.02e+09/3.66e+10 =  2% of the original kernel matrix.

torch.Size([101460, 2])
We keep 4.44e+07/4.30e+09 =  1% of the original kernel matrix.

torch.Size([21200, 2])
We keep 5.46e+06/1.34e+08 =  4% of the original kernel matrix.

torch.Size([29486, 2])
We keep 4.20e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([2262396, 2])
We keep 7.09e+09/1.35e+12 =  0% of the original kernel matrix.

torch.Size([330779, 2])
We keep 2.26e+08/2.61e+10 =  0% of the original kernel matrix.

torch.Size([16548, 2])
We keep 1.84e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([25970, 2])
We keep 3.22e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([265068, 2])
We keep 4.01e+08/2.51e+10 =  1% of the original kernel matrix.

torch.Size([107759, 2])
We keep 3.74e+07/3.56e+09 =  1% of the original kernel matrix.

torch.Size([79508, 2])
We keep 2.43e+08/3.42e+09 =  7% of the original kernel matrix.

torch.Size([56295, 2])
We keep 1.59e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([141586, 2])
We keep 1.10e+08/7.69e+09 =  1% of the original kernel matrix.

torch.Size([76336, 2])
We keep 2.22e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([338637, 2])
We keep 4.61e+08/4.57e+10 =  1% of the original kernel matrix.

torch.Size([120687, 2])
We keep 4.80e+07/4.80e+09 =  0% of the original kernel matrix.

torch.Size([40065, 2])
We keep 9.08e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([41088, 2])
We keep 7.29e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([17809, 2])
We keep 2.26e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([26966, 2])
We keep 3.41e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([154533, 2])
We keep 7.62e+07/7.79e+09 =  0% of the original kernel matrix.

torch.Size([80039, 2])
We keep 2.21e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([17600, 2])
We keep 2.46e+06/8.51e+07 =  2% of the original kernel matrix.

torch.Size([26831, 2])
We keep 3.54e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([31377, 2])
We keep 1.93e+07/5.17e+08 =  3% of the original kernel matrix.

torch.Size([34724, 2])
We keep 7.29e+06/5.11e+08 =  1% of the original kernel matrix.

torch.Size([275319, 2])
We keep 2.93e+08/2.91e+10 =  1% of the original kernel matrix.

torch.Size([107926, 2])
We keep 3.89e+07/3.83e+09 =  1% of the original kernel matrix.

torch.Size([61127, 2])
We keep 1.24e+08/2.49e+09 =  4% of the original kernel matrix.

torch.Size([49185, 2])
We keep 1.38e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([66775, 2])
We keep 3.56e+07/1.65e+09 =  2% of the original kernel matrix.

torch.Size([52364, 2])
We keep 1.17e+07/9.13e+08 =  1% of the original kernel matrix.

torch.Size([60815, 2])
We keep 4.74e+07/1.61e+09 =  2% of the original kernel matrix.

torch.Size([48954, 2])
We keep 1.15e+07/9.02e+08 =  1% of the original kernel matrix.

torch.Size([402904, 2])
We keep 1.13e+09/6.59e+10 =  1% of the original kernel matrix.

torch.Size([131286, 2])
We keep 5.77e+07/5.77e+09 =  1% of the original kernel matrix.

torch.Size([612453, 2])
We keep 2.74e+09/2.01e+11 =  1% of the original kernel matrix.

torch.Size([162675, 2])
We keep 9.53e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([108527, 2])
We keep 8.62e+07/3.62e+09 =  2% of the original kernel matrix.

torch.Size([65795, 2])
We keep 1.61e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([18437, 2])
We keep 2.22e+06/8.99e+07 =  2% of the original kernel matrix.

torch.Size([27529, 2])
We keep 3.61e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([208760, 2])
We keep 7.19e+08/2.79e+10 =  2% of the original kernel matrix.

torch.Size([93272, 2])
We keep 3.95e+07/3.75e+09 =  1% of the original kernel matrix.

torch.Size([69412, 2])
We keep 3.62e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([52173, 2])
We keep 1.23e+07/9.91e+08 =  1% of the original kernel matrix.

torch.Size([89187, 2])
We keep 5.85e+07/2.70e+09 =  2% of the original kernel matrix.

torch.Size([59609, 2])
We keep 1.38e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([120374, 2])
We keep 2.32e+08/5.78e+09 =  4% of the original kernel matrix.

torch.Size([69725, 2])
We keep 2.00e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([76612, 2])
We keep 1.12e+08/4.08e+09 =  2% of the original kernel matrix.

torch.Size([54571, 2])
We keep 1.72e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([170495, 2])
We keep 2.27e+08/1.26e+10 =  1% of the original kernel matrix.

torch.Size([83808, 2])
We keep 2.77e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([219446, 2])
We keep 1.52e+09/6.04e+10 =  2% of the original kernel matrix.

torch.Size([84509, 2])
We keep 5.48e+07/5.52e+09 =  0% of the original kernel matrix.

torch.Size([54103, 2])
We keep 2.31e+07/1.09e+09 =  2% of the original kernel matrix.

torch.Size([46939, 2])
We keep 9.53e+06/7.40e+08 =  1% of the original kernel matrix.

torch.Size([41366, 2])
We keep 1.10e+07/5.88e+08 =  1% of the original kernel matrix.

torch.Size([41876, 2])
We keep 7.61e+06/5.45e+08 =  1% of the original kernel matrix.

torch.Size([10410, 2])
We keep 8.80e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([20617, 2])
We keep 2.20e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([30652, 2])
We keep 1.80e+07/3.51e+08 =  5% of the original kernel matrix.

torch.Size([35814, 2])
We keep 6.08e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([122620, 2])
We keep 1.77e+08/7.35e+09 =  2% of the original kernel matrix.

torch.Size([70087, 2])
We keep 2.18e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([192176, 2])
We keep 2.30e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([89518, 2])
We keep 2.86e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([325828, 2])
We keep 5.56e+08/3.69e+10 =  1% of the original kernel matrix.

torch.Size([119680, 2])
We keep 4.41e+07/4.31e+09 =  1% of the original kernel matrix.

torch.Size([18430, 2])
We keep 2.60e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([27556, 2])
We keep 3.62e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([43815, 2])
We keep 9.62e+06/6.55e+08 =  1% of the original kernel matrix.

torch.Size([42489, 2])
We keep 7.75e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([124221, 2])
We keep 1.11e+08/6.02e+09 =  1% of the original kernel matrix.

torch.Size([70907, 2])
We keep 2.00e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([301270, 2])
We keep 1.24e+09/3.17e+10 =  3% of the original kernel matrix.

torch.Size([114729, 2])
We keep 4.03e+07/4.00e+09 =  1% of the original kernel matrix.

torch.Size([88982, 2])
We keep 5.04e+07/2.94e+09 =  1% of the original kernel matrix.

torch.Size([59495, 2])
We keep 1.47e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([118713, 2])
We keep 6.25e+07/4.61e+09 =  1% of the original kernel matrix.

torch.Size([69245, 2])
We keep 1.77e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([191484, 2])
We keep 7.61e+09/8.71e+10 =  8% of the original kernel matrix.

torch.Size([80065, 2])
We keep 6.52e+07/6.63e+09 =  0% of the original kernel matrix.

torch.Size([64545, 2])
We keep 2.93e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([51254, 2])
We keep 1.07e+07/8.49e+08 =  1% of the original kernel matrix.

torch.Size([6859, 2])
We keep 4.56e+05/8.75e+06 =  5% of the original kernel matrix.

torch.Size([17280, 2])
We keep 1.53e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([16080, 2])
We keep 1.86e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([25645, 2])
We keep 3.23e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([345506, 2])
We keep 3.29e+08/3.93e+10 =  0% of the original kernel matrix.

torch.Size([123688, 2])
We keep 4.43e+07/4.45e+09 =  0% of the original kernel matrix.

torch.Size([57218, 2])
We keep 2.63e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([47817, 2])
We keep 1.06e+07/8.29e+08 =  1% of the original kernel matrix.

torch.Size([142106, 2])
We keep 6.10e+07/6.23e+09 =  0% of the original kernel matrix.

torch.Size([76520, 2])
We keep 2.00e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([7464, 2])
We keep 3.93e+05/9.23e+06 =  4% of the original kernel matrix.

torch.Size([17961, 2])
We keep 1.54e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([8735, 2])
We keep 5.46e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([19148, 2])
We keep 1.82e+06/8.41e+07 =  2% of the original kernel matrix.

torch.Size([301117, 2])
We keep 4.29e+08/3.26e+10 =  1% of the original kernel matrix.

torch.Size([114317, 2])
We keep 4.12e+07/4.06e+09 =  1% of the original kernel matrix.

torch.Size([180584, 2])
We keep 1.39e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([86470, 2])
We keep 2.66e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([108700, 2])
We keep 5.25e+07/3.72e+09 =  1% of the original kernel matrix.

torch.Size([65891, 2])
We keep 1.62e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([12871, 2])
We keep 1.20e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([23011, 2])
We keep 2.59e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([14368, 2])
We keep 1.56e+06/4.85e+07 =  3% of the original kernel matrix.

torch.Size([24207, 2])
We keep 2.88e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([78129, 2])
We keep 5.99e+07/1.97e+09 =  3% of the original kernel matrix.

torch.Size([56001, 2])
We keep 1.23e+07/9.98e+08 =  1% of the original kernel matrix.

torch.Size([89989, 2])
We keep 5.14e+07/2.64e+09 =  1% of the original kernel matrix.

torch.Size([59878, 2])
We keep 1.39e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([157179, 2])
We keep 3.17e+08/1.02e+10 =  3% of the original kernel matrix.

torch.Size([80769, 2])
We keep 2.52e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([15014, 2])
We keep 2.67e+06/7.13e+07 =  3% of the original kernel matrix.

torch.Size([24610, 2])
We keep 3.31e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([42486, 2])
We keep 7.20e+08/3.02e+09 = 23% of the original kernel matrix.

torch.Size([39289, 2])
We keep 1.50e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([39725, 2])
We keep 1.59e+07/6.38e+08 =  2% of the original kernel matrix.

torch.Size([40415, 2])
We keep 7.91e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([39203, 2])
We keep 2.74e+07/7.95e+08 =  3% of the original kernel matrix.

torch.Size([39227, 2])
We keep 8.68e+06/6.33e+08 =  1% of the original kernel matrix.

torch.Size([459371, 2])
We keep 9.26e+08/7.25e+10 =  1% of the original kernel matrix.

torch.Size([141235, 2])
We keep 5.92e+07/6.05e+09 =  0% of the original kernel matrix.

torch.Size([18476, 2])
We keep 3.42e+06/9.40e+07 =  3% of the original kernel matrix.

torch.Size([27686, 2])
We keep 3.62e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([78551, 2])
We keep 4.21e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([56113, 2])
We keep 1.28e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([241157, 2])
We keep 3.06e+08/2.02e+10 =  1% of the original kernel matrix.

torch.Size([102277, 2])
We keep 3.38e+07/3.20e+09 =  1% of the original kernel matrix.

torch.Size([70222, 2])
We keep 2.60e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([53371, 2])
We keep 1.10e+07/8.81e+08 =  1% of the original kernel matrix.

torch.Size([113181, 2])
We keep 8.09e+07/4.18e+09 =  1% of the original kernel matrix.

torch.Size([67838, 2])
We keep 1.71e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([288718, 2])
We keep 3.04e+08/2.78e+10 =  1% of the original kernel matrix.

torch.Size([114884, 2])
We keep 3.93e+07/3.74e+09 =  1% of the original kernel matrix.

torch.Size([6787, 2])
We keep 4.08e+05/8.72e+06 =  4% of the original kernel matrix.

torch.Size([17084, 2])
We keep 1.53e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([102332, 2])
We keep 4.40e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([63859, 2])
We keep 1.52e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([44093, 2])
We keep 1.08e+07/6.55e+08 =  1% of the original kernel matrix.

torch.Size([43361, 2])
We keep 7.77e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([198649, 2])
We keep 1.95e+08/1.30e+10 =  1% of the original kernel matrix.

torch.Size([91207, 2])
We keep 2.70e+07/2.56e+09 =  1% of the original kernel matrix.

torch.Size([146575, 2])
We keep 1.39e+08/9.82e+09 =  1% of the original kernel matrix.

torch.Size([77704, 2])
We keep 2.45e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([70626, 2])
We keep 5.75e+07/2.20e+09 =  2% of the original kernel matrix.

torch.Size([52159, 2])
We keep 1.31e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([156078, 2])
We keep 1.14e+08/7.98e+09 =  1% of the original kernel matrix.

torch.Size([80398, 2])
We keep 2.26e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([30365, 2])
We keep 5.22e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([35591, 2])
We keep 5.66e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([23809, 2])
We keep 2.01e+07/5.11e+08 =  3% of the original kernel matrix.

torch.Size([29558, 2])
We keep 7.13e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([26244, 2])
We keep 4.38e+07/2.51e+08 = 17% of the original kernel matrix.

torch.Size([32825, 2])
We keep 5.25e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([23911, 2])
We keep 5.96e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([31207, 2])
We keep 5.14e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([216382, 2])
We keep 1.95e+08/1.62e+10 =  1% of the original kernel matrix.

torch.Size([96100, 2])
We keep 3.06e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([455731, 2])
We keep 1.99e+09/8.62e+10 =  2% of the original kernel matrix.

torch.Size([140730, 2])
We keep 6.55e+07/6.59e+09 =  0% of the original kernel matrix.

torch.Size([16803, 2])
We keep 4.92e+06/1.07e+08 =  4% of the original kernel matrix.

torch.Size([25971, 2])
We keep 3.87e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([41194, 2])
We keep 1.09e+07/5.36e+08 =  2% of the original kernel matrix.

torch.Size([41734, 2])
We keep 7.20e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([27633, 2])
We keep 5.40e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([34102, 2])
We keep 5.14e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([488507, 2])
We keep 9.48e+08/9.00e+10 =  1% of the original kernel matrix.

torch.Size([145377, 2])
We keep 6.43e+07/6.74e+09 =  0% of the original kernel matrix.

torch.Size([91078, 2])
We keep 4.26e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([59974, 2])
We keep 1.37e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([28970, 2])
We keep 7.83e+06/2.90e+08 =  2% of the original kernel matrix.

torch.Size([34841, 2])
We keep 5.56e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([143848, 2])
We keep 1.87e+08/8.63e+09 =  2% of the original kernel matrix.

torch.Size([76969, 2])
We keep 2.24e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([178224, 2])
We keep 1.13e+08/1.00e+10 =  1% of the original kernel matrix.

torch.Size([86264, 2])
We keep 2.45e+07/2.25e+09 =  1% of the original kernel matrix.

torch.Size([22197, 2])
We keep 4.37e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([29970, 2])
We keep 4.57e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([117156, 2])
We keep 1.20e+08/5.01e+09 =  2% of the original kernel matrix.

torch.Size([68915, 2])
We keep 1.86e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([21117, 2])
We keep 7.53e+06/1.23e+08 =  6% of the original kernel matrix.

torch.Size([29616, 2])
We keep 3.98e+06/2.49e+08 =  1% of the original kernel matrix.

time for making ranges is 5.585001707077026
Sorting X and nu_X
time for sorting X is 0.09600496292114258
Sorting Z and nu_Z
time for sorting Z is 0.00026726722717285156
Starting Optim
sum tnu_Z before tensor(46703520., device='cuda:0')
c= tensor(2453.2009, device='cuda:0')
c= tensor(136775.3750, device='cuda:0')
c= tensor(145911.9375, device='cuda:0')
c= tensor(150957.1719, device='cuda:0')
c= tensor(1619749.7500, device='cuda:0')
c= tensor(3206339., device='cuda:0')
c= tensor(4789284., device='cuda:0')
c= tensor(5632436.5000, device='cuda:0')
c= tensor(5674534.5000, device='cuda:0')
c= tensor(33221774., device='cuda:0')
c= tensor(33293262., device='cuda:0')
c= tensor(36809292., device='cuda:0')
c= tensor(36829412., device='cuda:0')
c= tensor(65004648., device='cuda:0')
c= tensor(65310720., device='cuda:0')
c= tensor(67896344., device='cuda:0')
c= tensor(70626352., device='cuda:0')
c= tensor(71758536., device='cuda:0')
c= tensor(96703632., device='cuda:0')
c= tensor(1.0574e+08, device='cuda:0')
c= tensor(1.0776e+08, device='cuda:0')
c= tensor(1.8087e+08, device='cuda:0')
c= tensor(1.8100e+08, device='cuda:0')
c= tensor(1.8119e+08, device='cuda:0')
c= tensor(1.8207e+08, device='cuda:0')
c= tensor(1.8362e+08, device='cuda:0')
c= tensor(1.8472e+08, device='cuda:0')
c= tensor(1.8480e+08, device='cuda:0')
c= tensor(1.8916e+08, device='cuda:0')
c= tensor(1.1189e+09, device='cuda:0')
c= tensor(1.1189e+09, device='cuda:0')
c= tensor(1.3259e+09, device='cuda:0')
c= tensor(1.3260e+09, device='cuda:0')
c= tensor(1.3260e+09, device='cuda:0')
c= tensor(1.3262e+09, device='cuda:0')
c= tensor(1.3450e+09, device='cuda:0')
c= tensor(1.3507e+09, device='cuda:0')
c= tensor(1.3507e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3509e+09, device='cuda:0')
c= tensor(1.3509e+09, device='cuda:0')
c= tensor(1.3509e+09, device='cuda:0')
c= tensor(1.3510e+09, device='cuda:0')
c= tensor(1.3511e+09, device='cuda:0')
c= tensor(1.3511e+09, device='cuda:0')
c= tensor(1.3511e+09, device='cuda:0')
c= tensor(1.3511e+09, device='cuda:0')
c= tensor(1.3511e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3518e+09, device='cuda:0')
c= tensor(1.3518e+09, device='cuda:0')
c= tensor(1.3518e+09, device='cuda:0')
c= tensor(1.3518e+09, device='cuda:0')
c= tensor(1.3518e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3526e+09, device='cuda:0')
c= tensor(1.3526e+09, device='cuda:0')
c= tensor(1.3526e+09, device='cuda:0')
c= tensor(1.3526e+09, device='cuda:0')
c= tensor(1.3527e+09, device='cuda:0')
c= tensor(1.3527e+09, device='cuda:0')
c= tensor(1.3527e+09, device='cuda:0')
c= tensor(1.3527e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3534e+09, device='cuda:0')
c= tensor(1.3534e+09, device='cuda:0')
c= tensor(1.3534e+09, device='cuda:0')
c= tensor(1.3534e+09, device='cuda:0')
c= tensor(1.3534e+09, device='cuda:0')
c= tensor(1.3535e+09, device='cuda:0')
c= tensor(1.3535e+09, device='cuda:0')
c= tensor(1.3535e+09, device='cuda:0')
c= tensor(1.3535e+09, device='cuda:0')
c= tensor(1.3535e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3538e+09, device='cuda:0')
c= tensor(1.3538e+09, device='cuda:0')
c= tensor(1.3538e+09, device='cuda:0')
c= tensor(1.3557e+09, device='cuda:0')
c= tensor(1.3560e+09, device='cuda:0')
c= tensor(1.3560e+09, device='cuda:0')
c= tensor(1.3560e+09, device='cuda:0')
c= tensor(1.3561e+09, device='cuda:0')
c= tensor(1.4178e+09, device='cuda:0')
c= tensor(1.5768e+09, device='cuda:0')
c= tensor(1.5768e+09, device='cuda:0')
c= tensor(1.5829e+09, device='cuda:0')
c= tensor(1.5844e+09, device='cuda:0')
c= tensor(1.5868e+09, device='cuda:0')
c= tensor(1.8190e+09, device='cuda:0')
c= tensor(1.8190e+09, device='cuda:0')
c= tensor(1.8190e+09, device='cuda:0')
c= tensor(1.8374e+09, device='cuda:0')
c= tensor(1.9343e+09, device='cuda:0')
c= tensor(1.9343e+09, device='cuda:0')
c= tensor(1.9347e+09, device='cuda:0')
c= tensor(2.1193e+09, device='cuda:0')
c= tensor(2.1256e+09, device='cuda:0')
c= tensor(2.1365e+09, device='cuda:0')
c= tensor(2.1376e+09, device='cuda:0')
c= tensor(2.1387e+09, device='cuda:0')
c= tensor(2.1388e+09, device='cuda:0')
c= tensor(2.1388e+09, device='cuda:0')
c= tensor(2.3424e+09, device='cuda:0')
c= tensor(2.3424e+09, device='cuda:0')
c= tensor(2.3425e+09, device='cuda:0')
c= tensor(2.3440e+09, device='cuda:0')
c= tensor(2.3454e+09, device='cuda:0')
c= tensor(2.3816e+09, device='cuda:0')
c= tensor(2.3851e+09, device='cuda:0')
c= tensor(2.3851e+09, device='cuda:0')
c= tensor(2.3893e+09, device='cuda:0')
c= tensor(2.3894e+09, device='cuda:0')
c= tensor(2.3920e+09, device='cuda:0')
c= tensor(2.3945e+09, device='cuda:0')
c= tensor(2.3945e+09, device='cuda:0')
c= tensor(2.3960e+09, device='cuda:0')
c= tensor(2.3960e+09, device='cuda:0')
c= tensor(2.3960e+09, device='cuda:0')
c= tensor(2.4023e+09, device='cuda:0')
c= tensor(2.4052e+09, device='cuda:0')
c= tensor(2.4062e+09, device='cuda:0')
c= tensor(2.4120e+09, device='cuda:0')
c= tensor(2.4501e+09, device='cuda:0')
c= tensor(2.4502e+09, device='cuda:0')
c= tensor(2.4504e+09, device='cuda:0')
c= tensor(2.4531e+09, device='cuda:0')
c= tensor(2.4531e+09, device='cuda:0')
c= tensor(2.4668e+09, device='cuda:0')
c= tensor(2.5067e+09, device='cuda:0')
c= tensor(2.5891e+09, device='cuda:0')
c= tensor(2.5896e+09, device='cuda:0')
c= tensor(2.5901e+09, device='cuda:0')
c= tensor(2.5902e+09, device='cuda:0')
c= tensor(2.5902e+09, device='cuda:0')
c= tensor(2.5929e+09, device='cuda:0')
c= tensor(2.5930e+09, device='cuda:0')
c= tensor(2.5936e+09, device='cuda:0')
c= tensor(2.6133e+09, device='cuda:0')
c= tensor(2.6155e+09, device='cuda:0')
c= tensor(2.6156e+09, device='cuda:0')
c= tensor(2.6157e+09, device='cuda:0')
c= tensor(2.6238e+09, device='cuda:0')
c= tensor(2.6241e+09, device='cuda:0')
c= tensor(2.6244e+09, device='cuda:0')
c= tensor(2.6245e+09, device='cuda:0')
c= tensor(2.8185e+09, device='cuda:0')
c= tensor(2.8187e+09, device='cuda:0')
c= tensor(2.8457e+09, device='cuda:0')
c= tensor(2.8459e+09, device='cuda:0')
c= tensor(2.8512e+09, device='cuda:0')
c= tensor(2.8523e+09, device='cuda:0')
c= tensor(2.9404e+09, device='cuda:0')
c= tensor(2.9416e+09, device='cuda:0')
c= tensor(2.9417e+09, device='cuda:0')
c= tensor(2.9490e+09, device='cuda:0')
c= tensor(2.9521e+09, device='cuda:0')
c= tensor(2.9522e+09, device='cuda:0')
c= tensor(2.9547e+09, device='cuda:0')
c= tensor(2.9630e+09, device='cuda:0')
c= tensor(2.9935e+09, device='cuda:0')
c= tensor(2.9957e+09, device='cuda:0')
c= tensor(2.9958e+09, device='cuda:0')
c= tensor(2.9958e+09, device='cuda:0')
c= tensor(3.0460e+09, device='cuda:0')
c= tensor(3.0920e+09, device='cuda:0')
c= tensor(3.0922e+09, device='cuda:0')
c= tensor(3.0922e+09, device='cuda:0')
c= tensor(3.0931e+09, device='cuda:0')
c= tensor(3.0974e+09, device='cuda:0')
c= tensor(3.1156e+09, device='cuda:0')
c= tensor(3.1156e+09, device='cuda:0')
c= tensor(3.1168e+09, device='cuda:0')
c= tensor(3.1169e+09, device='cuda:0')
c= tensor(3.1170e+09, device='cuda:0')
c= tensor(3.1171e+09, device='cuda:0')
c= tensor(3.1171e+09, device='cuda:0')
c= tensor(3.1189e+09, device='cuda:0')
c= tensor(3.1196e+09, device='cuda:0')
c= tensor(3.1198e+09, device='cuda:0')
c= tensor(3.1201e+09, device='cuda:0')
c= tensor(3.1202e+09, device='cuda:0')
c= tensor(3.2230e+09, device='cuda:0')
c= tensor(3.2230e+09, device='cuda:0')
c= tensor(3.2323e+09, device='cuda:0')
c= tensor(3.2323e+09, device='cuda:0')
c= tensor(3.2324e+09, device='cuda:0')
c= tensor(3.2324e+09, device='cuda:0')
c= tensor(3.2331e+09, device='cuda:0')
c= tensor(3.2332e+09, device='cuda:0')
c= tensor(3.2448e+09, device='cuda:0')
c= tensor(3.2448e+09, device='cuda:0')
c= tensor(3.2448e+09, device='cuda:0')
c= tensor(3.2755e+09, device='cuda:0')
c= tensor(3.2761e+09, device='cuda:0')
c= tensor(3.2765e+09, device='cuda:0')
c= tensor(3.2848e+09, device='cuda:0')
c= tensor(3.3016e+09, device='cuda:0')
c= tensor(3.3016e+09, device='cuda:0')
c= tensor(3.3016e+09, device='cuda:0')
c= tensor(3.3025e+09, device='cuda:0')
c= tensor(3.3025e+09, device='cuda:0')
c= tensor(3.3026e+09, device='cuda:0')
c= tensor(3.3027e+09, device='cuda:0')
c= tensor(3.3027e+09, device='cuda:0')
c= tensor(3.3028e+09, device='cuda:0')
c= tensor(3.3028e+09, device='cuda:0')
c= tensor(3.3029e+09, device='cuda:0')
c= tensor(3.3748e+09, device='cuda:0')
c= tensor(3.3749e+09, device='cuda:0')
c= tensor(3.3798e+09, device='cuda:0')
c= tensor(3.3800e+09, device='cuda:0')
c= tensor(3.3807e+09, device='cuda:0')
c= tensor(3.3836e+09, device='cuda:0')
c= tensor(3.8034e+09, device='cuda:0')
c= tensor(3.9319e+09, device='cuda:0')
c= tensor(3.9335e+09, device='cuda:0')
c= tensor(3.9574e+09, device='cuda:0')
c= tensor(3.9574e+09, device='cuda:0')
c= tensor(3.9614e+09, device='cuda:0')
c= tensor(3.9631e+09, device='cuda:0')
c= tensor(3.9697e+09, device='cuda:0')
c= tensor(3.9697e+09, device='cuda:0')
c= tensor(3.9733e+09, device='cuda:0')
c= tensor(4.0952e+09, device='cuda:0')
c= tensor(4.0973e+09, device='cuda:0')
c= tensor(4.0974e+09, device='cuda:0')
c= tensor(4.0991e+09, device='cuda:0')
c= tensor(4.0992e+09, device='cuda:0')
c= tensor(4.0992e+09, device='cuda:0')
c= tensor(4.1069e+09, device='cuda:0')
c= tensor(4.1075e+09, device='cuda:0')
c= tensor(4.1075e+09, device='cuda:0')
c= tensor(4.1130e+09, device='cuda:0')
c= tensor(4.1130e+09, device='cuda:0')
c= tensor(4.1130e+09, device='cuda:0')
c= tensor(4.1221e+09, device='cuda:0')
c= tensor(4.1251e+09, device='cuda:0')
c= tensor(4.1273e+09, device='cuda:0')
c= tensor(4.1376e+09, device='cuda:0')
c= tensor(4.1450e+09, device='cuda:0')
c= tensor(4.1451e+09, device='cuda:0')
c= tensor(4.1464e+09, device='cuda:0')
c= tensor(4.1527e+09, device='cuda:0')
c= tensor(4.1584e+09, device='cuda:0')
c= tensor(4.1585e+09, device='cuda:0')
c= tensor(4.2079e+09, device='cuda:0')
c= tensor(4.2633e+09, device='cuda:0')
c= tensor(4.2657e+09, device='cuda:0')
c= tensor(4.2664e+09, device='cuda:0')
c= tensor(4.2678e+09, device='cuda:0')
c= tensor(4.2679e+09, device='cuda:0')
c= tensor(4.2680e+09, device='cuda:0')
c= tensor(4.2680e+09, device='cuda:0')
c= tensor(4.2749e+09, device='cuda:0')
c= tensor(4.2823e+09, device='cuda:0')
c= tensor(4.3418e+09, device='cuda:0')
c= tensor(4.3755e+09, device='cuda:0')
c= tensor(4.3788e+09, device='cuda:0')
c= tensor(4.3792e+09, device='cuda:0')
c= tensor(4.3827e+09, device='cuda:0')
c= tensor(4.3827e+09, device='cuda:0')
c= tensor(4.3828e+09, device='cuda:0')
c= tensor(4.4095e+09, device='cuda:0')
c= tensor(4.4102e+09, device='cuda:0')
c= tensor(4.4103e+09, device='cuda:0')
c= tensor(4.4104e+09, device='cuda:0')
c= tensor(4.4152e+09, device='cuda:0')
c= tensor(4.4156e+09, device='cuda:0')
c= tensor(4.4230e+09, device='cuda:0')
c= tensor(4.4240e+09, device='cuda:0')
c= tensor(4.4245e+09, device='cuda:0')
c= tensor(4.4245e+09, device='cuda:0')
c= tensor(4.4245e+09, device='cuda:0')
c= tensor(4.4264e+09, device='cuda:0')
c= tensor(4.4288e+09, device='cuda:0')
c= tensor(4.4289e+09, device='cuda:0')
c= tensor(4.4347e+09, device='cuda:0')
c= tensor(4.4348e+09, device='cuda:0')
c= tensor(4.4351e+09, device='cuda:0')
c= tensor(4.4352e+09, device='cuda:0')
c= tensor(4.4407e+09, device='cuda:0')
c= tensor(4.4407e+09, device='cuda:0')
c= tensor(4.4426e+09, device='cuda:0')
c= tensor(4.4429e+09, device='cuda:0')
c= tensor(4.4432e+09, device='cuda:0')
c= tensor(4.4444e+09, device='cuda:0')
c= tensor(4.5100e+09, device='cuda:0')
c= tensor(4.5101e+09, device='cuda:0')
c= tensor(4.5104e+09, device='cuda:0')
c= tensor(4.5167e+09, device='cuda:0')
c= tensor(4.5168e+09, device='cuda:0')
c= tensor(4.6066e+09, device='cuda:0')
c= tensor(4.6067e+09, device='cuda:0')
c= tensor(4.6117e+09, device='cuda:0')
c= tensor(4.6481e+09, device='cuda:0')
c= tensor(4.6481e+09, device='cuda:0')
c= tensor(4.6754e+09, device='cuda:0')
c= tensor(4.6768e+09, device='cuda:0')
c= tensor(4.7979e+09, device='cuda:0')
c= tensor(4.7980e+09, device='cuda:0')
c= tensor(4.7990e+09, device='cuda:0')
c= tensor(4.7991e+09, device='cuda:0')
c= tensor(4.7991e+09, device='cuda:0')
c= tensor(4.7992e+09, device='cuda:0')
c= tensor(4.8034e+09, device='cuda:0')
c= tensor(4.8046e+09, device='cuda:0')
c= tensor(4.8185e+09, device='cuda:0')
c= tensor(4.8186e+09, device='cuda:0')
c= tensor(4.8186e+09, device='cuda:0')
c= tensor(4.8187e+09, device='cuda:0')
c= tensor(4.8401e+09, device='cuda:0')
c= tensor(4.8417e+09, device='cuda:0')
c= tensor(4.9045e+09, device='cuda:0')
c= tensor(4.9056e+09, device='cuda:0')
c= tensor(4.9057e+09, device='cuda:0')
c= tensor(4.9058e+09, device='cuda:0')
c= tensor(4.9062e+09, device='cuda:0')
c= tensor(5.2526e+09, device='cuda:0')
c= tensor(5.2527e+09, device='cuda:0')
c= tensor(5.2528e+09, device='cuda:0')
c= tensor(5.2600e+09, device='cuda:0')
c= tensor(5.2632e+09, device='cuda:0')
c= tensor(5.2635e+09, device='cuda:0')
c= tensor(5.2635e+09, device='cuda:0')
c= tensor(5.2740e+09, device='cuda:0')
c= tensor(5.2754e+09, device='cuda:0')
c= tensor(5.2843e+09, device='cuda:0')
c= tensor(5.2861e+09, device='cuda:0')
c= tensor(5.2920e+09, device='cuda:0')
c= tensor(5.2987e+09, device='cuda:0')
c= tensor(5.3480e+09, device='cuda:0')
c= tensor(5.3628e+09, device='cuda:0')
c= tensor(5.3639e+09, device='cuda:0')
c= tensor(5.3693e+09, device='cuda:0')
c= tensor(5.3739e+09, device='cuda:0')
c= tensor(5.3746e+09, device='cuda:0')
c= tensor(5.3747e+09, device='cuda:0')
c= tensor(5.3748e+09, device='cuda:0')
c= tensor(5.3877e+09, device='cuda:0')
c= tensor(5.3882e+09, device='cuda:0')
c= tensor(5.3882e+09, device='cuda:0')
c= tensor(5.3885e+09, device='cuda:0')
c= tensor(5.3887e+09, device='cuda:0')
c= tensor(5.3911e+09, device='cuda:0')
c= tensor(5.3913e+09, device='cuda:0')
c= tensor(5.3915e+09, device='cuda:0')
c= tensor(5.3916e+09, device='cuda:0')
c= tensor(5.3922e+09, device='cuda:0')
c= tensor(5.3922e+09, device='cuda:0')
c= tensor(5.3922e+09, device='cuda:0')
c= tensor(5.3925e+09, device='cuda:0')
c= tensor(5.4188e+09, device='cuda:0')
c= tensor(5.4189e+09, device='cuda:0')
c= tensor(5.4189e+09, device='cuda:0')
c= tensor(5.4189e+09, device='cuda:0')
c= tensor(5.4560e+09, device='cuda:0')
c= tensor(5.4856e+09, device='cuda:0')
c= tensor(5.4874e+09, device='cuda:0')
c= tensor(5.4875e+09, device='cuda:0')
c= tensor(5.5076e+09, device='cuda:0')
c= tensor(5.5141e+09, device='cuda:0')
c= tensor(5.5141e+09, device='cuda:0')
c= tensor(5.5144e+09, device='cuda:0')
c= tensor(5.5149e+09, device='cuda:0')
c= tensor(5.5204e+09, device='cuda:0')
c= tensor(5.5230e+09, device='cuda:0')
c= tensor(5.5269e+09, device='cuda:0')
c= tensor(5.5271e+09, device='cuda:0')
c= tensor(5.5302e+09, device='cuda:0')
c= tensor(5.5302e+09, device='cuda:0')
c= tensor(5.5309e+09, device='cuda:0')
c= tensor(5.5311e+09, device='cuda:0')
c= tensor(5.5322e+09, device='cuda:0')
c= tensor(5.5354e+09, device='cuda:0')
c= tensor(5.5587e+09, device='cuda:0')
c= tensor(5.5587e+09, device='cuda:0')
c= tensor(5.5588e+09, device='cuda:0')
c= tensor(5.5589e+09, device='cuda:0')
c= tensor(5.5644e+09, device='cuda:0')
c= tensor(5.5647e+09, device='cuda:0')
c= tensor(5.5648e+09, device='cuda:0')
c= tensor(5.5648e+09, device='cuda:0')
c= tensor(5.5902e+09, device='cuda:0')
c= tensor(5.5902e+09, device='cuda:0')
c= tensor(5.5928e+09, device='cuda:0')
c= tensor(5.5928e+09, device='cuda:0')
c= tensor(5.5929e+09, device='cuda:0')
c= tensor(5.5942e+09, device='cuda:0')
c= tensor(5.5942e+09, device='cuda:0')
c= tensor(5.5942e+09, device='cuda:0')
c= tensor(5.5996e+09, device='cuda:0')
c= tensor(5.6290e+09, device='cuda:0')
c= tensor(5.6396e+09, device='cuda:0')
c= tensor(5.6503e+09, device='cuda:0')
c= tensor(5.6508e+09, device='cuda:0')
c= tensor(5.6509e+09, device='cuda:0')
c= tensor(5.6510e+09, device='cuda:0')
c= tensor(5.6572e+09, device='cuda:0')
c= tensor(5.6576e+09, device='cuda:0')
c= tensor(5.6584e+09, device='cuda:0')
c= tensor(5.6584e+09, device='cuda:0')
c= tensor(5.7703e+09, device='cuda:0')
c= tensor(5.7712e+09, device='cuda:0')
c= tensor(5.7722e+09, device='cuda:0')
c= tensor(5.8094e+09, device='cuda:0')
c= tensor(5.8100e+09, device='cuda:0')
c= tensor(5.8269e+09, device='cuda:0')
c= tensor(5.8503e+09, device='cuda:0')
c= tensor(5.8600e+09, device='cuda:0')
c= tensor(5.8611e+09, device='cuda:0')
c= tensor(5.8613e+09, device='cuda:0')
c= tensor(5.8626e+09, device='cuda:0')
c= tensor(5.8628e+09, device='cuda:0')
c= tensor(5.8665e+09, device='cuda:0')
c= tensor(5.9960e+09, device='cuda:0')
c= tensor(6.0260e+09, device='cuda:0')
c= tensor(6.0337e+09, device='cuda:0')
c= tensor(6.0337e+09, device='cuda:0')
c= tensor(6.0366e+09, device='cuda:0')
c= tensor(6.0372e+09, device='cuda:0')
c= tensor(6.0376e+09, device='cuda:0')
c= tensor(6.0384e+09, device='cuda:0')
c= tensor(6.0728e+09, device='cuda:0')
c= tensor(6.0730e+09, device='cuda:0')
c= tensor(6.1181e+09, device='cuda:0')
c= tensor(6.1219e+09, device='cuda:0')
c= tensor(6.1231e+09, device='cuda:0')
c= tensor(6.1231e+09, device='cuda:0')
c= tensor(6.1355e+09, device='cuda:0')
c= tensor(6.1419e+09, device='cuda:0')
c= tensor(6.1419e+09, device='cuda:0')
c= tensor(6.2674e+09, device='cuda:0')
c= tensor(6.2701e+09, device='cuda:0')
c= tensor(6.2754e+09, device='cuda:0')
c= tensor(6.2760e+09, device='cuda:0')
c= tensor(6.2763e+09, device='cuda:0')
c= tensor(6.2763e+09, device='cuda:0')
c= tensor(6.2763e+09, device='cuda:0')
c= tensor(6.2768e+09, device='cuda:0')
c= tensor(6.3034e+09, device='cuda:0')
c= tensor(8.5699e+09, device='cuda:0')
c= tensor(8.5700e+09, device='cuda:0')
c= tensor(8.5784e+09, device='cuda:0')
c= tensor(8.5784e+09, device='cuda:0')
c= tensor(8.5786e+09, device='cuda:0')
c= tensor(8.5788e+09, device='cuda:0')
c= tensor(8.6200e+09, device='cuda:0')
c= tensor(8.6203e+09, device='cuda:0')
c= tensor(8.9158e+09, device='cuda:0')
c= tensor(8.9158e+09, device='cuda:0')
c= tensor(8.9296e+09, device='cuda:0')
c= tensor(8.9368e+09, device='cuda:0')
c= tensor(8.9400e+09, device='cuda:0')
c= tensor(8.9576e+09, device='cuda:0')
c= tensor(8.9577e+09, device='cuda:0')
c= tensor(8.9578e+09, device='cuda:0')
c= tensor(8.9595e+09, device='cuda:0')
c= tensor(8.9596e+09, device='cuda:0')
c= tensor(8.9600e+09, device='cuda:0')
c= tensor(8.9687e+09, device='cuda:0')
c= tensor(8.9747e+09, device='cuda:0')
c= tensor(8.9756e+09, device='cuda:0')
c= tensor(8.9770e+09, device='cuda:0')
c= tensor(9.0148e+09, device='cuda:0')
c= tensor(9.1106e+09, device='cuda:0')
c= tensor(9.1128e+09, device='cuda:0')
c= tensor(9.1129e+09, device='cuda:0')
c= tensor(9.1445e+09, device='cuda:0')
c= tensor(9.1452e+09, device='cuda:0')
c= tensor(9.1466e+09, device='cuda:0')
c= tensor(9.1546e+09, device='cuda:0')
c= tensor(9.1580e+09, device='cuda:0')
c= tensor(9.1648e+09, device='cuda:0')
c= tensor(9.2301e+09, device='cuda:0')
c= tensor(9.2308e+09, device='cuda:0')
c= tensor(9.2310e+09, device='cuda:0')
c= tensor(9.2310e+09, device='cuda:0')
c= tensor(9.2314e+09, device='cuda:0')
c= tensor(9.2364e+09, device='cuda:0')
c= tensor(9.2435e+09, device='cuda:0')
c= tensor(9.2625e+09, device='cuda:0')
c= tensor(9.2626e+09, device='cuda:0')
c= tensor(9.2629e+09, device='cuda:0')
c= tensor(9.2660e+09, device='cuda:0')
c= tensor(9.2993e+09, device='cuda:0')
c= tensor(9.3004e+09, device='cuda:0')
c= tensor(9.3025e+09, device='cuda:0')
c= tensor(9.5852e+09, device='cuda:0')
c= tensor(9.5861e+09, device='cuda:0')
c= tensor(9.5861e+09, device='cuda:0')
c= tensor(9.5862e+09, device='cuda:0')
c= tensor(9.5978e+09, device='cuda:0')
c= tensor(9.5992e+09, device='cuda:0')
c= tensor(9.6005e+09, device='cuda:0')
c= tensor(9.6005e+09, device='cuda:0')
c= tensor(9.6005e+09, device='cuda:0')
c= tensor(9.6148e+09, device='cuda:0')
c= tensor(9.6186e+09, device='cuda:0')
c= tensor(9.6200e+09, device='cuda:0')
c= tensor(9.6200e+09, device='cuda:0')
c= tensor(9.6200e+09, device='cuda:0')
c= tensor(9.6211e+09, device='cuda:0')
c= tensor(9.6223e+09, device='cuda:0')
c= tensor(9.6321e+09, device='cuda:0')
c= tensor(9.6322e+09, device='cuda:0')
c= tensor(9.6508e+09, device='cuda:0')
c= tensor(9.6511e+09, device='cuda:0')
c= tensor(9.6519e+09, device='cuda:0')
c= tensor(9.6842e+09, device='cuda:0')
c= tensor(9.6844e+09, device='cuda:0')
c= tensor(9.6856e+09, device='cuda:0')
c= tensor(9.6937e+09, device='cuda:0')
c= tensor(9.6943e+09, device='cuda:0')
c= tensor(9.6962e+09, device='cuda:0')
c= tensor(9.7048e+09, device='cuda:0')
c= tensor(9.7048e+09, device='cuda:0')
c= tensor(9.7057e+09, device='cuda:0')
c= tensor(9.7059e+09, device='cuda:0')
c= tensor(9.7100e+09, device='cuda:0')
c= tensor(9.7139e+09, device='cuda:0')
c= tensor(9.7152e+09, device='cuda:0')
c= tensor(9.7186e+09, device='cuda:0')
c= tensor(9.7188e+09, device='cuda:0')
c= tensor(9.7213e+09, device='cuda:0')
c= tensor(9.7223e+09, device='cuda:0')
c= tensor(9.7224e+09, device='cuda:0')
c= tensor(9.7278e+09, device='cuda:0')
c= tensor(9.8009e+09, device='cuda:0')
c= tensor(9.8010e+09, device='cuda:0')
c= tensor(9.8014e+09, device='cuda:0')
c= tensor(9.8015e+09, device='cuda:0')
c= tensor(9.8339e+09, device='cuda:0')
c= tensor(9.8347e+09, device='cuda:0')
c= tensor(9.8348e+09, device='cuda:0')
c= tensor(9.8391e+09, device='cuda:0')
c= tensor(9.8417e+09, device='cuda:0')
c= tensor(9.8418e+09, device='cuda:0')
c= tensor(9.8451e+09, device='cuda:0')
c= tensor(9.8452e+09, device='cuda:0')
memory (bytes)
5456113664
time for making loss 2 is 12.741418361663818
p0 True
it  0 : 2856595456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 69% |
shape of L is 
torch.Size([])
memory (bytes)
5456322560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5456875520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  107236720000.0
relative error loss 10.89225
shape of L is 
torch.Size([])
memory (bytes)
5579550720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 20% |
memory (bytes)
5579563008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  107236330000.0
relative error loss 10.89221
shape of L is 
torch.Size([])
memory (bytes)
5584818176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 20% |
memory (bytes)
5584875520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  107235344000.0
relative error loss 10.89211
shape of L is 
torch.Size([])
memory (bytes)
5586898944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5586944000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  107229150000.0
relative error loss 10.89148
shape of L is 
torch.Size([])
memory (bytes)
5589098496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5589102592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 20% |
error is  107161140000.0
relative error loss 10.884573
shape of L is 
torch.Size([])
memory (bytes)
5591240704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5591240704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  106821090000.0
relative error loss 10.850034
shape of L is 
torch.Size([])
memory (bytes)
5593272320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5593358336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  104965734000.0
relative error loss 10.661581
shape of L is 
torch.Size([])
memory (bytes)
5595492352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5595508736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  86325840000.0
relative error loss 8.76829
shape of L is 
torch.Size([])
memory (bytes)
5597659136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5597659136
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 20% |
error is  26387083000.0
relative error loss 2.6801891
shape of L is 
torch.Size([])
memory (bytes)
5599694848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5599694848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  16057033000.0
relative error loss 1.6309451
time to take a step is 200.16514205932617
it  1 : 3298093056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5601955840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5601955840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  16057033000.0
relative error loss 1.6309451
shape of L is 
torch.Size([])
memory (bytes)
5604052992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5604052992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  17603350000.0
relative error loss 1.7880077
shape of L is 
torch.Size([])
memory (bytes)
5606252544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 20% |
memory (bytes)
5606252544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  12312322000.0
relative error loss 1.2505873
shape of L is 
torch.Size([])
memory (bytes)
5608361984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5608361984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  10682004000.0
relative error loss 1.0849926
shape of L is 
torch.Size([])
memory (bytes)
5610536960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5610536960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  9955948000.0
relative error loss 1.0112456
shape of L is 
torch.Size([])
memory (bytes)
5612675072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 20% |
memory (bytes)
5612675072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  13722883000.0
relative error loss 1.3938608
shape of L is 
torch.Size([])
memory (bytes)
5614743552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 20% |
memory (bytes)
5614743552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  9487520000.0
relative error loss 0.9636665
shape of L is 
torch.Size([])
memory (bytes)
5616914432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 20% |
memory (bytes)
5616914432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 20% |
error is  9079749000.0
relative error loss 0.92224836
shape of L is 
torch.Size([])
memory (bytes)
5618806784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 20% |
memory (bytes)
5619011584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  8885721000.0
relative error loss 0.90254056
shape of L is 
torch.Size([])
memory (bytes)
5621063680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 20% |
memory (bytes)
5621063680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  8655785000.0
relative error loss 0.8791855
time to take a step is 200.02258920669556
it  2 : 3424236032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5623246848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 20% |
memory (bytes)
5623246848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  8655785000.0
relative error loss 0.8791855
shape of L is 
torch.Size([])
memory (bytes)
5625106432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 20% |
memory (bytes)
5625352192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  8258637000.0
relative error loss 0.8388464
shape of L is 
torch.Size([])
memory (bytes)
5627465728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 20% |
memory (bytes)
5627465728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  8293091000.0
relative error loss 0.84234595
shape of L is 
torch.Size([])
memory (bytes)
5629419520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5629587456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  7959839000.0
relative error loss 0.8084969
shape of L is 
torch.Size([])
memory (bytes)
5631696896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5631696896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  7925425700.0
relative error loss 0.80500144
shape of L is 
torch.Size([])
memory (bytes)
5633826816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5633826816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  7684001300.0
relative error loss 0.7804795
shape of L is 
torch.Size([])
memory (bytes)
5635837952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5635837952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  7252106000.0
relative error loss 0.736611
shape of L is 
torch.Size([])
memory (bytes)
5637935104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5637935104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  6978225000.0
relative error loss 0.7087924
shape of L is 
torch.Size([])
memory (bytes)
5640212480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 20% |
memory (bytes)
5640212480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  6625709000.0
relative error loss 0.6729866
shape of L is 
torch.Size([])
memory (bytes)
5642280960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5642280960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  7195487700.0
relative error loss 0.7308602
shape of L is 
torch.Size([])
memory (bytes)
5644410880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5644410880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 20% |
error is  6090131000.0
relative error loss 0.61858684
time to take a step is 213.8293387889862
it  3 : 3424236032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5646528512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5646528512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  6090131000.0
relative error loss 0.61858684
shape of L is 
torch.Size([])
memory (bytes)
5648748544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5648748544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 20% |
error is  5572433000.0
relative error loss 0.5660032
shape of L is 
torch.Size([])
memory (bytes)
5650747392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 20% |
memory (bytes)
5650747392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  5149735400.0
relative error loss 0.523069
shape of L is 
torch.Size([])
memory (bytes)
5652701184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5652815872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  4707913700.0
relative error loss 0.47819227
shape of L is 
torch.Size([])
memory (bytes)
5655162880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5655162880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  4468923400.0
relative error loss 0.45391753
shape of L is 
torch.Size([])
memory (bytes)
5657288704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5657288704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  7059275000.0
relative error loss 0.71702474
shape of L is 
torch.Size([])
memory (bytes)
5659369472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5659369472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  4060610600.0
relative error loss 0.41244438
shape of L is 
torch.Size([])
memory (bytes)
5661564928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5661564928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  3815941600.0
relative error loss 0.38759288
shape of L is 
torch.Size([])
memory (bytes)
5663547392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
5663547392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  3588405200.0
relative error loss 0.36448154
shape of L is 
torch.Size([])
memory (bytes)
5665800192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 20% |
memory (bytes)
5665800192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  3201100800.0
relative error loss 0.32514226
time to take a step is 192.34464859962463
c= tensor(2453.2009, device='cuda:0')
c= tensor(136775.3750, device='cuda:0')
c= tensor(145911.9375, device='cuda:0')
c= tensor(150957.1719, device='cuda:0')
c= tensor(1619749.7500, device='cuda:0')
c= tensor(3206339., device='cuda:0')
c= tensor(4789284., device='cuda:0')
c= tensor(5632436.5000, device='cuda:0')
c= tensor(5674534.5000, device='cuda:0')
c= tensor(33221774., device='cuda:0')
c= tensor(33293262., device='cuda:0')
c= tensor(36809292., device='cuda:0')
c= tensor(36829412., device='cuda:0')
c= tensor(65004648., device='cuda:0')
c= tensor(65310720., device='cuda:0')
c= tensor(67896344., device='cuda:0')
c= tensor(70626352., device='cuda:0')
c= tensor(71758536., device='cuda:0')
c= tensor(96703632., device='cuda:0')
c= tensor(1.0574e+08, device='cuda:0')
c= tensor(1.0776e+08, device='cuda:0')
c= tensor(1.8087e+08, device='cuda:0')
c= tensor(1.8100e+08, device='cuda:0')
c= tensor(1.8119e+08, device='cuda:0')
c= tensor(1.8207e+08, device='cuda:0')
c= tensor(1.8362e+08, device='cuda:0')
c= tensor(1.8472e+08, device='cuda:0')
c= tensor(1.8480e+08, device='cuda:0')
c= tensor(1.8916e+08, device='cuda:0')
c= tensor(1.1189e+09, device='cuda:0')
c= tensor(1.1189e+09, device='cuda:0')
c= tensor(1.3259e+09, device='cuda:0')
c= tensor(1.3260e+09, device='cuda:0')
c= tensor(1.3260e+09, device='cuda:0')
c= tensor(1.3262e+09, device='cuda:0')
c= tensor(1.3450e+09, device='cuda:0')
c= tensor(1.3507e+09, device='cuda:0')
c= tensor(1.3507e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3509e+09, device='cuda:0')
c= tensor(1.3509e+09, device='cuda:0')
c= tensor(1.3509e+09, device='cuda:0')
c= tensor(1.3510e+09, device='cuda:0')
c= tensor(1.3511e+09, device='cuda:0')
c= tensor(1.3511e+09, device='cuda:0')
c= tensor(1.3511e+09, device='cuda:0')
c= tensor(1.3511e+09, device='cuda:0')
c= tensor(1.3511e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3512e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3516e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3518e+09, device='cuda:0')
c= tensor(1.3518e+09, device='cuda:0')
c= tensor(1.3518e+09, device='cuda:0')
c= tensor(1.3518e+09, device='cuda:0')
c= tensor(1.3518e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3526e+09, device='cuda:0')
c= tensor(1.3526e+09, device='cuda:0')
c= tensor(1.3526e+09, device='cuda:0')
c= tensor(1.3526e+09, device='cuda:0')
c= tensor(1.3527e+09, device='cuda:0')
c= tensor(1.3527e+09, device='cuda:0')
c= tensor(1.3527e+09, device='cuda:0')
c= tensor(1.3527e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3531e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3533e+09, device='cuda:0')
c= tensor(1.3534e+09, device='cuda:0')
c= tensor(1.3534e+09, device='cuda:0')
c= tensor(1.3534e+09, device='cuda:0')
c= tensor(1.3534e+09, device='cuda:0')
c= tensor(1.3534e+09, device='cuda:0')
c= tensor(1.3535e+09, device='cuda:0')
c= tensor(1.3535e+09, device='cuda:0')
c= tensor(1.3535e+09, device='cuda:0')
c= tensor(1.3535e+09, device='cuda:0')
c= tensor(1.3535e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3537e+09, device='cuda:0')
c= tensor(1.3538e+09, device='cuda:0')
c= tensor(1.3538e+09, device='cuda:0')
c= tensor(1.3538e+09, device='cuda:0')
c= tensor(1.3557e+09, device='cuda:0')
c= tensor(1.3560e+09, device='cuda:0')
c= tensor(1.3560e+09, device='cuda:0')
c= tensor(1.3560e+09, device='cuda:0')
c= tensor(1.3561e+09, device='cuda:0')
c= tensor(1.4178e+09, device='cuda:0')
c= tensor(1.5768e+09, device='cuda:0')
c= tensor(1.5768e+09, device='cuda:0')
c= tensor(1.5829e+09, device='cuda:0')
c= tensor(1.5844e+09, device='cuda:0')
c= tensor(1.5868e+09, device='cuda:0')
c= tensor(1.8190e+09, device='cuda:0')
c= tensor(1.8190e+09, device='cuda:0')
c= tensor(1.8190e+09, device='cuda:0')
c= tensor(1.8374e+09, device='cuda:0')
c= tensor(1.9343e+09, device='cuda:0')
c= tensor(1.9343e+09, device='cuda:0')
c= tensor(1.9347e+09, device='cuda:0')
c= tensor(2.1193e+09, device='cuda:0')
c= tensor(2.1256e+09, device='cuda:0')
c= tensor(2.1365e+09, device='cuda:0')
c= tensor(2.1376e+09, device='cuda:0')
c= tensor(2.1387e+09, device='cuda:0')
c= tensor(2.1388e+09, device='cuda:0')
c= tensor(2.1388e+09, device='cuda:0')
c= tensor(2.3424e+09, device='cuda:0')
c= tensor(2.3424e+09, device='cuda:0')
c= tensor(2.3425e+09, device='cuda:0')
c= tensor(2.3440e+09, device='cuda:0')
c= tensor(2.3454e+09, device='cuda:0')
c= tensor(2.3816e+09, device='cuda:0')
c= tensor(2.3851e+09, device='cuda:0')
c= tensor(2.3851e+09, device='cuda:0')
c= tensor(2.3893e+09, device='cuda:0')
c= tensor(2.3894e+09, device='cuda:0')
c= tensor(2.3920e+09, device='cuda:0')
c= tensor(2.3945e+09, device='cuda:0')
c= tensor(2.3945e+09, device='cuda:0')
c= tensor(2.3960e+09, device='cuda:0')
c= tensor(2.3960e+09, device='cuda:0')
c= tensor(2.3960e+09, device='cuda:0')
c= tensor(2.4023e+09, device='cuda:0')
c= tensor(2.4052e+09, device='cuda:0')
c= tensor(2.4062e+09, device='cuda:0')
c= tensor(2.4120e+09, device='cuda:0')
c= tensor(2.4501e+09, device='cuda:0')
c= tensor(2.4502e+09, device='cuda:0')
c= tensor(2.4504e+09, device='cuda:0')
c= tensor(2.4531e+09, device='cuda:0')
c= tensor(2.4531e+09, device='cuda:0')
c= tensor(2.4668e+09, device='cuda:0')
c= tensor(2.5067e+09, device='cuda:0')
c= tensor(2.5891e+09, device='cuda:0')
c= tensor(2.5896e+09, device='cuda:0')
c= tensor(2.5901e+09, device='cuda:0')
c= tensor(2.5902e+09, device='cuda:0')
c= tensor(2.5902e+09, device='cuda:0')
c= tensor(2.5929e+09, device='cuda:0')
c= tensor(2.5930e+09, device='cuda:0')
c= tensor(2.5936e+09, device='cuda:0')
c= tensor(2.6133e+09, device='cuda:0')
c= tensor(2.6155e+09, device='cuda:0')
c= tensor(2.6156e+09, device='cuda:0')
c= tensor(2.6157e+09, device='cuda:0')
c= tensor(2.6238e+09, device='cuda:0')
c= tensor(2.6241e+09, device='cuda:0')
c= tensor(2.6244e+09, device='cuda:0')
c= tensor(2.6245e+09, device='cuda:0')
c= tensor(2.8185e+09, device='cuda:0')
c= tensor(2.8187e+09, device='cuda:0')
c= tensor(2.8457e+09, device='cuda:0')
c= tensor(2.8459e+09, device='cuda:0')
c= tensor(2.8512e+09, device='cuda:0')
c= tensor(2.8523e+09, device='cuda:0')
c= tensor(2.9404e+09, device='cuda:0')
c= tensor(2.9416e+09, device='cuda:0')
c= tensor(2.9417e+09, device='cuda:0')
c= tensor(2.9490e+09, device='cuda:0')
c= tensor(2.9521e+09, device='cuda:0')
c= tensor(2.9522e+09, device='cuda:0')
c= tensor(2.9547e+09, device='cuda:0')
c= tensor(2.9630e+09, device='cuda:0')
c= tensor(2.9935e+09, device='cuda:0')
c= tensor(2.9957e+09, device='cuda:0')
c= tensor(2.9958e+09, device='cuda:0')
c= tensor(2.9958e+09, device='cuda:0')
c= tensor(3.0460e+09, device='cuda:0')
c= tensor(3.0920e+09, device='cuda:0')
c= tensor(3.0922e+09, device='cuda:0')
c= tensor(3.0922e+09, device='cuda:0')
c= tensor(3.0931e+09, device='cuda:0')
c= tensor(3.0974e+09, device='cuda:0')
c= tensor(3.1156e+09, device='cuda:0')
c= tensor(3.1156e+09, device='cuda:0')
c= tensor(3.1168e+09, device='cuda:0')
c= tensor(3.1169e+09, device='cuda:0')
c= tensor(3.1170e+09, device='cuda:0')
c= tensor(3.1171e+09, device='cuda:0')
c= tensor(3.1171e+09, device='cuda:0')
c= tensor(3.1189e+09, device='cuda:0')
c= tensor(3.1196e+09, device='cuda:0')
c= tensor(3.1198e+09, device='cuda:0')
c= tensor(3.1201e+09, device='cuda:0')
c= tensor(3.1202e+09, device='cuda:0')
c= tensor(3.2230e+09, device='cuda:0')
c= tensor(3.2230e+09, device='cuda:0')
c= tensor(3.2323e+09, device='cuda:0')
c= tensor(3.2323e+09, device='cuda:0')
c= tensor(3.2324e+09, device='cuda:0')
c= tensor(3.2324e+09, device='cuda:0')
c= tensor(3.2331e+09, device='cuda:0')
c= tensor(3.2332e+09, device='cuda:0')
c= tensor(3.2448e+09, device='cuda:0')
c= tensor(3.2448e+09, device='cuda:0')
c= tensor(3.2448e+09, device='cuda:0')
c= tensor(3.2755e+09, device='cuda:0')
c= tensor(3.2761e+09, device='cuda:0')
c= tensor(3.2765e+09, device='cuda:0')
c= tensor(3.2848e+09, device='cuda:0')
c= tensor(3.3016e+09, device='cuda:0')
c= tensor(3.3016e+09, device='cuda:0')
c= tensor(3.3016e+09, device='cuda:0')
c= tensor(3.3025e+09, device='cuda:0')
c= tensor(3.3025e+09, device='cuda:0')
c= tensor(3.3026e+09, device='cuda:0')
c= tensor(3.3027e+09, device='cuda:0')
c= tensor(3.3027e+09, device='cuda:0')
c= tensor(3.3028e+09, device='cuda:0')
c= tensor(3.3028e+09, device='cuda:0')
c= tensor(3.3029e+09, device='cuda:0')
c= tensor(3.3748e+09, device='cuda:0')
c= tensor(3.3749e+09, device='cuda:0')
c= tensor(3.3798e+09, device='cuda:0')
c= tensor(3.3800e+09, device='cuda:0')
c= tensor(3.3807e+09, device='cuda:0')
c= tensor(3.3836e+09, device='cuda:0')
c= tensor(3.8034e+09, device='cuda:0')
c= tensor(3.9319e+09, device='cuda:0')
c= tensor(3.9335e+09, device='cuda:0')
c= tensor(3.9574e+09, device='cuda:0')
c= tensor(3.9574e+09, device='cuda:0')
c= tensor(3.9614e+09, device='cuda:0')
c= tensor(3.9631e+09, device='cuda:0')
c= tensor(3.9697e+09, device='cuda:0')
c= tensor(3.9697e+09, device='cuda:0')
c= tensor(3.9733e+09, device='cuda:0')
c= tensor(4.0952e+09, device='cuda:0')
c= tensor(4.0973e+09, device='cuda:0')
c= tensor(4.0974e+09, device='cuda:0')
c= tensor(4.0991e+09, device='cuda:0')
c= tensor(4.0992e+09, device='cuda:0')
c= tensor(4.0992e+09, device='cuda:0')
c= tensor(4.1069e+09, device='cuda:0')
c= tensor(4.1075e+09, device='cuda:0')
c= tensor(4.1075e+09, device='cuda:0')
c= tensor(4.1130e+09, device='cuda:0')
c= tensor(4.1130e+09, device='cuda:0')
c= tensor(4.1130e+09, device='cuda:0')
c= tensor(4.1221e+09, device='cuda:0')
c= tensor(4.1251e+09, device='cuda:0')
c= tensor(4.1273e+09, device='cuda:0')
c= tensor(4.1376e+09, device='cuda:0')
c= tensor(4.1450e+09, device='cuda:0')
c= tensor(4.1451e+09, device='cuda:0')
c= tensor(4.1464e+09, device='cuda:0')
c= tensor(4.1527e+09, device='cuda:0')
c= tensor(4.1584e+09, device='cuda:0')
c= tensor(4.1585e+09, device='cuda:0')
c= tensor(4.2079e+09, device='cuda:0')
c= tensor(4.2633e+09, device='cuda:0')
c= tensor(4.2657e+09, device='cuda:0')
c= tensor(4.2664e+09, device='cuda:0')
c= tensor(4.2678e+09, device='cuda:0')
c= tensor(4.2679e+09, device='cuda:0')
c= tensor(4.2680e+09, device='cuda:0')
c= tensor(4.2680e+09, device='cuda:0')
c= tensor(4.2749e+09, device='cuda:0')
c= tensor(4.2823e+09, device='cuda:0')
c= tensor(4.3418e+09, device='cuda:0')
c= tensor(4.3755e+09, device='cuda:0')
c= tensor(4.3788e+09, device='cuda:0')
c= tensor(4.3792e+09, device='cuda:0')
c= tensor(4.3827e+09, device='cuda:0')
c= tensor(4.3827e+09, device='cuda:0')
c= tensor(4.3828e+09, device='cuda:0')
c= tensor(4.4095e+09, device='cuda:0')
c= tensor(4.4102e+09, device='cuda:0')
c= tensor(4.4103e+09, device='cuda:0')
c= tensor(4.4104e+09, device='cuda:0')
c= tensor(4.4152e+09, device='cuda:0')
c= tensor(4.4156e+09, device='cuda:0')
c= tensor(4.4230e+09, device='cuda:0')
c= tensor(4.4240e+09, device='cuda:0')
c= tensor(4.4245e+09, device='cuda:0')
c= tensor(4.4245e+09, device='cuda:0')
c= tensor(4.4245e+09, device='cuda:0')
c= tensor(4.4264e+09, device='cuda:0')
c= tensor(4.4288e+09, device='cuda:0')
c= tensor(4.4289e+09, device='cuda:0')
c= tensor(4.4347e+09, device='cuda:0')
c= tensor(4.4348e+09, device='cuda:0')
c= tensor(4.4351e+09, device='cuda:0')
c= tensor(4.4352e+09, device='cuda:0')
c= tensor(4.4407e+09, device='cuda:0')
c= tensor(4.4407e+09, device='cuda:0')
c= tensor(4.4426e+09, device='cuda:0')
c= tensor(4.4429e+09, device='cuda:0')
c= tensor(4.4432e+09, device='cuda:0')
c= tensor(4.4444e+09, device='cuda:0')
c= tensor(4.5100e+09, device='cuda:0')
c= tensor(4.5101e+09, device='cuda:0')
c= tensor(4.5104e+09, device='cuda:0')
c= tensor(4.5167e+09, device='cuda:0')
c= tensor(4.5168e+09, device='cuda:0')
c= tensor(4.6066e+09, device='cuda:0')
c= tensor(4.6067e+09, device='cuda:0')
c= tensor(4.6117e+09, device='cuda:0')
c= tensor(4.6481e+09, device='cuda:0')
c= tensor(4.6481e+09, device='cuda:0')
c= tensor(4.6754e+09, device='cuda:0')
c= tensor(4.6768e+09, device='cuda:0')
c= tensor(4.7979e+09, device='cuda:0')
c= tensor(4.7980e+09, device='cuda:0')
c= tensor(4.7990e+09, device='cuda:0')
c= tensor(4.7991e+09, device='cuda:0')
c= tensor(4.7991e+09, device='cuda:0')
c= tensor(4.7992e+09, device='cuda:0')
c= tensor(4.8034e+09, device='cuda:0')
c= tensor(4.8046e+09, device='cuda:0')
c= tensor(4.8185e+09, device='cuda:0')
c= tensor(4.8186e+09, device='cuda:0')
c= tensor(4.8186e+09, device='cuda:0')
c= tensor(4.8187e+09, device='cuda:0')
c= tensor(4.8401e+09, device='cuda:0')
c= tensor(4.8417e+09, device='cuda:0')
c= tensor(4.9045e+09, device='cuda:0')
c= tensor(4.9056e+09, device='cuda:0')
c= tensor(4.9057e+09, device='cuda:0')
c= tensor(4.9058e+09, device='cuda:0')
c= tensor(4.9062e+09, device='cuda:0')
c= tensor(5.2526e+09, device='cuda:0')
c= tensor(5.2527e+09, device='cuda:0')
c= tensor(5.2528e+09, device='cuda:0')
c= tensor(5.2600e+09, device='cuda:0')
c= tensor(5.2632e+09, device='cuda:0')
c= tensor(5.2635e+09, device='cuda:0')
c= tensor(5.2635e+09, device='cuda:0')
c= tensor(5.2740e+09, device='cuda:0')
c= tensor(5.2754e+09, device='cuda:0')
c= tensor(5.2843e+09, device='cuda:0')
c= tensor(5.2861e+09, device='cuda:0')
c= tensor(5.2920e+09, device='cuda:0')
c= tensor(5.2987e+09, device='cuda:0')
c= tensor(5.3480e+09, device='cuda:0')
c= tensor(5.3628e+09, device='cuda:0')
c= tensor(5.3639e+09, device='cuda:0')
c= tensor(5.3693e+09, device='cuda:0')
c= tensor(5.3739e+09, device='cuda:0')
c= tensor(5.3746e+09, device='cuda:0')
c= tensor(5.3747e+09, device='cuda:0')
c= tensor(5.3748e+09, device='cuda:0')
c= tensor(5.3877e+09, device='cuda:0')
c= tensor(5.3882e+09, device='cuda:0')
c= tensor(5.3882e+09, device='cuda:0')
c= tensor(5.3885e+09, device='cuda:0')
c= tensor(5.3887e+09, device='cuda:0')
c= tensor(5.3911e+09, device='cuda:0')
c= tensor(5.3913e+09, device='cuda:0')
c= tensor(5.3915e+09, device='cuda:0')
c= tensor(5.3916e+09, device='cuda:0')
c= tensor(5.3922e+09, device='cuda:0')
c= tensor(5.3922e+09, device='cuda:0')
c= tensor(5.3922e+09, device='cuda:0')
c= tensor(5.3925e+09, device='cuda:0')
c= tensor(5.4188e+09, device='cuda:0')
c= tensor(5.4189e+09, device='cuda:0')
c= tensor(5.4189e+09, device='cuda:0')
c= tensor(5.4189e+09, device='cuda:0')
c= tensor(5.4560e+09, device='cuda:0')
c= tensor(5.4856e+09, device='cuda:0')
c= tensor(5.4874e+09, device='cuda:0')
c= tensor(5.4875e+09, device='cuda:0')
c= tensor(5.5076e+09, device='cuda:0')
c= tensor(5.5141e+09, device='cuda:0')
c= tensor(5.5141e+09, device='cuda:0')
c= tensor(5.5144e+09, device='cuda:0')
c= tensor(5.5149e+09, device='cuda:0')
c= tensor(5.5204e+09, device='cuda:0')
c= tensor(5.5230e+09, device='cuda:0')
c= tensor(5.5269e+09, device='cuda:0')
c= tensor(5.5271e+09, device='cuda:0')
c= tensor(5.5302e+09, device='cuda:0')
c= tensor(5.5302e+09, device='cuda:0')
c= tensor(5.5309e+09, device='cuda:0')
c= tensor(5.5311e+09, device='cuda:0')
c= tensor(5.5322e+09, device='cuda:0')
c= tensor(5.5354e+09, device='cuda:0')
c= tensor(5.5587e+09, device='cuda:0')
c= tensor(5.5587e+09, device='cuda:0')
c= tensor(5.5588e+09, device='cuda:0')
c= tensor(5.5589e+09, device='cuda:0')
c= tensor(5.5644e+09, device='cuda:0')
c= tensor(5.5647e+09, device='cuda:0')
c= tensor(5.5648e+09, device='cuda:0')
c= tensor(5.5648e+09, device='cuda:0')
c= tensor(5.5902e+09, device='cuda:0')
c= tensor(5.5902e+09, device='cuda:0')
c= tensor(5.5928e+09, device='cuda:0')
c= tensor(5.5928e+09, device='cuda:0')
c= tensor(5.5929e+09, device='cuda:0')
c= tensor(5.5942e+09, device='cuda:0')
c= tensor(5.5942e+09, device='cuda:0')
c= tensor(5.5942e+09, device='cuda:0')
c= tensor(5.5996e+09, device='cuda:0')
c= tensor(5.6290e+09, device='cuda:0')
c= tensor(5.6396e+09, device='cuda:0')
c= tensor(5.6503e+09, device='cuda:0')
c= tensor(5.6508e+09, device='cuda:0')
c= tensor(5.6509e+09, device='cuda:0')
c= tensor(5.6510e+09, device='cuda:0')
c= tensor(5.6572e+09, device='cuda:0')
c= tensor(5.6576e+09, device='cuda:0')
c= tensor(5.6584e+09, device='cuda:0')
c= tensor(5.6584e+09, device='cuda:0')
c= tensor(5.7703e+09, device='cuda:0')
c= tensor(5.7712e+09, device='cuda:0')
c= tensor(5.7722e+09, device='cuda:0')
c= tensor(5.8094e+09, device='cuda:0')
c= tensor(5.8100e+09, device='cuda:0')
c= tensor(5.8269e+09, device='cuda:0')
c= tensor(5.8503e+09, device='cuda:0')
c= tensor(5.8600e+09, device='cuda:0')
c= tensor(5.8611e+09, device='cuda:0')
c= tensor(5.8613e+09, device='cuda:0')
c= tensor(5.8626e+09, device='cuda:0')
c= tensor(5.8628e+09, device='cuda:0')
c= tensor(5.8665e+09, device='cuda:0')
c= tensor(5.9960e+09, device='cuda:0')
c= tensor(6.0260e+09, device='cuda:0')
c= tensor(6.0337e+09, device='cuda:0')
c= tensor(6.0337e+09, device='cuda:0')
c= tensor(6.0366e+09, device='cuda:0')
c= tensor(6.0372e+09, device='cuda:0')
c= tensor(6.0376e+09, device='cuda:0')
c= tensor(6.0384e+09, device='cuda:0')
c= tensor(6.0728e+09, device='cuda:0')
c= tensor(6.0730e+09, device='cuda:0')
c= tensor(6.1181e+09, device='cuda:0')
c= tensor(6.1219e+09, device='cuda:0')
c= tensor(6.1231e+09, device='cuda:0')
c= tensor(6.1231e+09, device='cuda:0')
c= tensor(6.1355e+09, device='cuda:0')
c= tensor(6.1419e+09, device='cuda:0')
c= tensor(6.1419e+09, device='cuda:0')
c= tensor(6.2674e+09, device='cuda:0')
c= tensor(6.2701e+09, device='cuda:0')
c= tensor(6.2754e+09, device='cuda:0')
c= tensor(6.2760e+09, device='cuda:0')
c= tensor(6.2763e+09, device='cuda:0')
c= tensor(6.2763e+09, device='cuda:0')
c= tensor(6.2763e+09, device='cuda:0')
c= tensor(6.2768e+09, device='cuda:0')
c= tensor(6.3034e+09, device='cuda:0')
c= tensor(8.5699e+09, device='cuda:0')
c= tensor(8.5700e+09, device='cuda:0')
c= tensor(8.5784e+09, device='cuda:0')
c= tensor(8.5784e+09, device='cuda:0')
c= tensor(8.5786e+09, device='cuda:0')
c= tensor(8.5788e+09, device='cuda:0')
c= tensor(8.6200e+09, device='cuda:0')
c= tensor(8.6203e+09, device='cuda:0')
c= tensor(8.9158e+09, device='cuda:0')
c= tensor(8.9158e+09, device='cuda:0')
c= tensor(8.9296e+09, device='cuda:0')
c= tensor(8.9368e+09, device='cuda:0')
c= tensor(8.9400e+09, device='cuda:0')
c= tensor(8.9576e+09, device='cuda:0')
c= tensor(8.9577e+09, device='cuda:0')
c= tensor(8.9578e+09, device='cuda:0')
c= tensor(8.9595e+09, device='cuda:0')
c= tensor(8.9596e+09, device='cuda:0')
c= tensor(8.9600e+09, device='cuda:0')
c= tensor(8.9687e+09, device='cuda:0')
c= tensor(8.9747e+09, device='cuda:0')
c= tensor(8.9756e+09, device='cuda:0')
c= tensor(8.9770e+09, device='cuda:0')
c= tensor(9.0148e+09, device='cuda:0')
c= tensor(9.1106e+09, device='cuda:0')
c= tensor(9.1128e+09, device='cuda:0')
c= tensor(9.1129e+09, device='cuda:0')
c= tensor(9.1445e+09, device='cuda:0')
c= tensor(9.1452e+09, device='cuda:0')
c= tensor(9.1466e+09, device='cuda:0')
c= tensor(9.1546e+09, device='cuda:0')
c= tensor(9.1580e+09, device='cuda:0')
c= tensor(9.1648e+09, device='cuda:0')
c= tensor(9.2301e+09, device='cuda:0')
c= tensor(9.2308e+09, device='cuda:0')
c= tensor(9.2310e+09, device='cuda:0')
c= tensor(9.2310e+09, device='cuda:0')
c= tensor(9.2314e+09, device='cuda:0')
c= tensor(9.2364e+09, device='cuda:0')
c= tensor(9.2435e+09, device='cuda:0')
c= tensor(9.2625e+09, device='cuda:0')
c= tensor(9.2626e+09, device='cuda:0')
c= tensor(9.2629e+09, device='cuda:0')
c= tensor(9.2660e+09, device='cuda:0')
c= tensor(9.2993e+09, device='cuda:0')
c= tensor(9.3004e+09, device='cuda:0')
c= tensor(9.3025e+09, device='cuda:0')
c= tensor(9.5852e+09, device='cuda:0')
c= tensor(9.5861e+09, device='cuda:0')
c= tensor(9.5861e+09, device='cuda:0')
c= tensor(9.5862e+09, device='cuda:0')
c= tensor(9.5978e+09, device='cuda:0')
c= tensor(9.5992e+09, device='cuda:0')
c= tensor(9.6005e+09, device='cuda:0')
c= tensor(9.6005e+09, device='cuda:0')
c= tensor(9.6005e+09, device='cuda:0')
c= tensor(9.6148e+09, device='cuda:0')
c= tensor(9.6186e+09, device='cuda:0')
c= tensor(9.6200e+09, device='cuda:0')
c= tensor(9.6200e+09, device='cuda:0')
c= tensor(9.6200e+09, device='cuda:0')
c= tensor(9.6211e+09, device='cuda:0')
c= tensor(9.6223e+09, device='cuda:0')
c= tensor(9.6321e+09, device='cuda:0')
c= tensor(9.6322e+09, device='cuda:0')
c= tensor(9.6508e+09, device='cuda:0')
c= tensor(9.6511e+09, device='cuda:0')
c= tensor(9.6519e+09, device='cuda:0')
c= tensor(9.6842e+09, device='cuda:0')
c= tensor(9.6844e+09, device='cuda:0')
c= tensor(9.6856e+09, device='cuda:0')
c= tensor(9.6937e+09, device='cuda:0')
c= tensor(9.6943e+09, device='cuda:0')
c= tensor(9.6962e+09, device='cuda:0')
c= tensor(9.7048e+09, device='cuda:0')
c= tensor(9.7048e+09, device='cuda:0')
c= tensor(9.7057e+09, device='cuda:0')
c= tensor(9.7059e+09, device='cuda:0')
c= tensor(9.7100e+09, device='cuda:0')
c= tensor(9.7139e+09, device='cuda:0')
c= tensor(9.7152e+09, device='cuda:0')
c= tensor(9.7186e+09, device='cuda:0')
c= tensor(9.7188e+09, device='cuda:0')
c= tensor(9.7213e+09, device='cuda:0')
c= tensor(9.7223e+09, device='cuda:0')
c= tensor(9.7224e+09, device='cuda:0')
c= tensor(9.7278e+09, device='cuda:0')
c= tensor(9.8009e+09, device='cuda:0')
c= tensor(9.8010e+09, device='cuda:0')
c= tensor(9.8014e+09, device='cuda:0')
c= tensor(9.8015e+09, device='cuda:0')
c= tensor(9.8339e+09, device='cuda:0')
c= tensor(9.8347e+09, device='cuda:0')
c= tensor(9.8348e+09, device='cuda:0')
c= tensor(9.8391e+09, device='cuda:0')
c= tensor(9.8417e+09, device='cuda:0')
c= tensor(9.8418e+09, device='cuda:0')
c= tensor(9.8451e+09, device='cuda:0')
c= tensor(9.8452e+09, device='cuda:0')
time to make c is 8.946790933609009
time for making loss is 8.946966409683228
p0 True
it  0 : 2856865280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5668024320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 20% |
memory (bytes)
5668265984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  3201100800.0
relative error loss 0.32514226
shape of L is 
torch.Size([])
memory (bytes)
5693976576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5693976576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  3167514000.0
relative error loss 0.3217308
shape of L is 
torch.Size([])
memory (bytes)
5697728512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 20% |
memory (bytes)
5697884160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  3119613400.0
relative error loss 0.3168654
shape of L is 
torch.Size([])
memory (bytes)
5700857856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 20% |
memory (bytes)
5701066752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  3086449700.0
relative error loss 0.31349692
shape of L is 
torch.Size([])
memory (bytes)
5704294400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 20% |
memory (bytes)
5704294400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  3055046100.0
relative error loss 0.31030717
shape of L is 
torch.Size([])
memory (bytes)
5707497472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 20% |
memory (bytes)
5707497472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 20% |
error is  3031925200.0
relative error loss 0.30795875
shape of L is 
torch.Size([])
memory (bytes)
5710704640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 20% |
memory (bytes)
5710704640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  3012395500.0
relative error loss 0.30597508
shape of L is 
torch.Size([])
memory (bytes)
5713920000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 20% |
memory (bytes)
5713920000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2996349000.0
relative error loss 0.3043452
shape of L is 
torch.Size([])
memory (bytes)
5717123072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5717123072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2983430700.0
relative error loss 0.30303305
shape of L is 
torch.Size([])
memory (bytes)
5720240128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5720240128
| ID | GPU | MEM |
------------------
|  0 |  3% |  0% |
|  1 | 99% | 20% |
error is  2972407300.0
relative error loss 0.30191338
time to take a step is 277.64052271842957
it  1 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5723537408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5723541504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2972407300.0
relative error loss 0.30191338
shape of L is 
torch.Size([])
memory (bytes)
5726773248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 20% |
memory (bytes)
5726773248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 20% |
error is  2963155500.0
relative error loss 0.30097365
shape of L is 
torch.Size([])
memory (bytes)
5729988608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 20% |
memory (bytes)
5729988608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2954770400.0
relative error loss 0.300122
shape of L is 
torch.Size([])
memory (bytes)
5733199872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5733199872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2945892900.0
relative error loss 0.29922026
shape of L is 
torch.Size([])
memory (bytes)
5736407040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 20% |
memory (bytes)
5736407040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2941093400.0
relative error loss 0.2987328
shape of L is 
torch.Size([])
memory (bytes)
5739622400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5739622400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2934675000.0
relative error loss 0.29808083
shape of L is 
torch.Size([])
memory (bytes)
5742772224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5742772224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2930726000.0
relative error loss 0.29767972
shape of L is 
torch.Size([])
memory (bytes)
5746036736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 20% |
memory (bytes)
5746036736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2925997000.0
relative error loss 0.2971994
shape of L is 
torch.Size([])
memory (bytes)
5749243904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 20% |
memory (bytes)
5749243904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2921778200.0
relative error loss 0.2967709
shape of L is 
torch.Size([])
memory (bytes)
5752438784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5752459264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2916395000.0
relative error loss 0.29622412
time to take a step is 316.83514738082886
it  2 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5755670528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5755670528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2916395000.0
relative error loss 0.29622412
shape of L is 
torch.Size([])
memory (bytes)
5758853120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 20% |
memory (bytes)
5758853120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 20% |
error is  2913808000.0
relative error loss 0.29596132
shape of L is 
torch.Size([])
memory (bytes)
5762088960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 20% |
memory (bytes)
5762088960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 20% |
error is  2909530000.0
relative error loss 0.29552683
shape of L is 
torch.Size([])
memory (bytes)
5765308416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5765308416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2904663600.0
relative error loss 0.29503253
shape of L is 
torch.Size([])
memory (bytes)
5768491008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 20% |
memory (bytes)
5768491008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2900505000.0
relative error loss 0.29461014
shape of L is 
torch.Size([])
memory (bytes)
5771730944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 20% |
memory (bytes)
5771730944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 20% |
error is  2897948700.0
relative error loss 0.29435048
shape of L is 
torch.Size([])
memory (bytes)
5774946304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5774946304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2895364600.0
relative error loss 0.294088
shape of L is 
torch.Size([])
memory (bytes)
5778124800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5778124800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2891006000.0
relative error loss 0.2936453
shape of L is 
torch.Size([])
memory (bytes)
5781368832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5781368832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2891589000.0
relative error loss 0.29370454
shape of L is 
torch.Size([])
memory (bytes)
5784576000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5784576000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2888925200.0
relative error loss 0.29343393
time to take a step is 315.131299495697
it  3 : 3426929664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5787795456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5787795456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2888925200.0
relative error loss 0.29343393
shape of L is 
torch.Size([])
memory (bytes)
5791010816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 20% |
memory (bytes)
5791010816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2887289900.0
relative error loss 0.29326785
shape of L is 
torch.Size([])
memory (bytes)
5794222080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 20% |
memory (bytes)
5794222080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 20% |
error is  2885039600.0
relative error loss 0.2930393
shape of L is 
torch.Size([])
memory (bytes)
5797396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5797441536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2883473000.0
relative error loss 0.29288015
shape of L is 
torch.Size([])
memory (bytes)
5800656896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 20% |
memory (bytes)
5800656896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2881773600.0
relative error loss 0.29270753
shape of L is 
torch.Size([])
memory (bytes)
5803855872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 20% |
memory (bytes)
5803855872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2880052700.0
relative error loss 0.29253274
shape of L is 
torch.Size([])
memory (bytes)
5807042560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5807042560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2878951400.0
relative error loss 0.2924209
shape of L is 
torch.Size([])
memory (bytes)
5810290688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5810290688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2876651500.0
relative error loss 0.29218727
shape of L is 
torch.Size([])
memory (bytes)
5813493760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 20% |
memory (bytes)
5813493760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 20% |
error is  2874784300.0
relative error loss 0.2919976
shape of L is 
torch.Size([])
memory (bytes)
5816647680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 20% |
memory (bytes)
5816700928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2873778700.0
relative error loss 0.29189548
time to take a step is 319.4742350578308
it  4 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5819908096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5819908096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2873778700.0
relative error loss 0.29189548
shape of L is 
torch.Size([])
memory (bytes)
5823123456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5823123456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2872615400.0
relative error loss 0.29177734
shape of L is 
torch.Size([])
memory (bytes)
5826297856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5826297856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2870532600.0
relative error loss 0.29156578
shape of L is 
torch.Size([])
memory (bytes)
5829550080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 20% |
memory (bytes)
5829550080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2869689300.0
relative error loss 0.29148012
shape of L is 
torch.Size([])
memory (bytes)
5832757248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 20% |
memory (bytes)
5832757248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 20% |
error is  2868554200.0
relative error loss 0.29136482
shape of L is 
torch.Size([])
memory (bytes)
5835866112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5835964416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2867876900.0
relative error loss 0.29129604
shape of L is 
torch.Size([])
memory (bytes)
5839179776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5839179776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2866199000.0
relative error loss 0.2911256
shape of L is 
torch.Size([])
memory (bytes)
5842374656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5842391040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2865078300.0
relative error loss 0.29101178
shape of L is 
torch.Size([])
memory (bytes)
5845594112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5845594112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2863656000.0
relative error loss 0.2908673
shape of L is 
torch.Size([])
memory (bytes)
5848809472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5848809472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2862407200.0
relative error loss 0.29074046
time to take a step is 316.3983619213104
it  5 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5852016640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 20% |
memory (bytes)
5852016640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2862407200.0
relative error loss 0.29074046
shape of L is 
torch.Size([])
memory (bytes)
5855211520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5855211520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2861567000.0
relative error loss 0.2906551
shape of L is 
torch.Size([])
memory (bytes)
5858422784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5858422784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2860561400.0
relative error loss 0.29055297
shape of L is 
torch.Size([])
memory (bytes)
5861593088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5861593088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2859351000.0
relative error loss 0.29043004
shape of L is 
torch.Size([])
memory (bytes)
5864833024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5864833024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2858729000.0
relative error loss 0.29036686
shape of L is 
torch.Size([])
memory (bytes)
5868044288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5868044288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2857971700.0
relative error loss 0.29028994
shape of L is 
torch.Size([])
memory (bytes)
5871169536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5871169536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2857643500.0
relative error loss 0.29025662
shape of L is 
torch.Size([])
memory (bytes)
5874458624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5874458624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2856332800.0
relative error loss 0.29012346
shape of L is 
torch.Size([])
memory (bytes)
5877673984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5877673984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2855843300.0
relative error loss 0.29007375
shape of L is 
torch.Size([])
memory (bytes)
5880889344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5880889344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2855306200.0
relative error loss 0.2900192
time to take a step is 317.5595078468323
it  6 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5884104704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5884104704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2855306200.0
relative error loss 0.2900192
shape of L is 
torch.Size([])
memory (bytes)
5887315968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5887315968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2854418400.0
relative error loss 0.28992903
shape of L is 
torch.Size([])
memory (bytes)
5890428928
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5890527232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2853810200.0
relative error loss 0.28986725
shape of L is 
torch.Size([])
memory (bytes)
5893726208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 20% |
memory (bytes)
5893726208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2853014000.0
relative error loss 0.28978637
shape of L is 
torch.Size([])
memory (bytes)
5896929280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 20% |
memory (bytes)
5896929280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2852478500.0
relative error loss 0.28973198
shape of L is 
torch.Size([])
memory (bytes)
5900140544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 20% |
memory (bytes)
5900140544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2851648500.0
relative error loss 0.28964767
shape of L is 
torch.Size([])
memory (bytes)
5903355904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 20% |
memory (bytes)
5903355904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2850963000.0
relative error loss 0.28957805
shape of L is 
torch.Size([])
memory (bytes)
5906571264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 20% |
memory (bytes)
5906571264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2850101200.0
relative error loss 0.28949052
shape of L is 
torch.Size([])
memory (bytes)
5909753856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 20% |
memory (bytes)
5909774336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2849751000.0
relative error loss 0.28945494
shape of L is 
torch.Size([])
memory (bytes)
5912981504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5912981504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2849133600.0
relative error loss 0.28939223
time to take a step is 318.50294399261475
it  7 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5916188672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
5916188672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2849133600.0
relative error loss 0.28939223
shape of L is 
torch.Size([])
memory (bytes)
5919264768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5919391744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2848743000.0
relative error loss 0.28935257
shape of L is 
torch.Size([])
memory (bytes)
5922607104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5922607104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2847933000.0
relative error loss 0.28927028
shape of L is 
torch.Size([])
memory (bytes)
5925814272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5925814272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2847302100.0
relative error loss 0.2892062
shape of L is 
torch.Size([])
memory (bytes)
5928882176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 20% |
memory (bytes)
5929009152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2846893600.0
relative error loss 0.28916472
shape of L is 
torch.Size([])
memory (bytes)
5932212224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5932212224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2846092300.0
relative error loss 0.28908333
shape of L is 
torch.Size([])
memory (bytes)
5935329280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 20% |
memory (bytes)
5935423488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2845479400.0
relative error loss 0.28902107
shape of L is 
torch.Size([])
memory (bytes)
5938630656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
5938630656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2845130200.0
relative error loss 0.2889856
shape of L is 
torch.Size([])
memory (bytes)
5941854208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5941854208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2844797000.0
relative error loss 0.28895175
shape of L is 
torch.Size([])
memory (bytes)
5944999936
| ID | GPU | MEM |
------------------
|  0 |  3% |  0% |
|  1 | 11% | 20% |
memory (bytes)
5944999936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2844308000.0
relative error loss 0.28890207
time to take a step is 316.757390499115
it  8 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5948260352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 20% |
memory (bytes)
5948260352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 20% |
error is  2844308000.0
relative error loss 0.28890207
shape of L is 
torch.Size([])
memory (bytes)
5951459328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 20% |
memory (bytes)
5951459328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 20% |
error is  2843755000.0
relative error loss 0.28884593
shape of L is 
torch.Size([])
memory (bytes)
5954637824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 20% |
memory (bytes)
5954637824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2843275800.0
relative error loss 0.28879726
shape of L is 
torch.Size([])
memory (bytes)
5957881856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 20% |
memory (bytes)
5957881856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2842467300.0
relative error loss 0.28871512
shape of L is 
torch.Size([])
memory (bytes)
5960986624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5961093120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2842830800.0
relative error loss 0.28875205
shape of L is 
torch.Size([])
memory (bytes)
5964304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5964304384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2842263600.0
relative error loss 0.28869444
shape of L is 
torch.Size([])
memory (bytes)
5967503360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 20% |
memory (bytes)
5967503360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2842020400.0
relative error loss 0.28866974
shape of L is 
torch.Size([])
memory (bytes)
5970714624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5970714624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2841647600.0
relative error loss 0.28863186
shape of L is 
torch.Size([])
memory (bytes)
5973925888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 20% |
memory (bytes)
5973925888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 20% |
error is  2841350700.0
relative error loss 0.2886017
shape of L is 
torch.Size([])
memory (bytes)
5977034752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 20% |
memory (bytes)
5977133056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2841005000.0
relative error loss 0.28856662
time to take a step is 317.2729380130768
it  9 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
5980348416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 20% |
memory (bytes)
5980348416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2841005000.0
relative error loss 0.28856662
shape of L is 
torch.Size([])
memory (bytes)
5983506432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5983506432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2840455700.0
relative error loss 0.2885108
shape of L is 
torch.Size([])
memory (bytes)
5986754560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 20% |
memory (bytes)
5986754560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2840154000.0
relative error loss 0.28848016
shape of L is 
torch.Size([])
memory (bytes)
5989961728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 20% |
memory (bytes)
5989961728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2839831000.0
relative error loss 0.28844735
shape of L is 
torch.Size([])
memory (bytes)
5993160704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5993168896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2839334400.0
relative error loss 0.28839692
shape of L is 
torch.Size([])
memory (bytes)
5996380160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 20% |
memory (bytes)
5996380160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2839131600.0
relative error loss 0.28837633
shape of L is 
torch.Size([])
memory (bytes)
5999595520
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 13% | 20% |
memory (bytes)
5999595520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2838883800.0
relative error loss 0.28835115
shape of L is 
torch.Size([])
memory (bytes)
6002761728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 20% |
memory (bytes)
6002794496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2838585300.0
relative error loss 0.28832084
shape of L is 
torch.Size([])
memory (bytes)
6006001664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6006001664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2838407700.0
relative error loss 0.28830278
shape of L is 
torch.Size([])
memory (bytes)
6009135104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 20% |
memory (bytes)
6009204736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 20% |
error is  2838082600.0
relative error loss 0.28826976
time to take a step is 318.0880992412567
it  10 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
6012436480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 20% |
memory (bytes)
6012436480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 20% |
error is  2838082600.0
relative error loss 0.28826976
shape of L is 
torch.Size([])
memory (bytes)
6015655936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
6015655936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2837807600.0
relative error loss 0.28824183
shape of L is 
torch.Size([])
memory (bytes)
6018822144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6018822144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2837564000.0
relative error loss 0.28821707
shape of L is 
torch.Size([])
memory (bytes)
6022070272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 20% |
memory (bytes)
6022070272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 20% |
error is  2837355000.0
relative error loss 0.28819585
shape of L is 
torch.Size([])
memory (bytes)
6025273344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
6025273344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2837158400.0
relative error loss 0.28817588
shape of L is 
torch.Size([])
memory (bytes)
6028484608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6028484608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2836884500.0
relative error loss 0.28814808
shape of L is 
torch.Size([])
memory (bytes)
6031691776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6031691776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2836592600.0
relative error loss 0.28811842
shape of L is 
torch.Size([])
memory (bytes)
6034821120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 20% |
memory (bytes)
6034894848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2836413000.0
relative error loss 0.28810018
shape of L is 
torch.Size([])
memory (bytes)
6038097920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6038097920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2836199000.0
relative error loss 0.28807843
shape of L is 
torch.Size([])
memory (bytes)
6041300992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 20% |
memory (bytes)
6041300992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2835962400.0
relative error loss 0.2880544
time to take a step is 318.5159032344818
it  11 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
6044336128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 20% |
memory (bytes)
6044508160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2835962400.0
relative error loss 0.2880544
shape of L is 
torch.Size([])
memory (bytes)
6047723520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 20% |
memory (bytes)
6047723520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2835718700.0
relative error loss 0.28802964
shape of L is 
torch.Size([])
memory (bytes)
6050897920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 20% |
memory (bytes)
6050897920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2835556400.0
relative error loss 0.28801316
shape of L is 
torch.Size([])
memory (bytes)
6054141952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
6054141952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2835338800.0
relative error loss 0.28799108
shape of L is 
torch.Size([])
memory (bytes)
6057357312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
6057357312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2835118000.0
relative error loss 0.28796867
shape of L is 
torch.Size([])
memory (bytes)
6060531712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 20% |
memory (bytes)
6060531712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2834968000.0
relative error loss 0.2879534
shape of L is 
torch.Size([])
memory (bytes)
6063767552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 20% |
memory (bytes)
6063767552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2834828800.0
relative error loss 0.28793928
shape of L is 
torch.Size([])
memory (bytes)
6066974720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 20% |
memory (bytes)
6066974720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 20% |
error is  2834672000.0
relative error loss 0.28792337
shape of L is 
torch.Size([])
memory (bytes)
6070194176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 20% |
memory (bytes)
6070194176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2834668500.0
relative error loss 0.28792298
shape of L is 
torch.Size([])
memory (bytes)
6073397248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 20% |
memory (bytes)
6073397248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2834567700.0
relative error loss 0.28791276
time to take a step is 317.5776791572571
it  12 : 3426929664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
6076583936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6076583936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2834567700.0
relative error loss 0.28791276
shape of L is 
torch.Size([])
memory (bytes)
6079815680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 20% |
memory (bytes)
6079815680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2834447400.0
relative error loss 0.28790054
shape of L is 
torch.Size([])
memory (bytes)
6083026944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 20% |
memory (bytes)
6083026944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 20% |
error is  2834371600.0
relative error loss 0.28789282
shape of L is 
torch.Size([])
memory (bytes)
6086234112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6086234112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2834270200.0
relative error loss 0.28788254
shape of L is 
torch.Size([])
memory (bytes)
6089433088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 20% |
memory (bytes)
6089433088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 20% |
error is  2834129000.0
relative error loss 0.28786817
shape of L is 
torch.Size([])
memory (bytes)
6092615680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 20% |
memory (bytes)
6092615680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 20% |
error is  2833952800.0
relative error loss 0.2878503
shape of L is 
torch.Size([])
memory (bytes)
6095859712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 20% |
memory (bytes)
6095859712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2833793000.0
relative error loss 0.28783408
shape of L is 
torch.Size([])
memory (bytes)
6099062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6099062784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2833580500.0
relative error loss 0.28781247
shape of L is 
torch.Size([])
memory (bytes)
6102274048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6102274048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2833368000.0
relative error loss 0.2877909
shape of L is 
torch.Size([])
memory (bytes)
6105477120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6105477120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2833207800.0
relative error loss 0.28777462
time to take a step is 318.0027141571045
it  13 : 3427198464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
6108655616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
6108655616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2833207800.0
relative error loss 0.28777462
shape of L is 
torch.Size([])
memory (bytes)
6111895552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
6111895552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2832983000.0
relative error loss 0.2877518
shape of L is 
torch.Size([])
memory (bytes)
6115106816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 20% |
memory (bytes)
6115106816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2832801300.0
relative error loss 0.28773332
shape of L is 
torch.Size([])
memory (bytes)
6118289408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6118318080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2832638500.0
relative error loss 0.2877168
shape of L is 
torch.Size([])
memory (bytes)
6121525248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 20% |
memory (bytes)
6121525248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2832523800.0
relative error loss 0.28770515
shape of L is 
torch.Size([])
memory (bytes)
6124728320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 20% |
memory (bytes)
6124728320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2832426000.0
relative error loss 0.2876952
shape of L is 
torch.Size([])
memory (bytes)
6127931392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 20% |
memory (bytes)
6127931392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 20% |
error is  2832391700.0
relative error loss 0.2876917
shape of L is 
torch.Size([])
memory (bytes)
6131142656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6131142656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2832173600.0
relative error loss 0.28766957
shape of L is 
torch.Size([])
memory (bytes)
6134366208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6134366208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2832121300.0
relative error loss 0.28766426
shape of L is 
torch.Size([])
memory (bytes)
6137569280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 20% |
memory (bytes)
6137569280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 20% |
error is  2832021500.0
relative error loss 0.28765413
time to take a step is 319.3461170196533
it  14 : 3426929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 20% |
shape of L is 
torch.Size([])
memory (bytes)
6140653568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 20% |
memory (bytes)
6140776448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2832021500.0
relative error loss 0.28765413
shape of L is 
torch.Size([])
memory (bytes)
6143987712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 20% |
memory (bytes)
6143987712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2831891000.0
relative error loss 0.28764087
shape of L is 
torch.Size([])
memory (bytes)
6147198976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 20% |
memory (bytes)
6147198976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2831836200.0
relative error loss 0.2876353
shape of L is 
torch.Size([])
memory (bytes)
6150320128
| ID | GPU | MEM |
------------------
|  0 | 27% |  0% |
|  1 | 28% | 20% |
memory (bytes)
6150320128
| ID | GPU | MEM |
------------------
|  0 | 21% |  0% |
|  1 | 91% | 20% |
error is  2831673300.0
relative error loss 0.28761876
shape of L is 
torch.Size([])
memory (bytes)
6153613312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
6153613312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2831601200.0
relative error loss 0.28761142
shape of L is 
torch.Size([])
memory (bytes)
6156828672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 20% |
memory (bytes)
6156828672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2831405600.0
relative error loss 0.28759158
shape of L is 
torch.Size([])
memory (bytes)
6160039936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 20% |
memory (bytes)
6160039936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2831189000.0
relative error loss 0.28756955
shape of L is 
torch.Size([])
memory (bytes)
6163247104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 20% |
memory (bytes)
6163247104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2830988800.0
relative error loss 0.28754923
shape of L is 
torch.Size([])
memory (bytes)
6166376448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
6166376448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 20% |
error is  2830800400.0
relative error loss 0.2875301
shape of L is 
torch.Size([])
memory (bytes)
6169669632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
6169669632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 20% |
error is  2830603300.0
relative error loss 0.28751007
time to take a step is 335.73287200927734
sum tnnu_Z after tensor(11716841., device='cuda:0')
shape of features
(5615,)
shape of features
(5615,)
number of orig particles 22461
number of new particles after remove low mass 22193
tnuZ shape should be parts x labs
torch.Size([22461, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  3200886800.0
relative error without small mass is  0.3251205
nnu_Z shape should be number of particles by maxV
(22461, 702)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
shape of features
(22461,)
Thu Feb 2 13:27:14 EST 2023
