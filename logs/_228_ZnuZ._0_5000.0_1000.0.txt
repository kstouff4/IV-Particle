Wed Feb 1 18:01:15 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 14546404
numbers of Z: 17547
shape of features
(17547,)
shape of features
(17547,)
ZX	Vol	Parts	Cubes	Eps
Z	0.0160956484117209	17547	17.547	0.09716321283972769
X	0.013329253480893982	236	0.236	0.3836759772759559
X	0.014051789958748066	8061	8.061	0.12035041054824358
X	0.014812862512535085	1446	1.446	0.21718186905024767
X	0.013361074252481092	745	0.745	0.2617550293399251
X	0.013700329448241951	33353	33.353	0.0743359168607746
X	0.013657343562338357	8829	8.829	0.11565158059525153
X	0.013630838740987394	23169	23.169	0.08379249010928166
X	0.013756439701512628	14914	14.914	0.09734283042973978
X	0.01361614400400916	2518	2.518	0.1755225029695592
X	0.0135846756515763	7501	7.501	0.12189242092540568
X	0.013631002440753126	2523	2.523	0.17547025815071243
X	0.013559038580012017	50162	50.162	0.06465736735604267
X	0.013670457500147329	3507	3.507	0.1573798381672416
X	0.013848271994697126	83755	83.755	0.05488600413491462
X	0.013595673942339518	9042	9.042	0.11456327299233762
X	0.013812360386601752	11673	11.673	0.10576982973664728
X	0.013803467906697559	24283	24.283	0.08283781276346527
X	0.013708414632922768	27142	27.142	0.07963704349384042
X	0.013796179282802124	69636	69.636	0.0582963912088271
X	0.013775685131786494	58931	58.931	0.06160128888037179
X	0.013210550558797197	1475	1.475	0.20767121274093206
X	0.01403033507059721	113533	113.533	0.04980985869611178
X	0.013719115081295046	5960	5.96	0.1320362799443192
X	0.013665718773174544	4947	4.947	0.14031265946428337
X	0.014455849011060902	27328	27.328	0.08087455762589638
X	0.013863150001166427	36029	36.029	0.07273384913078153
X	0.013783858692560099	30592	30.592	0.07666332427596778
X	0.01362933098115636	7184	7.184	0.12379489568385725
X	0.013797138794169899	20115	20.115	0.08819074243866526
X	0.013879289923229841	710531	710.531	0.026931553503329566
X	0.013665317060423038	2121	2.121	0.1860771367193933
X	0.01388523402987275	206383	206.383	0.040671772000943206
X	0.013651783330804914	4088	4.088	0.1494718810276392
X	0.013630171731319644	2637	2.637	0.17290082517669036
X	0.013726635451958949	1532	1.532	0.20769933763946416
X	0.013714151552817482	27546	27.546	0.0792568486834603
X	0.014187502415294623	32188	32.188	0.07610338694544298
X	0.013456282145041026	884	0.884	0.24783212833721813
X	0.013856582552180129	2943	2.943	0.16760563809295398
X	0.013314441144880382	1055	1.055	0.23282172047753355
X	0.013627362189755533	1793	1.793	0.19661221824717143
X	0.013341057934437399	343	0.343	0.33881714870787805
X	0.012729914706096	504	0.504	0.2934032943202611
X	0.013309762797123282	1054	1.054	0.23286804738195607
X	0.012951463492579576	285	0.285	0.35685462399672
X	0.013648096997081109	435	0.435	0.31539946172300476
X	0.013620579607725973	2035	2.035	0.18845618348386822
X	0.013540422045954294	1683	1.683	0.20037768820038207
X	0.013552097168573234	879	0.879	0.24888910000157272
X	0.013653817212098449	2640	2.64	0.1729351941785228
X	0.013691991695055987	5042	5.042	0.13951511515685378
X	0.013125973820285701	883	0.883	0.24588022963421155
X	0.013555512559978923	1002	1.002	0.23827733863435288
X	0.013139703138290344	702	0.702	0.26551118056966483
X	0.013586892811505309	4024	4.024	0.1500216919203721
X	0.013365852448211517	1375	1.375	0.21341813549779787
X	0.01357106928129576	863	0.863	0.2505346258744098
X	0.01360913990996577	1292	1.292	0.2192078260620672
X	0.013544319350531244	994	0.994	0.2388490950850857
X	0.013152486731430381	802	0.802	0.2540648014483607
X	0.013816532354846209	5806	5.806	0.1335080160570974
X	0.013322254362236993	725	0.725	0.26388410919190164
X	0.014565903616102178	5117	5.117	0.14172310922337306
X	0.013435119906796841	2014	2.014	0.18824669502494557
X	0.01365771670604977	937	0.937	0.24427527062114268
X	0.013441622219055479	809	0.809	0.2551728051346959
X	0.013248350377241895	549	0.549	0.2889769171384229
X	0.013058850127966391	906	0.906	0.24336544483617956
X	0.01345642844353612	920	0.92	0.24455731614916484
X	0.013064572481228325	731	0.731	0.26145238541830645
X	0.013510903952537881	1558	1.558	0.205449766071036
X	0.013239145448138234	1420	1.42	0.21047016249214573
X	0.0130853945995985	610	0.61	0.2778554989620855
X	0.013169608588889445	515	0.515	0.2946152429598894
X	0.013348937697170144	1384	1.384	0.21286464688134166
X	0.013642630336438241	3042	3.042	0.16490973758684443
X	0.013320374187547687	687	0.687	0.2686498253843152
X	0.012038294462346904	227	0.227	0.3757044753175296
X	0.013259946682441032	848	0.848	0.25006254210383777
X	0.013727350312199042	3852	3.852	0.15274493315044294
X	0.01333743638835254	1169	1.169	0.2251226971495722
X	0.013024601478774208	265	0.265	0.3663021968466388
X	0.013741139223633686	2861	2.861	0.1687206710384595
X	0.013161072719200771	494	0.494	0.29866760080091415
X	0.013278686976819182	802	0.802	0.25487481388428324
X	0.013055598100249406	383	0.383	0.3242395585789699
X	0.013492509843450688	1020	1.02	0.23649984327405035
X	0.011578073677646853	244	0.244	0.36203355719385305
X	0.013658694134323893	1578	1.578	0.20532132597843575
X	0.01331396900180073	1641	1.641	0.20093996401089387
X	0.013046375186718457	862	0.862	0.24735889527629715
X	0.012807486235782749	403	0.403	0.31675218342090294
X	0.013246708137306462	223	0.223	0.3901824231887837
X	0.013806349764880885	2509	2.509	0.17654661837506036
X	0.013139102418431041	1032	1.032	0.2335045424910515
X	0.013183719458386456	1594	1.594	0.20223200080552745
X	0.014657961285549817	1996	1.996	0.19437377878409226
X	0.013736827638631986	2450	2.45	0.1776533718903376
X	0.013693943107137686	3273	3.273	0.16113659093164007
X	0.013461272369316303	1287	1.287	0.2186934898147362
X	0.013533326937564203	5726	5.726	0.13320414623932422
X	0.01358416334408007	1681	1.681	0.20067274614627426
X	0.013479362982541208	1447	1.447	0.21041027587860445
X	0.013150680019578991	558	0.558	0.28670680688347344
X	0.012571612071627348	1367	1.367	0.2095113163824073
X	0.013588351921147712	479	0.479	0.3049840244252402
X	0.013542164564870904	2209	2.209	0.18301953898771156
X	0.012818764908664803	158	0.158	0.43290875137108875
X	0.01337099119211745	754	0.754	0.26077387807485825
X	0.013582734999062462	584	0.584	0.28544654681683684
X	0.012988626222295923	645	0.645	0.27206196773234664
X	0.01295298769583813	449	0.449	0.3066958296733583
X	0.012748529684919432	759	0.759	0.2560979534327646
X	0.013570996355149426	1572	1.572	0.20514128143441493
X	0.013429944891727188	1914	1.914	0.19144503119155737
X	0.012204446171983203	311	0.311	0.3398228379016588
X	0.013340782979120206	633	0.633	0.2762225963609587
X	0.013093513382140338	529	0.529	0.29142949794409806
X	0.013649495551228007	3320	3.32	0.16019889121302902
X	0.012767583958950026	324	0.324	0.3402942185950129
X	0.013597025665330822	3457	3.457	0.1578513059383149
X	0.013321840053914642	897	0.897	0.24580493089979952
X	0.013550044418376692	675	0.675	0.2717767590451928
X	0.013572237892721401	1465	1.465	0.2100251383862332
X	0.013752630972766668	572	0.572	0.28862232463426124
X	0.013707412096673704	1273	1.273	0.22082200828172205
X	0.013408076188147888	490	0.49	0.3013400086119117
X	0.013553050878683203	782	0.782	0.2587875854548087
X	0.014731941729061096	8463	8.463	0.12029434649441241
X	0.013408834645814063	994	0.994	0.23805001706084636
X	0.013625991458029821	2302	2.302	0.18089265415740044
X	0.0134380625456972	569	0.569	0.286907211105568
X	0.01342098343481425	2037	2.037	0.1874697083614984
X	0.012688128171678184	305	0.305	0.34649680887602047
X	0.01321587007063613	974	0.974	0.23851311689332735
X	0.013293272393671202	851	0.851	0.24997741993427816
X	0.013119504807394997	348	0.348	0.3353096608273888
X	0.01302590602771016	463	0.463	0.3041414764625849
X	0.013471269689499235	606	0.606	0.2811762506250139
X	0.013679681547575285	1008	1.008	0.23852751578723236
X	0.013474899894481694	1518	1.518	0.20705446106471898
X	0.013588635491627041	333	0.333	0.3442790045064995
X	0.013879288771379202	2755	2.755	0.17142804386982555
X	0.01368466984957337	8565	8.565	0.1169056907514172
X	0.014546666056040821	3796	3.796	0.1564872967635031
X	0.013349134480082502	585	0.585	0.2836388705665889
X	0.013606132330377328	699	0.699	0.2690001393121438
X	0.013247319929104969	962	0.962	0.23969058991676495
X	0.013148652136372124	559	0.559	0.28652101168363375
X	0.013356795768091236	842	0.842	0.2512638816285
X	0.012803389884684007	853	0.853	0.24667510013601157
X	0.014315564598126102	3388	3.388	0.16166711680944848
X	0.013451197501669704	1228	1.228	0.22208583574315802
X	0.013597764397218897	2432	2.432	0.17748758512487153
X	0.013460696141984734	877	0.877	0.24851692758133317
X	0.013852463711766691	6615	6.615	0.12793785277844386
X	0.013461004997567402	769	0.769	0.259647242834078
X	0.013313360635823302	691	0.691	0.2680833754140211
X	0.013483517821322745	1567	1.567	0.20491703925453342
X	0.013136743004737283	223	0.223	0.3890997456720333
X	0.014062508283860351	3207	3.207	0.16367710832820886
X	0.013160889200972097	373	0.373	0.32798863873898937
X	0.01335859207366146	1890	1.89	0.19191089374931042
X	0.01274027674755378	305	0.305	0.34697086399582616
X	0.013208457285915258	1001	1.001	0.23630487042638165
X	0.01335621658553721	743	0.743	0.26195792785888505
X	0.013648383648811898	1029	1.029	0.2367129125080535
X	0.013126854435246508	731	0.731	0.26186719571484274
X	0.013498523237844172	1382	1.382	0.2137598183928244
X	0.011287002225752959	102	0.102	0.48009385399455246
X	0.013131542577927693	538	0.538	0.2900755716889178
X	0.012893609690320766	653	0.653	0.2702840675107844
X	0.013703050147618018	2059	2.059	0.18809920794791377
X	0.013422625584081273	685	0.685	0.2695973598270306
X	0.013772677733341506	2685	2.685	0.17246118472703
X	0.013421225617400834	825	0.825	0.2533841293792144
X	0.013283295865845523	1176	1.176	0.22437071647423154
X	0.013459634521518306	2210	2.21	0.18261943630069755
X	0.013781906966029132	2363	2.363	0.18000387901804482
X	0.013148282816166576	844	0.844	0.2497519360311615
X	0.013614143787281057	2108	2.108	0.18622582583459277
X	0.01320061141477787	684	0.684	0.26823327268514885
X	0.01251207287952955	1232	1.232	0.21655731218161806
X	0.01384121248865361	887	0.887	0.24989074512721002
X	0.013675795681655112	3627	3.627	0.15564494346741772
X	0.013285263336090218	2177	2.177	0.1827415255246085
X	0.013584923372192966	832	0.832	0.25369463840762524
X	0.013008099223199584	1567	1.567	0.2024797588137897
X	0.013758028138352746	3025	3.025	0.1656826140951921
X	0.013798542631907676	8688	8.688	0.11667320708725076
X	0.012854505325297205	414	0.414	0.3143052792969477
X	0.012295605540100143	220	0.22	0.3823335538873234
X	0.01302713780942088	767	0.767	0.2570501615549058
X	0.0134737741214915	327	0.327	0.345393242577805
X	0.012943602991248498	971	0.971	0.23710751445268996
X	0.013444768298787822	1109	1.109	0.2297244702352488
X	0.013483216235798843	513	0.513	0.2973208585018233
X	0.012961097793754484	414	0.414	0.3151716524825451
X	0.013248388714799001	1108	1.108	0.2286692499341681
X	0.01319874824494932	292	0.292	0.35621863179886976
X	0.013646086318290755	4756	4.756	0.1420982587048467
X	0.012536197150041705	492	0.492	0.29426158521086093
X	0.01368904806670898	6192	6.192	0.13027090600902255
X	0.013585326319925	2596	2.596	0.1736154867157115
X	0.013289351699930903	1090	1.09	0.23015783608306858
X	0.0126774948772647	164	0.164	0.42598718255149476
X	0.014449083004441504	1850	1.85	0.1984066324222255
X	0.0135665401933643	1529	1.529	0.20702392593214686
X	0.013111431032494082	1079	1.079	0.22990207094822615
X	0.014718791450145615	3968	3.968	0.15479852901654564
X	0.013479721441931682	3651	3.651	0.15455736140633428
X	0.012560738524196549	543	0.543	0.2849301773998081
X	0.013085091252480016	440	0.44	0.3098189846988767
X	0.01363448325870162	481	0.481	0.3049049925025129
X	0.013122613545339657	338	0.338	0.33861111745829625
X	0.01278210811920264	264	0.264	0.3644736924759243
X	0.01291453020007307	228	0.228	0.38404423932432646
X	0.01300535945375342	321	0.321	0.3434570556818473
X	0.013556436896191721	1562	1.562	0.2055045818073997
X	0.013179217791403994	765	0.765	0.25827125125839895
X	0.013572971978692469	612	0.612	0.28095785384079147
X	0.013606106328568538	778	0.778	0.2595681615798846
X	0.013324140152075942	1334	1.334	0.2153580553185112
X	0.013216048387296243	461	0.461	0.30605553038334293
X	0.014095792350690566	1968	1.968	0.19276201898027376
X	0.0134352957790887	1449	1.449	0.21008398598021574
X	0.01319052172226091	469	0.469	0.30410930608654624
X	0.012952202865696521	284	0.284	0.35727977534813576
X	0.013309860800352593	2209	2.209	0.18196698494146205
X	0.012959248441317094	325	0.325	0.34163746048388155
X	0.013535669365395374	316	0.316	0.349889480005121
X	0.013157022117634849	648	0.648	0.27281036478941934
X	0.01301231882466878	854	0.854	0.24791279500301952
X	0.014712195837947998	2865	2.865	0.17252457132961468
X	0.01296748020071714	802	0.802	0.2528679232252154
X	0.012914247845954676	227	0.227	0.384604551903472
X	0.013600451085032779	1450	1.45	0.2108928083824723
X	0.01363937863281564	2896	2.896	0.167622382569137
X	0.012839231925849452	271	0.271	0.36184556250914307
X	0.013595174519280816	2526	2.526	0.1752469548423519
X	0.013737028707873598	27247	27.247	0.07958991447707507
X	0.013674802003729168	1444	1.444	0.211568635237395
X	0.013626581664529685	2367	2.367	0.17922402307691493
X	0.013047920343024579	256	0.256	0.3707664628759236
X	0.013889799203111366	2555	2.555	0.17583352702584537
X	0.013570932925934514	7894	7.894	0.11979467877433214
X	0.013731141611626724	67713	67.713	0.05875049501663395
X	0.013272008206617314	983	0.983	0.2381191779454812
X	0.013744348205501116	14130	14.13	0.09908182465482965
X	0.013585402042750855	23551	23.551	0.08324427609398326
X	0.013538874827836065	4670	4.67	0.14258983424245963
X	0.01478464886323232	113894	113.894	0.050633357086786034
X	0.013309115763198357	379	0.379	0.32746885217232513
X	0.013604079480940329	2998	2.998	0.16555635039356584
X	0.013737656941561798	39167	39.167	0.07052300125024019
X	0.013833716969809298	102735	102.735	0.051255462132223734
X	0.01356485969651202	1606	1.606	0.20365258251307308
X	0.01398335041865059	11995	11.995	0.10524553083129604
X	0.013652849388404255	17999	17.999	0.09119932853261581
X	0.013803970048140086	57310	57.31	0.06221921583445665
X	0.01357661054506213	12342	12.342	0.10322905207546666
X	0.013442629913245832	14200	14.2	0.09818955670047684
X	0.013753036295970886	13345	13.345	0.10100898420965065
X	0.013829669908474784	5580	5.58	0.1353295190790123
X	0.013140207756743068	2420	2.42	0.17576360648652758
X	0.0139161200462154	62298	62.298	0.060675684909697165
X	0.01325041282932094	929	0.929	0.24251460507316
X	0.013547500895301122	1079	1.079	0.2324230777917391
X	0.013642919665166411	16951	16.951	0.09301878574493794
X	0.013474397534256687	6280	6.28	0.12897827695241929
X	0.01424561230519543	128632	128.632	0.04802241902099382
X	0.013821742189855492	35961	35.961	0.0727071312452833
X	0.013098406210117297	341	0.341	0.3374074688571258
X	0.013647109663373	4565	4.565	0.14405664624877831
X	0.013701218908811177	5680	5.68	0.13411281854729618
X	0.013644799119471848	4195	4.195	0.14816480725208983
X	0.013822717571052467	28430	28.43	0.07863312308185641
X	0.013359036803721582	3913	3.913	0.15057577234764907
X	0.013574062209160779	8685	8.685	0.11605040777509744
X	0.013359012804757762	338	0.338	0.34063234835170614
X	0.013577146757006307	1698	1.698	0.19996636044452692
X	0.013789828192597074	13757	13.757	0.10007947981543901
X	0.01346315878763142	18778	18.778	0.08950195967124341
X	0.013449499907478232	5694	5.694	0.1331771012719565
X	0.013228083333536296	1937	1.937	0.18972407051121387
X	0.014043533995407444	121636	121.636	0.048693548738163867
X	0.013653348633779809	2899	2.899	0.16762173111639905
X	0.013654922056360732	8573	8.573	0.11678457033487238
X	0.013794411098492191	43683	43.683	0.06809735380730143
X	0.013529463985177856	900	0.9	0.24680049542368684
X	0.013794772029382297	18214	18.214	0.09115274084209729
X	0.01383727928638251	93250	93.25	0.05294203393306183
X	0.015110933749053831	80514	80.514	0.05725410534336535
X	0.0135738497817494	4594	4.594	0.1434951918596465
X	0.013628769983917762	3468	3.468	0.1578068524640816
X	0.013218475447519562	1883	1.883	0.19147423786754553
X	0.013744348722741368	1339	1.339	0.21732742260811966
X	0.013624380159612929	11697	11.697	0.10521574979513279
X	0.013675921040602201	4575	4.575	0.14405284131015106
X	0.013661299875256164	9143	9.143	0.11432321650609605
X	0.013884787990678167	28425	28.425	0.0787552642697315
X	0.013703975979213199	14393	14.393	0.0983780978336162
X	0.013643088165784582	2424	2.424	0.17787982358673393
X	0.013642791049488442	4051	4.051	0.14989264670083224
X	0.013703087963174085	38570	38.57	0.07082549234882428
X	0.013689192992711045	5032	5.032	0.13959795982951584
X	0.013613104220473918	1505	1.505	0.2083564214883912
X	0.014792212011160861	13287	13.287	0.10364190713454548
X	0.014182444980017282	163793	163.793	0.04424035365069974
X	0.013512835492497068	2588	2.588	0.17348452415070265
X	0.013415403185648968	31862	31.862	0.07495090803553059
X	0.013652165464069457	1337	1.337	0.2169485301120627
X	0.013553826835583462	9365	9.365	0.11311444015159237
X	0.013664571943543872	6358	6.358	0.12905014672334994
X	0.013507606768004743	45253	45.253	0.06683085405184869
X	0.013773236009993526	19920	19.92	0.08842645578286373
X	0.01344393411505114	1224	1.224	0.2222874715338839
X	0.01373071906263164	46484	46.484	0.06659831832714704
X	0.01382255884189418	42974	42.974	0.06851635054216094
X	0.013669048036607144	1515	1.515	0.20818137802979636
X	0.013732502729367924	18248	18.248	0.09095881782004006
X	0.014576160090083806	75849	75.849	0.05770734450831629
X	0.014411845011493354	126236	126.236	0.04851158369906793
X	0.01364711597276133	39581	39.581	0.07012152835042237
X	0.0137028367473324	2859	2.859	0.16860305624943456
X	0.013608531675346676	2165	2.165	0.18455158917949008
X	0.013651057563586028	2060	2.06	0.18783060579670133
X	0.01365328652093999	20132	20.132	0.08785842781273974
X	0.01367017956739321	1998	1.998	0.1898422672016992
X	0.013492177998611401	676	0.676	0.27125543919137446
X	0.013837670381252642	24571	24.571	0.08258098330827
X	0.013713235073109184	46890	46.89	0.06637734786691632
X	0.013315787866078264	19649	19.649	0.08783659091017958
X	0.01359385439636251	2002	2.002	0.18936200656045912
X	0.01337851067599228	4757	4.757	0.1411534636378328
X	0.013875189093549987	7991	7.991	0.12019302344077236
X	0.013536881387112055	4268	4.268	0.1469258023140077
X	0.013819883911082388	4317	4.317	0.14738075673006573
X	0.01337650702671276	1180	1.18	0.22463990402658693
X	0.013812524854777936	80234	80.234	0.055629464086064236
X	0.013819460039632055	13423	13.423	0.10097499214913461
X	0.013925686662632136	6953	6.953	0.12605153244298006
X	0.0136296982398901	7696	7.696	0.12098746964150413
X	0.013298642737551693	2137	2.137	0.18393636473256655
X	0.013824289919983605	139969	139.969	0.046224298012273575
X	0.013671528342993445	3079	3.079	0.1643623944231724
X	0.013704573001667043	31915	31.915	0.07544379968558891
X	0.01321757792040381	654	0.654	0.27239013630386444
X	0.013606944565111356	968	0.968	0.24133950597238107
X	0.013408756688316094	1384	1.384	0.21318213470407768
X	0.013727081783954001	6556	6.556	0.12793216771705584
X	0.013103709488666206	8451	8.451	0.11574317321614891
X	0.013834073742749163	7984	7.984	0.12010926777020801
X	0.013148089475952198	1823	1.823	0.1932080989850318
X	0.013325793802140978	1448	1.448	0.20955989669054678
X	0.013765019584506452	94704	94.704	0.05257785168607983
X	0.013671455609285468	11594	11.594	0.10564780707926544
X	0.013721641878155378	10585	10.585	0.10903645904782575
X	0.014835986751968433	31149	31.149	0.07809515826508062
X	0.013986702431397302	40986	40.986	0.06988113227724888
X	0.013623851385216728	2794	2.794	0.16957344057434848
X	0.013170368937211928	2180	2.18	0.18212957846366665
X	0.013568666496997247	3211	3.211	0.1616710606435251
X	0.012456240048291599	168	0.168	0.42010633362417893
X	0.014366013723200243	1394	1.394	0.21761617260309718
X	0.013684854221195567	8709	8.709	0.11625829825352886
X	0.01360519775825613	1322	1.322	0.21751596311248822
X	0.013399442957345731	918	0.918	0.24438883118979646
X	0.012995053609452266	898	0.898	0.24368787055891994
X	0.01365047628479454	1431	1.431	0.21208146937513056
X	0.013783704352425498	175451	175.451	0.042828878323118706
X	0.013594366552256132	6426	6.426	0.12837272244885647
X	0.01358801403393522	22093	22.093	0.08504205863668196
X	0.013296147554075603	12402	12.402	0.10234768716843311
X	0.013576559454670591	1350	1.35	0.2158499665926894
X	0.01355653760347865	2165	2.165	0.18431625065712332
X	0.014004508404259621	222382	222.382	0.03978530993003547
X	0.013884324544442569	212080	212.08	0.04030339961334425
X	0.013694371372811999	12091	12.091	0.104238133600731
X	0.013747906241922007	11150	11.15	0.10723105152273221
X	0.013390235627729402	549	0.549	0.2900048704299276
X	0.013412608048200029	25424	25.424	0.08080218742827235
X	0.013690099197542714	23190	23.19	0.08388840726229646
X	0.014600202534883268	63299	63.299	0.06132732338587066
X	0.01340403998241849	2690	2.69	0.17080260033762967
X	0.013727291015503015	37249	37.249	0.07169519895166619
X	0.013773338509933297	85601	85.601	0.05439013386778209
X	0.013342495012216937	11907	11.907	0.10386715397243539
X	0.013624058157163131	2167	2.167	0.18456493433254534
X	0.014661492030419056	6484	6.484	0.13125375015323923
X	0.013578042619838405	9308	9.308	0.11341232775681605
X	0.01344570007368059	1162	1.162	0.22618255057195744
X	0.013734903840245154	49388	49.388	0.06527317347769555
X	0.013544292686586246	10224	10.224	0.10982768508552833
X	0.013261594788298326	350	0.35	0.33587362625082867
X	0.013807211020420464	8946	8.946	0.11556483126240155
X	0.014852614505152963	12439	12.439	0.10608951902537551
X	0.012957660524099109	252	0.252	0.37185647861313165
X	0.013750167519229986	42727	42.727	0.06852807455150742
X	0.013818144127342382	40924	40.924	0.06963440687132742
X	0.013811071654847648	30360	30.36	0.07690865276797275
X	0.013791728256275207	39027	39.027	0.0706997439651294
X	0.013683745279035663	24590	24.59	0.08225244519809953
X	0.013571020202689509	3935	3.935	0.15108543063428354
X	0.013620144672204073	10329	10.329	0.1096582088178967
X	0.013784164529694458	18218	18.218	0.09112270082650083
X	0.013856705449162055	48825	48.825	0.06571622044457652
X	0.013339319967881302	1140	1.14	0.22702635663718412
X	0.013849698066857001	25450	25.45	0.08164272772808756
X	0.014009429448741579	59240	59.24	0.061839855514605586
X	0.013817578810346972	15131	15.131	0.09701854854038057
X	0.013760826843880454	12211	12.211	0.10406334130317661
X	0.013633166224549957	19995	19.995	0.08801535408826205
X	0.013651553833350738	3083	3.083	0.16421122986341993
X	0.012887899023049348	734	0.734	0.2599134106899708
X	0.013185218323630083	11298	11.298	0.10528388892144072
X	0.013843041298738692	56067	56.067	0.06273473463235578
X	0.013786058791917582	29867	29.867	0.07728279875492965
X	0.013723512550134919	181992	181.992	0.04224782238961733
X	0.013704660141121195	65028	65.028	0.059509909661950106
X	0.0135719706376874	34537	34.537	0.07324636895608037
X	0.013719529520495778	6508	6.508	0.12822239701541854
X	0.013824157201506676	36198	36.198	0.07255232973690734
X	0.013526853782067605	493	0.493	0.30161293220279706
X	0.013256379219707624	2937	2.937	0.16526224860040947
X	0.01376066304355736	87549	87.549	0.05396713839037348
X	0.013542355448571011	1629	1.629	0.20257742266878892
X	0.013835790337459965	1195	1.195	0.22622747886815636
X	0.01349819465899938	4774	4.774	0.14140488862182177
X	0.013903531797903225	21633	21.633	0.08629841011364896
X	0.013462335499039163	10847	10.847	0.1074657951593221
X	0.014799255410360162	37863	37.863	0.07311530253153029
X	0.013538149892967002	5775	5.775	0.13284211350444378
X	0.013183618892325565	6806	6.806	0.12465629988269279
X	0.013145174009363677	972	0.972	0.23825024624435465
X	0.013680927691943427	4133	4.133	0.1490333215275547
X	0.013764242417354498	11987	11.987	0.10471621493380961
X	0.013693382018658929	17136	17.136	0.09279696167478914
X	0.013532416078883069	765	0.765	0.2605581342612492
X	0.013791722573808632	44375	44.375	0.06773711810163399
X	0.01329713280257926	451	0.451	0.30893028716345133
X	0.013835293086115601	10182	10.182	0.1107605466014366
X	0.013411921564727402	1144	1.144	0.22717210041041114
X	0.013717020453430422	16343	16.343	0.09432844355098978
X	0.013782178341850767	1424	1.424	0.2131094093337143
X	0.013630055708914617	13091	13.091	0.10135416560439173
X	0.013679267294419506	2672	2.672	0.17234901035832909
X	0.013920359773925221	7729	7.729	0.1216678174561391
X	0.013739154948035319	20975	20.975	0.08684657265970805
X	0.013785636547885203	92143	92.143	0.05308699696610499
X	0.013517331563663379	2196	2.196	0.18326781848364632
X	0.01337670801740213	3595	3.595	0.15495899257523338
X	0.013868679571647774	45728	45.728	0.06718685345179604
X	0.0136208986151615	2609	2.609	0.1734777951451695
X	0.013878695715777361	201948	201.948	0.040960919689450766
X	0.013205188691476317	264	0.264	0.36845141450734864
X	0.013257484075586882	32069	32.069	0.07449467540819109
X	0.013833394084781047	69547	69.547	0.058373640991792146
X	0.013644033712274883	959	0.959	0.24231189027569472
X	0.013419254722023901	56828	56.828	0.06180951582502117
X	0.013631423995325223	8683	8.683	0.1162225701281543
X	0.014035582728472263	116911	116.911	0.04933157988879541
X	0.013678484817609582	2078	2.078	0.18741203789992264
X	0.013423924598027237	16068	16.068	0.09418300877813865
X	0.013084639257650433	419	0.419	0.31490719020992475
X	0.014619473186962516	1928	1.928	0.19646034295296796
X	0.013152708396540902	2557	2.557	0.1726214742047668
X	0.01376323714471056	9651	9.651	0.11255965650744242
X	0.013632873289641774	5525	5.525	0.13512997775696067
X	0.013692121349940875	22455	22.455	0.08479805545921962
X	0.013598030696568678	6070	6.07	0.13084659997229406
X	0.013226710900999122	744	0.744	0.2609914670244358
X	0.013533489813114231	2019	2.019	0.18854911305902666
X	0.014991940354033104	35415	35.415	0.07508563648148463
X	0.01345876743681035	4558	4.558	0.14346423800332195
X	0.013878990217459471	59391	59.391	0.06159504033475252
X	0.014894076682503818	42075	42.075	0.07073969021561305
X	0.01351630523012208	3422	3.422	0.15807359430487372
X	0.013489979556677257	2205	2.205	0.18289460682495767
X	0.013097481535482241	2717	2.717	0.16892712293494114
X	0.014791263807061137	333624	333.624	0.03539288779703254
X	0.01364242746921263	4014	4.014	0.1503504602129663
X	0.013665064907453128	6486	6.486	0.12819711492963698
X	0.013715749871347693	26337	26.337	0.08045463784027886
X	0.013585483840399449	11725	11.725	0.10503178660189402
X	0.013624116216187277	1078	1.078	0.23293237755396984
X	0.013471622243924444	2355	2.355	0.1788448045067462
X	0.014014742721291903	93860	93.86	0.05305197136550871
X	0.0137181922807936	5573	5.573	0.1350214023336863
X	0.013496954409000499	12611	12.611	0.10228895733755057
X	0.013671988111968843	22287	22.287	0.08496890464112904
X	0.014718049126272042	11242	11.242	0.1093961964380131
X	0.01428238634441051	30838	30.838	0.07736980890753141
X	0.013722717146713576	49878	49.878	0.06503947499754446
X	0.013840205872941868	21922	21.922	0.08578686402410199
X	0.01365172203818935	2550	2.55	0.17493729982990475
X	0.013778359318233995	22190	22.19	0.08531264391780141
X	0.013526948307486677	2134	2.134	0.1850696222159644
X	0.01361306064648787	4627	4.627	0.143290949319282
X	0.013616959992542574	7758	7.758	0.12062670420424693
X	0.013679699428210424	2145	2.145	0.18544555136183077
X	0.013832269029649106	23231	23.231	0.08412824724346948
X	0.013631147059507201	3239	3.239	0.16145090548410723
X	0.012693937913498793	414	0.414	0.3129911152414713
X	0.013585854968757171	7342	7.342	0.12276960672627206
X	0.013668022041837088	2208	2.208	0.18361248166865443
X	0.013806903258858602	43141	43.141	0.0684019828599078
X	0.013597341620939861	25263	25.263	0.08134353257524432
X	0.013640561620815814	4594	4.594	0.1437298879444096
X	0.012671595652978561	2058	2.058	0.1832857264886572
X	0.01404873245899467	11505	11.505	0.10688501921619886
X	0.013527718414127105	948	0.948	0.2425523047983823
X	0.013528498071267738	2088	2.088	0.1864259493020036
X	0.013834040771623703	10465	10.465	0.10974968258804926
X	0.013766226286323336	64961	64.961	0.05961937250858907
X	0.013345308413898038	2363	2.363	0.17808265878912924
X	0.012865417705242362	316	0.316	0.3440162331455537
X	0.013498388575630044	1126	1.126	0.22886594686168862
X	0.013929678931223577	97872	97.872	0.05221086411737225
X	0.014916875477638135	121224	121.224	0.04973892767453077
X	0.013215086251555734	6940	6.94	0.12394723523638146
X	0.01363676796359407	2572	2.572	0.17437337049555232
X	0.013689720576023834	13215	13.215	0.10118336974132944
X	0.015363668965299528	53115	53.115	0.06613417167362551
X	0.013198942655106427	929	0.929	0.2422001880501045
X	0.014155336501924493	5163	5.163	0.13996067369927276
X	0.013709954728123872	6460	6.46	0.1285092831164685
X	0.013742764598705215	36344	36.344	0.0723125696431709
X	0.013497038073133254	12931	12.931	0.10143833693749399
X	0.01386709774014286	47931	47.931	0.06613880780936114
X	0.01347078417353565	3794	3.794	0.1525568983647246
X	0.013699844296005211	2538	2.538	0.1754182084222802
X	0.01262369197336704	700	0.7	0.262238297249779
X	0.013595954265410583	8591	8.591	0.11653471686295615
X	0.013780167853814298	6419	6.419	0.12900178232079187
X	0.013563532868795677	18206	18.206	0.09065381707701453
X	0.013702591802952512	17161	17.161	0.0927726672743997
X	0.014070678221803815	115123	115.123	0.04962695679039174
X	0.013374614351599378	4147	4.147	0.1477460042381451
X	0.0131250800706271	1698	1.698	0.19772189244234895
X	0.0136263665732802	5846	5.846	0.13258887499503683
X	0.014009764035706711	251982	251.982	0.03816691877413324
X	0.013692619949291548	11094	11.094	0.1072669990692961
X	0.01281919707927262	1175	1.175	0.22178950207210316
X	0.013420518353415449	568	0.568	0.28695049937000533
X	0.013657378027683704	11058	11.058	0.10729107128014176
X	0.013615411771977752	600	0.6	0.2831130205439339
X	0.013612264646982938	10324	10.324	0.10965475343593921
X	0.013611102795845079	1476	1.476	0.2097018775079525
X	0.013882736541662654	5398	5.398	0.13700846497593602
X	0.013537880802530865	4806	4.806	0.1412284870015136
X	0.013591878243962498	1442	1.442	0.21123768777930835
X	0.013177849608510182	916	0.916	0.2432108941535129
X	0.01387154405427581	67036	67.036	0.05914784204244541
X	0.013829687285140804	135856	135.856	0.04669221882124922
X	0.013809061056716693	49421	49.421	0.06537587887995201
X	0.013434872195370082	6183	6.183	0.12952236455985267
X	0.013871708128859987	9934	9.934	0.11177257653669621
X	0.013653332295691599	4904	4.904	0.1406790411554455
X	0.014048584592919319	3438	3.438	0.15987316258286088
X	0.013970584325244093	36228	36.228	0.07278748897276817
X	0.014934641156409975	16163	16.163	0.09739969711723154
X	0.01372371813667212	34656	34.656	0.07343410141388382
X	0.013743200472215356	4945	4.945	0.14059629033902638
X	0.01436957059933729	151105	151.105	0.04564448574219394
X	0.013706911208998002	6466	6.466	0.12846001441386098
X	0.014236450842931552	18414	18.414	0.09178066456789799
X	0.014750781923497214	64996	64.996	0.06099713975768349
X	0.013845929496226546	22204	22.204	0.08543391317924297
X	0.013758840780909946	37099	37.099	0.07184665399227538
X	0.013665543467412063	46174	46.174	0.06664125028088451
X	0.01410238149326364	17418	17.418	0.09320332176847795
X	0.013816852128342053	6294	6.294	0.12996531299395475
X	0.013504538855415513	6182	6.182	0.12975285403689318
X	0.01366010064693036	16621	16.621	0.09366965411889842
X	0.013678432545209143	2355	2.355	0.17975534262648007
X	0.013823283394902809	42248	42.248	0.06890779490189941
X	0.014934781766608528	327617	327.617	0.03572268965180173
X	0.01439464958378009	49666	49.666	0.06617805074029047
X	0.01378064090053493	36496	36.496	0.07227831997276563
X	0.013692087341793903	10317	10.317	0.10989351804383996
X	0.01362713089004837	7605	7.605	0.12146050141890354
X	0.01456168128597147	16038	16.038	0.09683234233792157
X	0.01359715366300494	6431	6.431	0.12834821467303295
X	0.014215849688240642	42319	42.319	0.06951509842620741
X	0.013891603849842837	95795	95.795	0.052537553999369734
X	0.01333300172777818	1117	1.117	0.22853791141611132
X	0.013635927683483701	34773	34.773	0.07319490356390428
X	0.013793182077770985	35090	35.09	0.0732532729422378
X	0.013754170894060114	8838	8.838	0.1158848882466234
X	0.013694490087155236	809	0.809	0.2567630040303624
X	0.013702660461797711	26828	26.828	0.07993534731163708
X	0.013791382597070063	40766	40.766	0.06967921278990137
X	0.013224757900260538	1151	1.151	0.22565110724950155
X	0.013856623795852334	238047	238.047	0.03875534177296953
X	0.013813847481832127	9427	9.427	0.11358309971500243
X	0.013841278393781159	7617	7.617	0.12202928952546827
X	0.01365782619306006	2295	2.295	0.18121728962611514
X	0.013531320304793586	1250	1.25	0.22121269861814666
X	0.013401090905516411	1204	1.204	0.22327386419331927
X	0.013055675408262973	2207	2.207	0.18085575666217463
X	0.013611854613391502	5088	5.088	0.1388215000973103
X	0.013545318081763937	15629	15.629	0.09534238366867194
X	0.014925460746081904	585664	585.664	0.029427931678295684
X	0.01325164464503175	2569	2.569	0.1727833509236163
X	0.013655407232546553	12419	12.419	0.10321417440281543
X	0.013493861121893866	1769	1.769	0.1968503242075179
X	0.013645676380671753	5326	5.326	0.1368352432064043
X	0.01363510569684294	3354	3.354	0.15959960672803064
X	0.013756124537902495	44784	44.784	0.06747212774843565
X	0.013572352412083556	4879	4.879	0.14063975566248157
X	0.014187806017629262	412592	412.592	0.032518573023885416
X	0.013556941695893002	2013	2.013	0.18884521474357024
X	0.013925003633573777	51136	51.136	0.06481708881405902
X	0.013753045815610335	6476	6.476	0.1285377469605158
X	0.0137657648453598	47491	47.491	0.06618044412735381
X	0.013735049358216404	141502	141.502	0.04595744562817546
X	0.013509363355834683	7199	7.199	0.1233448164687584
X	0.013536268020879504	1584	1.584	0.2044472418112433
X	0.01385141052839225	47284	47.284	0.06641404480366139
X	0.01358411462195532	7590	7.59	0.12141244001887277
X	0.013392685160777573	14519	14.519	0.09734425328735348
X	0.013751664998203463	23398	23.398	0.08376427793380414
X	0.0136329701337049	5715	5.715	0.13361588201494692
X	0.015371721210909945	24692	24.692	0.08538640789045182
X	0.013188668111099517	8102	8.102	0.11763491243823122
X	0.014229774733672627	42161	42.161	0.0696245456217722
X	0.01382555515657407	97581	97.581	0.052132166252950835
X	0.013652198324408338	4801	4.801	0.1416740451288077
X	0.014319427396532778	2490	2.49	0.1791604322495662
X	0.013356015844982567	53182	53.182	0.06309148553510546
X	0.01354005567107706	8398	8.398	0.1172595419118338
X	0.013701388229587675	21646	21.646	0.085860940115748
X	0.013467002072714044	5406	5.406	0.1355600213403973
X	0.013831571996671985	25439	25.439	0.08161885578889111
X	0.013662768629942187	11482	11.482	0.10596775260542558
X	0.013407521154851602	21402	21.402	0.08556537217952634
X	0.013741659971998688	13133	13.133	0.10152159163424117
X	0.013672909669942293	8489	8.489	0.11721993341569181
X	0.01356812357257227	1665	1.665	0.20123422746770517
X	0.014548109048936433	10381	10.381	0.11190662179258269
X	0.01369980272766716	8954	8.954	0.1152300475653323
X	0.013706727397434988	31632	31.632	0.07567208631705913
X	0.013831098319795612	37791	37.791	0.07153019815832135
X	0.013696626666771041	4087	4.087	0.14964756692517683
X	0.013727455591092999	11347	11.347	0.10655397707622923
X	0.013649639577901562	24922	24.922	0.0818174797011618
X	0.01385393413281634	63311	63.311	0.060260303557348904
X	0.013862363097401856	21676	21.676	0.08615610216935382
X	0.013724014777503854	27650	27.65	0.07917632631062141
X	0.01358674492330043	21194	21.194	0.08622519079382863
X	0.013850606632893912	10136	10.136	0.11096875948407205
X	0.013594656669966633	497	0.497	0.3013033440535081
X	0.013488101562138394	3850	3.85	0.1518786399014244
X	0.013778672292543447	30134	30.134	0.07704010435680299
X	0.013636804796588462	13619	13.619	0.10004355934891625
X	0.01375842835433781	18128	18.128	0.09121641048380778
X	0.013561380115440832	709	0.709	0.26743559974218173
X	0.013454744332846883	2642	2.642	0.1720471748179922
X	0.013708978070096632	47062	47.062	0.06628952403180088
X	0.01353779963800058	13875	13.875	0.09918325650165535
X	0.013637603022175504	7413	7.413	0.1225315746785305
X	0.013527902387317053	751	0.751	0.26213811649372254
X	0.01351418930907381	906	0.906	0.24616176671808404
X	0.013569296953520629	10142	10.142	0.11019060820341561
X	0.013560250385799102	13511	13.511	0.10012135954428189
X	0.013702591606498046	32078	32.078	0.07531216695118026
X	0.013535097377674875	1048	1.048	0.23462090230190433
X	0.013824593513166799	41172	41.172	0.06950512074619154
X	0.013567420233401252	2088	2.088	0.18660456369478293
X	0.013570464798897931	4562	4.562	0.14381796765964072
X	0.013942575815480059	145451	145.451	0.04576590988304506
X	0.013623681008906022	5788	5.788	0.1330215430322435
X	0.0136818254881948	18928	18.928	0.08974558781998514
X	0.013926279848094218	58728	58.728	0.06189610239877917
X	0.013801134800572377	23475	23.475	0.08377280659557584
X	0.01521583914163212	32074	32.074	0.07799155770822182
X	0.013867946878081582	63813	63.813	0.060122126867519964
X	0.013156967677402445	1004	1.004	0.2357621219362014
X	0.01395066198003525	12458	12.458	0.10384418044622207
X	0.013741002657451288	7485	7.485	0.1224453244439171
X	0.013805944291425112	71381	71.381	0.05783106136638624
X	0.013705349856220605	61866	61.866	0.0605079858040833
X	0.01362204550095619	11066	11.066	0.10717262964048778
X	0.013698638043693947	15296	15.296	0.09639026447173737
X	0.013566425555303218	2558	2.558	0.17439002179582475
X	0.013425802196485067	1723	1.723	0.19825234011791892
X	0.013647882276744872	5164	5.164	0.1382588726538718
X	0.013323221771557021	4342	4.342	0.14531367266546263
X	0.013877644166047266	73096	73.096	0.05747434694062001
X	0.0138059885782741	85834	85.834	0.05438377944951654
X	0.01334636999193771	1675	1.675	0.19973282729213798
X	0.013681176602963143	6350	6.35	0.12915659129188223
X	0.013849085724772189	4556	4.556	0.1448591044724724
X	0.015008028128069888	129097	129.097	0.0488055444190412
X	0.013572084213914152	3631	3.631	0.1551934659155291
X	0.013819454275686382	4167	4.167	0.14912683394150245
X	0.013775778111412423	21536	21.536	0.08616225014425258
X	0.013941666300283104	33643	33.643	0.07455440741645192
X	0.01320888737352689	3524	3.524	0.1553376673195134
X	0.01392780817025994	14971	14.971	0.09762116557459383
X	0.013597667923200454	1132	1.132	0.22901960853115957
time for making epsilon is 0.800734281539917
epsilons are
[0.3836759772759559, 0.12035041054824358, 0.21718186905024767, 0.2617550293399251, 0.0743359168607746, 0.11565158059525153, 0.08379249010928166, 0.09734283042973978, 0.1755225029695592, 0.12189242092540568, 0.17547025815071243, 0.06465736735604267, 0.1573798381672416, 0.05488600413491462, 0.11456327299233762, 0.10576982973664728, 0.08283781276346527, 0.07963704349384042, 0.0582963912088271, 0.06160128888037179, 0.20767121274093206, 0.04980985869611178, 0.1320362799443192, 0.14031265946428337, 0.08087455762589638, 0.07273384913078153, 0.07666332427596778, 0.12379489568385725, 0.08819074243866526, 0.026931553503329566, 0.1860771367193933, 0.040671772000943206, 0.1494718810276392, 0.17290082517669036, 0.20769933763946416, 0.0792568486834603, 0.07610338694544298, 0.24783212833721813, 0.16760563809295398, 0.23282172047753355, 0.19661221824717143, 0.33881714870787805, 0.2934032943202611, 0.23286804738195607, 0.35685462399672, 0.31539946172300476, 0.18845618348386822, 0.20037768820038207, 0.24888910000157272, 0.1729351941785228, 0.13951511515685378, 0.24588022963421155, 0.23827733863435288, 0.26551118056966483, 0.1500216919203721, 0.21341813549779787, 0.2505346258744098, 0.2192078260620672, 0.2388490950850857, 0.2540648014483607, 0.1335080160570974, 0.26388410919190164, 0.14172310922337306, 0.18824669502494557, 0.24427527062114268, 0.2551728051346959, 0.2889769171384229, 0.24336544483617956, 0.24455731614916484, 0.26145238541830645, 0.205449766071036, 0.21047016249214573, 0.2778554989620855, 0.2946152429598894, 0.21286464688134166, 0.16490973758684443, 0.2686498253843152, 0.3757044753175296, 0.25006254210383777, 0.15274493315044294, 0.2251226971495722, 0.3663021968466388, 0.1687206710384595, 0.29866760080091415, 0.25487481388428324, 0.3242395585789699, 0.23649984327405035, 0.36203355719385305, 0.20532132597843575, 0.20093996401089387, 0.24735889527629715, 0.31675218342090294, 0.3901824231887837, 0.17654661837506036, 0.2335045424910515, 0.20223200080552745, 0.19437377878409226, 0.1776533718903376, 0.16113659093164007, 0.2186934898147362, 0.13320414623932422, 0.20067274614627426, 0.21041027587860445, 0.28670680688347344, 0.2095113163824073, 0.3049840244252402, 0.18301953898771156, 0.43290875137108875, 0.26077387807485825, 0.28544654681683684, 0.27206196773234664, 0.3066958296733583, 0.2560979534327646, 0.20514128143441493, 0.19144503119155737, 0.3398228379016588, 0.2762225963609587, 0.29142949794409806, 0.16019889121302902, 0.3402942185950129, 0.1578513059383149, 0.24580493089979952, 0.2717767590451928, 0.2100251383862332, 0.28862232463426124, 0.22082200828172205, 0.3013400086119117, 0.2587875854548087, 0.12029434649441241, 0.23805001706084636, 0.18089265415740044, 0.286907211105568, 0.1874697083614984, 0.34649680887602047, 0.23851311689332735, 0.24997741993427816, 0.3353096608273888, 0.3041414764625849, 0.2811762506250139, 0.23852751578723236, 0.20705446106471898, 0.3442790045064995, 0.17142804386982555, 0.1169056907514172, 0.1564872967635031, 0.2836388705665889, 0.2690001393121438, 0.23969058991676495, 0.28652101168363375, 0.2512638816285, 0.24667510013601157, 0.16166711680944848, 0.22208583574315802, 0.17748758512487153, 0.24851692758133317, 0.12793785277844386, 0.259647242834078, 0.2680833754140211, 0.20491703925453342, 0.3890997456720333, 0.16367710832820886, 0.32798863873898937, 0.19191089374931042, 0.34697086399582616, 0.23630487042638165, 0.26195792785888505, 0.2367129125080535, 0.26186719571484274, 0.2137598183928244, 0.48009385399455246, 0.2900755716889178, 0.2702840675107844, 0.18809920794791377, 0.2695973598270306, 0.17246118472703, 0.2533841293792144, 0.22437071647423154, 0.18261943630069755, 0.18000387901804482, 0.2497519360311615, 0.18622582583459277, 0.26823327268514885, 0.21655731218161806, 0.24989074512721002, 0.15564494346741772, 0.1827415255246085, 0.25369463840762524, 0.2024797588137897, 0.1656826140951921, 0.11667320708725076, 0.3143052792969477, 0.3823335538873234, 0.2570501615549058, 0.345393242577805, 0.23710751445268996, 0.2297244702352488, 0.2973208585018233, 0.3151716524825451, 0.2286692499341681, 0.35621863179886976, 0.1420982587048467, 0.29426158521086093, 0.13027090600902255, 0.1736154867157115, 0.23015783608306858, 0.42598718255149476, 0.1984066324222255, 0.20702392593214686, 0.22990207094822615, 0.15479852901654564, 0.15455736140633428, 0.2849301773998081, 0.3098189846988767, 0.3049049925025129, 0.33861111745829625, 0.3644736924759243, 0.38404423932432646, 0.3434570556818473, 0.2055045818073997, 0.25827125125839895, 0.28095785384079147, 0.2595681615798846, 0.2153580553185112, 0.30605553038334293, 0.19276201898027376, 0.21008398598021574, 0.30410930608654624, 0.35727977534813576, 0.18196698494146205, 0.34163746048388155, 0.349889480005121, 0.27281036478941934, 0.24791279500301952, 0.17252457132961468, 0.2528679232252154, 0.384604551903472, 0.2108928083824723, 0.167622382569137, 0.36184556250914307, 0.1752469548423519, 0.07958991447707507, 0.211568635237395, 0.17922402307691493, 0.3707664628759236, 0.17583352702584537, 0.11979467877433214, 0.05875049501663395, 0.2381191779454812, 0.09908182465482965, 0.08324427609398326, 0.14258983424245963, 0.050633357086786034, 0.32746885217232513, 0.16555635039356584, 0.07052300125024019, 0.051255462132223734, 0.20365258251307308, 0.10524553083129604, 0.09119932853261581, 0.06221921583445665, 0.10322905207546666, 0.09818955670047684, 0.10100898420965065, 0.1353295190790123, 0.17576360648652758, 0.060675684909697165, 0.24251460507316, 0.2324230777917391, 0.09301878574493794, 0.12897827695241929, 0.04802241902099382, 0.0727071312452833, 0.3374074688571258, 0.14405664624877831, 0.13411281854729618, 0.14816480725208983, 0.07863312308185641, 0.15057577234764907, 0.11605040777509744, 0.34063234835170614, 0.19996636044452692, 0.10007947981543901, 0.08950195967124341, 0.1331771012719565, 0.18972407051121387, 0.048693548738163867, 0.16762173111639905, 0.11678457033487238, 0.06809735380730143, 0.24680049542368684, 0.09115274084209729, 0.05294203393306183, 0.05725410534336535, 0.1434951918596465, 0.1578068524640816, 0.19147423786754553, 0.21732742260811966, 0.10521574979513279, 0.14405284131015106, 0.11432321650609605, 0.0787552642697315, 0.0983780978336162, 0.17787982358673393, 0.14989264670083224, 0.07082549234882428, 0.13959795982951584, 0.2083564214883912, 0.10364190713454548, 0.04424035365069974, 0.17348452415070265, 0.07495090803553059, 0.2169485301120627, 0.11311444015159237, 0.12905014672334994, 0.06683085405184869, 0.08842645578286373, 0.2222874715338839, 0.06659831832714704, 0.06851635054216094, 0.20818137802979636, 0.09095881782004006, 0.05770734450831629, 0.04851158369906793, 0.07012152835042237, 0.16860305624943456, 0.18455158917949008, 0.18783060579670133, 0.08785842781273974, 0.1898422672016992, 0.27125543919137446, 0.08258098330827, 0.06637734786691632, 0.08783659091017958, 0.18936200656045912, 0.1411534636378328, 0.12019302344077236, 0.1469258023140077, 0.14738075673006573, 0.22463990402658693, 0.055629464086064236, 0.10097499214913461, 0.12605153244298006, 0.12098746964150413, 0.18393636473256655, 0.046224298012273575, 0.1643623944231724, 0.07544379968558891, 0.27239013630386444, 0.24133950597238107, 0.21318213470407768, 0.12793216771705584, 0.11574317321614891, 0.12010926777020801, 0.1932080989850318, 0.20955989669054678, 0.05257785168607983, 0.10564780707926544, 0.10903645904782575, 0.07809515826508062, 0.06988113227724888, 0.16957344057434848, 0.18212957846366665, 0.1616710606435251, 0.42010633362417893, 0.21761617260309718, 0.11625829825352886, 0.21751596311248822, 0.24438883118979646, 0.24368787055891994, 0.21208146937513056, 0.042828878323118706, 0.12837272244885647, 0.08504205863668196, 0.10234768716843311, 0.2158499665926894, 0.18431625065712332, 0.03978530993003547, 0.04030339961334425, 0.104238133600731, 0.10723105152273221, 0.2900048704299276, 0.08080218742827235, 0.08388840726229646, 0.06132732338587066, 0.17080260033762967, 0.07169519895166619, 0.05439013386778209, 0.10386715397243539, 0.18456493433254534, 0.13125375015323923, 0.11341232775681605, 0.22618255057195744, 0.06527317347769555, 0.10982768508552833, 0.33587362625082867, 0.11556483126240155, 0.10608951902537551, 0.37185647861313165, 0.06852807455150742, 0.06963440687132742, 0.07690865276797275, 0.0706997439651294, 0.08225244519809953, 0.15108543063428354, 0.1096582088178967, 0.09112270082650083, 0.06571622044457652, 0.22702635663718412, 0.08164272772808756, 0.061839855514605586, 0.09701854854038057, 0.10406334130317661, 0.08801535408826205, 0.16421122986341993, 0.2599134106899708, 0.10528388892144072, 0.06273473463235578, 0.07728279875492965, 0.04224782238961733, 0.059509909661950106, 0.07324636895608037, 0.12822239701541854, 0.07255232973690734, 0.30161293220279706, 0.16526224860040947, 0.05396713839037348, 0.20257742266878892, 0.22622747886815636, 0.14140488862182177, 0.08629841011364896, 0.1074657951593221, 0.07311530253153029, 0.13284211350444378, 0.12465629988269279, 0.23825024624435465, 0.1490333215275547, 0.10471621493380961, 0.09279696167478914, 0.2605581342612492, 0.06773711810163399, 0.30893028716345133, 0.1107605466014366, 0.22717210041041114, 0.09432844355098978, 0.2131094093337143, 0.10135416560439173, 0.17234901035832909, 0.1216678174561391, 0.08684657265970805, 0.05308699696610499, 0.18326781848364632, 0.15495899257523338, 0.06718685345179604, 0.1734777951451695, 0.040960919689450766, 0.36845141450734864, 0.07449467540819109, 0.058373640991792146, 0.24231189027569472, 0.06180951582502117, 0.1162225701281543, 0.04933157988879541, 0.18741203789992264, 0.09418300877813865, 0.31490719020992475, 0.19646034295296796, 0.1726214742047668, 0.11255965650744242, 0.13512997775696067, 0.08479805545921962, 0.13084659997229406, 0.2609914670244358, 0.18854911305902666, 0.07508563648148463, 0.14346423800332195, 0.06159504033475252, 0.07073969021561305, 0.15807359430487372, 0.18289460682495767, 0.16892712293494114, 0.03539288779703254, 0.1503504602129663, 0.12819711492963698, 0.08045463784027886, 0.10503178660189402, 0.23293237755396984, 0.1788448045067462, 0.05305197136550871, 0.1350214023336863, 0.10228895733755057, 0.08496890464112904, 0.1093961964380131, 0.07736980890753141, 0.06503947499754446, 0.08578686402410199, 0.17493729982990475, 0.08531264391780141, 0.1850696222159644, 0.143290949319282, 0.12062670420424693, 0.18544555136183077, 0.08412824724346948, 0.16145090548410723, 0.3129911152414713, 0.12276960672627206, 0.18361248166865443, 0.0684019828599078, 0.08134353257524432, 0.1437298879444096, 0.1832857264886572, 0.10688501921619886, 0.2425523047983823, 0.1864259493020036, 0.10974968258804926, 0.05961937250858907, 0.17808265878912924, 0.3440162331455537, 0.22886594686168862, 0.05221086411737225, 0.04973892767453077, 0.12394723523638146, 0.17437337049555232, 0.10118336974132944, 0.06613417167362551, 0.2422001880501045, 0.13996067369927276, 0.1285092831164685, 0.0723125696431709, 0.10143833693749399, 0.06613880780936114, 0.1525568983647246, 0.1754182084222802, 0.262238297249779, 0.11653471686295615, 0.12900178232079187, 0.09065381707701453, 0.0927726672743997, 0.04962695679039174, 0.1477460042381451, 0.19772189244234895, 0.13258887499503683, 0.03816691877413324, 0.1072669990692961, 0.22178950207210316, 0.28695049937000533, 0.10729107128014176, 0.2831130205439339, 0.10965475343593921, 0.2097018775079525, 0.13700846497593602, 0.1412284870015136, 0.21123768777930835, 0.2432108941535129, 0.05914784204244541, 0.04669221882124922, 0.06537587887995201, 0.12952236455985267, 0.11177257653669621, 0.1406790411554455, 0.15987316258286088, 0.07278748897276817, 0.09739969711723154, 0.07343410141388382, 0.14059629033902638, 0.04564448574219394, 0.12846001441386098, 0.09178066456789799, 0.06099713975768349, 0.08543391317924297, 0.07184665399227538, 0.06664125028088451, 0.09320332176847795, 0.12996531299395475, 0.12975285403689318, 0.09366965411889842, 0.17975534262648007, 0.06890779490189941, 0.03572268965180173, 0.06617805074029047, 0.07227831997276563, 0.10989351804383996, 0.12146050141890354, 0.09683234233792157, 0.12834821467303295, 0.06951509842620741, 0.052537553999369734, 0.22853791141611132, 0.07319490356390428, 0.0732532729422378, 0.1158848882466234, 0.2567630040303624, 0.07993534731163708, 0.06967921278990137, 0.22565110724950155, 0.03875534177296953, 0.11358309971500243, 0.12202928952546827, 0.18121728962611514, 0.22121269861814666, 0.22327386419331927, 0.18085575666217463, 0.1388215000973103, 0.09534238366867194, 0.029427931678295684, 0.1727833509236163, 0.10321417440281543, 0.1968503242075179, 0.1368352432064043, 0.15959960672803064, 0.06747212774843565, 0.14063975566248157, 0.032518573023885416, 0.18884521474357024, 0.06481708881405902, 0.1285377469605158, 0.06618044412735381, 0.04595744562817546, 0.1233448164687584, 0.2044472418112433, 0.06641404480366139, 0.12141244001887277, 0.09734425328735348, 0.08376427793380414, 0.13361588201494692, 0.08538640789045182, 0.11763491243823122, 0.0696245456217722, 0.052132166252950835, 0.1416740451288077, 0.1791604322495662, 0.06309148553510546, 0.1172595419118338, 0.085860940115748, 0.1355600213403973, 0.08161885578889111, 0.10596775260542558, 0.08556537217952634, 0.10152159163424117, 0.11721993341569181, 0.20123422746770517, 0.11190662179258269, 0.1152300475653323, 0.07567208631705913, 0.07153019815832135, 0.14964756692517683, 0.10655397707622923, 0.0818174797011618, 0.060260303557348904, 0.08615610216935382, 0.07917632631062141, 0.08622519079382863, 0.11096875948407205, 0.3013033440535081, 0.1518786399014244, 0.07704010435680299, 0.10004355934891625, 0.09121641048380778, 0.26743559974218173, 0.1720471748179922, 0.06628952403180088, 0.09918325650165535, 0.1225315746785305, 0.26213811649372254, 0.24616176671808404, 0.11019060820341561, 0.10012135954428189, 0.07531216695118026, 0.23462090230190433, 0.06950512074619154, 0.18660456369478293, 0.14381796765964072, 0.04576590988304506, 0.1330215430322435, 0.08974558781998514, 0.06189610239877917, 0.08377280659557584, 0.07799155770822182, 0.060122126867519964, 0.2357621219362014, 0.10384418044622207, 0.1224453244439171, 0.05783106136638624, 0.0605079858040833, 0.10717262964048778, 0.09639026447173737, 0.17439002179582475, 0.19825234011791892, 0.1382588726538718, 0.14531367266546263, 0.05747434694062001, 0.05438377944951654, 0.19973282729213798, 0.12915659129188223, 0.1448591044724724, 0.0488055444190412, 0.1551934659155291, 0.14912683394150245, 0.08616225014425258, 0.07455440741645192, 0.1553376673195134, 0.09762116557459383, 0.22901960853115957]
0.09716321283972769
Making ranges
torch.Size([25823, 2])
We keep 5.28e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([761, 2])
We keep 8.02e+03/5.57e+04 = 14% of the original kernel matrix.

torch.Size([5971, 2])
We keep 2.77e+05/4.14e+06 =  6% of the original kernel matrix.

torch.Size([14096, 2])
We keep 2.34e+06/6.50e+07 =  3% of the original kernel matrix.

torch.Size([19429, 2])
We keep 3.25e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([3174, 2])
We keep 1.52e+05/2.09e+06 =  7% of the original kernel matrix.

torch.Size([9988, 2])
We keep 9.24e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([2012, 2])
We keep 4.75e+04/5.55e+05 =  8% of the original kernel matrix.

torch.Size([8454, 2])
We keep 5.74e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([53576, 2])
We keep 2.77e+07/1.11e+09 =  2% of the original kernel matrix.

torch.Size([37922, 2])
We keep 9.94e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([15340, 2])
We keep 2.46e+06/7.80e+07 =  3% of the original kernel matrix.

torch.Size([20471, 2])
We keep 3.45e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([37365, 2])
We keep 1.09e+07/5.37e+08 =  2% of the original kernel matrix.

torch.Size([32153, 2])
We keep 7.41e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([23776, 2])
We keep 4.60e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([24332, 2])
We keep 4.76e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([5513, 2])
We keep 4.22e+05/6.34e+06 =  6% of the original kernel matrix.

torch.Size([12460, 2])
We keep 1.33e+06/4.42e+07 =  3% of the original kernel matrix.

torch.Size([7612, 2])
We keep 5.29e+06/5.63e+07 =  9% of the original kernel matrix.

torch.Size([13923, 2])
We keep 2.91e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([5233, 2])
We keep 3.85e+05/6.37e+06 =  6% of the original kernel matrix.

torch.Size([12157, 2])
We keep 1.34e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([72300, 2])
We keep 5.98e+07/2.52e+09 =  2% of the original kernel matrix.

torch.Size([43048, 2])
We keep 1.39e+07/8.80e+08 =  1% of the original kernel matrix.

torch.Size([7564, 2])
We keep 5.60e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([14270, 2])
We keep 1.70e+06/6.15e+07 =  2% of the original kernel matrix.

torch.Size([131773, 2])
We keep 1.06e+08/7.01e+09 =  1% of the original kernel matrix.

torch.Size([60434, 2])
We keep 2.19e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([15970, 2])
We keep 2.77e+06/8.18e+07 =  3% of the original kernel matrix.

torch.Size([20600, 2])
We keep 3.37e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([19617, 2])
We keep 3.63e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([23263, 2])
We keep 4.24e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([37746, 2])
We keep 1.10e+07/5.90e+08 =  1% of the original kernel matrix.

torch.Size([32340, 2])
We keep 7.55e+06/4.26e+08 =  1% of the original kernel matrix.

torch.Size([39076, 2])
We keep 1.91e+07/7.37e+08 =  2% of the original kernel matrix.

torch.Size([32218, 2])
We keep 8.47e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([109064, 2])
We keep 7.64e+07/4.85e+09 =  1% of the original kernel matrix.

torch.Size([54529, 2])
We keep 1.86e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([93581, 2])
We keep 6.43e+07/3.47e+09 =  1% of the original kernel matrix.

torch.Size([50021, 2])
We keep 1.61e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([3141, 2])
We keep 1.66e+05/2.18e+06 =  7% of the original kernel matrix.

torch.Size([9658, 2])
We keep 9.26e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([178502, 2])
We keep 1.57e+08/1.29e+10 =  1% of the original kernel matrix.

torch.Size([71406, 2])
We keep 2.75e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([11649, 2])
We keep 1.31e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([17586, 2])
We keep 2.51e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([10003, 2])
We keep 9.01e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([16322, 2])
We keep 2.16e+06/8.68e+07 =  2% of the original kernel matrix.

torch.Size([25278, 2])
We keep 3.49e+07/7.47e+08 =  4% of the original kernel matrix.

torch.Size([23639, 2])
We keep 8.69e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([58266, 2])
We keep 2.05e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([39557, 2])
We keep 1.04e+07/6.32e+08 =  1% of the original kernel matrix.

torch.Size([41621, 2])
We keep 2.62e+07/9.36e+08 =  2% of the original kernel matrix.

torch.Size([32730, 2])
We keep 9.39e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([11763, 2])
We keep 3.86e+06/5.16e+07 =  7% of the original kernel matrix.

torch.Size([17614, 2])
We keep 2.87e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([27079, 2])
We keep 1.37e+07/4.05e+08 =  3% of the original kernel matrix.

torch.Size([26825, 2])
We keep 6.63e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([1287701, 2])
We keep 3.69e+09/5.05e+11 =  0% of the original kernel matrix.

torch.Size([200162, 2])
We keep 1.47e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([4294, 2])
We keep 3.13e+05/4.50e+06 =  6% of the original kernel matrix.

torch.Size([11057, 2])
We keep 1.19e+06/3.72e+07 =  3% of the original kernel matrix.

torch.Size([283948, 2])
We keep 9.23e+08/4.26e+10 =  2% of the original kernel matrix.

torch.Size([89816, 2])
We keep 4.85e+07/3.62e+09 =  1% of the original kernel matrix.

torch.Size([7877, 2])
We keep 1.01e+06/1.67e+07 =  6% of the original kernel matrix.

torch.Size([14455, 2])
We keep 1.92e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([6206, 2])
We keep 3.33e+05/6.95e+06 =  4% of the original kernel matrix.

torch.Size([13143, 2])
We keep 1.37e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([3395, 2])
We keep 1.72e+05/2.35e+06 =  7% of the original kernel matrix.

torch.Size([10189, 2])
We keep 9.36e+05/2.69e+07 =  3% of the original kernel matrix.

torch.Size([36918, 2])
We keep 2.91e+07/7.59e+08 =  3% of the original kernel matrix.

torch.Size([31266, 2])
We keep 8.46e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([51000, 2])
We keep 1.95e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([37259, 2])
We keep 9.67e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([2183, 2])
We keep 6.66e+04/7.81e+05 =  8% of the original kernel matrix.

torch.Size([8542, 2])
We keep 6.60e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([6502, 2])
We keep 4.28e+05/8.66e+06 =  4% of the original kernel matrix.

torch.Size([13412, 2])
We keep 1.51e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([2572, 2])
We keep 8.93e+04/1.11e+06 =  8% of the original kernel matrix.

torch.Size([9127, 2])
We keep 7.33e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([4188, 2])
We keep 1.86e+05/3.21e+06 =  5% of the original kernel matrix.

torch.Size([11146, 2])
We keep 1.04e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([610, 2])
We keep 2.42e+04/1.18e+05 = 20% of the original kernel matrix.

torch.Size([4879, 2])
We keep 3.52e+05/6.02e+06 =  5% of the original kernel matrix.

torch.Size([723, 2])
We keep 8.26e+04/2.54e+05 = 32% of the original kernel matrix.

torch.Size([5034, 2])
We keep 4.13e+05/8.84e+06 =  4% of the original kernel matrix.

torch.Size([2604, 2])
We keep 9.00e+04/1.11e+06 =  8% of the original kernel matrix.

torch.Size([9135, 2])
We keep 7.26e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([919, 2])
We keep 1.09e+04/8.12e+04 = 13% of the original kernel matrix.

torch.Size([6296, 2])
We keep 3.04e+05/5.00e+06 =  6% of the original kernel matrix.

torch.Size([1209, 2])
We keep 2.13e+04/1.89e+05 = 11% of the original kernel matrix.

torch.Size([7103, 2])
We keep 4.18e+05/7.63e+06 =  5% of the original kernel matrix.

torch.Size([4589, 2])
We keep 2.53e+05/4.14e+06 =  6% of the original kernel matrix.

torch.Size([11551, 2])
We keep 1.14e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([3807, 2])
We keep 1.96e+05/2.83e+06 =  6% of the original kernel matrix.

torch.Size([10654, 2])
We keep 1.02e+06/2.95e+07 =  3% of the original kernel matrix.

torch.Size([2208, 2])
We keep 6.31e+04/7.73e+05 =  8% of the original kernel matrix.

torch.Size([8577, 2])
We keep 6.47e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([5190, 2])
We keep 6.70e+05/6.97e+06 =  9% of the original kernel matrix.

torch.Size([12038, 2])
We keep 1.39e+06/4.63e+07 =  3% of the original kernel matrix.

torch.Size([9697, 2])
We keep 1.12e+06/2.54e+07 =  4% of the original kernel matrix.

torch.Size([15949, 2])
We keep 2.23e+06/8.85e+07 =  2% of the original kernel matrix.

torch.Size([2244, 2])
We keep 7.48e+04/7.80e+05 =  9% of the original kernel matrix.

torch.Size([8738, 2])
We keep 6.48e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([2274, 2])
We keep 8.13e+04/1.00e+06 =  8% of the original kernel matrix.

torch.Size([8639, 2])
We keep 6.98e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([1930, 2])
We keep 4.43e+04/4.93e+05 =  8% of the original kernel matrix.

torch.Size([8156, 2])
We keep 5.46e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([8218, 2])
We keep 6.93e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([14719, 2])
We keep 1.89e+06/7.06e+07 =  2% of the original kernel matrix.

torch.Size([2765, 2])
We keep 1.98e+05/1.89e+06 = 10% of the original kernel matrix.

torch.Size([9261, 2])
We keep 8.71e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([2204, 2])
We keep 5.95e+04/7.45e+05 =  7% of the original kernel matrix.

torch.Size([8750, 2])
We keep 6.33e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([3140, 2])
We keep 1.19e+05/1.67e+06 =  7% of the original kernel matrix.

torch.Size([9925, 2])
We keep 8.41e+05/2.27e+07 =  3% of the original kernel matrix.

torch.Size([2231, 2])
We keep 7.80e+04/9.88e+05 =  7% of the original kernel matrix.

torch.Size([8597, 2])
We keep 7.00e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([1916, 2])
We keep 5.79e+04/6.43e+05 =  9% of the original kernel matrix.

torch.Size([8058, 2])
We keep 6.05e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([11224, 2])
We keep 1.28e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([17385, 2])
We keep 2.44e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([1879, 2])
We keep 4.81e+04/5.26e+05 =  9% of the original kernel matrix.

torch.Size([8150, 2])
We keep 5.70e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([9773, 2])
We keep 1.14e+06/2.62e+07 =  4% of the original kernel matrix.

torch.Size([16110, 2])
We keep 2.28e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([4711, 2])
We keep 3.13e+05/4.06e+06 =  7% of the original kernel matrix.

torch.Size([11719, 2])
We keep 1.13e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([2216, 2])
We keep 7.96e+04/8.78e+05 =  9% of the original kernel matrix.

torch.Size([8622, 2])
We keep 6.74e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([2112, 2])
We keep 5.67e+04/6.54e+05 =  8% of the original kernel matrix.

torch.Size([8473, 2])
We keep 6.14e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([1345, 2])
We keep 3.08e+04/3.01e+05 = 10% of the original kernel matrix.

torch.Size([7188, 2])
We keep 4.81e+05/9.63e+06 =  4% of the original kernel matrix.

torch.Size([2452, 2])
We keep 6.70e+04/8.21e+05 =  8% of the original kernel matrix.

torch.Size([8974, 2])
We keep 6.56e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([2467, 2])
We keep 7.38e+04/8.46e+05 =  8% of the original kernel matrix.

torch.Size([9126, 2])
We keep 6.59e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([1846, 2])
We keep 4.71e+04/5.34e+05 =  8% of the original kernel matrix.

torch.Size([7957, 2])
We keep 5.69e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([3092, 2])
We keep 2.59e+05/2.43e+06 = 10% of the original kernel matrix.

torch.Size([9631, 2])
We keep 9.60e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([3417, 2])
We keep 1.32e+05/2.02e+06 =  6% of the original kernel matrix.

torch.Size([10203, 2])
We keep 8.90e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([1643, 2])
We keep 3.57e+04/3.72e+05 =  9% of the original kernel matrix.

torch.Size([7787, 2])
We keep 5.10e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([1293, 2])
We keep 3.45e+04/2.65e+05 = 13% of the original kernel matrix.

torch.Size([6970, 2])
We keep 4.53e+05/9.04e+06 =  5% of the original kernel matrix.

torch.Size([3220, 2])
We keep 1.33e+05/1.92e+06 =  6% of the original kernel matrix.

torch.Size([9856, 2])
We keep 8.73e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([6529, 2])
We keep 4.69e+05/9.25e+06 =  5% of the original kernel matrix.

torch.Size([13307, 2])
We keep 1.52e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([1837, 2])
We keep 4.77e+04/4.72e+05 = 10% of the original kernel matrix.

torch.Size([8017, 2])
We keep 5.28e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([680, 2])
We keep 8.06e+03/5.15e+04 = 15% of the original kernel matrix.

torch.Size([5521, 2])
We keep 2.63e+05/3.98e+06 =  6% of the original kernel matrix.

torch.Size([2080, 2])
We keep 6.30e+04/7.19e+05 =  8% of the original kernel matrix.

torch.Size([8452, 2])
We keep 6.25e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([8117, 2])
We keep 6.76e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([14652, 2])
We keep 1.83e+06/6.76e+07 =  2% of the original kernel matrix.

torch.Size([2680, 2])
We keep 9.71e+04/1.37e+06 =  7% of the original kernel matrix.

torch.Size([9181, 2])
We keep 7.90e+05/2.05e+07 =  3% of the original kernel matrix.

torch.Size([708, 2])
We keep 1.02e+04/7.02e+04 = 14% of the original kernel matrix.

torch.Size([5650, 2])
We keep 2.93e+05/4.65e+06 =  6% of the original kernel matrix.

torch.Size([6134, 2])
We keep 4.29e+05/8.19e+06 =  5% of the original kernel matrix.

torch.Size([13003, 2])
We keep 1.45e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([1375, 2])
We keep 2.70e+04/2.44e+05 = 11% of the original kernel matrix.

torch.Size([7234, 2])
We keep 4.37e+05/8.67e+06 =  5% of the original kernel matrix.

torch.Size([2005, 2])
We keep 5.21e+04/6.43e+05 =  8% of the original kernel matrix.

torch.Size([8368, 2])
We keep 6.11e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([1224, 2])
We keep 1.62e+04/1.47e+05 = 11% of the original kernel matrix.

torch.Size([7086, 2])
We keep 3.68e+05/6.72e+06 =  5% of the original kernel matrix.

torch.Size([2265, 2])
We keep 9.07e+04/1.04e+06 =  8% of the original kernel matrix.

torch.Size([8597, 2])
We keep 7.14e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([707, 2])
We keep 7.81e+03/5.95e+04 = 13% of the original kernel matrix.

torch.Size([5691, 2])
We keep 2.69e+05/4.28e+06 =  6% of the original kernel matrix.

torch.Size([3697, 2])
We keep 1.62e+05/2.49e+06 =  6% of the original kernel matrix.

torch.Size([10563, 2])
We keep 9.69e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([3724, 2])
We keep 1.85e+05/2.69e+06 =  6% of the original kernel matrix.

torch.Size([10419, 2])
We keep 9.94e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([2208, 2])
We keep 6.03e+04/7.43e+05 =  8% of the original kernel matrix.

torch.Size([8616, 2])
We keep 6.37e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([1058, 2])
We keep 2.00e+04/1.62e+05 = 12% of the original kernel matrix.

torch.Size([6506, 2])
We keep 3.86e+05/7.07e+06 =  5% of the original kernel matrix.

torch.Size([734, 2])
We keep 7.79e+03/4.97e+04 = 15% of the original kernel matrix.

torch.Size([5934, 2])
We keep 2.69e+05/3.91e+06 =  6% of the original kernel matrix.

torch.Size([5407, 2])
We keep 3.55e+05/6.30e+06 =  5% of the original kernel matrix.

torch.Size([12276, 2])
We keep 1.33e+06/4.40e+07 =  3% of the original kernel matrix.

torch.Size([2661, 2])
We keep 7.69e+04/1.07e+06 =  7% of the original kernel matrix.

torch.Size([9364, 2])
We keep 7.09e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([3671, 2])
We keep 1.82e+05/2.54e+06 =  7% of the original kernel matrix.

torch.Size([10539, 2])
We keep 9.71e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([4328, 2])
We keep 2.41e+05/3.98e+06 =  6% of the original kernel matrix.

torch.Size([11272, 2])
We keep 1.17e+06/3.50e+07 =  3% of the original kernel matrix.

torch.Size([4453, 2])
We keep 6.09e+05/6.00e+06 = 10% of the original kernel matrix.

torch.Size([11183, 2])
We keep 1.31e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([6955, 2])
We keep 5.38e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([13763, 2])
We keep 1.61e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([3273, 2])
We keep 1.13e+05/1.66e+06 =  6% of the original kernel matrix.

torch.Size([10176, 2])
We keep 8.34e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([11043, 2])
We keep 1.25e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([17027, 2])
We keep 2.44e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([3883, 2])
We keep 1.97e+05/2.83e+06 =  6% of the original kernel matrix.

torch.Size([10706, 2])
We keep 1.01e+06/2.95e+07 =  3% of the original kernel matrix.

torch.Size([3500, 2])
We keep 1.36e+05/2.09e+06 =  6% of the original kernel matrix.

torch.Size([10348, 2])
We keep 9.11e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([1468, 2])
We keep 3.06e+04/3.11e+05 =  9% of the original kernel matrix.

torch.Size([7454, 2])
We keep 4.82e+05/9.79e+06 =  4% of the original kernel matrix.

torch.Size([3028, 2])
We keep 2.05e+05/1.87e+06 = 10% of the original kernel matrix.

torch.Size([9430, 2])
We keep 8.36e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([1271, 2])
We keep 3.04e+04/2.29e+05 = 13% of the original kernel matrix.

torch.Size([7041, 2])
We keep 4.41e+05/8.41e+06 =  5% of the original kernel matrix.

torch.Size([4991, 2])
We keep 2.63e+05/4.88e+06 =  5% of the original kernel matrix.

torch.Size([11935, 2])
We keep 1.22e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([495, 2])
We keep 4.56e+03/2.50e+04 = 18% of the original kernel matrix.

torch.Size([4983, 2])
We keep 2.04e+05/2.77e+06 =  7% of the original kernel matrix.

torch.Size([1626, 2])
We keep 5.21e+04/5.69e+05 =  9% of the original kernel matrix.

torch.Size([7516, 2])
We keep 5.85e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([1629, 2])
We keep 3.60e+04/3.41e+05 = 10% of the original kernel matrix.

torch.Size([7846, 2])
We keep 4.94e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([1615, 2])
We keep 3.88e+04/4.16e+05 =  9% of the original kernel matrix.

torch.Size([7592, 2])
We keep 5.14e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([1074, 2])
We keep 2.20e+04/2.02e+05 = 10% of the original kernel matrix.

torch.Size([6339, 2])
We keep 4.04e+05/7.88e+06 =  5% of the original kernel matrix.

torch.Size([1643, 2])
We keep 5.51e+04/5.76e+05 =  9% of the original kernel matrix.

torch.Size([7556, 2])
We keep 5.71e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([3668, 2])
We keep 1.53e+05/2.47e+06 =  6% of the original kernel matrix.

torch.Size([10488, 2])
We keep 9.43e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([4257, 2])
We keep 2.24e+05/3.66e+06 =  6% of the original kernel matrix.

torch.Size([11134, 2])
We keep 1.09e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([844, 2])
We keep 1.32e+04/9.67e+04 = 13% of the original kernel matrix.

torch.Size([5992, 2])
We keep 3.19e+05/5.46e+06 =  5% of the original kernel matrix.

torch.Size([1461, 2])
We keep 3.86e+04/4.01e+05 =  9% of the original kernel matrix.

torch.Size([7238, 2])
We keep 5.19e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([1358, 2])
We keep 3.08e+04/2.80e+05 = 11% of the original kernel matrix.

torch.Size([7136, 2])
We keep 4.64e+05/9.28e+06 =  5% of the original kernel matrix.

torch.Size([6831, 2])
We keep 5.54e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([13678, 2])
We keep 1.64e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([982, 2])
We keep 1.37e+04/1.05e+05 = 13% of the original kernel matrix.

torch.Size([6415, 2])
We keep 3.41e+05/5.69e+06 =  6% of the original kernel matrix.

torch.Size([7617, 2])
We keep 5.44e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([14261, 2])
We keep 1.68e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([2312, 2])
We keep 7.28e+04/8.05e+05 =  9% of the original kernel matrix.

torch.Size([8770, 2])
We keep 6.43e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([1689, 2])
We keep 4.56e+04/4.56e+05 = 10% of the original kernel matrix.

torch.Size([7816, 2])
We keep 5.57e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([3446, 2])
We keep 1.48e+05/2.15e+06 =  6% of the original kernel matrix.

torch.Size([10289, 2])
We keep 9.08e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([1355, 2])
We keep 3.15e+04/3.27e+05 =  9% of the original kernel matrix.

torch.Size([7162, 2])
We keep 4.98e+05/1.00e+07 =  4% of the original kernel matrix.

torch.Size([3090, 2])
We keep 1.15e+05/1.62e+06 =  7% of the original kernel matrix.

torch.Size([9893, 2])
We keep 8.41e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([1524, 2])
We keep 2.43e+04/2.40e+05 = 10% of the original kernel matrix.

torch.Size([7752, 2])
We keep 4.40e+05/8.60e+06 =  5% of the original kernel matrix.

torch.Size([2089, 2])
We keep 4.84e+04/6.12e+05 =  7% of the original kernel matrix.

torch.Size([8583, 2])
We keep 5.89e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([14872, 2])
We keep 2.10e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([20078, 2])
We keep 3.32e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([2332, 2])
We keep 1.02e+05/9.88e+05 = 10% of the original kernel matrix.

torch.Size([8763, 2])
We keep 7.06e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([5209, 2])
We keep 3.00e+05/5.30e+06 =  5% of the original kernel matrix.

torch.Size([12187, 2])
We keep 1.25e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([1447, 2])
We keep 4.39e+04/3.24e+05 = 13% of the original kernel matrix.

torch.Size([7277, 2])
We keep 4.85e+05/9.98e+06 =  4% of the original kernel matrix.

torch.Size([4672, 2])
We keep 2.47e+05/4.15e+06 =  5% of the original kernel matrix.

torch.Size([11555, 2])
We keep 1.14e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([896, 2])
We keep 1.32e+04/9.30e+04 = 14% of the original kernel matrix.

torch.Size([6065, 2])
We keep 3.21e+05/5.35e+06 =  6% of the original kernel matrix.

torch.Size([2396, 2])
We keep 7.25e+04/9.49e+05 =  7% of the original kernel matrix.

torch.Size([8973, 2])
We keep 6.87e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([2083, 2])
We keep 6.02e+04/7.24e+05 =  8% of the original kernel matrix.

torch.Size([8380, 2])
We keep 6.34e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([987, 2])
We keep 1.39e+04/1.21e+05 = 11% of the original kernel matrix.

torch.Size([6482, 2])
We keep 3.52e+05/6.11e+06 =  5% of the original kernel matrix.

torch.Size([1267, 2])
We keep 2.56e+04/2.14e+05 = 11% of the original kernel matrix.

torch.Size([7031, 2])
We keep 4.24e+05/8.12e+06 =  5% of the original kernel matrix.

torch.Size([1638, 2])
We keep 4.17e+04/3.67e+05 = 11% of the original kernel matrix.

torch.Size([7750, 2])
We keep 5.08e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([2187, 2])
We keep 8.29e+04/1.02e+06 =  8% of the original kernel matrix.

torch.Size([8484, 2])
We keep 7.14e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([3461, 2])
We keep 1.47e+05/2.30e+06 =  6% of the original kernel matrix.

torch.Size([10207, 2])
We keep 9.37e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([859, 2])
We keep 1.47e+04/1.11e+05 = 13% of the original kernel matrix.

torch.Size([6064, 2])
We keep 3.44e+05/5.84e+06 =  5% of the original kernel matrix.

torch.Size([5957, 2])
We keep 4.06e+05/7.59e+06 =  5% of the original kernel matrix.

torch.Size([12839, 2])
We keep 1.46e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([14988, 2])
We keep 2.25e+06/7.34e+07 =  3% of the original kernel matrix.

torch.Size([20058, 2])
We keep 3.33e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([7707, 2])
We keep 6.84e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([14455, 2])
We keep 1.83e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([1638, 2])
We keep 3.20e+04/3.42e+05 =  9% of the original kernel matrix.

torch.Size([7801, 2])
We keep 4.97e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([1785, 2])
We keep 4.00e+04/4.89e+05 =  8% of the original kernel matrix.

torch.Size([8186, 2])
We keep 5.58e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([2008, 2])
We keep 1.08e+05/9.25e+05 = 11% of the original kernel matrix.

torch.Size([8080, 2])
We keep 6.54e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([1428, 2])
We keep 3.39e+04/3.12e+05 = 10% of the original kernel matrix.

torch.Size([7222, 2])
We keep 4.86e+05/9.81e+06 =  4% of the original kernel matrix.

torch.Size([1945, 2])
We keep 7.05e+04/7.09e+05 =  9% of the original kernel matrix.

torch.Size([8087, 2])
We keep 6.36e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([2022, 2])
We keep 6.36e+04/7.28e+05 =  8% of the original kernel matrix.

torch.Size([8187, 2])
We keep 6.28e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([6937, 2])
We keep 5.64e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([13749, 2])
We keep 1.68e+06/5.94e+07 =  2% of the original kernel matrix.

torch.Size([3105, 2])
We keep 9.92e+04/1.51e+06 =  6% of the original kernel matrix.

torch.Size([10007, 2])
We keep 7.96e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([4171, 2])
We keep 6.04e+05/5.91e+06 = 10% of the original kernel matrix.

torch.Size([10680, 2])
We keep 1.31e+06/4.27e+07 =  3% of the original kernel matrix.

torch.Size([1964, 2])
We keep 6.31e+04/7.69e+05 =  8% of the original kernel matrix.

torch.Size([8153, 2])
We keep 6.53e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([12312, 2])
We keep 1.58e+06/4.38e+07 =  3% of the original kernel matrix.

torch.Size([18121, 2])
We keep 2.72e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([2025, 2])
We keep 4.98e+04/5.91e+05 =  8% of the original kernel matrix.

torch.Size([8426, 2])
We keep 5.90e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([1791, 2])
We keep 4.26e+04/4.77e+05 =  8% of the original kernel matrix.

torch.Size([8112, 2])
We keep 5.54e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([3343, 2])
We keep 1.71e+05/2.46e+06 =  6% of the original kernel matrix.

torch.Size([10021, 2])
We keep 9.65e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([650, 2])
We keep 1.03e+04/4.97e+04 = 20% of the original kernel matrix.

torch.Size([5547, 2])
We keep 2.58e+05/3.91e+06 =  6% of the original kernel matrix.

torch.Size([6575, 2])
We keep 6.17e+05/1.03e+07 =  5% of the original kernel matrix.

torch.Size([13532, 2])
We keep 1.60e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([1070, 2])
We keep 1.72e+04/1.39e+05 = 12% of the original kernel matrix.

torch.Size([6591, 2])
We keep 3.68e+05/6.55e+06 =  5% of the original kernel matrix.

torch.Size([4354, 2])
We keep 2.18e+05/3.57e+06 =  6% of the original kernel matrix.

torch.Size([11267, 2])
We keep 1.09e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([810, 2])
We keep 1.25e+04/9.30e+04 = 13% of the original kernel matrix.

torch.Size([5963, 2])
We keep 3.24e+05/5.35e+06 =  6% of the original kernel matrix.

torch.Size([2263, 2])
We keep 1.04e+05/1.00e+06 = 10% of the original kernel matrix.

torch.Size([8496, 2])
We keep 6.89e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([1939, 2])
We keep 4.63e+04/5.52e+05 =  8% of the original kernel matrix.

torch.Size([8238, 2])
We keep 5.76e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([2356, 2])
We keep 7.56e+04/1.06e+06 =  7% of the original kernel matrix.

torch.Size([8911, 2])
We keep 6.80e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([1761, 2])
We keep 4.65e+04/5.34e+05 =  8% of the original kernel matrix.

torch.Size([7856, 2])
We keep 5.65e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([3235, 2])
We keep 1.75e+05/1.91e+06 =  9% of the original kernel matrix.

torch.Size([10059, 2])
We keep 8.88e+05/2.42e+07 =  3% of the original kernel matrix.

torch.Size([382, 2])
We keep 2.25e+03/1.04e+04 = 21% of the original kernel matrix.

torch.Size([4662, 2])
We keep 1.60e+05/1.79e+06 =  8% of the original kernel matrix.

torch.Size([1295, 2])
We keep 3.05e+04/2.89e+05 = 10% of the original kernel matrix.

torch.Size([6996, 2])
We keep 4.73e+05/9.44e+06 =  5% of the original kernel matrix.

torch.Size([1520, 2])
We keep 4.74e+04/4.26e+05 = 11% of the original kernel matrix.

torch.Size([7280, 2])
We keep 5.22e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([4910, 2])
We keep 2.39e+05/4.24e+06 =  5% of the original kernel matrix.

torch.Size([11948, 2])
We keep 1.16e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([1795, 2])
We keep 4.17e+04/4.69e+05 =  8% of the original kernel matrix.

torch.Size([8084, 2])
We keep 5.35e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([5876, 2])
We keep 3.79e+05/7.21e+06 =  5% of the original kernel matrix.

torch.Size([12790, 2])
We keep 1.37e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([2043, 2])
We keep 5.78e+04/6.81e+05 =  8% of the original kernel matrix.

torch.Size([8413, 2])
We keep 6.08e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([2736, 2])
We keep 1.09e+05/1.38e+06 =  7% of the original kernel matrix.

torch.Size([9335, 2])
We keep 7.96e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([4709, 2])
We keep 3.17e+05/4.88e+06 =  6% of the original kernel matrix.

torch.Size([11579, 2])
We keep 1.23e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([4733, 2])
We keep 4.51e+05/5.58e+06 =  8% of the original kernel matrix.

torch.Size([11588, 2])
We keep 1.29e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([2089, 2])
We keep 9.13e+04/7.12e+05 = 12% of the original kernel matrix.

torch.Size([8394, 2])
We keep 6.11e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([4639, 2])
We keep 2.60e+05/4.44e+06 =  5% of the original kernel matrix.

torch.Size([11533, 2])
We keep 1.16e+06/3.70e+07 =  3% of the original kernel matrix.

torch.Size([1829, 2])
We keep 4.12e+04/4.68e+05 =  8% of the original kernel matrix.

torch.Size([8018, 2])
We keep 5.42e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([2961, 2])
We keep 1.43e+05/1.52e+06 =  9% of the original kernel matrix.

torch.Size([9489, 2])
We keep 7.88e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([2265, 2])
We keep 7.50e+04/7.87e+05 =  9% of the original kernel matrix.

torch.Size([8828, 2])
We keep 6.38e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([7693, 2])
We keep 5.79e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([14326, 2])
We keep 1.74e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([4989, 2])
We keep 2.82e+05/4.74e+06 =  5% of the original kernel matrix.

torch.Size([11842, 2])
We keep 1.21e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([2135, 2])
We keep 5.52e+04/6.92e+05 =  7% of the original kernel matrix.

torch.Size([8589, 2])
We keep 6.24e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([3150, 2])
We keep 2.48e+05/2.46e+06 = 10% of the original kernel matrix.

torch.Size([9553, 2])
We keep 9.58e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([6530, 2])
We keep 4.56e+05/9.15e+06 =  4% of the original kernel matrix.

torch.Size([13360, 2])
We keep 1.53e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([14458, 2])
We keep 2.77e+06/7.55e+07 =  3% of the original kernel matrix.

torch.Size([19655, 2])
We keep 3.40e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([1115, 2])
We keep 1.98e+04/1.71e+05 = 11% of the original kernel matrix.

torch.Size([6782, 2])
We keep 3.99e+05/7.26e+06 =  5% of the original kernel matrix.

torch.Size([605, 2])
We keep 9.02e+03/4.84e+04 = 18% of the original kernel matrix.

torch.Size([5249, 2])
We keep 2.71e+05/3.86e+06 =  7% of the original kernel matrix.

torch.Size([2060, 2])
We keep 5.62e+04/5.88e+05 =  9% of the original kernel matrix.

torch.Size([8433, 2])
We keep 5.87e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([1039, 2])
We keep 1.38e+04/1.07e+05 = 12% of the original kernel matrix.

torch.Size([6703, 2])
We keep 3.39e+05/5.74e+06 =  5% of the original kernel matrix.

torch.Size([1979, 2])
We keep 1.04e+05/9.43e+05 = 11% of the original kernel matrix.

torch.Size([7960, 2])
We keep 6.88e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([2602, 2])
We keep 1.00e+05/1.23e+06 =  8% of the original kernel matrix.

torch.Size([9135, 2])
We keep 7.68e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([1337, 2])
We keep 2.97e+04/2.63e+05 = 11% of the original kernel matrix.

torch.Size([7168, 2])
We keep 4.65e+05/9.00e+06 =  5% of the original kernel matrix.

torch.Size([1045, 2])
We keep 2.27e+04/1.71e+05 = 13% of the original kernel matrix.

torch.Size([6529, 2])
We keep 3.98e+05/7.26e+06 =  5% of the original kernel matrix.

torch.Size([2755, 2])
We keep 9.09e+04/1.23e+06 =  7% of the original kernel matrix.

torch.Size([9367, 2])
We keep 7.49e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([890, 2])
We keep 1.06e+04/8.53e+04 = 12% of the original kernel matrix.

torch.Size([6273, 2])
We keep 3.12e+05/5.12e+06 =  6% of the original kernel matrix.

torch.Size([9828, 2])
We keep 8.05e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([16171, 2])
We keep 2.09e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([1349, 2])
We keep 2.39e+04/2.42e+05 =  9% of the original kernel matrix.

torch.Size([7225, 2])
We keep 4.39e+05/8.63e+06 =  5% of the original kernel matrix.

torch.Size([12019, 2])
We keep 1.26e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([17816, 2])
We keep 2.57e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([6039, 2])
We keep 3.49e+05/6.74e+06 =  5% of the original kernel matrix.

torch.Size([13040, 2])
We keep 1.36e+06/4.56e+07 =  2% of the original kernel matrix.

torch.Size([2623, 2])
We keep 9.69e+04/1.19e+06 =  8% of the original kernel matrix.

torch.Size([9230, 2])
We keep 7.31e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([522, 2])
We keep 4.44e+03/2.69e+04 = 16% of the original kernel matrix.

torch.Size([5221, 2])
We keep 2.20e+05/2.88e+06 =  7% of the original kernel matrix.

torch.Size([4053, 2])
We keep 2.33e+05/3.42e+06 =  6% of the original kernel matrix.

torch.Size([11006, 2])
We keep 1.08e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([3674, 2])
We keep 1.49e+05/2.34e+06 =  6% of the original kernel matrix.

torch.Size([10592, 2])
We keep 9.37e+05/2.68e+07 =  3% of the original kernel matrix.

torch.Size([2568, 2])
We keep 9.16e+04/1.16e+06 =  7% of the original kernel matrix.

torch.Size([9132, 2])
We keep 7.32e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([7927, 2])
We keep 7.00e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([14558, 2])
We keep 1.89e+06/6.96e+07 =  2% of the original kernel matrix.

torch.Size([7440, 2])
We keep 6.78e+05/1.33e+07 =  5% of the original kernel matrix.

torch.Size([14162, 2])
We keep 1.75e+06/6.41e+07 =  2% of the original kernel matrix.

torch.Size([1510, 2])
We keep 3.16e+04/2.95e+05 = 10% of the original kernel matrix.

torch.Size([7494, 2])
We keep 4.66e+05/9.53e+06 =  4% of the original kernel matrix.

torch.Size([1117, 2])
We keep 2.17e+04/1.94e+05 = 11% of the original kernel matrix.

torch.Size([6734, 2])
We keep 4.13e+05/7.72e+06 =  5% of the original kernel matrix.

torch.Size([1359, 2])
We keep 2.53e+04/2.31e+05 = 10% of the original kernel matrix.

torch.Size([7289, 2])
We keep 4.28e+05/8.44e+06 =  5% of the original kernel matrix.

torch.Size([930, 2])
We keep 1.54e+04/1.14e+05 = 13% of the original kernel matrix.

torch.Size([6259, 2])
We keep 3.47e+05/5.93e+06 =  5% of the original kernel matrix.

torch.Size([606, 2])
We keep 1.15e+04/6.97e+04 = 16% of the original kernel matrix.

torch.Size([5261, 2])
We keep 2.92e+05/4.63e+06 =  6% of the original kernel matrix.

torch.Size([695, 2])
We keep 7.30e+03/5.20e+04 = 14% of the original kernel matrix.

torch.Size([5802, 2])
We keep 2.65e+05/4.00e+06 =  6% of the original kernel matrix.

torch.Size([844, 2])
We keep 1.52e+04/1.03e+05 = 14% of the original kernel matrix.

torch.Size([5991, 2])
We keep 3.34e+05/5.63e+06 =  5% of the original kernel matrix.

torch.Size([3254, 2])
We keep 3.15e+05/2.44e+06 = 12% of the original kernel matrix.

torch.Size([9878, 2])
We keep 9.25e+05/2.74e+07 =  3% of the original kernel matrix.

torch.Size([1761, 2])
We keep 6.80e+04/5.85e+05 = 11% of the original kernel matrix.

torch.Size([7774, 2])
We keep 5.71e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([1338, 2])
We keep 6.50e+04/3.75e+05 = 17% of the original kernel matrix.

torch.Size([6901, 2])
We keep 5.08e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([1871, 2])
We keep 5.18e+04/6.05e+05 =  8% of the original kernel matrix.

torch.Size([8077, 2])
We keep 5.79e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([3160, 2])
We keep 1.22e+05/1.78e+06 =  6% of the original kernel matrix.

torch.Size([9904, 2])
We keep 8.43e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([1138, 2])
We keep 2.85e+04/2.13e+05 = 13% of the original kernel matrix.

torch.Size([6621, 2])
We keep 4.27e+05/8.09e+06 =  5% of the original kernel matrix.

torch.Size([4002, 2])
We keep 2.54e+05/3.87e+06 =  6% of the original kernel matrix.

torch.Size([10785, 2])
We keep 1.15e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([3329, 2])
We keep 1.39e+05/2.10e+06 =  6% of the original kernel matrix.

torch.Size([10049, 2])
We keep 9.16e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([1309, 2])
We keep 2.44e+04/2.20e+05 = 11% of the original kernel matrix.

torch.Size([7179, 2])
We keep 4.33e+05/8.23e+06 =  5% of the original kernel matrix.

torch.Size([809, 2])
We keep 1.10e+04/8.07e+04 = 13% of the original kernel matrix.

torch.Size([5995, 2])
We keep 3.08e+05/4.98e+06 =  6% of the original kernel matrix.

torch.Size([4780, 2])
We keep 2.98e+05/4.88e+06 =  6% of the original kernel matrix.

torch.Size([11493, 2])
We keep 1.20e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([911, 2])
We keep 1.43e+04/1.06e+05 = 13% of the original kernel matrix.

torch.Size([6192, 2])
We keep 3.37e+05/5.70e+06 =  5% of the original kernel matrix.

torch.Size([921, 2])
We keep 1.26e+04/9.99e+04 = 12% of the original kernel matrix.

torch.Size([6334, 2])
We keep 3.29e+05/5.54e+06 =  5% of the original kernel matrix.

torch.Size([1514, 2])
We keep 4.54e+04/4.20e+05 = 10% of the original kernel matrix.

torch.Size([7363, 2])
We keep 5.31e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([2162, 2])
We keep 6.46e+04/7.29e+05 =  8% of the original kernel matrix.

torch.Size([8567, 2])
We keep 6.37e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([6012, 2])
We keep 4.26e+05/8.21e+06 =  5% of the original kernel matrix.

torch.Size([13028, 2])
We keep 1.49e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([2050, 2])
We keep 5.64e+04/6.43e+05 =  8% of the original kernel matrix.

torch.Size([8342, 2])
We keep 6.07e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([570, 2])
We keep 9.49e+03/5.15e+04 = 18% of the original kernel matrix.

torch.Size([5117, 2])
We keep 2.73e+05/3.98e+06 =  6% of the original kernel matrix.

torch.Size([3612, 2])
We keep 1.39e+05/2.10e+06 =  6% of the original kernel matrix.

torch.Size([10507, 2])
We keep 8.86e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([6594, 2])
We keep 3.81e+05/8.39e+06 =  4% of the original kernel matrix.

torch.Size([13595, 2])
We keep 1.44e+06/5.08e+07 =  2% of the original kernel matrix.

torch.Size([773, 2])
We keep 1.04e+04/7.34e+04 = 14% of the original kernel matrix.

torch.Size([5848, 2])
We keep 2.94e+05/4.76e+06 =  6% of the original kernel matrix.

torch.Size([5125, 2])
We keep 4.65e+05/6.38e+06 =  7% of the original kernel matrix.

torch.Size([12083, 2])
We keep 1.35e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([26899, 2])
We keep 5.61e+07/7.42e+08 =  7% of the original kernel matrix.

torch.Size([25350, 2])
We keep 8.62e+06/4.78e+08 =  1% of the original kernel matrix.

torch.Size([3195, 2])
We keep 2.15e+05/2.09e+06 = 10% of the original kernel matrix.

torch.Size([10019, 2])
We keep 7.79e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([5412, 2])
We keep 3.03e+05/5.60e+06 =  5% of the original kernel matrix.

torch.Size([12389, 2])
We keep 1.28e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([620, 2])
We keep 1.12e+04/6.55e+04 = 17% of the original kernel matrix.

torch.Size([5256, 2])
We keep 3.00e+05/4.49e+06 =  6% of the original kernel matrix.

torch.Size([5473, 2])
We keep 4.12e+05/6.53e+06 =  6% of the original kernel matrix.

torch.Size([12369, 2])
We keep 1.36e+06/4.48e+07 =  3% of the original kernel matrix.

torch.Size([13827, 2])
We keep 4.23e+06/6.23e+07 =  6% of the original kernel matrix.

torch.Size([19172, 2])
We keep 3.17e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([70097, 2])
We keep 3.66e+08/4.59e+09 =  7% of the original kernel matrix.

torch.Size([42239, 2])
We keep 1.85e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([2403, 2])
We keep 7.36e+04/9.66e+05 =  7% of the original kernel matrix.

torch.Size([8993, 2])
We keep 6.91e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([20231, 2])
We keep 8.34e+06/2.00e+08 =  4% of the original kernel matrix.

torch.Size([23760, 2])
We keep 5.11e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([32304, 2])
We keep 1.63e+07/5.55e+08 =  2% of the original kernel matrix.

torch.Size([28788, 2])
We keep 7.24e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([8782, 2])
We keep 1.02e+06/2.18e+07 =  4% of the original kernel matrix.

torch.Size([15251, 2])
We keep 2.11e+06/8.19e+07 =  2% of the original kernel matrix.

torch.Size([122380, 2])
We keep 4.41e+08/1.30e+10 =  3% of the original kernel matrix.

torch.Size([56590, 2])
We keep 2.88e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([1206, 2])
We keep 1.52e+04/1.44e+05 = 10% of the original kernel matrix.

torch.Size([7093, 2])
We keep 3.52e+05/6.65e+06 =  5% of the original kernel matrix.

torch.Size([6230, 2])
We keep 6.07e+05/8.99e+06 =  6% of the original kernel matrix.

torch.Size([13047, 2])
We keep 1.51e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([54812, 2])
We keep 4.78e+07/1.53e+09 =  3% of the original kernel matrix.

torch.Size([37263, 2])
We keep 1.15e+07/6.87e+08 =  1% of the original kernel matrix.

torch.Size([160073, 2])
We keep 1.68e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([67284, 2])
We keep 2.58e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([3615, 2])
We keep 2.60e+05/2.58e+06 = 10% of the original kernel matrix.

torch.Size([10466, 2])
We keep 9.58e+05/2.82e+07 =  3% of the original kernel matrix.

torch.Size([19566, 2])
We keep 3.84e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([23280, 2])
We keep 4.33e+06/2.10e+08 =  2% of the original kernel matrix.

torch.Size([24501, 2])
We keep 2.92e+07/3.24e+08 =  9% of the original kernel matrix.

torch.Size([25386, 2])
We keep 6.14e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([66428, 2])
We keep 1.77e+08/3.28e+09 =  5% of the original kernel matrix.

torch.Size([41349, 2])
We keep 1.59e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([18008, 2])
We keep 6.38e+06/1.52e+08 =  4% of the original kernel matrix.

torch.Size([21921, 2])
We keep 4.55e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([18410, 2])
We keep 9.41e+06/2.02e+08 =  4% of the original kernel matrix.

torch.Size([21922, 2])
We keep 4.95e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([20386, 2])
We keep 6.15e+06/1.78e+08 =  3% of the original kernel matrix.

torch.Size([23775, 2])
We keep 4.67e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([10539, 2])
We keep 1.84e+06/3.11e+07 =  5% of the original kernel matrix.

torch.Size([16841, 2])
We keep 2.32e+06/9.79e+07 =  2% of the original kernel matrix.

torch.Size([3536, 2])
We keep 8.40e+05/5.86e+06 = 14% of the original kernel matrix.

torch.Size([9598, 2])
We keep 1.31e+06/4.25e+07 =  3% of the original kernel matrix.

torch.Size([63601, 2])
We keep 1.68e+08/3.88e+09 =  4% of the original kernel matrix.

torch.Size([40047, 2])
We keep 1.68e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([2235, 2])
We keep 1.06e+05/8.63e+05 = 12% of the original kernel matrix.

torch.Size([8516, 2])
We keep 6.28e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([2703, 2])
We keep 9.42e+04/1.16e+06 =  8% of the original kernel matrix.

torch.Size([9256, 2])
We keep 7.59e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([25271, 2])
We keep 9.80e+06/2.87e+08 =  3% of the original kernel matrix.

torch.Size([26131, 2])
We keep 5.73e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([10693, 2])
We keep 1.72e+06/3.94e+07 =  4% of the original kernel matrix.

torch.Size([16480, 2])
We keep 2.63e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([186693, 2])
We keep 2.64e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([72753, 2])
We keep 3.18e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([57976, 2])
We keep 2.27e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([39341, 2])
We keep 1.05e+07/6.31e+08 =  1% of the original kernel matrix.

torch.Size([977, 2])
We keep 1.46e+04/1.16e+05 = 12% of the original kernel matrix.

torch.Size([6300, 2])
We keep 3.53e+05/5.98e+06 =  5% of the original kernel matrix.

torch.Size([8955, 2])
We keep 1.57e+06/2.08e+07 =  7% of the original kernel matrix.

torch.Size([15379, 2])
We keep 2.07e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([9921, 2])
We keep 1.57e+06/3.23e+07 =  4% of the original kernel matrix.

torch.Size([16007, 2])
We keep 2.39e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([7902, 2])
We keep 8.31e+05/1.76e+07 =  4% of the original kernel matrix.

torch.Size([14631, 2])
We keep 1.95e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([44544, 2])
We keep 1.43e+07/8.08e+08 =  1% of the original kernel matrix.

torch.Size([34800, 2])
We keep 8.43e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([7272, 2])
We keep 1.71e+06/1.53e+07 = 11% of the original kernel matrix.

torch.Size([13905, 2])
We keep 1.77e+06/6.87e+07 =  2% of the original kernel matrix.

torch.Size([14363, 2])
We keep 2.67e+06/7.54e+07 =  3% of the original kernel matrix.

torch.Size([19449, 2])
We keep 3.36e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([969, 2])
We keep 1.69e+04/1.14e+05 = 14% of the original kernel matrix.

torch.Size([6405, 2])
We keep 3.44e+05/5.93e+06 =  5% of the original kernel matrix.

torch.Size([3951, 2])
We keep 1.77e+05/2.88e+06 =  6% of the original kernel matrix.

torch.Size([10853, 2])
We keep 1.01e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([15235, 2])
We keep 1.05e+07/1.89e+08 =  5% of the original kernel matrix.

torch.Size([19781, 2])
We keep 5.00e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([26599, 2])
We keep 1.18e+07/3.53e+08 =  3% of the original kernel matrix.

torch.Size([26472, 2])
We keep 6.28e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([10248, 2])
We keep 1.51e+06/3.24e+07 =  4% of the original kernel matrix.

torch.Size([16250, 2])
We keep 2.45e+06/9.99e+07 =  2% of the original kernel matrix.

torch.Size([4280, 2])
We keep 2.44e+05/3.75e+06 =  6% of the original kernel matrix.

torch.Size([10952, 2])
We keep 1.09e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([155991, 2])
We keep 1.69e+08/1.48e+10 =  1% of the original kernel matrix.

torch.Size([66440, 2])
We keep 2.97e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([6328, 2])
We keep 3.79e+05/8.40e+06 =  4% of the original kernel matrix.

torch.Size([13275, 2])
We keep 1.44e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([15198, 2])
We keep 2.01e+06/7.35e+07 =  2% of the original kernel matrix.

torch.Size([20243, 2])
We keep 3.26e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([66841, 2])
We keep 3.50e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([42004, 2])
We keep 1.24e+07/7.67e+08 =  1% of the original kernel matrix.

torch.Size([2041, 2])
We keep 6.70e+04/8.10e+05 =  8% of the original kernel matrix.

torch.Size([8373, 2])
We keep 6.57e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([29092, 2])
We keep 8.47e+06/3.32e+08 =  2% of the original kernel matrix.

torch.Size([28395, 2])
We keep 6.07e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([129158, 2])
We keep 2.22e+08/8.70e+09 =  2% of the original kernel matrix.

torch.Size([59331, 2])
We keep 2.37e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([98434, 2])
We keep 1.72e+08/6.48e+09 =  2% of the original kernel matrix.

torch.Size([51085, 2])
We keep 2.18e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([8894, 2])
We keep 1.08e+06/2.11e+07 =  5% of the original kernel matrix.

torch.Size([15298, 2])
We keep 2.06e+06/8.06e+07 =  2% of the original kernel matrix.

torch.Size([7100, 2])
We keep 6.69e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([13726, 2])
We keep 1.71e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([3743, 2])
We keep 3.00e+05/3.55e+06 =  8% of the original kernel matrix.

torch.Size([10146, 2])
We keep 1.10e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([3247, 2])
We keep 1.20e+05/1.79e+06 =  6% of the original kernel matrix.

torch.Size([10141, 2])
We keep 8.60e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([17039, 2])
We keep 5.86e+06/1.37e+08 =  4% of the original kernel matrix.

torch.Size([21063, 2])
We keep 4.13e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([9571, 2])
We keep 8.69e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([15976, 2])
We keep 2.04e+06/8.03e+07 =  2% of the original kernel matrix.

torch.Size([14827, 2])
We keep 3.42e+06/8.36e+07 =  4% of the original kernel matrix.

torch.Size([19906, 2])
We keep 3.44e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([34428, 2])
We keep 4.59e+07/8.08e+08 =  5% of the original kernel matrix.

torch.Size([30439, 2])
We keep 7.29e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([21083, 2])
We keep 1.03e+07/2.07e+08 =  4% of the original kernel matrix.

torch.Size([24121, 2])
We keep 4.92e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([4185, 2])
We keep 4.35e+05/5.88e+06 =  7% of the original kernel matrix.

torch.Size([10756, 2])
We keep 1.29e+06/4.25e+07 =  3% of the original kernel matrix.

torch.Size([8439, 2])
We keep 7.12e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([14936, 2])
We keep 1.84e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([22774, 2])
We keep 7.80e+07/1.49e+09 =  5% of the original kernel matrix.

torch.Size([21838, 2])
We keep 1.09e+07/6.77e+08 =  1% of the original kernel matrix.

torch.Size([9124, 2])
We keep 1.22e+06/2.53e+07 =  4% of the original kernel matrix.

torch.Size([15626, 2])
We keep 2.13e+06/8.83e+07 =  2% of the original kernel matrix.

torch.Size([3516, 2])
We keep 1.99e+05/2.27e+06 =  8% of the original kernel matrix.

torch.Size([10408, 2])
We keep 8.89e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([19699, 2])
We keep 4.77e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([23426, 2])
We keep 4.76e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([210635, 2])
We keep 4.03e+08/2.68e+10 =  1% of the original kernel matrix.

torch.Size([76275, 2])
We keep 3.88e+07/2.87e+09 =  1% of the original kernel matrix.

torch.Size([5554, 2])
We keep 4.42e+05/6.70e+06 =  6% of the original kernel matrix.

torch.Size([12412, 2])
We keep 1.39e+06/4.54e+07 =  3% of the original kernel matrix.

torch.Size([36957, 2])
We keep 5.70e+07/1.02e+09 =  5% of the original kernel matrix.

torch.Size([30473, 2])
We keep 9.16e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([2906, 2])
We keep 1.81e+05/1.79e+06 = 10% of the original kernel matrix.

torch.Size([9453, 2])
We keep 8.64e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([10117, 2])
We keep 5.71e+06/8.77e+07 =  6% of the original kernel matrix.

torch.Size([15192, 2])
We keep 3.65e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([9902, 2])
We keep 2.21e+06/4.04e+07 =  5% of the original kernel matrix.

torch.Size([16351, 2])
We keep 2.36e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([39729, 2])
We keep 1.23e+08/2.05e+09 =  5% of the original kernel matrix.

torch.Size([29169, 2])
We keep 1.30e+07/7.94e+08 =  1% of the original kernel matrix.

torch.Size([28630, 2])
We keep 7.76e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([27786, 2])
We keep 6.48e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([3200, 2])
We keep 1.11e+05/1.50e+06 =  7% of the original kernel matrix.

torch.Size([10058, 2])
We keep 8.15e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([75772, 2])
We keep 3.21e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([45075, 2])
We keep 1.31e+07/8.16e+08 =  1% of the original kernel matrix.

torch.Size([70136, 2])
We keep 2.74e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([43026, 2])
We keep 1.20e+07/7.54e+08 =  1% of the original kernel matrix.

torch.Size([3456, 2])
We keep 1.54e+05/2.30e+06 =  6% of the original kernel matrix.

torch.Size([10360, 2])
We keep 8.88e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([29257, 2])
We keep 7.92e+06/3.33e+08 =  2% of the original kernel matrix.

torch.Size([28635, 2])
We keep 6.00e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([112822, 2])
We keep 7.00e+07/5.75e+09 =  1% of the original kernel matrix.

torch.Size([55812, 2])
We keep 1.98e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([192682, 2])
We keep 1.68e+08/1.59e+10 =  1% of the original kernel matrix.

torch.Size([75038, 2])
We keep 3.12e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([32562, 2])
We keep 1.96e+08/1.57e+09 = 12% of the original kernel matrix.

torch.Size([28566, 2])
We keep 1.13e+07/6.95e+08 =  1% of the original kernel matrix.

torch.Size([3380, 2])
We keep 1.93e+06/8.17e+06 = 23% of the original kernel matrix.

torch.Size([9113, 2])
We keep 1.49e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([4865, 2])
We keep 2.69e+05/4.69e+06 =  5% of the original kernel matrix.

torch.Size([11784, 2])
We keep 1.20e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([3467, 2])
We keep 3.10e+05/4.24e+06 =  7% of the original kernel matrix.

torch.Size([10203, 2])
We keep 1.09e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([29458, 2])
We keep 1.60e+07/4.05e+08 =  3% of the original kernel matrix.

torch.Size([28225, 2])
We keep 6.59e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([4521, 2])
We keep 2.27e+05/3.99e+06 =  5% of the original kernel matrix.

torch.Size([11627, 2])
We keep 1.12e+06/3.51e+07 =  3% of the original kernel matrix.

torch.Size([1745, 2])
We keep 4.17e+04/4.57e+05 =  9% of the original kernel matrix.

torch.Size([7909, 2])
We keep 5.43e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([37855, 2])
We keep 1.33e+07/6.04e+08 =  2% of the original kernel matrix.

torch.Size([32411, 2])
We keep 7.61e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([74434, 2])
We keep 3.93e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([44079, 2])
We keep 1.33e+07/8.23e+08 =  1% of the original kernel matrix.

torch.Size([17175, 2])
We keep 2.14e+07/3.86e+08 =  5% of the original kernel matrix.

torch.Size([18329, 2])
We keep 6.48e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([4438, 2])
We keep 2.56e+05/4.01e+06 =  6% of the original kernel matrix.

torch.Size([11446, 2])
We keep 1.14e+06/3.51e+07 =  3% of the original kernel matrix.

torch.Size([8720, 2])
We keep 1.01e+06/2.26e+07 =  4% of the original kernel matrix.

torch.Size([15109, 2])
We keep 2.11e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([14582, 2])
We keep 2.20e+06/6.39e+07 =  3% of the original kernel matrix.

torch.Size([19854, 2])
We keep 3.15e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([8707, 2])
We keep 9.89e+05/1.82e+07 =  5% of the original kernel matrix.

torch.Size([15319, 2])
We keep 1.93e+06/7.49e+07 =  2% of the original kernel matrix.

torch.Size([8872, 2])
We keep 7.91e+05/1.86e+07 =  4% of the original kernel matrix.

torch.Size([15341, 2])
We keep 1.98e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([2854, 2])
We keep 1.11e+05/1.39e+06 =  7% of the original kernel matrix.

torch.Size([9669, 2])
We keep 7.95e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([89514, 2])
We keep 1.71e+08/6.44e+09 =  2% of the original kernel matrix.

torch.Size([47471, 2])
We keep 2.12e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([21684, 2])
We keep 3.94e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([24682, 2])
We keep 4.66e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([12952, 2])
We keep 1.80e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([18556, 2])
We keep 2.80e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([13054, 2])
We keep 3.79e+06/5.92e+07 =  6% of the original kernel matrix.

torch.Size([18645, 2])
We keep 3.07e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([3323, 2])
We keep 5.05e+05/4.57e+06 = 11% of the original kernel matrix.

torch.Size([9677, 2])
We keep 1.19e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([201904, 2])
We keep 3.12e+08/1.96e+10 =  1% of the original kernel matrix.

torch.Size([75732, 2])
We keep 3.39e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([6730, 2])
We keep 5.07e+05/9.48e+06 =  5% of the original kernel matrix.

torch.Size([13408, 2])
We keep 1.56e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([50688, 2])
We keep 1.98e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([36820, 2])
We keep 9.62e+06/5.60e+08 =  1% of the original kernel matrix.

torch.Size([1648, 2])
We keep 4.13e+04/4.28e+05 =  9% of the original kernel matrix.

torch.Size([7703, 2])
We keep 5.06e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([2330, 2])
We keep 7.85e+04/9.37e+05 =  8% of the original kernel matrix.

torch.Size([8866, 2])
We keep 6.96e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([3372, 2])
We keep 1.28e+05/1.92e+06 =  6% of the original kernel matrix.

torch.Size([10166, 2])
We keep 8.81e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([12243, 2])
We keep 2.21e+06/4.30e+07 =  5% of the original kernel matrix.

torch.Size([18113, 2])
We keep 2.48e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([11146, 2])
We keep 7.85e+06/7.14e+07 = 10% of the original kernel matrix.

torch.Size([16864, 2])
We keep 3.38e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([11169, 2])
We keep 8.50e+06/6.37e+07 = 13% of the original kernel matrix.

torch.Size([17215, 2])
We keep 3.17e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([4321, 2])
We keep 2.05e+05/3.32e+06 =  6% of the original kernel matrix.

torch.Size([11140, 2])
We keep 1.07e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([3397, 2])
We keep 1.37e+05/2.10e+06 =  6% of the original kernel matrix.

torch.Size([10259, 2])
We keep 9.04e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([144571, 2])
We keep 1.63e+08/8.97e+09 =  1% of the original kernel matrix.

torch.Size([63201, 2])
We keep 2.39e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([18847, 2])
We keep 4.14e+06/1.34e+08 =  3% of the original kernel matrix.

torch.Size([22540, 2])
We keep 4.21e+06/2.03e+08 =  2% of the original kernel matrix.

torch.Size([16633, 2])
We keep 5.33e+06/1.12e+08 =  4% of the original kernel matrix.

torch.Size([21168, 2])
We keep 3.91e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([45053, 2])
We keep 2.46e+07/9.70e+08 =  2% of the original kernel matrix.

torch.Size([35284, 2])
We keep 9.47e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([42277, 2])
We keep 5.49e+07/1.68e+09 =  3% of the original kernel matrix.

torch.Size([31463, 2])
We keep 1.19e+07/7.19e+08 =  1% of the original kernel matrix.

torch.Size([6169, 2])
We keep 3.87e+05/7.81e+06 =  4% of the original kernel matrix.

torch.Size([12981, 2])
We keep 1.44e+06/4.90e+07 =  2% of the original kernel matrix.

torch.Size([5280, 2])
We keep 2.55e+05/4.75e+06 =  5% of the original kernel matrix.

torch.Size([12274, 2])
We keep 1.19e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([5699, 2])
We keep 1.07e+06/1.03e+07 = 10% of the original kernel matrix.

torch.Size([12476, 2])
We keep 1.60e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([470, 2])
We keep 5.68e+03/2.82e+04 = 20% of the original kernel matrix.

torch.Size([4930, 2])
We keep 2.30e+05/2.95e+06 =  7% of the original kernel matrix.

torch.Size([3168, 2])
We keep 1.33e+05/1.94e+06 =  6% of the original kernel matrix.

torch.Size([9955, 2])
We keep 9.10e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([13317, 2])
We keep 3.10e+06/7.58e+07 =  4% of the original kernel matrix.

torch.Size([18765, 2])
We keep 3.41e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([2473, 2])
We keep 1.90e+05/1.75e+06 = 10% of the original kernel matrix.

torch.Size([8519, 2])
We keep 8.74e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([2414, 2])
We keep 7.05e+04/8.43e+05 =  8% of the original kernel matrix.

torch.Size([8974, 2])
We keep 6.67e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([2341, 2])
We keep 7.03e+04/8.06e+05 =  8% of the original kernel matrix.

torch.Size([8704, 2])
We keep 6.41e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([3381, 2])
We keep 1.41e+05/2.05e+06 =  6% of the original kernel matrix.

torch.Size([10253, 2])
We keep 8.96e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([223856, 2])
We keep 6.49e+08/3.08e+10 =  2% of the original kernel matrix.

torch.Size([78001, 2])
We keep 4.23e+07/3.08e+09 =  1% of the original kernel matrix.

torch.Size([10225, 2])
We keep 2.83e+06/4.13e+07 =  6% of the original kernel matrix.

torch.Size([16256, 2])
We keep 2.69e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([34443, 2])
We keep 1.38e+07/4.88e+08 =  2% of the original kernel matrix.

torch.Size([30449, 2])
We keep 6.98e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([13521, 2])
We keep 1.30e+07/1.54e+08 =  8% of the original kernel matrix.

torch.Size([18242, 2])
We keep 4.52e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([3116, 2])
We keep 1.36e+05/1.82e+06 =  7% of the original kernel matrix.

torch.Size([9809, 2])
We keep 8.67e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([4467, 2])
We keep 3.13e+05/4.69e+06 =  6% of the original kernel matrix.

torch.Size([11226, 2])
We keep 1.20e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([326098, 2])
We keep 1.28e+09/4.95e+10 =  2% of the original kernel matrix.

torch.Size([96026, 2])
We keep 4.83e+07/3.90e+09 =  1% of the original kernel matrix.

torch.Size([343901, 2])
We keep 4.39e+08/4.50e+10 =  0% of the original kernel matrix.

torch.Size([100199, 2])
We keep 4.84e+07/3.72e+09 =  1% of the original kernel matrix.

torch.Size([19666, 2])
We keep 4.39e+06/1.46e+08 =  3% of the original kernel matrix.

torch.Size([23271, 2])
We keep 4.33e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([18643, 2])
We keep 4.02e+06/1.24e+08 =  3% of the original kernel matrix.

torch.Size([22725, 2])
We keep 4.12e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([1433, 2])
We keep 3.01e+04/3.01e+05 =  9% of the original kernel matrix.

torch.Size([7321, 2])
We keep 4.77e+05/9.63e+06 =  4% of the original kernel matrix.

torch.Size([21973, 2])
We keep 3.52e+07/6.46e+08 =  5% of the original kernel matrix.

torch.Size([21623, 2])
We keep 8.02e+06/4.46e+08 =  1% of the original kernel matrix.

torch.Size([33228, 2])
We keep 2.18e+07/5.38e+08 =  4% of the original kernel matrix.

torch.Size([29948, 2])
We keep 7.15e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([72635, 2])
We keep 1.19e+08/4.01e+09 =  2% of the original kernel matrix.

torch.Size([43025, 2])
We keep 1.73e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([5681, 2])
We keep 4.10e+05/7.24e+06 =  5% of the original kernel matrix.

torch.Size([12534, 2])
We keep 1.41e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([55750, 2])
We keep 3.06e+07/1.39e+09 =  2% of the original kernel matrix.

torch.Size([38264, 2])
We keep 1.09e+07/6.54e+08 =  1% of the original kernel matrix.

torch.Size([124611, 2])
We keep 1.42e+08/7.33e+09 =  1% of the original kernel matrix.

torch.Size([58242, 2])
We keep 2.21e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([17718, 2])
We keep 4.84e+06/1.42e+08 =  3% of the original kernel matrix.

torch.Size([21621, 2])
We keep 4.27e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([4259, 2])
We keep 3.59e+05/4.70e+06 =  7% of the original kernel matrix.

torch.Size([10982, 2])
We keep 1.22e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([12270, 2])
We keep 1.83e+06/4.20e+07 =  4% of the original kernel matrix.

torch.Size([18412, 2])
We keep 2.72e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([16146, 2])
We keep 2.56e+06/8.66e+07 =  2% of the original kernel matrix.

torch.Size([20865, 2])
We keep 3.51e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([2638, 2])
We keep 1.01e+05/1.35e+06 =  7% of the original kernel matrix.

torch.Size([9270, 2])
We keep 7.62e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([74573, 2])
We keep 5.52e+07/2.44e+09 =  2% of the original kernel matrix.

torch.Size([44303, 2])
We keep 1.40e+07/8.67e+08 =  1% of the original kernel matrix.

torch.Size([14215, 2])
We keep 5.42e+06/1.05e+08 =  5% of the original kernel matrix.

torch.Size([19072, 2])
We keep 3.93e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([985, 2])
We keep 1.41e+04/1.22e+05 = 11% of the original kernel matrix.

torch.Size([6524, 2])
We keep 3.59e+05/6.14e+06 =  5% of the original kernel matrix.

torch.Size([15368, 2])
We keep 2.40e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([20497, 2])
We keep 3.33e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([19705, 2])
We keep 3.99e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([23467, 2])
We keep 4.47e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([659, 2])
We keep 8.95e+03/6.35e+04 = 14% of the original kernel matrix.

torch.Size([5507, 2])
We keep 2.91e+05/4.42e+06 =  6% of the original kernel matrix.

torch.Size([57246, 2])
We keep 7.96e+07/1.83e+09 =  4% of the original kernel matrix.

torch.Size([37943, 2])
We keep 1.23e+07/7.50e+08 =  1% of the original kernel matrix.

torch.Size([64694, 2])
We keep 3.15e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([41453, 2])
We keep 1.15e+07/7.18e+08 =  1% of the original kernel matrix.

torch.Size([48704, 2])
We keep 2.08e+07/9.22e+08 =  2% of the original kernel matrix.

torch.Size([36356, 2])
We keep 9.16e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([61438, 2])
We keep 4.00e+07/1.52e+09 =  2% of the original kernel matrix.

torch.Size([40261, 2])
We keep 1.14e+07/6.85e+08 =  1% of the original kernel matrix.

torch.Size([37220, 2])
We keep 1.37e+07/6.05e+08 =  2% of the original kernel matrix.

torch.Size([31470, 2])
We keep 7.66e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([6928, 2])
We keep 1.19e+06/1.55e+07 =  7% of the original kernel matrix.

torch.Size([13429, 2])
We keep 1.87e+06/6.90e+07 =  2% of the original kernel matrix.

torch.Size([15827, 2])
We keep 4.04e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([20513, 2])
We keep 3.91e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([29535, 2])
We keep 6.13e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([28933, 2])
We keep 5.99e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([81483, 2])
We keep 3.88e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([46213, 2])
We keep 1.33e+07/8.57e+08 =  1% of the original kernel matrix.

torch.Size([2754, 2])
We keep 9.57e+04/1.30e+06 =  7% of the original kernel matrix.

torch.Size([9438, 2])
We keep 7.56e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([32704, 2])
We keep 3.64e+07/6.48e+08 =  5% of the original kernel matrix.

torch.Size([29706, 2])
We keep 6.93e+06/4.47e+08 =  1% of the original kernel matrix.

torch.Size([65269, 2])
We keep 1.08e+08/3.51e+09 =  3% of the original kernel matrix.

torch.Size([41138, 2])
We keep 1.47e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([23725, 2])
We keep 6.52e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([24484, 2])
We keep 4.82e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([13433, 2])
We keep 7.01e+06/1.49e+08 =  4% of the original kernel matrix.

torch.Size([18547, 2])
We keep 4.22e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([11288, 2])
We keep 7.27e+07/4.00e+08 = 18% of the original kernel matrix.

torch.Size([15606, 2])
We keep 6.63e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([6577, 2])
We keep 4.98e+05/9.50e+06 =  5% of the original kernel matrix.

torch.Size([13466, 2])
We keep 1.54e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([1818, 2])
We keep 4.77e+04/5.39e+05 =  8% of the original kernel matrix.

torch.Size([7932, 2])
We keep 5.71e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([12261, 2])
We keep 9.77e+06/1.28e+08 =  7% of the original kernel matrix.

torch.Size([17155, 2])
We keep 4.21e+06/1.98e+08 =  2% of the original kernel matrix.

torch.Size([92305, 2])
We keep 4.63e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([49460, 2])
We keep 1.53e+07/9.84e+08 =  1% of the original kernel matrix.

torch.Size([47931, 2])
We keep 1.52e+07/8.92e+08 =  1% of the original kernel matrix.

torch.Size([36134, 2])
We keep 8.88e+06/5.24e+08 =  1% of the original kernel matrix.

torch.Size([256067, 2])
We keep 6.74e+08/3.31e+10 =  2% of the original kernel matrix.

torch.Size([84656, 2])
We keep 4.21e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([80465, 2])
We keep 2.28e+08/4.23e+09 =  5% of the original kernel matrix.

torch.Size([45988, 2])
We keep 1.77e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([50341, 2])
We keep 3.13e+07/1.19e+09 =  2% of the original kernel matrix.

torch.Size([36231, 2])
We keep 1.04e+07/6.06e+08 =  1% of the original kernel matrix.

torch.Size([9582, 2])
We keep 1.43e+06/4.24e+07 =  3% of the original kernel matrix.

torch.Size([16050, 2])
We keep 2.64e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([58199, 2])
We keep 2.50e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([39475, 2])
We keep 1.06e+07/6.35e+08 =  1% of the original kernel matrix.

torch.Size([1281, 2])
We keep 2.82e+04/2.43e+05 = 11% of the original kernel matrix.

torch.Size([6989, 2])
We keep 4.39e+05/8.65e+06 =  5% of the original kernel matrix.

torch.Size([5549, 2])
We keep 6.61e+05/8.63e+06 =  7% of the original kernel matrix.

torch.Size([12073, 2])
We keep 1.52e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([135585, 2])
We keep 1.19e+08/7.66e+09 =  1% of the original kernel matrix.

torch.Size([61346, 2])
We keep 2.26e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([3694, 2])
We keep 1.98e+05/2.65e+06 =  7% of the original kernel matrix.

torch.Size([10493, 2])
We keep 9.70e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([2809, 2])
We keep 1.08e+05/1.43e+06 =  7% of the original kernel matrix.

torch.Size([9565, 2])
We keep 7.98e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([7732, 2])
We keep 1.15e+06/2.28e+07 =  5% of the original kernel matrix.

torch.Size([14578, 2])
We keep 2.00e+06/8.38e+07 =  2% of the original kernel matrix.

torch.Size([27950, 2])
We keep 1.88e+07/4.68e+08 =  4% of the original kernel matrix.

torch.Size([27294, 2])
We keep 7.02e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([16293, 2])
We keep 6.09e+06/1.18e+08 =  5% of the original kernel matrix.

torch.Size([20667, 2])
We keep 4.03e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([53907, 2])
We keep 3.76e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([38117, 2])
We keep 1.12e+07/6.64e+08 =  1% of the original kernel matrix.

torch.Size([5686, 2])
We keep 3.59e+06/3.34e+07 = 10% of the original kernel matrix.

torch.Size([10954, 2])
We keep 2.49e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([8928, 2])
We keep 3.26e+06/4.63e+07 =  7% of the original kernel matrix.

torch.Size([14967, 2])
We keep 2.82e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([2296, 2])
We keep 8.11e+04/9.45e+05 =  8% of the original kernel matrix.

torch.Size([8774, 2])
We keep 6.92e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([7956, 2])
We keep 1.28e+06/1.71e+07 =  7% of the original kernel matrix.

torch.Size([14629, 2])
We keep 1.88e+06/7.25e+07 =  2% of the original kernel matrix.

torch.Size([20021, 2])
We keep 4.06e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([23691, 2])
We keep 4.36e+06/2.10e+08 =  2% of the original kernel matrix.

torch.Size([22125, 2])
We keep 1.46e+07/2.94e+08 =  4% of the original kernel matrix.

torch.Size([24077, 2])
We keep 5.86e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([1948, 2])
We keep 5.65e+04/5.85e+05 =  9% of the original kernel matrix.

torch.Size([8188, 2])
We keep 5.97e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([66414, 2])
We keep 4.86e+07/1.97e+09 =  2% of the original kernel matrix.

torch.Size([41563, 2])
We keep 1.25e+07/7.79e+08 =  1% of the original kernel matrix.

torch.Size([1133, 2])
We keep 2.25e+04/2.03e+05 = 11% of the original kernel matrix.

torch.Size([6723, 2])
We keep 4.05e+05/7.91e+06 =  5% of the original kernel matrix.

torch.Size([17915, 2])
We keep 2.65e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([22271, 2])
We keep 3.75e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([2406, 2])
We keep 1.34e+05/1.31e+06 = 10% of the original kernel matrix.

torch.Size([8872, 2])
We keep 7.59e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([25908, 2])
We keep 5.39e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([27079, 2])
We keep 5.51e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([3066, 2])
We keep 1.77e+05/2.03e+06 =  8% of the original kernel matrix.

torch.Size([9745, 2])
We keep 9.09e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([19812, 2])
We keep 9.16e+06/1.71e+08 =  5% of the original kernel matrix.

torch.Size([23112, 2])
We keep 4.66e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([5695, 2])
We keep 4.25e+05/7.14e+06 =  5% of the original kernel matrix.

torch.Size([12659, 2])
We keep 1.41e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([13892, 2])
We keep 2.40e+06/5.97e+07 =  4% of the original kernel matrix.

torch.Size([19225, 2])
We keep 3.06e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([32245, 2])
We keep 1.06e+07/4.40e+08 =  2% of the original kernel matrix.

torch.Size([29818, 2])
We keep 6.80e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([121856, 2])
We keep 1.85e+08/8.49e+09 =  2% of the original kernel matrix.

torch.Size([57296, 2])
We keep 2.35e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([4736, 2])
We keep 3.10e+05/4.82e+06 =  6% of the original kernel matrix.

torch.Size([11597, 2])
We keep 1.21e+06/3.85e+07 =  3% of the original kernel matrix.

torch.Size([5108, 2])
We keep 1.22e+06/1.29e+07 =  9% of the original kernel matrix.

torch.Size([11222, 2])
We keep 1.78e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([61047, 2])
We keep 5.79e+07/2.09e+09 =  2% of the original kernel matrix.

torch.Size([39183, 2])
We keep 1.32e+07/8.02e+08 =  1% of the original kernel matrix.

torch.Size([5649, 2])
We keep 3.91e+05/6.81e+06 =  5% of the original kernel matrix.

torch.Size([12545, 2])
We keep 1.39e+06/4.58e+07 =  3% of the original kernel matrix.

torch.Size([322761, 2])
We keep 4.25e+08/4.08e+10 =  1% of the original kernel matrix.

torch.Size([96903, 2])
We keep 4.69e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([664, 2])
We keep 1.04e+04/6.97e+04 = 14% of the original kernel matrix.

torch.Size([5479, 2])
We keep 3.06e+05/4.63e+06 =  6% of the original kernel matrix.

torch.Size([49176, 2])
We keep 2.17e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([35685, 2])
We keep 9.63e+06/5.63e+08 =  1% of the original kernel matrix.

torch.Size([111286, 2])
We keep 6.76e+07/4.84e+09 =  1% of the original kernel matrix.

torch.Size([55290, 2])
We keep 1.84e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([2265, 2])
We keep 8.32e+04/9.20e+05 =  9% of the original kernel matrix.

torch.Size([8555, 2])
We keep 6.92e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([67079, 2])
We keep 8.81e+07/3.23e+09 =  2% of the original kernel matrix.

torch.Size([40583, 2])
We keep 1.54e+07/9.97e+08 =  1% of the original kernel matrix.

torch.Size([9297, 2])
We keep 3.96e+06/7.54e+07 =  5% of the original kernel matrix.

torch.Size([14953, 2])
We keep 3.31e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([157457, 2])
We keep 3.37e+08/1.37e+10 =  2% of the original kernel matrix.

torch.Size([66070, 2])
We keep 2.90e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([4932, 2])
We keep 2.52e+05/4.32e+06 =  5% of the original kernel matrix.

torch.Size([11918, 2])
We keep 1.17e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([20541, 2])
We keep 9.13e+06/2.58e+08 =  3% of the original kernel matrix.

torch.Size([22843, 2])
We keep 5.54e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([1099, 2])
We keep 2.11e+04/1.76e+05 = 12% of the original kernel matrix.

torch.Size([6620, 2])
We keep 3.98e+05/7.35e+06 =  5% of the original kernel matrix.

torch.Size([4234, 2])
We keep 2.61e+05/3.72e+06 =  7% of the original kernel matrix.

torch.Size([11092, 2])
We keep 1.14e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([5145, 2])
We keep 5.98e+05/6.54e+06 =  9% of the original kernel matrix.

torch.Size([12065, 2])
We keep 1.36e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([15615, 2])
We keep 3.15e+06/9.31e+07 =  3% of the original kernel matrix.

torch.Size([20541, 2])
We keep 3.50e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([9711, 2])
We keep 1.44e+06/3.05e+07 =  4% of the original kernel matrix.

torch.Size([16007, 2])
We keep 2.43e+06/9.69e+07 =  2% of the original kernel matrix.

torch.Size([33665, 2])
We keep 1.52e+07/5.04e+08 =  3% of the original kernel matrix.

torch.Size([30336, 2])
We keep 7.11e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([10979, 2])
We keep 1.33e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([16912, 2])
We keep 2.55e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([1952, 2])
We keep 4.87e+04/5.54e+05 =  8% of the original kernel matrix.

torch.Size([8205, 2])
We keep 5.81e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([4547, 2])
We keep 2.82e+05/4.08e+06 =  6% of the original kernel matrix.

torch.Size([11503, 2])
We keep 1.12e+06/3.54e+07 =  3% of the original kernel matrix.

torch.Size([38691, 2])
We keep 7.49e+07/1.25e+09 =  5% of the original kernel matrix.

torch.Size([31911, 2])
We keep 1.07e+07/6.21e+08 =  1% of the original kernel matrix.

torch.Size([7001, 2])
We keep 3.57e+06/2.08e+07 = 17% of the original kernel matrix.

torch.Size([13585, 2])
We keep 2.06e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([97672, 2])
We keep 4.90e+07/3.53e+09 =  1% of the original kernel matrix.

torch.Size([51175, 2])
We keep 1.59e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([53739, 2])
We keep 5.58e+07/1.77e+09 =  3% of the original kernel matrix.

torch.Size([37405, 2])
We keep 1.24e+07/7.38e+08 =  1% of the original kernel matrix.

torch.Size([5973, 2])
We keep 1.01e+06/1.17e+07 =  8% of the original kernel matrix.

torch.Size([12685, 2])
We keep 1.61e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([4945, 2])
We keep 3.09e+05/4.86e+06 =  6% of the original kernel matrix.

torch.Size([11867, 2])
We keep 1.22e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([4376, 2])
We keep 5.98e+05/7.38e+06 =  8% of the original kernel matrix.

torch.Size([10684, 2])
We keep 1.33e+06/4.77e+07 =  2% of the original kernel matrix.

torch.Size([503502, 2])
We keep 1.44e+09/1.11e+11 =  1% of the original kernel matrix.

torch.Size([121112, 2])
We keep 7.30e+07/5.85e+09 =  1% of the original kernel matrix.

torch.Size([8141, 2])
We keep 7.01e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([14647, 2])
We keep 1.86e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([12080, 2])
We keep 1.65e+06/4.21e+07 =  3% of the original kernel matrix.

torch.Size([17934, 2])
We keep 2.70e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([33019, 2])
We keep 4.27e+07/6.94e+08 =  6% of the original kernel matrix.

torch.Size([28888, 2])
We keep 8.41e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([14982, 2])
We keep 6.68e+06/1.37e+08 =  4% of the original kernel matrix.

torch.Size([19350, 2])
We keep 4.34e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([2574, 2])
We keep 9.39e+04/1.16e+06 =  8% of the original kernel matrix.

torch.Size([9160, 2])
We keep 7.23e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([4878, 2])
We keep 3.71e+05/5.55e+06 =  6% of the original kernel matrix.

torch.Size([11629, 2])
We keep 1.25e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([108415, 2])
We keep 2.80e+08/8.81e+09 =  3% of the original kernel matrix.

torch.Size([52938, 2])
We keep 2.40e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([10405, 2])
We keep 1.18e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([16656, 2])
We keep 2.38e+06/9.78e+07 =  2% of the original kernel matrix.

torch.Size([15559, 2])
We keep 6.95e+06/1.59e+08 =  4% of the original kernel matrix.

torch.Size([19653, 2])
We keep 4.53e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([35806, 2])
We keep 1.28e+07/4.97e+08 =  2% of the original kernel matrix.

torch.Size([31645, 2])
We keep 7.15e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([17233, 2])
We keep 1.59e+07/1.26e+08 = 12% of the original kernel matrix.

torch.Size([21702, 2])
We keep 4.11e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([43201, 2])
We keep 5.63e+07/9.51e+08 =  5% of the original kernel matrix.

torch.Size([34015, 2])
We keep 9.44e+06/5.41e+08 =  1% of the original kernel matrix.

torch.Size([67914, 2])
We keep 7.00e+07/2.49e+09 =  2% of the original kernel matrix.

torch.Size([41522, 2])
We keep 1.40e+07/8.75e+08 =  1% of the original kernel matrix.

torch.Size([34697, 2])
We keep 9.83e+06/4.81e+08 =  2% of the original kernel matrix.

torch.Size([31154, 2])
We keep 7.00e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([5388, 2])
We keep 4.81e+05/6.50e+06 =  7% of the original kernel matrix.

torch.Size([12504, 2])
We keep 1.34e+06/4.47e+07 =  3% of the original kernel matrix.

torch.Size([35333, 2])
We keep 8.94e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([31152, 2])
We keep 6.95e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([4112, 2])
We keep 3.75e+05/4.55e+06 =  8% of the original kernel matrix.

torch.Size([10848, 2])
We keep 1.18e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([8955, 2])
We keep 2.26e+06/2.14e+07 = 10% of the original kernel matrix.

torch.Size([15435, 2])
We keep 2.13e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([12830, 2])
We keep 2.25e+06/6.02e+07 =  3% of the original kernel matrix.

torch.Size([18369, 2])
We keep 3.07e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([4760, 2])
We keep 2.53e+05/4.60e+06 =  5% of the original kernel matrix.

torch.Size([11732, 2])
We keep 1.16e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([27195, 2])
We keep 2.74e+07/5.40e+08 =  5% of the original kernel matrix.

torch.Size([26573, 2])
We keep 7.58e+06/4.08e+08 =  1% of the original kernel matrix.

torch.Size([6404, 2])
We keep 5.58e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([13074, 2])
We keep 1.61e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([1087, 2])
We keep 2.10e+04/1.71e+05 = 12% of the original kernel matrix.

torch.Size([6604, 2])
We keep 3.99e+05/7.26e+06 =  5% of the original kernel matrix.

torch.Size([13215, 2])
We keep 2.02e+06/5.39e+07 =  3% of the original kernel matrix.

torch.Size([18696, 2])
We keep 2.98e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([2184, 2])
We keep 7.80e+05/4.88e+06 = 16% of the original kernel matrix.

torch.Size([7620, 2])
We keep 1.01e+06/3.87e+07 =  2% of the original kernel matrix.

torch.Size([47523, 2])
We keep 6.52e+07/1.86e+09 =  3% of the original kernel matrix.

torch.Size([33477, 2])
We keep 1.25e+07/7.57e+08 =  1% of the original kernel matrix.

torch.Size([25289, 2])
We keep 3.77e+07/6.38e+08 =  5% of the original kernel matrix.

torch.Size([24598, 2])
We keep 8.05e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([8170, 2])
We keep 1.02e+06/2.11e+07 =  4% of the original kernel matrix.

torch.Size([14405, 2])
We keep 2.07e+06/8.06e+07 =  2% of the original kernel matrix.

torch.Size([3259, 2])
We keep 5.04e+05/4.24e+06 = 11% of the original kernel matrix.

torch.Size([9345, 2])
We keep 1.11e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([16726, 2])
We keep 5.14e+06/1.32e+08 =  3% of the original kernel matrix.

torch.Size([21195, 2])
We keep 4.27e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([2350, 2])
We keep 7.29e+04/8.99e+05 =  8% of the original kernel matrix.

torch.Size([8881, 2])
We keep 6.76e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([4772, 2])
We keep 2.52e+05/4.36e+06 =  5% of the original kernel matrix.

torch.Size([11671, 2])
We keep 1.17e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([17939, 2])
We keep 2.94e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([22292, 2])
We keep 3.85e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([96454, 2])
We keep 9.22e+07/4.22e+09 =  2% of the original kernel matrix.

torch.Size([50846, 2])
We keep 1.77e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([5140, 2])
We keep 3.37e+05/5.58e+06 =  6% of the original kernel matrix.

torch.Size([11949, 2])
We keep 1.28e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([792, 2])
We keep 1.36e+04/9.99e+04 = 13% of the original kernel matrix.

torch.Size([5745, 2])
We keep 3.34e+05/5.54e+06 =  6% of the original kernel matrix.

torch.Size([2538, 2])
We keep 1.02e+05/1.27e+06 =  8% of the original kernel matrix.

torch.Size([9024, 2])
We keep 7.59e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([130643, 2])
We keep 1.85e+08/9.58e+09 =  1% of the original kernel matrix.

torch.Size([59719, 2])
We keep 2.54e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([171181, 2])
We keep 3.90e+08/1.47e+10 =  2% of the original kernel matrix.

torch.Size([69829, 2])
We keep 2.93e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([11179, 2])
We keep 2.41e+06/4.82e+07 =  5% of the original kernel matrix.

torch.Size([16904, 2])
We keep 2.88e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([5779, 2])
We keep 3.90e+05/6.62e+06 =  5% of the original kernel matrix.

torch.Size([12601, 2])
We keep 1.36e+06/4.51e+07 =  3% of the original kernel matrix.

torch.Size([18088, 2])
We keep 6.62e+06/1.75e+08 =  3% of the original kernel matrix.

torch.Size([21948, 2])
We keep 4.71e+06/2.32e+08 =  2% of the original kernel matrix.

torch.Size([80817, 2])
We keep 4.27e+07/2.82e+09 =  1% of the original kernel matrix.

torch.Size([46300, 2])
We keep 1.49e+07/9.32e+08 =  1% of the original kernel matrix.

torch.Size([2413, 2])
We keep 6.80e+04/8.63e+05 =  7% of the original kernel matrix.

torch.Size([8963, 2])
We keep 6.74e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([8096, 2])
We keep 1.59e+06/2.67e+07 =  5% of the original kernel matrix.

torch.Size([14634, 2])
We keep 2.18e+06/9.06e+07 =  2% of the original kernel matrix.

torch.Size([12613, 2])
We keep 1.37e+06/4.17e+07 =  3% of the original kernel matrix.

torch.Size([18356, 2])
We keep 2.64e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([28836, 2])
We keep 2.25e+08/1.32e+09 = 17% of the original kernel matrix.

torch.Size([25327, 2])
We keep 1.10e+07/6.38e+08 =  1% of the original kernel matrix.

torch.Size([18516, 2])
We keep 1.34e+07/1.67e+08 =  8% of the original kernel matrix.

torch.Size([22005, 2])
We keep 4.55e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([77187, 2])
We keep 4.37e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([45232, 2])
We keep 1.35e+07/8.41e+08 =  1% of the original kernel matrix.

torch.Size([8337, 2])
We keep 6.34e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([14894, 2])
We keep 1.80e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([5153, 2])
We keep 4.61e+05/6.44e+06 =  7% of the original kernel matrix.

torch.Size([12045, 2])
We keep 1.38e+06/4.45e+07 =  3% of the original kernel matrix.

torch.Size([1946, 2])
We keep 4.62e+04/4.90e+05 =  9% of the original kernel matrix.

torch.Size([8222, 2])
We keep 5.51e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([14130, 2])
We keep 3.02e+06/7.38e+07 =  4% of the original kernel matrix.

torch.Size([19436, 2])
We keep 3.43e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([12006, 2])
We keep 1.64e+06/4.12e+07 =  3% of the original kernel matrix.

torch.Size([17886, 2])
We keep 2.69e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([24574, 2])
We keep 1.21e+07/3.31e+08 =  3% of the original kernel matrix.

torch.Size([25533, 2])
We keep 6.13e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([27051, 2])
We keep 6.86e+06/2.94e+08 =  2% of the original kernel matrix.

torch.Size([27080, 2])
We keep 5.79e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([178692, 2])
We keep 1.48e+08/1.33e+10 =  1% of the original kernel matrix.

torch.Size([71386, 2])
We keep 2.87e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([8120, 2])
We keep 1.22e+06/1.72e+07 =  7% of the original kernel matrix.

torch.Size([14566, 2])
We keep 1.94e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([3636, 2])
We keep 2.60e+05/2.88e+06 =  9% of the original kernel matrix.

torch.Size([10283, 2])
We keep 1.02e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([11102, 2])
We keep 1.85e+06/3.42e+07 =  5% of the original kernel matrix.

torch.Size([17198, 2])
We keep 2.46e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([251928, 2])
We keep 3.42e+09/6.35e+10 =  5% of the original kernel matrix.

torch.Size([84280, 2])
We keep 5.51e+07/4.42e+09 =  1% of the original kernel matrix.

torch.Size([18654, 2])
We keep 3.27e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([22590, 2])
We keep 4.08e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([2513, 2])
We keep 1.38e+05/1.38e+06 =  9% of the original kernel matrix.

torch.Size([8700, 2])
We keep 7.86e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([1482, 2])
We keep 3.95e+04/3.23e+05 = 12% of the original kernel matrix.

torch.Size([7401, 2])
We keep 4.88e+05/9.97e+06 =  4% of the original kernel matrix.

torch.Size([18139, 2])
We keep 3.79e+06/1.22e+08 =  3% of the original kernel matrix.

torch.Size([22279, 2])
We keep 4.06e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([1487, 2])
We keep 3.86e+04/3.60e+05 = 10% of the original kernel matrix.

torch.Size([7460, 2])
We keep 4.95e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([13764, 2])
We keep 8.89e+06/1.07e+08 =  8% of the original kernel matrix.

torch.Size([18821, 2])
We keep 3.87e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([3336, 2])
We keep 1.52e+05/2.18e+06 =  6% of the original kernel matrix.

torch.Size([10043, 2])
We keep 9.28e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([11000, 2])
We keep 1.00e+06/2.91e+07 =  3% of the original kernel matrix.

torch.Size([17141, 2])
We keep 2.31e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([9293, 2])
We keep 9.47e+05/2.31e+07 =  4% of the original kernel matrix.

torch.Size([15774, 2])
We keep 2.11e+06/8.43e+07 =  2% of the original kernel matrix.

torch.Size([3506, 2])
We keep 1.36e+05/2.08e+06 =  6% of the original kernel matrix.

torch.Size([10363, 2])
We keep 9.12e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([2300, 2])
We keep 6.75e+04/8.39e+05 =  8% of the original kernel matrix.

torch.Size([8742, 2])
We keep 6.64e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([106898, 2])
We keep 6.42e+07/4.49e+09 =  1% of the original kernel matrix.

torch.Size([53997, 2])
We keep 1.77e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([204657, 2])
We keep 2.89e+08/1.85e+10 =  1% of the original kernel matrix.

torch.Size([77215, 2])
We keep 3.33e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([74400, 2])
We keep 5.72e+07/2.44e+09 =  2% of the original kernel matrix.

torch.Size([44488, 2])
We keep 1.39e+07/8.67e+08 =  1% of the original kernel matrix.

torch.Size([9402, 2])
We keep 3.88e+06/3.82e+07 = 10% of the original kernel matrix.

torch.Size([15333, 2])
We keep 2.59e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([17368, 2])
We keep 2.60e+06/9.87e+07 =  2% of the original kernel matrix.

torch.Size([21725, 2])
We keep 3.68e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([9684, 2])
We keep 1.18e+06/2.40e+07 =  4% of the original kernel matrix.

torch.Size([16000, 2])
We keep 2.22e+06/8.61e+07 =  2% of the original kernel matrix.

torch.Size([7105, 2])
We keep 6.18e+05/1.18e+07 =  5% of the original kernel matrix.

torch.Size([13996, 2])
We keep 1.70e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([58623, 2])
We keep 2.07e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([39500, 2])
We keep 1.05e+07/6.36e+08 =  1% of the original kernel matrix.

torch.Size([22790, 2])
We keep 1.71e+07/2.61e+08 =  6% of the original kernel matrix.

torch.Size([24154, 2])
We keep 5.08e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([41087, 2])
We keep 8.84e+07/1.20e+09 =  7% of the original kernel matrix.

torch.Size([32668, 2])
We keep 9.69e+06/6.08e+08 =  1% of the original kernel matrix.

torch.Size([9529, 2])
We keep 1.33e+06/2.45e+07 =  5% of the original kernel matrix.

torch.Size([15836, 2])
We keep 2.20e+06/8.68e+07 =  2% of the original kernel matrix.

torch.Size([194145, 2])
We keep 5.00e+08/2.28e+10 =  2% of the original kernel matrix.

torch.Size([74428, 2])
We keep 3.60e+07/2.65e+09 =  1% of the original kernel matrix.

torch.Size([11593, 2])
We keep 2.21e+06/4.18e+07 =  5% of the original kernel matrix.

torch.Size([17587, 2])
We keep 2.71e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([27422, 2])
We keep 9.87e+06/3.39e+08 =  2% of the original kernel matrix.

torch.Size([27709, 2])
We keep 6.15e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([97948, 2])
We keep 9.11e+07/4.22e+09 =  2% of the original kernel matrix.

torch.Size([51521, 2])
We keep 1.73e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([35276, 2])
We keep 9.62e+06/4.93e+08 =  1% of the original kernel matrix.

torch.Size([31529, 2])
We keep 6.99e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([41044, 2])
We keep 8.70e+07/1.38e+09 =  6% of the original kernel matrix.

torch.Size([32033, 2])
We keep 1.03e+07/6.51e+08 =  1% of the original kernel matrix.

torch.Size([72026, 2])
We keep 7.34e+07/2.13e+09 =  3% of the original kernel matrix.

torch.Size([43267, 2])
We keep 1.28e+07/8.10e+08 =  1% of the original kernel matrix.

torch.Size([27556, 2])
We keep 6.21e+06/3.03e+08 =  2% of the original kernel matrix.

torch.Size([27685, 2])
We keep 5.85e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([12270, 2])
We keep 1.49e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([18071, 2])
We keep 2.62e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([10259, 2])
We keep 2.18e+06/3.82e+07 =  5% of the original kernel matrix.

torch.Size([16199, 2])
We keep 2.61e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([25489, 2])
We keep 8.34e+06/2.76e+08 =  3% of the original kernel matrix.

torch.Size([26532, 2])
We keep 5.57e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([5256, 2])
We keep 3.07e+05/5.55e+06 =  5% of the original kernel matrix.

torch.Size([12367, 2])
We keep 1.25e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([68870, 2])
We keep 4.44e+07/1.78e+09 =  2% of the original kernel matrix.

torch.Size([42539, 2])
We keep 1.17e+07/7.41e+08 =  1% of the original kernel matrix.

torch.Size([520698, 2])
We keep 1.06e+09/1.07e+11 =  0% of the original kernel matrix.

torch.Size([123390, 2])
We keep 7.38e+07/5.75e+09 =  1% of the original kernel matrix.

torch.Size([69262, 2])
We keep 1.07e+08/2.47e+09 =  4% of the original kernel matrix.

torch.Size([42723, 2])
We keep 1.43e+07/8.71e+08 =  1% of the original kernel matrix.

torch.Size([39015, 2])
We keep 4.77e+07/1.33e+09 =  3% of the original kernel matrix.

torch.Size([30491, 2])
We keep 1.08e+07/6.40e+08 =  1% of the original kernel matrix.

torch.Size([15156, 2])
We keep 5.55e+06/1.06e+08 =  5% of the original kernel matrix.

torch.Size([20165, 2])
We keep 3.96e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([8512, 2])
We keep 8.54e+06/5.78e+07 = 14% of the original kernel matrix.

torch.Size([14734, 2])
We keep 3.12e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([25075, 2])
We keep 5.58e+06/2.57e+08 =  2% of the original kernel matrix.

torch.Size([25636, 2])
We keep 5.19e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([9561, 2])
We keep 2.07e+06/4.14e+07 =  5% of the original kernel matrix.

torch.Size([15643, 2])
We keep 2.73e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([57867, 2])
We keep 6.34e+07/1.79e+09 =  3% of the original kernel matrix.

torch.Size([38412, 2])
We keep 1.24e+07/7.43e+08 =  1% of the original kernel matrix.

torch.Size([150265, 2])
We keep 1.04e+08/9.18e+09 =  1% of the original kernel matrix.

torch.Size([64941, 2])
We keep 2.42e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([2333, 2])
We keep 1.88e+05/1.25e+06 = 15% of the original kernel matrix.

torch.Size([8565, 2])
We keep 7.20e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([36786, 2])
We keep 8.47e+07/1.21e+09 =  7% of the original kernel matrix.

torch.Size([30438, 2])
We keep 9.94e+06/6.10e+08 =  1% of the original kernel matrix.

torch.Size([44504, 2])
We keep 6.94e+07/1.23e+09 =  5% of the original kernel matrix.

torch.Size([34327, 2])
We keep 1.04e+07/6.16e+08 =  1% of the original kernel matrix.

torch.Size([15085, 2])
We keep 2.44e+06/7.81e+07 =  3% of the original kernel matrix.

torch.Size([20106, 2])
We keep 3.40e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([1996, 2])
We keep 5.51e+04/6.54e+05 =  8% of the original kernel matrix.

torch.Size([8337, 2])
We keep 6.10e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([41594, 2])
We keep 1.70e+07/7.20e+08 =  2% of the original kernel matrix.

torch.Size([33615, 2])
We keep 8.29e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([65794, 2])
We keep 2.76e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([41587, 2])
We keep 1.17e+07/7.15e+08 =  1% of the original kernel matrix.

torch.Size([2615, 2])
We keep 1.04e+05/1.32e+06 =  7% of the original kernel matrix.

torch.Size([9141, 2])
We keep 7.70e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([401501, 2])
We keep 5.37e+08/5.67e+10 =  0% of the original kernel matrix.

torch.Size([107446, 2])
We keep 5.42e+07/4.18e+09 =  1% of the original kernel matrix.

torch.Size([13918, 2])
We keep 3.73e+06/8.89e+07 =  4% of the original kernel matrix.

torch.Size([19420, 2])
We keep 3.50e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([13359, 2])
We keep 2.35e+06/5.80e+07 =  4% of the original kernel matrix.

torch.Size([18790, 2])
We keep 3.06e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([4774, 2])
We keep 3.25e+05/5.27e+06 =  6% of the original kernel matrix.

torch.Size([11803, 2])
We keep 1.21e+06/4.03e+07 =  3% of the original kernel matrix.

torch.Size([3116, 2])
We keep 1.05e+05/1.56e+06 =  6% of the original kernel matrix.

torch.Size([9940, 2])
We keep 8.26e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([2824, 2])
We keep 1.32e+05/1.45e+06 =  9% of the original kernel matrix.

torch.Size([9372, 2])
We keep 7.94e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([4504, 2])
We keep 3.83e+05/4.87e+06 =  7% of the original kernel matrix.

torch.Size([11142, 2])
We keep 1.19e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([9362, 2])
We keep 1.70e+06/2.59e+07 =  6% of the original kernel matrix.

torch.Size([15867, 2])
We keep 2.23e+06/8.93e+07 =  2% of the original kernel matrix.

torch.Size([23271, 2])
We keep 6.86e+06/2.44e+08 =  2% of the original kernel matrix.

torch.Size([25335, 2])
We keep 5.39e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([530253, 2])
We keep 8.88e+09/3.43e+11 =  2% of the original kernel matrix.

torch.Size([110563, 2])
We keep 1.25e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([5287, 2])
We keep 4.41e+05/6.60e+06 =  6% of the original kernel matrix.

torch.Size([12110, 2])
We keep 1.32e+06/4.51e+07 =  2% of the original kernel matrix.

torch.Size([20090, 2])
We keep 4.55e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([23701, 2])
We keep 4.40e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([4123, 2])
We keep 1.95e+05/3.13e+06 =  6% of the original kernel matrix.

torch.Size([11029, 2])
We keep 1.03e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([10475, 2])
We keep 1.19e+06/2.84e+07 =  4% of the original kernel matrix.

torch.Size([16538, 2])
We keep 2.32e+06/9.35e+07 =  2% of the original kernel matrix.

torch.Size([6835, 2])
We keep 6.06e+05/1.12e+07 =  5% of the original kernel matrix.

torch.Size([13670, 2])
We keep 1.67e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([38450, 2])
We keep 1.51e+08/2.01e+09 =  7% of the original kernel matrix.

torch.Size([29767, 2])
We keep 1.29e+07/7.86e+08 =  1% of the original kernel matrix.

torch.Size([8516, 2])
We keep 1.35e+06/2.38e+07 =  5% of the original kernel matrix.

torch.Size([14951, 2])
We keep 2.18e+06/8.56e+07 =  2% of the original kernel matrix.

torch.Size([677216, 2])
We keep 1.40e+09/1.70e+11 =  0% of the original kernel matrix.

torch.Size([144873, 2])
We keep 8.96e+07/7.24e+09 =  1% of the original kernel matrix.

torch.Size([4590, 2])
We keep 2.27e+05/4.05e+06 =  5% of the original kernel matrix.

torch.Size([11577, 2])
We keep 1.13e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([83344, 2])
We keep 6.76e+07/2.61e+09 =  2% of the original kernel matrix.

torch.Size([47062, 2])
We keep 1.41e+07/8.97e+08 =  1% of the original kernel matrix.

torch.Size([11448, 2])
We keep 4.02e+06/4.19e+07 =  9% of the original kernel matrix.

torch.Size([17582, 2])
We keep 2.69e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([71346, 2])
We keep 4.38e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([43366, 2])
We keep 1.35e+07/8.33e+08 =  1% of the original kernel matrix.

torch.Size([183627, 2])
We keep 2.94e+08/2.00e+10 =  1% of the original kernel matrix.

torch.Size([71513, 2])
We keep 3.41e+07/2.48e+09 =  1% of the original kernel matrix.

torch.Size([12980, 2])
We keep 1.91e+06/5.18e+07 =  3% of the original kernel matrix.

torch.Size([18455, 2])
We keep 2.91e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([3596, 2])
We keep 1.94e+05/2.51e+06 =  7% of the original kernel matrix.

torch.Size([10379, 2])
We keep 9.86e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([77248, 2])
We keep 3.49e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([45225, 2])
We keep 1.32e+07/8.30e+08 =  1% of the original kernel matrix.

torch.Size([12479, 2])
We keep 3.44e+06/5.76e+07 =  5% of the original kernel matrix.

torch.Size([18105, 2])
We keep 3.12e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([17601, 2])
We keep 1.31e+07/2.11e+08 =  6% of the original kernel matrix.

torch.Size([19785, 2])
We keep 4.68e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([36686, 2])
We keep 1.16e+07/5.47e+08 =  2% of the original kernel matrix.

torch.Size([31542, 2])
We keep 7.38e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([7593, 2])
We keep 3.07e+06/3.27e+07 =  9% of the original kernel matrix.

torch.Size([14198, 2])
We keep 2.28e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([38000, 2])
We keep 1.62e+07/6.10e+08 =  2% of the original kernel matrix.

torch.Size([33525, 2])
We keep 7.90e+06/4.33e+08 =  1% of the original kernel matrix.

torch.Size([14117, 2])
We keep 2.65e+06/6.56e+07 =  4% of the original kernel matrix.

torch.Size([19149, 2])
We keep 3.16e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([60887, 2])
We keep 6.30e+07/1.78e+09 =  3% of the original kernel matrix.

torch.Size([40070, 2])
We keep 1.23e+07/7.40e+08 =  1% of the original kernel matrix.

torch.Size([123034, 2])
We keep 2.32e+08/9.52e+09 =  2% of the original kernel matrix.

torch.Size([57283, 2])
We keep 2.48e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([9530, 2])
We keep 1.07e+06/2.30e+07 =  4% of the original kernel matrix.

torch.Size([15808, 2])
We keep 2.16e+06/8.42e+07 =  2% of the original kernel matrix.

torch.Size([5080, 2])
We keep 3.84e+05/6.20e+06 =  6% of the original kernel matrix.

torch.Size([11992, 2])
We keep 1.34e+06/4.37e+07 =  3% of the original kernel matrix.

torch.Size([68991, 2])
We keep 8.75e+07/2.83e+09 =  3% of the original kernel matrix.

torch.Size([42033, 2])
We keep 1.48e+07/9.33e+08 =  1% of the original kernel matrix.

torch.Size([14142, 2])
We keep 2.57e+06/7.05e+07 =  3% of the original kernel matrix.

torch.Size([19054, 2])
We keep 3.29e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([33755, 2])
We keep 1.68e+07/4.69e+08 =  3% of the original kernel matrix.

torch.Size([30579, 2])
We keep 6.82e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([9163, 2])
We keep 1.45e+06/2.92e+07 =  4% of the original kernel matrix.

torch.Size([15388, 2])
We keep 2.35e+06/9.49e+07 =  2% of the original kernel matrix.

torch.Size([29906, 2])
We keep 2.29e+07/6.47e+08 =  3% of the original kernel matrix.

torch.Size([27513, 2])
We keep 7.97e+06/4.46e+08 =  1% of the original kernel matrix.

torch.Size([15951, 2])
We keep 5.97e+06/1.32e+08 =  4% of the original kernel matrix.

torch.Size([20338, 2])
We keep 4.23e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([24490, 2])
We keep 2.41e+07/4.58e+08 =  5% of the original kernel matrix.

torch.Size([24838, 2])
We keep 7.01e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([20192, 2])
We keep 5.49e+06/1.72e+08 =  3% of the original kernel matrix.

torch.Size([23362, 2])
We keep 4.66e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([15069, 2])
We keep 2.56e+06/7.21e+07 =  3% of the original kernel matrix.

torch.Size([20187, 2])
We keep 3.35e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([3778, 2])
We keep 1.83e+05/2.77e+06 =  6% of the original kernel matrix.

torch.Size([10637, 2])
We keep 1.00e+06/2.92e+07 =  3% of the original kernel matrix.

torch.Size([17738, 2])
We keep 3.44e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([22194, 2])
We keep 3.96e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([11558, 2])
We keep 6.64e+06/8.02e+07 =  8% of the original kernel matrix.

torch.Size([17086, 2])
We keep 3.53e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([47222, 2])
We keep 4.37e+07/1.00e+09 =  4% of the original kernel matrix.

torch.Size([35289, 2])
We keep 9.66e+06/5.55e+08 =  1% of the original kernel matrix.

torch.Size([61700, 2])
We keep 3.49e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([40539, 2])
We keep 1.07e+07/6.63e+08 =  1% of the original kernel matrix.

torch.Size([8054, 2])
We keep 8.33e+05/1.67e+07 =  4% of the original kernel matrix.

torch.Size([14560, 2])
We keep 1.90e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([18116, 2])
We keep 3.21e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([22209, 2])
We keep 4.06e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([36583, 2])
We keep 1.84e+07/6.21e+08 =  2% of the original kernel matrix.

torch.Size([31370, 2])
We keep 7.93e+06/4.37e+08 =  1% of the original kernel matrix.

torch.Size([98468, 2])
We keep 1.51e+08/4.01e+09 =  3% of the original kernel matrix.

torch.Size([51686, 2])
We keep 1.63e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([32358, 2])
We keep 1.08e+07/4.70e+08 =  2% of the original kernel matrix.

torch.Size([29808, 2])
We keep 7.03e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([44041, 2])
We keep 1.38e+07/7.65e+08 =  1% of the original kernel matrix.

torch.Size([34750, 2])
We keep 8.23e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([21006, 2])
We keep 5.22e+07/4.49e+08 = 11% of the original kernel matrix.

torch.Size([22375, 2])
We keep 6.95e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([17421, 2])
We keep 2.95e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([21896, 2])
We keep 3.59e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([1253, 2])
We keep 2.79e+04/2.47e+05 = 11% of the original kernel matrix.

torch.Size([7063, 2])
We keep 4.45e+05/8.72e+06 =  5% of the original kernel matrix.

torch.Size([7807, 2])
We keep 9.24e+05/1.48e+07 =  6% of the original kernel matrix.

torch.Size([14404, 2])
We keep 1.77e+06/6.76e+07 =  2% of the original kernel matrix.

torch.Size([49149, 2])
We keep 1.52e+07/9.08e+08 =  1% of the original kernel matrix.

torch.Size([36715, 2])
We keep 8.92e+06/5.29e+08 =  1% of the original kernel matrix.

torch.Size([20333, 2])
We keep 5.46e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([23108, 2])
We keep 4.74e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([29729, 2])
We keep 6.67e+06/3.29e+08 =  2% of the original kernel matrix.

torch.Size([28938, 2])
We keep 5.93e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([1981, 2])
We keep 4.44e+04/5.03e+05 =  8% of the original kernel matrix.

torch.Size([8440, 2])
We keep 5.61e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([5786, 2])
We keep 4.29e+05/6.98e+06 =  6% of the original kernel matrix.

torch.Size([12665, 2])
We keep 1.39e+06/4.64e+07 =  3% of the original kernel matrix.

torch.Size([67999, 2])
We keep 5.59e+07/2.21e+09 =  2% of the original kernel matrix.

torch.Size([42024, 2])
We keep 1.34e+07/8.26e+08 =  1% of the original kernel matrix.

torch.Size([21866, 2])
We keep 5.36e+06/1.93e+08 =  2% of the original kernel matrix.

torch.Size([24830, 2])
We keep 5.01e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([13380, 2])
We keep 2.01e+06/5.50e+07 =  3% of the original kernel matrix.

torch.Size([18853, 2])
We keep 2.98e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([1817, 2])
We keep 4.55e+04/5.64e+05 =  8% of the original kernel matrix.

torch.Size([8075, 2])
We keep 5.72e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([2151, 2])
We keep 7.99e+04/8.21e+05 =  9% of the original kernel matrix.

torch.Size([8511, 2])
We keep 6.63e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([17117, 2])
We keep 3.26e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([21657, 2])
We keep 3.77e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([21781, 2])
We keep 5.38e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([24506, 2])
We keep 4.79e+06/2.37e+08 =  2% of the original kernel matrix.

torch.Size([42860, 2])
We keep 4.37e+07/1.03e+09 =  4% of the original kernel matrix.

torch.Size([33542, 2])
We keep 9.74e+06/5.63e+08 =  1% of the original kernel matrix.

torch.Size([2292, 2])
We keep 9.11e+04/1.10e+06 =  8% of the original kernel matrix.

torch.Size([8745, 2])
We keep 7.04e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([34470, 2])
We keep 1.72e+08/1.70e+09 = 10% of the original kernel matrix.

torch.Size([28431, 2])
We keep 1.21e+07/7.22e+08 =  1% of the original kernel matrix.

torch.Size([4267, 2])
We keep 3.38e+05/4.36e+06 =  7% of the original kernel matrix.

torch.Size([10946, 2])
We keep 1.19e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([8141, 2])
We keep 1.08e+06/2.08e+07 =  5% of the original kernel matrix.

torch.Size([14379, 2])
We keep 2.09e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([209257, 2])
We keep 3.19e+08/2.12e+10 =  1% of the original kernel matrix.

torch.Size([77969, 2])
We keep 3.49e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([9250, 2])
We keep 2.72e+06/3.35e+07 =  8% of the original kernel matrix.

torch.Size([15738, 2])
We keep 2.22e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([28208, 2])
We keep 1.24e+07/3.58e+08 =  3% of the original kernel matrix.

torch.Size([27394, 2])
We keep 6.07e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([95638, 2])
We keep 6.83e+07/3.45e+09 =  1% of the original kernel matrix.

torch.Size([50571, 2])
We keep 1.59e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([37029, 2])
We keep 1.31e+07/5.51e+08 =  2% of the original kernel matrix.

torch.Size([31995, 2])
We keep 7.44e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([45921, 2])
We keep 2.91e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([35455, 2])
We keep 9.71e+06/5.63e+08 =  1% of the original kernel matrix.

torch.Size([103927, 2])
We keep 8.14e+07/4.07e+09 =  1% of the original kernel matrix.

torch.Size([52990, 2])
We keep 1.67e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([2282, 2])
We keep 1.04e+05/1.01e+06 = 10% of the original kernel matrix.

torch.Size([8578, 2])
We keep 7.16e+05/1.76e+07 =  4% of the original kernel matrix.

torch.Size([19899, 2])
We keep 4.44e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([23625, 2])
We keep 4.56e+06/2.19e+08 =  2% of the original kernel matrix.

torch.Size([13735, 2])
We keep 1.94e+06/5.60e+07 =  3% of the original kernel matrix.

torch.Size([19243, 2])
We keep 2.98e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([110130, 2])
We keep 8.20e+07/5.10e+09 =  1% of the original kernel matrix.

torch.Size([54962, 2])
We keep 1.91e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([88799, 2])
We keep 7.52e+07/3.83e+09 =  1% of the original kernel matrix.

torch.Size([48831, 2])
We keep 1.67e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([15392, 2])
We keep 4.54e+06/1.22e+08 =  3% of the original kernel matrix.

torch.Size([19993, 2])
We keep 4.13e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([24588, 2])
We keep 5.39e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([26005, 2])
We keep 5.16e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([5643, 2])
We keep 3.30e+05/6.54e+06 =  5% of the original kernel matrix.

torch.Size([12730, 2])
We keep 1.34e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([2418, 2])
We keep 3.77e+05/2.97e+06 = 12% of the original kernel matrix.

torch.Size([8237, 2])
We keep 1.06e+06/3.02e+07 =  3% of the original kernel matrix.

torch.Size([8559, 2])
We keep 2.90e+06/2.67e+07 = 10% of the original kernel matrix.

torch.Size([15073, 2])
We keep 2.22e+06/9.06e+07 =  2% of the original kernel matrix.

torch.Size([8246, 2])
We keep 9.40e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([14618, 2])
We keep 2.00e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([114220, 2])
We keep 9.15e+07/5.34e+09 =  1% of the original kernel matrix.

torch.Size([55872, 2])
We keep 1.94e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([124710, 2])
We keep 2.06e+08/7.37e+09 =  2% of the original kernel matrix.

torch.Size([58797, 2])
We keep 2.23e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([3393, 2])
We keep 2.20e+05/2.81e+06 =  7% of the original kernel matrix.

torch.Size([10090, 2])
We keep 9.78e+05/2.94e+07 =  3% of the original kernel matrix.

torch.Size([12084, 2])
We keep 1.46e+06/4.03e+07 =  3% of the original kernel matrix.

torch.Size([18031, 2])
We keep 2.54e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([9307, 2])
We keep 8.25e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([15768, 2])
We keep 2.02e+06/7.99e+07 =  2% of the original kernel matrix.

torch.Size([166877, 2])
We keep 2.94e+08/1.67e+10 =  1% of the original kernel matrix.

torch.Size([68275, 2])
We keep 3.23e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([7524, 2])
We keep 6.17e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([14194, 2])
We keep 1.75e+06/6.37e+07 =  2% of the original kernel matrix.

torch.Size([8286, 2])
We keep 8.73e+05/1.74e+07 =  5% of the original kernel matrix.

torch.Size([14719, 2])
We keep 1.93e+06/7.31e+07 =  2% of the original kernel matrix.

torch.Size([32963, 2])
We keep 1.13e+07/4.64e+08 =  2% of the original kernel matrix.

torch.Size([30222, 2])
We keep 6.92e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([54940, 2])
We keep 1.91e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([38562, 2])
We keep 9.94e+06/5.90e+08 =  1% of the original kernel matrix.

torch.Size([6301, 2])
We keep 9.30e+05/1.24e+07 =  7% of the original kernel matrix.

torch.Size([12937, 2])
We keep 1.71e+06/6.18e+07 =  2% of the original kernel matrix.

torch.Size([23300, 2])
We keep 8.18e+06/2.24e+08 =  3% of the original kernel matrix.

torch.Size([24548, 2])
We keep 5.00e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([2472, 2])
We keep 1.17e+05/1.28e+06 =  9% of the original kernel matrix.

torch.Size([8924, 2])
We keep 7.46e+05/1.99e+07 =  3% of the original kernel matrix.

time for making ranges is 2.1055002212524414
Sorting X and nu_X
time for sorting X is 0.06176042556762695
Sorting Z and nu_Z
time for sorting Z is 0.0002570152282714844
Starting Optim
sum tnu_Z before tensor(16602551., device='cuda:0')
c= tensor(272.9326, device='cuda:0')
c= tensor(34870.0312, device='cuda:0')
c= tensor(37201.7305, device='cuda:0')
c= tensor(38121.1992, device='cuda:0')
c= tensor(524546.3125, device='cuda:0')
c= tensor(559295.5625, device='cuda:0')
c= tensor(760798.1875, device='cuda:0')
c= tensor(831083.1250, device='cuda:0')
c= tensor(839043.8750, device='cuda:0')
c= tensor(1692002.6250, device='cuda:0')
c= tensor(1697219.2500, device='cuda:0')
c= tensor(3401642., device='cuda:0')
c= tensor(3409038., device='cuda:0')
c= tensor(6059640., device='cuda:0')
c= tensor(6116726.5000, device='cuda:0')
c= tensor(6167112., device='cuda:0')
c= tensor(6381631., device='cuda:0')
c= tensor(6863342., device='cuda:0')
c= tensor(8599049., device='cuda:0')
c= tensor(9927896., device='cuda:0')
c= tensor(9930423., device='cuda:0')
c= tensor(15566450., device='cuda:0')
c= tensor(15583416., device='cuda:0')
c= tensor(15599517., device='cuda:0')
c= tensor(16633908., device='cuda:0')
c= tensor(17087706., device='cuda:0')
c= tensor(17621618., device='cuda:0')
c= tensor(17703954., device='cuda:0')
c= tensor(18165268., device='cuda:0')
c= tensor(1.5193e+08, device='cuda:0')
c= tensor(1.5194e+08, device='cuda:0')
c= tensor(1.8571e+08, device='cuda:0')
c= tensor(1.8572e+08, device='cuda:0')
c= tensor(1.8573e+08, device='cuda:0')
c= tensor(1.8573e+08, device='cuda:0')
c= tensor(1.8621e+08, device='cuda:0')
c= tensor(1.8658e+08, device='cuda:0')
c= tensor(1.8659e+08, device='cuda:0')
c= tensor(1.8659e+08, device='cuda:0')
c= tensor(1.8659e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8661e+08, device='cuda:0')
c= tensor(1.8661e+08, device='cuda:0')
c= tensor(1.8662e+08, device='cuda:0')
c= tensor(1.8663e+08, device='cuda:0')
c= tensor(1.8663e+08, device='cuda:0')
c= tensor(1.8663e+08, device='cuda:0')
c= tensor(1.8664e+08, device='cuda:0')
c= tensor(1.8664e+08, device='cuda:0')
c= tensor(1.8665e+08, device='cuda:0')
c= tensor(1.8665e+08, device='cuda:0')
c= tensor(1.8665e+08, device='cuda:0')
c= tensor(1.8665e+08, device='cuda:0')
c= tensor(1.8665e+08, device='cuda:0')
c= tensor(1.8667e+08, device='cuda:0')
c= tensor(1.8668e+08, device='cuda:0')
c= tensor(1.8669e+08, device='cuda:0')
c= tensor(1.8669e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8671e+08, device='cuda:0')
c= tensor(1.8671e+08, device='cuda:0')
c= tensor(1.8671e+08, device='cuda:0')
c= tensor(1.8671e+08, device='cuda:0')
c= tensor(1.8672e+08, device='cuda:0')
c= tensor(1.8672e+08, device='cuda:0')
c= tensor(1.8672e+08, device='cuda:0')
c= tensor(1.8672e+08, device='cuda:0')
c= tensor(1.8673e+08, device='cuda:0')
c= tensor(1.8673e+08, device='cuda:0')
c= tensor(1.8673e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8676e+08, device='cuda:0')
c= tensor(1.8677e+08, device='cuda:0')
c= tensor(1.8677e+08, device='cuda:0')
c= tensor(1.8678e+08, device='cuda:0')
c= tensor(1.8679e+08, device='cuda:0')
c= tensor(1.8680e+08, device='cuda:0')
c= tensor(1.8680e+08, device='cuda:0')
c= tensor(1.8680e+08, device='cuda:0')
c= tensor(1.8680e+08, device='cuda:0')
c= tensor(1.8680e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8682e+08, device='cuda:0')
c= tensor(1.8682e+08, device='cuda:0')
c= tensor(1.8682e+08, device='cuda:0')
c= tensor(1.8682e+08, device='cuda:0')
c= tensor(1.8683e+08, device='cuda:0')
c= tensor(1.8683e+08, device='cuda:0')
c= tensor(1.8683e+08, device='cuda:0')
c= tensor(1.8683e+08, device='cuda:0')
c= tensor(1.8683e+08, device='cuda:0')
c= tensor(1.8684e+08, device='cuda:0')
c= tensor(1.8684e+08, device='cuda:0')
c= tensor(1.8684e+08, device='cuda:0')
c= tensor(1.8684e+08, device='cuda:0')
c= tensor(1.8684e+08, device='cuda:0')
c= tensor(1.8687e+08, device='cuda:0')
c= tensor(1.8687e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8689e+08, device='cuda:0')
c= tensor(1.8689e+08, device='cuda:0')
c= tensor(1.8689e+08, device='cuda:0')
c= tensor(1.8689e+08, device='cuda:0')
c= tensor(1.8690e+08, device='cuda:0')
c= tensor(1.8693e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8695e+08, device='cuda:0')
c= tensor(1.8695e+08, device='cuda:0')
c= tensor(1.8696e+08, device='cuda:0')
c= tensor(1.8696e+08, device='cuda:0')
c= tensor(1.8698e+08, device='cuda:0')
c= tensor(1.8698e+08, device='cuda:0')
c= tensor(1.8699e+08, device='cuda:0')
c= tensor(1.8699e+08, device='cuda:0')
c= tensor(1.8699e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8702e+08, device='cuda:0')
c= tensor(1.8702e+08, device='cuda:0')
c= tensor(1.8702e+08, device='cuda:0')
c= tensor(1.8703e+08, device='cuda:0')
c= tensor(1.8703e+08, device='cuda:0')
c= tensor(1.8703e+08, device='cuda:0')
c= tensor(1.8704e+08, device='cuda:0')
c= tensor(1.8704e+08, device='cuda:0')
c= tensor(1.8704e+08, device='cuda:0')
c= tensor(1.8704e+08, device='cuda:0')
c= tensor(1.8705e+08, device='cuda:0')
c= tensor(1.8705e+08, device='cuda:0')
c= tensor(1.8705e+08, device='cuda:0')
c= tensor(1.8706e+08, device='cuda:0')
c= tensor(1.8706e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8712e+08, device='cuda:0')
c= tensor(1.8712e+08, device='cuda:0')
c= tensor(1.8713e+08, device='cuda:0')
c= tensor(1.8713e+08, device='cuda:0')
c= tensor(1.8714e+08, device='cuda:0')
c= tensor(1.8715e+08, device='cuda:0')
c= tensor(1.8715e+08, device='cuda:0')
c= tensor(1.8715e+08, device='cuda:0')
c= tensor(1.8715e+08, device='cuda:0')
c= tensor(1.8716e+08, device='cuda:0')
c= tensor(1.8716e+08, device='cuda:0')
c= tensor(1.8717e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8719e+08, device='cuda:0')
c= tensor(1.8719e+08, device='cuda:0')
c= tensor(1.8719e+08, device='cuda:0')
c= tensor(1.8719e+08, device='cuda:0')
c= tensor(1.8719e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8721e+08, device='cuda:0')
c= tensor(1.8721e+08, device='cuda:0')
c= tensor(1.8721e+08, device='cuda:0')
c= tensor(1.8721e+08, device='cuda:0')
c= tensor(1.8722e+08, device='cuda:0')
c= tensor(1.8722e+08, device='cuda:0')
c= tensor(1.8723e+08, device='cuda:0')
c= tensor(1.8846e+08, device='cuda:0')
c= tensor(1.8847e+08, device='cuda:0')
c= tensor(1.8848e+08, device='cuda:0')
c= tensor(1.8848e+08, device='cuda:0')
c= tensor(1.8848e+08, device='cuda:0')
c= tensor(1.8860e+08, device='cuda:0')
c= tensor(1.9756e+08, device='cuda:0')
c= tensor(1.9756e+08, device='cuda:0')
c= tensor(1.9771e+08, device='cuda:0')
c= tensor(1.9807e+08, device='cuda:0')
c= tensor(1.9808e+08, device='cuda:0')
c= tensor(2.1298e+08, device='cuda:0')
c= tensor(2.1298e+08, device='cuda:0')
c= tensor(2.1299e+08, device='cuda:0')
c= tensor(2.1379e+08, device='cuda:0')
c= tensor(2.1858e+08, device='cuda:0')
c= tensor(2.1858e+08, device='cuda:0')
c= tensor(2.1865e+08, device='cuda:0')
c= tensor(2.2032e+08, device='cuda:0')
c= tensor(2.2660e+08, device='cuda:0')
c= tensor(2.2674e+08, device='cuda:0')
c= tensor(2.2694e+08, device='cuda:0')
c= tensor(2.2710e+08, device='cuda:0')
c= tensor(2.2714e+08, device='cuda:0')
c= tensor(2.2715e+08, device='cuda:0')
c= tensor(2.3231e+08, device='cuda:0')
c= tensor(2.3232e+08, device='cuda:0')
c= tensor(2.3232e+08, device='cuda:0')
c= tensor(2.3248e+08, device='cuda:0')
c= tensor(2.3251e+08, device='cuda:0')
c= tensor(2.4053e+08, device='cuda:0')
c= tensor(2.4096e+08, device='cuda:0')
c= tensor(2.4096e+08, device='cuda:0')
c= tensor(2.4098e+08, device='cuda:0')
c= tensor(2.4100e+08, device='cuda:0')
c= tensor(2.4102e+08, device='cuda:0')
c= tensor(2.4152e+08, device='cuda:0')
c= tensor(2.4172e+08, device='cuda:0')
c= tensor(2.4176e+08, device='cuda:0')
c= tensor(2.4176e+08, device='cuda:0')
c= tensor(2.4177e+08, device='cuda:0')
c= tensor(2.4196e+08, device='cuda:0')
c= tensor(2.4225e+08, device='cuda:0')
c= tensor(2.4227e+08, device='cuda:0')
c= tensor(2.4228e+08, device='cuda:0')
c= tensor(2.5330e+08, device='cuda:0')
c= tensor(2.5331e+08, device='cuda:0')
c= tensor(2.5336e+08, device='cuda:0')
c= tensor(2.5419e+08, device='cuda:0')
c= tensor(2.5419e+08, device='cuda:0')
c= tensor(2.5434e+08, device='cuda:0')
c= tensor(2.6001e+08, device='cuda:0')
c= tensor(2.6606e+08, device='cuda:0')
c= tensor(2.6607e+08, device='cuda:0')
c= tensor(2.6608e+08, device='cuda:0')
c= tensor(2.6609e+08, device='cuda:0')
c= tensor(2.6609e+08, device='cuda:0')
c= tensor(2.6626e+08, device='cuda:0')
c= tensor(2.6628e+08, device='cuda:0')
c= tensor(2.6640e+08, device='cuda:0')
c= tensor(2.7063e+08, device='cuda:0')
c= tensor(2.7090e+08, device='cuda:0')
c= tensor(2.7091e+08, device='cuda:0')
c= tensor(2.7092e+08, device='cuda:0')
c= tensor(2.7397e+08, device='cuda:0')
c= tensor(2.7401e+08, device='cuda:0')
c= tensor(2.7401e+08, device='cuda:0')
c= tensor(2.7409e+08, device='cuda:0')
c= tensor(2.8764e+08, device='cuda:0')
c= tensor(2.8765e+08, device='cuda:0')
c= tensor(2.8868e+08, device='cuda:0')
c= tensor(2.8868e+08, device='cuda:0')
c= tensor(2.8903e+08, device='cuda:0')
c= tensor(2.8918e+08, device='cuda:0')
c= tensor(2.9295e+08, device='cuda:0')
c= tensor(2.9333e+08, device='cuda:0')
c= tensor(2.9333e+08, device='cuda:0')
c= tensor(2.9433e+08, device='cuda:0')
c= tensor(2.9495e+08, device='cuda:0')
c= tensor(2.9495e+08, device='cuda:0')
c= tensor(2.9508e+08, device='cuda:0')
c= tensor(2.9791e+08, device='cuda:0')
c= tensor(3.0215e+08, device='cuda:0')
c= tensor(3.0588e+08, device='cuda:0')
c= tensor(3.0590e+08, device='cuda:0')
c= tensor(3.0590e+08, device='cuda:0')
c= tensor(3.0592e+08, device='cuda:0')
c= tensor(3.0625e+08, device='cuda:0')
c= tensor(3.0625e+08, device='cuda:0')
c= tensor(3.0625e+08, device='cuda:0')
c= tensor(3.0657e+08, device='cuda:0')
c= tensor(3.0730e+08, device='cuda:0')
c= tensor(3.0785e+08, device='cuda:0')
c= tensor(3.0785e+08, device='cuda:0')
c= tensor(3.0787e+08, device='cuda:0')
c= tensor(3.0791e+08, device='cuda:0')
c= tensor(3.0792e+08, device='cuda:0')
c= tensor(3.0793e+08, device='cuda:0')
c= tensor(3.0793e+08, device='cuda:0')
c= tensor(3.1452e+08, device='cuda:0')
c= tensor(3.1459e+08, device='cuda:0')
c= tensor(3.1462e+08, device='cuda:0')
c= tensor(3.1473e+08, device='cuda:0')
c= tensor(3.1473e+08, device='cuda:0')
c= tensor(3.2551e+08, device='cuda:0')
c= tensor(3.2551e+08, device='cuda:0')
c= tensor(3.2596e+08, device='cuda:0')
c= tensor(3.2596e+08, device='cuda:0')
c= tensor(3.2596e+08, device='cuda:0')
c= tensor(3.2596e+08, device='cuda:0')
c= tensor(3.2605e+08, device='cuda:0')
c= tensor(3.2621e+08, device='cuda:0')
c= tensor(3.2632e+08, device='cuda:0')
c= tensor(3.2633e+08, device='cuda:0')
c= tensor(3.2633e+08, device='cuda:0')
c= tensor(3.3025e+08, device='cuda:0')
c= tensor(3.3037e+08, device='cuda:0')
c= tensor(3.3052e+08, device='cuda:0')
c= tensor(3.3109e+08, device='cuda:0')
c= tensor(3.3204e+08, device='cuda:0')
c= tensor(3.3204e+08, device='cuda:0')
c= tensor(3.3205e+08, device='cuda:0')
c= tensor(3.3206e+08, device='cuda:0')
c= tensor(3.3206e+08, device='cuda:0')
c= tensor(3.3206e+08, device='cuda:0')
c= tensor(3.3211e+08, device='cuda:0')
c= tensor(3.3211e+08, device='cuda:0')
c= tensor(3.3211e+08, device='cuda:0')
c= tensor(3.3212e+08, device='cuda:0')
c= tensor(3.3212e+08, device='cuda:0')
c= tensor(3.5171e+08, device='cuda:0')
c= tensor(3.5188e+08, device='cuda:0')
c= tensor(3.5218e+08, device='cuda:0')
c= tensor(3.5243e+08, device='cuda:0')
c= tensor(3.5243e+08, device='cuda:0')
c= tensor(3.5244e+08, device='cuda:0')
c= tensor(4.2257e+08, device='cuda:0')
c= tensor(4.3814e+08, device='cuda:0')
c= tensor(4.3825e+08, device='cuda:0')
c= tensor(4.3832e+08, device='cuda:0')
c= tensor(4.3832e+08, device='cuda:0')
c= tensor(4.3932e+08, device='cuda:0')
c= tensor(4.3984e+08, device='cuda:0')
c= tensor(4.4282e+08, device='cuda:0')
c= tensor(4.4282e+08, device='cuda:0')
c= tensor(4.4371e+08, device='cuda:0')
c= tensor(4.4772e+08, device='cuda:0')
c= tensor(4.4780e+08, device='cuda:0')
c= tensor(4.4780e+08, device='cuda:0')
c= tensor(4.4783e+08, device='cuda:0')
c= tensor(4.4787e+08, device='cuda:0')
c= tensor(4.4787e+08, device='cuda:0')
c= tensor(4.4929e+08, device='cuda:0')
c= tensor(4.4939e+08, device='cuda:0')
c= tensor(4.4939e+08, device='cuda:0')
c= tensor(4.4943e+08, device='cuda:0')
c= tensor(4.4949e+08, device='cuda:0')
c= tensor(4.4949e+08, device='cuda:0')
c= tensor(4.5102e+08, device='cuda:0')
c= tensor(4.5180e+08, device='cuda:0')
c= tensor(4.5223e+08, device='cuda:0')
c= tensor(4.5303e+08, device='cuda:0')
c= tensor(4.5333e+08, device='cuda:0')
c= tensor(4.5335e+08, device='cuda:0')
c= tensor(4.5342e+08, device='cuda:0')
c= tensor(4.5353e+08, device='cuda:0')
c= tensor(4.5441e+08, device='cuda:0')
c= tensor(4.5441e+08, device='cuda:0')
c= tensor(4.5770e+08, device='cuda:0')
c= tensor(4.6417e+08, device='cuda:0')
c= tensor(4.6448e+08, device='cuda:0')
c= tensor(4.6473e+08, device='cuda:0')
c= tensor(4.6737e+08, device='cuda:0')
c= tensor(4.6738e+08, device='cuda:0')
c= tensor(4.6738e+08, device='cuda:0')
c= tensor(4.6759e+08, device='cuda:0')
c= tensor(4.6845e+08, device='cuda:0')
c= tensor(4.6888e+08, device='cuda:0')
c= tensor(4.9050e+08, device='cuda:0')
c= tensor(4.9613e+08, device='cuda:0')
c= tensor(4.9677e+08, device='cuda:0')
c= tensor(4.9685e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(5.0065e+08, device='cuda:0')
c= tensor(5.0066e+08, device='cuda:0')
c= tensor(5.0066e+08, device='cuda:0')
c= tensor(5.0072e+08, device='cuda:0')
c= tensor(5.0115e+08, device='cuda:0')
c= tensor(5.0125e+08, device='cuda:0')
c= tensor(5.0193e+08, device='cuda:0')
c= tensor(5.0198e+08, device='cuda:0')
c= tensor(5.0204e+08, device='cuda:0')
c= tensor(5.0204e+08, device='cuda:0')
c= tensor(5.0210e+08, device='cuda:0')
c= tensor(5.0216e+08, device='cuda:0')
c= tensor(5.0245e+08, device='cuda:0')
c= tensor(5.0246e+08, device='cuda:0')
c= tensor(5.0337e+08, device='cuda:0')
c= tensor(5.0337e+08, device='cuda:0')
c= tensor(5.0343e+08, device='cuda:0')
c= tensor(5.0343e+08, device='cuda:0')
c= tensor(5.0351e+08, device='cuda:0')
c= tensor(5.0351e+08, device='cuda:0')
c= tensor(5.0367e+08, device='cuda:0')
c= tensor(5.0368e+08, device='cuda:0')
c= tensor(5.0372e+08, device='cuda:0')
c= tensor(5.0404e+08, device='cuda:0')
c= tensor(5.0974e+08, device='cuda:0')
c= tensor(5.0975e+08, device='cuda:0')
c= tensor(5.0977e+08, device='cuda:0')
c= tensor(5.1156e+08, device='cuda:0')
c= tensor(5.1157e+08, device='cuda:0')
c= tensor(5.2653e+08, device='cuda:0')
c= tensor(5.2653e+08, device='cuda:0')
c= tensor(5.2697e+08, device='cuda:0')
c= tensor(5.2831e+08, device='cuda:0')
c= tensor(5.2831e+08, device='cuda:0')
c= tensor(5.3068e+08, device='cuda:0')
c= tensor(5.3087e+08, device='cuda:0')
c= tensor(5.4024e+08, device='cuda:0')
c= tensor(5.4025e+08, device='cuda:0')
c= tensor(5.4040e+08, device='cuda:0')
c= tensor(5.4040e+08, device='cuda:0')
c= tensor(5.4041e+08, device='cuda:0')
c= tensor(5.4042e+08, device='cuda:0')
c= tensor(5.4057e+08, device='cuda:0')
c= tensor(5.4059e+08, device='cuda:0')
c= tensor(5.4088e+08, device='cuda:0')
c= tensor(5.4089e+08, device='cuda:0')
c= tensor(5.4090e+08, device='cuda:0')
c= tensor(5.4090e+08, device='cuda:0')
c= tensor(5.4265e+08, device='cuda:0')
c= tensor(5.4269e+08, device='cuda:0')
c= tensor(5.4369e+08, device='cuda:0')
c= tensor(5.4473e+08, device='cuda:0')
c= tensor(5.4475e+08, device='cuda:0')
c= tensor(5.4475e+08, device='cuda:0')
c= tensor(5.4477e+08, device='cuda:0')
c= tensor(5.9254e+08, device='cuda:0')
c= tensor(5.9255e+08, device='cuda:0')
c= tensor(5.9257e+08, device='cuda:0')
c= tensor(5.9341e+08, device='cuda:0')
c= tensor(5.9372e+08, device='cuda:0')
c= tensor(5.9373e+08, device='cuda:0')
c= tensor(5.9373e+08, device='cuda:0')
c= tensor(6.0585e+08, device='cuda:0')
c= tensor(6.0587e+08, device='cuda:0')
c= tensor(6.0604e+08, device='cuda:0')
c= tensor(6.0623e+08, device='cuda:0')
c= tensor(6.0653e+08, device='cuda:0')
c= tensor(6.0760e+08, device='cuda:0')
c= tensor(6.0929e+08, device='cuda:0')
c= tensor(6.0947e+08, device='cuda:0')
c= tensor(6.0948e+08, device='cuda:0')
c= tensor(6.0974e+08, device='cuda:0')
c= tensor(6.0975e+08, device='cuda:0')
c= tensor(6.0978e+08, device='cuda:0')
c= tensor(6.0983e+08, device='cuda:0')
c= tensor(6.0983e+08, device='cuda:0')
c= tensor(6.1071e+08, device='cuda:0')
c= tensor(6.1072e+08, device='cuda:0')
c= tensor(6.1072e+08, device='cuda:0')
c= tensor(6.1074e+08, device='cuda:0')
c= tensor(6.1078e+08, device='cuda:0')
c= tensor(6.1260e+08, device='cuda:0')
c= tensor(6.1355e+08, device='cuda:0')
c= tensor(6.1358e+08, device='cuda:0')
c= tensor(6.1359e+08, device='cuda:0')
c= tensor(6.1369e+08, device='cuda:0')
c= tensor(6.1369e+08, device='cuda:0')
c= tensor(6.1370e+08, device='cuda:0')
c= tensor(6.1375e+08, device='cuda:0')
c= tensor(6.1669e+08, device='cuda:0')
c= tensor(6.1670e+08, device='cuda:0')
c= tensor(6.1670e+08, device='cuda:0')
c= tensor(6.1670e+08, device='cuda:0')
c= tensor(6.2230e+08, device='cuda:0')
c= tensor(6.3226e+08, device='cuda:0')
c= tensor(6.3232e+08, device='cuda:0')
c= tensor(6.3232e+08, device='cuda:0')
c= tensor(6.3243e+08, device='cuda:0')
c= tensor(6.3316e+08, device='cuda:0')
c= tensor(6.3316e+08, device='cuda:0')
c= tensor(6.3322e+08, device='cuda:0')
c= tensor(6.3324e+08, device='cuda:0')
c= tensor(6.4101e+08, device='cuda:0')
c= tensor(6.4131e+08, device='cuda:0')
c= tensor(6.4226e+08, device='cuda:0')
c= tensor(6.4227e+08, device='cuda:0')
c= tensor(6.4229e+08, device='cuda:0')
c= tensor(6.4229e+08, device='cuda:0')
c= tensor(6.4235e+08, device='cuda:0')
c= tensor(6.4238e+08, device='cuda:0')
c= tensor(6.4272e+08, device='cuda:0')
c= tensor(6.4290e+08, device='cuda:0')
c= tensor(6.4633e+08, device='cuda:0')
c= tensor(6.4635e+08, device='cuda:0')
c= tensor(6.4636e+08, device='cuda:0')
c= tensor(6.4639e+08, device='cuda:0')
c= tensor(7.5965e+08, device='cuda:0')
c= tensor(7.5971e+08, device='cuda:0')
c= tensor(7.5972e+08, device='cuda:0')
c= tensor(7.5972e+08, device='cuda:0')
c= tensor(7.5978e+08, device='cuda:0')
c= tensor(7.5978e+08, device='cuda:0')
c= tensor(7.5995e+08, device='cuda:0')
c= tensor(7.5996e+08, device='cuda:0')
c= tensor(7.5997e+08, device='cuda:0')
c= tensor(7.5999e+08, device='cuda:0')
c= tensor(7.5999e+08, device='cuda:0')
c= tensor(7.5999e+08, device='cuda:0')
c= tensor(7.6151e+08, device='cuda:0')
c= tensor(7.7132e+08, device='cuda:0')
c= tensor(7.7263e+08, device='cuda:0')
c= tensor(7.7271e+08, device='cuda:0')
c= tensor(7.7278e+08, device='cuda:0')
c= tensor(7.7280e+08, device='cuda:0')
c= tensor(7.7281e+08, device='cuda:0')
c= tensor(7.7315e+08, device='cuda:0')
c= tensor(7.7341e+08, device='cuda:0')
c= tensor(7.7519e+08, device='cuda:0')
c= tensor(7.7521e+08, device='cuda:0')
c= tensor(7.8998e+08, device='cuda:0')
c= tensor(7.9001e+08, device='cuda:0')
c= tensor(7.9019e+08, device='cuda:0')
c= tensor(7.9426e+08, device='cuda:0')
c= tensor(7.9445e+08, device='cuda:0')
c= tensor(7.9607e+08, device='cuda:0')
c= tensor(7.9750e+08, device='cuda:0')
c= tensor(7.9767e+08, device='cuda:0')
c= tensor(7.9769e+08, device='cuda:0')
c= tensor(7.9772e+08, device='cuda:0')
c= tensor(7.9787e+08, device='cuda:0')
c= tensor(7.9787e+08, device='cuda:0')
c= tensor(7.9868e+08, device='cuda:0')
c= tensor(8.2739e+08, device='cuda:0')
c= tensor(8.3136e+08, device='cuda:0')
c= tensor(8.3226e+08, device='cuda:0')
c= tensor(8.3234e+08, device='cuda:0')
c= tensor(8.3247e+08, device='cuda:0')
c= tensor(8.3256e+08, device='cuda:0')
c= tensor(8.3261e+08, device='cuda:0')
c= tensor(8.3373e+08, device='cuda:0')
c= tensor(8.3647e+08, device='cuda:0')
c= tensor(8.3647e+08, device='cuda:0')
c= tensor(8.3871e+08, device='cuda:0')
c= tensor(8.4109e+08, device='cuda:0')
c= tensor(8.4114e+08, device='cuda:0')
c= tensor(8.4114e+08, device='cuda:0')
c= tensor(8.4149e+08, device='cuda:0')
c= tensor(8.4202e+08, device='cuda:0')
c= tensor(8.4203e+08, device='cuda:0')
c= tensor(8.5558e+08, device='cuda:0')
c= tensor(8.5570e+08, device='cuda:0')
c= tensor(8.5574e+08, device='cuda:0')
c= tensor(8.5575e+08, device='cuda:0')
c= tensor(8.5576e+08, device='cuda:0')
c= tensor(8.5576e+08, device='cuda:0')
c= tensor(8.5576e+08, device='cuda:0')
c= tensor(8.5581e+08, device='cuda:0')
c= tensor(8.5594e+08, device='cuda:0')
c= tensor(1.1930e+09, device='cuda:0')
c= tensor(1.1931e+09, device='cuda:0')
c= tensor(1.1931e+09, device='cuda:0')
c= tensor(1.1931e+09, device='cuda:0')
c= tensor(1.1931e+09, device='cuda:0')
c= tensor(1.1932e+09, device='cuda:0')
c= tensor(1.1980e+09, device='cuda:0')
c= tensor(1.1980e+09, device='cuda:0')
c= tensor(1.2477e+09, device='cuda:0')
c= tensor(1.2477e+09, device='cuda:0')
c= tensor(1.2495e+09, device='cuda:0')
c= tensor(1.2496e+09, device='cuda:0')
c= tensor(1.2506e+09, device='cuda:0')
c= tensor(1.2609e+09, device='cuda:0')
c= tensor(1.2610e+09, device='cuda:0')
c= tensor(1.2610e+09, device='cuda:0')
c= tensor(1.2617e+09, device='cuda:0')
c= tensor(1.2617e+09, device='cuda:0')
c= tensor(1.2620e+09, device='cuda:0')
c= tensor(1.2622e+09, device='cuda:0')
c= tensor(1.2624e+09, device='cuda:0')
c= tensor(1.2627e+09, device='cuda:0')
c= tensor(1.2627e+09, device='cuda:0')
c= tensor(1.2641e+09, device='cuda:0')
c= tensor(1.2694e+09, device='cuda:0')
c= tensor(1.2694e+09, device='cuda:0')
c= tensor(1.2695e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2723e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2730e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2740e+09, device='cuda:0')
c= tensor(1.2740e+09, device='cuda:0')
c= tensor(1.2740e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2742e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2761e+09, device='cuda:0')
c= tensor(1.2761e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2766e+09, device='cuda:0')
c= tensor(1.2798e+09, device='cuda:0')
c= tensor(1.2800e+09, device='cuda:0')
c= tensor(1.2804e+09, device='cuda:0')
c= tensor(1.2831e+09, device='cuda:0')
c= tensor(1.2832e+09, device='cuda:0')
c= tensor(1.2832e+09, device='cuda:0')
c= tensor(1.2832e+09, device='cuda:0')
c= tensor(1.2836e+09, device='cuda:0')
c= tensor(1.2837e+09, device='cuda:0')
c= tensor(1.2838e+09, device='cuda:0')
c= tensor(1.2838e+09, device='cuda:0')
c= tensor(1.2838e+09, device='cuda:0')
c= tensor(1.2849e+09, device='cuda:0')
c= tensor(1.2850e+09, device='cuda:0')
c= tensor(1.2850e+09, device='cuda:0')
c= tensor(1.2850e+09, device='cuda:0')
c= tensor(1.2850e+09, device='cuda:0')
c= tensor(1.2851e+09, device='cuda:0')
c= tensor(1.2852e+09, device='cuda:0')
c= tensor(1.2862e+09, device='cuda:0')
c= tensor(1.2862e+09, device='cuda:0')
c= tensor(1.2945e+09, device='cuda:0')
c= tensor(1.2946e+09, device='cuda:0')
c= tensor(1.2946e+09, device='cuda:0')
c= tensor(1.3059e+09, device='cuda:0')
c= tensor(1.3060e+09, device='cuda:0')
c= tensor(1.3063e+09, device='cuda:0')
c= tensor(1.3080e+09, device='cuda:0')
c= tensor(1.3082e+09, device='cuda:0')
c= tensor(1.3089e+09, device='cuda:0')
c= tensor(1.3107e+09, device='cuda:0')
c= tensor(1.3107e+09, device='cuda:0')
c= tensor(1.3108e+09, device='cuda:0')
c= tensor(1.3108e+09, device='cuda:0')
c= tensor(1.3124e+09, device='cuda:0')
c= tensor(1.3144e+09, device='cuda:0')
c= tensor(1.3145e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3147e+09, device='cuda:0')
c= tensor(1.3147e+09, device='cuda:0')
c= tensor(1.3147e+09, device='cuda:0')
c= tensor(1.3169e+09, device='cuda:0')
c= tensor(1.3223e+09, device='cuda:0')
c= tensor(1.3223e+09, device='cuda:0')
c= tensor(1.3224e+09, device='cuda:0')
c= tensor(1.3224e+09, device='cuda:0')
c= tensor(1.3301e+09, device='cuda:0')
c= tensor(1.3301e+09, device='cuda:0')
c= tensor(1.3301e+09, device='cuda:0')
c= tensor(1.3303e+09, device='cuda:0')
c= tensor(1.3307e+09, device='cuda:0')
c= tensor(1.3307e+09, device='cuda:0')
c= tensor(1.3308e+09, device='cuda:0')
c= tensor(1.3308e+09, device='cuda:0')
memory (bytes)
3665731584
time for making loss 2 is 14.451125144958496
p0 True
it  0 : 947054080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 16% |
shape of L is 
torch.Size([])
memory (bytes)
3666120704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3666726912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  16716830000.0
relative error loss 12.561291
shape of L is 
torch.Size([])
memory (bytes)
3922087936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  7% |
memory (bytes)
3922132992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  16716715000.0
relative error loss 12.561205
shape of L is 
torch.Size([])
memory (bytes)
3924152320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3924176896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  16716187000.0
relative error loss 12.560808
shape of L is 
torch.Size([])
memory (bytes)
3925221376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
3925225472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  16712642000.0
relative error loss 12.558144
shape of L is 
torch.Size([])
memory (bytes)
3927203840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3927220224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  16674999000.0
relative error loss 12.529859
shape of L is 
torch.Size([])
memory (bytes)
3929350144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3929362432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  16302596000.0
relative error loss 12.25003
shape of L is 
torch.Size([])
memory (bytes)
3931463680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
3931463680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  14544846000.0
relative error loss 10.929228
shape of L is 
torch.Size([])
memory (bytes)
3933593600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
3933593600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  4089478100.0
relative error loss 3.0728986
shape of L is 
torch.Size([])
memory (bytes)
3935653888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
3935653888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  2525124000.0
relative error loss 1.8974183
shape of L is 
torch.Size([])
memory (bytes)
3937841152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
3937857536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1780135200.0
relative error loss 1.3376218
time to take a step is 256.38990235328674
it  1 : 1390508544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3939758080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3939999744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1780135200.0
relative error loss 1.3376218
shape of L is 
torch.Size([])
memory (bytes)
3942125568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3942125568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1576867700.0
relative error loss 1.1848834
shape of L is 
torch.Size([])
memory (bytes)
3944235008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
3944235008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1390404100.0
relative error loss 1.0447717
shape of L is 
torch.Size([])
memory (bytes)
3946364928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3946385408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1274520600.0
relative error loss 0.95769495
shape of L is 
torch.Size([])
memory (bytes)
3948527616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3948527616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  3074504200.0
relative error loss 2.3102312
shape of L is 
torch.Size([])
memory (bytes)
3950641152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3950641152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  1236944100.0
relative error loss 0.92945945
shape of L is 
torch.Size([])
memory (bytes)
3952689152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
3952689152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  1202300200.0
relative error loss 0.9034274
shape of L is 
torch.Size([])
memory (bytes)
3954929664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  8% |
memory (bytes)
3954929664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  1118209700.0
relative error loss 0.8402405
shape of L is 
torch.Size([])
memory (bytes)
3956838400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
3956838400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1074615000.0
relative error loss 0.8074828
shape of L is 
torch.Size([])
memory (bytes)
3959074816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
3959169024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  2392514800.0
relative error loss 1.7977736
shape of L is 
torch.Size([])
memory (bytes)
3961266176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  8% |
memory (bytes)
3961278464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  1004849200.0
relative error loss 0.75505966
time to take a step is 270.2176887989044
it  2 : 1390509056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
3963322368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3963322368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1004849200.0
relative error loss 0.75505966
shape of L is 
torch.Size([])
memory (bytes)
3965407232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
3965407232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  944734340.0
relative error loss 0.70988834
shape of L is 
torch.Size([])
memory (bytes)
3967619072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3967631360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  874816200.0
relative error loss 0.6573508
shape of L is 
torch.Size([])
memory (bytes)
3969667072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3969667072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  823190600.0
relative error loss 0.61855847
shape of L is 
torch.Size([])
memory (bytes)
3971670016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
3971891200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  781145900.0
relative error loss 0.58696544
shape of L is 
torch.Size([])
memory (bytes)
3974017024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3974017024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  764422800.0
relative error loss 0.5743994
shape of L is 
torch.Size([])
memory (bytes)
3976122368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3976122368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  717780160.0
relative error loss 0.5393514
shape of L is 
torch.Size([])
memory (bytes)
3978215424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
3978215424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  662826560.0
relative error loss 0.49805838
shape of L is 
torch.Size([])
memory (bytes)
3980427264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
3980427264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  600467900.0
relative error loss 0.4512011
shape of L is 
torch.Size([])
memory (bytes)
3982499840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  8% |
memory (bytes)
3982499840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  539684030.0
relative error loss 0.40552714
time to take a step is 244.92804741859436
it  3 : 1390508544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
3984494592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3984732160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  539684030.0
relative error loss 0.40552714
shape of L is 
torch.Size([])
memory (bytes)
3986849792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3986862080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  508384000.0
relative error loss 0.3820078
shape of L is 
torch.Size([])
memory (bytes)
3989012480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3989012480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  463782270.0
relative error loss 0.34849337
shape of L is 
torch.Size([])
memory (bytes)
3990925312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
3990925312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  401753660.0
relative error loss 0.30188408
shape of L is 
torch.Size([])
memory (bytes)
3993145344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3993145344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  373988670.0
relative error loss 0.28102103
shape of L is 
torch.Size([])
memory (bytes)
3995447296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3995447296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  343847230.0
relative error loss 0.25837228
shape of L is 
torch.Size([])
memory (bytes)
3997515776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3997515776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  319124220.0
relative error loss 0.239795
shape of L is 
torch.Size([])
memory (bytes)
3999604736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3999604736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  290118720.0
relative error loss 0.2179998
shape of L is 
torch.Size([])
memory (bytes)
4001857536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4001857536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  272062080.0
relative error loss 0.20443176
shape of L is 
torch.Size([])
memory (bytes)
4004028416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4004028416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  248075900.0
relative error loss 0.18640816
time to take a step is 241.9168312549591
c= tensor(272.9326, device='cuda:0')
c= tensor(34870.0312, device='cuda:0')
c= tensor(37201.7305, device='cuda:0')
c= tensor(38121.1992, device='cuda:0')
c= tensor(524546.3125, device='cuda:0')
c= tensor(559295.5625, device='cuda:0')
c= tensor(760798.1875, device='cuda:0')
c= tensor(831083.1250, device='cuda:0')
c= tensor(839043.8750, device='cuda:0')
c= tensor(1692002.6250, device='cuda:0')
c= tensor(1697219.2500, device='cuda:0')
c= tensor(3401642., device='cuda:0')
c= tensor(3409038., device='cuda:0')
c= tensor(6059640., device='cuda:0')
c= tensor(6116726.5000, device='cuda:0')
c= tensor(6167112., device='cuda:0')
c= tensor(6381631., device='cuda:0')
c= tensor(6863342., device='cuda:0')
c= tensor(8599049., device='cuda:0')
c= tensor(9927896., device='cuda:0')
c= tensor(9930423., device='cuda:0')
c= tensor(15566450., device='cuda:0')
c= tensor(15583416., device='cuda:0')
c= tensor(15599517., device='cuda:0')
c= tensor(16633908., device='cuda:0')
c= tensor(17087706., device='cuda:0')
c= tensor(17621618., device='cuda:0')
c= tensor(17703954., device='cuda:0')
c= tensor(18165268., device='cuda:0')
c= tensor(1.5193e+08, device='cuda:0')
c= tensor(1.5194e+08, device='cuda:0')
c= tensor(1.8571e+08, device='cuda:0')
c= tensor(1.8572e+08, device='cuda:0')
c= tensor(1.8573e+08, device='cuda:0')
c= tensor(1.8573e+08, device='cuda:0')
c= tensor(1.8621e+08, device='cuda:0')
c= tensor(1.8658e+08, device='cuda:0')
c= tensor(1.8659e+08, device='cuda:0')
c= tensor(1.8659e+08, device='cuda:0')
c= tensor(1.8659e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8660e+08, device='cuda:0')
c= tensor(1.8661e+08, device='cuda:0')
c= tensor(1.8661e+08, device='cuda:0')
c= tensor(1.8662e+08, device='cuda:0')
c= tensor(1.8663e+08, device='cuda:0')
c= tensor(1.8663e+08, device='cuda:0')
c= tensor(1.8663e+08, device='cuda:0')
c= tensor(1.8664e+08, device='cuda:0')
c= tensor(1.8664e+08, device='cuda:0')
c= tensor(1.8665e+08, device='cuda:0')
c= tensor(1.8665e+08, device='cuda:0')
c= tensor(1.8665e+08, device='cuda:0')
c= tensor(1.8665e+08, device='cuda:0')
c= tensor(1.8665e+08, device='cuda:0')
c= tensor(1.8667e+08, device='cuda:0')
c= tensor(1.8668e+08, device='cuda:0')
c= tensor(1.8669e+08, device='cuda:0')
c= tensor(1.8669e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8670e+08, device='cuda:0')
c= tensor(1.8671e+08, device='cuda:0')
c= tensor(1.8671e+08, device='cuda:0')
c= tensor(1.8671e+08, device='cuda:0')
c= tensor(1.8671e+08, device='cuda:0')
c= tensor(1.8672e+08, device='cuda:0')
c= tensor(1.8672e+08, device='cuda:0')
c= tensor(1.8672e+08, device='cuda:0')
c= tensor(1.8672e+08, device='cuda:0')
c= tensor(1.8673e+08, device='cuda:0')
c= tensor(1.8673e+08, device='cuda:0')
c= tensor(1.8673e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8674e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8675e+08, device='cuda:0')
c= tensor(1.8676e+08, device='cuda:0')
c= tensor(1.8677e+08, device='cuda:0')
c= tensor(1.8677e+08, device='cuda:0')
c= tensor(1.8678e+08, device='cuda:0')
c= tensor(1.8679e+08, device='cuda:0')
c= tensor(1.8680e+08, device='cuda:0')
c= tensor(1.8680e+08, device='cuda:0')
c= tensor(1.8680e+08, device='cuda:0')
c= tensor(1.8680e+08, device='cuda:0')
c= tensor(1.8680e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8681e+08, device='cuda:0')
c= tensor(1.8682e+08, device='cuda:0')
c= tensor(1.8682e+08, device='cuda:0')
c= tensor(1.8682e+08, device='cuda:0')
c= tensor(1.8682e+08, device='cuda:0')
c= tensor(1.8683e+08, device='cuda:0')
c= tensor(1.8683e+08, device='cuda:0')
c= tensor(1.8683e+08, device='cuda:0')
c= tensor(1.8683e+08, device='cuda:0')
c= tensor(1.8683e+08, device='cuda:0')
c= tensor(1.8684e+08, device='cuda:0')
c= tensor(1.8684e+08, device='cuda:0')
c= tensor(1.8684e+08, device='cuda:0')
c= tensor(1.8684e+08, device='cuda:0')
c= tensor(1.8684e+08, device='cuda:0')
c= tensor(1.8687e+08, device='cuda:0')
c= tensor(1.8687e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8688e+08, device='cuda:0')
c= tensor(1.8689e+08, device='cuda:0')
c= tensor(1.8689e+08, device='cuda:0')
c= tensor(1.8689e+08, device='cuda:0')
c= tensor(1.8689e+08, device='cuda:0')
c= tensor(1.8690e+08, device='cuda:0')
c= tensor(1.8693e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8694e+08, device='cuda:0')
c= tensor(1.8695e+08, device='cuda:0')
c= tensor(1.8695e+08, device='cuda:0')
c= tensor(1.8696e+08, device='cuda:0')
c= tensor(1.8696e+08, device='cuda:0')
c= tensor(1.8698e+08, device='cuda:0')
c= tensor(1.8698e+08, device='cuda:0')
c= tensor(1.8699e+08, device='cuda:0')
c= tensor(1.8699e+08, device='cuda:0')
c= tensor(1.8699e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8700e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8701e+08, device='cuda:0')
c= tensor(1.8702e+08, device='cuda:0')
c= tensor(1.8702e+08, device='cuda:0')
c= tensor(1.8702e+08, device='cuda:0')
c= tensor(1.8703e+08, device='cuda:0')
c= tensor(1.8703e+08, device='cuda:0')
c= tensor(1.8703e+08, device='cuda:0')
c= tensor(1.8704e+08, device='cuda:0')
c= tensor(1.8704e+08, device='cuda:0')
c= tensor(1.8704e+08, device='cuda:0')
c= tensor(1.8704e+08, device='cuda:0')
c= tensor(1.8705e+08, device='cuda:0')
c= tensor(1.8705e+08, device='cuda:0')
c= tensor(1.8705e+08, device='cuda:0')
c= tensor(1.8706e+08, device='cuda:0')
c= tensor(1.8706e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8711e+08, device='cuda:0')
c= tensor(1.8712e+08, device='cuda:0')
c= tensor(1.8712e+08, device='cuda:0')
c= tensor(1.8713e+08, device='cuda:0')
c= tensor(1.8713e+08, device='cuda:0')
c= tensor(1.8714e+08, device='cuda:0')
c= tensor(1.8715e+08, device='cuda:0')
c= tensor(1.8715e+08, device='cuda:0')
c= tensor(1.8715e+08, device='cuda:0')
c= tensor(1.8715e+08, device='cuda:0')
c= tensor(1.8716e+08, device='cuda:0')
c= tensor(1.8716e+08, device='cuda:0')
c= tensor(1.8717e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8718e+08, device='cuda:0')
c= tensor(1.8719e+08, device='cuda:0')
c= tensor(1.8719e+08, device='cuda:0')
c= tensor(1.8719e+08, device='cuda:0')
c= tensor(1.8719e+08, device='cuda:0')
c= tensor(1.8719e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8720e+08, device='cuda:0')
c= tensor(1.8721e+08, device='cuda:0')
c= tensor(1.8721e+08, device='cuda:0')
c= tensor(1.8721e+08, device='cuda:0')
c= tensor(1.8721e+08, device='cuda:0')
c= tensor(1.8722e+08, device='cuda:0')
c= tensor(1.8722e+08, device='cuda:0')
c= tensor(1.8723e+08, device='cuda:0')
c= tensor(1.8846e+08, device='cuda:0')
c= tensor(1.8847e+08, device='cuda:0')
c= tensor(1.8848e+08, device='cuda:0')
c= tensor(1.8848e+08, device='cuda:0')
c= tensor(1.8848e+08, device='cuda:0')
c= tensor(1.8860e+08, device='cuda:0')
c= tensor(1.9756e+08, device='cuda:0')
c= tensor(1.9756e+08, device='cuda:0')
c= tensor(1.9771e+08, device='cuda:0')
c= tensor(1.9807e+08, device='cuda:0')
c= tensor(1.9808e+08, device='cuda:0')
c= tensor(2.1298e+08, device='cuda:0')
c= tensor(2.1298e+08, device='cuda:0')
c= tensor(2.1299e+08, device='cuda:0')
c= tensor(2.1379e+08, device='cuda:0')
c= tensor(2.1858e+08, device='cuda:0')
c= tensor(2.1858e+08, device='cuda:0')
c= tensor(2.1865e+08, device='cuda:0')
c= tensor(2.2032e+08, device='cuda:0')
c= tensor(2.2660e+08, device='cuda:0')
c= tensor(2.2674e+08, device='cuda:0')
c= tensor(2.2694e+08, device='cuda:0')
c= tensor(2.2710e+08, device='cuda:0')
c= tensor(2.2714e+08, device='cuda:0')
c= tensor(2.2715e+08, device='cuda:0')
c= tensor(2.3231e+08, device='cuda:0')
c= tensor(2.3232e+08, device='cuda:0')
c= tensor(2.3232e+08, device='cuda:0')
c= tensor(2.3248e+08, device='cuda:0')
c= tensor(2.3251e+08, device='cuda:0')
c= tensor(2.4053e+08, device='cuda:0')
c= tensor(2.4096e+08, device='cuda:0')
c= tensor(2.4096e+08, device='cuda:0')
c= tensor(2.4098e+08, device='cuda:0')
c= tensor(2.4100e+08, device='cuda:0')
c= tensor(2.4102e+08, device='cuda:0')
c= tensor(2.4152e+08, device='cuda:0')
c= tensor(2.4172e+08, device='cuda:0')
c= tensor(2.4176e+08, device='cuda:0')
c= tensor(2.4176e+08, device='cuda:0')
c= tensor(2.4177e+08, device='cuda:0')
c= tensor(2.4196e+08, device='cuda:0')
c= tensor(2.4225e+08, device='cuda:0')
c= tensor(2.4227e+08, device='cuda:0')
c= tensor(2.4228e+08, device='cuda:0')
c= tensor(2.5330e+08, device='cuda:0')
c= tensor(2.5331e+08, device='cuda:0')
c= tensor(2.5336e+08, device='cuda:0')
c= tensor(2.5419e+08, device='cuda:0')
c= tensor(2.5419e+08, device='cuda:0')
c= tensor(2.5434e+08, device='cuda:0')
c= tensor(2.6001e+08, device='cuda:0')
c= tensor(2.6606e+08, device='cuda:0')
c= tensor(2.6607e+08, device='cuda:0')
c= tensor(2.6608e+08, device='cuda:0')
c= tensor(2.6609e+08, device='cuda:0')
c= tensor(2.6609e+08, device='cuda:0')
c= tensor(2.6626e+08, device='cuda:0')
c= tensor(2.6628e+08, device='cuda:0')
c= tensor(2.6640e+08, device='cuda:0')
c= tensor(2.7063e+08, device='cuda:0')
c= tensor(2.7090e+08, device='cuda:0')
c= tensor(2.7091e+08, device='cuda:0')
c= tensor(2.7092e+08, device='cuda:0')
c= tensor(2.7397e+08, device='cuda:0')
c= tensor(2.7401e+08, device='cuda:0')
c= tensor(2.7401e+08, device='cuda:0')
c= tensor(2.7409e+08, device='cuda:0')
c= tensor(2.8764e+08, device='cuda:0')
c= tensor(2.8765e+08, device='cuda:0')
c= tensor(2.8868e+08, device='cuda:0')
c= tensor(2.8868e+08, device='cuda:0')
c= tensor(2.8903e+08, device='cuda:0')
c= tensor(2.8918e+08, device='cuda:0')
c= tensor(2.9295e+08, device='cuda:0')
c= tensor(2.9333e+08, device='cuda:0')
c= tensor(2.9333e+08, device='cuda:0')
c= tensor(2.9433e+08, device='cuda:0')
c= tensor(2.9495e+08, device='cuda:0')
c= tensor(2.9495e+08, device='cuda:0')
c= tensor(2.9508e+08, device='cuda:0')
c= tensor(2.9791e+08, device='cuda:0')
c= tensor(3.0215e+08, device='cuda:0')
c= tensor(3.0588e+08, device='cuda:0')
c= tensor(3.0590e+08, device='cuda:0')
c= tensor(3.0590e+08, device='cuda:0')
c= tensor(3.0592e+08, device='cuda:0')
c= tensor(3.0625e+08, device='cuda:0')
c= tensor(3.0625e+08, device='cuda:0')
c= tensor(3.0625e+08, device='cuda:0')
c= tensor(3.0657e+08, device='cuda:0')
c= tensor(3.0730e+08, device='cuda:0')
c= tensor(3.0785e+08, device='cuda:0')
c= tensor(3.0785e+08, device='cuda:0')
c= tensor(3.0787e+08, device='cuda:0')
c= tensor(3.0791e+08, device='cuda:0')
c= tensor(3.0792e+08, device='cuda:0')
c= tensor(3.0793e+08, device='cuda:0')
c= tensor(3.0793e+08, device='cuda:0')
c= tensor(3.1452e+08, device='cuda:0')
c= tensor(3.1459e+08, device='cuda:0')
c= tensor(3.1462e+08, device='cuda:0')
c= tensor(3.1473e+08, device='cuda:0')
c= tensor(3.1473e+08, device='cuda:0')
c= tensor(3.2551e+08, device='cuda:0')
c= tensor(3.2551e+08, device='cuda:0')
c= tensor(3.2596e+08, device='cuda:0')
c= tensor(3.2596e+08, device='cuda:0')
c= tensor(3.2596e+08, device='cuda:0')
c= tensor(3.2596e+08, device='cuda:0')
c= tensor(3.2605e+08, device='cuda:0')
c= tensor(3.2621e+08, device='cuda:0')
c= tensor(3.2632e+08, device='cuda:0')
c= tensor(3.2633e+08, device='cuda:0')
c= tensor(3.2633e+08, device='cuda:0')
c= tensor(3.3025e+08, device='cuda:0')
c= tensor(3.3037e+08, device='cuda:0')
c= tensor(3.3052e+08, device='cuda:0')
c= tensor(3.3109e+08, device='cuda:0')
c= tensor(3.3204e+08, device='cuda:0')
c= tensor(3.3204e+08, device='cuda:0')
c= tensor(3.3205e+08, device='cuda:0')
c= tensor(3.3206e+08, device='cuda:0')
c= tensor(3.3206e+08, device='cuda:0')
c= tensor(3.3206e+08, device='cuda:0')
c= tensor(3.3211e+08, device='cuda:0')
c= tensor(3.3211e+08, device='cuda:0')
c= tensor(3.3211e+08, device='cuda:0')
c= tensor(3.3212e+08, device='cuda:0')
c= tensor(3.3212e+08, device='cuda:0')
c= tensor(3.5171e+08, device='cuda:0')
c= tensor(3.5188e+08, device='cuda:0')
c= tensor(3.5218e+08, device='cuda:0')
c= tensor(3.5243e+08, device='cuda:0')
c= tensor(3.5243e+08, device='cuda:0')
c= tensor(3.5244e+08, device='cuda:0')
c= tensor(4.2257e+08, device='cuda:0')
c= tensor(4.3814e+08, device='cuda:0')
c= tensor(4.3825e+08, device='cuda:0')
c= tensor(4.3832e+08, device='cuda:0')
c= tensor(4.3832e+08, device='cuda:0')
c= tensor(4.3932e+08, device='cuda:0')
c= tensor(4.3984e+08, device='cuda:0')
c= tensor(4.4282e+08, device='cuda:0')
c= tensor(4.4282e+08, device='cuda:0')
c= tensor(4.4371e+08, device='cuda:0')
c= tensor(4.4772e+08, device='cuda:0')
c= tensor(4.4780e+08, device='cuda:0')
c= tensor(4.4780e+08, device='cuda:0')
c= tensor(4.4783e+08, device='cuda:0')
c= tensor(4.4787e+08, device='cuda:0')
c= tensor(4.4787e+08, device='cuda:0')
c= tensor(4.4929e+08, device='cuda:0')
c= tensor(4.4939e+08, device='cuda:0')
c= tensor(4.4939e+08, device='cuda:0')
c= tensor(4.4943e+08, device='cuda:0')
c= tensor(4.4949e+08, device='cuda:0')
c= tensor(4.4949e+08, device='cuda:0')
c= tensor(4.5102e+08, device='cuda:0')
c= tensor(4.5180e+08, device='cuda:0')
c= tensor(4.5223e+08, device='cuda:0')
c= tensor(4.5303e+08, device='cuda:0')
c= tensor(4.5333e+08, device='cuda:0')
c= tensor(4.5335e+08, device='cuda:0')
c= tensor(4.5342e+08, device='cuda:0')
c= tensor(4.5353e+08, device='cuda:0')
c= tensor(4.5441e+08, device='cuda:0')
c= tensor(4.5441e+08, device='cuda:0')
c= tensor(4.5770e+08, device='cuda:0')
c= tensor(4.6417e+08, device='cuda:0')
c= tensor(4.6448e+08, device='cuda:0')
c= tensor(4.6473e+08, device='cuda:0')
c= tensor(4.6737e+08, device='cuda:0')
c= tensor(4.6738e+08, device='cuda:0')
c= tensor(4.6738e+08, device='cuda:0')
c= tensor(4.6759e+08, device='cuda:0')
c= tensor(4.6845e+08, device='cuda:0')
c= tensor(4.6888e+08, device='cuda:0')
c= tensor(4.9050e+08, device='cuda:0')
c= tensor(4.9613e+08, device='cuda:0')
c= tensor(4.9677e+08, device='cuda:0')
c= tensor(4.9685e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(5.0065e+08, device='cuda:0')
c= tensor(5.0066e+08, device='cuda:0')
c= tensor(5.0066e+08, device='cuda:0')
c= tensor(5.0072e+08, device='cuda:0')
c= tensor(5.0115e+08, device='cuda:0')
c= tensor(5.0125e+08, device='cuda:0')
c= tensor(5.0193e+08, device='cuda:0')
c= tensor(5.0198e+08, device='cuda:0')
c= tensor(5.0204e+08, device='cuda:0')
c= tensor(5.0204e+08, device='cuda:0')
c= tensor(5.0210e+08, device='cuda:0')
c= tensor(5.0216e+08, device='cuda:0')
c= tensor(5.0245e+08, device='cuda:0')
c= tensor(5.0246e+08, device='cuda:0')
c= tensor(5.0337e+08, device='cuda:0')
c= tensor(5.0337e+08, device='cuda:0')
c= tensor(5.0343e+08, device='cuda:0')
c= tensor(5.0343e+08, device='cuda:0')
c= tensor(5.0351e+08, device='cuda:0')
c= tensor(5.0351e+08, device='cuda:0')
c= tensor(5.0367e+08, device='cuda:0')
c= tensor(5.0368e+08, device='cuda:0')
c= tensor(5.0372e+08, device='cuda:0')
c= tensor(5.0404e+08, device='cuda:0')
c= tensor(5.0974e+08, device='cuda:0')
c= tensor(5.0975e+08, device='cuda:0')
c= tensor(5.0977e+08, device='cuda:0')
c= tensor(5.1156e+08, device='cuda:0')
c= tensor(5.1157e+08, device='cuda:0')
c= tensor(5.2653e+08, device='cuda:0')
c= tensor(5.2653e+08, device='cuda:0')
c= tensor(5.2697e+08, device='cuda:0')
c= tensor(5.2831e+08, device='cuda:0')
c= tensor(5.2831e+08, device='cuda:0')
c= tensor(5.3068e+08, device='cuda:0')
c= tensor(5.3087e+08, device='cuda:0')
c= tensor(5.4024e+08, device='cuda:0')
c= tensor(5.4025e+08, device='cuda:0')
c= tensor(5.4040e+08, device='cuda:0')
c= tensor(5.4040e+08, device='cuda:0')
c= tensor(5.4041e+08, device='cuda:0')
c= tensor(5.4042e+08, device='cuda:0')
c= tensor(5.4057e+08, device='cuda:0')
c= tensor(5.4059e+08, device='cuda:0')
c= tensor(5.4088e+08, device='cuda:0')
c= tensor(5.4089e+08, device='cuda:0')
c= tensor(5.4090e+08, device='cuda:0')
c= tensor(5.4090e+08, device='cuda:0')
c= tensor(5.4265e+08, device='cuda:0')
c= tensor(5.4269e+08, device='cuda:0')
c= tensor(5.4369e+08, device='cuda:0')
c= tensor(5.4473e+08, device='cuda:0')
c= tensor(5.4475e+08, device='cuda:0')
c= tensor(5.4475e+08, device='cuda:0')
c= tensor(5.4477e+08, device='cuda:0')
c= tensor(5.9254e+08, device='cuda:0')
c= tensor(5.9255e+08, device='cuda:0')
c= tensor(5.9257e+08, device='cuda:0')
c= tensor(5.9341e+08, device='cuda:0')
c= tensor(5.9372e+08, device='cuda:0')
c= tensor(5.9373e+08, device='cuda:0')
c= tensor(5.9373e+08, device='cuda:0')
c= tensor(6.0585e+08, device='cuda:0')
c= tensor(6.0587e+08, device='cuda:0')
c= tensor(6.0604e+08, device='cuda:0')
c= tensor(6.0623e+08, device='cuda:0')
c= tensor(6.0653e+08, device='cuda:0')
c= tensor(6.0760e+08, device='cuda:0')
c= tensor(6.0929e+08, device='cuda:0')
c= tensor(6.0947e+08, device='cuda:0')
c= tensor(6.0948e+08, device='cuda:0')
c= tensor(6.0974e+08, device='cuda:0')
c= tensor(6.0975e+08, device='cuda:0')
c= tensor(6.0978e+08, device='cuda:0')
c= tensor(6.0983e+08, device='cuda:0')
c= tensor(6.0983e+08, device='cuda:0')
c= tensor(6.1071e+08, device='cuda:0')
c= tensor(6.1072e+08, device='cuda:0')
c= tensor(6.1072e+08, device='cuda:0')
c= tensor(6.1074e+08, device='cuda:0')
c= tensor(6.1078e+08, device='cuda:0')
c= tensor(6.1260e+08, device='cuda:0')
c= tensor(6.1355e+08, device='cuda:0')
c= tensor(6.1358e+08, device='cuda:0')
c= tensor(6.1359e+08, device='cuda:0')
c= tensor(6.1369e+08, device='cuda:0')
c= tensor(6.1369e+08, device='cuda:0')
c= tensor(6.1370e+08, device='cuda:0')
c= tensor(6.1375e+08, device='cuda:0')
c= tensor(6.1669e+08, device='cuda:0')
c= tensor(6.1670e+08, device='cuda:0')
c= tensor(6.1670e+08, device='cuda:0')
c= tensor(6.1670e+08, device='cuda:0')
c= tensor(6.2230e+08, device='cuda:0')
c= tensor(6.3226e+08, device='cuda:0')
c= tensor(6.3232e+08, device='cuda:0')
c= tensor(6.3232e+08, device='cuda:0')
c= tensor(6.3243e+08, device='cuda:0')
c= tensor(6.3316e+08, device='cuda:0')
c= tensor(6.3316e+08, device='cuda:0')
c= tensor(6.3322e+08, device='cuda:0')
c= tensor(6.3324e+08, device='cuda:0')
c= tensor(6.4101e+08, device='cuda:0')
c= tensor(6.4131e+08, device='cuda:0')
c= tensor(6.4226e+08, device='cuda:0')
c= tensor(6.4227e+08, device='cuda:0')
c= tensor(6.4229e+08, device='cuda:0')
c= tensor(6.4229e+08, device='cuda:0')
c= tensor(6.4235e+08, device='cuda:0')
c= tensor(6.4238e+08, device='cuda:0')
c= tensor(6.4272e+08, device='cuda:0')
c= tensor(6.4290e+08, device='cuda:0')
c= tensor(6.4633e+08, device='cuda:0')
c= tensor(6.4635e+08, device='cuda:0')
c= tensor(6.4636e+08, device='cuda:0')
c= tensor(6.4639e+08, device='cuda:0')
c= tensor(7.5965e+08, device='cuda:0')
c= tensor(7.5971e+08, device='cuda:0')
c= tensor(7.5972e+08, device='cuda:0')
c= tensor(7.5972e+08, device='cuda:0')
c= tensor(7.5978e+08, device='cuda:0')
c= tensor(7.5978e+08, device='cuda:0')
c= tensor(7.5995e+08, device='cuda:0')
c= tensor(7.5996e+08, device='cuda:0')
c= tensor(7.5997e+08, device='cuda:0')
c= tensor(7.5999e+08, device='cuda:0')
c= tensor(7.5999e+08, device='cuda:0')
c= tensor(7.5999e+08, device='cuda:0')
c= tensor(7.6151e+08, device='cuda:0')
c= tensor(7.7132e+08, device='cuda:0')
c= tensor(7.7263e+08, device='cuda:0')
c= tensor(7.7271e+08, device='cuda:0')
c= tensor(7.7278e+08, device='cuda:0')
c= tensor(7.7280e+08, device='cuda:0')
c= tensor(7.7281e+08, device='cuda:0')
c= tensor(7.7315e+08, device='cuda:0')
c= tensor(7.7341e+08, device='cuda:0')
c= tensor(7.7519e+08, device='cuda:0')
c= tensor(7.7521e+08, device='cuda:0')
c= tensor(7.8998e+08, device='cuda:0')
c= tensor(7.9001e+08, device='cuda:0')
c= tensor(7.9019e+08, device='cuda:0')
c= tensor(7.9426e+08, device='cuda:0')
c= tensor(7.9445e+08, device='cuda:0')
c= tensor(7.9607e+08, device='cuda:0')
c= tensor(7.9750e+08, device='cuda:0')
c= tensor(7.9767e+08, device='cuda:0')
c= tensor(7.9769e+08, device='cuda:0')
c= tensor(7.9772e+08, device='cuda:0')
c= tensor(7.9787e+08, device='cuda:0')
c= tensor(7.9787e+08, device='cuda:0')
c= tensor(7.9868e+08, device='cuda:0')
c= tensor(8.2739e+08, device='cuda:0')
c= tensor(8.3136e+08, device='cuda:0')
c= tensor(8.3226e+08, device='cuda:0')
c= tensor(8.3234e+08, device='cuda:0')
c= tensor(8.3247e+08, device='cuda:0')
c= tensor(8.3256e+08, device='cuda:0')
c= tensor(8.3261e+08, device='cuda:0')
c= tensor(8.3373e+08, device='cuda:0')
c= tensor(8.3647e+08, device='cuda:0')
c= tensor(8.3647e+08, device='cuda:0')
c= tensor(8.3871e+08, device='cuda:0')
c= tensor(8.4109e+08, device='cuda:0')
c= tensor(8.4114e+08, device='cuda:0')
c= tensor(8.4114e+08, device='cuda:0')
c= tensor(8.4149e+08, device='cuda:0')
c= tensor(8.4202e+08, device='cuda:0')
c= tensor(8.4203e+08, device='cuda:0')
c= tensor(8.5558e+08, device='cuda:0')
c= tensor(8.5570e+08, device='cuda:0')
c= tensor(8.5574e+08, device='cuda:0')
c= tensor(8.5575e+08, device='cuda:0')
c= tensor(8.5576e+08, device='cuda:0')
c= tensor(8.5576e+08, device='cuda:0')
c= tensor(8.5576e+08, device='cuda:0')
c= tensor(8.5581e+08, device='cuda:0')
c= tensor(8.5594e+08, device='cuda:0')
c= tensor(1.1930e+09, device='cuda:0')
c= tensor(1.1931e+09, device='cuda:0')
c= tensor(1.1931e+09, device='cuda:0')
c= tensor(1.1931e+09, device='cuda:0')
c= tensor(1.1931e+09, device='cuda:0')
c= tensor(1.1932e+09, device='cuda:0')
c= tensor(1.1980e+09, device='cuda:0')
c= tensor(1.1980e+09, device='cuda:0')
c= tensor(1.2477e+09, device='cuda:0')
c= tensor(1.2477e+09, device='cuda:0')
c= tensor(1.2495e+09, device='cuda:0')
c= tensor(1.2496e+09, device='cuda:0')
c= tensor(1.2506e+09, device='cuda:0')
c= tensor(1.2609e+09, device='cuda:0')
c= tensor(1.2610e+09, device='cuda:0')
c= tensor(1.2610e+09, device='cuda:0')
c= tensor(1.2617e+09, device='cuda:0')
c= tensor(1.2617e+09, device='cuda:0')
c= tensor(1.2620e+09, device='cuda:0')
c= tensor(1.2622e+09, device='cuda:0')
c= tensor(1.2624e+09, device='cuda:0')
c= tensor(1.2627e+09, device='cuda:0')
c= tensor(1.2627e+09, device='cuda:0')
c= tensor(1.2641e+09, device='cuda:0')
c= tensor(1.2694e+09, device='cuda:0')
c= tensor(1.2694e+09, device='cuda:0')
c= tensor(1.2695e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2723e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2730e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2740e+09, device='cuda:0')
c= tensor(1.2740e+09, device='cuda:0')
c= tensor(1.2740e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2742e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2761e+09, device='cuda:0')
c= tensor(1.2761e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2766e+09, device='cuda:0')
c= tensor(1.2798e+09, device='cuda:0')
c= tensor(1.2800e+09, device='cuda:0')
c= tensor(1.2804e+09, device='cuda:0')
c= tensor(1.2831e+09, device='cuda:0')
c= tensor(1.2832e+09, device='cuda:0')
c= tensor(1.2832e+09, device='cuda:0')
c= tensor(1.2832e+09, device='cuda:0')
c= tensor(1.2836e+09, device='cuda:0')
c= tensor(1.2837e+09, device='cuda:0')
c= tensor(1.2838e+09, device='cuda:0')
c= tensor(1.2838e+09, device='cuda:0')
c= tensor(1.2838e+09, device='cuda:0')
c= tensor(1.2849e+09, device='cuda:0')
c= tensor(1.2850e+09, device='cuda:0')
c= tensor(1.2850e+09, device='cuda:0')
c= tensor(1.2850e+09, device='cuda:0')
c= tensor(1.2850e+09, device='cuda:0')
c= tensor(1.2851e+09, device='cuda:0')
c= tensor(1.2852e+09, device='cuda:0')
c= tensor(1.2862e+09, device='cuda:0')
c= tensor(1.2862e+09, device='cuda:0')
c= tensor(1.2945e+09, device='cuda:0')
c= tensor(1.2946e+09, device='cuda:0')
c= tensor(1.2946e+09, device='cuda:0')
c= tensor(1.3059e+09, device='cuda:0')
c= tensor(1.3060e+09, device='cuda:0')
c= tensor(1.3063e+09, device='cuda:0')
c= tensor(1.3080e+09, device='cuda:0')
c= tensor(1.3082e+09, device='cuda:0')
c= tensor(1.3089e+09, device='cuda:0')
c= tensor(1.3107e+09, device='cuda:0')
c= tensor(1.3107e+09, device='cuda:0')
c= tensor(1.3108e+09, device='cuda:0')
c= tensor(1.3108e+09, device='cuda:0')
c= tensor(1.3124e+09, device='cuda:0')
c= tensor(1.3144e+09, device='cuda:0')
c= tensor(1.3145e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3147e+09, device='cuda:0')
c= tensor(1.3147e+09, device='cuda:0')
c= tensor(1.3147e+09, device='cuda:0')
c= tensor(1.3169e+09, device='cuda:0')
c= tensor(1.3223e+09, device='cuda:0')
c= tensor(1.3223e+09, device='cuda:0')
c= tensor(1.3224e+09, device='cuda:0')
c= tensor(1.3224e+09, device='cuda:0')
c= tensor(1.3301e+09, device='cuda:0')
c= tensor(1.3301e+09, device='cuda:0')
c= tensor(1.3301e+09, device='cuda:0')
c= tensor(1.3303e+09, device='cuda:0')
c= tensor(1.3307e+09, device='cuda:0')
c= tensor(1.3307e+09, device='cuda:0')
c= tensor(1.3308e+09, device='cuda:0')
c= tensor(1.3308e+09, device='cuda:0')
time to make c is 11.121798753738403
time for making loss is 11.121888875961304
p0 True
it  0 : 947265024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4006133760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4006449152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  248075900.0
relative error loss 0.18640816
shape of L is 
torch.Size([])
memory (bytes)
4033302528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4033302528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  243982340.0
relative error loss 0.18333219
shape of L is 
torch.Size([])
memory (bytes)
4036718592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4036894720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  233556220.0
relative error loss 0.17549784
shape of L is 
torch.Size([])
memory (bytes)
4040097792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4040097792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  229742080.0
relative error loss 0.17263184
shape of L is 
torch.Size([])
memory (bytes)
4043345920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4043345920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  225787780.0
relative error loss 0.16966051
shape of L is 
torch.Size([])
memory (bytes)
4046540800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4046565376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  223104380.0
relative error loss 0.16764417
shape of L is 
torch.Size([])
memory (bytes)
4049784832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4049784832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  220884350.0
relative error loss 0.165976
shape of L is 
torch.Size([])
memory (bytes)
4052992000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4052992000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  219507460.0
relative error loss 0.16494139
shape of L is 
torch.Size([])
memory (bytes)
4056236032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4056236032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  217758080.0
relative error loss 0.16362688
shape of L is 
torch.Size([])
memory (bytes)
4059439104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4059439104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  216801920.0
relative error loss 0.1629084
time to take a step is 305.2229928970337
it  1 : 1393462272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4062633984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4062658560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  216801920.0
relative error loss 0.1629084
shape of L is 
torch.Size([])
memory (bytes)
4065853440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4065878016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  215897730.0
relative error loss 0.16222897
shape of L is 
torch.Size([])
memory (bytes)
4069117952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4069117952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  215229950.0
relative error loss 0.1617272
shape of L is 
torch.Size([])
memory (bytes)
4072341504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4072341504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  214568700.0
relative error loss 0.16123033
shape of L is 
torch.Size([])
memory (bytes)
4075536384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4075536384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  213685250.0
relative error loss 0.16056648
shape of L is 
torch.Size([])
memory (bytes)
4078780416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4078780416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  213019400.0
relative error loss 0.16006614
shape of L is 
torch.Size([])
memory (bytes)
4081950720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4081950720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  212590460.0
relative error loss 0.15974385
shape of L is 
torch.Size([])
memory (bytes)
4085260288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4085260288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  212114560.0
relative error loss 0.15938625
shape of L is 
torch.Size([])
memory (bytes)
4088406016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4088406016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  211586560.0
relative error loss 0.1589895
shape of L is 
torch.Size([])
memory (bytes)
4091437056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4091699200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  211282300.0
relative error loss 0.15876088
time to take a step is 289.6909234523773
it  2 : 1396008960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4094922752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4094922752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  211282300.0
relative error loss 0.15876088
shape of L is 
torch.Size([])
memory (bytes)
4098113536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4098113536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  210946940.0
relative error loss 0.15850888
shape of L is 
torch.Size([])
memory (bytes)
4101373952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4101373952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  210684930.0
relative error loss 0.158312
shape of L is 
torch.Size([])
memory (bytes)
4104589312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4104589312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  210422140.0
relative error loss 0.15811454
shape of L is 
torch.Size([])
memory (bytes)
4107837440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4107837440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  210399870.0
relative error loss 0.1580978
shape of L is 
torch.Size([])
memory (bytes)
4111032320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4111032320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  210186880.0
relative error loss 0.15793775
shape of L is 
torch.Size([])
memory (bytes)
4114251776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4114272256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  209879800.0
relative error loss 0.15770702
shape of L is 
torch.Size([])
memory (bytes)
4117499904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4117499904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  8% |
error is  209656700.0
relative error loss 0.15753937
shape of L is 
torch.Size([])
memory (bytes)
4120715264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4120715264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  209405180.0
relative error loss 0.15735038
shape of L is 
torch.Size([])
memory (bytes)
4123934720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4123934720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  209190910.0
relative error loss 0.15718937
time to take a step is 303.5067985057831
it  3 : 1394311168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4127166464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4127166464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  209190910.0
relative error loss 0.15718937
shape of L is 
torch.Size([])
memory (bytes)
4130377728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4130377728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  209017860.0
relative error loss 0.15705933
shape of L is 
torch.Size([])
memory (bytes)
4133601280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4133605376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  208837630.0
relative error loss 0.1569239
shape of L is 
torch.Size([])
memory (bytes)
4136824832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4136824832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  208709120.0
relative error loss 0.15682735
shape of L is 
torch.Size([])
memory (bytes)
4140040192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4140040192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  208575490.0
relative error loss 0.15672693
shape of L is 
torch.Size([])
memory (bytes)
4143267840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4143267840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  208455040.0
relative error loss 0.15663642
shape of L is 
torch.Size([])
memory (bytes)
4146487296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4146487296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  208327170.0
relative error loss 0.15654033
shape of L is 
torch.Size([])
memory (bytes)
4149710848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4149710848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  208251000.0
relative error loss 0.15648311
shape of L is 
torch.Size([])
memory (bytes)
4152934400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4152942592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  208113540.0
relative error loss 0.15637982
shape of L is 
torch.Size([])
memory (bytes)
4156162048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4156162048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  208146820.0
relative error loss 0.15640482
shape of L is 
torch.Size([])
memory (bytes)
4159389696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4159389696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  208044930.0
relative error loss 0.15632826
time to take a step is 325.5952808856964
it  4 : 1395160576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4162621440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4162621440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  208044930.0
relative error loss 0.15632826
shape of L is 
torch.Size([])
memory (bytes)
4165828608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4165828608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  207949440.0
relative error loss 0.15625651
shape of L is 
torch.Size([])
memory (bytes)
4169064448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4169064448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  207830400.0
relative error loss 0.15616706
shape of L is 
torch.Size([])
memory (bytes)
4172271616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4172271616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  207745280.0
relative error loss 0.1561031
shape of L is 
torch.Size([])
memory (bytes)
4175491072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4175515648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  207648510.0
relative error loss 0.15603039
shape of L is 
torch.Size([])
memory (bytes)
4178739200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4178739200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  207555710.0
relative error loss 0.15596065
shape of L is 
torch.Size([])
memory (bytes)
4181950464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4181950464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  207477890.0
relative error loss 0.15590218
shape of L is 
torch.Size([])
memory (bytes)
4185214976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4185214976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  207390200.0
relative error loss 0.15583628
shape of L is 
torch.Size([])
memory (bytes)
4188397568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4188397568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  207298690.0
relative error loss 0.15576752
shape of L is 
torch.Size([])
memory (bytes)
4191653888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4191653888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  207237380.0
relative error loss 0.15572146
time to take a step is 289.8743860721588
it  5 : 1393462272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4194889728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4194893824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  207237380.0
relative error loss 0.15572146
shape of L is 
torch.Size([])
memory (bytes)
4197990400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4198100992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  207168640.0
relative error loss 0.15566981
shape of L is 
torch.Size([])
memory (bytes)
4201336832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4201336832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  207179000.0
relative error loss 0.15567759
shape of L is 
torch.Size([])
memory (bytes)
4204507136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4204507136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  207101060.0
relative error loss 0.15561903
shape of L is 
torch.Size([])
memory (bytes)
4207681536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4207792128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  207021310.0
relative error loss 0.1555591
shape of L is 
torch.Size([])
memory (bytes)
4210978816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4211003392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  206951420.0
relative error loss 0.15550658
shape of L is 
torch.Size([])
memory (bytes)
4214116352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4214116352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  206880130.0
relative error loss 0.15545301
shape of L is 
torch.Size([])
memory (bytes)
4217434112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  8% |
memory (bytes)
4217434112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  206800130.0
relative error loss 0.1553929
shape of L is 
torch.Size([])
memory (bytes)
4220620800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4220620800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206726140.0
relative error loss 0.1553373
shape of L is 
torch.Size([])
memory (bytes)
4223885312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4223885312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206662660.0
relative error loss 0.15528959
time to take a step is 289.7154002189636
it  6 : 1396008960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4227026944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4227026944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206662660.0
relative error loss 0.15528959
shape of L is 
torch.Size([])
memory (bytes)
4230209536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4230209536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206614020.0
relative error loss 0.15525305
shape of L is 
torch.Size([])
memory (bytes)
4233539584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4233543680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206555520.0
relative error loss 0.1552091
shape of L is 
torch.Size([])
memory (bytes)
4236632064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4236763136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206527100.0
relative error loss 0.15518774
shape of L is 
torch.Size([])
memory (bytes)
4239970304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4239970304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206492670.0
relative error loss 0.15516187
shape of L is 
torch.Size([])
memory (bytes)
4243197952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4243197952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206468100.0
relative error loss 0.1551434
shape of L is 
torch.Size([])
memory (bytes)
4246421504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4246421504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  206442240.0
relative error loss 0.15512398
shape of L is 
torch.Size([])
memory (bytes)
4249640960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4249640960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206428540.0
relative error loss 0.15511368
shape of L is 
torch.Size([])
memory (bytes)
4252852224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4252852224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206391680.0
relative error loss 0.15508598
shape of L is 
torch.Size([])
memory (bytes)
4256075776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4256075776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  8% |
error is  206375040.0
relative error loss 0.15507348
time to take a step is 289.80703616142273
it  7 : 1394311168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4259299328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4259299328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  206375040.0
relative error loss 0.15507348
shape of L is 
torch.Size([])
memory (bytes)
4262518784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  7% |
memory (bytes)
4262518784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  206352640.0
relative error loss 0.15505664
shape of L is 
torch.Size([])
memory (bytes)
4265738240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4265738240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206316800.0
relative error loss 0.15502971
shape of L is 
torch.Size([])
memory (bytes)
4268957696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4268957696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206274180.0
relative error loss 0.15499769
shape of L is 
torch.Size([])
memory (bytes)
4272193536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4272193536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  206234370.0
relative error loss 0.15496777
shape of L is 
torch.Size([])
memory (bytes)
4275372032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4275372032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  206181000.0
relative error loss 0.15492767
shape of L is 
torch.Size([])
memory (bytes)
4278644736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4278644736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  206114940.0
relative error loss 0.15487804
shape of L is 
torch.Size([])
memory (bytes)
4281704448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4281868288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  206048000.0
relative error loss 0.15482773
shape of L is 
torch.Size([])
memory (bytes)
4285100032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4285100032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205997310.0
relative error loss 0.15478964
shape of L is 
torch.Size([])
memory (bytes)
4288274432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4288274432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205963260.0
relative error loss 0.15476406
time to take a step is 291.15383100509644
it  8 : 1396008960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4291547136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4291547136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  205963260.0
relative error loss 0.15476406
shape of L is 
torch.Size([])
memory (bytes)
4294758400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4294762496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205902200.0
relative error loss 0.15471819
shape of L is 
torch.Size([])
memory (bytes)
4297973760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4297973760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205895300.0
relative error loss 0.15471299
shape of L is 
torch.Size([])
memory (bytes)
4301213696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4301213696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  8% |
error is  205843840.0
relative error loss 0.15467432
shape of L is 
torch.Size([])
memory (bytes)
4304412672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4304412672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205824130.0
relative error loss 0.15465951
shape of L is 
torch.Size([])
memory (bytes)
4307607552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4307607552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205784320.0
relative error loss 0.1546296
shape of L is 
torch.Size([])
memory (bytes)
4310888448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4310888448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205752960.0
relative error loss 0.15460604
shape of L is 
torch.Size([])
memory (bytes)
4314030080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  8% |
memory (bytes)
4314030080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  205714180.0
relative error loss 0.1545769
shape of L is 
torch.Size([])
memory (bytes)
4317339648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4317339648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205682430.0
relative error loss 0.15455304
shape of L is 
torch.Size([])
memory (bytes)
4320550912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4320550912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205655170.0
relative error loss 0.15453255
time to take a step is 292.15978026390076
it  9 : 1396008960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4323774464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4323774464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  205655170.0
relative error loss 0.15453255
shape of L is 
torch.Size([])
memory (bytes)
4326981632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4326981632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  205629180.0
relative error loss 0.15451303
shape of L is 
torch.Size([])
memory (bytes)
4330164224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4330209280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205593730.0
relative error loss 0.15448639
shape of L is 
torch.Size([])
memory (bytes)
4333412352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4333424640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  205571330.0
relative error loss 0.15446955
shape of L is 
torch.Size([])
memory (bytes)
4336640000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4336640000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  8% |
error is  205552130.0
relative error loss 0.15445513
shape of L is 
torch.Size([])
memory (bytes)
4339859456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  8% |
memory (bytes)
4339859456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  205546370.0
relative error loss 0.1544508
shape of L is 
torch.Size([])
memory (bytes)
4343078912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4343078912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205499400.0
relative error loss 0.1544155
shape of L is 
torch.Size([])
memory (bytes)
4346249216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  8% |
memory (bytes)
4346249216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  8% |
error is  205484670.0
relative error loss 0.15440445
shape of L is 
torch.Size([])
memory (bytes)
4349538304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4349538304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  205465860.0
relative error loss 0.1543903
shape of L is 
torch.Size([])
memory (bytes)
4352675840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4352675840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205434880.0
relative error loss 0.15436703
time to take a step is 304.2972550392151
it  10 : 1393462272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4355973120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4355973120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  205434880.0
relative error loss 0.15436703
shape of L is 
torch.Size([])
memory (bytes)
4359020544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4359184384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205420420.0
relative error loss 0.15435615
shape of L is 
torch.Size([])
memory (bytes)
4362301440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4362416128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205374340.0
relative error loss 0.15432154
shape of L is 
torch.Size([])
memory (bytes)
4365639680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4365639680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  8% |
error is  205360130.0
relative error loss 0.15431085
shape of L is 
torch.Size([])
memory (bytes)
4368846848
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4368846848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  8% |
error is  205324030.0
relative error loss 0.15428373
shape of L is 
torch.Size([])
memory (bytes)
4372086784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4372086784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205317630.0
relative error loss 0.15427892
shape of L is 
torch.Size([])
memory (bytes)
4375273472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4375273472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205281020.0
relative error loss 0.15425141
shape of L is 
torch.Size([])
memory (bytes)
4378443776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4378443776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  205260930.0
relative error loss 0.15423632
shape of L is 
torch.Size([])
memory (bytes)
4381761536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4381761536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205241340.0
relative error loss 0.1542216
shape of L is 
torch.Size([])
memory (bytes)
4384911360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4384911360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205216770.0
relative error loss 0.15420313
time to take a step is 302.95992398262024
it  11 : 1395160064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4388212736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4388212736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  205216770.0
relative error loss 0.15420313
shape of L is 
torch.Size([])
memory (bytes)
4391317504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4391317504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205204220.0
relative error loss 0.15419371
shape of L is 
torch.Size([])
memory (bytes)
4394606592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4394651648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  205173120.0
relative error loss 0.15417033
shape of L is 
torch.Size([])
memory (bytes)
4397875200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4397875200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205160320.0
relative error loss 0.15416072
shape of L is 
torch.Size([])
memory (bytes)
4400967680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4400967680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205133310.0
relative error loss 0.15414043
shape of L is 
torch.Size([])
memory (bytes)
4404326400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  8% |
memory (bytes)
4404326400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  205104900.0
relative error loss 0.15411907
shape of L is 
torch.Size([])
memory (bytes)
4407468032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  8% |
memory (bytes)
4407554048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  205073920.0
relative error loss 0.1540958
shape of L is 
torch.Size([])
memory (bytes)
4410683392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4410765312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205054460.0
relative error loss 0.15408118
shape of L is 
torch.Size([])
memory (bytes)
4414005248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4414005248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  205028350.0
relative error loss 0.15406156
shape of L is 
torch.Size([])
memory (bytes)
4417138688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4417138688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  205011970.0
relative error loss 0.15404925
time to take a step is 302.8213760852814
it  12 : 1395160064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4420456448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4420456448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  205011970.0
relative error loss 0.15404925
shape of L is 
torch.Size([])
memory (bytes)
4423630848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4423630848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  8% |
error is  204990460.0
relative error loss 0.15403308
shape of L is 
torch.Size([])
memory (bytes)
4426813440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4426813440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  204973440.0
relative error loss 0.1540203
shape of L is 
torch.Size([])
memory (bytes)
4430127104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4430127104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  204962430.0
relative error loss 0.15401202
shape of L is 
torch.Size([])
memory (bytes)
4433285120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4433285120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204933380.0
relative error loss 0.1539902
shape of L is 
torch.Size([])
memory (bytes)
4436557824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4436557824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204927870.0
relative error loss 0.15398605
shape of L is 
torch.Size([])
memory (bytes)
4439785472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4439785472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  204895740.0
relative error loss 0.15396191
shape of L is 
torch.Size([])
memory (bytes)
4443004928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4443004928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  204884600.0
relative error loss 0.15395354
shape of L is 
torch.Size([])
memory (bytes)
4446236672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4446236672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204863870.0
relative error loss 0.15393797
shape of L is 
torch.Size([])
memory (bytes)
4449415168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4449415168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204848770.0
relative error loss 0.15392661
time to take a step is 337.7500035762787
it  13 : 1394311168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4452675584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4452675584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  204848770.0
relative error loss 0.15392661
shape of L is 
torch.Size([])
memory (bytes)
4455899136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4455899136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  204829570.0
relative error loss 0.15391219
shape of L is 
torch.Size([])
memory (bytes)
4459126784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4459126784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204813950.0
relative error loss 0.15390044
shape of L is 
torch.Size([])
memory (bytes)
4462354432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4462354432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  8% |
error is  204801800.0
relative error loss 0.15389131
shape of L is 
torch.Size([])
memory (bytes)
4465426432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4465577984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204785020.0
relative error loss 0.15387872
shape of L is 
torch.Size([])
memory (bytes)
4468801536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4468801536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  204766850.0
relative error loss 0.15386505
shape of L is 
torch.Size([])
memory (bytes)
4471988224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4472025088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204756350.0
relative error loss 0.15385717
shape of L is 
torch.Size([])
memory (bytes)
4475244544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  8% |
memory (bytes)
4475244544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  204743420.0
relative error loss 0.15384746
shape of L is 
torch.Size([])
memory (bytes)
4478377984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4478464000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204717570.0
relative error loss 0.15382802
shape of L is 
torch.Size([])
memory (bytes)
4481687552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4481687552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204692860.0
relative error loss 0.15380946
time to take a step is 390.0692296028137
it  14 : 1396008960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4484902912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4484902912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204692860.0
relative error loss 0.15380946
shape of L is 
torch.Size([])
memory (bytes)
4488101888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4488130560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204677500.0
relative error loss 0.15379792
shape of L is 
torch.Size([])
memory (bytes)
4491350016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4491350016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204665090.0
relative error loss 0.1537886
shape of L is 
torch.Size([])
memory (bytes)
4494467072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4494569472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204647550.0
relative error loss 0.15377541
shape of L is 
torch.Size([])
memory (bytes)
4497793024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4497793024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204631300.0
relative error loss 0.1537632
shape of L is 
torch.Size([])
memory (bytes)
4501016576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4501016576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204618750.0
relative error loss 0.15375377
shape of L is 
torch.Size([])
memory (bytes)
4504231936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4504231936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204601470.0
relative error loss 0.1537408
shape of L is 
torch.Size([])
memory (bytes)
4507459584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4507459584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204584830.0
relative error loss 0.15372829
shape of L is 
torch.Size([])
memory (bytes)
4510691328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4510691328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  204576260.0
relative error loss 0.15372184
shape of L is 
torch.Size([])
memory (bytes)
4513906688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4513906688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  8% |
error is  204552450.0
relative error loss 0.15370396
time to take a step is 389.6936318874359
sum tnnu_Z after tensor(5576334., device='cuda:0')
shape of features
(4387,)
shape of features
(4387,)
number of orig particles 17547
number of new particles after remove low mass 17547
tnuZ shape should be parts x labs
torch.Size([17547, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  248045680.0
relative error without small mass is  0.18638545
nnu_Z shape should be number of particles by maxV
(17547, 702)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
shape of features
(17547,)
Wed Feb 1 19:37:59 EST 2023
