Thu Feb 2 13:27:14 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 46618256
numbers of Z: 22867
shape of features
(22867,)
shape of features
(22867,)
ZX	Vol	Parts	Cubes	Eps
Z	0.020008478388479387	22867	22.867	0.09564632552400364
X	0.01561780809440735	2581	2.581	0.18222614699610917
X	0.0174816511904848	35011	35.011	0.07933399266462424
X	0.0160326619340719	4817	4.817	0.14930567992962024
X	0.015943085456548402	4548	4.548	0.15190916363434576
X	0.017618530995581458	34285	34.285	0.08009803225065913
X	0.016984540840348426	56151	56.151	0.06712723702299296
X	0.01723448046672327	61652	61.652	0.06538583519050845
X	0.016424130478104766	57974	57.974	0.06567746375872144
X	0.016282397300741915	7966	7.966	0.12690907592162917
X	0.01731218068540941	104129	104.129	0.054987000027289956
X	0.015951305112590877	12241	12.241	0.10922610359613609
X	0.015927291499288093	104511	104.511	0.05341459933664238
X	0.017538673046555788	10808	10.808	0.1175124277566509
X	0.016070597907265796	299424	299.424	0.03772087940249045
X	0.015912303657002237	23778	23.778	0.08746868117006107
X	0.01879062897668971	75787	75.787	0.0628225461153739
X	0.016601180370095663	59460	59.46	0.06535889298975388
X	0.018628546319112767	38265	38.265	0.07866696877781747
X	0.016230951951798474	211813	211.813	0.04247471370519848
X	0.017198924375323357	117025	117.025	0.05277248506941987
X	0.018177635697804874	41540	41.54	0.07592013430962755
X	0.017845218467087393	376236	376.236	0.0361982165272465
X	0.01600117560748706	16058	16.058	0.09988190402977573
X	0.018277070773952075	31123	31.123	0.08374168853809615
X	0.016159597454705913	12634	12.634	0.10855001517226917
X	0.01725755332959718	79928	79.928	0.05999201180551258
X	0.017964183554976382	61647	61.647	0.06629770767007497
X	0.017453705054580007	15782	15.782	0.10341301446027011
X	0.01597918093951371	71919	71.919	0.06056712486953965
X	0.018750192169897928	1790706	1790.706	0.021877304493497257
X	0.015956250812332783	11900	11.9	0.11027099609004831
X	0.01786325514846624	541605	541.605	0.032069533720363155
X	0.018000067930611003	16264	16.264	0.10343851082156426
X	0.016446434541397242	10303	10.303	0.11686990392047528
X	0.017617651671895183	20160	20.16	0.09560615014365322
X	0.019320155942158933	195797	195.797	0.04620988255570488
X	0.016149773276926644	116004	116.004	0.05182800456749546
X	0.01619272795483741	2242	2.242	0.19329834564587575
X	0.01632112521710778	6152	6.152	0.13843458081437415
X	0.015736842973765566	3916	3.916	0.15898579006559327
X	0.015943150702338288	4897	4.897	0.14821131108574162
X	0.015819167855145547	2336	2.336	0.18919236172691314
X	0.015765349900465894	1958	1.958	0.2004304226107696
X	0.015698277620761546	4274	4.274	0.1542904914841045
X	0.01751243607174127	1749	1.749	0.21553555285520667
X	0.0164908398266787	4427	4.427	0.1550164369776467
X	0.017725868533464962	8546	8.546	0.12753073964729314
X	0.015940224325877817	5375	5.375	0.14367195015568676
X	0.017155841636712188	2648	2.648	0.18642236617081248
X	0.017022563494829404	17844	17.844	0.09844135495556008
X	0.016766574516664825	15087	15.087	0.10358109179135216
X	0.018755400686481856	2899	2.899	0.18633425892273858
X	0.01586059799257805	7403	7.403	0.128915143400376
X	0.01579326132689195	3422	3.422	0.16649349580406964
X	0.016938086204526408	7015	7.015	0.13415665497529705
X	0.01595913715642616	7864	7.864	0.12660629528533834
X	0.01742368292799386	3137	3.137	0.1770958891426052
X	0.015873069971365172	4657	4.657	0.1504936692090882
X	0.01607948366526654	4581	4.581	0.15197445770460266
X	0.018032648209235044	5157	5.157	0.15178218237615182
X	0.017271964639219938	7478	7.478	0.13218598952888885
X	0.015934860519777504	2728	2.728	0.18009485258713645
X	0.019738311337305303	14202	14.202	0.11159725071155352
X	0.015871220954534	11674	11.674	0.11078055832153033
X	0.01597270290086552	4199	4.199	0.15610285510808444
X	0.016404715395035168	2998	2.998	0.1762161427367114
X	0.018487728209693455	9482	9.482	0.12492840607466851
X	0.018200525246866363	8082	8.082	0.13107555092320905
X	0.015863708294870403	5685	5.685	0.14078555009824156
X	0.015805698081181617	4181	4.181	0.1557798105811148
X	0.0159553249587804	5036	5.036	0.1468723275757333
X	0.01578263662856353	3280	3.28	0.16882440896572987
X	0.016800349078421515	5127	5.127	0.14853141875093842
X	0.016302952404388	2673	2.673	0.18270724426987045
X	0.017652963920072002	5741	5.741	0.14541527498721285
X	0.0171984597399498	9441	9.441	0.12213042043209049
X	0.015637621663268233	2197	2.197	0.19235945955387235
X	0.015838975263111087	1338	1.338	0.2279066455706049
X	0.016404552530746918	4902	4.902	0.14957661266035818
X	0.01945151101101998	12986	12.986	0.11441755110813229
X	0.018007507374338157	6229	6.229	0.1424552382751479
X	0.01579893060711854	2112	2.112	0.19557406346236933
X	0.016029604485174292	6876	6.876	0.13259555930440398
X	0.01817182032438116	2915	2.915	0.18404327733068285
X	0.016628735356365762	2659	2.659	0.18423845997868166
X	0.01638227105526498	2128	2.128	0.19745470859124592
X	0.01811233777863221	11079	11.079	0.1178034426005435
X	0.015814975519997526	2124	2.124	0.19527111399809308
X	0.01825189119108352	5620	5.62	0.14808935849914107
X	0.015951390115007884	3728	3.728	0.1623457327119532
X	0.016477416921962827	6870	6.87	0.13385793815854963
X	0.015722879241432647	2585	2.585	0.1825396347268333
X	0.01585382198159747	2091	2.091	0.19645359541136498
X	0.01798743997491796	5578	5.578	0.14773961034222016
X	0.016688783830303804	6348	6.348	0.1380159429881879
X	0.017731592920516563	5863	5.863	0.14461364944258112
X	0.01580906713370406	5198	5.198	0.14488496587966349
X	0.015697756629428804	7081	7.081	0.13039101492875238
X	0.016110335638593456	4468	4.468	0.15334294791388428
X	0.015874228782488817	3969	3.969	0.15873420165875912
X	0.01647036949040382	22329	22.329	0.09035343947579882
X	0.016893417817222662	8126	8.126	0.12762827569660082
X	0.018468506733569645	11540	11.54	0.11697022476312723
X	0.015665145043671027	2489	2.489	0.18463038469601192
X	0.01845698619674257	6827	6.827	0.1393082900656093
X	0.016399968540707008	3185	3.185	0.17268098850701757
X	0.018277286920029487	10846	10.846	0.11900011373550602
X	0.01597425919904841	1084	1.084	0.24516801733429244
X	0.016944904503246424	6744	6.744	0.1359483229938259
X	0.01585129064861682	2461	2.461	0.18605913826368325
X	0.015740451624693803	2387	2.387	0.1875231645405044
X	0.015941162144086505	1768	1.768	0.20813537804121693
X	0.018202023368996584	2973	2.973	0.1829398222609312
X	0.016833053306769902	4526	4.526	0.15493496904431311
X	0.015949424044405157	5426	5.426	0.143247946547609
X	0.016851787303747184	2546	2.546	0.1877568785668264
X	0.015911361208120894	2938	2.938	0.17561105331800203
X	0.016697275389379585	1872	1.872	0.20738623243599888
X	0.015941693902593094	9025	9.025	0.12088221756324903
X	0.015749525279243524	2496	2.496	0.18478822576002601
X	0.01724175675187601	18088	18.088	0.09841553434236872
X	0.016737973267700965	3588	3.588	0.16708987925282448
X	0.016727104244233224	3331	3.331	0.17124400519785035
X	0.016043708618222737	3846	3.846	0.16097749388966237
X	0.015844902729076	2167	2.167	0.19409315609680647
X	0.01833677982836098	3116	3.116	0.18054073539387328
X	0.01570292964497538	2013	2.013	0.19832568820742527
X	0.015941257472351006	4423	4.423	0.15332108324248764
X	0.01898823984034316	12649	12.649	0.1145010595145767
X	0.015994123978345594	5541	5.541	0.14238276943587036
X	0.017541056584293183	10121	10.121	0.12011874191371566
X	0.015999961736884807	2246	2.246	0.19241388009205035
X	0.01594407243052884	6011	6.011	0.13842594920600632
X	0.018249303293588676	2449	2.449	0.19532260207166696
X	0.01675950569783904	4839	4.839	0.1512986993006801
X	0.015841829551195036	4006	4.006	0.15813626016385265
X	0.015842162272412426	2175	2.175	0.1938437184080093
X	0.01606487326019111	2962	2.962	0.17569689364490004
X	0.016803392668229018	3000	3.0	0.17759275336813551
X	0.015884471806132804	2537	2.537	0.1843107941146246
X	0.015870210500625342	2856	2.856	0.1771229464225481
X	0.015615727484444928	1436	1.436	0.22154872778031523
X	0.016569544894220214	8496	8.496	0.12493919175251736
X	0.016821923771619292	14224	14.224	0.10575103802855289
X	0.01692976634432547	5565	5.565	0.14489784680327425
X	0.018281812844480647	4513	4.513	0.15941088580212337
X	0.01864640627488296	2564	2.564	0.19374286035956564
X	0.017471938053355802	4550	4.55	0.15659452057868414
X	0.017605783661437533	2159	2.159	0.20128013090476057
X	0.015791011255224658	2871	2.871	0.17651932180366972
X	0.015897221139213855	3704	3.704	0.16251124315207932
X	0.017047032267644965	6404	6.404	0.13859018888066177
X	0.015990620608020204	4764	4.764	0.14972621828590177
X	0.018490581049826444	21816	21.816	0.09463645811383982
X	0.017934594875122722	3578	3.578	0.1711395601781759
X	0.01926416830382158	13254	13.254	0.11327509433653843
X	0.015901225479767758	4247	4.247	0.1552802106895453
X	0.015876762576429684	3382	3.382	0.1674413752532539
X	0.018140533347539164	7915	7.915	0.13184582359551536
X	0.017081032773383117	1632	1.632	0.21874156946944753
X	0.018025759157033878	10210	10.21	0.12086186013700218
X	0.01574353865712329	2492	2.492	0.18486361396424844
X	0.017611232810046624	5780	5.78	0.14497306026194362
X	0.01617022889004738	1728	1.728	0.21072892083143097
X	0.0163699728235355	3638	3.638	0.1650929094558128
X	0.015858267610511203	2546	2.546	0.18399207376575075
X	0.01687277399026039	3778	3.778	0.16468024781883245
X	0.01582731602197425	5436	5.436	0.14279377507338065
X	0.016207929489848404	4628	4.628	0.1518604664358849
X	0.01580183394610905	1113	1.113	0.2421423631096798
X	0.017639150676973005	1599	1.599	0.22260917942998856
X	0.0158186204581293	2391	2.391	0.18772826942664242
X	0.017550862793948616	7076	7.076	0.13536411522515232
X	0.015902641159759895	2956	2.956	0.17522185613116154
X	0.01773303307381244	6323	6.323	0.1410219170212065
X	0.018459012707269646	4499	4.499	0.16008998301266691
X	0.015899802275270398	5784	5.784	0.14008376739982936
X	0.016579721879945157	5757	5.757	0.14227447898701898
X	0.015845035143874885	6201	6.201	0.13671321353774718
X	0.01600554913470679	3989	3.989	0.1589042504964378
X	0.015865840652371597	4389	4.389	0.15347318372657193
X	0.01620862627895024	4847	4.847	0.14954012168864653
X	0.016392611219646407	5105	5.105	0.14753128693342732
X	0.01587827780138698	2423	2.423	0.18713283365646066
X	0.01587629431320471	5640	5.64	0.14119630956372378
X	0.018699623790823978	12599	12.599	0.11406846510861893
X	0.0159481327496238	3831	3.831	0.1608666051119031
X	0.015911056337755986	4160	4.16	0.1563874485738285
X	0.01605382291683093	5773	5.773	0.14062384557603894
X	0.0162004488825144	15853	15.853	0.10072529071031557
X	0.018409698331673194	3111	3.111	0.1808765310646573
X	0.015806322262987203	1112	1.112	0.24223785655793836
X	0.017243234466993603	7203	7.203	0.13377299165982634
X	0.017616300323463934	2561	2.561	0.19018154696735007
X	0.01658179955157315	4899	4.899	0.15014403182260802
X	0.01706800669777497	5294	5.294	0.1477291806868457
X	0.01583553151922489	2839	2.839	0.17734641652242833
X	0.015758910501514144	2450	2.45	0.1859744864989956
X	0.01585800221853226	4897	4.897	0.14794698645960566
X	0.015899953110599257	1909	1.909	0.20270446033079945
X	0.01718471200344367	6016	6.016	0.141887720050318
X	0.01598741162991627	2324	2.324	0.1901869342211877
X	0.016262831116720965	10065	10.065	0.1173439736988552
X	0.01794883983465551	7098	7.098	0.13623858648805579
X	0.01583312720011375	4146	4.146	0.15630725431267198
X	0.015788451722140717	2105	2.105	0.19574731532985565
X	0.01827468926894796	6161	6.161	0.14368108453398631
X	0.01589402171212528	4144	4.144	0.15653255896718524
X	0.017598774290733317	7254	7.254	0.1343697853435853
X	0.018481837073061346	10760	10.76	0.11975977057538005
X	0.01854997318050806	6363	6.363	0.14285445465342775
X	0.015930842711311944	6967	6.967	0.1317440407780675
X	0.015858762181169102	4259	4.259	0.15499602083123834
X	0.015956134392447998	3988	3.988	0.15875381794342452
X	0.01575923139961334	1188	1.188	0.23672249558864958
X	0.01595038369935424	1958	1.958	0.20121150793000003
X	0.015828424851050106	1596	1.596	0.2148497918230496
X	0.015568943941032877	1296	1.296	0.22902596292639174
X	0.015923182326397795	3876	3.876	0.16015802324002437
X	0.015904484646770686	3965	3.965	0.1588883798010675
X	0.015835742282395874	4708	4.708	0.14983064773407723
X	0.01774930202794132	3054	3.054	0.17979215956247252
X	0.015817999202432684	5625	5.625	0.1411485239292884
X	0.015815385524879357	3700	3.7	0.16229034605442802
X	0.017126813552946638	4348	4.348	0.1579291670531917
X	0.018484205768088062	4078	4.078	0.16549486552677006
X	0.015822695547037365	2341	2.341	0.1890716231894399
X	0.015765463947290267	2426	2.426	0.1866116071117424
X	0.01744759367888865	4849	4.849	0.1532361098693865
X	0.015820073034570604	1376	1.376	0.22569919845863518
X	0.015787594150186095	1837	1.837	0.20483410364239985
X	0.015911752272779522	3070	3.07	0.17305860996928393
X	0.01953041308605292	6274	6.274	0.14601214182081818
X	0.01901462889195754	4038	4.038	0.16761279945964974
X	0.016276110873435112	3048	3.048	0.1747881124119895
X	0.015966426693252905	2912	2.912	0.1763351074933449
X	0.0169010139164986	4342	4.342	0.15730443967184307
X	0.017660736408830188	7820	7.82	0.1312000554292902
X	0.015849498016986204	3158	3.158	0.17121210312515708
X	0.016545291640819564	13107	13.107	0.10807461799197866
X	0.01866044700695225	59179	59.179	0.06806410532165279
X	0.016293019353872874	18554	18.554	0.09576085324354598
X	0.01670604144228001	6862	6.862	0.13452642886104604
X	0.01580082899647667	1226	1.226	0.23445696253622778
X	0.017068126684418285	7168	7.168	0.1335352591753826
X	0.018539370933798146	171677	171.677	0.04762053747053705
X	0.01597809460080557	310928	310.928	0.03717820218840214
X	0.01592754951410797	4184	4.184	0.15614176922440134
X	0.01607090577870722	114204	114.204	0.052013931315169626
X	0.016783368506403613	28368	28.368	0.08394924681154144
X	0.015949292926084203	33172	33.172	0.07834114701717329
X	0.01911147902650696	163425	163.425	0.048901847635045924
X	0.015798450093621297	2474	2.474	0.185526063203917
X	0.01601850947817724	7536	7.536	0.1285761461100832
X	0.016080381813977838	296286	296.286	0.03786126107054147
X	0.016312168928627614	477528	477.528	0.03244673159382591
X	0.015980119317588113	6061	6.061	0.13814820442452969
X	0.016230552574287873	34348	34.348	0.07788920728583672
X	0.015989502511386647	19056	19.056	0.09431940863250082
X	0.015991784064924457	78621	78.621	0.05881021735509283
X	0.017264667393440083	80389	80.389	0.05988534141993355
X	0.019246260500232534	48201	48.201	0.07363711590353805
X	0.017820773759351958	43833	43.833	0.07408127592864594
X	0.015920394443134533	14245	14.245	0.10377604691810134
X	0.01659394787910368	3911	3.911	0.16189022819596083
X	0.018062011944476118	532609	532.609	0.03236824913042596
X	0.015962856517539706	6644	6.644	0.13393487048955036
X	0.01586044149093393	4036	4.036	0.1578052223623171
X	0.01887007482458006	52290	52.29	0.0711953550433381
X	0.01582908440353403	40787	40.787	0.07294205770372805
X	0.019264329710114603	309807	309.807	0.039617544897747804
X	0.016210519269766795	114845	114.845	0.05206688687025456
X	0.01633692919887699	3515	3.515	0.16688412750462386
X	0.016073549235524145	36006	36.006	0.07642679294426985
X	0.018306457258163072	12316	12.316	0.11412432150615873
X	0.016058296445002545	49837	49.837	0.06855658770582494
X	0.01604964204836904	72619	72.619	0.06046049459253768
X	0.01586667447983916	5243	5.243	0.1446445311329552
X	0.01763144832744632	60415	60.415	0.06633067288167954
X	0.015817263463464137	1009	1.009	0.250272651138394
X	0.016897921914278094	6870	6.87	0.13498707513964217
X	0.01598459052543677	82672	82.672	0.057824828993903794
X	0.016055427640741284	62900	62.9	0.06343424044764541
X	0.0160189361153669	53688	53.688	0.06682189914003898
X	0.017352691527304984	20028	20.028	0.09533296225412019
X	0.01727581068966644	276672	276.672	0.039672637645263596
X	0.01941299692834106	16707	16.707	0.105131139350507
X	0.01708705340605761	27389	27.389	0.08544703557287364
X	0.017906534206401904	67150	67.15	0.06436580026274458
X	0.016231869142483136	8374	8.374	0.12468432507712549
X	0.0165134487517212	135657	135.657	0.04956006395377388
X	0.01872803304651317	263549	263.549	0.04142001623460709
X	0.017600666006188488	313118	313.118	0.038306607264875836
X	0.01601302240387969	33698	33.698	0.07803505277793917
X	0.015925697567104654	31317	31.317	0.07981896718321553
X	0.01582605375224882	24057	24.057	0.08697151915592893
X	0.015932493882995204	6798	6.798	0.13283143570217928
X	0.016075658754963126	27149	27.149	0.08397273795544839
X	0.01786635905819302	9497	9.497	0.12344775790903931
X	0.015966535147029764	19867	19.867	0.0929735605511735
X	0.017362076683942605	135044	135.044	0.0504710250960579
X	0.016074129019951183	61806	61.806	0.06383109148984041
X	0.01785740212879633	12027	12.027	0.11408283960031651
X	0.015956805171917554	9079	9.079	0.12068018727488744
X	0.017259701122685826	69072	69.072	0.06298595906018158
X	0.016311287695363375	19781	19.781	0.09377347316231899
X	0.01590676562357363	25626	25.626	0.08530353328511987
X	0.01874749707264059	13988	13.988	0.11025438986861072
X	0.017976712300974976	352692	352.692	0.0370770255649263
X	0.01599145847819389	14327	14.327	0.10373156686394987
X	0.015962478634458717	37257	37.257	0.07538717728874889
X	0.016948070360977407	7397	7.397	0.13183221425217498
X	0.015943737038544105	43164	43.164	0.07175014994575173
X	0.017740526798811926	28545	28.545	0.08533859495233516
X	0.01654859152262586	375961	375.961	0.03530796982006148
X	0.016466985423188526	59507	59.507	0.06516514234978787
X	0.016669912241536782	13957	13.957	0.1060995996101916
X	0.017105477823695515	127970	127.97	0.051129986740570704
X	0.016313475902378725	126175	126.175	0.050565898757075824
X	0.016272422857800804	8888	8.888	0.1223346240902526
X	0.01608626047915394	96621	96.621	0.055012475682956384
X	0.016583529847727943	118496	118.496	0.05191877402796613
X	0.018210510448763127	302681	302.681	0.03918432047777668
X	0.016477408239416	68911	68.911	0.06206786161836228
X	0.015626404318684443	2521	2.521	0.18369418299125392
X	0.016193151912622966	25168	25.168	0.08632983183880492
X	0.018482075107630155	113322	113.322	0.05463599207566904
X	0.016229282825821038	71732	71.732	0.06093434254375545
X	0.01765491290036906	26874	26.874	0.08693161651258982
X	0.018365381162818457	7193	7.193	0.13667737482356457
X	0.01611516551625563	57534	57.534	0.0654289675244465
X	0.01814985354568915	170789	170.789	0.047366476154519103
X	0.01849185123984278	74212	74.212	0.06292676683842517
X	0.01728787694791773	6734	6.734	0.1369271336103157
X	0.017926801861548526	28724	28.724	0.08545798205013216
X	0.01695972062033792	12674	12.674	0.11019663433317788
X	0.016541376599065648	8372	8.372	0.12548182022715812
X	0.0170723057982226	17439	17.439	0.09929412075641739
X	0.018073364348539865	6848	6.848	0.1381948155979833
X	0.01589172186622665	30477	30.477	0.08048832942520086
X	0.01610700335859531	48099	48.099	0.06944264174828683
X	0.017939163701371205	21844	21.844	0.0936461674872956
X	0.018400108560869887	21366	21.366	0.09514056374019805
X	0.015926656673502487	8218	8.218	0.12467694739662213
X	0.01818683904243521	440604	440.604	0.03455967071691089
X	0.01828707685669006	18210	18.21	0.10014089028887231
X	0.01839005157276018	114835	114.835	0.054304550615380594
X	0.015937230442764124	2554	2.554	0.18410432302664767
X	0.01860615489240382	10415	10.415	0.12133863897259795
X	0.016524187066843228	5045	5.045	0.148509069657971
X	0.016118028641573228	34457	34.457	0.07762676480489918
X	0.01778183964476372	4981	4.981	0.15283402447809566
X	0.015961061408723203	157236	157.236	0.04664838296471218
X	0.01844896350846962	8887	8.887	0.12756717278514795
X	0.016691228871764344	8746	8.746	0.12403938751121804
X	0.016577552846501854	186738	186.738	0.04460969349651475
X	0.015966412927458064	38447	38.447	0.07460735320316549
X	0.017585041799024936	36833	36.833	0.07815716011266384
X	0.018966244402823906	185938	185.938	0.046723754045773065
X	0.017620719029039344	251744	251.744	0.041211792000743076
X	0.01878352802756504	7575	7.575	0.13535205171478465
X	0.015887812999359926	6422	6.422	0.13524813114740175
X	0.01591786740992893	22280	22.28	0.08939705912056096
X	0.016564349649218425	1931	1.931	0.20470606268702754
X	0.017392009243090038	9596	9.596	0.12192324307477456
X	0.016563586636621486	21012	21.012	0.09237667591049191
X	0.0159219186247581	3950	3.95	0.1591473585378681
X	0.018396960035584366	12476	12.476	0.11382130647905347
X	0.01601836804075663	10153	10.153	0.11641473678825602
X	0.015979965516505375	6788	6.788	0.13302849202147762
X	0.015997623411021837	334445	334.445	0.03630030331212364
X	0.016635650544584327	13272	13.272	0.10782044963512404
X	0.01598160056635536	92168	92.168	0.05576307349769117
X	0.016019232889608725	13335	13.335	0.10630398987659065
X	0.018409882505759523	25604	25.604	0.08958756503639784
X	0.017101835667759857	72906	72.906	0.06167272705709278
X	0.01744510489194952	703585	703.585	0.02915987587762775
X	0.016705462005374518	674002	674.002	0.02915630168137252
X	0.01639584949831008	26788	26.788	0.08490459817814705
X	0.017287515140189478	110804	110.804	0.053834296851218384
X	0.018543240492316416	5483	5.483	0.1501029156870329
X	0.0162505815401602	22809	22.809	0.08931427707449922
X	0.016524181184283247	55594	55.594	0.0667365894645005
X	0.018267738493461184	48069	48.069	0.072433571487315
X	0.015945782319560124	6976	6.976	0.13172851209695696
X	0.015977520091861253	74557	74.557	0.05984212163490005
X	0.018459663149654765	403343	403.343	0.03576979209226155
X	0.01733863158643937	50478	50.478	0.07003324122260475
X	0.01659819244854508	11440	11.44	0.11320829693947332
X	0.019669068348018046	59017	59.017	0.0693322913545595
X	0.017846055150951643	11900	11.9	0.11446299090782704
X	0.015981938763625858	4609	4.609	0.15135876822099606
X	0.018494428178536387	152538	152.538	0.04949419747306126
X	0.01672510527530254	34961	34.961	0.07820988930968444
X	0.01584651231763584	2653	2.653	0.18143962830538496
X	0.016468219766495894	134932	134.932	0.049603299465690955
X	0.01965642390824105	21825	21.825	0.09657174650490201
X	0.018052976343578903	3192	3.192	0.17816756454294286
X	0.016237357894928375	142670	142.67	0.048461207815474645
X	0.01630960505383787	97183	97.183	0.05515916895665136
X	0.01948897601889124	101481	101.481	0.05769454610614217
X	0.017504503941289794	193731	193.731	0.04487284058498545
X	0.016475995093793196	141945	141.945	0.04878023088876493
X	0.018560466110207462	16150	16.15	0.10474632238382384
X	0.0164923194367412	62169	62.169	0.06425436245036416
X	0.017104521225588108	159060	159.06	0.047553590623058486
X	0.017281635465810942	210354	210.354	0.043472173570135685
X	0.018691587506075715	18456	18.456	0.1004236962580398
X	0.016437375555415745	244710	244.71	0.04064997226503417
X	0.018837781082809077	241533	241.533	0.04272523137829367
X	0.016297889105563384	95496	95.496	0.05546879429260054
X	0.017440659398568863	43008	43.008	0.07401809177682338
X	0.018607032275438553	29171	29.171	0.08608125655408895
X	0.016037313720594634	12978	12.978	0.10731026936928731
X	0.01593391797864633	8188	8.188	0.12484799756207718
X	0.01591111684665456	6058	6.058	0.13797184294031792
X	0.016889849781232782	141564	141.564	0.04922937481315196
X	0.017652752777127044	136437	136.437	0.05057781837223628
X	0.01604395860827049	317715	317.715	0.03696221065782611
X	0.017210483882914147	234794	234.794	0.041850632768442884
X	0.01862603844271023	79811	79.811	0.06156765052648576
X	0.01637185029290256	13554	13.554	0.10649848452632055
X	0.018987140844549843	130611	130.611	0.05258072933343285
X	0.01591823738591202	4167	4.167	0.1563233399668142
X	0.016038972878495653	14827	14.827	0.10265365611303691
X	0.016487251091131402	234569	234.569	0.04126918368887129
X	0.018408502901392548	32060	32.06	0.08311606692362095
X	0.016332866278024882	4943	4.943	0.14894431518210954
X	0.0159421790621912	14619	14.619	0.10293032377936001
X	0.018346929868828035	98399	98.399	0.05712912495129681
X	0.015902308746228017	27440	27.44	0.08337298638604432
X	0.01817911100734527	163542	163.542	0.0480818436992993
X	0.015960524351478562	15195	15.195	0.10165189602591218
X	0.016018057141841756	19412	19.412	0.09379503547346824
X	0.01591249676098015	7607	7.607	0.12789148711122192
X	0.01665125353397602	3127	3.127	0.1746249725067851
X	0.015996889918960536	56951	56.951	0.06549046398854456
X	0.01676793550030805	73578	73.578	0.061081618854052555
X	0.016502435499118107	7666	7.666	0.12911986887707075
X	0.016567827173097543	126519	126.519	0.05078123612361014
X	0.015991575109273445	5283	5.283	0.14465614165143476
X	0.01741050960087312	39026	39.026	0.07641040136365844
X	0.01602735988767916	7767	7.767	0.1273120646519163
X	0.016649027114575433	129228	129.228	0.050506128481989154
X	0.019082402419059327	15840	15.84	0.10640432744330566
X	0.016995471198590427	64029	64.029	0.06426662554013096
X	0.01602368425790291	25835	25.835	0.08528080757180423
X	0.017448589194937435	34214	34.214	0.0798948567247576
X	0.016008004502712968	56815	56.815	0.06555785402746873
X	0.017764998657044225	391494	391.494	0.03566810388178461
X	0.016319778506785943	15698	15.698	0.10130323442730703
X	0.015826035869326602	12841	12.841	0.10721554723093368
X	0.017411880204643234	100370	100.37	0.055771704218831986
X	0.01661635061283995	17785	17.785	0.09775986488151266
X	0.01821627399831536	581742	581.742	0.03151928642465722
X	0.01588717272364003	5152	5.152	0.1455538132308152
X	0.017596928950244783	115680	115.68	0.05338175735155122
X	0.01808039278067597	383238	383.238	0.03613375525470673
X	0.016460704662125007	5157	5.157	0.14723704650684244
X	0.01864147065909566	207688	207.688	0.04477369355491969
X	0.01780581127633042	30689	30.689	0.083405177240362
X	0.019308316106534235	708215	708.215	0.030097223024285746
X	0.016502897784217388	10829	10.829	0.1150775530431675
X	0.01649181040152498	41327	41.327	0.07362265657923342
X	0.016000075179088	7107	7.107	0.13106253665621354
X	0.016223880892377574	8496	8.496	0.12406427686710611
X	0.01707656542231998	10971	10.971	0.11589146078501886
X	0.017479227065779656	59258	59.258	0.06656691211507075
X	0.016001689909735203	34366	34.366	0.07750783774868467
X	0.017230072621061913	119092	119.092	0.052497042773089406
X	0.016242657480026443	18773	18.773	0.09528864148622301
X	0.017270843221743132	5562	5.562	0.1458906701072797
X	0.015972685359140357	6991	6.991	0.13170821932448123
X	0.019044234665286142	220922	220.922	0.04417482652260081
X	0.019157952776235786	77872	77.872	0.06265983638023982
X	0.018681078999287202	306657	306.657	0.039347428792911886
X	0.01837508831074405	41394	41.394	0.07628351211604145
X	0.01599944864553116	8710	8.71	0.12246978457513312
X	0.01604682515841639	6293	6.293	0.13661879893664672
X	0.015923566452918234	21943	21.943	0.08986311051986581
X	0.017892835366168647	777009	777.009	0.02845013766783081
X	0.0166823647447408	12444	12.444	0.11026369368342816
X	0.015987613169416368	22933	22.933	0.08866949877961976
X	0.01598846174610161	106091	106.091	0.05321605949145653
X	0.017063166801222825	62282	62.282	0.06494798704089041
X	0.01588597354798501	8150	8.15	0.1249161936024156
X	0.015661746859695902	4660	4.66	0.1497906692520702
X	0.01821380881660659	80194	80.194	0.061012614048758164
X	0.016563936484217764	50777	50.777	0.06883862272933519
X	0.015834178533071074	39972	39.972	0.07344234796358319
X	0.01607478661034908	67311	67.311	0.06204208598325487
X	0.01848796713543374	246495	246.495	0.04217227359698386
X	0.018691840205287778	109211	109.211	0.05552158964369024
X	0.01912821530081791	267914	267.914	0.0414851833919171
X	0.01804237328150177	133710	133.71	0.05129124705868006
X	0.01591810881159394	21843	21.843	0.08998975327368114
X	0.01624125427441941	59155	59.155	0.0649944144860022
X	0.016742361391797977	60913	60.913	0.06501829464853662
X	0.018001595257961363	24672	24.672	0.0900261916805154
X	0.016229780580954076	14158	14.158	0.10465747360317353
X	0.015973242276121114	8731	8.731	0.12230466720390765
X	0.016022715097880787	50055	50.055	0.06840631799512943
X	0.01674106713659436	23511	23.511	0.08929709804461412
X	0.0158017980023088	3316	3.316	0.16827928625295582
X	0.015920776443067615	21935	21.935	0.08986878463963992
X	0.016051293086476102	17890	17.89	0.09644948093497735
X	0.01775247661521413	48436	48.436	0.07156437667084094
X	0.016862109228285377	8600	8.6	0.12516161230314127
X	0.016017385847659618	21597	21.597	0.09051753871295959
X	0.016643140269406827	8965	8.965	0.12290276679251184
X	0.016635557085197258	24723	24.723	0.08762850317442773
X	0.015887204939598	5880	5.88	0.1392804166408906
X	0.01585416548306739	12397	12.397	0.1085447764620961
X	0.01602344438719421	26260	26.26	0.08481780943651632
X	0.016053726202804083	250529	250.529	0.04001651671754154
X	0.018145690802349363	9924	9.924	0.12228192030959771
X	0.0159326103385735	2825	2.825	0.17800115795602045
X	0.016018369509974845	5503	5.503	0.14278182596420388
X	0.0161049882206475	199149	199.149	0.04324415617990291
X	0.01885022328541809	254324	254.324	0.042005844253295274
X	0.016002692739718154	52182	52.182	0.06743584162853535
X	0.01820899749857166	14299	14.299	0.10839105831454414
X	0.016022247838976718	172902	172.902	0.04525235055516493
X	0.01906166336765225	185677	185.677	0.04682389763481364
X	0.015761788605817686	3001	3.001	0.17382537208775176
X	0.017591476612251487	16733	16.733	0.10168170692172253
X	0.01606745228279654	42411	42.411	0.07235849112382695
X	0.01873402794038045	27147	27.147	0.0883696442981975
X	0.019485798881672272	35596	35.596	0.08180358545227608
X	0.016478523155621344	116410	116.41	0.052116590856415804
X	0.017307098474573752	24067	24.067	0.08959162948914208
X	0.015919907251063715	8127	8.127	0.1251228873968288
X	0.016376871873702535	4437	4.437	0.1545422298189955
X	0.01701364799409832	29403	29.403	0.0833301057729219
X	0.015950891137982804	25633	25.633	0.08537456512429907
X	0.015986497810523276	32417	32.417	0.07900601747835635
X	0.017850916323760083	132356	132.356	0.05128286662721943
X	0.01705080837809525	366329	366.329	0.03597144557848329
X	0.015934193431707037	9714	9.714	0.11793534864684423
X	0.01584295080950819	3884	3.884	0.1597787248482722
X	0.017142519413375686	12944	12.944	0.10981678249473056
X	0.016988820395091343	41011	41.011	0.07454541816951721
X	0.01793495704720469	32899	32.899	0.0816906869013403
X	0.015957852835247928	4999	4.999	0.1472415700205974
X	0.017750281181478905	6882	6.882	0.13713980352543925
X	0.017661702713383465	70022	70.022	0.06318287055101501
X	0.016010305183492772	5626	5.626	0.14170982471748508
X	0.01653520015294799	73223	73.223	0.06089579613116438
X	0.017352984092117767	7247	7.247	0.13378434676796655
X	0.01655690295060331	18370	18.37	0.09659544530974949
X	0.016448234091789152	12775	12.775	0.10878928517884263
X	0.01803351636964181	11191	11.191	0.11723856864984003
X	0.015934593170630885	6050	6.05	0.1381004843481952
X	0.01818682832593009	107295	107.295	0.055342549358098465
X	0.018253934718190503	349080	349.08	0.037394739189793094
X	0.01612097011060903	154203	154.203	0.04710851887643679
X	0.016194438631406505	88144	88.144	0.056849203074440344
X	0.016471783129072878	27860	27.86	0.08393050467071345
X	0.01628238063852157	10984	10.984	0.11402124873257208
X	0.0197497488469391	20968	20.968	0.09802454980696122
X	0.01740612993998099	178718	178.718	0.046009195759490175
X	0.01908223641673959	18759	18.759	0.10057109907219297
X	0.017207853113353544	30510	30.51	0.08262183333907583
X	0.015944192824199024	9514	9.514	0.1187808595296079
X	0.019017468522111	584723	584.723	0.0319203400783548
X	0.016453059252000742	30740	30.74	0.0811921919408447
X	0.017727327535510828	48204	48.204	0.07164513912996766
X	0.01954479812271209	240238	240.238	0.04333077882652404
X	0.016845181280375175	45872	45.872	0.07161038431375857
X	0.01709830228105996	28295	28.295	0.08454367020691866
X	0.018669805965065747	217121	217.121	0.044138000772257845
X	0.017863583254661395	120805	120.805	0.05288037864765067
X	0.016015405668905653	43712	43.712	0.07155595056742814
X	0.015896243428435427	16098	16.098	0.09958047508403287
X	0.016036759565037317	39809	39.809	0.07385475298020205
X	0.015883529019931997	15637	15.637	0.10052278714597353
X	0.016084895393558635	110274	110.274	0.05263989781740204
X	0.01883256426180841	767348	767.348	0.029060674996926265
X	0.017222065379445237	136377	136.377	0.050170454300222565
X	0.016533629893067375	138552	138.552	0.04923249031674742
X	0.016054375646395398	12063	12.063	0.10999662607189431
X	0.016106002858954263	80854	80.854	0.05840213904057903
X	0.019752376963633946	52702	52.702	0.07209925151683243
X	0.015910217854934718	11342	11.342	0.11194262700870924
X	0.017635619116023078	29232	29.232	0.08449753898861206
X	0.01901335936989397	395228	395.228	0.03636947656942422
X	0.015920014153420342	37019	37.019	0.07548133780826022
X	0.01655177269355584	151197	151.197	0.04783734996445783
X	0.01655559647654035	71615	71.615	0.06137341349172924
X	0.016536344460291397	55252	55.252	0.06689040657716427
X	0.015896376632641495	7943	7.943	0.12601953142862488
X	0.019029332476752127	89289	89.289	0.05973220290299839
X	0.01789638578686875	166496	166.496	0.04754671420238664
X	0.015961658797445197	3598	3.598	0.16431313732908362
X	0.01821903652939585	679004	679.004	0.029937648934357342
X	0.0163247905187248	52405	52.405	0.06778885481428001
X	0.0160649456589398	83140	83.14	0.05781267706665019
X	0.016324478791688597	8947	8.947	0.12219514813867945
X	0.015965478011033735	22938	22.938	0.08862211815002285
X	0.015916046631082204	3025	3.025	0.17392817551899664
X	0.015614320879073234	5804	5.804	0.13908017615599708
X	0.017705095576452206	32763	32.763	0.08145258490455949
X	0.016055188881497408	96642	96.642	0.054973050464024314
X	0.018556734405624182	2133873	2133.873	0.02056416096088855
X	0.01784205293740079	4767	4.767	0.15526249371850936
X	0.01603891271921129	169926	169.926	0.04553077365711465
X	0.018155081413151482	9774	9.774	0.12292549204584169
X	0.01601646112762637	21729	21.729	0.09033213495795031
X	0.0164461959954553	17050	17.05	0.09880532691155126
X	0.01800725906354806	207982	207.982	0.04423920231184765
X	0.01724830796638512	13935	13.935	0.10736923074872132
X	0.01841404834780903	1034791	1034.791	0.02610751555460185
X	0.015923931965562053	6921	6.921	0.13201617781251576
X	0.016489259750122325	154497	154.497	0.047434431818823085
X	0.01657204192836883	44605	44.605	0.07188928428819591
X	0.017086312460903627	100714	100.714	0.05535873613205416
X	0.018181496619323438	220671	220.671	0.04351391130251591
X	0.01603458069034629	21515	21.515	0.09066480928661362
X	0.017212843139236916	10585	10.585	0.11759456490085753
X	0.019898939644967778	89065	89.065	0.06067935478120132
X	0.017795461232797998	12533	12.533	0.11239614166713816
X	0.016489663213324353	22810	22.81	0.08974883781186106
X	0.016837432428338326	116255	116.255	0.052515568968246135
X	0.016073684715503307	47666	47.666	0.0696042212635501
X	0.01965333101226804	58756	58.756	0.06941627669583192
X	0.01812894906956722	31233	31.233	0.08341669441468469
X	0.01741118157110341	261489	261.489	0.04053140016398864
X	0.016117750785165877	385438	385.438	0.03470962879302376
X	0.016593646946555463	51122	51.122	0.06872446007381564
X	0.015979472280363855	7754	7.754	0.12725617892050228
X	0.0162209649825769	127127	127.127	0.0503438350866409
X	0.015881586537569	39030	39.03	0.0741023153796308
X	0.01836078003577151	46279	46.279	0.07347998413654352
X	0.016341888277263034	60403	60.403	0.06467670842488335
X	0.017915018907923425	75106	75.106	0.06201749390665167
X	0.01694082414705632	90681	90.681	0.05716621142688173
X	0.015839710029160802	202558	202.558	0.04276278111793003
X	0.016238277379625594	31470	31.47	0.08020739367656737
X	0.015922674467046694	20681	20.681	0.09165331141112194
X	0.017987941897191436	8136	8.136	0.1302737292017223
X	0.01690041706256991	19080	19.08	0.09603724297985562
X	0.01602357383616011	62178	62.178	0.06363668467121411
X	0.01595288726232064	132928	132.928	0.0493258156188768
X	0.016340032217734515	144858	144.858	0.04831739088685884
X	0.017788063490834418	12548	12.548	0.11233576666939571
X	0.016962122342035202	22855	22.855	0.09053843610955165
X	0.016566323871697805	74697	74.697	0.06053050946166406
X	0.01631112400495816	120927	120.927	0.05128458791429173
X	0.0176463812156775	73624	73.624	0.0621172302483719
X	0.016592018972325196	72886	72.886	0.061059283858259795
X	0.016035971157208083	118654	118.654	0.05131814635438298
X	0.016149675406272535	32460	32.46	0.07923889807431589
X	0.01599240323120443	2862	2.862	0.17745219157960787
X	0.018089147887483947	8137	8.137	0.13051224626806476
X	0.01687094610772886	184542	184.542	0.045048611027743546
X	0.016239753290758028	33321	33.321	0.07869620690890128
X	0.016248557022689476	82568	82.568	0.058165796286697866
X	0.0159582728261888	2820	2.82	0.17820187082227812
X	0.01628033055934986	4438	4.438	0.15422637122897057
X	0.016209940136546627	98950	98.95	0.054716948692098914
X	0.015944571509620163	104066	104.066	0.05350996459116062
X	0.017473175893525806	59631	59.631	0.0664201602038619
X	0.01593208083866509	4438	4.438	0.1531187592036125
X	0.016233982385537277	8253	8.253	0.12529615439112513
X	0.016652452888623687	46876	46.876	0.07082325606520139
X	0.01641374841438046	40347	40.347	0.07409669571104856
X	0.016791981005242224	113794	113.794	0.0528438182527051
X	0.01600326506627589	9339	9.339	0.11966562343474302
X	0.01672714107160561	33282	33.282	0.07950676830177715
X	0.015852522888700916	28154	28.154	0.08257579449019177
X	0.018475841783305868	24684	24.684	0.09079520194764094
X	0.017945930728482357	296356	296.356	0.03926908248950079
X	0.016037620244908377	12552	12.552	0.10851147452653971
X	0.01609035955668803	41180	41.18	0.07310714132809233
X	0.016943299428861067	117407	117.407	0.05245271810540537
X	0.0162222003604232	91985	91.985	0.05607865421267464
X	0.01797877555999906	111313	111.313	0.05445921886662316
X	0.018254725006942984	171579	171.579	0.04738458203871706
X	0.018223116200851156	8948	8.948	0.12675513331036398
X	0.01792016269102893	49258	49.258	0.07138742626299516
X	0.016032331386642194	23767	23.767	0.08770158419623619
X	0.016465164476175468	101900	101.9	0.05446664563678999
X	0.016269586852997415	103695	103.695	0.05393527568311982
X	0.017880215860368698	43447	43.447	0.074382537155609
X	0.016273844317997187	94327	94.327	0.05566959283839034
X	0.016187765837483804	14519	14.519	0.10369315040824285
X	0.015833801662510652	22016	22.016	0.08959468797927
X	0.015902192882605123	5657	5.657	0.14113138798212782
X	0.017868368023446064	24425	24.425	0.09010525196975254
X	0.017346423998543657	141245	141.245	0.049706397846154535
X	0.01875086712794327	270795	270.795	0.041063904229340205
X	0.016575216374760508	14906	14.906	0.10360149676488951
X	0.01610928278362903	27772	27.772	0.08339813706575759
X	0.01595575512075222	15781	15.781	0.10036777135870575
X	0.018713945241173778	251939	251.939	0.042036187228727026
X	0.018004969737798224	41073	41.073	0.07596472853424582
X	0.01590039776786301	13159	13.159	0.1065111402379395
X	0.018205840698605046	55671	55.671	0.06889598368298604
X	0.01728398275043998	110137	110.137	0.05393907906306653
X	0.01589577562435138	12959	12.959	0.10704590988771653
X	0.016976335909015382	61862	61.862	0.0649840340489947
X	0.01607214638151535	8684	8.684	0.12277728386144415
time for making epsilon is 2.4383363723754883
epsilons are
[0.18222614699610917, 0.07933399266462424, 0.14930567992962024, 0.15190916363434576, 0.08009803225065913, 0.06712723702299296, 0.06538583519050845, 0.06567746375872144, 0.12690907592162917, 0.054987000027289956, 0.10922610359613609, 0.05341459933664238, 0.1175124277566509, 0.03772087940249045, 0.08746868117006107, 0.0628225461153739, 0.06535889298975388, 0.07866696877781747, 0.04247471370519848, 0.05277248506941987, 0.07592013430962755, 0.0361982165272465, 0.09988190402977573, 0.08374168853809615, 0.10855001517226917, 0.05999201180551258, 0.06629770767007497, 0.10341301446027011, 0.06056712486953965, 0.021877304493497257, 0.11027099609004831, 0.032069533720363155, 0.10343851082156426, 0.11686990392047528, 0.09560615014365322, 0.04620988255570488, 0.05182800456749546, 0.19329834564587575, 0.13843458081437415, 0.15898579006559327, 0.14821131108574162, 0.18919236172691314, 0.2004304226107696, 0.1542904914841045, 0.21553555285520667, 0.1550164369776467, 0.12753073964729314, 0.14367195015568676, 0.18642236617081248, 0.09844135495556008, 0.10358109179135216, 0.18633425892273858, 0.128915143400376, 0.16649349580406964, 0.13415665497529705, 0.12660629528533834, 0.1770958891426052, 0.1504936692090882, 0.15197445770460266, 0.15178218237615182, 0.13218598952888885, 0.18009485258713645, 0.11159725071155352, 0.11078055832153033, 0.15610285510808444, 0.1762161427367114, 0.12492840607466851, 0.13107555092320905, 0.14078555009824156, 0.1557798105811148, 0.1468723275757333, 0.16882440896572987, 0.14853141875093842, 0.18270724426987045, 0.14541527498721285, 0.12213042043209049, 0.19235945955387235, 0.2279066455706049, 0.14957661266035818, 0.11441755110813229, 0.1424552382751479, 0.19557406346236933, 0.13259555930440398, 0.18404327733068285, 0.18423845997868166, 0.19745470859124592, 0.1178034426005435, 0.19527111399809308, 0.14808935849914107, 0.1623457327119532, 0.13385793815854963, 0.1825396347268333, 0.19645359541136498, 0.14773961034222016, 0.1380159429881879, 0.14461364944258112, 0.14488496587966349, 0.13039101492875238, 0.15334294791388428, 0.15873420165875912, 0.09035343947579882, 0.12762827569660082, 0.11697022476312723, 0.18463038469601192, 0.1393082900656093, 0.17268098850701757, 0.11900011373550602, 0.24516801733429244, 0.1359483229938259, 0.18605913826368325, 0.1875231645405044, 0.20813537804121693, 0.1829398222609312, 0.15493496904431311, 0.143247946547609, 0.1877568785668264, 0.17561105331800203, 0.20738623243599888, 0.12088221756324903, 0.18478822576002601, 0.09841553434236872, 0.16708987925282448, 0.17124400519785035, 0.16097749388966237, 0.19409315609680647, 0.18054073539387328, 0.19832568820742527, 0.15332108324248764, 0.1145010595145767, 0.14238276943587036, 0.12011874191371566, 0.19241388009205035, 0.13842594920600632, 0.19532260207166696, 0.1512986993006801, 0.15813626016385265, 0.1938437184080093, 0.17569689364490004, 0.17759275336813551, 0.1843107941146246, 0.1771229464225481, 0.22154872778031523, 0.12493919175251736, 0.10575103802855289, 0.14489784680327425, 0.15941088580212337, 0.19374286035956564, 0.15659452057868414, 0.20128013090476057, 0.17651932180366972, 0.16251124315207932, 0.13859018888066177, 0.14972621828590177, 0.09463645811383982, 0.1711395601781759, 0.11327509433653843, 0.1552802106895453, 0.1674413752532539, 0.13184582359551536, 0.21874156946944753, 0.12086186013700218, 0.18486361396424844, 0.14497306026194362, 0.21072892083143097, 0.1650929094558128, 0.18399207376575075, 0.16468024781883245, 0.14279377507338065, 0.1518604664358849, 0.2421423631096798, 0.22260917942998856, 0.18772826942664242, 0.13536411522515232, 0.17522185613116154, 0.1410219170212065, 0.16008998301266691, 0.14008376739982936, 0.14227447898701898, 0.13671321353774718, 0.1589042504964378, 0.15347318372657193, 0.14954012168864653, 0.14753128693342732, 0.18713283365646066, 0.14119630956372378, 0.11406846510861893, 0.1608666051119031, 0.1563874485738285, 0.14062384557603894, 0.10072529071031557, 0.1808765310646573, 0.24223785655793836, 0.13377299165982634, 0.19018154696735007, 0.15014403182260802, 0.1477291806868457, 0.17734641652242833, 0.1859744864989956, 0.14794698645960566, 0.20270446033079945, 0.141887720050318, 0.1901869342211877, 0.1173439736988552, 0.13623858648805579, 0.15630725431267198, 0.19574731532985565, 0.14368108453398631, 0.15653255896718524, 0.1343697853435853, 0.11975977057538005, 0.14285445465342775, 0.1317440407780675, 0.15499602083123834, 0.15875381794342452, 0.23672249558864958, 0.20121150793000003, 0.2148497918230496, 0.22902596292639174, 0.16015802324002437, 0.1588883798010675, 0.14983064773407723, 0.17979215956247252, 0.1411485239292884, 0.16229034605442802, 0.1579291670531917, 0.16549486552677006, 0.1890716231894399, 0.1866116071117424, 0.1532361098693865, 0.22569919845863518, 0.20483410364239985, 0.17305860996928393, 0.14601214182081818, 0.16761279945964974, 0.1747881124119895, 0.1763351074933449, 0.15730443967184307, 0.1312000554292902, 0.17121210312515708, 0.10807461799197866, 0.06806410532165279, 0.09576085324354598, 0.13452642886104604, 0.23445696253622778, 0.1335352591753826, 0.04762053747053705, 0.03717820218840214, 0.15614176922440134, 0.052013931315169626, 0.08394924681154144, 0.07834114701717329, 0.048901847635045924, 0.185526063203917, 0.1285761461100832, 0.03786126107054147, 0.03244673159382591, 0.13814820442452969, 0.07788920728583672, 0.09431940863250082, 0.05881021735509283, 0.05988534141993355, 0.07363711590353805, 0.07408127592864594, 0.10377604691810134, 0.16189022819596083, 0.03236824913042596, 0.13393487048955036, 0.1578052223623171, 0.0711953550433381, 0.07294205770372805, 0.039617544897747804, 0.05206688687025456, 0.16688412750462386, 0.07642679294426985, 0.11412432150615873, 0.06855658770582494, 0.06046049459253768, 0.1446445311329552, 0.06633067288167954, 0.250272651138394, 0.13498707513964217, 0.057824828993903794, 0.06343424044764541, 0.06682189914003898, 0.09533296225412019, 0.039672637645263596, 0.105131139350507, 0.08544703557287364, 0.06436580026274458, 0.12468432507712549, 0.04956006395377388, 0.04142001623460709, 0.038306607264875836, 0.07803505277793917, 0.07981896718321553, 0.08697151915592893, 0.13283143570217928, 0.08397273795544839, 0.12344775790903931, 0.0929735605511735, 0.0504710250960579, 0.06383109148984041, 0.11408283960031651, 0.12068018727488744, 0.06298595906018158, 0.09377347316231899, 0.08530353328511987, 0.11025438986861072, 0.0370770255649263, 0.10373156686394987, 0.07538717728874889, 0.13183221425217498, 0.07175014994575173, 0.08533859495233516, 0.03530796982006148, 0.06516514234978787, 0.1060995996101916, 0.051129986740570704, 0.050565898757075824, 0.1223346240902526, 0.055012475682956384, 0.05191877402796613, 0.03918432047777668, 0.06206786161836228, 0.18369418299125392, 0.08632983183880492, 0.05463599207566904, 0.06093434254375545, 0.08693161651258982, 0.13667737482356457, 0.0654289675244465, 0.047366476154519103, 0.06292676683842517, 0.1369271336103157, 0.08545798205013216, 0.11019663433317788, 0.12548182022715812, 0.09929412075641739, 0.1381948155979833, 0.08048832942520086, 0.06944264174828683, 0.0936461674872956, 0.09514056374019805, 0.12467694739662213, 0.03455967071691089, 0.10014089028887231, 0.054304550615380594, 0.18410432302664767, 0.12133863897259795, 0.148509069657971, 0.07762676480489918, 0.15283402447809566, 0.04664838296471218, 0.12756717278514795, 0.12403938751121804, 0.04460969349651475, 0.07460735320316549, 0.07815716011266384, 0.046723754045773065, 0.041211792000743076, 0.13535205171478465, 0.13524813114740175, 0.08939705912056096, 0.20470606268702754, 0.12192324307477456, 0.09237667591049191, 0.1591473585378681, 0.11382130647905347, 0.11641473678825602, 0.13302849202147762, 0.03630030331212364, 0.10782044963512404, 0.05576307349769117, 0.10630398987659065, 0.08958756503639784, 0.06167272705709278, 0.02915987587762775, 0.02915630168137252, 0.08490459817814705, 0.053834296851218384, 0.1501029156870329, 0.08931427707449922, 0.0667365894645005, 0.072433571487315, 0.13172851209695696, 0.05984212163490005, 0.03576979209226155, 0.07003324122260475, 0.11320829693947332, 0.0693322913545595, 0.11446299090782704, 0.15135876822099606, 0.04949419747306126, 0.07820988930968444, 0.18143962830538496, 0.049603299465690955, 0.09657174650490201, 0.17816756454294286, 0.048461207815474645, 0.05515916895665136, 0.05769454610614217, 0.04487284058498545, 0.04878023088876493, 0.10474632238382384, 0.06425436245036416, 0.047553590623058486, 0.043472173570135685, 0.1004236962580398, 0.04064997226503417, 0.04272523137829367, 0.05546879429260054, 0.07401809177682338, 0.08608125655408895, 0.10731026936928731, 0.12484799756207718, 0.13797184294031792, 0.04922937481315196, 0.05057781837223628, 0.03696221065782611, 0.041850632768442884, 0.06156765052648576, 0.10649848452632055, 0.05258072933343285, 0.1563233399668142, 0.10265365611303691, 0.04126918368887129, 0.08311606692362095, 0.14894431518210954, 0.10293032377936001, 0.05712912495129681, 0.08337298638604432, 0.0480818436992993, 0.10165189602591218, 0.09379503547346824, 0.12789148711122192, 0.1746249725067851, 0.06549046398854456, 0.061081618854052555, 0.12911986887707075, 0.05078123612361014, 0.14465614165143476, 0.07641040136365844, 0.1273120646519163, 0.050506128481989154, 0.10640432744330566, 0.06426662554013096, 0.08528080757180423, 0.0798948567247576, 0.06555785402746873, 0.03566810388178461, 0.10130323442730703, 0.10721554723093368, 0.055771704218831986, 0.09775986488151266, 0.03151928642465722, 0.1455538132308152, 0.05338175735155122, 0.03613375525470673, 0.14723704650684244, 0.04477369355491969, 0.083405177240362, 0.030097223024285746, 0.1150775530431675, 0.07362265657923342, 0.13106253665621354, 0.12406427686710611, 0.11589146078501886, 0.06656691211507075, 0.07750783774868467, 0.052497042773089406, 0.09528864148622301, 0.1458906701072797, 0.13170821932448123, 0.04417482652260081, 0.06265983638023982, 0.039347428792911886, 0.07628351211604145, 0.12246978457513312, 0.13661879893664672, 0.08986311051986581, 0.02845013766783081, 0.11026369368342816, 0.08866949877961976, 0.05321605949145653, 0.06494798704089041, 0.1249161936024156, 0.1497906692520702, 0.061012614048758164, 0.06883862272933519, 0.07344234796358319, 0.06204208598325487, 0.04217227359698386, 0.05552158964369024, 0.0414851833919171, 0.05129124705868006, 0.08998975327368114, 0.0649944144860022, 0.06501829464853662, 0.0900261916805154, 0.10465747360317353, 0.12230466720390765, 0.06840631799512943, 0.08929709804461412, 0.16827928625295582, 0.08986878463963992, 0.09644948093497735, 0.07156437667084094, 0.12516161230314127, 0.09051753871295959, 0.12290276679251184, 0.08762850317442773, 0.1392804166408906, 0.1085447764620961, 0.08481780943651632, 0.04001651671754154, 0.12228192030959771, 0.17800115795602045, 0.14278182596420388, 0.04324415617990291, 0.042005844253295274, 0.06743584162853535, 0.10839105831454414, 0.04525235055516493, 0.04682389763481364, 0.17382537208775176, 0.10168170692172253, 0.07235849112382695, 0.0883696442981975, 0.08180358545227608, 0.052116590856415804, 0.08959162948914208, 0.1251228873968288, 0.1545422298189955, 0.0833301057729219, 0.08537456512429907, 0.07900601747835635, 0.05128286662721943, 0.03597144557848329, 0.11793534864684423, 0.1597787248482722, 0.10981678249473056, 0.07454541816951721, 0.0816906869013403, 0.1472415700205974, 0.13713980352543925, 0.06318287055101501, 0.14170982471748508, 0.06089579613116438, 0.13378434676796655, 0.09659544530974949, 0.10878928517884263, 0.11723856864984003, 0.1381004843481952, 0.055342549358098465, 0.037394739189793094, 0.04710851887643679, 0.056849203074440344, 0.08393050467071345, 0.11402124873257208, 0.09802454980696122, 0.046009195759490175, 0.10057109907219297, 0.08262183333907583, 0.1187808595296079, 0.0319203400783548, 0.0811921919408447, 0.07164513912996766, 0.04333077882652404, 0.07161038431375857, 0.08454367020691866, 0.044138000772257845, 0.05288037864765067, 0.07155595056742814, 0.09958047508403287, 0.07385475298020205, 0.10052278714597353, 0.05263989781740204, 0.029060674996926265, 0.050170454300222565, 0.04923249031674742, 0.10999662607189431, 0.05840213904057903, 0.07209925151683243, 0.11194262700870924, 0.08449753898861206, 0.03636947656942422, 0.07548133780826022, 0.04783734996445783, 0.06137341349172924, 0.06689040657716427, 0.12601953142862488, 0.05973220290299839, 0.04754671420238664, 0.16431313732908362, 0.029937648934357342, 0.06778885481428001, 0.05781267706665019, 0.12219514813867945, 0.08862211815002285, 0.17392817551899664, 0.13908017615599708, 0.08145258490455949, 0.054973050464024314, 0.02056416096088855, 0.15526249371850936, 0.04553077365711465, 0.12292549204584169, 0.09033213495795031, 0.09880532691155126, 0.04423920231184765, 0.10736923074872132, 0.02610751555460185, 0.13201617781251576, 0.047434431818823085, 0.07188928428819591, 0.05535873613205416, 0.04351391130251591, 0.09066480928661362, 0.11759456490085753, 0.06067935478120132, 0.11239614166713816, 0.08974883781186106, 0.052515568968246135, 0.0696042212635501, 0.06941627669583192, 0.08341669441468469, 0.04053140016398864, 0.03470962879302376, 0.06872446007381564, 0.12725617892050228, 0.0503438350866409, 0.0741023153796308, 0.07347998413654352, 0.06467670842488335, 0.06201749390665167, 0.05716621142688173, 0.04276278111793003, 0.08020739367656737, 0.09165331141112194, 0.1302737292017223, 0.09603724297985562, 0.06363668467121411, 0.0493258156188768, 0.04831739088685884, 0.11233576666939571, 0.09053843610955165, 0.06053050946166406, 0.05128458791429173, 0.0621172302483719, 0.061059283858259795, 0.05131814635438298, 0.07923889807431589, 0.17745219157960787, 0.13051224626806476, 0.045048611027743546, 0.07869620690890128, 0.058165796286697866, 0.17820187082227812, 0.15422637122897057, 0.054716948692098914, 0.05350996459116062, 0.0664201602038619, 0.1531187592036125, 0.12529615439112513, 0.07082325606520139, 0.07409669571104856, 0.0528438182527051, 0.11966562343474302, 0.07950676830177715, 0.08257579449019177, 0.09079520194764094, 0.03926908248950079, 0.10851147452653971, 0.07310714132809233, 0.05245271810540537, 0.05607865421267464, 0.05445921886662316, 0.04738458203871706, 0.12675513331036398, 0.07138742626299516, 0.08770158419623619, 0.05446664563678999, 0.05393527568311982, 0.074382537155609, 0.05566959283839034, 0.10369315040824285, 0.08959468797927, 0.14113138798212782, 0.09010525196975254, 0.049706397846154535, 0.041063904229340205, 0.10360149676488951, 0.08339813706575759, 0.10036777135870575, 0.042036187228727026, 0.07596472853424582, 0.1065111402379395, 0.06889598368298604, 0.05393907906306653, 0.10704590988771653, 0.0649840340489947, 0.12277728386144415]
0.09564632552400364
Making ranges
torch.Size([38851, 2])
We keep 6.69e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([6295, 2])
We keep 3.05e+05/6.66e+06 =  4% of the original kernel matrix.

torch.Size([15730, 2])
We keep 1.42e+06/5.90e+07 =  2% of the original kernel matrix.

torch.Size([55767, 2])
We keep 2.08e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([45607, 2])
We keep 1.07e+07/8.01e+08 =  1% of the original kernel matrix.

torch.Size([10506, 2])
We keep 1.13e+06/2.32e+07 =  4% of the original kernel matrix.

torch.Size([19965, 2])
We keep 2.22e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([10291, 2])
We keep 1.25e+06/2.07e+07 =  6% of the original kernel matrix.

torch.Size([19694, 2])
We keep 2.12e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([54532, 2])
We keep 3.05e+07/1.18e+09 =  2% of the original kernel matrix.

torch.Size([45202, 2])
We keep 1.05e+07/7.84e+08 =  1% of the original kernel matrix.

torch.Size([95724, 2])
We keep 4.96e+07/3.15e+09 =  1% of the original kernel matrix.

torch.Size([58698, 2])
We keep 1.59e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([100911, 2])
We keep 5.69e+07/3.80e+09 =  1% of the original kernel matrix.

torch.Size([60398, 2])
We keep 1.73e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([105493, 2])
We keep 3.98e+07/3.36e+09 =  1% of the original kernel matrix.

torch.Size([61649, 2])
We keep 1.61e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([16018, 2])
We keep 2.01e+06/6.35e+07 =  3% of the original kernel matrix.

torch.Size([24431, 2])
We keep 3.20e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([125665, 2])
We keep 7.92e+08/1.08e+10 =  7% of the original kernel matrix.

torch.Size([68027, 2])
We keep 2.72e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([22593, 2])
We keep 5.02e+06/1.50e+08 =  3% of the original kernel matrix.

torch.Size([29118, 2])
We keep 4.54e+06/2.80e+08 =  1% of the original kernel matrix.

torch.Size([157898, 2])
We keep 1.79e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([75654, 2])
We keep 2.62e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([19968, 2])
We keep 2.65e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([27416, 2])
We keep 4.18e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([555843, 2])
We keep 7.88e+08/8.97e+10 =  0% of the original kernel matrix.

torch.Size([147376, 2])
We keep 6.81e+07/6.85e+09 =  0% of the original kernel matrix.

torch.Size([41657, 2])
We keep 8.08e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([39537, 2])
We keep 7.63e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([117087, 2])
We keep 7.87e+07/5.74e+09 =  1% of the original kernel matrix.

torch.Size([64819, 2])
We keep 2.08e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([104364, 2])
We keep 6.43e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([61704, 2])
We keep 1.65e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([56596, 2])
We keep 3.12e+07/1.46e+09 =  2% of the original kernel matrix.

torch.Size([45888, 2])
We keep 1.15e+07/8.75e+08 =  1% of the original kernel matrix.

torch.Size([353686, 2])
We keep 5.05e+08/4.49e+10 =  1% of the original kernel matrix.

torch.Size([117898, 2])
We keep 4.92e+07/4.84e+09 =  1% of the original kernel matrix.

torch.Size([191043, 2])
We keep 2.16e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([85252, 2])
We keep 2.99e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([56847, 2])
We keep 5.98e+07/1.73e+09 =  3% of the original kernel matrix.

torch.Size([45094, 2])
We keep 1.26e+07/9.50e+08 =  1% of the original kernel matrix.

torch.Size([634652, 2])
We keep 1.30e+09/1.42e+11 =  0% of the original kernel matrix.

torch.Size([158994, 2])
We keep 8.21e+07/8.60e+09 =  0% of the original kernel matrix.

torch.Size([29744, 2])
We keep 5.01e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([33955, 2])
We keep 5.58e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([46761, 2])
We keep 3.37e+07/9.69e+08 =  3% of the original kernel matrix.

torch.Size([42295, 2])
We keep 9.79e+06/7.12e+08 =  1% of the original kernel matrix.

torch.Size([16954, 2])
We keep 1.21e+07/1.60e+08 =  7% of the original kernel matrix.

torch.Size([24209, 2])
We keep 4.63e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([132492, 2])
We keep 6.59e+07/6.39e+09 =  1% of the original kernel matrix.

torch.Size([69908, 2])
We keep 2.12e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([80483, 2])
We keep 7.36e+07/3.80e+09 =  1% of the original kernel matrix.

torch.Size([52931, 2])
We keep 1.75e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([26521, 2])
We keep 5.19e+06/2.49e+08 =  2% of the original kernel matrix.

torch.Size([32096, 2])
We keep 5.59e+06/3.61e+08 =  1% of the original kernel matrix.

torch.Size([111115, 2])
We keep 1.35e+08/5.17e+09 =  2% of the original kernel matrix.

torch.Size([63384, 2])
We keep 1.96e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([3376244, 2])
We keep 1.70e+10/3.21e+12 =  0% of the original kernel matrix.

torch.Size([384488, 2])
We keep 3.52e+08/4.09e+10 =  0% of the original kernel matrix.

torch.Size([21532, 2])
We keep 5.75e+06/1.42e+08 =  4% of the original kernel matrix.

torch.Size([28284, 2])
We keep 4.40e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([839488, 2])
We keep 3.58e+09/2.93e+11 =  1% of the original kernel matrix.

torch.Size([186939, 2])
We keep 1.19e+08/1.24e+10 =  0% of the original kernel matrix.

torch.Size([25959, 2])
We keep 6.84e+06/2.65e+08 =  2% of the original kernel matrix.

torch.Size([31490, 2])
We keep 5.80e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([20143, 2])
We keep 2.43e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([27432, 2])
We keep 3.92e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([29041, 2])
We keep 1.07e+07/4.06e+08 =  2% of the original kernel matrix.

torch.Size([31239, 2])
We keep 6.42e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([261450, 2])
We keep 6.10e+08/3.83e+10 =  1% of the original kernel matrix.

torch.Size([101459, 2])
We keep 4.81e+07/4.48e+09 =  1% of the original kernel matrix.

torch.Size([200622, 2])
We keep 2.78e+08/1.35e+10 =  2% of the original kernel matrix.

torch.Size([87849, 2])
We keep 2.96e+07/2.65e+09 =  1% of the original kernel matrix.

torch.Size([5306, 2])
We keep 2.53e+05/5.03e+06 =  5% of the original kernel matrix.

torch.Size([14714, 2])
We keep 1.31e+06/5.13e+07 =  2% of the original kernel matrix.

torch.Size([12847, 2])
We keep 1.29e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([21645, 2])
We keep 2.69e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([8767, 2])
We keep 7.01e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([18038, 2])
We keep 1.96e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([11366, 2])
We keep 7.74e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([20598, 2])
We keep 2.23e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([4465, 2])
We keep 3.94e+05/5.46e+06 =  7% of the original kernel matrix.

torch.Size([13005, 2])
We keep 1.32e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([4800, 2])
We keep 1.89e+05/3.83e+06 =  4% of the original kernel matrix.

torch.Size([14077, 2])
We keep 1.18e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([9490, 2])
We keep 6.78e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([18461, 2])
We keep 2.01e+06/9.77e+07 =  2% of the original kernel matrix.

torch.Size([4203, 2])
We keep 1.78e+05/3.06e+06 =  5% of the original kernel matrix.

torch.Size([13634, 2])
We keep 1.11e+06/4.00e+07 =  2% of the original kernel matrix.

torch.Size([9662, 2])
We keep 9.63e+05/1.96e+07 =  4% of the original kernel matrix.

torch.Size([18957, 2])
We keep 2.09e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([15683, 2])
We keep 2.51e+06/7.30e+07 =  3% of the original kernel matrix.

torch.Size([24137, 2])
We keep 3.55e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([11590, 2])
We keep 9.58e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([20528, 2])
We keep 2.44e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([6141, 2])
We keep 3.41e+05/7.01e+06 =  4% of the original kernel matrix.

torch.Size([15735, 2])
We keep 1.49e+06/6.06e+07 =  2% of the original kernel matrix.

torch.Size([28753, 2])
We keep 8.20e+06/3.18e+08 =  2% of the original kernel matrix.

torch.Size([33226, 2])
We keep 6.26e+06/4.08e+08 =  1% of the original kernel matrix.

torch.Size([25924, 2])
We keep 5.41e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([31402, 2])
We keep 5.49e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([6228, 2])
We keep 4.34e+05/8.40e+06 =  5% of the original kernel matrix.

torch.Size([15653, 2])
We keep 1.63e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([14469, 2])
We keep 1.88e+06/5.48e+07 =  3% of the original kernel matrix.

torch.Size([22971, 2])
We keep 3.10e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([7289, 2])
We keep 5.67e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([16499, 2])
We keep 1.77e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([14401, 2])
We keep 1.48e+06/4.92e+07 =  3% of the original kernel matrix.

torch.Size([22939, 2])
We keep 3.01e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([15063, 2])
We keep 2.22e+06/6.18e+07 =  3% of the original kernel matrix.

torch.Size([23416, 2])
We keep 3.26e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([7158, 2])
We keep 4.19e+05/9.84e+06 =  4% of the original kernel matrix.

torch.Size([16768, 2])
We keep 1.66e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([10952, 2])
We keep 7.88e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([20026, 2])
We keep 2.19e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([10358, 2])
We keep 7.41e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([19623, 2])
We keep 2.17e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([10205, 2])
We keep 1.26e+06/2.66e+07 =  4% of the original kernel matrix.

torch.Size([19390, 2])
We keep 2.43e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([15143, 2])
We keep 1.65e+06/5.59e+07 =  2% of the original kernel matrix.

torch.Size([23749, 2])
We keep 3.13e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([6744, 2])
We keep 3.36e+05/7.44e+06 =  4% of the original kernel matrix.

torch.Size([16464, 2])
We keep 1.48e+06/6.24e+07 =  2% of the original kernel matrix.

torch.Size([21080, 2])
We keep 5.60e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([28413, 2])
We keep 5.39e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([22828, 2])
We keep 2.86e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([29434, 2])
We keep 4.38e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([9422, 2])
We keep 6.56e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([18787, 2])
We keep 2.02e+06/9.60e+07 =  2% of the original kernel matrix.

torch.Size([7093, 2])
We keep 3.96e+05/8.99e+06 =  4% of the original kernel matrix.

torch.Size([16709, 2])
We keep 1.61e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([17332, 2])
We keep 2.78e+06/8.99e+07 =  3% of the original kernel matrix.

torch.Size([25311, 2])
We keep 3.83e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([15003, 2])
We keep 1.96e+06/6.53e+07 =  2% of the original kernel matrix.

torch.Size([23420, 2])
We keep 3.41e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([12911, 2])
We keep 9.98e+05/3.23e+07 =  3% of the original kernel matrix.

torch.Size([21931, 2])
We keep 2.50e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([9521, 2])
We keep 6.67e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([18746, 2])
We keep 2.02e+06/9.56e+07 =  2% of the original kernel matrix.

torch.Size([10620, 2])
We keep 1.17e+06/2.54e+07 =  4% of the original kernel matrix.

torch.Size([19716, 2])
We keep 2.25e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([7452, 2])
We keep 4.63e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([16762, 2])
We keep 1.69e+06/7.50e+07 =  2% of the original kernel matrix.

torch.Size([11352, 2])
We keep 8.46e+05/2.63e+07 =  3% of the original kernel matrix.

torch.Size([20489, 2])
We keep 2.35e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([5894, 2])
We keep 3.72e+05/7.14e+06 =  5% of the original kernel matrix.

torch.Size([15230, 2])
We keep 1.47e+06/6.11e+07 =  2% of the original kernel matrix.

torch.Size([11703, 2])
We keep 1.21e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([20691, 2])
We keep 2.63e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([17251, 2])
We keep 2.72e+06/8.91e+07 =  3% of the original kernel matrix.

torch.Size([25268, 2])
We keep 3.72e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([5555, 2])
We keep 2.24e+05/4.83e+06 =  4% of the original kernel matrix.

torch.Size([15160, 2])
We keep 1.25e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([3414, 2])
We keep 1.07e+05/1.79e+06 =  5% of the original kernel matrix.

torch.Size([12412, 2])
We keep 9.09e+05/3.06e+07 =  2% of the original kernel matrix.

torch.Size([11158, 2])
We keep 8.09e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([20209, 2])
We keep 2.28e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([20525, 2])
We keep 4.00e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([27743, 2])
We keep 4.92e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([12651, 2])
We keep 1.32e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([21438, 2])
We keep 2.80e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([5111, 2])
We keep 2.31e+05/4.46e+06 =  5% of the original kernel matrix.

torch.Size([14504, 2])
We keep 1.22e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([14173, 2])
We keep 1.58e+06/4.73e+07 =  3% of the original kernel matrix.

torch.Size([22861, 2])
We keep 2.94e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([6168, 2])
We keep 4.17e+05/8.50e+06 =  4% of the original kernel matrix.

torch.Size([15625, 2])
We keep 1.64e+06/6.67e+07 =  2% of the original kernel matrix.

torch.Size([6177, 2])
We keep 3.28e+05/7.07e+06 =  4% of the original kernel matrix.

torch.Size([15759, 2])
We keep 1.48e+06/6.08e+07 =  2% of the original kernel matrix.

torch.Size([5519, 2])
We keep 2.17e+05/4.53e+06 =  4% of the original kernel matrix.

torch.Size([15220, 2])
We keep 1.25e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([19055, 2])
We keep 4.29e+06/1.23e+08 =  3% of the original kernel matrix.

torch.Size([26644, 2])
We keep 4.28e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([5318, 2])
We keep 2.36e+05/4.51e+06 =  5% of the original kernel matrix.

torch.Size([14591, 2])
We keep 1.24e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([11258, 2])
We keep 1.16e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([20401, 2])
We keep 2.59e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([8800, 2])
We keep 5.52e+05/1.39e+07 =  3% of the original kernel matrix.

torch.Size([18266, 2])
We keep 1.86e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([14345, 2])
We keep 1.47e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([22925, 2])
We keep 2.93e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([6176, 2])
We keep 3.04e+05/6.68e+06 =  4% of the original kernel matrix.

torch.Size([15671, 2])
We keep 1.42e+06/5.91e+07 =  2% of the original kernel matrix.

torch.Size([5282, 2])
We keep 2.24e+05/4.37e+06 =  5% of the original kernel matrix.

torch.Size([14803, 2])
We keep 1.23e+06/4.78e+07 =  2% of the original kernel matrix.

torch.Size([10772, 2])
We keep 1.23e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([19900, 2])
We keep 2.57e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([12891, 2])
We keep 1.41e+06/4.03e+07 =  3% of the original kernel matrix.

torch.Size([21648, 2])
We keep 2.81e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([12252, 2])
We keep 1.11e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([21239, 2])
We keep 2.68e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([11641, 2])
We keep 9.67e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([20616, 2])
We keep 2.38e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([13147, 2])
We keep 1.98e+06/5.01e+07 =  3% of the original kernel matrix.

torch.Size([21812, 2])
We keep 3.00e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([10077, 2])
We keep 7.84e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([19346, 2])
We keep 2.14e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([9841, 2])
We keep 5.53e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([19287, 2])
We keep 1.94e+06/9.08e+07 =  2% of the original kernel matrix.

torch.Size([38212, 2])
We keep 8.98e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([38340, 2])
We keep 7.33e+06/5.11e+08 =  1% of the original kernel matrix.

torch.Size([15816, 2])
We keep 2.17e+06/6.60e+07 =  3% of the original kernel matrix.

torch.Size([24019, 2])
We keep 3.34e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([19996, 2])
We keep 3.72e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([27388, 2])
We keep 4.42e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([5747, 2])
We keep 3.02e+05/6.20e+06 =  4% of the original kernel matrix.

torch.Size([15036, 2])
We keep 1.39e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([12744, 2])
We keep 1.54e+06/4.66e+07 =  3% of the original kernel matrix.

torch.Size([21507, 2])
We keep 3.01e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([7250, 2])
We keep 4.82e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([16686, 2])
We keep 1.69e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([18991, 2])
We keep 2.83e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([26609, 2])
We keep 4.27e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([3028, 2])
We keep 8.44e+04/1.18e+06 =  7% of the original kernel matrix.

torch.Size([12127, 2])
We keep 7.84e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([13723, 2])
We keep 1.73e+06/4.55e+07 =  3% of the original kernel matrix.

torch.Size([22433, 2])
We keep 2.86e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([5857, 2])
We keep 3.25e+05/6.06e+06 =  5% of the original kernel matrix.

torch.Size([15321, 2])
We keep 1.38e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([5820, 2])
We keep 2.83e+05/5.70e+06 =  4% of the original kernel matrix.

torch.Size([15243, 2])
We keep 1.37e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([4608, 2])
We keep 1.69e+05/3.13e+06 =  5% of the original kernel matrix.

torch.Size([13903, 2])
We keep 1.09e+06/4.04e+07 =  2% of the original kernel matrix.

torch.Size([6016, 2])
We keep 4.51e+05/8.84e+06 =  5% of the original kernel matrix.

torch.Size([15430, 2])
We keep 1.65e+06/6.80e+07 =  2% of the original kernel matrix.

torch.Size([9410, 2])
We keep 9.38e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([18968, 2])
We keep 2.18e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([11296, 2])
We keep 1.13e+06/2.94e+07 =  3% of the original kernel matrix.

torch.Size([20343, 2])
We keep 2.45e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([5771, 2])
We keep 3.28e+05/6.48e+06 =  5% of the original kernel matrix.

torch.Size([15108, 2])
We keep 1.44e+06/5.82e+07 =  2% of the original kernel matrix.

torch.Size([6924, 2])
We keep 3.50e+05/8.63e+06 =  4% of the original kernel matrix.

torch.Size([16641, 2])
We keep 1.53e+06/6.72e+07 =  2% of the original kernel matrix.

torch.Size([4831, 2])
We keep 1.76e+05/3.50e+06 =  5% of the original kernel matrix.

torch.Size([14300, 2])
We keep 1.15e+06/4.28e+07 =  2% of the original kernel matrix.

torch.Size([18379, 2])
We keep 2.24e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([26036, 2])
We keep 3.56e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([5940, 2])
We keep 3.17e+05/6.23e+06 =  5% of the original kernel matrix.

torch.Size([15343, 2])
We keep 1.40e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([29625, 2])
We keep 5.53e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([34110, 2])
We keep 6.23e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([7924, 2])
We keep 5.92e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([17373, 2])
We keep 1.82e+06/8.20e+07 =  2% of the original kernel matrix.

torch.Size([7339, 2])
We keep 5.18e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([16804, 2])
We keep 1.73e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([8769, 2])
We keep 5.78e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([18398, 2])
We keep 1.89e+06/8.79e+07 =  2% of the original kernel matrix.

torch.Size([5492, 2])
We keep 2.24e+05/4.70e+06 =  4% of the original kernel matrix.

torch.Size([14925, 2])
We keep 1.26e+06/4.96e+07 =  2% of the original kernel matrix.

torch.Size([6846, 2])
We keep 4.20e+05/9.71e+06 =  4% of the original kernel matrix.

torch.Size([16463, 2])
We keep 1.70e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([5392, 2])
We keep 1.86e+05/4.05e+06 =  4% of the original kernel matrix.

torch.Size([14994, 2])
We keep 1.19e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([10404, 2])
We keep 7.59e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([19717, 2])
We keep 2.10e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([21156, 2])
We keep 3.81e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([28413, 2])
We keep 4.85e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([12704, 2])
We keep 9.85e+05/3.07e+07 =  3% of the original kernel matrix.

torch.Size([21565, 2])
We keep 2.49e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([18392, 2])
We keep 2.87e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([26172, 2])
We keep 4.03e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([5380, 2])
We keep 2.53e+05/5.04e+06 =  5% of the original kernel matrix.

torch.Size([14739, 2])
We keep 1.31e+06/5.14e+07 =  2% of the original kernel matrix.

torch.Size([12944, 2])
We keep 1.25e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([21846, 2])
We keep 2.65e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([5038, 2])
We keep 3.51e+05/6.00e+06 =  5% of the original kernel matrix.

torch.Size([14280, 2])
We keep 1.44e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([10695, 2])
We keep 8.32e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([19858, 2])
We keep 2.26e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([9406, 2])
We keep 6.78e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([18801, 2])
We keep 1.92e+06/9.16e+07 =  2% of the original kernel matrix.

torch.Size([5487, 2])
We keep 2.52e+05/4.73e+06 =  5% of the original kernel matrix.

torch.Size([15051, 2])
We keep 1.27e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([6907, 2])
We keep 4.27e+05/8.77e+06 =  4% of the original kernel matrix.

torch.Size([16288, 2])
We keep 1.61e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([6879, 2])
We keep 4.26e+05/9.00e+06 =  4% of the original kernel matrix.

torch.Size([16423, 2])
We keep 1.62e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([6161, 2])
We keep 2.96e+05/6.44e+06 =  4% of the original kernel matrix.

torch.Size([15658, 2])
We keep 1.41e+06/5.80e+07 =  2% of the original kernel matrix.

torch.Size([6893, 2])
We keep 3.64e+05/8.16e+06 =  4% of the original kernel matrix.

torch.Size([16403, 2])
We keep 1.54e+06/6.53e+07 =  2% of the original kernel matrix.

torch.Size([3828, 2])
We keep 1.14e+05/2.06e+06 =  5% of the original kernel matrix.

torch.Size([13089, 2])
We keep 9.49e+05/3.28e+07 =  2% of the original kernel matrix.

torch.Size([16122, 2])
We keep 2.16e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([24354, 2])
We keep 3.49e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([24315, 2])
We keep 4.80e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([30290, 2])
We keep 5.19e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([11466, 2])
We keep 1.14e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([20498, 2])
We keep 2.55e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([9716, 2])
We keep 8.54e+05/2.04e+07 =  4% of the original kernel matrix.

torch.Size([19053, 2])
We keep 2.19e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([5557, 2])
We keep 3.47e+05/6.57e+06 =  5% of the original kernel matrix.

torch.Size([15016, 2])
We keep 1.48e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([9533, 2])
We keep 9.34e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([19050, 2])
We keep 2.22e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([4961, 2])
We keep 2.40e+05/4.66e+06 =  5% of the original kernel matrix.

torch.Size([14504, 2])
We keep 1.29e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([6622, 2])
We keep 3.95e+05/8.24e+06 =  4% of the original kernel matrix.

torch.Size([16061, 2])
We keep 1.54e+06/6.57e+07 =  2% of the original kernel matrix.

torch.Size([8298, 2])
We keep 5.35e+05/1.37e+07 =  3% of the original kernel matrix.

torch.Size([17418, 2])
We keep 1.84e+06/8.47e+07 =  2% of the original kernel matrix.

torch.Size([13010, 2])
We keep 1.46e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([21964, 2])
We keep 2.81e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([10545, 2])
We keep 8.96e+05/2.27e+07 =  3% of the original kernel matrix.

torch.Size([19721, 2])
We keep 2.23e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([29538, 2])
We keep 1.20e+07/4.76e+08 =  2% of the original kernel matrix.

torch.Size([33424, 2])
We keep 7.42e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([7804, 2])
We keep 5.32e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([17277, 2])
We keep 1.86e+06/8.18e+07 =  2% of the original kernel matrix.

torch.Size([20838, 2])
We keep 4.98e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([28068, 2])
We keep 5.08e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([10209, 2])
We keep 6.29e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([19522, 2])
We keep 2.03e+06/9.71e+07 =  2% of the original kernel matrix.

torch.Size([7803, 2])
We keep 5.16e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([17182, 2])
We keep 1.73e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([15130, 2])
We keep 2.14e+06/6.26e+07 =  3% of the original kernel matrix.

torch.Size([23568, 2])
We keep 3.30e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([4053, 2])
We keep 1.59e+05/2.66e+06 =  5% of the original kernel matrix.

torch.Size([13297, 2])
We keep 1.05e+06/3.73e+07 =  2% of the original kernel matrix.

torch.Size([19082, 2])
We keep 2.87e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([27015, 2])
We keep 4.07e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([5806, 2])
We keep 3.08e+05/6.21e+06 =  4% of the original kernel matrix.

torch.Size([15109, 2])
We keep 1.40e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([11609, 2])
We keep 1.19e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([20633, 2])
We keep 2.64e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([4317, 2])
We keep 1.81e+05/2.99e+06 =  6% of the original kernel matrix.

torch.Size([13598, 2])
We keep 1.11e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([8419, 2])
We keep 5.48e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([17779, 2])
We keep 1.85e+06/8.32e+07 =  2% of the original kernel matrix.

torch.Size([6440, 2])
We keep 2.80e+05/6.48e+06 =  4% of the original kernel matrix.

torch.Size([16001, 2])
We keep 1.40e+06/5.82e+07 =  2% of the original kernel matrix.

torch.Size([8593, 2])
We keep 5.46e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([18106, 2])
We keep 1.86e+06/8.64e+07 =  2% of the original kernel matrix.

torch.Size([12288, 2])
We keep 9.70e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([21037, 2])
We keep 2.44e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([10553, 2])
We keep 7.64e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([19638, 2])
We keep 2.18e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([3117, 2])
We keep 7.68e+04/1.24e+06 =  6% of the original kernel matrix.

torch.Size([12066, 2])
We keep 7.93e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([3736, 2])
We keep 1.52e+05/2.56e+06 =  5% of the original kernel matrix.

torch.Size([12843, 2])
We keep 1.06e+06/3.66e+07 =  2% of the original kernel matrix.

torch.Size([6008, 2])
We keep 2.60e+05/5.72e+06 =  4% of the original kernel matrix.

torch.Size([15430, 2])
We keep 1.36e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([14322, 2])
We keep 1.42e+06/5.01e+07 =  2% of the original kernel matrix.

torch.Size([22929, 2])
We keep 3.02e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([6650, 2])
We keep 4.58e+05/8.74e+06 =  5% of the original kernel matrix.

torch.Size([16085, 2])
We keep 1.59e+06/6.76e+07 =  2% of the original kernel matrix.

torch.Size([11874, 2])
We keep 1.66e+06/4.00e+07 =  4% of the original kernel matrix.

torch.Size([21031, 2])
We keep 2.82e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([9235, 2])
We keep 8.52e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([18955, 2])
We keep 2.18e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([12638, 2])
We keep 1.15e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([21499, 2])
We keep 2.59e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([12847, 2])
We keep 1.04e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([21733, 2])
We keep 2.58e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([12543, 2])
We keep 1.53e+06/3.85e+07 =  3% of the original kernel matrix.

torch.Size([21380, 2])
We keep 2.73e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([9387, 2])
We keep 6.23e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([18669, 2])
We keep 1.98e+06/9.12e+07 =  2% of the original kernel matrix.

torch.Size([10267, 2])
We keep 7.20e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([19572, 2])
We keep 2.08e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([10452, 2])
We keep 8.86e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([19402, 2])
We keep 2.27e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([10690, 2])
We keep 1.12e+06/2.61e+07 =  4% of the original kernel matrix.

torch.Size([19894, 2])
We keep 2.39e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([5955, 2])
We keep 2.87e+05/5.87e+06 =  4% of the original kernel matrix.

torch.Size([15474, 2])
We keep 1.37e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([11822, 2])
We keep 1.20e+06/3.18e+07 =  3% of the original kernel matrix.

torch.Size([20835, 2])
We keep 2.55e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([21281, 2])
We keep 3.91e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([28320, 2])
We keep 4.77e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([8917, 2])
We keep 6.02e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([18357, 2])
We keep 1.89e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([9003, 2])
We keep 7.27e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([18389, 2])
We keep 2.03e+06/9.51e+07 =  2% of the original kernel matrix.

torch.Size([12563, 2])
We keep 1.09e+06/3.33e+07 =  3% of the original kernel matrix.

torch.Size([21415, 2])
We keep 2.55e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([26761, 2])
We keep 6.48e+06/2.51e+08 =  2% of the original kernel matrix.

torch.Size([31716, 2])
We keep 5.67e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([6771, 2])
We keep 4.78e+05/9.68e+06 =  4% of the original kernel matrix.

torch.Size([16361, 2])
We keep 1.71e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([2844, 2])
We keep 7.90e+04/1.24e+06 =  6% of the original kernel matrix.

torch.Size([11453, 2])
We keep 8.03e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([14306, 2])
We keep 1.64e+06/5.19e+07 =  3% of the original kernel matrix.

torch.Size([22850, 2])
We keep 3.05e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([5899, 2])
We keep 2.95e+05/6.56e+06 =  4% of the original kernel matrix.

torch.Size([15641, 2])
We keep 1.46e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([10487, 2])
We keep 9.69e+05/2.40e+07 =  4% of the original kernel matrix.

torch.Size([19629, 2])
We keep 2.31e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([11346, 2])
We keep 1.05e+06/2.80e+07 =  3% of the original kernel matrix.

torch.Size([20289, 2])
We keep 2.42e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([6982, 2])
We keep 3.52e+05/8.06e+06 =  4% of the original kernel matrix.

torch.Size([16613, 2])
We keep 1.54e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([5660, 2])
We keep 3.24e+05/6.00e+06 =  5% of the original kernel matrix.

torch.Size([15082, 2])
We keep 1.40e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([11206, 2])
We keep 8.03e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([20171, 2])
We keep 2.25e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([4773, 2])
We keep 1.93e+05/3.64e+06 =  5% of the original kernel matrix.

torch.Size([14195, 2])
We keep 1.16e+06/4.37e+07 =  2% of the original kernel matrix.

torch.Size([12837, 2])
We keep 1.12e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([21679, 2])
We keep 2.67e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([5815, 2])
We keep 2.50e+05/5.40e+06 =  4% of the original kernel matrix.

torch.Size([15361, 2])
We keep 1.34e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([19955, 2])
We keep 2.37e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([27182, 2])
We keep 3.92e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([13872, 2])
We keep 1.65e+06/5.04e+07 =  3% of the original kernel matrix.

torch.Size([22628, 2])
We keep 3.08e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([9828, 2])
We keep 5.98e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([19218, 2])
We keep 2.00e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([5515, 2])
We keep 2.21e+05/4.43e+06 =  4% of the original kernel matrix.

torch.Size([14845, 2])
We keep 1.24e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([11429, 2])
We keep 1.58e+06/3.80e+07 =  4% of the original kernel matrix.

torch.Size([20632, 2])
We keep 2.79e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([8923, 2])
We keep 8.08e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([18061, 2])
We keep 2.02e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([13764, 2])
We keep 1.99e+06/5.26e+07 =  3% of the original kernel matrix.

torch.Size([22555, 2])
We keep 3.08e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([18935, 2])
We keep 3.00e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([26671, 2])
We keep 4.25e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([12383, 2])
We keep 1.38e+06/4.05e+07 =  3% of the original kernel matrix.

torch.Size([21079, 2])
We keep 2.88e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([13915, 2])
We keep 1.79e+06/4.85e+07 =  3% of the original kernel matrix.

torch.Size([22530, 2])
We keep 2.95e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([9830, 2])
We keep 6.52e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([19056, 2])
We keep 2.04e+06/9.74e+07 =  2% of the original kernel matrix.

torch.Size([9555, 2])
We keep 5.55e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([18970, 2])
We keep 1.95e+06/9.12e+07 =  2% of the original kernel matrix.

torch.Size([3077, 2])
We keep 9.70e+04/1.41e+06 =  6% of the original kernel matrix.

torch.Size([11968, 2])
We keep 8.42e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([4540, 2])
We keep 2.07e+05/3.83e+06 =  5% of the original kernel matrix.

torch.Size([13686, 2])
We keep 1.18e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([4064, 2])
We keep 1.38e+05/2.55e+06 =  5% of the original kernel matrix.

torch.Size([13325, 2])
We keep 1.02e+06/3.65e+07 =  2% of the original kernel matrix.

torch.Size([3584, 2])
We keep 1.02e+05/1.68e+06 =  6% of the original kernel matrix.

torch.Size([12695, 2])
We keep 8.70e+05/2.96e+07 =  2% of the original kernel matrix.

torch.Size([8871, 2])
We keep 5.94e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([18283, 2])
We keep 1.91e+06/8.86e+07 =  2% of the original kernel matrix.

torch.Size([9500, 2])
We keep 6.03e+05/1.57e+07 =  3% of the original kernel matrix.

torch.Size([18968, 2])
We keep 1.95e+06/9.07e+07 =  2% of the original kernel matrix.

torch.Size([9633, 2])
We keep 1.04e+06/2.22e+07 =  4% of the original kernel matrix.

torch.Size([18869, 2])
We keep 2.24e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([6921, 2])
We keep 4.37e+05/9.33e+06 =  4% of the original kernel matrix.

torch.Size([16623, 2])
We keep 1.63e+06/6.98e+07 =  2% of the original kernel matrix.

torch.Size([11882, 2])
We keep 1.13e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([20692, 2])
We keep 2.53e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([8397, 2])
We keep 6.09e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([17608, 2])
We keep 1.85e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([9321, 2])
We keep 7.85e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([18732, 2])
We keep 2.11e+06/9.94e+07 =  2% of the original kernel matrix.

torch.Size([8755, 2])
We keep 7.23e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([18290, 2])
We keep 2.06e+06/9.33e+07 =  2% of the original kernel matrix.

torch.Size([5803, 2])
We keep 2.55e+05/5.48e+06 =  4% of the original kernel matrix.

torch.Size([15206, 2])
We keep 1.34e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([5921, 2])
We keep 3.05e+05/5.89e+06 =  5% of the original kernel matrix.

torch.Size([15335, 2])
We keep 1.37e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([10559, 2])
We keep 8.63e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([19819, 2])
We keep 2.31e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([3595, 2])
We keep 1.14e+05/1.89e+06 =  6% of the original kernel matrix.

torch.Size([12703, 2])
We keep 9.35e+05/3.15e+07 =  2% of the original kernel matrix.

torch.Size([4789, 2])
We keep 1.73e+05/3.37e+06 =  5% of the original kernel matrix.

torch.Size([14190, 2])
We keep 1.12e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([7125, 2])
We keep 4.29e+05/9.42e+06 =  4% of the original kernel matrix.

torch.Size([16564, 2])
We keep 1.63e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([11689, 2])
We keep 1.62e+06/3.94e+07 =  4% of the original kernel matrix.

torch.Size([20738, 2])
We keep 2.85e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([8035, 2])
We keep 6.83e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([17525, 2])
We keep 2.06e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([6792, 2])
We keep 4.57e+05/9.29e+06 =  4% of the original kernel matrix.

torch.Size([16251, 2])
We keep 1.64e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([6903, 2])
We keep 4.14e+05/8.48e+06 =  4% of the original kernel matrix.

torch.Size([16277, 2])
We keep 1.58e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([9606, 2])
We keep 7.00e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([18896, 2])
We keep 2.10e+06/9.93e+07 =  2% of the original kernel matrix.

torch.Size([15387, 2])
We keep 1.72e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([23868, 2])
We keep 3.24e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([7802, 2])
We keep 4.28e+05/9.97e+06 =  4% of the original kernel matrix.

torch.Size([17266, 2])
We keep 1.65e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([23841, 2])
We keep 4.09e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([29885, 2])
We keep 4.81e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([65966, 2])
We keep 1.12e+08/3.50e+09 =  3% of the original kernel matrix.

torch.Size([48165, 2])
We keep 1.70e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([29904, 2])
We keep 1.24e+07/3.44e+08 =  3% of the original kernel matrix.

torch.Size([32221, 2])
We keep 5.56e+06/4.24e+08 =  1% of the original kernel matrix.

torch.Size([14018, 2])
We keep 1.39e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([22667, 2])
We keep 2.93e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([3123, 2])
We keep 9.23e+04/1.50e+06 =  6% of the original kernel matrix.

torch.Size([11854, 2])
We keep 8.68e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([14535, 2])
We keep 1.62e+06/5.14e+07 =  3% of the original kernel matrix.

torch.Size([23012, 2])
We keep 3.07e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([181551, 2])
We keep 2.51e+09/2.95e+10 =  8% of the original kernel matrix.

torch.Size([82358, 2])
We keep 4.13e+07/3.93e+09 =  1% of the original kernel matrix.

torch.Size([411234, 2])
We keep 2.57e+09/9.67e+10 =  2% of the original kernel matrix.

torch.Size([123373, 2])
We keep 7.16e+07/7.11e+09 =  1% of the original kernel matrix.

torch.Size([9533, 2])
We keep 6.24e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([19051, 2])
We keep 1.98e+06/9.57e+07 =  2% of the original kernel matrix.

torch.Size([176800, 2])
We keep 2.08e+08/1.30e+10 =  1% of the original kernel matrix.

torch.Size([81234, 2])
We keep 2.92e+07/2.61e+09 =  1% of the original kernel matrix.

torch.Size([43552, 2])
We keep 1.87e+07/8.05e+08 =  2% of the original kernel matrix.

torch.Size([39935, 2])
We keep 8.99e+06/6.49e+08 =  1% of the original kernel matrix.

torch.Size([47826, 2])
We keep 3.89e+07/1.10e+09 =  3% of the original kernel matrix.

torch.Size([41450, 2])
We keep 1.03e+07/7.59e+08 =  1% of the original kernel matrix.

torch.Size([201391, 2])
We keep 1.00e+09/2.67e+10 =  3% of the original kernel matrix.

torch.Size([87506, 2])
We keep 4.03e+07/3.74e+09 =  1% of the original kernel matrix.

torch.Size([6091, 2])
We keep 2.90e+05/6.12e+06 =  4% of the original kernel matrix.

torch.Size([15537, 2])
We keep 1.39e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([15625, 2])
We keep 1.67e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([23882, 2])
We keep 3.10e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([522907, 2])
We keep 9.01e+08/8.78e+10 =  1% of the original kernel matrix.

torch.Size([141369, 2])
We keep 6.79e+07/6.78e+09 =  1% of the original kernel matrix.

torch.Size([862725, 2])
We keep 1.85e+09/2.28e+11 =  0% of the original kernel matrix.

torch.Size([190846, 2])
We keep 1.02e+08/1.09e+10 =  0% of the original kernel matrix.

torch.Size([13155, 2])
We keep 1.28e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([22035, 2])
We keep 2.63e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([59651, 2])
We keep 1.87e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([47025, 2])
We keep 1.04e+07/7.85e+08 =  1% of the original kernel matrix.

torch.Size([31757, 2])
We keep 9.33e+06/3.63e+08 =  2% of the original kernel matrix.

torch.Size([34979, 2])
We keep 6.60e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([119829, 2])
We keep 7.10e+08/6.18e+09 = 11% of the original kernel matrix.

torch.Size([66202, 2])
We keep 1.97e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([107143, 2])
We keep 2.22e+08/6.46e+09 =  3% of the original kernel matrix.

torch.Size([61906, 2])
We keep 2.17e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([57273, 2])
We keep 6.29e+07/2.32e+09 =  2% of the original kernel matrix.

torch.Size([44428, 2])
We keep 1.42e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([66220, 2])
We keep 4.71e+07/1.92e+09 =  2% of the original kernel matrix.

torch.Size([49433, 2])
We keep 1.29e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([26854, 2])
We keep 4.70e+06/2.03e+08 =  2% of the original kernel matrix.

torch.Size([31945, 2])
We keep 5.05e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([8679, 2])
We keep 5.99e+05/1.53e+07 =  3% of the original kernel matrix.

torch.Size([17950, 2])
We keep 1.94e+06/8.94e+07 =  2% of the original kernel matrix.

torch.Size([555726, 2])
We keep 4.97e+09/2.84e+11 =  1% of the original kernel matrix.

torch.Size([141244, 2])
We keep 1.18e+08/1.22e+10 =  0% of the original kernel matrix.

torch.Size([13691, 2])
We keep 1.44e+06/4.41e+07 =  3% of the original kernel matrix.

torch.Size([22615, 2])
We keep 2.86e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([9253, 2])
We keep 6.18e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([18767, 2])
We keep 1.96e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([77185, 2])
We keep 7.22e+07/2.73e+09 =  2% of the original kernel matrix.

torch.Size([52845, 2])
We keep 1.49e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([61062, 2])
We keep 3.69e+07/1.66e+09 =  2% of the original kernel matrix.

torch.Size([45776, 2])
We keep 1.21e+07/9.33e+08 =  1% of the original kernel matrix.

torch.Size([453152, 2])
We keep 1.20e+09/9.60e+10 =  1% of the original kernel matrix.

torch.Size([133312, 2])
We keep 7.19e+07/7.08e+09 =  1% of the original kernel matrix.

torch.Size([200418, 2])
We keep 1.58e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([87445, 2])
We keep 2.88e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([7919, 2])
We keep 6.02e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([17335, 2])
We keep 1.77e+06/8.04e+07 =  2% of the original kernel matrix.

torch.Size([60660, 2])
We keep 6.03e+07/1.30e+09 =  4% of the original kernel matrix.

torch.Size([47183, 2])
We keep 1.10e+07/8.23e+08 =  1% of the original kernel matrix.

torch.Size([20268, 2])
We keep 4.67e+06/1.52e+08 =  3% of the original kernel matrix.

torch.Size([27544, 2])
We keep 4.70e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([70501, 2])
We keep 7.81e+07/2.48e+09 =  3% of the original kernel matrix.

torch.Size([50396, 2])
We keep 1.43e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([125342, 2])
We keep 6.83e+07/5.27e+09 =  1% of the original kernel matrix.

torch.Size([68299, 2])
We keep 1.90e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([11834, 2])
We keep 9.24e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([20689, 2])
We keep 2.40e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([98088, 2])
We keep 5.39e+07/3.65e+09 =  1% of the original kernel matrix.

torch.Size([59447, 2])
We keep 1.67e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([2449, 2])
We keep 6.69e+04/1.02e+06 =  6% of the original kernel matrix.

torch.Size([10718, 2])
We keep 7.63e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([13886, 2])
We keep 1.53e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([22429, 2])
We keep 2.94e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([111429, 2])
We keep 1.36e+08/6.83e+09 =  1% of the original kernel matrix.

torch.Size([62737, 2])
We keep 2.22e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([97906, 2])
We keep 8.12e+07/3.96e+09 =  2% of the original kernel matrix.

torch.Size([59291, 2])
We keep 1.72e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([78301, 2])
We keep 5.61e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([51366, 2])
We keep 1.52e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([29078, 2])
We keep 5.04e+07/4.01e+08 = 12% of the original kernel matrix.

torch.Size([31472, 2])
We keep 6.36e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([433802, 2])
We keep 5.46e+08/7.65e+10 =  0% of the original kernel matrix.

torch.Size([130832, 2])
We keep 6.24e+07/6.33e+09 =  0% of the original kernel matrix.

torch.Size([26096, 2])
We keep 5.16e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([31791, 2])
We keep 5.91e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([44983, 2])
We keep 1.09e+07/7.50e+08 =  1% of the original kernel matrix.

torch.Size([41433, 2])
We keep 8.66e+06/6.26e+08 =  1% of the original kernel matrix.

torch.Size([100241, 2])
We keep 6.41e+07/4.51e+09 =  1% of the original kernel matrix.

torch.Size([60553, 2])
We keep 1.82e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([16599, 2])
We keep 2.41e+06/7.01e+07 =  3% of the original kernel matrix.

torch.Size([24746, 2])
We keep 3.42e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([222271, 2])
We keep 3.33e+08/1.84e+10 =  1% of the original kernel matrix.

torch.Size([92388, 2])
We keep 3.39e+07/3.10e+09 =  1% of the original kernel matrix.

torch.Size([356457, 2])
We keep 1.07e+09/6.95e+10 =  1% of the original kernel matrix.

torch.Size([117827, 2])
We keep 6.25e+07/6.03e+09 =  1% of the original kernel matrix.

torch.Size([485946, 2])
We keep 1.20e+09/9.80e+10 =  1% of the original kernel matrix.

torch.Size([137410, 2])
We keep 7.20e+07/7.16e+09 =  1% of the original kernel matrix.

torch.Size([55912, 2])
We keep 2.35e+07/1.14e+09 =  2% of the original kernel matrix.

torch.Size([45342, 2])
We keep 1.03e+07/7.71e+08 =  1% of the original kernel matrix.

torch.Size([52186, 2])
We keep 2.08e+07/9.81e+08 =  2% of the original kernel matrix.

torch.Size([43562, 2])
We keep 9.66e+06/7.16e+08 =  1% of the original kernel matrix.

torch.Size([38378, 2])
We keep 1.31e+07/5.79e+08 =  2% of the original kernel matrix.

torch.Size([36959, 2])
We keep 7.80e+06/5.50e+08 =  1% of the original kernel matrix.

torch.Size([14823, 2])
We keep 1.28e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([23237, 2])
We keep 2.88e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([39180, 2])
We keep 2.51e+07/7.37e+08 =  3% of the original kernel matrix.

torch.Size([37971, 2])
We keep 8.32e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([17092, 2])
We keep 2.52e+06/9.02e+07 =  2% of the original kernel matrix.

torch.Size([25341, 2])
We keep 3.81e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([34133, 2])
We keep 7.95e+06/3.95e+08 =  2% of the original kernel matrix.

torch.Size([35716, 2])
We keep 6.52e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([171407, 2])
We keep 9.05e+08/1.82e+10 =  4% of the original kernel matrix.

torch.Size([81364, 2])
We keep 3.10e+07/3.09e+09 =  1% of the original kernel matrix.

torch.Size([93818, 2])
We keep 1.10e+08/3.82e+09 =  2% of the original kernel matrix.

torch.Size([58644, 2])
We keep 1.71e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([18971, 2])
We keep 4.54e+06/1.45e+08 =  3% of the original kernel matrix.

torch.Size([26487, 2])
We keep 4.63e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([18842, 2])
We keep 2.28e+06/8.24e+07 =  2% of the original kernel matrix.

torch.Size([26570, 2])
We keep 3.49e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([53785, 2])
We keep 1.32e+08/4.77e+09 =  2% of the original kernel matrix.

torch.Size([42979, 2])
We keep 1.83e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([32064, 2])
We keep 1.46e+07/3.91e+08 =  3% of the original kernel matrix.

torch.Size([35036, 2])
We keep 6.66e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([34963, 2])
We keep 3.88e+07/6.57e+08 =  5% of the original kernel matrix.

torch.Size([35392, 2])
We keep 8.21e+06/5.86e+08 =  1% of the original kernel matrix.

torch.Size([22446, 2])
We keep 5.12e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([29211, 2])
We keep 5.22e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([466680, 2])
We keep 2.57e+09/1.24e+11 =  2% of the original kernel matrix.

torch.Size([133306, 2])
We keep 8.13e+07/8.07e+09 =  1% of the original kernel matrix.

torch.Size([26391, 2])
We keep 5.48e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([31609, 2])
We keep 5.16e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([49329, 2])
We keep 6.06e+07/1.39e+09 =  4% of the original kernel matrix.

torch.Size([41119, 2])
We keep 1.08e+07/8.52e+08 =  1% of the original kernel matrix.

torch.Size([13577, 2])
We keep 2.56e+06/5.47e+07 =  4% of the original kernel matrix.

torch.Size([22367, 2])
We keep 3.17e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([45627, 2])
We keep 5.72e+07/1.86e+09 =  3% of the original kernel matrix.

torch.Size([37448, 2])
We keep 1.28e+07/9.87e+08 =  1% of the original kernel matrix.

torch.Size([42887, 2])
We keep 3.94e+07/8.15e+08 =  4% of the original kernel matrix.

torch.Size([40263, 2])
We keep 8.74e+06/6.53e+08 =  1% of the original kernel matrix.

torch.Size([445811, 2])
We keep 2.61e+09/1.41e+11 =  1% of the original kernel matrix.

torch.Size([121594, 2])
We keep 8.56e+07/8.60e+09 =  0% of the original kernel matrix.

torch.Size([94005, 2])
We keep 4.37e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([58997, 2])
We keep 1.65e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([25017, 2])
We keep 4.23e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([30958, 2])
We keep 5.02e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([208624, 2])
We keep 1.49e+08/1.64e+10 =  0% of the original kernel matrix.

torch.Size([89541, 2])
We keep 3.19e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([219127, 2])
We keep 1.41e+08/1.59e+10 =  0% of the original kernel matrix.

torch.Size([92110, 2])
We keep 3.10e+07/2.89e+09 =  1% of the original kernel matrix.

torch.Size([17201, 2])
We keep 2.66e+06/7.90e+07 =  3% of the original kernel matrix.

torch.Size([25344, 2])
We keep 3.46e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([165712, 2])
We keep 1.86e+08/9.34e+09 =  1% of the original kernel matrix.

torch.Size([78922, 2])
We keep 2.49e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([190657, 2])
We keep 1.29e+08/1.40e+10 =  0% of the original kernel matrix.

torch.Size([85543, 2])
We keep 2.98e+07/2.71e+09 =  1% of the original kernel matrix.

torch.Size([491550, 2])
We keep 7.12e+08/9.16e+10 =  0% of the original kernel matrix.

torch.Size([138448, 2])
We keep 6.89e+07/6.92e+09 =  0% of the original kernel matrix.

torch.Size([99486, 2])
We keep 3.95e+08/4.75e+09 =  8% of the original kernel matrix.

torch.Size([60565, 2])
We keep 1.85e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([5982, 2])
We keep 2.99e+05/6.36e+06 =  4% of the original kernel matrix.

torch.Size([15353, 2])
We keep 1.39e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([43167, 2])
We keep 9.49e+06/6.33e+08 =  1% of the original kernel matrix.

torch.Size([40594, 2])
We keep 8.07e+06/5.76e+08 =  1% of the original kernel matrix.

torch.Size([111901, 2])
We keep 5.49e+08/1.28e+10 =  4% of the original kernel matrix.

torch.Size([63227, 2])
We keep 2.95e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([112375, 2])
We keep 5.35e+08/5.15e+09 = 10% of the original kernel matrix.

torch.Size([63830, 2])
We keep 1.90e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([43193, 2])
We keep 3.39e+07/7.22e+08 =  4% of the original kernel matrix.

torch.Size([40554, 2])
We keep 8.52e+06/6.15e+08 =  1% of the original kernel matrix.

torch.Size([13999, 2])
We keep 1.96e+06/5.17e+07 =  3% of the original kernel matrix.

torch.Size([22547, 2])
We keep 3.10e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([103674, 2])
We keep 4.70e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([61386, 2])
We keep 1.59e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([254587, 2])
We keep 2.81e+08/2.92e+10 =  0% of the original kernel matrix.

torch.Size([99560, 2])
We keep 4.16e+07/3.91e+09 =  1% of the original kernel matrix.

torch.Size([79469, 2])
We keep 2.63e+08/5.51e+09 =  4% of the original kernel matrix.

torch.Size([52434, 2])
We keep 2.04e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([12866, 2])
We keep 1.81e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([21643, 2])
We keep 2.97e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([41876, 2])
We keep 1.74e+07/8.25e+08 =  2% of the original kernel matrix.

torch.Size([39529, 2])
We keep 9.23e+06/6.57e+08 =  1% of the original kernel matrix.

torch.Size([23052, 2])
We keep 3.47e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([29602, 2])
We keep 4.72e+06/2.90e+08 =  1% of the original kernel matrix.

torch.Size([16625, 2])
We keep 1.87e+06/7.01e+07 =  2% of the original kernel matrix.

torch.Size([24809, 2])
We keep 3.40e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([29874, 2])
We keep 5.23e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([34128, 2])
We keep 6.05e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([13750, 2])
We keep 1.53e+06/4.69e+07 =  3% of the original kernel matrix.

torch.Size([22643, 2])
We keep 2.92e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([46859, 2])
We keep 2.34e+07/9.29e+08 =  2% of the original kernel matrix.

torch.Size([40981, 2])
We keep 9.35e+06/6.97e+08 =  1% of the original kernel matrix.

torch.Size([86936, 2])
We keep 2.88e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([56224, 2])
We keep 1.36e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([34045, 2])
We keep 1.24e+07/4.77e+08 =  2% of the original kernel matrix.

torch.Size([36600, 2])
We keep 7.18e+06/5.00e+08 =  1% of the original kernel matrix.

torch.Size([30881, 2])
We keep 2.45e+07/4.57e+08 =  5% of the original kernel matrix.

torch.Size([33246, 2])
We keep 6.92e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([15950, 2])
We keep 2.36e+06/6.75e+07 =  3% of the original kernel matrix.

torch.Size([24168, 2])
We keep 3.36e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([671504, 2])
We keep 1.95e+09/1.94e+11 =  1% of the original kernel matrix.

torch.Size([164850, 2])
We keep 9.53e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([28374, 2])
We keep 7.04e+06/3.32e+08 =  2% of the original kernel matrix.

torch.Size([33141, 2])
We keep 6.44e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([171530, 2])
We keep 1.40e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([80246, 2])
We keep 2.96e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([6160, 2])
We keep 2.84e+05/6.52e+06 =  4% of the original kernel matrix.

torch.Size([15840, 2])
We keep 1.38e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([17765, 2])
We keep 3.57e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([25597, 2])
We keep 4.14e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([11167, 2])
We keep 8.74e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([20304, 2])
We keep 2.34e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([59766, 2])
We keep 4.40e+07/1.19e+09 =  3% of the original kernel matrix.

torch.Size([47471, 2])
We keep 1.00e+07/7.88e+08 =  1% of the original kernel matrix.

torch.Size([10533, 2])
We keep 9.24e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([20131, 2])
We keep 2.35e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([183011, 2])
We keep 7.32e+08/2.47e+10 =  2% of the original kernel matrix.

torch.Size([78348, 2])
We keep 3.90e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([16352, 2])
We keep 2.24e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([24591, 2])
We keep 3.66e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([17096, 2])
We keep 2.31e+06/7.65e+07 =  3% of the original kernel matrix.

torch.Size([25080, 2])
We keep 3.52e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([295210, 2])
We keep 4.75e+08/3.49e+10 =  1% of the original kernel matrix.

torch.Size([107793, 2])
We keep 4.40e+07/4.27e+09 =  1% of the original kernel matrix.

torch.Size([65317, 2])
We keep 2.86e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([49042, 2])
We keep 1.14e+07/8.79e+08 =  1% of the original kernel matrix.

torch.Size([53276, 2])
We keep 4.04e+07/1.36e+09 =  2% of the original kernel matrix.

torch.Size([44186, 2])
We keep 1.12e+07/8.42e+08 =  1% of the original kernel matrix.

torch.Size([253679, 2])
We keep 6.39e+08/3.46e+10 =  1% of the original kernel matrix.

torch.Size([100011, 2])
We keep 4.57e+07/4.25e+09 =  1% of the original kernel matrix.

torch.Size([300501, 2])
We keep 8.04e+08/6.34e+10 =  1% of the original kernel matrix.

torch.Size([102815, 2])
We keep 5.93e+07/5.76e+09 =  1% of the original kernel matrix.

torch.Size([13823, 2])
We keep 1.91e+06/5.74e+07 =  3% of the original kernel matrix.

torch.Size([22632, 2])
We keep 3.29e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([13925, 2])
We keep 1.17e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([22469, 2])
We keep 2.77e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([35039, 2])
We keep 4.43e+07/4.96e+08 =  8% of the original kernel matrix.

torch.Size([36143, 2])
We keep 7.49e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([4493, 2])
We keep 2.24e+05/3.73e+06 =  5% of the original kernel matrix.

torch.Size([13721, 2])
We keep 1.18e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([17642, 2])
We keep 2.72e+06/9.21e+07 =  2% of the original kernel matrix.

torch.Size([25621, 2])
We keep 3.80e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([34591, 2])
We keep 9.95e+06/4.42e+08 =  2% of the original kernel matrix.

torch.Size([36152, 2])
We keep 7.10e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([8959, 2])
We keep 6.73e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([18423, 2])
We keep 1.94e+06/9.03e+07 =  2% of the original kernel matrix.

torch.Size([21155, 2])
We keep 3.96e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([28168, 2])
We keep 4.72e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([19805, 2])
We keep 2.39e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([26906, 2])
We keep 3.94e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([14163, 2])
We keep 1.30e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([22788, 2])
We keep 2.89e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([572320, 2])
We keep 1.20e+09/1.12e+11 =  1% of the original kernel matrix.

torch.Size([149721, 2])
We keep 7.55e+07/7.65e+09 =  0% of the original kernel matrix.

torch.Size([20588, 2])
We keep 5.46e+06/1.76e+08 =  3% of the original kernel matrix.

torch.Size([27467, 2])
We keep 4.95e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([139580, 2])
We keep 1.72e+08/8.49e+09 =  2% of the original kernel matrix.

torch.Size([71495, 2])
We keep 2.43e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([20954, 2])
We keep 7.76e+06/1.78e+08 =  4% of the original kernel matrix.

torch.Size([27848, 2])
We keep 4.90e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([30198, 2])
We keep 3.47e+07/6.56e+08 =  5% of the original kernel matrix.

torch.Size([32727, 2])
We keep 8.48e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([90720, 2])
We keep 1.06e+08/5.32e+09 =  1% of the original kernel matrix.

torch.Size([55490, 2])
We keep 2.01e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([1164723, 2])
We keep 7.70e+09/4.95e+11 =  1% of the original kernel matrix.

torch.Size([216844, 2])
We keep 1.43e+08/1.61e+10 =  0% of the original kernel matrix.

torch.Size([1269743, 2])
We keep 2.72e+09/4.54e+11 =  0% of the original kernel matrix.

torch.Size([230386, 2])
We keep 1.40e+08/1.54e+10 =  0% of the original kernel matrix.

torch.Size([44886, 2])
We keep 1.73e+07/7.18e+08 =  2% of the original kernel matrix.

torch.Size([41255, 2])
We keep 8.35e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([172323, 2])
We keep 5.01e+08/1.23e+10 =  4% of the original kernel matrix.

torch.Size([80800, 2])
We keep 2.90e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([11273, 2])
We keep 1.09e+06/3.01e+07 =  3% of the original kernel matrix.

torch.Size([20354, 2])
We keep 2.56e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([31408, 2])
We keep 3.27e+07/5.20e+08 =  6% of the original kernel matrix.

torch.Size([33469, 2])
We keep 7.68e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([72871, 2])
We keep 1.23e+08/3.09e+09 =  3% of the original kernel matrix.

torch.Size([50823, 2])
We keep 1.56e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([75752, 2])
We keep 4.28e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([52994, 2])
We keep 1.39e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([14769, 2])
We keep 1.37e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([23211, 2])
We keep 2.94e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([123287, 2])
We keep 7.73e+07/5.56e+09 =  1% of the original kernel matrix.

torch.Size([67114, 2])
We keep 2.01e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([592572, 2])
We keep 2.23e+09/1.63e+11 =  1% of the original kernel matrix.

torch.Size([152568, 2])
We keep 9.11e+07/9.22e+09 =  0% of the original kernel matrix.

torch.Size([75315, 2])
We keep 4.39e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([51207, 2])
We keep 1.46e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([19680, 2])
We keep 3.57e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([26879, 2])
We keep 4.38e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([72473, 2])
We keep 1.27e+08/3.48e+09 =  3% of the original kernel matrix.

torch.Size([51389, 2])
We keep 1.73e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([20926, 2])
We keep 3.33e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([27911, 2])
We keep 4.55e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([10442, 2])
We keep 7.59e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([19891, 2])
We keep 2.12e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([230986, 2])
We keep 3.05e+08/2.33e+10 =  1% of the original kernel matrix.

torch.Size([94743, 2])
We keep 3.78e+07/3.49e+09 =  1% of the original kernel matrix.

torch.Size([55323, 2])
We keep 2.45e+07/1.22e+09 =  2% of the original kernel matrix.

torch.Size([44833, 2])
We keep 1.05e+07/7.99e+08 =  1% of the original kernel matrix.

torch.Size([6272, 2])
We keep 3.36e+05/7.04e+06 =  4% of the original kernel matrix.

torch.Size([15600, 2])
We keep 1.47e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([224979, 2])
We keep 2.11e+08/1.82e+10 =  1% of the original kernel matrix.

torch.Size([93153, 2])
We keep 3.32e+07/3.09e+09 =  1% of the original kernel matrix.

torch.Size([31551, 2])
We keep 1.04e+07/4.76e+08 =  2% of the original kernel matrix.

torch.Size([35278, 2])
We keep 7.35e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([6831, 2])
We keep 5.61e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([16351, 2])
We keep 1.70e+06/7.30e+07 =  2% of the original kernel matrix.

torch.Size([206011, 2])
We keep 3.29e+08/2.04e+10 =  1% of the original kernel matrix.

torch.Size([87803, 2])
We keep 3.55e+07/3.26e+09 =  1% of the original kernel matrix.

torch.Size([162799, 2])
We keep 1.07e+08/9.44e+09 =  1% of the original kernel matrix.

torch.Size([78331, 2])
We keep 2.49e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([138860, 2])
We keep 2.19e+08/1.03e+10 =  2% of the original kernel matrix.

torch.Size([71659, 2])
We keep 2.67e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([282275, 2])
We keep 6.61e+08/3.75e+10 =  1% of the original kernel matrix.

torch.Size([104931, 2])
We keep 4.68e+07/4.43e+09 =  1% of the original kernel matrix.

torch.Size([212865, 2])
We keep 3.43e+08/2.01e+10 =  1% of the original kernel matrix.

torch.Size([89282, 2])
We keep 3.54e+07/3.25e+09 =  1% of the original kernel matrix.

torch.Size([24508, 2])
We keep 8.35e+06/2.61e+08 =  3% of the original kernel matrix.

torch.Size([30235, 2])
We keep 5.84e+06/3.69e+08 =  1% of the original kernel matrix.

torch.Size([94844, 2])
We keep 7.89e+07/3.86e+09 =  2% of the original kernel matrix.

torch.Size([58044, 2])
We keep 1.72e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([260932, 2])
We keep 2.10e+08/2.53e+10 =  0% of the original kernel matrix.

torch.Size([101350, 2])
We keep 3.84e+07/3.64e+09 =  1% of the original kernel matrix.

torch.Size([343039, 2])
We keep 3.72e+08/4.42e+10 =  0% of the original kernel matrix.

torch.Size([117600, 2])
We keep 4.93e+07/4.81e+09 =  1% of the original kernel matrix.

torch.Size([28218, 2])
We keep 6.44e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([33139, 2])
We keep 6.46e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([284315, 2])
We keep 2.74e+09/5.99e+10 =  4% of the original kernel matrix.

torch.Size([105459, 2])
We keep 5.21e+07/5.60e+09 =  0% of the original kernel matrix.

torch.Size([312910, 2])
We keep 2.25e+09/5.83e+10 =  3% of the original kernel matrix.

torch.Size([110570, 2])
We keep 5.39e+07/5.52e+09 =  0% of the original kernel matrix.

torch.Size([157826, 2])
We keep 1.57e+08/9.12e+09 =  1% of the original kernel matrix.

torch.Size([77230, 2])
We keep 2.44e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([58389, 2])
We keep 6.71e+07/1.85e+09 =  3% of the original kernel matrix.

torch.Size([46349, 2])
We keep 1.20e+07/9.83e+08 =  1% of the original kernel matrix.

torch.Size([34881, 2])
We keep 5.75e+07/8.51e+08 =  6% of the original kernel matrix.

torch.Size([35290, 2])
We keep 9.48e+06/6.67e+08 =  1% of the original kernel matrix.

torch.Size([23219, 2])
We keep 4.80e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([29345, 2])
We keep 4.78e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([17022, 2])
We keep 1.63e+06/6.70e+07 =  2% of the original kernel matrix.

torch.Size([25103, 2])
We keep 3.34e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([12333, 2])
We keep 1.40e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([21157, 2])
We keep 2.69e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([235614, 2])
We keep 1.83e+08/2.00e+10 =  0% of the original kernel matrix.

torch.Size([95687, 2])
We keep 3.47e+07/3.24e+09 =  1% of the original kernel matrix.

torch.Size([217075, 2])
We keep 1.92e+08/1.86e+10 =  1% of the original kernel matrix.

torch.Size([91798, 2])
We keep 3.34e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([530114, 2])
We keep 1.00e+09/1.01e+11 =  0% of the original kernel matrix.

torch.Size([143430, 2])
We keep 7.15e+07/7.27e+09 =  0% of the original kernel matrix.

torch.Size([342361, 2])
We keep 1.06e+09/5.51e+10 =  1% of the original kernel matrix.

torch.Size([115261, 2])
We keep 5.62e+07/5.37e+09 =  1% of the original kernel matrix.

torch.Size([118351, 2])
We keep 9.79e+07/6.37e+09 =  1% of the original kernel matrix.

torch.Size([65448, 2])
We keep 2.16e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([22601, 2])
We keep 3.55e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([28799, 2])
We keep 4.86e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([195647, 2])
We keep 1.96e+08/1.71e+10 =  1% of the original kernel matrix.

torch.Size([86391, 2])
We keep 3.28e+07/2.99e+09 =  1% of the original kernel matrix.

torch.Size([8695, 2])
We keep 8.62e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([18001, 2])
We keep 2.01e+06/9.53e+07 =  2% of the original kernel matrix.

torch.Size([25415, 2])
We keep 5.74e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([30852, 2])
We keep 5.32e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([401900, 2])
We keep 7.17e+08/5.50e+10 =  1% of the original kernel matrix.

torch.Size([125625, 2])
We keep 5.53e+07/5.36e+09 =  1% of the original kernel matrix.

torch.Size([49267, 2])
We keep 1.82e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([43220, 2])
We keep 1.01e+07/7.33e+08 =  1% of the original kernel matrix.

torch.Size([10771, 2])
We keep 8.70e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([19802, 2])
We keep 2.30e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([25997, 2])
We keep 4.40e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([31267, 2])
We keep 5.15e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([119957, 2])
We keep 2.27e+08/9.68e+09 =  2% of the original kernel matrix.

torch.Size([65133, 2])
We keep 2.59e+07/2.25e+09 =  1% of the original kernel matrix.

torch.Size([46732, 2])
We keep 1.39e+07/7.53e+08 =  1% of the original kernel matrix.

torch.Size([41437, 2])
We keep 8.72e+06/6.27e+08 =  1% of the original kernel matrix.

torch.Size([244350, 2])
We keep 3.92e+08/2.67e+10 =  1% of the original kernel matrix.

torch.Size([97729, 2])
We keep 4.03e+07/3.74e+09 =  1% of the original kernel matrix.

torch.Size([25163, 2])
We keep 1.14e+07/2.31e+08 =  4% of the original kernel matrix.

torch.Size([30241, 2])
We keep 5.32e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([32451, 2])
We keep 1.10e+07/3.77e+08 =  2% of the original kernel matrix.

torch.Size([35369, 2])
We keep 6.61e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([15524, 2])
We keep 1.82e+06/5.79e+07 =  3% of the original kernel matrix.

torch.Size([23915, 2])
We keep 3.19e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([7019, 2])
We keep 4.39e+05/9.78e+06 =  4% of the original kernel matrix.

torch.Size([16561, 2])
We keep 1.64e+06/7.15e+07 =  2% of the original kernel matrix.

torch.Size([102730, 2])
We keep 5.59e+07/3.24e+09 =  1% of the original kernel matrix.

torch.Size([60717, 2])
We keep 1.59e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([112479, 2])
We keep 1.11e+08/5.41e+09 =  2% of the original kernel matrix.

torch.Size([63436, 2])
We keep 2.02e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([15218, 2])
We keep 2.00e+06/5.88e+07 =  3% of the original kernel matrix.

torch.Size([23643, 2])
We keep 3.15e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([193144, 2])
We keep 2.28e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([85501, 2])
We keep 3.08e+07/2.89e+09 =  1% of the original kernel matrix.

torch.Size([11215, 2])
We keep 1.17e+06/2.79e+07 =  4% of the original kernel matrix.

torch.Size([20342, 2])
We keep 2.41e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([64092, 2])
We keep 2.54e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([49156, 2])
We keep 1.17e+07/8.92e+08 =  1% of the original kernel matrix.

torch.Size([14839, 2])
We keep 2.20e+06/6.03e+07 =  3% of the original kernel matrix.

torch.Size([23513, 2])
We keep 3.19e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([218608, 2])
We keep 1.63e+08/1.67e+10 =  0% of the original kernel matrix.

torch.Size([91099, 2])
We keep 3.21e+07/2.96e+09 =  1% of the original kernel matrix.

torch.Size([20799, 2])
We keep 1.82e+07/2.51e+08 =  7% of the original kernel matrix.

torch.Size([27654, 2])
We keep 5.93e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([99216, 2])
We keep 1.11e+08/4.10e+09 =  2% of the original kernel matrix.

torch.Size([59169, 2])
We keep 1.79e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([44522, 2])
We keep 1.35e+07/6.67e+08 =  2% of the original kernel matrix.

torch.Size([41036, 2])
We keep 8.25e+06/5.91e+08 =  1% of the original kernel matrix.

torch.Size([53796, 2])
We keep 2.17e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([44886, 2])
We keep 1.05e+07/7.82e+08 =  1% of the original kernel matrix.

torch.Size([99972, 2])
We keep 4.53e+07/3.23e+09 =  1% of the original kernel matrix.

torch.Size([60443, 2])
We keep 1.58e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([529605, 2])
We keep 2.07e+09/1.53e+11 =  1% of the original kernel matrix.

torch.Size([143306, 2])
We keep 8.53e+07/8.95e+09 =  0% of the original kernel matrix.

torch.Size([28170, 2])
We keep 4.95e+06/2.46e+08 =  2% of the original kernel matrix.

torch.Size([33001, 2])
We keep 5.57e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([22086, 2])
We keep 6.89e+06/1.65e+08 =  4% of the original kernel matrix.

torch.Size([28426, 2])
We keep 4.76e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([157433, 2])
We keep 1.65e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([76663, 2])
We keep 2.63e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([30231, 2])
We keep 7.78e+06/3.16e+08 =  2% of the original kernel matrix.

torch.Size([34177, 2])
We keep 6.31e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([953243, 2])
We keep 2.29e+09/3.38e+11 =  0% of the original kernel matrix.

torch.Size([200187, 2])
We keep 1.25e+08/1.33e+10 =  0% of the original kernel matrix.

torch.Size([11076, 2])
We keep 1.06e+06/2.65e+07 =  3% of the original kernel matrix.

torch.Size([20064, 2])
We keep 2.37e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([173889, 2])
We keep 1.52e+08/1.34e+10 =  1% of the original kernel matrix.

torch.Size([80340, 2])
We keep 2.96e+07/2.65e+09 =  1% of the original kernel matrix.

torch.Size([646473, 2])
We keep 1.02e+09/1.47e+11 =  0% of the original kernel matrix.

torch.Size([159494, 2])
We keep 8.54e+07/8.76e+09 =  0% of the original kernel matrix.

torch.Size([11303, 2])
We keep 9.70e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([20349, 2])
We keep 2.33e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([207362, 2])
We keep 7.04e+08/4.31e+10 =  1% of the original kernel matrix.

torch.Size([85388, 2])
We keep 4.84e+07/4.75e+09 =  1% of the original kernel matrix.

torch.Size([32647, 2])
We keep 5.99e+07/9.42e+08 =  6% of the original kernel matrix.

torch.Size([33249, 2])
We keep 9.77e+06/7.02e+08 =  1% of the original kernel matrix.

torch.Size([1047738, 2])
We keep 5.61e+09/5.02e+11 =  1% of the original kernel matrix.

torch.Size([207034, 2])
We keep 1.53e+08/1.62e+10 =  0% of the original kernel matrix.

torch.Size([20315, 2])
We keep 2.76e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([27580, 2])
We keep 4.16e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([69377, 2])
We keep 3.08e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([50226, 2])
We keep 1.21e+07/9.45e+08 =  1% of the original kernel matrix.

torch.Size([10622, 2])
We keep 4.37e+06/5.05e+07 =  8% of the original kernel matrix.

torch.Size([19499, 2])
We keep 3.03e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([16813, 2])
We keep 2.19e+06/7.22e+07 =  3% of the original kernel matrix.

torch.Size([24858, 2])
We keep 3.48e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([18287, 2])
We keep 3.75e+06/1.20e+08 =  3% of the original kernel matrix.

torch.Size([25973, 2])
We keep 4.28e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([93358, 2])
We keep 7.21e+07/3.51e+09 =  2% of the original kernel matrix.

torch.Size([58805, 2])
We keep 1.62e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([56132, 2])
We keep 3.83e+07/1.18e+09 =  3% of the original kernel matrix.

torch.Size([45198, 2])
We keep 1.05e+07/7.86e+08 =  1% of the original kernel matrix.

torch.Size([186037, 2])
We keep 2.22e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([84222, 2])
We keep 3.03e+07/2.72e+09 =  1% of the original kernel matrix.

torch.Size([32888, 2])
We keep 6.84e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([33877, 2])
We keep 6.05e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([11870, 2])
We keep 1.01e+06/3.09e+07 =  3% of the original kernel matrix.

torch.Size([20841, 2])
We keep 2.55e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([14470, 2])
We keep 1.78e+06/4.89e+07 =  3% of the original kernel matrix.

torch.Size([23019, 2])
We keep 2.93e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([259443, 2])
We keep 7.14e+08/4.88e+10 =  1% of the original kernel matrix.

torch.Size([99745, 2])
We keep 5.35e+07/5.05e+09 =  1% of the original kernel matrix.

torch.Size([93865, 2])
We keep 1.87e+08/6.06e+09 =  3% of the original kernel matrix.

torch.Size([57778, 2])
We keep 2.15e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([491240, 2])
We keep 1.08e+09/9.40e+10 =  1% of the original kernel matrix.

torch.Size([138768, 2])
We keep 7.08e+07/7.01e+09 =  1% of the original kernel matrix.

torch.Size([63268, 2])
We keep 4.25e+07/1.71e+09 =  2% of the original kernel matrix.

torch.Size([48431, 2])
We keep 1.24e+07/9.47e+08 =  1% of the original kernel matrix.

torch.Size([15379, 2])
We keep 2.89e+06/7.59e+07 =  3% of the original kernel matrix.

torch.Size([23849, 2])
We keep 3.53e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([13818, 2])
We keep 1.32e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([22451, 2])
We keep 2.75e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([31088, 2])
We keep 3.37e+07/4.81e+08 =  7% of the original kernel matrix.

torch.Size([33275, 2])
We keep 7.44e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([1390784, 2])
We keep 5.79e+09/6.04e+11 =  0% of the original kernel matrix.

torch.Size([239780, 2])
We keep 1.59e+08/1.78e+10 =  0% of the original kernel matrix.

torch.Size([23231, 2])
We keep 3.92e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([29722, 2])
We keep 4.68e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([40612, 2])
We keep 1.01e+07/5.26e+08 =  1% of the original kernel matrix.

torch.Size([39270, 2])
We keep 7.46e+06/5.24e+08 =  1% of the original kernel matrix.

torch.Size([167263, 2])
We keep 1.91e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([78545, 2])
We keep 2.75e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([88583, 2])
We keep 1.03e+08/3.88e+09 =  2% of the original kernel matrix.

torch.Size([55628, 2])
We keep 1.69e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([16446, 2])
We keep 3.10e+06/6.64e+07 =  4% of the original kernel matrix.

torch.Size([24628, 2])
We keep 3.32e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([9904, 2])
We keep 1.15e+06/2.17e+07 =  5% of the original kernel matrix.

torch.Size([19013, 2])
We keep 2.19e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([101396, 2])
We keep 1.55e+08/6.43e+09 =  2% of the original kernel matrix.

torch.Size([60387, 2])
We keep 2.15e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([83649, 2])
We keep 5.36e+07/2.58e+09 =  2% of the original kernel matrix.

torch.Size([55365, 2])
We keep 1.44e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([47136, 2])
We keep 1.93e+08/1.60e+09 = 12% of the original kernel matrix.

torch.Size([40635, 2])
We keep 1.21e+07/9.14e+08 =  1% of the original kernel matrix.

torch.Size([118076, 2])
We keep 7.47e+07/4.53e+09 =  1% of the original kernel matrix.

torch.Size([65640, 2])
We keep 1.83e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([307856, 2])
We keep 1.30e+09/6.08e+10 =  2% of the original kernel matrix.

torch.Size([108348, 2])
We keep 5.87e+07/5.64e+09 =  1% of the original kernel matrix.

torch.Size([151036, 2])
We keep 2.07e+08/1.19e+10 =  1% of the original kernel matrix.

torch.Size([75150, 2])
We keep 2.86e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([366360, 2])
We keep 8.83e+08/7.18e+10 =  1% of the original kernel matrix.

torch.Size([118394, 2])
We keep 6.25e+07/6.13e+09 =  1% of the original kernel matrix.

torch.Size([206247, 2])
We keep 2.84e+08/1.79e+10 =  1% of the original kernel matrix.

torch.Size([89049, 2])
We keep 3.39e+07/3.06e+09 =  1% of the original kernel matrix.

torch.Size([32120, 2])
We keep 3.24e+07/4.77e+08 =  6% of the original kernel matrix.

torch.Size([34460, 2])
We keep 7.26e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([100341, 2])
We keep 7.46e+07/3.50e+09 =  2% of the original kernel matrix.

torch.Size([60883, 2])
We keep 1.61e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([82151, 2])
We keep 2.42e+08/3.71e+09 =  6% of the original kernel matrix.

torch.Size([54222, 2])
We keep 1.72e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([38146, 2])
We keep 2.54e+07/6.09e+08 =  4% of the original kernel matrix.

torch.Size([38160, 2])
We keep 8.20e+06/5.64e+08 =  1% of the original kernel matrix.

torch.Size([25207, 2])
We keep 4.16e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([30636, 2])
We keep 5.11e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([17434, 2])
We keep 2.00e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([25416, 2])
We keep 3.49e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([77226, 2])
We keep 7.22e+07/2.51e+09 =  2% of the original kernel matrix.

torch.Size([53042, 2])
We keep 1.39e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([35605, 2])
We keep 1.36e+07/5.53e+08 =  2% of the original kernel matrix.

torch.Size([36392, 2])
We keep 7.75e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([7417, 2])
We keep 5.39e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([16696, 2])
We keep 1.69e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([37047, 2])
We keep 9.13e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([37461, 2])
We keep 7.26e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([21813, 2])
We keep 1.89e+07/3.20e+08 =  5% of the original kernel matrix.

torch.Size([27273, 2])
We keep 5.52e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([68077, 2])
We keep 1.03e+08/2.35e+09 =  4% of the original kernel matrix.

torch.Size([49635, 2])
We keep 1.40e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([15860, 2])
We keep 2.47e+06/7.40e+07 =  3% of the original kernel matrix.

torch.Size([24070, 2])
We keep 3.50e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([38055, 2])
We keep 8.94e+06/4.66e+08 =  1% of the original kernel matrix.

torch.Size([38007, 2])
We keep 7.09e+06/4.94e+08 =  1% of the original kernel matrix.

torch.Size([15616, 2])
We keep 3.90e+06/8.04e+07 =  4% of the original kernel matrix.

torch.Size([23674, 2])
We keep 3.49e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([39828, 2])
We keep 1.44e+07/6.11e+08 =  2% of the original kernel matrix.

torch.Size([38673, 2])
We keep 8.08e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([12379, 2])
We keep 1.17e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([21321, 2])
We keep 2.60e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([23010, 2])
We keep 3.29e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([29371, 2])
We keep 4.59e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([46330, 2])
We keep 1.07e+07/6.90e+08 =  1% of the original kernel matrix.

torch.Size([42248, 2])
We keep 8.21e+06/6.00e+08 =  1% of the original kernel matrix.

torch.Size([419874, 2])
We keep 8.96e+08/6.28e+10 =  1% of the original kernel matrix.

torch.Size([127996, 2])
We keep 5.83e+07/5.73e+09 =  1% of the original kernel matrix.

torch.Size([17469, 2])
We keep 3.03e+06/9.85e+07 =  3% of the original kernel matrix.

torch.Size([25424, 2])
We keep 4.01e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([6965, 2])
We keep 3.50e+05/7.98e+06 =  4% of the original kernel matrix.

torch.Size([16501, 2])
We keep 1.51e+06/6.46e+07 =  2% of the original kernel matrix.

torch.Size([11489, 2])
We keep 1.27e+06/3.03e+07 =  4% of the original kernel matrix.

torch.Size([20424, 2])
We keep 2.51e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([320881, 2])
We keep 8.18e+08/3.97e+10 =  2% of the original kernel matrix.

torch.Size([112086, 2])
We keep 4.78e+07/4.55e+09 =  1% of the original kernel matrix.

torch.Size([377712, 2])
We keep 7.09e+08/6.47e+10 =  1% of the original kernel matrix.

torch.Size([122423, 2])
We keep 5.85e+07/5.82e+09 =  1% of the original kernel matrix.

torch.Size([74378, 2])
We keep 5.42e+07/2.72e+09 =  1% of the original kernel matrix.

torch.Size([49988, 2])
We keep 1.48e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([22898, 2])
We keep 5.75e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([29555, 2])
We keep 5.30e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([249981, 2])
We keep 4.53e+08/2.99e+10 =  1% of the original kernel matrix.

torch.Size([98363, 2])
We keep 4.22e+07/3.95e+09 =  1% of the original kernel matrix.

torch.Size([268333, 2])
We keep 2.76e+08/3.45e+10 =  0% of the original kernel matrix.

torch.Size([103864, 2])
We keep 4.49e+07/4.25e+09 =  1% of the original kernel matrix.

torch.Size([7210, 2])
We keep 3.93e+05/9.01e+06 =  4% of the original kernel matrix.

torch.Size([16631, 2])
We keep 1.59e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([26946, 2])
We keep 1.22e+07/2.80e+08 =  4% of the original kernel matrix.

torch.Size([31670, 2])
We keep 5.90e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([77296, 2])
We keep 2.33e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([53232, 2])
We keep 1.22e+07/9.70e+08 =  1% of the original kernel matrix.

torch.Size([31120, 2])
We keep 3.92e+07/7.37e+08 =  5% of the original kernel matrix.

torch.Size([33471, 2])
We keep 8.77e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([46529, 2])
We keep 5.57e+07/1.27e+09 =  4% of the original kernel matrix.

torch.Size([41322, 2])
We keep 1.13e+07/8.14e+08 =  1% of the original kernel matrix.

torch.Size([196483, 2])
We keep 1.63e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([86718, 2])
We keep 2.95e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([38563, 2])
We keep 9.21e+06/5.79e+08 =  1% of the original kernel matrix.

torch.Size([38886, 2])
We keep 7.84e+06/5.50e+08 =  1% of the original kernel matrix.

torch.Size([16134, 2])
We keep 3.75e+06/6.60e+07 =  5% of the original kernel matrix.

torch.Size([24401, 2])
We keep 3.13e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([10083, 2])
We keep 7.12e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([19357, 2])
We keep 2.12e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([46173, 2])
We keep 2.53e+07/8.65e+08 =  2% of the original kernel matrix.

torch.Size([41345, 2])
We keep 9.20e+06/6.72e+08 =  1% of the original kernel matrix.

torch.Size([44456, 2])
We keep 1.37e+07/6.57e+08 =  2% of the original kernel matrix.

torch.Size([41033, 2])
We keep 8.17e+06/5.86e+08 =  1% of the original kernel matrix.

torch.Size([51641, 2])
We keep 1.83e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([43373, 2])
We keep 9.97e+06/7.41e+08 =  1% of the original kernel matrix.

torch.Size([184704, 2])
We keep 5.47e+08/1.75e+10 =  3% of the original kernel matrix.

torch.Size([82634, 2])
We keep 3.37e+07/3.03e+09 =  1% of the original kernel matrix.

torch.Size([650934, 2])
We keep 1.02e+09/1.34e+11 =  0% of the original kernel matrix.

torch.Size([159692, 2])
We keep 8.13e+07/8.38e+09 =  0% of the original kernel matrix.

torch.Size([18527, 2])
We keep 3.19e+06/9.44e+07 =  3% of the original kernel matrix.

torch.Size([26133, 2])
We keep 3.85e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([8986, 2])
We keep 5.81e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([18252, 2])
We keep 1.90e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([23087, 2])
We keep 3.49e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([29675, 2])
We keep 4.80e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([69357, 2])
We keep 2.24e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([50860, 2])
We keep 1.19e+07/9.38e+08 =  1% of the original kernel matrix.

torch.Size([51659, 2])
We keep 1.75e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([44057, 2])
We keep 1.01e+07/7.52e+08 =  1% of the original kernel matrix.

torch.Size([10986, 2])
We keep 1.16e+06/2.50e+07 =  4% of the original kernel matrix.

torch.Size([20105, 2])
We keep 2.29e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([13632, 2])
We keep 1.80e+06/4.74e+07 =  3% of the original kernel matrix.

torch.Size([22371, 2])
We keep 2.99e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([106645, 2])
We keep 3.85e+08/4.90e+09 =  7% of the original kernel matrix.

torch.Size([62388, 2])
We keep 1.95e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([12690, 2])
We keep 1.14e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([21652, 2])
We keep 2.52e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([98551, 2])
We keep 1.45e+08/5.36e+09 =  2% of the original kernel matrix.

torch.Size([58870, 2])
We keep 2.02e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([14467, 2])
We keep 1.59e+06/5.25e+07 =  3% of the original kernel matrix.

torch.Size([23055, 2])
We keep 3.09e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([31866, 2])
We keep 5.51e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([35242, 2])
We keep 6.22e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([21245, 2])
We keep 6.92e+06/1.63e+08 =  4% of the original kernel matrix.

torch.Size([27942, 2])
We keep 4.72e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([20177, 2])
We keep 2.93e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([27384, 2])
We keep 4.32e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([13203, 2])
We keep 1.18e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([21919, 2])
We keep 2.66e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([166950, 2])
We keep 1.10e+08/1.15e+10 =  0% of the original kernel matrix.

torch.Size([79361, 2])
We keep 2.76e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([563239, 2])
We keep 1.33e+09/1.22e+11 =  1% of the original kernel matrix.

torch.Size([148518, 2])
We keep 7.92e+07/7.98e+09 =  0% of the original kernel matrix.

torch.Size([250036, 2])
We keep 3.32e+08/2.38e+10 =  1% of the original kernel matrix.

torch.Size([98850, 2])
We keep 3.78e+07/3.53e+09 =  1% of the original kernel matrix.

torch.Size([130356, 2])
We keep 2.24e+08/7.77e+09 =  2% of the original kernel matrix.

torch.Size([68153, 2])
We keep 2.26e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([47649, 2])
We keep 1.51e+07/7.76e+08 =  1% of the original kernel matrix.

torch.Size([42569, 2])
We keep 8.77e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([19413, 2])
We keep 4.36e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([26521, 2])
We keep 4.23e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([28160, 2])
We keep 1.18e+07/4.40e+08 =  2% of the original kernel matrix.

torch.Size([33191, 2])
We keep 7.39e+06/4.79e+08 =  1% of the original kernel matrix.

torch.Size([282078, 2])
We keep 2.73e+08/3.19e+10 =  0% of the original kernel matrix.

torch.Size([106880, 2])
We keep 4.31e+07/4.09e+09 =  1% of the original kernel matrix.

torch.Size([26917, 2])
We keep 1.23e+07/3.52e+08 =  3% of the original kernel matrix.

torch.Size([32143, 2])
We keep 6.53e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([47872, 2])
We keep 5.25e+07/9.31e+08 =  5% of the original kernel matrix.

torch.Size([42106, 2])
We keep 9.66e+06/6.98e+08 =  1% of the original kernel matrix.

torch.Size([18120, 2])
We keep 2.65e+06/9.05e+07 =  2% of the original kernel matrix.

torch.Size([25864, 2])
We keep 3.73e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([683135, 2])
We keep 4.90e+09/3.42e+11 =  1% of the original kernel matrix.

torch.Size([164788, 2])
We keep 1.29e+08/1.34e+10 =  0% of the original kernel matrix.

torch.Size([50320, 2])
We keep 3.15e+07/9.45e+08 =  3% of the original kernel matrix.

torch.Size([43397, 2])
We keep 9.18e+06/7.03e+08 =  1% of the original kernel matrix.

torch.Size([73749, 2])
We keep 4.89e+07/2.32e+09 =  2% of the original kernel matrix.

torch.Size([52001, 2])
We keep 1.41e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([341007, 2])
We keep 1.03e+09/5.77e+10 =  1% of the original kernel matrix.

torch.Size([116449, 2])
We keep 5.69e+07/5.49e+09 =  1% of the original kernel matrix.

torch.Size([79845, 2])
We keep 2.65e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([53921, 2])
We keep 1.33e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([44573, 2])
We keep 1.51e+07/8.01e+08 =  1% of the original kernel matrix.

torch.Size([40933, 2])
We keep 8.74e+06/6.47e+08 =  1% of the original kernel matrix.

torch.Size([305169, 2])
We keep 6.65e+08/4.71e+10 =  1% of the original kernel matrix.

torch.Size([109277, 2])
We keep 5.19e+07/4.96e+09 =  1% of the original kernel matrix.

torch.Size([187438, 2])
We keep 1.31e+08/1.46e+10 =  0% of the original kernel matrix.

torch.Size([84542, 2])
We keep 3.04e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([74808, 2])
We keep 3.14e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([52083, 2])
We keep 1.25e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([27902, 2])
We keep 7.57e+06/2.59e+08 =  2% of the original kernel matrix.

torch.Size([32235, 2])
We keep 5.67e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([68762, 2])
We keep 2.69e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([50298, 2])
We keep 1.15e+07/9.10e+08 =  1% of the original kernel matrix.

torch.Size([26751, 2])
We keep 7.84e+06/2.45e+08 =  3% of the original kernel matrix.

torch.Size([31747, 2])
We keep 5.51e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([194440, 2])
We keep 1.38e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([85741, 2])
We keep 2.77e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([1290577, 2])
We keep 4.29e+09/5.89e+11 =  0% of the original kernel matrix.

torch.Size([229157, 2])
We keep 1.62e+08/1.75e+10 =  0% of the original kernel matrix.

torch.Size([193651, 2])
We keep 6.68e+08/1.86e+10 =  3% of the original kernel matrix.

torch.Size([85453, 2])
We keep 3.43e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([163590, 2])
We keep 3.18e+08/1.92e+10 =  1% of the original kernel matrix.

torch.Size([74925, 2])
We keep 3.49e+07/3.17e+09 =  1% of the original kernel matrix.

torch.Size([22358, 2])
We keep 4.70e+06/1.46e+08 =  3% of the original kernel matrix.

torch.Size([29144, 2])
We keep 4.38e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([98953, 2])
We keep 1.63e+08/6.54e+09 =  2% of the original kernel matrix.

torch.Size([58996, 2])
We keep 2.18e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([66161, 2])
We keep 1.13e+08/2.78e+09 =  4% of the original kernel matrix.

torch.Size([48977, 2])
We keep 1.57e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([20017, 2])
We keep 4.02e+06/1.29e+08 =  3% of the original kernel matrix.

torch.Size([27130, 2])
We keep 4.32e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([44842, 2])
We keep 2.05e+07/8.55e+08 =  2% of the original kernel matrix.

torch.Size([41103, 2])
We keep 9.42e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([627956, 2])
We keep 1.03e+09/1.56e+11 =  0% of the original kernel matrix.

torch.Size([157839, 2])
We keep 8.80e+07/9.04e+09 =  0% of the original kernel matrix.

torch.Size([43030, 2])
We keep 5.20e+07/1.37e+09 =  3% of the original kernel matrix.

torch.Size([37968, 2])
We keep 1.11e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([160498, 2])
We keep 7.35e+08/2.29e+10 =  3% of the original kernel matrix.

torch.Size([76283, 2])
We keep 3.60e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([109702, 2])
We keep 1.13e+08/5.13e+09 =  2% of the original kernel matrix.

torch.Size([63171, 2])
We keep 1.94e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([92903, 2])
We keep 5.54e+07/3.05e+09 =  1% of the original kernel matrix.

torch.Size([58128, 2])
We keep 1.56e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([15445, 2])
We keep 2.32e+06/6.31e+07 =  3% of the original kernel matrix.

torch.Size([23859, 2])
We keep 3.26e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([134024, 2])
We keep 2.19e+08/7.97e+09 =  2% of the original kernel matrix.

torch.Size([70797, 2])
We keep 2.39e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([260170, 2])
We keep 2.33e+08/2.77e+10 =  0% of the original kernel matrix.

torch.Size([101169, 2])
We keep 4.03e+07/3.81e+09 =  1% of the original kernel matrix.

torch.Size([8508, 2])
We keep 5.83e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([18110, 2])
We keep 1.79e+06/8.23e+07 =  2% of the original kernel matrix.

torch.Size([1170092, 2])
We keep 2.92e+09/4.61e+11 =  0% of the original kernel matrix.

torch.Size([220721, 2])
We keep 1.43e+08/1.55e+10 =  0% of the original kernel matrix.

torch.Size([83710, 2])
We keep 6.01e+07/2.75e+09 =  2% of the original kernel matrix.

torch.Size([55724, 2])
We keep 1.41e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([129522, 2])
We keep 1.44e+08/6.91e+09 =  2% of the original kernel matrix.

torch.Size([68464, 2])
We keep 2.23e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([16394, 2])
We keep 4.72e+06/8.00e+07 =  5% of the original kernel matrix.

torch.Size([24764, 2])
We keep 3.45e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([39659, 2])
We keep 9.96e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([38738, 2])
We keep 7.50e+06/5.25e+08 =  1% of the original kernel matrix.

torch.Size([7000, 2])
We keep 4.14e+05/9.15e+06 =  4% of the original kernel matrix.

torch.Size([16541, 2])
We keep 1.61e+06/6.92e+07 =  2% of the original kernel matrix.

torch.Size([11513, 2])
We keep 1.96e+06/3.37e+07 =  5% of the original kernel matrix.

torch.Size([20446, 2])
We keep 2.60e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([49451, 2])
We keep 3.52e+07/1.07e+09 =  3% of the original kernel matrix.

torch.Size([42956, 2])
We keep 1.03e+07/7.49e+08 =  1% of the original kernel matrix.

torch.Size([138521, 2])
We keep 2.18e+08/9.34e+09 =  2% of the original kernel matrix.

torch.Size([70853, 2])
We keep 2.42e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([2790736, 2])
We keep 4.22e+10/4.55e+12 =  0% of the original kernel matrix.

torch.Size([303937, 2])
We keep 4.26e+08/4.88e+10 =  0% of the original kernel matrix.

torch.Size([9506, 2])
We keep 1.03e+06/2.27e+07 =  4% of the original kernel matrix.

torch.Size([18958, 2])
We keep 2.27e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([290318, 2])
We keep 2.61e+08/2.89e+10 =  0% of the original kernel matrix.

torch.Size([107933, 2])
We keep 4.10e+07/3.89e+09 =  1% of the original kernel matrix.

torch.Size([17001, 2])
We keep 4.55e+06/9.55e+07 =  4% of the original kernel matrix.

torch.Size([25112, 2])
We keep 3.91e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([37715, 2])
We keep 8.75e+06/4.72e+08 =  1% of the original kernel matrix.

torch.Size([37897, 2])
We keep 7.06e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([25181, 2])
We keep 1.08e+07/2.91e+08 =  3% of the original kernel matrix.

torch.Size([30335, 2])
We keep 6.07e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([271953, 2])
We keep 1.10e+09/4.33e+10 =  2% of the original kernel matrix.

torch.Size([102602, 2])
We keep 4.99e+07/4.76e+09 =  1% of the original kernel matrix.

torch.Size([24210, 2])
We keep 4.97e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([30260, 2])
We keep 5.11e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([1824479, 2])
We keep 5.98e+09/1.07e+12 =  0% of the original kernel matrix.

torch.Size([279124, 2])
We keep 2.12e+08/2.37e+10 =  0% of the original kernel matrix.

torch.Size([14444, 2])
We keep 1.56e+06/4.79e+07 =  3% of the original kernel matrix.

torch.Size([23008, 2])
We keep 2.93e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([257911, 2])
We keep 3.42e+08/2.39e+10 =  1% of the original kernel matrix.

torch.Size([100493, 2])
We keep 3.77e+07/3.53e+09 =  1% of the original kernel matrix.

torch.Size([66552, 2])
We keep 1.41e+08/1.99e+09 =  7% of the original kernel matrix.

torch.Size([49392, 2])
We keep 1.31e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([157763, 2])
We keep 1.46e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([76935, 2])
We keep 2.63e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([307466, 2])
We keep 5.76e+08/4.87e+10 =  1% of the original kernel matrix.

torch.Size([108995, 2])
We keep 5.13e+07/5.05e+09 =  1% of the original kernel matrix.

torch.Size([37759, 2])
We keep 8.40e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([37848, 2])
We keep 7.03e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([19373, 2])
We keep 3.24e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([26844, 2])
We keep 4.15e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([131036, 2])
We keep 8.51e+07/7.93e+09 =  1% of the original kernel matrix.

torch.Size([69414, 2])
We keep 2.39e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([21493, 2])
We keep 4.14e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([28489, 2])
We keep 4.72e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([32798, 2])
We keep 1.45e+07/5.20e+08 =  2% of the original kernel matrix.

torch.Size([34032, 2])
We keep 7.65e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([181241, 2])
We keep 1.57e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([82060, 2])
We keep 2.92e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([64118, 2])
We keep 1.10e+08/2.27e+09 =  4% of the original kernel matrix.

torch.Size([48489, 2])
We keep 1.25e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([78834, 2])
We keep 7.95e+07/3.45e+09 =  2% of the original kernel matrix.

torch.Size([53541, 2])
We keep 1.70e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([43586, 2])
We keep 2.74e+07/9.76e+08 =  2% of the original kernel matrix.

torch.Size([39752, 2])
We keep 9.77e+06/7.14e+08 =  1% of the original kernel matrix.

torch.Size([392398, 2])
We keep 1.00e+09/6.84e+10 =  1% of the original kernel matrix.

torch.Size([122867, 2])
We keep 6.17e+07/5.98e+09 =  1% of the original kernel matrix.

torch.Size([587475, 2])
We keep 2.01e+09/1.49e+11 =  1% of the original kernel matrix.

torch.Size([150379, 2])
We keep 8.67e+07/8.81e+09 =  0% of the original kernel matrix.

torch.Size([89296, 2])
We keep 5.11e+07/2.61e+09 =  1% of the original kernel matrix.

torch.Size([56579, 2])
We keep 1.46e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([16144, 2])
We keep 1.83e+06/6.01e+07 =  3% of the original kernel matrix.

torch.Size([24388, 2])
We keep 3.20e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([175376, 2])
We keep 4.13e+08/1.62e+10 =  2% of the original kernel matrix.

torch.Size([80792, 2])
We keep 3.13e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([61430, 2])
We keep 2.89e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([46587, 2])
We keep 1.14e+07/8.92e+08 =  1% of the original kernel matrix.

torch.Size([69023, 2])
We keep 5.16e+07/2.14e+09 =  2% of the original kernel matrix.

torch.Size([50136, 2])
We keep 1.35e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([96646, 2])
We keep 1.28e+08/3.65e+09 =  3% of the original kernel matrix.

torch.Size([58828, 2])
We keep 1.70e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([88454, 2])
We keep 1.39e+08/5.64e+09 =  2% of the original kernel matrix.

torch.Size([55230, 2])
We keep 2.09e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([132678, 2])
We keep 1.87e+08/8.22e+09 =  2% of the original kernel matrix.

torch.Size([69343, 2])
We keep 2.42e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([186000, 2])
We keep 1.12e+09/4.10e+10 =  2% of the original kernel matrix.

torch.Size([75669, 2])
We keep 4.64e+07/4.63e+09 =  1% of the original kernel matrix.

torch.Size([51332, 2])
We keep 2.68e+07/9.90e+08 =  2% of the original kernel matrix.

torch.Size([43527, 2])
We keep 9.56e+06/7.20e+08 =  1% of the original kernel matrix.

torch.Size([35641, 2])
We keep 7.94e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([37047, 2])
We keep 6.95e+06/4.73e+08 =  1% of the original kernel matrix.

torch.Size([14863, 2])
We keep 2.36e+06/6.62e+07 =  3% of the original kernel matrix.

torch.Size([23336, 2])
We keep 3.41e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([30213, 2])
We keep 1.40e+07/3.64e+08 =  3% of the original kernel matrix.

torch.Size([32470, 2])
We keep 6.23e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([85975, 2])
We keep 1.13e+08/3.87e+09 =  2% of the original kernel matrix.

torch.Size([54498, 2])
We keep 1.71e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([212136, 2])
We keep 2.78e+08/1.77e+10 =  1% of the original kernel matrix.

torch.Size([89470, 2])
We keep 3.33e+07/3.04e+09 =  1% of the original kernel matrix.

torch.Size([245573, 2])
We keep 3.01e+08/2.10e+10 =  1% of the original kernel matrix.

torch.Size([97890, 2])
We keep 3.55e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([21877, 2])
We keep 3.75e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([28931, 2])
We keep 4.74e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([38899, 2])
We keep 7.74e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([38238, 2])
We keep 7.39e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([120890, 2])
We keep 1.12e+08/5.58e+09 =  2% of the original kernel matrix.

torch.Size([66387, 2])
We keep 2.03e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([206988, 2])
We keep 4.18e+08/1.46e+10 =  2% of the original kernel matrix.

torch.Size([89342, 2])
We keep 2.99e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([116634, 2])
We keep 8.38e+07/5.42e+09 =  1% of the original kernel matrix.

torch.Size([64911, 2])
We keep 2.01e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([126639, 2])
We keep 7.61e+07/5.31e+09 =  1% of the original kernel matrix.

torch.Size([68306, 2])
We keep 1.94e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([108118, 2])
We keep 2.34e+09/1.41e+10 = 16% of the original kernel matrix.

torch.Size([61566, 2])
We keep 2.92e+07/2.71e+09 =  1% of the original kernel matrix.

torch.Size([55620, 2])
We keep 2.10e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([45720, 2])
We keep 9.60e+06/7.42e+08 =  1% of the original kernel matrix.

torch.Size([6632, 2])
We keep 4.24e+05/8.19e+06 =  5% of the original kernel matrix.

torch.Size([16214, 2])
We keep 1.52e+06/6.54e+07 =  2% of the original kernel matrix.

torch.Size([15230, 2])
We keep 2.04e+06/6.62e+07 =  3% of the original kernel matrix.

torch.Size([23641, 2])
We keep 3.44e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([307192, 2])
We keep 3.13e+08/3.41e+10 =  0% of the original kernel matrix.

torch.Size([111271, 2])
We keep 4.33e+07/4.22e+09 =  1% of the original kernel matrix.

torch.Size([51239, 2])
We keep 3.07e+07/1.11e+09 =  2% of the original kernel matrix.

torch.Size([43039, 2])
We keep 1.02e+07/7.62e+08 =  1% of the original kernel matrix.

torch.Size([146201, 2])
We keep 6.70e+07/6.82e+09 =  0% of the original kernel matrix.

torch.Size([73915, 2])
We keep 2.16e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([7122, 2])
We keep 3.31e+05/7.95e+06 =  4% of the original kernel matrix.

torch.Size([16717, 2])
We keep 1.50e+06/6.45e+07 =  2% of the original kernel matrix.

torch.Size([10073, 2])
We keep 7.61e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([19438, 2])
We keep 2.12e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([161350, 2])
We keep 1.84e+08/9.79e+09 =  1% of the original kernel matrix.

torch.Size([77600, 2])
We keep 2.55e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([169636, 2])
We keep 1.36e+08/1.08e+10 =  1% of the original kernel matrix.

torch.Size([79455, 2])
We keep 2.69e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([100102, 2])
We keep 5.93e+07/3.56e+09 =  1% of the original kernel matrix.

torch.Size([60137, 2])
We keep 1.68e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([10402, 2])
We keep 6.81e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([19803, 2])
We keep 2.10e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([16271, 2])
We keep 2.03e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([24447, 2])
We keep 3.39e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([79881, 2])
We keep 5.22e+07/2.20e+09 =  2% of the original kernel matrix.

torch.Size([53759, 2])
We keep 1.36e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([69209, 2])
We keep 3.64e+07/1.63e+09 =  2% of the original kernel matrix.

torch.Size([50197, 2])
We keep 1.20e+07/9.23e+08 =  1% of the original kernel matrix.

torch.Size([175867, 2])
We keep 3.12e+08/1.29e+10 =  2% of the original kernel matrix.

torch.Size([81609, 2])
We keep 2.93e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([16902, 2])
We keep 2.99e+06/8.72e+07 =  3% of the original kernel matrix.

torch.Size([25087, 2])
We keep 3.63e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([31216, 2])
We keep 6.46e+08/1.11e+09 = 58% of the original kernel matrix.

torch.Size([32445, 2])
We keep 1.02e+07/7.61e+08 =  1% of the original kernel matrix.

torch.Size([45914, 2])
We keep 1.77e+07/7.93e+08 =  2% of the original kernel matrix.

torch.Size([40878, 2])
We keep 8.84e+06/6.44e+08 =  1% of the original kernel matrix.

torch.Size([34226, 2])
We keep 2.34e+07/6.09e+08 =  3% of the original kernel matrix.

torch.Size([35438, 2])
We keep 8.26e+06/5.64e+08 =  1% of the original kernel matrix.

torch.Size([475758, 2])
We keep 1.06e+09/8.78e+10 =  1% of the original kernel matrix.

torch.Size([136943, 2])
We keep 6.60e+07/6.78e+09 =  0% of the original kernel matrix.

torch.Size([23138, 2])
We keep 5.15e+06/1.58e+08 =  3% of the original kernel matrix.

torch.Size([29633, 2])
We keep 4.53e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([68481, 2])
We keep 2.94e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([49750, 2])
We keep 1.20e+07/9.42e+08 =  1% of the original kernel matrix.

torch.Size([196655, 2])
We keep 2.33e+08/1.38e+10 =  1% of the original kernel matrix.

torch.Size([86728, 2])
We keep 2.97e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([152062, 2])
We keep 2.92e+08/8.46e+09 =  3% of the original kernel matrix.

torch.Size([75705, 2])
We keep 2.41e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([169749, 2])
We keep 2.55e+08/1.24e+10 =  2% of the original kernel matrix.

torch.Size([80151, 2])
We keep 2.87e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([259812, 2])
We keep 3.41e+08/2.94e+10 =  1% of the original kernel matrix.

torch.Size([100972, 2])
We keep 4.18e+07/3.92e+09 =  1% of the original kernel matrix.

torch.Size([16328, 2])
We keep 3.12e+06/8.01e+07 =  3% of the original kernel matrix.

torch.Size([24596, 2])
We keep 3.64e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([80485, 2])
We keep 3.45e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([54112, 2])
We keep 1.42e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([41864, 2])
We keep 8.73e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([40374, 2])
We keep 7.57e+06/5.43e+08 =  1% of the original kernel matrix.

torch.Size([168670, 2])
We keep 1.83e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([79687, 2])
We keep 2.56e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([159677, 2])
We keep 1.60e+08/1.08e+10 =  1% of the original kernel matrix.

torch.Size([77319, 2])
We keep 2.61e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([61031, 2])
We keep 4.75e+07/1.89e+09 =  2% of the original kernel matrix.

torch.Size([46622, 2])
We keep 1.30e+07/9.94e+08 =  1% of the original kernel matrix.

torch.Size([162510, 2])
We keep 1.23e+08/8.90e+09 =  1% of the original kernel matrix.

torch.Size([78304, 2])
We keep 2.46e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([26603, 2])
We keep 3.96e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([31585, 2])
We keep 5.18e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([27652, 2])
We keep 1.76e+07/4.85e+08 =  3% of the original kernel matrix.

torch.Size([30724, 2])
We keep 7.26e+06/5.03e+08 =  1% of the original kernel matrix.

torch.Size([12485, 2])
We keep 1.15e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([21593, 2])
We keep 2.53e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([36590, 2])
We keep 1.79e+07/5.97e+08 =  3% of the original kernel matrix.

torch.Size([37301, 2])
We keep 8.04e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([221816, 2])
We keep 2.74e+08/2.00e+10 =  1% of the original kernel matrix.

torch.Size([92735, 2])
We keep 3.49e+07/3.23e+09 =  1% of the original kernel matrix.

torch.Size([384552, 2])
We keep 1.34e+09/7.33e+10 =  1% of the original kernel matrix.

torch.Size([122252, 2])
We keep 6.41e+07/6.19e+09 =  1% of the original kernel matrix.

torch.Size([21872, 2])
We keep 8.70e+06/2.22e+08 =  3% of the original kernel matrix.

torch.Size([28079, 2])
We keep 5.44e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([48321, 2])
We keep 1.26e+07/7.71e+08 =  1% of the original kernel matrix.

torch.Size([42924, 2])
We keep 8.39e+06/6.35e+08 =  1% of the original kernel matrix.

torch.Size([28420, 2])
We keep 5.64e+06/2.49e+08 =  2% of the original kernel matrix.

torch.Size([33053, 2])
We keep 5.53e+06/3.61e+08 =  1% of the original kernel matrix.

torch.Size([355693, 2])
We keep 7.39e+08/6.35e+10 =  1% of the original kernel matrix.

torch.Size([117868, 2])
We keep 5.92e+07/5.76e+09 =  1% of the original kernel matrix.

torch.Size([64179, 2])
We keep 2.69e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([48873, 2])
We keep 1.23e+07/9.39e+08 =  1% of the original kernel matrix.

torch.Size([24217, 2])
We keep 4.57e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([30141, 2])
We keep 4.72e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([83132, 2])
We keep 7.81e+07/3.10e+09 =  2% of the original kernel matrix.

torch.Size([55218, 2])
We keep 1.56e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([179990, 2])
We keep 1.26e+08/1.21e+10 =  1% of the original kernel matrix.

torch.Size([82736, 2])
We keep 2.81e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([22752, 2])
We keep 4.04e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([29003, 2])
We keep 4.74e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([100769, 2])
We keep 8.93e+07/3.83e+09 =  2% of the original kernel matrix.

torch.Size([60343, 2])
We keep 1.73e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([17325, 2])
We keep 2.48e+06/7.54e+07 =  3% of the original kernel matrix.

torch.Size([25424, 2])
We keep 3.39e+06/1.99e+08 =  1% of the original kernel matrix.

time for making ranges is 5.14048171043396
Sorting X and nu_X
time for sorting X is 0.09639501571655273
Sorting Z and nu_Z
time for sorting Z is 0.0002620220184326172
Starting Optim
sum tnu_Z before tensor(47563904., device='cuda:0')
c= tensor(4386.2046, device='cuda:0')
c= tensor(352942.2812, device='cuda:0')
c= tensor(385226.7188, device='cuda:0')
c= tensor(400771.5312, device='cuda:0')
c= tensor(978589.1250, device='cuda:0')
c= tensor(1954480., device='cuda:0')
c= tensor(3152117.7500, device='cuda:0')
c= tensor(3904155.5000, device='cuda:0')
c= tensor(3959520.2500, device='cuda:0')
c= tensor(65442152., device='cuda:0')
c= tensor(65530416., device='cuda:0')
c= tensor(71061424., device='cuda:0')
c= tensor(71101624., device='cuda:0')
c= tensor(94570872., device='cuda:0')
c= tensor(94798088., device='cuda:0')
c= tensor(96263512., device='cuda:0')
c= tensor(97786872., device='cuda:0')
c= tensor(98643232., device='cuda:0')
c= tensor(1.1362e+08, device='cuda:0')
c= tensor(1.1966e+08, device='cuda:0')
c= tensor(1.2115e+08, device='cuda:0')
c= tensor(1.7054e+08, device='cuda:0')
c= tensor(1.7061e+08, device='cuda:0')
c= tensor(1.7135e+08, device='cuda:0')
c= tensor(1.7171e+08, device='cuda:0')
c= tensor(1.7317e+08, device='cuda:0')
c= tensor(1.7493e+08, device='cuda:0')
c= tensor(1.7502e+08, device='cuda:0')
c= tensor(1.7983e+08, device='cuda:0')
c= tensor(8.4953e+08, device='cuda:0')
c= tensor(8.4963e+08, device='cuda:0')
c= tensor(9.9467e+08, device='cuda:0')
c= tensor(9.9479e+08, device='cuda:0')
c= tensor(9.9484e+08, device='cuda:0')
c= tensor(9.9501e+08, device='cuda:0')
c= tensor(1.0123e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0196e+09, device='cuda:0')
c= tensor(1.0197e+09, device='cuda:0')
c= tensor(1.0197e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0199e+09, device='cuda:0')
c= tensor(1.0199e+09, device='cuda:0')
c= tensor(1.0199e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0202e+09, device='cuda:0')
c= tensor(1.0202e+09, device='cuda:0')
c= tensor(1.0202e+09, device='cuda:0')
c= tensor(1.0202e+09, device='cuda:0')
c= tensor(1.0202e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0227e+09, device='cuda:0')
c= tensor(1.0227e+09, device='cuda:0')
c= tensor(1.0227e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0255e+09, device='cuda:0')
c= tensor(1.0258e+09, device='cuda:0')
c= tensor(1.0258e+09, device='cuda:0')
c= tensor(1.0258e+09, device='cuda:0')
c= tensor(1.0259e+09, device='cuda:0')
c= tensor(1.1201e+09, device='cuda:0')
c= tensor(1.2173e+09, device='cuda:0')
c= tensor(1.2173e+09, device='cuda:0')
c= tensor(1.2229e+09, device='cuda:0')
c= tensor(1.2234e+09, device='cuda:0')
c= tensor(1.2243e+09, device='cuda:0')
c= tensor(1.2654e+09, device='cuda:0')
c= tensor(1.2654e+09, device='cuda:0')
c= tensor(1.2655e+09, device='cuda:0')
c= tensor(1.2907e+09, device='cuda:0')
c= tensor(1.3575e+09, device='cuda:0')
c= tensor(1.3575e+09, device='cuda:0')
c= tensor(1.3579e+09, device='cuda:0')
c= tensor(1.3584e+09, device='cuda:0')
c= tensor(1.3819e+09, device='cuda:0')
c= tensor(1.3887e+09, device='cuda:0')
c= tensor(1.3904e+09, device='cuda:0')
c= tensor(1.3916e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.5851e+09, device='cuda:0')
c= tensor(1.5852e+09, device='cuda:0')
c= tensor(1.5852e+09, device='cuda:0')
c= tensor(1.5866e+09, device='cuda:0')
c= tensor(1.5875e+09, device='cuda:0')
c= tensor(1.6285e+09, device='cuda:0')
c= tensor(1.6323e+09, device='cuda:0')
c= tensor(1.6323e+09, device='cuda:0')
c= tensor(1.6338e+09, device='cuda:0')
c= tensor(1.6338e+09, device='cuda:0')
c= tensor(1.6361e+09, device='cuda:0')
c= tensor(1.6387e+09, device='cuda:0')
c= tensor(1.6387e+09, device='cuda:0')
c= tensor(1.6399e+09, device='cuda:0')
c= tensor(1.6399e+09, device='cuda:0')
c= tensor(1.6399e+09, device='cuda:0')
c= tensor(1.6434e+09, device='cuda:0')
c= tensor(1.6460e+09, device='cuda:0')
c= tensor(1.6472e+09, device='cuda:0')
c= tensor(1.6489e+09, device='cuda:0')
c= tensor(1.6849e+09, device='cuda:0')
c= tensor(1.6850e+09, device='cuda:0')
c= tensor(1.6853e+09, device='cuda:0')
c= tensor(1.6871e+09, device='cuda:0')
c= tensor(1.6872e+09, device='cuda:0')
c= tensor(1.6964e+09, device='cuda:0')
c= tensor(1.7303e+09, device='cuda:0')
c= tensor(1.7821e+09, device='cuda:0')
c= tensor(1.7825e+09, device='cuda:0')
c= tensor(1.7830e+09, device='cuda:0')
c= tensor(1.7833e+09, device='cuda:0')
c= tensor(1.7833e+09, device='cuda:0')
c= tensor(1.7843e+09, device='cuda:0')
c= tensor(1.7844e+09, device='cuda:0')
c= tensor(1.7847e+09, device='cuda:0')
c= tensor(1.8479e+09, device='cuda:0')
c= tensor(1.8511e+09, device='cuda:0')
c= tensor(1.8512e+09, device='cuda:0')
c= tensor(1.8513e+09, device='cuda:0')
c= tensor(1.8576e+09, device='cuda:0')
c= tensor(1.8583e+09, device='cuda:0')
c= tensor(1.8595e+09, device='cuda:0')
c= tensor(1.8596e+09, device='cuda:0')
c= tensor(1.9473e+09, device='cuda:0')
c= tensor(1.9473e+09, device='cuda:0')
c= tensor(1.9491e+09, device='cuda:0')
c= tensor(1.9492e+09, device='cuda:0')
c= tensor(1.9529e+09, device='cuda:0')
c= tensor(1.9549e+09, device='cuda:0')
c= tensor(2.0481e+09, device='cuda:0')
c= tensor(2.0504e+09, device='cuda:0')
c= tensor(2.0505e+09, device='cuda:0')
c= tensor(2.0558e+09, device='cuda:0')
c= tensor(2.0598e+09, device='cuda:0')
c= tensor(2.0599e+09, device='cuda:0')
c= tensor(2.0641e+09, device='cuda:0')
c= tensor(2.0706e+09, device='cuda:0')
c= tensor(2.0911e+09, device='cuda:0')
c= tensor(2.1226e+09, device='cuda:0')
c= tensor(2.1226e+09, device='cuda:0')
c= tensor(2.1228e+09, device='cuda:0')
c= tensor(2.1415e+09, device='cuda:0')
c= tensor(2.1624e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1641e+09, device='cuda:0')
c= tensor(2.1717e+09, device='cuda:0')
c= tensor(2.1824e+09, device='cuda:0')
c= tensor(2.1824e+09, device='cuda:0')
c= tensor(2.1830e+09, device='cuda:0')
c= tensor(2.1831e+09, device='cuda:0')
c= tensor(2.1831e+09, device='cuda:0')
c= tensor(2.1832e+09, device='cuda:0')
c= tensor(2.1832e+09, device='cuda:0')
c= tensor(2.1843e+09, device='cuda:0')
c= tensor(2.1850e+09, device='cuda:0')
c= tensor(2.1853e+09, device='cuda:0')
c= tensor(2.1858e+09, device='cuda:0')
c= tensor(2.1859e+09, device='cuda:0')
c= tensor(2.2592e+09, device='cuda:0')
c= tensor(2.2593e+09, device='cuda:0')
c= tensor(2.2639e+09, device='cuda:0')
c= tensor(2.2639e+09, device='cuda:0')
c= tensor(2.2639e+09, device='cuda:0')
c= tensor(2.2639e+09, device='cuda:0')
c= tensor(2.2658e+09, device='cuda:0')
c= tensor(2.2658e+09, device='cuda:0')
c= tensor(2.2852e+09, device='cuda:0')
c= tensor(2.2852e+09, device='cuda:0')
c= tensor(2.2853e+09, device='cuda:0')
c= tensor(2.2988e+09, device='cuda:0')
c= tensor(2.2995e+09, device='cuda:0')
c= tensor(2.3007e+09, device='cuda:0')
c= tensor(2.3172e+09, device='cuda:0')
c= tensor(2.3380e+09, device='cuda:0')
c= tensor(2.3380e+09, device='cuda:0')
c= tensor(2.3380e+09, device='cuda:0')
c= tensor(2.3391e+09, device='cuda:0')
c= tensor(2.3391e+09, device='cuda:0')
c= tensor(2.3392e+09, device='cuda:0')
c= tensor(2.3393e+09, device='cuda:0')
c= tensor(2.3394e+09, device='cuda:0')
c= tensor(2.3394e+09, device='cuda:0')
c= tensor(2.3394e+09, device='cuda:0')
c= tensor(2.3395e+09, device='cuda:0')
c= tensor(2.3899e+09, device='cuda:0')
c= tensor(2.3900e+09, device='cuda:0')
c= tensor(2.3945e+09, device='cuda:0')
c= tensor(2.3948e+09, device='cuda:0')
c= tensor(2.3955e+09, device='cuda:0')
c= tensor(2.3977e+09, device='cuda:0')
c= tensor(2.7730e+09, device='cuda:0')
c= tensor(2.8761e+09, device='cuda:0')
c= tensor(2.8767e+09, device='cuda:0')
c= tensor(2.8937e+09, device='cuda:0')
c= tensor(2.8937e+09, device='cuda:0')
c= tensor(2.8946e+09, device='cuda:0')
c= tensor(2.8974e+09, device='cuda:0')
c= tensor(2.8985e+09, device='cuda:0')
c= tensor(2.8985e+09, device='cuda:0')
c= tensor(2.9014e+09, device='cuda:0')
c= tensor(2.9813e+09, device='cuda:0')
c= tensor(2.9823e+09, device='cuda:0')
c= tensor(2.9823e+09, device='cuda:0')
c= tensor(2.9861e+09, device='cuda:0')
c= tensor(2.9862e+09, device='cuda:0')
c= tensor(2.9862e+09, device='cuda:0')
c= tensor(2.9954e+09, device='cuda:0')
c= tensor(2.9959e+09, device='cuda:0')
c= tensor(2.9960e+09, device='cuda:0')
c= tensor(3.0006e+09, device='cuda:0')
c= tensor(3.0008e+09, device='cuda:0')
c= tensor(3.0008e+09, device='cuda:0')
c= tensor(3.0087e+09, device='cuda:0')
c= tensor(3.0117e+09, device='cuda:0')
c= tensor(3.0172e+09, device='cuda:0')
c= tensor(3.0367e+09, device='cuda:0')
c= tensor(3.0464e+09, device='cuda:0')
c= tensor(3.0466e+09, device='cuda:0')
c= tensor(3.0488e+09, device='cuda:0')
c= tensor(3.0543e+09, device='cuda:0')
c= tensor(3.0649e+09, device='cuda:0')
c= tensor(3.0650e+09, device='cuda:0')
c= tensor(3.2779e+09, device='cuda:0')
c= tensor(3.4366e+09, device='cuda:0')
c= tensor(3.4422e+09, device='cuda:0')
c= tensor(3.4448e+09, device='cuda:0')
c= tensor(3.4466e+09, device='cuda:0')
c= tensor(3.4467e+09, device='cuda:0')
c= tensor(3.4467e+09, device='cuda:0')
c= tensor(3.4467e+09, device='cuda:0')
c= tensor(3.4508e+09, device='cuda:0')
c= tensor(3.4589e+09, device='cuda:0')
c= tensor(3.5051e+09, device='cuda:0')
c= tensor(3.5376e+09, device='cuda:0')
c= tensor(3.5400e+09, device='cuda:0')
c= tensor(3.5402e+09, device='cuda:0')
c= tensor(3.5450e+09, device='cuda:0')
c= tensor(3.5450e+09, device='cuda:0')
c= tensor(3.5451e+09, device='cuda:0')
c= tensor(3.5687e+09, device='cuda:0')
c= tensor(3.5691e+09, device='cuda:0')
c= tensor(3.5691e+09, device='cuda:0')
c= tensor(3.5692e+09, device='cuda:0')
c= tensor(3.5762e+09, device='cuda:0')
c= tensor(3.5765e+09, device='cuda:0')
c= tensor(3.5864e+09, device='cuda:0')
c= tensor(3.5866e+09, device='cuda:0')
c= tensor(3.5868e+09, device='cuda:0')
c= tensor(3.5868e+09, device='cuda:0')
c= tensor(3.5869e+09, device='cuda:0')
c= tensor(3.5881e+09, device='cuda:0')
c= tensor(3.5906e+09, device='cuda:0')
c= tensor(3.5906e+09, device='cuda:0')
c= tensor(3.5964e+09, device='cuda:0')
c= tensor(3.5964e+09, device='cuda:0')
c= tensor(3.5969e+09, device='cuda:0')
c= tensor(3.5969e+09, device='cuda:0')
c= tensor(3.6003e+09, device='cuda:0')
c= tensor(3.6008e+09, device='cuda:0')
c= tensor(3.6033e+09, device='cuda:0')
c= tensor(3.6036e+09, device='cuda:0')
c= tensor(3.6040e+09, device='cuda:0')
c= tensor(3.6053e+09, device='cuda:0')
c= tensor(3.6822e+09, device='cuda:0')
c= tensor(3.6823e+09, device='cuda:0')
c= tensor(3.6825e+09, device='cuda:0')
c= tensor(3.6884e+09, device='cuda:0')
c= tensor(3.6885e+09, device='cuda:0')
c= tensor(3.7754e+09, device='cuda:0')
c= tensor(3.7754e+09, device='cuda:0')
c= tensor(3.7791e+09, device='cuda:0')
c= tensor(3.8066e+09, device='cuda:0')
c= tensor(3.8066e+09, device='cuda:0')
c= tensor(3.8310e+09, device='cuda:0')
c= tensor(3.8327e+09, device='cuda:0')
c= tensor(4.0131e+09, device='cuda:0')
c= tensor(4.0131e+09, device='cuda:0')
c= tensor(4.0138e+09, device='cuda:0')
c= tensor(4.0139e+09, device='cuda:0')
c= tensor(4.0139e+09, device='cuda:0')
c= tensor(4.0140e+09, device='cuda:0')
c= tensor(4.0173e+09, device='cuda:0')
c= tensor(4.0181e+09, device='cuda:0')
c= tensor(4.0241e+09, device='cuda:0')
c= tensor(4.0242e+09, device='cuda:0')
c= tensor(4.0243e+09, device='cuda:0')
c= tensor(4.0243e+09, device='cuda:0')
c= tensor(4.0452e+09, device='cuda:0')
c= tensor(4.0495e+09, device='cuda:0')
c= tensor(4.0811e+09, device='cuda:0')
c= tensor(4.0821e+09, device='cuda:0')
c= tensor(4.0822e+09, device='cuda:0')
c= tensor(4.0822e+09, device='cuda:0')
c= tensor(4.0832e+09, device='cuda:0')
c= tensor(4.3753e+09, device='cuda:0')
c= tensor(4.3753e+09, device='cuda:0')
c= tensor(4.3755e+09, device='cuda:0')
c= tensor(4.3814e+09, device='cuda:0')
c= tensor(4.3852e+09, device='cuda:0')
c= tensor(4.3853e+09, device='cuda:0')
c= tensor(4.3853e+09, device='cuda:0')
c= tensor(4.3920e+09, device='cuda:0')
c= tensor(4.3932e+09, device='cuda:0')
c= tensor(4.3987e+09, device='cuda:0')
c= tensor(4.4001e+09, device='cuda:0')
c= tensor(4.4429e+09, device='cuda:0')
c= tensor(4.4481e+09, device='cuda:0')
c= tensor(4.4781e+09, device='cuda:0')
c= tensor(4.4856e+09, device='cuda:0')
c= tensor(4.4864e+09, device='cuda:0')
c= tensor(4.4886e+09, device='cuda:0')
c= tensor(4.4960e+09, device='cuda:0')
c= tensor(4.4966e+09, device='cuda:0')
c= tensor(4.4967e+09, device='cuda:0')
c= tensor(4.4967e+09, device='cuda:0')
c= tensor(4.4998e+09, device='cuda:0')
c= tensor(4.5001e+09, device='cuda:0')
c= tensor(4.5001e+09, device='cuda:0')
c= tensor(4.5003e+09, device='cuda:0')
c= tensor(4.5014e+09, device='cuda:0')
c= tensor(4.5045e+09, device='cuda:0')
c= tensor(4.5045e+09, device='cuda:0')
c= tensor(4.5047e+09, device='cuda:0')
c= tensor(4.5048e+09, device='cuda:0')
c= tensor(4.5051e+09, device='cuda:0')
c= tensor(4.5051e+09, device='cuda:0')
c= tensor(4.5052e+09, device='cuda:0')
c= tensor(4.5054e+09, device='cuda:0')
c= tensor(4.5331e+09, device='cuda:0')
c= tensor(4.5332e+09, device='cuda:0')
c= tensor(4.5332e+09, device='cuda:0')
c= tensor(4.5332e+09, device='cuda:0')
c= tensor(4.5625e+09, device='cuda:0')
c= tensor(4.5862e+09, device='cuda:0')
c= tensor(4.5874e+09, device='cuda:0')
c= tensor(4.5875e+09, device='cuda:0')
c= tensor(4.6007e+09, device='cuda:0')
c= tensor(4.6072e+09, device='cuda:0')
c= tensor(4.6072e+09, device='cuda:0')
c= tensor(4.6078e+09, device='cuda:0')
c= tensor(4.6082e+09, device='cuda:0')
c= tensor(4.6098e+09, device='cuda:0')
c= tensor(4.6114e+09, device='cuda:0')
c= tensor(4.6155e+09, device='cuda:0')
c= tensor(4.6156e+09, device='cuda:0')
c= tensor(4.6158e+09, device='cuda:0')
c= tensor(4.6158e+09, device='cuda:0')
c= tensor(4.6163e+09, device='cuda:0')
c= tensor(4.6166e+09, device='cuda:0')
c= tensor(4.6175e+09, device='cuda:0')
c= tensor(4.6298e+09, device='cuda:0')
c= tensor(4.6563e+09, device='cuda:0')
c= tensor(4.6563e+09, device='cuda:0')
c= tensor(4.6564e+09, device='cuda:0')
c= tensor(4.6564e+09, device='cuda:0')
c= tensor(4.6570e+09, device='cuda:0')
c= tensor(4.6574e+09, device='cuda:0')
c= tensor(4.6574e+09, device='cuda:0')
c= tensor(4.6574e+09, device='cuda:0')
c= tensor(4.6683e+09, device='cuda:0')
c= tensor(4.6683e+09, device='cuda:0')
c= tensor(4.6719e+09, device='cuda:0')
c= tensor(4.6719e+09, device='cuda:0')
c= tensor(4.6720e+09, device='cuda:0')
c= tensor(4.6723e+09, device='cuda:0')
c= tensor(4.6723e+09, device='cuda:0')
c= tensor(4.6723e+09, device='cuda:0')
c= tensor(4.6753e+09, device='cuda:0')
c= tensor(4.7159e+09, device='cuda:0')
c= tensor(4.7280e+09, device='cuda:0')
c= tensor(4.7337e+09, device='cuda:0')
c= tensor(4.7341e+09, device='cuda:0')
c= tensor(4.7342e+09, device='cuda:0')
c= tensor(4.7344e+09, device='cuda:0')
c= tensor(4.7415e+09, device='cuda:0')
c= tensor(4.7417e+09, device='cuda:0')
c= tensor(4.7427e+09, device='cuda:0')
c= tensor(4.7428e+09, device='cuda:0')
c= tensor(4.9183e+09, device='cuda:0')
c= tensor(4.9190e+09, device='cuda:0')
c= tensor(4.9201e+09, device='cuda:0')
c= tensor(4.9675e+09, device='cuda:0')
c= tensor(4.9679e+09, device='cuda:0')
c= tensor(4.9685e+09, device='cuda:0')
c= tensor(4.9887e+09, device='cuda:0')
c= tensor(4.9941e+09, device='cuda:0')
c= tensor(4.9947e+09, device='cuda:0')
c= tensor(4.9949e+09, device='cuda:0')
c= tensor(4.9955e+09, device='cuda:0')
c= tensor(4.9956e+09, device='cuda:0')
c= tensor(4.9990e+09, device='cuda:0')
c= tensor(5.1402e+09, device='cuda:0')
c= tensor(5.1627e+09, device='cuda:0')
c= tensor(5.1711e+09, device='cuda:0')
c= tensor(5.1711e+09, device='cuda:0')
c= tensor(5.1751e+09, device='cuda:0')
c= tensor(5.1787e+09, device='cuda:0')
c= tensor(5.1788e+09, device='cuda:0')
c= tensor(5.1792e+09, device='cuda:0')
c= tensor(5.2102e+09, device='cuda:0')
c= tensor(5.2112e+09, device='cuda:0')
c= tensor(5.2445e+09, device='cuda:0')
c= tensor(5.2479e+09, device='cuda:0')
c= tensor(5.2494e+09, device='cuda:0')
c= tensor(5.2494e+09, device='cuda:0')
c= tensor(5.2555e+09, device='cuda:0')
c= tensor(5.2614e+09, device='cuda:0')
c= tensor(5.2614e+09, device='cuda:0')
c= tensor(5.3501e+09, device='cuda:0')
c= tensor(5.3530e+09, device='cuda:0')
c= tensor(5.3570e+09, device='cuda:0')
c= tensor(5.3571e+09, device='cuda:0')
c= tensor(5.3573e+09, device='cuda:0')
c= tensor(5.3573e+09, device='cuda:0')
c= tensor(5.3573e+09, device='cuda:0')
c= tensor(5.3582e+09, device='cuda:0')
c= tensor(5.3642e+09, device='cuda:0')
c= tensor(7.1864e+09, device='cuda:0')
c= tensor(7.1864e+09, device='cuda:0')
c= tensor(7.1928e+09, device='cuda:0')
c= tensor(7.1929e+09, device='cuda:0')
c= tensor(7.1930e+09, device='cuda:0')
c= tensor(7.1933e+09, device='cuda:0')
c= tensor(7.2322e+09, device='cuda:0')
c= tensor(7.2323e+09, device='cuda:0')
c= tensor(7.4725e+09, device='cuda:0')
c= tensor(7.4725e+09, device='cuda:0')
c= tensor(7.4829e+09, device='cuda:0')
c= tensor(7.4864e+09, device='cuda:0')
c= tensor(7.4906e+09, device='cuda:0')
c= tensor(7.5104e+09, device='cuda:0')
c= tensor(7.5105e+09, device='cuda:0')
c= tensor(7.5106e+09, device='cuda:0')
c= tensor(7.5123e+09, device='cuda:0')
c= tensor(7.5124e+09, device='cuda:0')
c= tensor(7.5127e+09, device='cuda:0')
c= tensor(7.5167e+09, device='cuda:0')
c= tensor(7.5228e+09, device='cuda:0')
c= tensor(7.5247e+09, device='cuda:0')
c= tensor(7.5253e+09, device='cuda:0')
c= tensor(7.5563e+09, device='cuda:0')
c= tensor(7.6210e+09, device='cuda:0')
c= tensor(7.6221e+09, device='cuda:0')
c= tensor(7.6221e+09, device='cuda:0')
c= tensor(7.6403e+09, device='cuda:0')
c= tensor(7.6409e+09, device='cuda:0')
c= tensor(7.6421e+09, device='cuda:0')
c= tensor(7.6457e+09, device='cuda:0')
c= tensor(7.6494e+09, device='cuda:0')
c= tensor(7.6546e+09, device='cuda:0')
c= tensor(7.7021e+09, device='cuda:0')
c= tensor(7.7028e+09, device='cuda:0')
c= tensor(7.7029e+09, device='cuda:0')
c= tensor(7.7029e+09, device='cuda:0')
c= tensor(7.7034e+09, device='cuda:0')
c= tensor(7.7063e+09, device='cuda:0')
c= tensor(7.7145e+09, device='cuda:0')
c= tensor(7.7232e+09, device='cuda:0')
c= tensor(7.7232e+09, device='cuda:0')
c= tensor(7.7234e+09, device='cuda:0')
c= tensor(7.7267e+09, device='cuda:0')
c= tensor(7.7380e+09, device='cuda:0')
c= tensor(7.7398e+09, device='cuda:0')
c= tensor(7.7428e+09, device='cuda:0')
c= tensor(7.8473e+09, device='cuda:0')
c= tensor(7.8480e+09, device='cuda:0')
c= tensor(7.8480e+09, device='cuda:0')
c= tensor(7.8480e+09, device='cuda:0')
c= tensor(7.8611e+09, device='cuda:0')
c= tensor(7.8628e+09, device='cuda:0')
c= tensor(7.8643e+09, device='cuda:0')
c= tensor(7.8643e+09, device='cuda:0')
c= tensor(7.8643e+09, device='cuda:0')
c= tensor(7.8700e+09, device='cuda:0')
c= tensor(7.8734e+09, device='cuda:0')
c= tensor(7.8747e+09, device='cuda:0')
c= tensor(7.8748e+09, device='cuda:0')
c= tensor(7.8748e+09, device='cuda:0')
c= tensor(7.8758e+09, device='cuda:0')
c= tensor(7.8765e+09, device='cuda:0')
c= tensor(7.8857e+09, device='cuda:0')
c= tensor(7.8857e+09, device='cuda:0')
c= tensor(7.9039e+09, device='cuda:0')
c= tensor(7.9043e+09, device='cuda:0')
c= tensor(7.9048e+09, device='cuda:0')
c= tensor(7.9541e+09, device='cuda:0')
c= tensor(7.9544e+09, device='cuda:0')
c= tensor(7.9552e+09, device='cuda:0')
c= tensor(7.9610e+09, device='cuda:0')
c= tensor(7.9670e+09, device='cuda:0')
c= tensor(7.9729e+09, device='cuda:0')
c= tensor(7.9816e+09, device='cuda:0')
c= tensor(7.9816e+09, device='cuda:0')
c= tensor(7.9823e+09, device='cuda:0')
c= tensor(7.9825e+09, device='cuda:0')
c= tensor(7.9861e+09, device='cuda:0')
c= tensor(7.9905e+09, device='cuda:0')
c= tensor(7.9915e+09, device='cuda:0')
c= tensor(7.9946e+09, device='cuda:0')
c= tensor(7.9947e+09, device='cuda:0')
c= tensor(7.9967e+09, device='cuda:0')
c= tensor(7.9967e+09, device='cuda:0')
c= tensor(7.9971e+09, device='cuda:0')
c= tensor(8.0042e+09, device='cuda:0')
c= tensor(8.0470e+09, device='cuda:0')
c= tensor(8.0472e+09, device='cuda:0')
c= tensor(8.0476e+09, device='cuda:0')
c= tensor(8.0477e+09, device='cuda:0')
c= tensor(8.0714e+09, device='cuda:0')
c= tensor(8.0719e+09, device='cuda:0')
c= tensor(8.0720e+09, device='cuda:0')
c= tensor(8.0734e+09, device='cuda:0')
c= tensor(8.0762e+09, device='cuda:0')
c= tensor(8.0763e+09, device='cuda:0')
c= tensor(8.0785e+09, device='cuda:0')
c= tensor(8.0786e+09, device='cuda:0')
memory (bytes)
5329002496
time for making loss 2 is 12.059360027313232
p0 True
it  0 : 2665190400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 55% |
shape of L is 
torch.Size([])
memory (bytes)
5329391616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5329981440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  106952350000.0
relative error loss 13.238996
shape of L is 
torch.Size([])
memory (bytes)
5481594880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5481857024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  106951960000.0
relative error loss 13.238947
shape of L is 
torch.Size([])
memory (bytes)
5487108096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5487108096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  106951025000.0
relative error loss 13.2388315
shape of L is 
torch.Size([])
memory (bytes)
5489229824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5489274880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  106945050000.0
relative error loss 13.238092
shape of L is 
torch.Size([])
memory (bytes)
5491359744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5491474432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  106900010000.0
relative error loss 13.232517
shape of L is 
torch.Size([])
memory (bytes)
5493534720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5493534720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  106664650000.0
relative error loss 13.2033825
shape of L is 
torch.Size([])
memory (bytes)
5495685120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5495685120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  105376510000.0
relative error loss 13.043932
shape of L is 
torch.Size([])
memory (bytes)
5497765888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5497835520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  92055320000.0
relative error loss 11.394981
shape of L is 
torch.Size([])
memory (bytes)
5499817984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5499981824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  25582359000.0
relative error loss 3.1666882
shape of L is 
torch.Size([])
memory (bytes)
5502103552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5502103552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  14622061000.0
relative error loss 1.809978
time to take a step is 211.11141896247864
it  1 : 3114669568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5504102400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5504102400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  14622061000.0
relative error loss 1.809978
shape of L is 
torch.Size([])
memory (bytes)
5506342912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5506342912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  16220417000.0
relative error loss 2.007829
shape of L is 
torch.Size([])
memory (bytes)
5508497408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5508497408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  10945439000.0
relative error loss 1.3548708
shape of L is 
torch.Size([])
memory (bytes)
5510672384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5510717440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 12% |
error is  9048488000.0
relative error loss 1.1200585
shape of L is 
torch.Size([])
memory (bytes)
5512794112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5512794112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  8247208000.0
relative error loss 1.0208728
shape of L is 
torch.Size([])
memory (bytes)
5514915840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5514915840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  10321037000.0
relative error loss 1.2775799
shape of L is 
torch.Size([])
memory (bytes)
5517062144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5517062144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  7957992000.0
relative error loss 0.9850725
shape of L is 
torch.Size([])
memory (bytes)
5519134720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5519216640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  7629418500.0
relative error loss 0.94440037
shape of L is 
torch.Size([])
memory (bytes)
5521149952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5521149952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  8026832400.0
relative error loss 0.9935939
shape of L is 
torch.Size([])
memory (bytes)
5523382272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5523431424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  7316072400.0
relative error loss 0.9056131
time to take a step is 202.62475109100342
it  2 : 3243092992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5525442560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5525442560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  7316072400.0
relative error loss 0.9056131
shape of L is 
torch.Size([])
memory (bytes)
5527584768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5527584768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  7094137000.0
relative error loss 0.87814105
shape of L is 
torch.Size([])
memory (bytes)
5529726976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5529776128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  6767649300.0
relative error loss 0.83772707
shape of L is 
torch.Size([])
memory (bytes)
5531721728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5531721728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6472573400.0
relative error loss 0.8012014
shape of L is 
torch.Size([])
memory (bytes)
5533966336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5533966336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  6984768000.0
relative error loss 0.8646029
shape of L is 
torch.Size([])
memory (bytes)
5536079872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5536129024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6188398600.0
relative error loss 0.76602507
shape of L is 
torch.Size([])
memory (bytes)
5538127872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5538127872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5749832700.0
relative error loss 0.71173763
shape of L is 
torch.Size([])
memory (bytes)
5540327424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5540327424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  5265626000.0
relative error loss 0.6518006
shape of L is 
torch.Size([])
memory (bytes)
5542473728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5542522880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4854028300.0
relative error loss 0.6008513
shape of L is 
torch.Size([])
memory (bytes)
5544583168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5544583168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4433981400.0
relative error loss 0.5488562
time to take a step is 202.6821186542511
it  3 : 3243893248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5546721280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5546721280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4433981400.0
relative error loss 0.5488562
shape of L is 
torch.Size([])
memory (bytes)
5548883968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5548883968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  4013944000.0
relative error loss 0.4968623
shape of L is 
torch.Size([])
memory (bytes)
5550911488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5550911488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  3636727300.0
relative error loss 0.45016885
shape of L is 
torch.Size([])
memory (bytes)
5553111040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5553111040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  3257468400.0
relative error loss 0.40322265
shape of L is 
torch.Size([])
memory (bytes)
5555286016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5555339264
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 99% | 12% |
error is  2936797700.0
relative error loss 0.36352873
shape of L is 
torch.Size([])
memory (bytes)
5557391360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5557391360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2671655400.0
relative error loss 0.33070835
shape of L is 
torch.Size([])
memory (bytes)
5559554048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5559554048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2444855800.0
relative error loss 0.30263418
shape of L is 
torch.Size([])
memory (bytes)
5561516032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5561753600
| ID | GPU | MEM |
------------------
|  0 |  4% |  0% |
|  1 | 99% | 12% |
error is  2241627600.0
relative error loss 0.27747777
shape of L is 
torch.Size([])
memory (bytes)
5563830272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5563883520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2043862000.0
relative error loss 0.25299752
shape of L is 
torch.Size([])
memory (bytes)
5565968384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5566017536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1923455000.0
relative error loss 0.23809306
time to take a step is 200.7295298576355
c= tensor(4386.2046, device='cuda:0')
c= tensor(352942.2812, device='cuda:0')
c= tensor(385226.7188, device='cuda:0')
c= tensor(400771.5312, device='cuda:0')
c= tensor(978589.1250, device='cuda:0')
c= tensor(1954480., device='cuda:0')
c= tensor(3152117.7500, device='cuda:0')
c= tensor(3904155.5000, device='cuda:0')
c= tensor(3959520.2500, device='cuda:0')
c= tensor(65442152., device='cuda:0')
c= tensor(65530416., device='cuda:0')
c= tensor(71061424., device='cuda:0')
c= tensor(71101624., device='cuda:0')
c= tensor(94570872., device='cuda:0')
c= tensor(94798088., device='cuda:0')
c= tensor(96263512., device='cuda:0')
c= tensor(97786872., device='cuda:0')
c= tensor(98643232., device='cuda:0')
c= tensor(1.1362e+08, device='cuda:0')
c= tensor(1.1966e+08, device='cuda:0')
c= tensor(1.2115e+08, device='cuda:0')
c= tensor(1.7054e+08, device='cuda:0')
c= tensor(1.7061e+08, device='cuda:0')
c= tensor(1.7135e+08, device='cuda:0')
c= tensor(1.7171e+08, device='cuda:0')
c= tensor(1.7317e+08, device='cuda:0')
c= tensor(1.7493e+08, device='cuda:0')
c= tensor(1.7502e+08, device='cuda:0')
c= tensor(1.7983e+08, device='cuda:0')
c= tensor(8.4953e+08, device='cuda:0')
c= tensor(8.4963e+08, device='cuda:0')
c= tensor(9.9467e+08, device='cuda:0')
c= tensor(9.9479e+08, device='cuda:0')
c= tensor(9.9484e+08, device='cuda:0')
c= tensor(9.9501e+08, device='cuda:0')
c= tensor(1.0123e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0195e+09, device='cuda:0')
c= tensor(1.0196e+09, device='cuda:0')
c= tensor(1.0197e+09, device='cuda:0')
c= tensor(1.0197e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0198e+09, device='cuda:0')
c= tensor(1.0199e+09, device='cuda:0')
c= tensor(1.0199e+09, device='cuda:0')
c= tensor(1.0199e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0201e+09, device='cuda:0')
c= tensor(1.0202e+09, device='cuda:0')
c= tensor(1.0202e+09, device='cuda:0')
c= tensor(1.0202e+09, device='cuda:0')
c= tensor(1.0202e+09, device='cuda:0')
c= tensor(1.0202e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0203e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0204e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0227e+09, device='cuda:0')
c= tensor(1.0227e+09, device='cuda:0')
c= tensor(1.0227e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0255e+09, device='cuda:0')
c= tensor(1.0258e+09, device='cuda:0')
c= tensor(1.0258e+09, device='cuda:0')
c= tensor(1.0258e+09, device='cuda:0')
c= tensor(1.0259e+09, device='cuda:0')
c= tensor(1.1201e+09, device='cuda:0')
c= tensor(1.2173e+09, device='cuda:0')
c= tensor(1.2173e+09, device='cuda:0')
c= tensor(1.2229e+09, device='cuda:0')
c= tensor(1.2234e+09, device='cuda:0')
c= tensor(1.2243e+09, device='cuda:0')
c= tensor(1.2654e+09, device='cuda:0')
c= tensor(1.2654e+09, device='cuda:0')
c= tensor(1.2655e+09, device='cuda:0')
c= tensor(1.2907e+09, device='cuda:0')
c= tensor(1.3575e+09, device='cuda:0')
c= tensor(1.3575e+09, device='cuda:0')
c= tensor(1.3579e+09, device='cuda:0')
c= tensor(1.3584e+09, device='cuda:0')
c= tensor(1.3819e+09, device='cuda:0')
c= tensor(1.3887e+09, device='cuda:0')
c= tensor(1.3904e+09, device='cuda:0')
c= tensor(1.3916e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.5851e+09, device='cuda:0')
c= tensor(1.5852e+09, device='cuda:0')
c= tensor(1.5852e+09, device='cuda:0')
c= tensor(1.5866e+09, device='cuda:0')
c= tensor(1.5875e+09, device='cuda:0')
c= tensor(1.6285e+09, device='cuda:0')
c= tensor(1.6323e+09, device='cuda:0')
c= tensor(1.6323e+09, device='cuda:0')
c= tensor(1.6338e+09, device='cuda:0')
c= tensor(1.6338e+09, device='cuda:0')
c= tensor(1.6361e+09, device='cuda:0')
c= tensor(1.6387e+09, device='cuda:0')
c= tensor(1.6387e+09, device='cuda:0')
c= tensor(1.6399e+09, device='cuda:0')
c= tensor(1.6399e+09, device='cuda:0')
c= tensor(1.6399e+09, device='cuda:0')
c= tensor(1.6434e+09, device='cuda:0')
c= tensor(1.6460e+09, device='cuda:0')
c= tensor(1.6472e+09, device='cuda:0')
c= tensor(1.6489e+09, device='cuda:0')
c= tensor(1.6849e+09, device='cuda:0')
c= tensor(1.6850e+09, device='cuda:0')
c= tensor(1.6853e+09, device='cuda:0')
c= tensor(1.6871e+09, device='cuda:0')
c= tensor(1.6872e+09, device='cuda:0')
c= tensor(1.6964e+09, device='cuda:0')
c= tensor(1.7303e+09, device='cuda:0')
c= tensor(1.7821e+09, device='cuda:0')
c= tensor(1.7825e+09, device='cuda:0')
c= tensor(1.7830e+09, device='cuda:0')
c= tensor(1.7833e+09, device='cuda:0')
c= tensor(1.7833e+09, device='cuda:0')
c= tensor(1.7843e+09, device='cuda:0')
c= tensor(1.7844e+09, device='cuda:0')
c= tensor(1.7847e+09, device='cuda:0')
c= tensor(1.8479e+09, device='cuda:0')
c= tensor(1.8511e+09, device='cuda:0')
c= tensor(1.8512e+09, device='cuda:0')
c= tensor(1.8513e+09, device='cuda:0')
c= tensor(1.8576e+09, device='cuda:0')
c= tensor(1.8583e+09, device='cuda:0')
c= tensor(1.8595e+09, device='cuda:0')
c= tensor(1.8596e+09, device='cuda:0')
c= tensor(1.9473e+09, device='cuda:0')
c= tensor(1.9473e+09, device='cuda:0')
c= tensor(1.9491e+09, device='cuda:0')
c= tensor(1.9492e+09, device='cuda:0')
c= tensor(1.9529e+09, device='cuda:0')
c= tensor(1.9549e+09, device='cuda:0')
c= tensor(2.0481e+09, device='cuda:0')
c= tensor(2.0504e+09, device='cuda:0')
c= tensor(2.0505e+09, device='cuda:0')
c= tensor(2.0558e+09, device='cuda:0')
c= tensor(2.0598e+09, device='cuda:0')
c= tensor(2.0599e+09, device='cuda:0')
c= tensor(2.0641e+09, device='cuda:0')
c= tensor(2.0706e+09, device='cuda:0')
c= tensor(2.0911e+09, device='cuda:0')
c= tensor(2.1226e+09, device='cuda:0')
c= tensor(2.1226e+09, device='cuda:0')
c= tensor(2.1228e+09, device='cuda:0')
c= tensor(2.1415e+09, device='cuda:0')
c= tensor(2.1624e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1641e+09, device='cuda:0')
c= tensor(2.1717e+09, device='cuda:0')
c= tensor(2.1824e+09, device='cuda:0')
c= tensor(2.1824e+09, device='cuda:0')
c= tensor(2.1830e+09, device='cuda:0')
c= tensor(2.1831e+09, device='cuda:0')
c= tensor(2.1831e+09, device='cuda:0')
c= tensor(2.1832e+09, device='cuda:0')
c= tensor(2.1832e+09, device='cuda:0')
c= tensor(2.1843e+09, device='cuda:0')
c= tensor(2.1850e+09, device='cuda:0')
c= tensor(2.1853e+09, device='cuda:0')
c= tensor(2.1858e+09, device='cuda:0')
c= tensor(2.1859e+09, device='cuda:0')
c= tensor(2.2592e+09, device='cuda:0')
c= tensor(2.2593e+09, device='cuda:0')
c= tensor(2.2639e+09, device='cuda:0')
c= tensor(2.2639e+09, device='cuda:0')
c= tensor(2.2639e+09, device='cuda:0')
c= tensor(2.2639e+09, device='cuda:0')
c= tensor(2.2658e+09, device='cuda:0')
c= tensor(2.2658e+09, device='cuda:0')
c= tensor(2.2852e+09, device='cuda:0')
c= tensor(2.2852e+09, device='cuda:0')
c= tensor(2.2853e+09, device='cuda:0')
c= tensor(2.2988e+09, device='cuda:0')
c= tensor(2.2995e+09, device='cuda:0')
c= tensor(2.3007e+09, device='cuda:0')
c= tensor(2.3172e+09, device='cuda:0')
c= tensor(2.3380e+09, device='cuda:0')
c= tensor(2.3380e+09, device='cuda:0')
c= tensor(2.3380e+09, device='cuda:0')
c= tensor(2.3391e+09, device='cuda:0')
c= tensor(2.3391e+09, device='cuda:0')
c= tensor(2.3392e+09, device='cuda:0')
c= tensor(2.3393e+09, device='cuda:0')
c= tensor(2.3394e+09, device='cuda:0')
c= tensor(2.3394e+09, device='cuda:0')
c= tensor(2.3394e+09, device='cuda:0')
c= tensor(2.3395e+09, device='cuda:0')
c= tensor(2.3899e+09, device='cuda:0')
c= tensor(2.3900e+09, device='cuda:0')
c= tensor(2.3945e+09, device='cuda:0')
c= tensor(2.3948e+09, device='cuda:0')
c= tensor(2.3955e+09, device='cuda:0')
c= tensor(2.3977e+09, device='cuda:0')
c= tensor(2.7730e+09, device='cuda:0')
c= tensor(2.8761e+09, device='cuda:0')
c= tensor(2.8767e+09, device='cuda:0')
c= tensor(2.8937e+09, device='cuda:0')
c= tensor(2.8937e+09, device='cuda:0')
c= tensor(2.8946e+09, device='cuda:0')
c= tensor(2.8974e+09, device='cuda:0')
c= tensor(2.8985e+09, device='cuda:0')
c= tensor(2.8985e+09, device='cuda:0')
c= tensor(2.9014e+09, device='cuda:0')
c= tensor(2.9813e+09, device='cuda:0')
c= tensor(2.9823e+09, device='cuda:0')
c= tensor(2.9823e+09, device='cuda:0')
c= tensor(2.9861e+09, device='cuda:0')
c= tensor(2.9862e+09, device='cuda:0')
c= tensor(2.9862e+09, device='cuda:0')
c= tensor(2.9954e+09, device='cuda:0')
c= tensor(2.9959e+09, device='cuda:0')
c= tensor(2.9960e+09, device='cuda:0')
c= tensor(3.0006e+09, device='cuda:0')
c= tensor(3.0008e+09, device='cuda:0')
c= tensor(3.0008e+09, device='cuda:0')
c= tensor(3.0087e+09, device='cuda:0')
c= tensor(3.0117e+09, device='cuda:0')
c= tensor(3.0172e+09, device='cuda:0')
c= tensor(3.0367e+09, device='cuda:0')
c= tensor(3.0464e+09, device='cuda:0')
c= tensor(3.0466e+09, device='cuda:0')
c= tensor(3.0488e+09, device='cuda:0')
c= tensor(3.0543e+09, device='cuda:0')
c= tensor(3.0649e+09, device='cuda:0')
c= tensor(3.0650e+09, device='cuda:0')
c= tensor(3.2779e+09, device='cuda:0')
c= tensor(3.4366e+09, device='cuda:0')
c= tensor(3.4422e+09, device='cuda:0')
c= tensor(3.4448e+09, device='cuda:0')
c= tensor(3.4466e+09, device='cuda:0')
c= tensor(3.4467e+09, device='cuda:0')
c= tensor(3.4467e+09, device='cuda:0')
c= tensor(3.4467e+09, device='cuda:0')
c= tensor(3.4508e+09, device='cuda:0')
c= tensor(3.4589e+09, device='cuda:0')
c= tensor(3.5051e+09, device='cuda:0')
c= tensor(3.5376e+09, device='cuda:0')
c= tensor(3.5400e+09, device='cuda:0')
c= tensor(3.5402e+09, device='cuda:0')
c= tensor(3.5450e+09, device='cuda:0')
c= tensor(3.5450e+09, device='cuda:0')
c= tensor(3.5451e+09, device='cuda:0')
c= tensor(3.5687e+09, device='cuda:0')
c= tensor(3.5691e+09, device='cuda:0')
c= tensor(3.5691e+09, device='cuda:0')
c= tensor(3.5692e+09, device='cuda:0')
c= tensor(3.5762e+09, device='cuda:0')
c= tensor(3.5765e+09, device='cuda:0')
c= tensor(3.5864e+09, device='cuda:0')
c= tensor(3.5866e+09, device='cuda:0')
c= tensor(3.5868e+09, device='cuda:0')
c= tensor(3.5868e+09, device='cuda:0')
c= tensor(3.5869e+09, device='cuda:0')
c= tensor(3.5881e+09, device='cuda:0')
c= tensor(3.5906e+09, device='cuda:0')
c= tensor(3.5906e+09, device='cuda:0')
c= tensor(3.5964e+09, device='cuda:0')
c= tensor(3.5964e+09, device='cuda:0')
c= tensor(3.5969e+09, device='cuda:0')
c= tensor(3.5969e+09, device='cuda:0')
c= tensor(3.6003e+09, device='cuda:0')
c= tensor(3.6008e+09, device='cuda:0')
c= tensor(3.6033e+09, device='cuda:0')
c= tensor(3.6036e+09, device='cuda:0')
c= tensor(3.6040e+09, device='cuda:0')
c= tensor(3.6053e+09, device='cuda:0')
c= tensor(3.6822e+09, device='cuda:0')
c= tensor(3.6823e+09, device='cuda:0')
c= tensor(3.6825e+09, device='cuda:0')
c= tensor(3.6884e+09, device='cuda:0')
c= tensor(3.6885e+09, device='cuda:0')
c= tensor(3.7754e+09, device='cuda:0')
c= tensor(3.7754e+09, device='cuda:0')
c= tensor(3.7791e+09, device='cuda:0')
c= tensor(3.8066e+09, device='cuda:0')
c= tensor(3.8066e+09, device='cuda:0')
c= tensor(3.8310e+09, device='cuda:0')
c= tensor(3.8327e+09, device='cuda:0')
c= tensor(4.0131e+09, device='cuda:0')
c= tensor(4.0131e+09, device='cuda:0')
c= tensor(4.0138e+09, device='cuda:0')
c= tensor(4.0139e+09, device='cuda:0')
c= tensor(4.0139e+09, device='cuda:0')
c= tensor(4.0140e+09, device='cuda:0')
c= tensor(4.0173e+09, device='cuda:0')
c= tensor(4.0181e+09, device='cuda:0')
c= tensor(4.0241e+09, device='cuda:0')
c= tensor(4.0242e+09, device='cuda:0')
c= tensor(4.0243e+09, device='cuda:0')
c= tensor(4.0243e+09, device='cuda:0')
c= tensor(4.0452e+09, device='cuda:0')
c= tensor(4.0495e+09, device='cuda:0')
c= tensor(4.0811e+09, device='cuda:0')
c= tensor(4.0821e+09, device='cuda:0')
c= tensor(4.0822e+09, device='cuda:0')
c= tensor(4.0822e+09, device='cuda:0')
c= tensor(4.0832e+09, device='cuda:0')
c= tensor(4.3753e+09, device='cuda:0')
c= tensor(4.3753e+09, device='cuda:0')
c= tensor(4.3755e+09, device='cuda:0')
c= tensor(4.3814e+09, device='cuda:0')
c= tensor(4.3852e+09, device='cuda:0')
c= tensor(4.3853e+09, device='cuda:0')
c= tensor(4.3853e+09, device='cuda:0')
c= tensor(4.3920e+09, device='cuda:0')
c= tensor(4.3932e+09, device='cuda:0')
c= tensor(4.3987e+09, device='cuda:0')
c= tensor(4.4001e+09, device='cuda:0')
c= tensor(4.4429e+09, device='cuda:0')
c= tensor(4.4481e+09, device='cuda:0')
c= tensor(4.4781e+09, device='cuda:0')
c= tensor(4.4856e+09, device='cuda:0')
c= tensor(4.4864e+09, device='cuda:0')
c= tensor(4.4886e+09, device='cuda:0')
c= tensor(4.4960e+09, device='cuda:0')
c= tensor(4.4966e+09, device='cuda:0')
c= tensor(4.4967e+09, device='cuda:0')
c= tensor(4.4967e+09, device='cuda:0')
c= tensor(4.4998e+09, device='cuda:0')
c= tensor(4.5001e+09, device='cuda:0')
c= tensor(4.5001e+09, device='cuda:0')
c= tensor(4.5003e+09, device='cuda:0')
c= tensor(4.5014e+09, device='cuda:0')
c= tensor(4.5045e+09, device='cuda:0')
c= tensor(4.5045e+09, device='cuda:0')
c= tensor(4.5047e+09, device='cuda:0')
c= tensor(4.5048e+09, device='cuda:0')
c= tensor(4.5051e+09, device='cuda:0')
c= tensor(4.5051e+09, device='cuda:0')
c= tensor(4.5052e+09, device='cuda:0')
c= tensor(4.5054e+09, device='cuda:0')
c= tensor(4.5331e+09, device='cuda:0')
c= tensor(4.5332e+09, device='cuda:0')
c= tensor(4.5332e+09, device='cuda:0')
c= tensor(4.5332e+09, device='cuda:0')
c= tensor(4.5625e+09, device='cuda:0')
c= tensor(4.5862e+09, device='cuda:0')
c= tensor(4.5874e+09, device='cuda:0')
c= tensor(4.5875e+09, device='cuda:0')
c= tensor(4.6007e+09, device='cuda:0')
c= tensor(4.6072e+09, device='cuda:0')
c= tensor(4.6072e+09, device='cuda:0')
c= tensor(4.6078e+09, device='cuda:0')
c= tensor(4.6082e+09, device='cuda:0')
c= tensor(4.6098e+09, device='cuda:0')
c= tensor(4.6114e+09, device='cuda:0')
c= tensor(4.6155e+09, device='cuda:0')
c= tensor(4.6156e+09, device='cuda:0')
c= tensor(4.6158e+09, device='cuda:0')
c= tensor(4.6158e+09, device='cuda:0')
c= tensor(4.6163e+09, device='cuda:0')
c= tensor(4.6166e+09, device='cuda:0')
c= tensor(4.6175e+09, device='cuda:0')
c= tensor(4.6298e+09, device='cuda:0')
c= tensor(4.6563e+09, device='cuda:0')
c= tensor(4.6563e+09, device='cuda:0')
c= tensor(4.6564e+09, device='cuda:0')
c= tensor(4.6564e+09, device='cuda:0')
c= tensor(4.6570e+09, device='cuda:0')
c= tensor(4.6574e+09, device='cuda:0')
c= tensor(4.6574e+09, device='cuda:0')
c= tensor(4.6574e+09, device='cuda:0')
c= tensor(4.6683e+09, device='cuda:0')
c= tensor(4.6683e+09, device='cuda:0')
c= tensor(4.6719e+09, device='cuda:0')
c= tensor(4.6719e+09, device='cuda:0')
c= tensor(4.6720e+09, device='cuda:0')
c= tensor(4.6723e+09, device='cuda:0')
c= tensor(4.6723e+09, device='cuda:0')
c= tensor(4.6723e+09, device='cuda:0')
c= tensor(4.6753e+09, device='cuda:0')
c= tensor(4.7159e+09, device='cuda:0')
c= tensor(4.7280e+09, device='cuda:0')
c= tensor(4.7337e+09, device='cuda:0')
c= tensor(4.7341e+09, device='cuda:0')
c= tensor(4.7342e+09, device='cuda:0')
c= tensor(4.7344e+09, device='cuda:0')
c= tensor(4.7415e+09, device='cuda:0')
c= tensor(4.7417e+09, device='cuda:0')
c= tensor(4.7427e+09, device='cuda:0')
c= tensor(4.7428e+09, device='cuda:0')
c= tensor(4.9183e+09, device='cuda:0')
c= tensor(4.9190e+09, device='cuda:0')
c= tensor(4.9201e+09, device='cuda:0')
c= tensor(4.9675e+09, device='cuda:0')
c= tensor(4.9679e+09, device='cuda:0')
c= tensor(4.9685e+09, device='cuda:0')
c= tensor(4.9887e+09, device='cuda:0')
c= tensor(4.9941e+09, device='cuda:0')
c= tensor(4.9947e+09, device='cuda:0')
c= tensor(4.9949e+09, device='cuda:0')
c= tensor(4.9955e+09, device='cuda:0')
c= tensor(4.9956e+09, device='cuda:0')
c= tensor(4.9990e+09, device='cuda:0')
c= tensor(5.1402e+09, device='cuda:0')
c= tensor(5.1627e+09, device='cuda:0')
c= tensor(5.1711e+09, device='cuda:0')
c= tensor(5.1711e+09, device='cuda:0')
c= tensor(5.1751e+09, device='cuda:0')
c= tensor(5.1787e+09, device='cuda:0')
c= tensor(5.1788e+09, device='cuda:0')
c= tensor(5.1792e+09, device='cuda:0')
c= tensor(5.2102e+09, device='cuda:0')
c= tensor(5.2112e+09, device='cuda:0')
c= tensor(5.2445e+09, device='cuda:0')
c= tensor(5.2479e+09, device='cuda:0')
c= tensor(5.2494e+09, device='cuda:0')
c= tensor(5.2494e+09, device='cuda:0')
c= tensor(5.2555e+09, device='cuda:0')
c= tensor(5.2614e+09, device='cuda:0')
c= tensor(5.2614e+09, device='cuda:0')
c= tensor(5.3501e+09, device='cuda:0')
c= tensor(5.3530e+09, device='cuda:0')
c= tensor(5.3570e+09, device='cuda:0')
c= tensor(5.3571e+09, device='cuda:0')
c= tensor(5.3573e+09, device='cuda:0')
c= tensor(5.3573e+09, device='cuda:0')
c= tensor(5.3573e+09, device='cuda:0')
c= tensor(5.3582e+09, device='cuda:0')
c= tensor(5.3642e+09, device='cuda:0')
c= tensor(7.1864e+09, device='cuda:0')
c= tensor(7.1864e+09, device='cuda:0')
c= tensor(7.1928e+09, device='cuda:0')
c= tensor(7.1929e+09, device='cuda:0')
c= tensor(7.1930e+09, device='cuda:0')
c= tensor(7.1933e+09, device='cuda:0')
c= tensor(7.2322e+09, device='cuda:0')
c= tensor(7.2323e+09, device='cuda:0')
c= tensor(7.4725e+09, device='cuda:0')
c= tensor(7.4725e+09, device='cuda:0')
c= tensor(7.4829e+09, device='cuda:0')
c= tensor(7.4864e+09, device='cuda:0')
c= tensor(7.4906e+09, device='cuda:0')
c= tensor(7.5104e+09, device='cuda:0')
c= tensor(7.5105e+09, device='cuda:0')
c= tensor(7.5106e+09, device='cuda:0')
c= tensor(7.5123e+09, device='cuda:0')
c= tensor(7.5124e+09, device='cuda:0')
c= tensor(7.5127e+09, device='cuda:0')
c= tensor(7.5167e+09, device='cuda:0')
c= tensor(7.5228e+09, device='cuda:0')
c= tensor(7.5247e+09, device='cuda:0')
c= tensor(7.5253e+09, device='cuda:0')
c= tensor(7.5563e+09, device='cuda:0')
c= tensor(7.6210e+09, device='cuda:0')
c= tensor(7.6221e+09, device='cuda:0')
c= tensor(7.6221e+09, device='cuda:0')
c= tensor(7.6403e+09, device='cuda:0')
c= tensor(7.6409e+09, device='cuda:0')
c= tensor(7.6421e+09, device='cuda:0')
c= tensor(7.6457e+09, device='cuda:0')
c= tensor(7.6494e+09, device='cuda:0')
c= tensor(7.6546e+09, device='cuda:0')
c= tensor(7.7021e+09, device='cuda:0')
c= tensor(7.7028e+09, device='cuda:0')
c= tensor(7.7029e+09, device='cuda:0')
c= tensor(7.7029e+09, device='cuda:0')
c= tensor(7.7034e+09, device='cuda:0')
c= tensor(7.7063e+09, device='cuda:0')
c= tensor(7.7145e+09, device='cuda:0')
c= tensor(7.7232e+09, device='cuda:0')
c= tensor(7.7232e+09, device='cuda:0')
c= tensor(7.7234e+09, device='cuda:0')
c= tensor(7.7267e+09, device='cuda:0')
c= tensor(7.7380e+09, device='cuda:0')
c= tensor(7.7398e+09, device='cuda:0')
c= tensor(7.7428e+09, device='cuda:0')
c= tensor(7.8473e+09, device='cuda:0')
c= tensor(7.8480e+09, device='cuda:0')
c= tensor(7.8480e+09, device='cuda:0')
c= tensor(7.8480e+09, device='cuda:0')
c= tensor(7.8611e+09, device='cuda:0')
c= tensor(7.8628e+09, device='cuda:0')
c= tensor(7.8643e+09, device='cuda:0')
c= tensor(7.8643e+09, device='cuda:0')
c= tensor(7.8643e+09, device='cuda:0')
c= tensor(7.8700e+09, device='cuda:0')
c= tensor(7.8734e+09, device='cuda:0')
c= tensor(7.8747e+09, device='cuda:0')
c= tensor(7.8748e+09, device='cuda:0')
c= tensor(7.8748e+09, device='cuda:0')
c= tensor(7.8758e+09, device='cuda:0')
c= tensor(7.8765e+09, device='cuda:0')
c= tensor(7.8857e+09, device='cuda:0')
c= tensor(7.8857e+09, device='cuda:0')
c= tensor(7.9039e+09, device='cuda:0')
c= tensor(7.9043e+09, device='cuda:0')
c= tensor(7.9048e+09, device='cuda:0')
c= tensor(7.9541e+09, device='cuda:0')
c= tensor(7.9544e+09, device='cuda:0')
c= tensor(7.9552e+09, device='cuda:0')
c= tensor(7.9610e+09, device='cuda:0')
c= tensor(7.9670e+09, device='cuda:0')
c= tensor(7.9729e+09, device='cuda:0')
c= tensor(7.9816e+09, device='cuda:0')
c= tensor(7.9816e+09, device='cuda:0')
c= tensor(7.9823e+09, device='cuda:0')
c= tensor(7.9825e+09, device='cuda:0')
c= tensor(7.9861e+09, device='cuda:0')
c= tensor(7.9905e+09, device='cuda:0')
c= tensor(7.9915e+09, device='cuda:0')
c= tensor(7.9946e+09, device='cuda:0')
c= tensor(7.9947e+09, device='cuda:0')
c= tensor(7.9967e+09, device='cuda:0')
c= tensor(7.9967e+09, device='cuda:0')
c= tensor(7.9971e+09, device='cuda:0')
c= tensor(8.0042e+09, device='cuda:0')
c= tensor(8.0470e+09, device='cuda:0')
c= tensor(8.0472e+09, device='cuda:0')
c= tensor(8.0476e+09, device='cuda:0')
c= tensor(8.0477e+09, device='cuda:0')
c= tensor(8.0714e+09, device='cuda:0')
c= tensor(8.0719e+09, device='cuda:0')
c= tensor(8.0720e+09, device='cuda:0')
c= tensor(8.0734e+09, device='cuda:0')
c= tensor(8.0762e+09, device='cuda:0')
c= tensor(8.0763e+09, device='cuda:0')
c= tensor(8.0785e+09, device='cuda:0')
c= tensor(8.0786e+09, device='cuda:0')
time to make c is 8.402135610580444
time for making loss is 8.402376651763916
p0 True
it  0 : 2665465344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5568147456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5568352256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1923455000.0
relative error loss 0.23809306
shape of L is 
torch.Size([])
memory (bytes)
5594189824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5594406912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1908523000.0
relative error loss 0.23624472
shape of L is 
torch.Size([])
memory (bytes)
5597868032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5597978624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1825760800.0
relative error loss 0.22600007
shape of L is 
torch.Size([])
memory (bytes)
5600997376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5600997376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1801345000.0
relative error loss 0.22297779
shape of L is 
torch.Size([])
memory (bytes)
5604270080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5604380672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1781605900.0
relative error loss 0.2205344
shape of L is 
torch.Size([])
memory (bytes)
5607550976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5607550976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1765594600.0
relative error loss 0.21855247
shape of L is 
torch.Size([])
memory (bytes)
5610598400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5610598400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1751823400.0
relative error loss 0.2168478
shape of L is 
torch.Size([])
memory (bytes)
5613891584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5613989888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1743691800.0
relative error loss 0.21584123
shape of L is 
torch.Size([])
memory (bytes)
5617098752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5617197056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1731221000.0
relative error loss 0.21429756
shape of L is 
torch.Size([])
memory (bytes)
5620416512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5620416512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1724185100.0
relative error loss 0.21342662
time to take a step is 307.8710284233093
it  1 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5623615488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5623619584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1724185100.0
relative error loss 0.21342662
shape of L is 
torch.Size([])
memory (bytes)
5626626048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5626851328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1716981800.0
relative error loss 0.21253496
shape of L is 
torch.Size([])
memory (bytes)
5629960192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5630054400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1710962700.0
relative error loss 0.2117899
shape of L is 
torch.Size([])
memory (bytes)
5633040384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 12% |
memory (bytes)
5633265664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  1706030600.0
relative error loss 0.21117939
shape of L is 
torch.Size([])
memory (bytes)
5636464640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5636464640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1699339300.0
relative error loss 0.21035111
shape of L is 
torch.Size([])
memory (bytes)
5639663616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5639671808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1694257200.0
relative error loss 0.20972203
shape of L is 
torch.Size([])
memory (bytes)
5642706944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5642883072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1691278800.0
relative error loss 0.20935336
shape of L is 
torch.Size([])
memory (bytes)
5645987840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 12% |
memory (bytes)
5646082048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1688502800.0
relative error loss 0.20900972
shape of L is 
torch.Size([])
memory (bytes)
5649244160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5649244160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1684538400.0
relative error loss 0.208519
shape of L is 
torch.Size([])
memory (bytes)
5652418560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5652512768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1681247700.0
relative error loss 0.20811166
time to take a step is 305.3475708961487
it  2 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5655609344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5655703552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1681247700.0
relative error loss 0.20811166
shape of L is 
torch.Size([])
memory (bytes)
5658923008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5658923008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1677521900.0
relative error loss 0.20765047
shape of L is 
torch.Size([])
memory (bytes)
5662121984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5662121984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1675763200.0
relative error loss 0.20743276
shape of L is 
torch.Size([])
memory (bytes)
5665255424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5665337344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1673469400.0
relative error loss 0.20714884
shape of L is 
torch.Size([])
memory (bytes)
5668352000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5668544512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1672163800.0
relative error loss 0.20698722
shape of L is 
torch.Size([])
memory (bytes)
5671661568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5671755776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1669639700.0
relative error loss 0.20667477
shape of L is 
torch.Size([])
memory (bytes)
5674967040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5674967040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1668030000.0
relative error loss 0.20647551
shape of L is 
torch.Size([])
memory (bytes)
5678190592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5678190592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1666614800.0
relative error loss 0.20630033
shape of L is 
torch.Size([])
memory (bytes)
5681385472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5681385472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1664678400.0
relative error loss 0.20606065
shape of L is 
torch.Size([])
memory (bytes)
5684518912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5684613120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1662908400.0
relative error loss 0.20584156
time to take a step is 323.636572599411
it  3 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5687726080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5687726080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1662908400.0
relative error loss 0.20584156
shape of L is 
torch.Size([])
memory (bytes)
5690941440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5690941440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1661706800.0
relative error loss 0.2056928
shape of L is 
torch.Size([])
memory (bytes)
5694148608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5694242816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1660066800.0
relative error loss 0.2054898
shape of L is 
torch.Size([])
memory (bytes)
5697441792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5697441792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1658533900.0
relative error loss 0.20530005
shape of L is 
torch.Size([])
memory (bytes)
5700558848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5700558848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1657312800.0
relative error loss 0.20514889
shape of L is 
torch.Size([])
memory (bytes)
5703831552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5703831552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1656524800.0
relative error loss 0.20505136
shape of L is 
torch.Size([])
memory (bytes)
5706973184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5706973184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1655563800.0
relative error loss 0.2049324
shape of L is 
torch.Size([])
memory (bytes)
5710188544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5710282752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1654728200.0
relative error loss 0.20482896
shape of L is 
torch.Size([])
memory (bytes)
5713395712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5713489920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1653873700.0
relative error loss 0.2047232
shape of L is 
torch.Size([])
memory (bytes)
5716557824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5716701184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1653315100.0
relative error loss 0.20465405
time to take a step is 306.4559361934662
it  4 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5719883776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5719883776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1653315100.0
relative error loss 0.20465405
shape of L is 
torch.Size([])
memory (bytes)
5723115520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5723115520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1652596700.0
relative error loss 0.20456512
shape of L is 
torch.Size([])
memory (bytes)
5726187520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5726330880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1651896800.0
relative error loss 0.20447849
shape of L is 
torch.Size([])
memory (bytes)
5729452032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5729546240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1651241500.0
relative error loss 0.20439737
shape of L is 
torch.Size([])
memory (bytes)
5732712448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5732712448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1650858500.0
relative error loss 0.20434996
shape of L is 
torch.Size([])
memory (bytes)
5735870464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5735952384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1650235900.0
relative error loss 0.2042729
shape of L is 
torch.Size([])
memory (bytes)
5739061248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5739159552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1649487900.0
relative error loss 0.2041803
shape of L is 
torch.Size([])
memory (bytes)
5742284800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5742387200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1649263100.0
relative error loss 0.20415248
shape of L is 
torch.Size([])
memory (bytes)
5745496064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5745598464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1648623100.0
relative error loss 0.20407325
shape of L is 
torch.Size([])
memory (bytes)
5748797440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5748801536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1648330800.0
relative error loss 0.20403707
time to take a step is 307.2392327785492
it  5 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5751885824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5751885824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1648330800.0
relative error loss 0.20403707
shape of L is 
torch.Size([])
memory (bytes)
5755174912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5755174912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1647703000.0
relative error loss 0.20395936
shape of L is 
torch.Size([])
memory (bytes)
5758341120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5758435328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1646872600.0
relative error loss 0.20385657
shape of L is 
torch.Size([])
memory (bytes)
5761552384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5761646592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1646359600.0
relative error loss 0.20379306
shape of L is 
torch.Size([])
memory (bytes)
5764849664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5764849664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 12% |
error is  1645354500.0
relative error loss 0.20366865
shape of L is 
torch.Size([])
memory (bytes)
5767966720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5767966720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1644671000.0
relative error loss 0.20358405
shape of L is 
torch.Size([])
memory (bytes)
5771239424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5771268096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1643718100.0
relative error loss 0.2034661
shape of L is 
torch.Size([])
memory (bytes)
5774397440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5774487552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1642978300.0
relative error loss 0.20337452
shape of L is 
torch.Size([])
memory (bytes)
5777690624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5777690624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1641956900.0
relative error loss 0.20324808
shape of L is 
torch.Size([])
memory (bytes)
5780893696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5780893696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1641619500.0
relative error loss 0.20320632
time to take a step is 305.4318561553955
it  6 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5784018944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5784113152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1641619500.0
relative error loss 0.20320632
shape of L is 
torch.Size([])
memory (bytes)
5787226112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5787320320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1641053700.0
relative error loss 0.20313628
shape of L is 
torch.Size([])
memory (bytes)
5790425088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5790523392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1640834600.0
relative error loss 0.20310916
shape of L is 
torch.Size([])
memory (bytes)
5793734656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5793738752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1640177700.0
relative error loss 0.20302784
shape of L is 
torch.Size([])
memory (bytes)
5796921344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5796921344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1639755300.0
relative error loss 0.20297556
shape of L is 
torch.Size([])
memory (bytes)
5800067072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5800161280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1639319600.0
relative error loss 0.20292163
shape of L is 
torch.Size([])
memory (bytes)
5803319296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5803319296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 12% |
error is  1638601700.0
relative error loss 0.20283277
shape of L is 
torch.Size([])
memory (bytes)
5806575616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5806575616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1639088100.0
relative error loss 0.20289297
shape of L is 
torch.Size([])
memory (bytes)
5809774592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5809774592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1638275600.0
relative error loss 0.20279239
shape of L is 
torch.Size([])
memory (bytes)
5812973568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5812973568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1637945300.0
relative error loss 0.20275152
time to take a step is 312.82852482795715
it  7 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5816205312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5816205312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1637945300.0
relative error loss 0.20275152
shape of L is 
torch.Size([])
memory (bytes)
5819314176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5819408384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1637556700.0
relative error loss 0.20270342
shape of L is 
torch.Size([])
memory (bytes)
5822513152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5822607360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1637071900.0
relative error loss 0.2026434
shape of L is 
torch.Size([])
memory (bytes)
5825810432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5825810432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1636647400.0
relative error loss 0.20259085
shape of L is 
torch.Size([])
memory (bytes)
5828927488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 12% |
memory (bytes)
5829021696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1636332000.0
relative error loss 0.20255181
shape of L is 
torch.Size([])
memory (bytes)
5832232960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5832232960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1635988500.0
relative error loss 0.20250928
shape of L is 
torch.Size([])
memory (bytes)
5835362304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5835362304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1635369500.0
relative error loss 0.20243266
shape of L is 
torch.Size([])
memory (bytes)
5838647296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5838647296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1634944500.0
relative error loss 0.20238006
shape of L is 
torch.Size([])
memory (bytes)
5841719296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5841854464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1634592300.0
relative error loss 0.20233646
shape of L is 
torch.Size([])
memory (bytes)
5845020672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5845020672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1634326500.0
relative error loss 0.20230357
time to take a step is 306.81956028938293
it  8 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5848264704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5848264704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1634326500.0
relative error loss 0.20230357
shape of L is 
torch.Size([])
memory (bytes)
5851328512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5851467776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1633805300.0
relative error loss 0.20223905
shape of L is 
torch.Size([])
memory (bytes)
5854584832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5854679040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1634213900.0
relative error loss 0.20228963
shape of L is 
torch.Size([])
memory (bytes)
5857787904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5857882112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1633589800.0
relative error loss 0.20221236
shape of L is 
torch.Size([])
memory (bytes)
5861097472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5861097472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1633303000.0
relative error loss 0.20217687
shape of L is 
torch.Size([])
memory (bytes)
5864202240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5864202240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1632881700.0
relative error loss 0.20212471
shape of L is 
torch.Size([])
memory (bytes)
5867413504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5867413504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1632647200.0
relative error loss 0.20209569
shape of L is 
torch.Size([])
memory (bytes)
5870694400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5870694400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1632351200.0
relative error loss 0.20205906
shape of L is 
torch.Size([])
memory (bytes)
5873831936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5873831936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1632112100.0
relative error loss 0.20202947
shape of L is 
torch.Size([])
memory (bytes)
5877129216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5877129216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1631938600.0
relative error loss 0.20200798
time to take a step is 253.89498686790466
it  9 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5880250368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5880250368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1631938600.0
relative error loss 0.20200798
shape of L is 
torch.Size([])
memory (bytes)
5883555840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5883555840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1631705100.0
relative error loss 0.20197907
shape of L is 
torch.Size([])
memory (bytes)
5886726144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5886726144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1631268400.0
relative error loss 0.20192501
shape of L is 
torch.Size([])
memory (bytes)
5889961984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5889961984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1631098900.0
relative error loss 0.20190403
shape of L is 
torch.Size([])
memory (bytes)
5893079040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5893173248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1630884400.0
relative error loss 0.20187747
shape of L is 
torch.Size([])
memory (bytes)
5896347648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5896347648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1630526000.0
relative error loss 0.20183311
shape of L is 
torch.Size([])
memory (bytes)
5899489280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5899583488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1630594000.0
relative error loss 0.20184155
shape of L is 
torch.Size([])
memory (bytes)
5902737408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5902737408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1630304300.0
relative error loss 0.20180568
shape of L is 
torch.Size([])
memory (bytes)
5906001920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 12% |
memory (bytes)
5906006016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1629991400.0
relative error loss 0.20176695
shape of L is 
torch.Size([])
memory (bytes)
5909217280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 12% |
memory (bytes)
5909217280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1629763100.0
relative error loss 0.20173869
time to take a step is 241.5448658466339
it  10 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5912436736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5912436736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1629763100.0
relative error loss 0.20173869
shape of L is 
torch.Size([])
memory (bytes)
5915590656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5915590656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1629419000.0
relative error loss 0.2016961
shape of L is 
torch.Size([])
memory (bytes)
5918724096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5918871552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1629151700.0
relative error loss 0.20166302
shape of L is 
torch.Size([])
memory (bytes)
5921976320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5922074624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1628992000.0
relative error loss 0.20164324
shape of L is 
torch.Size([])
memory (bytes)
5925277696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5925289984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1628785200.0
relative error loss 0.20161763
shape of L is 
torch.Size([])
memory (bytes)
5928484864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5928484864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1628636200.0
relative error loss 0.2015992
shape of L is 
torch.Size([])
memory (bytes)
5931630592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5931630592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 12% |
error is  1628473900.0
relative error loss 0.2015791
shape of L is 
torch.Size([])
memory (bytes)
5934891008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 12% |
memory (bytes)
5934891008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1628344800.0
relative error loss 0.20156313
shape of L is 
torch.Size([])
memory (bytes)
5938003968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5938003968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1628252200.0
relative error loss 0.20155166
shape of L is 
torch.Size([])
memory (bytes)
5941309440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5941309440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 12% |
error is  1628130800.0
relative error loss 0.20153664
time to take a step is 241.3532736301422
it  11 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5944430592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5944430592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1628130800.0
relative error loss 0.20153664
shape of L is 
torch.Size([])
memory (bytes)
5947723776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5947723776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1627989500.0
relative error loss 0.20151915
shape of L is 
torch.Size([])
memory (bytes)
5950840832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5950935040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1627827200.0
relative error loss 0.20149906
shape of L is 
torch.Size([])
memory (bytes)
5954154496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5954154496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1627722800.0
relative error loss 0.20148613
shape of L is 
torch.Size([])
memory (bytes)
5957259264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5957357568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1627557400.0
relative error loss 0.20146565
shape of L is 
torch.Size([])
memory (bytes)
5960445952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 13% |
memory (bytes)
5960572928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1627406800.0
relative error loss 0.20144702
shape of L is 
torch.Size([])
memory (bytes)
5963718656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 13% |
memory (bytes)
5963718656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1627276800.0
relative error loss 0.20143092
shape of L is 
torch.Size([])
memory (bytes)
5966823424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5967003648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1627083300.0
relative error loss 0.20140697
shape of L is 
torch.Size([])
memory (bytes)
5970108416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5970202624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1626916400.0
relative error loss 0.2013863
shape of L is 
torch.Size([])
memory (bytes)
5973364736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5973364736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1626792400.0
relative error loss 0.20137097
time to take a step is 241.96913814544678
it  12 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5976621056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5976621056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1626792400.0
relative error loss 0.20137097
shape of L is 
torch.Size([])
memory (bytes)
5979729920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5979729920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1626492400.0
relative error loss 0.20133384
shape of L is 
torch.Size([])
memory (bytes)
5983031296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5983035392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1626200600.0
relative error loss 0.2012977
shape of L is 
torch.Size([])
memory (bytes)
5986144256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5986144256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1626054700.0
relative error loss 0.20127964
shape of L is 
torch.Size([])
memory (bytes)
5989359616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5989453824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1625631200.0
relative error loss 0.20122723
shape of L is 
torch.Size([])
memory (bytes)
5992566784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
5992656896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 13% |
error is  1625412600.0
relative error loss 0.20120017
shape of L is 
torch.Size([])
memory (bytes)
5995769856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5995769856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1625205800.0
relative error loss 0.20117456
shape of L is 
torch.Size([])
memory (bytes)
5999005696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5999099904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1625035300.0
relative error loss 0.20115346
shape of L is 
torch.Size([])
memory (bytes)
6002204672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
6002307072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1624903200.0
relative error loss 0.20113711
shape of L is 
torch.Size([])
memory (bytes)
6005460992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
6005534720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1624637400.0
relative error loss 0.20110421
time to take a step is 242.3711290359497
it  13 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
6008635392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
6008635392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1624637400.0
relative error loss 0.20110421
shape of L is 
torch.Size([])
memory (bytes)
6011928576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
6011932672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1624405500.0
relative error loss 0.20107551
shape of L is 
torch.Size([])
memory (bytes)
6015115264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
6015115264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 13% |
error is  1624259100.0
relative error loss 0.20105737
shape of L is 
torch.Size([])
memory (bytes)
6018371584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 13% |
memory (bytes)
6018371584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1624068100.0
relative error loss 0.20103374
shape of L is 
torch.Size([])
memory (bytes)
6021488640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
6021488640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1623882800.0
relative error loss 0.2010108
shape of L is 
torch.Size([])
memory (bytes)
6024802304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
6024802304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1623653900.0
relative error loss 0.20098247
shape of L is 
torch.Size([])
memory (bytes)
6027960320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
6028009472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1623383000.0
relative error loss 0.20094894
shape of L is 
torch.Size([])
memory (bytes)
6031110144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 13% |
memory (bytes)
6031224832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1623266300.0
relative error loss 0.20093448
shape of L is 
torch.Size([])
memory (bytes)
6034423808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
6034427904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1623135200.0
relative error loss 0.20091826
shape of L is 
torch.Size([])
memory (bytes)
6037434368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
6037643264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1622912500.0
relative error loss 0.20089069
time to take a step is 241.25780081748962
it  14 : 3245837312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
6040850432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
6040854528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1622912500.0
relative error loss 0.20089069
shape of L is 
torch.Size([])
memory (bytes)
6043967488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 13% |
memory (bytes)
6044061696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1622785500.0
relative error loss 0.20087497
shape of L is 
torch.Size([])
memory (bytes)
6047236096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
6047236096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1622626800.0
relative error loss 0.20085533
shape of L is 
torch.Size([])
memory (bytes)
6050377728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
6050471936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1622483500.0
relative error loss 0.20083758
shape of L is 
torch.Size([])
memory (bytes)
6053588992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
6053588992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1622352900.0
relative error loss 0.20082143
shape of L is 
torch.Size([])
memory (bytes)
6056898560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
6056898560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1622225400.0
relative error loss 0.20080563
shape of L is 
torch.Size([])
memory (bytes)
6060007424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
6060007424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1622110700.0
relative error loss 0.20079145
shape of L is 
torch.Size([])
memory (bytes)
6063312896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
6063316992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1622014500.0
relative error loss 0.20077953
shape of L is 
torch.Size([])
memory (bytes)
6066524160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
6066524160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1621928400.0
relative error loss 0.20076889
shape of L is 
torch.Size([])
memory (bytes)
6069575680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 13% |
memory (bytes)
6069747712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1621849100.0
relative error loss 0.20075905
time to take a step is 242.43834114074707
sum tnnu_Z after tensor(15736263., device='cuda:0')
shape of features
(5717,)
shape of features
(5717,)
number of orig particles 22867
number of new particles after remove low mass 21032
tnuZ shape should be parts x labs
torch.Size([22867, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1923322900.0
relative error without small mass is  0.2380767
nnu_Z shape should be number of particles by maxV
(22867, 702)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
shape of features
(22867,)
Thu Feb 2 14:53:10 EST 2023
