Tue Jan 31 23:05:43 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 65341256
numbers of Z: 28423
shape of features
(28423,)
shape of features
(28423,)
ZX	Vol	Parts	Cubes	Eps
Z	0.020008476673750077	28423	28.423	0.08895725436147013
X	0.019326191931089646	2251	2.251	0.20476556832164403
X	0.019675163844601558	31370	31.37	0.08559885319994913
X	0.01963649146587363	3809	3.809	0.17274983457613213
X	0.019234666252633564	23350	23.35	0.09374164925649509
X	0.01989619010499364	20026	20.026	0.09978346251823243
X	0.019303465962585833	65335	65.335	0.06660341018990974
X	0.019503716904222985	84970	84.97	0.0612280726397024
X	0.019277508476527767	69516	69.516	0.06521117757894826
X	0.019620346461675854	24721	24.721	0.09258633203700302
X	0.0195948217533125	25831	25.831	0.09120109078172203
X	0.019317346750468877	19975	19.975	0.09889026997216072
X	0.0199671330245477	142675	142.675	0.05191855659831582
X	0.019083162631353866	11476	11.476	0.11847315041940709
X	0.01974775669356152	482867	482.867	0.034453240886101494
X	0.01935859693554865	36713	36.713	0.08078883293232264
X	0.019382038569139858	69067	69.067	0.0654700923760875
X	0.01960131838087583	85523	85.523	0.06119756167939607
X	0.019483538209625565	39835	39.835	0.07878937740435094
X	0.019947293119932385	188607	188.607	0.04729066133661751
X	0.01997252637325414	142038	142.038	0.05200073533089812
X	0.019100529758258003	71579	71.579	0.06438033941648623
X	0.019739950391282096	259789	259.789	0.0423553654141326
X	0.01905583357038467	15218	15.218	0.10778461888716245
X	0.019971762490685045	73245	73.245	0.06484544423024288
X	0.019776672468814593	7445	7.445	0.13849353681433782
X	0.019592079341536738	97245	97.245	0.05862341088738248
X	0.019718838875530716	103361	103.361	0.05756716981094225
X	0.019608624055668496	15898	15.898	0.1072428159811354
X	0.019794795612131644	206370	206.37	0.04577568155445508
X	0.019955480198477454	1583624	1583.624	0.023270374323000987
X	0.01970240163843314	27459	27.459	0.08952510045110058
X	0.019985039921120995	1397453	1397.453	0.02427295234583223
X	0.01983436679286922	44440	44.44	0.07642136020827206
X	0.01933806009315735	19660	19.66	0.09945114722596549
X	0.01932846255605974	27998	27.998	0.08838056366543467
X	0.019983222229005282	404563	404.563	0.03669102004014721
X	0.019991870134169476	116017	116.017	0.055647278602135586
X	0.018715255650935205	2028	2.028	0.20975352646355175
X	0.01886608765971461	6875	6.875	0.14000269051035819
X	0.01898280415100521	4463	4.463	0.16202343182475956
X	0.019315642468273723	6020	6.02	0.14749282329309665
X	0.018609455375354956	5185	5.185	0.1531069719937217
X	0.018733827895995415	1867	1.867	0.2156887048952246
X	0.01871477557112554	7708	7.708	0.1344046535687685
X	0.018972090934272427	1645	1.645	0.22593478407957843
X	0.01913833493826625	2515	2.515	0.19669291674070474
X	0.01877232947021494	5648	5.648	0.14923630693494783
X	0.019062845484234356	7633	7.633	0.1356742684091794
X	0.018874592094401624	3188	3.188	0.18090604842275507
X	0.01987675246056199	24849	24.849	0.09282796668057473
X	0.019820004374890822	23152	23.152	0.09495221001650508
X	0.019192611538937535	2681	2.681	0.19272841646425723
X	0.019301653092988622	12778	12.778	0.1147388626642149
X	0.01929175325307467	5443	5.443	0.15246767862986788
X	0.01987963157970477	8083	8.083	0.1349828471233692
X	0.019331619470235446	9629	9.629	0.12615244835567868
X	0.019108011952425584	2994	2.994	0.18549031386777354
X	0.01927299244222836	8255	8.255	0.13266123148291584
X	0.019313599814097755	6130	6.13	0.14660009485012163
X	0.019213253200468145	4865	4.865	0.158066324147152
X	0.019822831976452973	6664	6.664	0.14381700395173894
X	0.01922363195523335	3101	3.101	0.18370087529380574
X	0.01996376337546509	11519	11.519	0.12011830958917166
X	0.01928638588765005	19468	19.468	0.099688066214527
X	0.019234137226717563	4794	4.794	0.15890036452653156
X	0.01891879158246227	4571	4.571	0.16055631448842042
X	0.019225840515710347	5607	5.607	0.1507943002934812
X	0.019595542276543983	10915	10.915	0.12153774720971712
X	0.01926100433540678	9229	9.229	0.1277933027329889
X	0.019239601220290004	5939	5.939	0.14796565249879232
X	0.018627990882927596	5628	5.628	0.14902895036164251
X	0.019335658852774943	6308	6.308	0.14526325233335663
X	0.01927768036748297	6815	6.815	0.14142612377611605
X	0.01923208165349486	5773	5.773	0.14935103808333017
X	0.019279863932159406	7093	7.093	0.13955904906398467
X	0.019838020969691224	13261	13.261	0.11436874530162182
X	0.018896815467900906	1620	1.62	0.226790331515419
X	0.019217598033497693	2116	2.116	0.2086385138794897
X	0.01914696533829177	5970	5.97	0.14747165019195432
X	0.01977629828975187	25854	25.854	0.0914546418203124
X	0.018996004458048582	5094	5.094	0.15507237585338376
X	0.019247175617806467	3422	3.422	0.17783981637097182
X	0.019278689178788576	7711	7.711	0.13572367152829815
X	0.019227227207096357	3415	3.415	0.17789974140069717
X	0.019009251230270197	3344	3.344	0.17847042063406707
X	0.019154369613545014	2555	2.555	0.19571569439634684
X	0.019336733572225207	4699	4.699	0.1602479771920224
X	0.01899972301700154	2396	2.396	0.19941300835877
X	0.01921602472957018	7558	7.558	0.13648519789745608
X	0.019161666166229517	5695	5.695	0.14984646903604776
X	0.01876504099250385	5866	5.866	0.14734514131021156
X	0.019228063402574658	3696	3.696	0.1732744494604364
X	0.019289354847798992	3753	3.753	0.17257572743374627
X	0.019149038071474476	7607	7.607	0.13603307504385032
X	0.019203315010895304	7191	7.191	0.1387380669453552
X	0.019300080666369894	7493	7.493	0.13707801194635844
X	0.019140228578068522	3091	3.091	0.18363242592761417
X	0.019201754843450144	10855	10.855	0.12094011138790228
X	0.019345223526538675	6517	6.517	0.14371717673018533
X	0.01956229014015318	6490	6.49	0.14445247912487258
X	0.019278298051191873	18112	18.112	0.10210196341428088
X	0.019860961392545846	6996	6.996	0.1415957295481621
X	0.01903932527398742	11330	11.33	0.11888869169240728
X	0.018664714250717238	3020	3.02	0.18351495750977956
X	0.019249713057446743	15677	15.677	0.10708297638127247
X	0.019764869413869283	5579	5.579	0.15244476996656967
X	0.019839075981635545	7974	7.974	0.1355028417047103
X	0.019208122231299297	2684	2.684	0.1927084683910777
X	0.019203417947391613	5028	5.028	0.1563127564885183
X	0.01937920220286134	2412	2.412	0.20028704706432365
X	0.019538709845260495	4347	4.347	0.1650322222750083
X	0.01916681499168864	2494	2.494	0.19734123142000692
X	0.019004665318552633	2878	2.878	0.18761021396557034
X	0.018832076486849893	5539	5.539	0.1503680662721029
X	0.01895277260913643	9213	9.213	0.127181485597128
X	0.019042740967641646	5373	5.373	0.15246525101224057
X	0.01930353633540136	4218	4.218	0.16602625330350412
X	0.019010119203331188	1794	1.794	0.2196447365243832
X	0.01965928245808814	11585	11.585	0.11927713752147484
X	0.019316810202099827	3062	3.062	0.18477508416246036
X	0.019621396714094744	16015	16.015	0.10700424125742815
X	0.01962138841380976	4419	4.419	0.16436218691383622
X	0.018753393086517404	3251	3.251	0.17934432099405515
X	0.019325294004940482	3570	3.57	0.1755844148241584
X	0.018916329354863183	4747	4.747	0.15854013064977665
X	0.019125049388972016	3222	3.222	0.1810613175908941
X	0.019064530502302845	2647	2.647	0.19311857925487388
X	0.018972782455959007	6872	6.872	0.14028652538363126
X	0.019227073410651028	17952	17.952	0.10231361939064683
X	0.01860275382246829	3445	3.445	0.17544029040614648
X	0.019036792095638716	13099	13.099	0.11327094964861513
X	0.019134943396215277	2418	2.418	0.1992769014628724
X	0.01975989638152524	5507	5.507	0.15309342133988543
X	0.01902873463720091	2783	2.783	0.18980117694948284
X	0.019110367626258024	8391	8.391	0.13156845460480163
X	0.019966947979387858	3206	3.206	0.18398512950147838
X	0.01922455979740238	2783	2.783	0.19045003900238275
X	0.019183456764223564	3131	3.131	0.18298462485491682
X	0.018812692263019695	4298	4.298	0.16357923773214544
X	0.018959961685800105	2696	2.696	0.1915898610733012
X	0.019506677707935106	7074	7.074	0.14022951038661038
X	0.019058349006249743	3051	3.051	0.18416807810479874
X	0.01978282922841771	20022	20.022	0.09960022396629363
X	0.019247445136961756	10673	10.673	0.12172007005856277
X	0.019870558110606444	8349	8.349	0.1335135063220946
X	0.019264344703171753	3164	3.164	0.1826021648080991
X	0.019251327407635976	3449	3.449	0.17738728850247937
X	0.01960709142539815	3902	3.902	0.17128078167444905
X	0.01913909222987472	3261	3.261	0.18038074604950957
X	0.019292851608596222	3960	3.96	0.1695249972489485
X	0.019164650761415918	4546	4.546	0.16154387369109713
X	0.019724408520852173	4694	4.694	0.16136907189330266
X	0.01928508662963227	7677	7.677	0.13593877473690222
X	0.0196039771468457	13956	13.956	0.11199391042919245
X	0.018935527951641124	4563	4.563	0.16069744923284082
X	0.019458262121908862	33236	33.236	0.08365625774172121
X	0.019021103220053837	6142	6.142	0.14576120952165522
X	0.01927278042230892	7287	7.287	0.13829247036444609
X	0.019096429327130918	5882	5.882	0.14807294585347655
X	0.01879504027571845	925	0.925	0.27287715342274144
X	0.0194828764603102	12443	12.443	0.11612061444086975
X	0.018774508646326002	2715	2.715	0.19051664074623983
X	0.01989028280556657	12443	12.443	0.11692443652340147
X	0.018518932067401305	1788	1.788	0.21797974559115943
X	0.01881677141913427	7752	7.752	0.13439314434238653
X	0.018706401136199022	3077	3.077	0.18251038543318593
X	0.019264350432342876	2961	2.961	0.18668323616372273
X	0.018888005816084387	7062	7.062	0.1388095878924625
X	0.018783339585127883	7455	7.455	0.1360739695279631
X	0.019114392833810074	1747	1.747	0.22200147881937848
X	0.018793199134627826	1866	1.866	0.2159548815005569
X	0.01847865116236859	3763	3.763	0.16997250810633938
X	0.019431871574179242	11259	11.259	0.11995129402669849
X	0.01909145134237498	2983	2.983	0.18566436811630235
X	0.01971103996936939	7875	7.875	0.13577495224352049
X	0.01927715128624948	20160	20.16	0.09851842101314225
X	0.019302728838454873	10415	10.415	0.12283434429174103
X	0.01878950816833476	3536	3.536	0.17450225840306455
X	0.019167174265403066	9432	9.432	0.12666348504132036
X	0.019213232332933323	5645	5.645	0.15042227847062842
X	0.019909021708356824	5767	5.767	0.15113555885309887
X	0.01919274918092486	4137	4.137	0.16678252925949663
X	0.01900156746845911	6668	6.668	0.1417744416665299
X	0.01910804622440564	5293	5.293	0.15340451068728916
X	0.019676678719658367	8079	8.079	0.13454412195665205
X	0.019297309860351206	7427	7.427	0.13747628323508862
X	0.019032109608740893	4559	4.559	0.161017265167386
X	0.01926974300267016	8235	8.235	0.13276107887685598
X	0.019587270077605613	11162	11.162	0.1206175856509101
X	0.01987817893874152	13094	13.094	0.11493035628956773
X	0.01921929557951786	4783	4.783	0.15898117228640543
X	0.01931925671801344	1032	1.032	0.2655238717135536
X	0.018967534101326042	5453	5.453	0.1515159867841969
X	0.019240776767389493	7166	7.166	0.13898948072226408
X	0.01927045917697867	8596	8.596	0.13087756876759035
X	0.01930290639733572	4935	4.935	0.15755971354985
X	0.01919727896096353	5172	5.172	0.1548318797908637
X	0.019192057488862352	2764	2.764	0.1907777982779753
X	0.01897315562565917	6317	6.317	0.1442811396371308
X	0.019333419130069553	2002	2.002	0.2129519057402599
X	0.019837725747761034	5709	5.709	0.15146441540417657
X	0.019116595140798488	3863	3.863	0.17041007914282563
X	0.019920230799746665	12442	12.442	0.1169862239011999
X	0.01947163642191245	4303	4.303	0.16540312171015795
X	0.019312834452961166	9135	9.135	0.1283450594177246
X	0.018936282273596675	2865	2.865	0.18766791579270553
X	0.019911536602167674	6400	6.4	0.14598501119173501
X	0.01987359426434943	6582	6.582	0.14453494018063215
X	0.01903176648755452	4716	4.716	0.15920930065709352
X	0.01969327240484624	12467	12.467	0.11646228576348773
X	0.01915485573295327	8583	8.583	0.13068124327770758
X	0.019234366496361448	9602	9.602	0.12605847873050396
X	0.019129594589845162	5679	5.679	0.14990333644111817
X	0.018968814430083785	6044	6.044	0.1464103944698338
X	0.018750211554885928	1682	1.682	0.22338811333540778
X	0.018733960338606324	2493	2.493	0.19587053338912389
X	0.019148475124774862	1716	1.716	0.2234630063056656
X	0.01878685246514598	2221	2.221	0.20375197264332498
X	0.01885753164928526	23116	23.116	0.09343811170110364
X	0.0192362304371242	4261	4.261	0.1652733432147565
X	0.019088543589902737	4559	4.559	0.16117625750242648
X	0.018833192784631803	2029	2.029	0.21015866532587335
X	0.01926788851963761	11043	11.043	0.12038776474091058
X	0.019290081864387083	6398	6.398	0.14446521169819126
X	0.019277845556700816	5281	5.281	0.15397401802624577
X	0.01982501332272029	6845	6.845	0.14254325719687624
X	0.01921637805258808	2813	2.813	0.18974365730856826
X	0.01913131622453637	2921	2.921	0.18709884123177922
X	0.019658355578056706	7795	7.795	0.13611636903601765
X	0.018611291430032643	1285	1.285	0.24375739163932092
X	0.018752713227688	4216	4.216	0.16445781025825298
X	0.01916936453549171	4668	4.668	0.1601372217664307
X	0.019954858384940717	4912	4.912	0.1595622300135244
X	0.019909863053391694	7721	7.721	0.1371297150154534
X	0.019832750597345364	5497	5.497	0.15337423321883423
X	0.018961526110643258	5719	5.719	0.1491137631801942
X	0.019251386691275837	6780	6.78	0.14160462597211373
X	0.019928726336830096	7909	7.909	0.13607739579456957
X	0.01894013946560901	4979	4.979	0.15610387439905726
X	0.019526149654083	31588	31.588	0.08518533351298273
X	0.019536642364482937	80191	80.191	0.06245608619514603
X	0.01977652239085461	8003	8.003	0.13519657912609523
X	0.01978196839820349	7556	7.556	0.13782431780062762
X	0.01918620273418365	1774	1.774	0.2211456857189387
X	0.019162141407209152	11550	11.55	0.1183824577511422
X	0.019859766885066733	15809	15.809	0.10790044827846036
X	0.019326237869688595	203524	203.524	0.04562230571014975
X	0.01927391781363012	6454	6.454	0.14400592038990873
X	0.01965535030995681	242372	242.372	0.043284537425021176
X	0.01936147021179117	82992	82.992	0.06156029616792381
X	0.019562293652637164	50645	50.645	0.07282733190416894
X	0.01996260990175777	193576	193.576	0.04689450286564949
X	0.0190707197897427	4145	4.145	0.16632116436822442
X	0.019557433003380094	7599	7.599	0.13704140490370323
X	0.019988296431062265	386043	386.043	0.03727177134965385
X	0.01950866037334784	632962	632.962	0.0313533008759432
X	0.019935958818701156	6980	6.98	0.14188197821832757
X	0.019450861644272736	56342	56.342	0.07015126849424011
X	0.019659704318066195	31714	31.714	0.08526588811734374
X	0.019943795906513878	36513	36.513	0.08174353873298004
X	0.019321247358881355	128539	128.539	0.05317001072211254
X	0.019165646073525163	76321	76.321	0.06308989490090014
X	0.01979828819817174	33868	33.868	0.08361406998368494
X	0.01937553401489977	26668	26.668	0.08989890699329454
X	0.019245454069556547	5107	5.107	0.15561594751847535
X	0.019340124407196815	202231	202.231	0.045730278332067026
X	0.01899108896456025	11443	11.443	0.11839589824372548
X	0.01916946537675596	5840	5.84	0.14861605127198516
X	0.019218522599278556	31048	31.048	0.08522404101193237
X	0.019002541998882592	104771	104.771	0.0566052705719814
X	0.019944631968893643	318173	318.173	0.03972411538143269
X	0.019533362959361984	117837	117.837	0.054932805656618025
X	0.01878341867236787	3556	3.556	0.17415567419096797
X	0.0190601004078115	52692	52.692	0.07125141611609971
X	0.019201685100050544	12365	12.365	0.11580173710506683
X	0.019278444166912607	48980	48.98	0.07328544574983943
X	0.01934829992143768	92261	92.261	0.05941209741034613
X	0.019883600766229876	26740	26.74	0.09059645631169565
X	0.019242202233398047	105091	105.091	0.05678449267240692
X	0.01922928209399432	1457	1.457	0.2363203781674602
X	0.019011470892366898	11734	11.734	0.11745095300558234
X	0.019397837888143817	124521	124.521	0.05380676830206712
X	0.01957854894506174	83321	83.321	0.061708076489038886
X	0.01910656294460129	118212	118.212	0.05447204351330459
X	0.018950211145760575	5576	5.576	0.15034780640575826
X	0.01935307325423822	332698	332.698	0.03874656884399535
X	0.019313018662683007	21837	21.837	0.09598846838461898
X	0.0193279194925968	41741	41.741	0.0773643271688492
X	0.019330842721871987	115891	115.891	0.05504700468485128
X	0.019253611317886006	4194	4.194	0.1661988156790073
X	0.01931021624741653	145753	145.753	0.0509788316880029
X	0.01996442060719575	315808	315.808	0.03983619606576706
X	0.01999364101064625	909784	909.784	0.028010306757669943
X	0.01963271514793483	48242	48.242	0.07410571246814207
X	0.019290826188662805	41209	41.209	0.07764608876379137
X	0.018838761336774777	11321	11.321	0.11850113715807434
X	0.01917414661293789	8826	8.826	0.12951419591054697
X	0.01968655607694205	212479	212.479	0.04524992997889243
X	0.019542866706186966	14020	14.02	0.11170692202581105
X	0.019294160943391113	41899	41.899	0.07722194683092859
X	0.019862589237314092	33511	33.511	0.0840006844039389
X	0.019455631197131437	35751	35.751	0.08164301674155611
X	0.01921148843149737	24655	24.655	0.09202065825219963
X	0.019335141950760464	7478	7.478	0.1372526172415694
X	0.019893621787369576	204520	204.52	0.04598957091920441
X	0.019301832030957505	60727	60.727	0.06824521951310789
X	0.019298752454919965	28513	28.513	0.08780018691457873
X	0.01984023906185429	10963	10.963	0.12186317685376419
X	0.019508956970273213	107203	107.203	0.056668585270338426
X	0.01943962651633678	24368	24.368	0.0927447518611057
X	0.019452575255744783	247012	247.012	0.042863373708782286
X	0.018988237080363422	12021	12.021	0.11646122380017503
X	0.019400685460976605	50917	50.917	0.0724966751020386
X	0.019934538337947585	15522	15.522	0.10869748635419786
X	0.019921956544521367	242429	242.429	0.043475955890695826
X	0.019272551829226173	56570	56.57	0.06984216303991407
X	0.019314671509817055	15838	15.838	0.10683879864172258
X	0.019490215290024742	173856	173.856	0.048217976875473974
X	0.019954539494851327	237033	237.033	0.04382725784396556
X	0.01928272106874465	16114	16.114	0.10616670039482506
X	0.019392543034630227	178657	178.657	0.04770218169426974
X	0.01999384787763548	202504	202.504	0.04621904054597122
X	0.019865466450369684	521546	521.546	0.033646150458899016
X	0.019333193358998302	26399	26.399	0.09013747182869924
X	0.01896919344444216	4257	4.257	0.16455651140347985
X	0.019318963073821378	18283	18.283	0.10185416302783869
X	0.019356597378467337	14303	14.303	0.11061158766700725
X	0.01915511488088231	33155	33.155	0.08328726027248681
X	0.019630311655581704	58285	58.285	0.06957557399642347
X	0.018980559191487296	3347	3.347	0.17832727105075824
X	0.019996188489237447	101209	101.209	0.05824286043946561
X	0.019980342944194162	234864	234.864	0.0439807024661433
X	0.01894578650674283	19389	19.389	0.09923215192886048
X	0.019197508447164814	10937	10.937	0.12062821047187078
X	0.01983631544498322	59294	59.294	0.06941980972856895
X	0.01994163132077715	17117	17.117	0.10522308018533451
X	0.019948079594600786	26079	26.079	0.09145414334185052
X	0.019565626316055323	18610	18.61	0.10168318174854411
X	0.019130560934592058	10383	10.383	0.12259373691038468
X	0.019345090646260346	76789	76.789	0.06315735287222317
X	0.019299265322043135	53644	53.644	0.07112242254182984
X	0.019711038702120882	27554	27.554	0.08943515943375258
X	0.019972563941377988	67299	67.299	0.06670244238394449
X	0.01933230780395264	12437	12.437	0.1158393260922847
X	0.0199186970270492	1168774	1168.774	0.025734167771240005
X	0.019324418360092924	15825	15.825	0.10688601953687192
X	0.019714329559494367	136378	136.378	0.052482264422836475
X	0.019221699469639468	2968	2.968	0.18639859252844967
X	0.019244791536541104	6651	6.651	0.14249799523521997
X	0.019108851439446368	5835	5.835	0.14850163704086636
X	0.019821038384971505	15771	15.771	0.10791679950198128
X	0.01924705888005572	8208	8.208	0.13285431842708537
X	0.01975540390677341	122563	122.563	0.05442212684506285
X	0.01929526442734022	6300	6.3	0.1452234428224187
X	0.01944577401726982	6684	6.684	0.14275662251184956
X	0.019968254700551857	343805	343.805	0.0387265943464302
X	0.01959140810273283	77698	77.698	0.06317597267394523
X	0.019850728746995436	51692	51.692	0.07268604518924351
X	0.019990284513924955	168851	168.851	0.049102670548891794
X	0.020006280639993927	451100	451.1	0.03539684139168675
X	0.018905621722663454	7224	7.224	0.13780693562978757
X	0.019227300949325957	10608	10.608	0.12192561016433576
X	0.019153222783290492	35067	35.067	0.08174246856579338
X	0.019189783767281584	4052	4.052	0.1679320339706099
X	0.01927835799381442	9516	9.516	0.12653340716893693
X	0.019399755807808765	20434	20.434	0.09828357790265556
X	0.018990928295787537	7494	7.494	0.13633609013478387
X	0.019567188595492623	10310	10.31	0.12381024658164994
X	0.018997857251354326	9949	9.949	0.12406284316871106
X	0.01928166348912347	12752	12.752	0.11477713986695909
X	0.01999920698213828	475310	475.31	0.03478125709154277
X	0.018917059424143835	21154	21.154	0.09634304668634568
X	0.019926027117548654	113586	113.586	0.05597988093031343
X	0.019757241298599043	9575	9.575	0.12730999676867705
X	0.019517161568973946	6313	6.313	0.14567788334843979
X	0.019081778871711565	102555	102.555	0.05708920649155053
X	0.019728489540541908	328413	328.413	0.03916437184360091
X	0.01934316580578339	608517	608.517	0.03167755203626333
X	0.01938793210822604	27476	27.476	0.08902787230015247
X	0.019911418357758198	57562	57.562	0.07019757465767429
X	0.019246530793649207	4615	4.615	0.16096331877488734
X	0.01960344841530832	17769	17.769	0.1033292226733781
X	0.019964579877973117	1187694	1187.694	0.025616426045284336
X	0.01945994639296353	28471	28.471	0.08808723393691317
X	0.01885298173325858	11441	11.441	0.11811508143357144
X	0.019333452495422968	59868	59.868	0.06860749382778195
X	0.01993604530107762	846543	846.543	0.028663551270503265
X	0.019173903203427427	75363	75.363	0.06336519622664645
X	0.01876462753646161	18093	18.093	0.10122235916259044
X	0.019980090226439803	23132	23.132	0.09523459819945491
X	0.019517589037780687	15211	15.211	0.10866494578014682
X	0.01932551028524038	6109	6.109	0.14679804832292997
X	0.019976692840146144	230561	230.561	0.04424992934900978
X	0.019449447707891105	26250	26.25	0.09048834709512488
X	0.01915683943089344	3069	3.069	0.18412340181913397
X	0.019563837377963262	97293	97.293	0.0585855911408322
X	0.019991850636310118	24332	24.332	0.09366091514295018
X	0.018903363204400368	2480	2.48	0.19680183008388885
X	0.019973010335531914	100722	100.722	0.058314030811295936
X	0.019470543444043587	133335	133.335	0.052659650132673146
X	0.019987130752278058	278203	278.203	0.04157154974723035
X	0.0198779350965469	269500	269.5	0.041937657278086016
X	0.019823486570701574	312485	312.485	0.039882614616257854
X	0.01919974719471418	38331	38.331	0.07941730041666926
X	0.019693849250701466	42656	42.656	0.07728884676008123
X	0.019934913334033497	179996	179.996	0.04802313405650364
X	0.01996518497655147	289628	289.628	0.041002556053077524
X	0.01930911867542876	17832	17.832	0.10268826088466393
X	0.019898029185815087	128856	128.856	0.05364984512846727
X	0.019752632611310765	313952	313.952	0.03977289869652799
X	0.019968097913169686	249792	249.792	0.04307772149042602
X	0.019927747504518444	150601	150.601	0.050957735323623236
X	0.019500285374012594	61150	61.15	0.06832004890387237
X	0.019709508854657026	19447	19.447	0.100447946485686
X	0.01925503786868839	8926	8.926	0.12920991224548667
X	0.019836975784339267	32162	32.162	0.08512245460580879
X	0.01967630968681321	180451	180.451	0.047774349833451475
X	0.019360512394279432	156924	156.924	0.049782399513207336
X	0.019280394786614734	337381	337.381	0.03851812164479345
X	0.01995038339400985	167644	167.644	0.049187461582962264
X	0.019308335906738208	92674	92.674	0.05928283703773326
X	0.019307725366543203	23485	23.485	0.09367999295829676
X	0.019941338792350796	311115	311.115	0.04002006476667361
X	0.019294101763999556	8903	8.903	0.12940847831955896
X	0.019801218800236425	16459	16.459	0.10635620337566491
X	0.01993909964412179	210370	210.37	0.045593952830475244
X	0.019291693637979578	62542	62.542	0.0675667241965909
X	0.01919575076219272	8359	8.359	0.13193203431608638
X	0.019226536553257175	27132	27.132	0.08915377762994366
X	0.01978401790469508	614460	614.46	0.03181317100633901
X	0.01983667494938414	28040	28.04	0.08910394979853366
X	0.019985197809510632	157179	157.179	0.050284944669819025
X	0.019175777781599332	8010	8.01	0.13377454752896095
X	0.018963706355143814	8177	8.177	0.13236598872895686
X	0.019258993310984192	8068	8.068	0.13364597927777377
X	0.01982842333131507	10442	10.442	0.1238325542843711
X	0.019703929121459737	34419	34.419	0.08303315296646871
X	0.019435412015540096	96099	96.099	0.05869822398233918
X	0.019358036434988554	9577	9.577	0.12643789935486577
X	0.019327642114002914	188262	188.262	0.04682438360652844
X	0.019151790504027228	11142	11.142	0.11978857799319559
X	0.019799602423476715	90634	90.634	0.060226613082968464
X	0.018856522715508094	15997	15.997	0.10563496382845894
X	0.019248311942429482	150464	150.464	0.05038717559461494
X	0.019299840765068785	9395	9.395	0.1271214917100745
X	0.019989610838532502	30407	30.407	0.08695146862733098
X	0.019639262260683667	39774	39.774	0.07903909642719047
X	0.019832654986439227	36152	36.152	0.08186209273935781
X	0.019949674478835815	97289	97.289	0.058969035531789185
X	0.01997418707062521	608160	608.16	0.03202460292297838
X	0.01953396033079182	12631	12.631	0.11564251338460609
X	0.01922490923741063	9830	9.83	0.125055729344677
X	0.019571597412661436	72930	72.93	0.06450202561609794
X	0.01919430504429575	28847	28.847	0.08730194363318744
X	0.019997201506367378	648720	648.72	0.03135479592586282
X	0.01923835385188351	4633	4.633	0.1607318226204868
X	0.019657471713003437	164096	164.096	0.04929579650444787
X	0.019963928098398632	360864	360.864	0.03810372897301278
X	0.019223745807249913	7731	7.731	0.1354775852913271
X	0.01924132779122148	271656	271.656	0.04137505450123332
X	0.019296136625528437	27275	27.275	0.08910495777612207
X	0.019991777675537427	685816	685.816	0.030776171979230698
X	0.01943256931015458	19256	19.256	0.10030472295096604
X	0.01914614979979141	31400	31.4	0.08479767304156442
X	0.0187196152844933	12841	12.841	0.11338772046664358
X	0.019223479588525706	6306	6.306	0.14499710953703013
X	0.01936036043137255	11110	11.11	0.12033716979429583
X	0.019310391066764073	59321	59.321	0.06879035254439636
X	0.018933130318620314	9532	9.532	0.12570314112858225
X	0.019732621568660267	196676	196.676	0.04646698275990974
X	0.01933754565765127	24996	24.996	0.09180019675846476
X	0.019826210533941686	8314	8.314	0.133601058136837
X	0.01923855230056554	33086	33.086	0.0834659564312102
X	0.019957746726888437	266734	266.734	0.0421383678199764
X	0.01975314358750958	124902	124.902	0.05407820602698458
X	0.019988048928278095	375203	375.203	0.03762715368256162
X	0.01968396530355554	31723	31.723	0.0852928801626546
X	0.01921491659935211	10612	10.612	0.12188410917021765
X	0.019835765058953177	5609	5.609	0.15235422602036072
X	0.019184282135613944	7589	7.589	0.13622401196960363
X	0.019969157793628456	255304	255.304	0.042766204647987076
X	0.019919050610656634	13234	13.234	0.11460208040790379
X	0.01931809429968444	32040	32.04	0.08448065221844106
X	0.019230222814033873	50856	50.856	0.07231261039223288
X	0.019941205879612935	30901	30.901	0.08641576145429226
X	0.019098183409163763	13795	13.795	0.11145257793744759
X	0.019821733914084545	6317	6.317	0.146400846662596
X	0.019903510118952243	300869	300.869	0.040443687291863956
X	0.01984714099817976	61880	61.88	0.06845143389415242
X	0.01929534771318191	16681	16.681	0.10497281468134616
X	0.019333282021400215	21147	21.147	0.09705522937179357
X	0.019979973677362196	119224	119.224	0.05513284550332636
X	0.01993254675591993	133526	133.526	0.05304758908182993
X	0.019973763377624448	497548	497.548	0.034240621473265474
X	0.019789845014281496	154610	154.61	0.050396642525740575
X	0.019291375029409318	8832	8.832	0.12974820983644142
X	0.01928703704306527	50322	50.322	0.07263889133571806
X	0.019768941707277412	60645	60.645	0.0688223461278139
X	0.019138892400836147	13378	13.378	0.1126787108842862
X	0.018994284177970687	17264	17.264	0.10323504893707598
X	0.01952898889038381	9066	9.066	0.1291480979893526
X	0.01949156417579027	29602	29.602	0.0869978552149844
X	0.01987527491241032	37996	37.996	0.08057341205288156
X	0.018788009618426738	5278	5.278	0.15268760957566888
X	0.019195214218262376	21371	21.371	0.09648416431838422
X	0.019327199733561475	9211	9.211	0.1280228184851314
X	0.019142800698573836	40874	40.874	0.07765798405738758
X	0.019288057973738296	9578	9.578	0.12628096442670142
X	0.019932132876190424	34114	34.114	0.08360014734084119
X	0.019928252233596278	22792	22.792	0.09562297657901205
X	0.019811824243308265	24677	24.677	0.09294171924243172
X	0.01897016911488249	8826	8.826	0.12905329371590843
X	0.019031220601426306	18898	18.898	0.10023443182254857
X	0.01933501203061536	28668	28.668	0.08769651805687928
X	0.019311441048964366	170579	170.579	0.04837597222038446
X	0.019727323229468692	12577	12.577	0.11618864844987452
X	0.019199162365400896	3147	3.147	0.18272382569492332
X	0.01925045401532531	9606	9.606	0.12607610931781996
X	0.019998900741655473	155710	155.71	0.050454108879861945
X	0.019980210554700277	822413	822.413	0.02896254342599955
X	0.018991728613747264	79307	79.307	0.06209895967389339
X	0.019867440934484504	10834	10.834	0.1224008310314993
X	0.019642402950293066	219123	219.123	0.044754380539289006
X	0.0199609979810113	191422	191.422	0.04706847567002838
X	0.019094113612545878	4503	4.503	0.16185738458853266
X	0.01980285596382986	11600	11.6	0.11951523776449086
X	0.01946078743215307	45168	45.168	0.07552832429882265
X	0.019750196145270404	151089	151.089	0.050751185191181075
X	0.01984972541035318	447007	447.007	0.035411695931181424
X	0.019998755049656047	268607	268.607	0.042068969920940114
X	0.019475692980024262	29478	29.478	0.08709601840609445
X	0.019247429663048148	6297	6.297	0.1451263756088881
X	0.019225681766021536	4626	4.626	0.16077753787093169
X	0.019765669978803076	92093	92.093	0.05987262596635631
X	0.01925439916324212	18830	18.83	0.10074570731532669
X	0.019143242825891308	38989	38.989	0.07889045677162053
X	0.019962606826199207	208007	208.007	0.04578393337720477
X	0.01998544791063459	712213	712.213	0.03038794486930722
X	0.019114405772779734	15309	15.309	0.10768073040421923
X	0.019293621998180965	3453	3.453	0.17744852582003037
X	0.019875652985735216	35660	35.66	0.08229624548033572
X	0.01949371055807253	150501	150.501	0.050596255375247666
X	0.019176841726829392	56503	56.503	0.06975390501070441
X	0.018963044971671044	10673	10.673	0.12111758104251812
X	0.019098572096598897	6405	6.405	0.14393306980581286
X	0.01919047397889416	36120	36.12	0.08099272043532893
X	0.01931614143829809	4361	4.361	0.16422707708119777
X	0.019706969779384555	70716	70.716	0.06531817688097974
X	0.01988608741828785	9094	9.094	0.12979704285528362
X	0.019320865621602894	20908	20.908	0.09740278234799765
X	0.019754844317042355	13754	13.754	0.11282747364278697
X	0.019254870004322848	9726	9.726	0.1255650503774315
X	0.019037002072432814	7738	7.738	0.13499673885148744
X	0.019618952564235386	211710	211.71	0.04525273318907501
X	0.019987613586734926	466577	466.577	0.03499015552205094
X	0.019348721211098048	135338	135.338	0.05228906036196385
X	0.019268411426582453	123943	123.943	0.0537701567286852
X	0.019303210552755776	27731	27.731	0.08862467842072955
X	0.0188133790476548	12937	12.937	0.11329508663033792
X	0.01963535232966533	24224	24.224	0.09323900140606932
X	0.019570372265151372	204453	204.453	0.045744111836132265
X	0.019939025128506035	38313	38.313	0.08043639294110667
X	0.019743391601076077	55176	55.176	0.07099436006896698
X	0.01967847807233327	18116	18.116	0.10279604323965665
X	0.020005782257412803	1576218	1576.218	0.023326330159103008
X	0.01933381581002805	21959	21.959	0.09584475230770562
X	0.01960095557362505	85294	85.294	0.06125190319869603
X	0.01993894752866654	272221	272.221	0.04184017912475916
X	0.019739932798016026	64198	64.198	0.06749548522021534
X	0.01931678968615088	35335	35.335	0.08176672787440183
X	0.019992273199023518	1007717	1007.717	0.027071222244390222
X	0.019882017817786497	180934	180.934	0.04789756381679341
X	0.01927062249418348	94267	94.267	0.05890859787326394
X	0.019341022358903864	10114	10.114	0.12412297012777687
X	0.01971011680366522	53132	53.132	0.07185290721466463
X	0.019318607005451845	18825	18.825	0.10086649758864645
X	0.01998531179323925	224753	224.753	0.04463427418199625
X	0.01998521362154828	1352428	1352.428	0.024539453146616288
X	0.019969780369374877	132847	132.847	0.053170879698952185
X	0.019962893771630554	257135	257.135	0.04265999173735112
X	0.019751311206736386	90214	90.214	0.060270852006240334
X	0.01928238579249115	42582	42.582	0.0767912205897332
X	0.019793318167607366	51022	51.022	0.07293237826615181
X	0.019206501319895177	18518	18.518	0.10122428711914212
X	0.01997215295119959	405362	405.362	0.03666012570220842
X	0.01995746617588196	506196	506.196	0.03403524628868783
X	0.019500807501353502	100607	100.607	0.057872840493183704
X	0.01989853741006439	1530172	1530.172	0.02351582944709179
X	0.019708569576383266	203654	203.654	0.045911419448530374
X	0.019602358106362235	58550	58.55	0.0694374568624228
X	0.01932846100769188	9487	9.487	0.1267718393604671
X	0.01982612180949889	88708	88.708	0.06068643988810394
X	0.019565443239566112	162897	162.897	0.049339211151029176
X	0.019224689371314184	11994	11.994	0.11703032980583558
X	0.019908960113421425	811766	811.766	0.029053998272958063
X	0.019365520190633166	21565	21.565	0.09647761710036445
X	0.01979376164541862	40789	40.789	0.07858296770831602
X	0.019353038310231042	9095	9.095	0.12862206692458547
X	0.019217227011922938	17829	17.829	0.10253085489269965
X	0.019048636402218586	3595	3.595	0.17433650388553124
X	0.01950745007959678	11956	11.956	0.11772576808446618
X	0.019662912880602963	31317	31.317	0.08562933426637025
X	0.019378391151169322	47076	47.076	0.07438854334449753
X	0.01999603196765198	3630049	3630.049	0.017660802953104668
X	0.019201242125401106	22263	22.263	0.09518792927484264
X	0.019582025263757144	218446	218.446	0.044754616148082856
X	0.019227989605142222	10254	10.254	0.12331432268218963
X	0.019971424741571583	32437	32.437	0.08507255098779909
X	0.019708782656999206	13457	13.457	0.11356308519980607
X	0.019289398048670952	261641	261.641	0.041931232206057864
X	0.019231881447762885	84637	84.637	0.061022103006397196
X	0.019777529522340225	1176830	1176.83	0.025614513017392284
X	0.019344544995229782	11027	11.027	0.12060548186284566
X	0.01978970391272168	226352	226.352	0.044383170232509754
X	0.019492266087307943	35540	35.54	0.0818555823542948
X	0.019769956203857884	184239	184.239	0.04751981193052542
X	0.01996134014644739	289922	289.922	0.04098606027652065
X	0.019914032448287787	29469	29.469	0.08775353168538408
X	0.01903394515120059	16374	16.374	0.10514565783585127
X	0.019909749896323072	98192	98.192	0.058748470748206806
X	0.0195692492794975	29460	29.46	0.08725302133287581
X	0.01988724301184384	71635	71.635	0.0652353662727564
X	0.01961672514410341	342041	342.041	0.038564062662453893
X	0.019084383165190403	14844	14.844	0.10873659449372883
X	0.019887131891019025	95106	95.106	0.059354649654807135
X	0.019320804793443325	36908	36.908	0.08059378768820444
X	0.019976906945215988	170731	170.731	0.04891085869648748
X	0.019960882777007968	327051	327.051	0.039372055407397764
X	0.019327144260957184	54246	54.246	0.0708924506523439
X	0.019876130941886772	16467	16.467	0.10647290934693651
X	0.019225777963025777	223420	223.42	0.0441489749881456
X	0.019776885628770596	138738	138.738	0.05223811423865655
X	0.019955035210949816	722617	722.617	0.030226053126724505
X	0.019250488565836523	74431	74.431	0.06371318278284246
X	0.019549982259759662	63423	63.423	0.06755121978537335
X	0.019726630554076016	38375	38.375	0.08010657694692491
X	0.018894801385525312	175375	175.375	0.04758370076057327
X	0.019946505304731098	231411	231.411	0.0441734113229978
X	0.019653527321686674	39099	39.099	0.07951058563778225
X	0.019853542533802764	9372	9.372	0.12843061934373792
X	0.01993877263272576	14661	14.661	0.11079281809127188
X	0.01900343753047653	87039	87.039	0.06021519423813636
X	0.01962311286423535	115229	115.229	0.05542878635934046
X	0.01934877500439236	97273	97.273	0.058374125584276064
X	0.019010251122901507	14200	14.2	0.11021308774913804
X	0.01940335813502716	31827	31.827	0.08479307761359597
X	0.019785759670982467	61541	61.541	0.06850612356207404
X	0.019664323312491586	58260	58.26	0.06962568943542646
X	0.019554189470248458	36139	36.139	0.08148691870364254
X	0.019992509300384074	80877	80.877	0.06275968537970097
X	0.01967794250204727	23276	23.276	0.09455629485545468
X	0.01934262037555409	35805	35.805	0.08144364647325748
X	0.01890248462599815	4039	4.039	0.16726882780547245
X	0.01989949865571333	12509	12.509	0.11673645861497378
X	0.01965180272596446	185474	185.474	0.04731946065005255
X	0.019298638685358284	27421	27.421	0.0889503774872625
X	0.019325524883008108	97901	97.901	0.05822570067604758
X	0.01923992516812037	3345	3.345	0.17917156432742729
X	0.01904757442192894	9146	9.146	0.1277035079733972
X	0.019593838998127364	57593	57.593	0.06980983495703207
X	0.019232857213023914	93562	93.562	0.05901758536137742
X	0.01992531704112455	38198	38.198	0.08049857738288518
X	0.019876111167073076	8270	8.27	0.13394976905752637
X	0.01904416969752925	9794	9.794	0.12481515326162047
X	0.01927372894666153	47294	47.294	0.07414011124492034
X	0.01931263728472488	29674	29.674	0.08666062773054797
X	0.01951806575038077	146552	146.552	0.05106794033942208
X	0.019586816240381975	8967	8.967	0.12974946019664832
X	0.019261039942545622	14172	14.172	0.11076847995902045
X	0.019235663294062212	63032	63.032	0.06732587808625834
X	0.01896095700973538	8887	8.887	0.12873649806577986
X	0.019988252595832593	214157	214.157	0.04536081527146862
X	0.019580754818945927	15178	15.178	0.10886082642586205
X	0.019455019561060063	126423	126.423	0.05358812066999856
X	0.019627869589493217	176701	176.701	0.04807043499905269
X	0.01989244464871619	47542	47.542	0.07479459878442522
X	0.01998029801113333	168456	168.456	0.04913283520882592
X	0.019823556917819195	166192	166.192	0.04922554002983178
X	0.019028776314135333	3689	3.689	0.1727828905650368
X	0.019216660422066482	70897	70.897	0.06471674797240555
X	0.019340518435560634	24905	24.905	0.09191657962460413
X	0.019672905786985898	180115	180.115	0.04780128188617078
X	0.019755164066390017	153873	153.873	0.05044747183884599
X	0.01925781860117085	44149	44.149	0.07583949052779479
X	0.019966648033127303	99164	99.164	0.058611617746632816
X	0.01899405600419993	28448	28.448	0.08740212498160416
X	0.01926305101492632	42247	42.247	0.07696791700895159
X	0.019275310130411637	7537	7.537	0.1367521888513071
X	0.019750861041118575	13077	13.077	0.11473413257338912
X	0.019971460318662235	264792	264.792	0.04225080493782865
X	0.01998368640158199	689715	689.715	0.030713924442136626
X	0.019904514483688632	38116	38.116	0.08052821825412862
X	0.01955732932582008	17776	17.776	0.10323457378851625
X	0.019571355820645314	47339	47.339	0.07449616983748485
X	0.01998077146863984	186992	186.992	0.047452934590514556
X	0.01927446041803403	55183	55.183	0.07042480297512792
X	0.019559994984826586	14333	14.333	0.11092017499446782
X	0.01992277339410932	59698	59.698	0.06936334116611674
X	0.019953761249567752	201194	201.194	0.046288159785434914
X	0.019772061824029574	10734	10.734	0.12258296534103545
X	0.019403363115346836	54703	54.703	0.07078729043673607
X	0.018962085134379456	31895	31.895	0.08408550985285451
time for making epsilon is 3.402949333190918
epsilons are
[0.20476556832164403, 0.08559885319994913, 0.17274983457613213, 0.09374164925649509, 0.09978346251823243, 0.06660341018990974, 0.0612280726397024, 0.06521117757894826, 0.09258633203700302, 0.09120109078172203, 0.09889026997216072, 0.05191855659831582, 0.11847315041940709, 0.034453240886101494, 0.08078883293232264, 0.0654700923760875, 0.06119756167939607, 0.07878937740435094, 0.04729066133661751, 0.05200073533089812, 0.06438033941648623, 0.0423553654141326, 0.10778461888716245, 0.06484544423024288, 0.13849353681433782, 0.05862341088738248, 0.05756716981094225, 0.1072428159811354, 0.04577568155445508, 0.023270374323000987, 0.08952510045110058, 0.02427295234583223, 0.07642136020827206, 0.09945114722596549, 0.08838056366543467, 0.03669102004014721, 0.055647278602135586, 0.20975352646355175, 0.14000269051035819, 0.16202343182475956, 0.14749282329309665, 0.1531069719937217, 0.2156887048952246, 0.1344046535687685, 0.22593478407957843, 0.19669291674070474, 0.14923630693494783, 0.1356742684091794, 0.18090604842275507, 0.09282796668057473, 0.09495221001650508, 0.19272841646425723, 0.1147388626642149, 0.15246767862986788, 0.1349828471233692, 0.12615244835567868, 0.18549031386777354, 0.13266123148291584, 0.14660009485012163, 0.158066324147152, 0.14381700395173894, 0.18370087529380574, 0.12011830958917166, 0.099688066214527, 0.15890036452653156, 0.16055631448842042, 0.1507943002934812, 0.12153774720971712, 0.1277933027329889, 0.14796565249879232, 0.14902895036164251, 0.14526325233335663, 0.14142612377611605, 0.14935103808333017, 0.13955904906398467, 0.11436874530162182, 0.226790331515419, 0.2086385138794897, 0.14747165019195432, 0.0914546418203124, 0.15507237585338376, 0.17783981637097182, 0.13572367152829815, 0.17789974140069717, 0.17847042063406707, 0.19571569439634684, 0.1602479771920224, 0.19941300835877, 0.13648519789745608, 0.14984646903604776, 0.14734514131021156, 0.1732744494604364, 0.17257572743374627, 0.13603307504385032, 0.1387380669453552, 0.13707801194635844, 0.18363242592761417, 0.12094011138790228, 0.14371717673018533, 0.14445247912487258, 0.10210196341428088, 0.1415957295481621, 0.11888869169240728, 0.18351495750977956, 0.10708297638127247, 0.15244476996656967, 0.1355028417047103, 0.1927084683910777, 0.1563127564885183, 0.20028704706432365, 0.1650322222750083, 0.19734123142000692, 0.18761021396557034, 0.1503680662721029, 0.127181485597128, 0.15246525101224057, 0.16602625330350412, 0.2196447365243832, 0.11927713752147484, 0.18477508416246036, 0.10700424125742815, 0.16436218691383622, 0.17934432099405515, 0.1755844148241584, 0.15854013064977665, 0.1810613175908941, 0.19311857925487388, 0.14028652538363126, 0.10231361939064683, 0.17544029040614648, 0.11327094964861513, 0.1992769014628724, 0.15309342133988543, 0.18980117694948284, 0.13156845460480163, 0.18398512950147838, 0.19045003900238275, 0.18298462485491682, 0.16357923773214544, 0.1915898610733012, 0.14022951038661038, 0.18416807810479874, 0.09960022396629363, 0.12172007005856277, 0.1335135063220946, 0.1826021648080991, 0.17738728850247937, 0.17128078167444905, 0.18038074604950957, 0.1695249972489485, 0.16154387369109713, 0.16136907189330266, 0.13593877473690222, 0.11199391042919245, 0.16069744923284082, 0.08365625774172121, 0.14576120952165522, 0.13829247036444609, 0.14807294585347655, 0.27287715342274144, 0.11612061444086975, 0.19051664074623983, 0.11692443652340147, 0.21797974559115943, 0.13439314434238653, 0.18251038543318593, 0.18668323616372273, 0.1388095878924625, 0.1360739695279631, 0.22200147881937848, 0.2159548815005569, 0.16997250810633938, 0.11995129402669849, 0.18566436811630235, 0.13577495224352049, 0.09851842101314225, 0.12283434429174103, 0.17450225840306455, 0.12666348504132036, 0.15042227847062842, 0.15113555885309887, 0.16678252925949663, 0.1417744416665299, 0.15340451068728916, 0.13454412195665205, 0.13747628323508862, 0.161017265167386, 0.13276107887685598, 0.1206175856509101, 0.11493035628956773, 0.15898117228640543, 0.2655238717135536, 0.1515159867841969, 0.13898948072226408, 0.13087756876759035, 0.15755971354985, 0.1548318797908637, 0.1907777982779753, 0.1442811396371308, 0.2129519057402599, 0.15146441540417657, 0.17041007914282563, 0.1169862239011999, 0.16540312171015795, 0.1283450594177246, 0.18766791579270553, 0.14598501119173501, 0.14453494018063215, 0.15920930065709352, 0.11646228576348773, 0.13068124327770758, 0.12605847873050396, 0.14990333644111817, 0.1464103944698338, 0.22338811333540778, 0.19587053338912389, 0.2234630063056656, 0.20375197264332498, 0.09343811170110364, 0.1652733432147565, 0.16117625750242648, 0.21015866532587335, 0.12038776474091058, 0.14446521169819126, 0.15397401802624577, 0.14254325719687624, 0.18974365730856826, 0.18709884123177922, 0.13611636903601765, 0.24375739163932092, 0.16445781025825298, 0.1601372217664307, 0.1595622300135244, 0.1371297150154534, 0.15337423321883423, 0.1491137631801942, 0.14160462597211373, 0.13607739579456957, 0.15610387439905726, 0.08518533351298273, 0.06245608619514603, 0.13519657912609523, 0.13782431780062762, 0.2211456857189387, 0.1183824577511422, 0.10790044827846036, 0.04562230571014975, 0.14400592038990873, 0.043284537425021176, 0.06156029616792381, 0.07282733190416894, 0.04689450286564949, 0.16632116436822442, 0.13704140490370323, 0.03727177134965385, 0.0313533008759432, 0.14188197821832757, 0.07015126849424011, 0.08526588811734374, 0.08174353873298004, 0.05317001072211254, 0.06308989490090014, 0.08361406998368494, 0.08989890699329454, 0.15561594751847535, 0.045730278332067026, 0.11839589824372548, 0.14861605127198516, 0.08522404101193237, 0.0566052705719814, 0.03972411538143269, 0.054932805656618025, 0.17415567419096797, 0.07125141611609971, 0.11580173710506683, 0.07328544574983943, 0.05941209741034613, 0.09059645631169565, 0.05678449267240692, 0.2363203781674602, 0.11745095300558234, 0.05380676830206712, 0.061708076489038886, 0.05447204351330459, 0.15034780640575826, 0.03874656884399535, 0.09598846838461898, 0.0773643271688492, 0.05504700468485128, 0.1661988156790073, 0.0509788316880029, 0.03983619606576706, 0.028010306757669943, 0.07410571246814207, 0.07764608876379137, 0.11850113715807434, 0.12951419591054697, 0.04524992997889243, 0.11170692202581105, 0.07722194683092859, 0.0840006844039389, 0.08164301674155611, 0.09202065825219963, 0.1372526172415694, 0.04598957091920441, 0.06824521951310789, 0.08780018691457873, 0.12186317685376419, 0.056668585270338426, 0.0927447518611057, 0.042863373708782286, 0.11646122380017503, 0.0724966751020386, 0.10869748635419786, 0.043475955890695826, 0.06984216303991407, 0.10683879864172258, 0.048217976875473974, 0.04382725784396556, 0.10616670039482506, 0.04770218169426974, 0.04621904054597122, 0.033646150458899016, 0.09013747182869924, 0.16455651140347985, 0.10185416302783869, 0.11061158766700725, 0.08328726027248681, 0.06957557399642347, 0.17832727105075824, 0.05824286043946561, 0.0439807024661433, 0.09923215192886048, 0.12062821047187078, 0.06941980972856895, 0.10522308018533451, 0.09145414334185052, 0.10168318174854411, 0.12259373691038468, 0.06315735287222317, 0.07112242254182984, 0.08943515943375258, 0.06670244238394449, 0.1158393260922847, 0.025734167771240005, 0.10688601953687192, 0.052482264422836475, 0.18639859252844967, 0.14249799523521997, 0.14850163704086636, 0.10791679950198128, 0.13285431842708537, 0.05442212684506285, 0.1452234428224187, 0.14275662251184956, 0.0387265943464302, 0.06317597267394523, 0.07268604518924351, 0.049102670548891794, 0.03539684139168675, 0.13780693562978757, 0.12192561016433576, 0.08174246856579338, 0.1679320339706099, 0.12653340716893693, 0.09828357790265556, 0.13633609013478387, 0.12381024658164994, 0.12406284316871106, 0.11477713986695909, 0.03478125709154277, 0.09634304668634568, 0.05597988093031343, 0.12730999676867705, 0.14567788334843979, 0.05708920649155053, 0.03916437184360091, 0.03167755203626333, 0.08902787230015247, 0.07019757465767429, 0.16096331877488734, 0.1033292226733781, 0.025616426045284336, 0.08808723393691317, 0.11811508143357144, 0.06860749382778195, 0.028663551270503265, 0.06336519622664645, 0.10122235916259044, 0.09523459819945491, 0.10866494578014682, 0.14679804832292997, 0.04424992934900978, 0.09048834709512488, 0.18412340181913397, 0.0585855911408322, 0.09366091514295018, 0.19680183008388885, 0.058314030811295936, 0.052659650132673146, 0.04157154974723035, 0.041937657278086016, 0.039882614616257854, 0.07941730041666926, 0.07728884676008123, 0.04802313405650364, 0.041002556053077524, 0.10268826088466393, 0.05364984512846727, 0.03977289869652799, 0.04307772149042602, 0.050957735323623236, 0.06832004890387237, 0.100447946485686, 0.12920991224548667, 0.08512245460580879, 0.047774349833451475, 0.049782399513207336, 0.03851812164479345, 0.049187461582962264, 0.05928283703773326, 0.09367999295829676, 0.04002006476667361, 0.12940847831955896, 0.10635620337566491, 0.045593952830475244, 0.0675667241965909, 0.13193203431608638, 0.08915377762994366, 0.03181317100633901, 0.08910394979853366, 0.050284944669819025, 0.13377454752896095, 0.13236598872895686, 0.13364597927777377, 0.1238325542843711, 0.08303315296646871, 0.05869822398233918, 0.12643789935486577, 0.04682438360652844, 0.11978857799319559, 0.060226613082968464, 0.10563496382845894, 0.05038717559461494, 0.1271214917100745, 0.08695146862733098, 0.07903909642719047, 0.08186209273935781, 0.058969035531789185, 0.03202460292297838, 0.11564251338460609, 0.125055729344677, 0.06450202561609794, 0.08730194363318744, 0.03135479592586282, 0.1607318226204868, 0.04929579650444787, 0.03810372897301278, 0.1354775852913271, 0.04137505450123332, 0.08910495777612207, 0.030776171979230698, 0.10030472295096604, 0.08479767304156442, 0.11338772046664358, 0.14499710953703013, 0.12033716979429583, 0.06879035254439636, 0.12570314112858225, 0.04646698275990974, 0.09180019675846476, 0.133601058136837, 0.0834659564312102, 0.0421383678199764, 0.05407820602698458, 0.03762715368256162, 0.0852928801626546, 0.12188410917021765, 0.15235422602036072, 0.13622401196960363, 0.042766204647987076, 0.11460208040790379, 0.08448065221844106, 0.07231261039223288, 0.08641576145429226, 0.11145257793744759, 0.146400846662596, 0.040443687291863956, 0.06845143389415242, 0.10497281468134616, 0.09705522937179357, 0.05513284550332636, 0.05304758908182993, 0.034240621473265474, 0.050396642525740575, 0.12974820983644142, 0.07263889133571806, 0.0688223461278139, 0.1126787108842862, 0.10323504893707598, 0.1291480979893526, 0.0869978552149844, 0.08057341205288156, 0.15268760957566888, 0.09648416431838422, 0.1280228184851314, 0.07765798405738758, 0.12628096442670142, 0.08360014734084119, 0.09562297657901205, 0.09294171924243172, 0.12905329371590843, 0.10023443182254857, 0.08769651805687928, 0.04837597222038446, 0.11618864844987452, 0.18272382569492332, 0.12607610931781996, 0.050454108879861945, 0.02896254342599955, 0.06209895967389339, 0.1224008310314993, 0.044754380539289006, 0.04706847567002838, 0.16185738458853266, 0.11951523776449086, 0.07552832429882265, 0.050751185191181075, 0.035411695931181424, 0.042068969920940114, 0.08709601840609445, 0.1451263756088881, 0.16077753787093169, 0.05987262596635631, 0.10074570731532669, 0.07889045677162053, 0.04578393337720477, 0.03038794486930722, 0.10768073040421923, 0.17744852582003037, 0.08229624548033572, 0.050596255375247666, 0.06975390501070441, 0.12111758104251812, 0.14393306980581286, 0.08099272043532893, 0.16422707708119777, 0.06531817688097974, 0.12979704285528362, 0.09740278234799765, 0.11282747364278697, 0.1255650503774315, 0.13499673885148744, 0.04525273318907501, 0.03499015552205094, 0.05228906036196385, 0.0537701567286852, 0.08862467842072955, 0.11329508663033792, 0.09323900140606932, 0.045744111836132265, 0.08043639294110667, 0.07099436006896698, 0.10279604323965665, 0.023326330159103008, 0.09584475230770562, 0.06125190319869603, 0.04184017912475916, 0.06749548522021534, 0.08176672787440183, 0.027071222244390222, 0.04789756381679341, 0.05890859787326394, 0.12412297012777687, 0.07185290721466463, 0.10086649758864645, 0.04463427418199625, 0.024539453146616288, 0.053170879698952185, 0.04265999173735112, 0.060270852006240334, 0.0767912205897332, 0.07293237826615181, 0.10122428711914212, 0.03666012570220842, 0.03403524628868783, 0.057872840493183704, 0.02351582944709179, 0.045911419448530374, 0.0694374568624228, 0.1267718393604671, 0.06068643988810394, 0.049339211151029176, 0.11703032980583558, 0.029053998272958063, 0.09647761710036445, 0.07858296770831602, 0.12862206692458547, 0.10253085489269965, 0.17433650388553124, 0.11772576808446618, 0.08562933426637025, 0.07438854334449753, 0.017660802953104668, 0.09518792927484264, 0.044754616148082856, 0.12331432268218963, 0.08507255098779909, 0.11356308519980607, 0.041931232206057864, 0.061022103006397196, 0.025614513017392284, 0.12060548186284566, 0.044383170232509754, 0.0818555823542948, 0.04751981193052542, 0.04098606027652065, 0.08775353168538408, 0.10514565783585127, 0.058748470748206806, 0.08725302133287581, 0.0652353662727564, 0.038564062662453893, 0.10873659449372883, 0.059354649654807135, 0.08059378768820444, 0.04891085869648748, 0.039372055407397764, 0.0708924506523439, 0.10647290934693651, 0.0441489749881456, 0.05223811423865655, 0.030226053126724505, 0.06371318278284246, 0.06755121978537335, 0.08010657694692491, 0.04758370076057327, 0.0441734113229978, 0.07951058563778225, 0.12843061934373792, 0.11079281809127188, 0.06021519423813636, 0.05542878635934046, 0.058374125584276064, 0.11021308774913804, 0.08479307761359597, 0.06850612356207404, 0.06962568943542646, 0.08148691870364254, 0.06275968537970097, 0.09455629485545468, 0.08144364647325748, 0.16726882780547245, 0.11673645861497378, 0.04731946065005255, 0.0889503774872625, 0.05822570067604758, 0.17917156432742729, 0.1277035079733972, 0.06980983495703207, 0.05901758536137742, 0.08049857738288518, 0.13394976905752637, 0.12481515326162047, 0.07414011124492034, 0.08666062773054797, 0.05106794033942208, 0.12974946019664832, 0.11076847995902045, 0.06732587808625834, 0.12873649806577986, 0.04536081527146862, 0.10886082642586205, 0.05358812066999856, 0.04807043499905269, 0.07479459878442522, 0.04913283520882592, 0.04922554002983178, 0.1727828905650368, 0.06471674797240555, 0.09191657962460413, 0.04780128188617078, 0.05044747183884599, 0.07583949052779479, 0.058611617746632816, 0.08740212498160416, 0.07696791700895159, 0.1367521888513071, 0.11473413257338912, 0.04225080493782865, 0.030713924442136626, 0.08052821825412862, 0.10323457378851625, 0.07449616983748485, 0.047452934590514556, 0.07042480297512792, 0.11092017499446782, 0.06936334116611674, 0.046288159785434914, 0.12258296534103545, 0.07078729043673607, 0.08408550985285451]
0.08895725436147013
Making ranges
torch.Size([53710, 2])
We keep 8.12e+06/8.08e+08 =  1% of the original kernel matrix.

torch.Size([5704, 2])
We keep 2.28e+05/5.07e+06 =  4% of the original kernel matrix.

torch.Size([19014, 2])
We keep 1.35e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([55124, 2])
We keep 1.31e+07/9.84e+08 =  1% of the original kernel matrix.

torch.Size([56952, 2])
We keep 9.71e+06/8.92e+08 =  1% of the original kernel matrix.

torch.Size([9592, 2])
We keep 4.63e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([23877, 2])
We keep 1.95e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([35309, 2])
We keep 1.69e+07/5.45e+08 =  3% of the original kernel matrix.

torch.Size([44354, 2])
We keep 7.54e+06/6.64e+08 =  1% of the original kernel matrix.

torch.Size([37094, 2])
We keep 8.64e+06/4.01e+08 =  2% of the original kernel matrix.

torch.Size([46920, 2])
We keep 6.65e+06/5.69e+08 =  1% of the original kernel matrix.

torch.Size([122748, 2])
We keep 4.24e+07/4.27e+09 =  0% of the original kernel matrix.

torch.Size([80832, 2])
We keep 1.75e+07/1.86e+09 =  0% of the original kernel matrix.

torch.Size([155189, 2])
We keep 6.55e+07/7.22e+09 =  0% of the original kernel matrix.

torch.Size([91863, 2])
We keep 2.20e+07/2.42e+09 =  0% of the original kernel matrix.

torch.Size([132319, 2])
We keep 4.41e+07/4.83e+09 =  0% of the original kernel matrix.

torch.Size([84123, 2])
We keep 1.85e+07/1.98e+09 =  0% of the original kernel matrix.

torch.Size([41867, 2])
We keep 1.29e+07/6.11e+08 =  2% of the original kernel matrix.

torch.Size([49090, 2])
We keep 7.90e+06/7.03e+08 =  1% of the original kernel matrix.

torch.Size([42219, 2])
We keep 1.59e+07/6.67e+08 =  2% of the original kernel matrix.

torch.Size([48560, 2])
We keep 7.98e+06/7.34e+08 =  1% of the original kernel matrix.

torch.Size([36102, 2])
We keep 6.32e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([45623, 2])
We keep 6.69e+06/5.68e+08 =  1% of the original kernel matrix.

torch.Size([227261, 2])
We keep 2.42e+08/2.04e+10 =  1% of the original kernel matrix.

torch.Size([112431, 2])
We keep 3.42e+07/4.06e+09 =  0% of the original kernel matrix.

torch.Size([24144, 2])
We keep 2.48e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([36704, 2])
We keep 4.32e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([896841, 2])
We keep 1.70e+09/2.33e+11 =  0% of the original kernel matrix.

torch.Size([232354, 2])
We keep 1.01e+08/1.37e+10 =  0% of the original kernel matrix.

torch.Size([64818, 2])
We keep 1.50e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([60289, 2])
We keep 1.09e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([131104, 2])
We keep 5.73e+07/4.77e+09 =  1% of the original kernel matrix.

torch.Size([83671, 2])
We keep 1.83e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([155753, 2])
We keep 8.01e+07/7.31e+09 =  1% of the original kernel matrix.

torch.Size([92287, 2])
We keep 2.21e+07/2.43e+09 =  0% of the original kernel matrix.

torch.Size([69445, 2])
We keep 2.00e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([61984, 2])
We keep 1.16e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([316591, 2])
We keep 3.84e+08/3.56e+10 =  1% of the original kernel matrix.

torch.Size([135938, 2])
We keep 4.40e+07/5.36e+09 =  0% of the original kernel matrix.

torch.Size([249397, 2])
We keep 1.97e+08/2.02e+10 =  0% of the original kernel matrix.

torch.Size([119540, 2])
We keep 3.36e+07/4.04e+09 =  0% of the original kernel matrix.

torch.Size([123525, 2])
We keep 8.24e+07/5.12e+09 =  1% of the original kernel matrix.

torch.Size([80950, 2])
We keep 1.91e+07/2.03e+09 =  0% of the original kernel matrix.

torch.Size([466125, 2])
We keep 4.15e+08/6.75e+10 =  0% of the original kernel matrix.

torch.Size([167060, 2])
We keep 5.71e+07/7.38e+09 =  0% of the original kernel matrix.

torch.Size([29989, 2])
We keep 3.79e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([41341, 2])
We keep 5.35e+06/4.33e+08 =  1% of the original kernel matrix.

torch.Size([121765, 2])
We keep 1.19e+08/5.36e+09 =  2% of the original kernel matrix.

torch.Size([81837, 2])
We keep 1.98e+07/2.08e+09 =  0% of the original kernel matrix.

torch.Size([12980, 2])
We keep 1.26e+07/5.54e+07 = 22% of the original kernel matrix.

torch.Size([26981, 2])
We keep 2.81e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([176433, 2])
We keep 7.81e+07/9.46e+09 =  0% of the original kernel matrix.

torch.Size([98944, 2])
We keep 2.45e+07/2.76e+09 =  0% of the original kernel matrix.

torch.Size([182186, 2])
We keep 9.79e+07/1.07e+10 =  0% of the original kernel matrix.

torch.Size([100501, 2])
We keep 2.60e+07/2.94e+09 =  0% of the original kernel matrix.

torch.Size([30504, 2])
We keep 5.94e+06/2.53e+08 =  2% of the original kernel matrix.

torch.Size([42048, 2])
We keep 5.52e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([242308, 2])
We keep 6.44e+08/4.26e+10 =  1% of the original kernel matrix.

torch.Size([115616, 2])
We keep 4.78e+07/5.87e+09 =  0% of the original kernel matrix.

torch.Size([3331471, 2])
We keep 1.10e+10/2.51e+12 =  0% of the original kernel matrix.

torch.Size([465360, 2])
We keep 3.00e+08/4.50e+10 =  0% of the original kernel matrix.

torch.Size([45753, 2])
We keep 1.08e+07/7.54e+08 =  1% of the original kernel matrix.

torch.Size([49771, 2])
We keep 8.37e+06/7.80e+08 =  1% of the original kernel matrix.

torch.Size([2766744, 2])
We keep 9.25e+09/1.95e+12 =  0% of the original kernel matrix.

torch.Size([423763, 2])
We keep 2.70e+08/3.97e+10 =  0% of the original kernel matrix.

torch.Size([72992, 2])
We keep 5.00e+07/1.97e+09 =  2% of the original kernel matrix.

torch.Size([63466, 2])
We keep 1.28e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([37151, 2])
We keep 6.23e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([46491, 2])
We keep 6.60e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([47943, 2])
We keep 1.19e+07/7.84e+08 =  1% of the original kernel matrix.

torch.Size([50851, 2])
We keep 8.43e+06/7.96e+08 =  1% of the original kernel matrix.

torch.Size([708038, 2])
We keep 1.43e+09/1.64e+11 =  0% of the original kernel matrix.

torch.Size([202892, 2])
We keep 8.72e+07/1.15e+10 =  0% of the original kernel matrix.

torch.Size([204427, 2])
We keep 1.52e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([107689, 2])
We keep 2.86e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([5513, 2])
We keep 1.75e+05/4.11e+06 =  4% of the original kernel matrix.

torch.Size([18954, 2])
We keep 1.25e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([16098, 2])
We keep 1.19e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([29755, 2])
We keep 2.94e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([10838, 2])
We keep 6.21e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([24737, 2])
We keep 2.17e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([14698, 2])
We keep 9.24e+05/3.62e+07 =  2% of the original kernel matrix.

torch.Size([28636, 2])
We keep 2.68e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([8765, 2])
We keep 1.41e+06/2.69e+07 =  5% of the original kernel matrix.

torch.Size([21165, 2])
We keep 2.38e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([5191, 2])
We keep 1.48e+05/3.49e+06 =  4% of the original kernel matrix.

torch.Size([18466, 2])
We keep 1.18e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([17432, 2])
We keep 1.38e+06/5.94e+07 =  2% of the original kernel matrix.

torch.Size([30825, 2])
We keep 3.22e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([4522, 2])
We keep 1.27e+05/2.71e+06 =  4% of the original kernel matrix.

torch.Size([17436, 2])
We keep 1.09e+06/4.68e+07 =  2% of the original kernel matrix.

torch.Size([6854, 2])
We keep 2.37e+05/6.33e+06 =  3% of the original kernel matrix.

torch.Size([20706, 2])
We keep 1.44e+06/7.15e+07 =  2% of the original kernel matrix.

torch.Size([13597, 2])
We keep 8.79e+05/3.19e+07 =  2% of the original kernel matrix.

torch.Size([27309, 2])
We keep 2.54e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([17685, 2])
We keep 1.36e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([31248, 2])
We keep 3.19e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([8282, 2])
We keep 3.75e+05/1.02e+07 =  3% of the original kernel matrix.

torch.Size([22186, 2])
We keep 1.71e+06/9.06e+07 =  1% of the original kernel matrix.

torch.Size([41601, 2])
We keep 1.22e+07/6.17e+08 =  1% of the original kernel matrix.

torch.Size([49116, 2])
We keep 8.07e+06/7.06e+08 =  1% of the original kernel matrix.

torch.Size([41796, 2])
We keep 7.30e+06/5.36e+08 =  1% of the original kernel matrix.

torch.Size([49910, 2])
We keep 7.59e+06/6.58e+08 =  1% of the original kernel matrix.

torch.Size([7199, 2])
We keep 2.78e+05/7.19e+06 =  3% of the original kernel matrix.

torch.Size([20998, 2])
We keep 1.51e+06/7.62e+07 =  1% of the original kernel matrix.

torch.Size([23461, 2])
We keep 4.89e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([35955, 2])
We keep 4.74e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([13203, 2])
We keep 8.65e+05/2.96e+07 =  2% of the original kernel matrix.

torch.Size([27126, 2])
We keep 2.53e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([17929, 2])
We keep 1.50e+06/6.53e+07 =  2% of the original kernel matrix.

torch.Size([31598, 2])
We keep 3.37e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([18752, 2])
We keep 2.94e+06/9.27e+07 =  3% of the original kernel matrix.

torch.Size([32117, 2])
We keep 3.83e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([7744, 2])
We keep 3.20e+05/8.96e+06 =  3% of the original kernel matrix.

torch.Size([21787, 2])
We keep 1.62e+06/8.51e+07 =  1% of the original kernel matrix.

torch.Size([18834, 2])
We keep 1.48e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([32047, 2])
We keep 3.40e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([14348, 2])
We keep 9.80e+05/3.76e+07 =  2% of the original kernel matrix.

torch.Size([28281, 2])
We keep 2.74e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([11643, 2])
We keep 7.46e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([25517, 2])
We keep 2.32e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([15725, 2])
We keep 1.11e+06/4.44e+07 =  2% of the original kernel matrix.

torch.Size([29764, 2])
We keep 2.92e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([7753, 2])
We keep 3.52e+05/9.62e+06 =  3% of the original kernel matrix.

torch.Size([21692, 2])
We keep 1.66e+06/8.81e+07 =  1% of the original kernel matrix.

torch.Size([24122, 2])
We keep 2.66e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([37245, 2])
We keep 4.39e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([36659, 2])
We keep 5.35e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([46333, 2])
We keep 6.60e+06/5.53e+08 =  1% of the original kernel matrix.

torch.Size([11826, 2])
We keep 6.98e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([25812, 2])
We keep 2.29e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([11245, 2])
We keep 6.32e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([25204, 2])
We keep 2.19e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([13616, 2])
We keep 8.34e+05/3.14e+07 =  2% of the original kernel matrix.

torch.Size([27613, 2])
We keep 2.54e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([22412, 2])
We keep 2.33e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([35191, 2])
We keep 4.22e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([20538, 2])
We keep 2.06e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([33702, 2])
We keep 3.69e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([14007, 2])
We keep 9.65e+05/3.53e+07 =  2% of the original kernel matrix.

torch.Size([27757, 2])
We keep 2.66e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([12942, 2])
We keep 1.08e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([26791, 2])
We keep 2.56e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([14622, 2])
We keep 1.06e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([28302, 2])
We keep 2.77e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([15920, 2])
We keep 1.16e+06/4.64e+07 =  2% of the original kernel matrix.

torch.Size([29515, 2])
We keep 2.96e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([13349, 2])
We keep 9.79e+05/3.33e+07 =  2% of the original kernel matrix.

torch.Size([27148, 2])
We keep 2.62e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([15789, 2])
We keep 1.28e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([29288, 2])
We keep 3.04e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([25249, 2])
We keep 4.10e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([37660, 2])
We keep 4.91e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([4630, 2])
We keep 1.24e+05/2.62e+06 =  4% of the original kernel matrix.

torch.Size([17700, 2])
We keep 1.08e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([5577, 2])
We keep 2.07e+05/4.48e+06 =  4% of the original kernel matrix.

torch.Size([18949, 2])
We keep 1.28e+06/6.01e+07 =  2% of the original kernel matrix.

torch.Size([14163, 2])
We keep 1.29e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([28176, 2])
We keep 2.67e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([44736, 2])
We keep 8.44e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([50720, 2])
We keep 8.27e+06/7.35e+08 =  1% of the original kernel matrix.

torch.Size([12142, 2])
We keep 8.42e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([25996, 2])
We keep 2.38e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([8364, 2])
We keep 4.73e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([22261, 2])
We keep 1.83e+06/9.73e+07 =  1% of the original kernel matrix.

torch.Size([16850, 2])
We keep 1.70e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([30717, 2])
We keep 3.25e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([8123, 2])
We keep 4.37e+05/1.17e+07 =  3% of the original kernel matrix.

torch.Size([22043, 2])
We keep 1.83e+06/9.71e+07 =  1% of the original kernel matrix.

torch.Size([8616, 2])
We keep 3.86e+05/1.12e+07 =  3% of the original kernel matrix.

torch.Size([22628, 2])
We keep 1.78e+06/9.50e+07 =  1% of the original kernel matrix.

torch.Size([6881, 2])
We keep 2.37e+05/6.53e+06 =  3% of the original kernel matrix.

torch.Size([20740, 2])
We keep 1.46e+06/7.26e+07 =  2% of the original kernel matrix.

torch.Size([11297, 2])
We keep 7.09e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([25259, 2])
We keep 2.26e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([6224, 2])
We keep 2.29e+05/5.74e+06 =  3% of the original kernel matrix.

torch.Size([19745, 2])
We keep 1.40e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([17329, 2])
We keep 1.33e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([30984, 2])
We keep 3.18e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([13271, 2])
We keep 9.88e+05/3.24e+07 =  3% of the original kernel matrix.

torch.Size([27013, 2])
We keep 2.56e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([14508, 2])
We keep 8.75e+05/3.44e+07 =  2% of the original kernel matrix.

torch.Size([28222, 2])
We keep 2.62e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([9456, 2])
We keep 4.69e+05/1.37e+07 =  3% of the original kernel matrix.

torch.Size([23647, 2])
We keep 1.90e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([9246, 2])
We keep 5.37e+05/1.41e+07 =  3% of the original kernel matrix.

torch.Size([23372, 2])
We keep 1.94e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([16810, 2])
We keep 1.69e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([30511, 2])
We keep 3.22e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([15828, 2])
We keep 1.54e+06/5.17e+07 =  2% of the original kernel matrix.

torch.Size([29500, 2])
We keep 3.05e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([16989, 2])
We keep 1.35e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([30707, 2])
We keep 3.16e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([8096, 2])
We keep 3.35e+05/9.55e+06 =  3% of the original kernel matrix.

torch.Size([22158, 2])
We keep 1.67e+06/8.79e+07 =  1% of the original kernel matrix.

torch.Size([20307, 2])
We keep 3.19e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([33493, 2])
We keep 4.18e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([15105, 2])
We keep 1.12e+06/4.25e+07 =  2% of the original kernel matrix.

torch.Size([29020, 2])
We keep 2.86e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([14877, 2])
We keep 1.10e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([28886, 2])
We keep 2.86e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([34505, 2])
We keep 4.96e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([44659, 2])
We keep 6.20e+06/5.15e+08 =  1% of the original kernel matrix.

torch.Size([16155, 2])
We keep 1.25e+06/4.89e+07 =  2% of the original kernel matrix.

torch.Size([30046, 2])
We keep 3.03e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([24026, 2])
We keep 2.37e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([36694, 2])
We keep 4.28e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([7316, 2])
We keep 3.32e+05/9.12e+06 =  3% of the original kernel matrix.

torch.Size([20829, 2])
We keep 1.63e+06/8.58e+07 =  1% of the original kernel matrix.

torch.Size([28401, 2])
We keep 6.47e+06/2.46e+08 =  2% of the original kernel matrix.

torch.Size([40058, 2])
We keep 5.59e+06/4.46e+08 =  1% of the original kernel matrix.

torch.Size([12824, 2])
We keep 1.10e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([26793, 2])
We keep 2.59e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([18218, 2])
We keep 1.48e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([31786, 2])
We keep 3.32e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([6806, 2])
We keep 2.95e+05/7.20e+06 =  4% of the original kernel matrix.

torch.Size([20502, 2])
We keep 1.54e+06/7.63e+07 =  2% of the original kernel matrix.

torch.Size([12126, 2])
We keep 7.82e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([26163, 2])
We keep 2.37e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([5999, 2])
We keep 2.58e+05/5.82e+06 =  4% of the original kernel matrix.

torch.Size([19421, 2])
We keep 1.41e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([10270, 2])
We keep 6.10e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([24153, 2])
We keep 2.14e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([6323, 2])
We keep 2.89e+05/6.22e+06 =  4% of the original kernel matrix.

torch.Size([19846, 2])
We keep 1.45e+06/7.09e+07 =  2% of the original kernel matrix.

torch.Size([7308, 2])
We keep 3.29e+05/8.28e+06 =  3% of the original kernel matrix.

torch.Size([21012, 2])
We keep 1.59e+06/8.18e+07 =  1% of the original kernel matrix.

torch.Size([12876, 2])
We keep 9.46e+05/3.07e+07 =  3% of the original kernel matrix.

torch.Size([26798, 2])
We keep 2.47e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([19627, 2])
We keep 2.00e+06/8.49e+07 =  2% of the original kernel matrix.

torch.Size([32806, 2])
We keep 3.68e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([11505, 2])
We keep 1.38e+06/2.89e+07 =  4% of the original kernel matrix.

torch.Size([25306, 2])
We keep 2.48e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([10489, 2])
We keep 5.51e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([24641, 2])
We keep 2.07e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([4953, 2])
We keep 1.60e+05/3.22e+06 =  4% of the original kernel matrix.

torch.Size([18263, 2])
We keep 1.16e+06/5.10e+07 =  2% of the original kernel matrix.

torch.Size([23764, 2])
We keep 2.66e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([36942, 2])
We keep 4.40e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([7614, 2])
We keep 3.58e+05/9.38e+06 =  3% of the original kernel matrix.

torch.Size([21341, 2])
We keep 1.66e+06/8.70e+07 =  1% of the original kernel matrix.

torch.Size([30807, 2])
We keep 3.94e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([42242, 2])
We keep 5.62e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([10442, 2])
We keep 7.00e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([24571, 2])
We keep 2.15e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([8230, 2])
We keep 3.68e+05/1.06e+07 =  3% of the original kernel matrix.

torch.Size([22156, 2])
We keep 1.72e+06/9.24e+07 =  1% of the original kernel matrix.

torch.Size([8760, 2])
We keep 4.33e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([22646, 2])
We keep 1.85e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([11444, 2])
We keep 8.29e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([25408, 2])
We keep 2.28e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([8309, 2])
We keep 3.61e+05/1.04e+07 =  3% of the original kernel matrix.

torch.Size([22169, 2])
We keep 1.73e+06/9.16e+07 =  1% of the original kernel matrix.

torch.Size([6962, 2])
We keep 2.83e+05/7.01e+06 =  4% of the original kernel matrix.

torch.Size([20565, 2])
We keep 1.50e+06/7.52e+07 =  1% of the original kernel matrix.

torch.Size([16246, 2])
We keep 1.44e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([30046, 2])
We keep 2.97e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([34644, 2])
We keep 4.83e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([44993, 2])
We keep 6.11e+06/5.10e+08 =  1% of the original kernel matrix.

torch.Size([8846, 2])
We keep 3.84e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([22691, 2])
We keep 1.77e+06/9.79e+07 =  1% of the original kernel matrix.

torch.Size([26675, 2])
We keep 2.82e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([38718, 2])
We keep 4.79e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([6219, 2])
We keep 2.52e+05/5.85e+06 =  4% of the original kernel matrix.

torch.Size([19797, 2])
We keep 1.40e+06/6.87e+07 =  2% of the original kernel matrix.

torch.Size([12641, 2])
We keep 9.95e+05/3.03e+07 =  3% of the original kernel matrix.

torch.Size([26683, 2])
We keep 2.54e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([6714, 2])
We keep 3.30e+05/7.75e+06 =  4% of the original kernel matrix.

torch.Size([20187, 2])
We keep 1.55e+06/7.91e+07 =  1% of the original kernel matrix.

torch.Size([17758, 2])
We keep 4.22e+06/7.04e+07 =  5% of the original kernel matrix.

torch.Size([31323, 2])
We keep 3.48e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([7885, 2])
We keep 3.87e+05/1.03e+07 =  3% of the original kernel matrix.

torch.Size([21782, 2])
We keep 1.72e+06/9.11e+07 =  1% of the original kernel matrix.

torch.Size([7185, 2])
We keep 3.05e+05/7.75e+06 =  3% of the original kernel matrix.

torch.Size([20915, 2])
We keep 1.55e+06/7.91e+07 =  1% of the original kernel matrix.

torch.Size([7837, 2])
We keep 3.33e+05/9.80e+06 =  3% of the original kernel matrix.

torch.Size([21641, 2])
We keep 1.69e+06/8.90e+07 =  1% of the original kernel matrix.

torch.Size([10995, 2])
We keep 5.56e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([24929, 2])
We keep 2.10e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([7183, 2])
We keep 2.68e+05/7.27e+06 =  3% of the original kernel matrix.

torch.Size([20929, 2])
We keep 1.51e+06/7.66e+07 =  1% of the original kernel matrix.

torch.Size([15862, 2])
We keep 1.31e+06/5.00e+07 =  2% of the original kernel matrix.

torch.Size([29621, 2])
We keep 3.04e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([7535, 2])
We keep 4.23e+05/9.31e+06 =  4% of the original kernel matrix.

torch.Size([21275, 2])
We keep 1.68e+06/8.67e+07 =  1% of the original kernel matrix.

torch.Size([36094, 2])
We keep 5.98e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([45615, 2])
We keep 6.78e+06/5.69e+08 =  1% of the original kernel matrix.

torch.Size([22470, 2])
We keep 2.19e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([35399, 2])
We keep 4.11e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([18258, 2])
We keep 1.68e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([32210, 2])
We keep 3.45e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([8097, 2])
We keep 3.44e+05/1.00e+07 =  3% of the original kernel matrix.

torch.Size([21975, 2])
We keep 1.70e+06/8.99e+07 =  1% of the original kernel matrix.

torch.Size([8594, 2])
We keep 4.65e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([22693, 2])
We keep 1.84e+06/9.80e+07 =  1% of the original kernel matrix.

torch.Size([9493, 2])
We keep 5.45e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([23568, 2])
We keep 1.97e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([8583, 2])
We keep 3.78e+05/1.06e+07 =  3% of the original kernel matrix.

torch.Size([22590, 2])
We keep 1.74e+06/9.27e+07 =  1% of the original kernel matrix.

torch.Size([9275, 2])
We keep 5.85e+05/1.57e+07 =  3% of the original kernel matrix.

torch.Size([23118, 2])
We keep 1.98e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([10399, 2])
We keep 7.30e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([24184, 2])
We keep 2.18e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([11443, 2])
We keep 6.45e+05/2.20e+07 =  2% of the original kernel matrix.

torch.Size([25631, 2])
We keep 2.25e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([17677, 2])
We keep 1.39e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([31149, 2])
We keep 3.22e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([22752, 2])
We keep 5.05e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([35597, 2])
We keep 5.07e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([11027, 2])
We keep 6.94e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([24931, 2])
We keep 2.20e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([54679, 2])
We keep 1.72e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([55489, 2])
We keep 1.01e+07/9.45e+08 =  1% of the original kernel matrix.

torch.Size([14890, 2])
We keep 9.27e+05/3.77e+07 =  2% of the original kernel matrix.

torch.Size([28597, 2])
We keep 2.70e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([16007, 2])
We keep 1.48e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([29712, 2])
We keep 3.12e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([14562, 2])
We keep 9.11e+05/3.46e+07 =  2% of the original kernel matrix.

torch.Size([28432, 2])
We keep 2.64e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([2652, 2])
We keep 5.60e+04/8.56e+05 =  6% of the original kernel matrix.

torch.Size([14224, 2])
We keep 7.51e+05/2.63e+07 =  2% of the original kernel matrix.

torch.Size([25163, 2])
We keep 3.25e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([37587, 2])
We keep 4.64e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([6703, 2])
We keep 3.23e+05/7.37e+06 =  4% of the original kernel matrix.

torch.Size([20171, 2])
We keep 1.52e+06/7.72e+07 =  1% of the original kernel matrix.

torch.Size([23586, 2])
We keep 4.50e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([36331, 2])
We keep 4.69e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([4902, 2])
We keep 1.72e+05/3.20e+06 =  5% of the original kernel matrix.

torch.Size([17890, 2])
We keep 1.15e+06/5.08e+07 =  2% of the original kernel matrix.

torch.Size([16520, 2])
We keep 1.57e+06/6.01e+07 =  2% of the original kernel matrix.

torch.Size([30160, 2])
We keep 3.22e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([7836, 2])
We keep 3.31e+05/9.47e+06 =  3% of the original kernel matrix.

torch.Size([21535, 2])
We keep 1.66e+06/8.75e+07 =  1% of the original kernel matrix.

torch.Size([7812, 2])
We keep 3.07e+05/8.77e+06 =  3% of the original kernel matrix.

torch.Size([21724, 2])
We keep 1.63e+06/8.42e+07 =  1% of the original kernel matrix.

torch.Size([15917, 2])
We keep 1.35e+06/4.99e+07 =  2% of the original kernel matrix.

torch.Size([29543, 2])
We keep 3.00e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([17540, 2])
We keep 1.31e+06/5.56e+07 =  2% of the original kernel matrix.

torch.Size([31060, 2])
We keep 3.15e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([4573, 2])
We keep 1.41e+05/3.05e+06 =  4% of the original kernel matrix.

torch.Size([17537, 2])
We keep 1.14e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([4826, 2])
We keep 1.51e+05/3.48e+06 =  4% of the original kernel matrix.

torch.Size([17792, 2])
We keep 1.19e+06/5.30e+07 =  2% of the original kernel matrix.

torch.Size([9052, 2])
We keep 6.91e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([22833, 2])
We keep 1.91e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([23403, 2])
We keep 2.64e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([36092, 2])
We keep 4.29e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([7592, 2])
We keep 3.56e+05/8.90e+06 =  3% of the original kernel matrix.

torch.Size([21514, 2])
We keep 1.61e+06/8.48e+07 =  1% of the original kernel matrix.

torch.Size([17426, 2])
We keep 1.50e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([31102, 2])
We keep 3.29e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([28987, 2])
We keep 7.78e+07/4.06e+08 = 19% of the original kernel matrix.

torch.Size([39783, 2])
We keep 6.91e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([22129, 2])
We keep 2.24e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([35021, 2])
We keep 4.04e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([9050, 2])
We keep 4.07e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([22815, 2])
We keep 1.81e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([18203, 2])
We keep 3.28e+06/8.90e+07 =  3% of the original kernel matrix.

torch.Size([31718, 2])
We keep 3.75e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([13537, 2])
We keep 8.80e+05/3.19e+07 =  2% of the original kernel matrix.

torch.Size([27349, 2])
We keep 2.57e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([13794, 2])
We keep 8.79e+05/3.33e+07 =  2% of the original kernel matrix.

torch.Size([27951, 2])
We keep 2.63e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([9364, 2])
We keep 7.26e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([23229, 2])
We keep 2.07e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([13460, 2])
We keep 1.59e+06/4.45e+07 =  3% of the original kernel matrix.

torch.Size([27109, 2])
We keep 2.88e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([12996, 2])
We keep 7.99e+05/2.80e+07 =  2% of the original kernel matrix.

torch.Size([26945, 2])
We keep 2.44e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([18233, 2])
We keep 1.47e+06/6.53e+07 =  2% of the original kernel matrix.

torch.Size([32073, 2])
We keep 3.35e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([17267, 2])
We keep 1.33e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([30876, 2])
We keep 3.12e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([10992, 2])
We keep 6.16e+05/2.08e+07 =  2% of the original kernel matrix.

torch.Size([24946, 2])
We keep 2.19e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([18281, 2])
We keep 1.77e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([31828, 2])
We keep 3.41e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([23406, 2])
We keep 2.41e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([36524, 2])
We keep 4.28e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([25387, 2])
We keep 2.98e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([37937, 2])
We keep 4.84e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([11540, 2])
We keep 7.87e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([25448, 2])
We keep 2.29e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([2904, 2])
We keep 6.17e+04/1.07e+06 =  5% of the original kernel matrix.

torch.Size([14869, 2])
We keep 8.09e+05/2.93e+07 =  2% of the original kernel matrix.

torch.Size([13526, 2])
We keep 8.17e+05/2.97e+07 =  2% of the original kernel matrix.

torch.Size([27411, 2])
We keep 2.48e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([15629, 2])
We keep 2.33e+06/5.14e+07 =  4% of the original kernel matrix.

torch.Size([29586, 2])
We keep 3.09e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([16835, 2])
We keep 2.50e+06/7.39e+07 =  3% of the original kernel matrix.

torch.Size([30487, 2])
We keep 3.49e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([11994, 2])
We keep 7.61e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([26143, 2])
We keep 2.31e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([12313, 2])
We keep 7.49e+05/2.67e+07 =  2% of the original kernel matrix.

torch.Size([26226, 2])
We keep 2.41e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([7044, 2])
We keep 2.89e+05/7.64e+06 =  3% of the original kernel matrix.

torch.Size([20685, 2])
We keep 1.53e+06/7.86e+07 =  1% of the original kernel matrix.

torch.Size([14384, 2])
We keep 1.40e+06/3.99e+07 =  3% of the original kernel matrix.

torch.Size([28200, 2])
We keep 2.82e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([5264, 2])
We keep 1.64e+05/4.01e+06 =  4% of the original kernel matrix.

torch.Size([18609, 2])
We keep 1.23e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([14256, 2])
We keep 8.27e+05/3.26e+07 =  2% of the original kernel matrix.

torch.Size([28554, 2])
We keep 2.59e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([9944, 2])
We keep 4.63e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([23958, 2])
We keep 1.95e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([26569, 2])
We keep 2.64e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([39283, 2])
We keep 4.64e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([10647, 2])
We keep 5.72e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([24645, 2])
We keep 2.13e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([19799, 2])
We keep 1.80e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([33115, 2])
We keep 3.66e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([7651, 2])
We keep 2.95e+05/8.21e+06 =  3% of the original kernel matrix.

torch.Size([21551, 2])
We keep 1.59e+06/8.14e+07 =  1% of the original kernel matrix.

torch.Size([14292, 2])
We keep 1.34e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([28373, 2])
We keep 2.83e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([15166, 2])
We keep 1.12e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([29244, 2])
We keep 2.90e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([11737, 2])
We keep 6.76e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([25779, 2])
We keep 2.25e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([25213, 2])
We keep 3.32e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([37774, 2])
We keep 4.68e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([18434, 2])
We keep 1.81e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([31917, 2])
We keep 3.50e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([18047, 2])
We keep 4.37e+06/9.22e+07 =  4% of the original kernel matrix.

torch.Size([31444, 2])
We keep 3.78e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([13651, 2])
We keep 8.80e+05/3.23e+07 =  2% of the original kernel matrix.

torch.Size([27403, 2])
We keep 2.56e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([14465, 2])
We keep 9.62e+05/3.65e+07 =  2% of the original kernel matrix.

torch.Size([28175, 2])
We keep 2.71e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([4224, 2])
We keep 1.37e+05/2.83e+06 =  4% of the original kernel matrix.

torch.Size([16698, 2])
We keep 1.10e+06/4.78e+07 =  2% of the original kernel matrix.

torch.Size([6457, 2])
We keep 2.79e+05/6.22e+06 =  4% of the original kernel matrix.

torch.Size([19916, 2])
We keep 1.43e+06/7.09e+07 =  2% of the original kernel matrix.

torch.Size([4677, 2])
We keep 1.43e+05/2.94e+06 =  4% of the original kernel matrix.

torch.Size([17643, 2])
We keep 1.12e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([6001, 2])
We keep 2.08e+05/4.93e+06 =  4% of the original kernel matrix.

torch.Size([19476, 2])
We keep 1.33e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([40321, 2])
We keep 1.02e+07/5.34e+08 =  1% of the original kernel matrix.

torch.Size([48023, 2])
We keep 7.56e+06/6.57e+08 =  1% of the original kernel matrix.

torch.Size([10722, 2])
We keep 5.67e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([24742, 2])
We keep 2.10e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([10528, 2])
We keep 7.79e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([24587, 2])
We keep 2.21e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([5351, 2])
We keep 1.83e+05/4.12e+06 =  4% of the original kernel matrix.

torch.Size([18583, 2])
We keep 1.25e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([22581, 2])
We keep 2.46e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([35337, 2])
We keep 4.25e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([13067, 2])
We keep 1.81e+06/4.09e+07 =  4% of the original kernel matrix.

torch.Size([26840, 2])
We keep 2.82e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([12161, 2])
We keep 8.43e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([26061, 2])
We keep 2.43e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([15394, 2])
We keep 1.39e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([29524, 2])
We keep 2.97e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([7306, 2])
We keep 2.82e+05/7.91e+06 =  3% of the original kernel matrix.

torch.Size([21262, 2])
We keep 1.57e+06/8.00e+07 =  1% of the original kernel matrix.

torch.Size([7603, 2])
We keep 3.21e+05/8.53e+06 =  3% of the original kernel matrix.

torch.Size([21519, 2])
We keep 1.62e+06/8.30e+07 =  1% of the original kernel matrix.

torch.Size([17290, 2])
We keep 1.44e+06/6.08e+07 =  2% of the original kernel matrix.

torch.Size([30982, 2])
We keep 3.28e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([3613, 2])
We keep 8.79e+04/1.65e+06 =  5% of the original kernel matrix.

torch.Size([16094, 2])
We keep 9.19e+05/3.65e+07 =  2% of the original kernel matrix.

torch.Size([10064, 2])
We keep 7.13e+05/1.78e+07 =  4% of the original kernel matrix.

torch.Size([24087, 2])
We keep 2.07e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([11138, 2])
We keep 6.82e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([25136, 2])
We keep 2.24e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([11629, 2])
We keep 7.44e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([25614, 2])
We keep 2.33e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([17055, 2])
We keep 1.75e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([31176, 2])
We keep 3.25e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([12779, 2])
We keep 9.43e+05/3.02e+07 =  3% of the original kernel matrix.

torch.Size([26755, 2])
We keep 2.56e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([12937, 2])
We keep 1.39e+06/3.27e+07 =  4% of the original kernel matrix.

torch.Size([26817, 2])
We keep 2.59e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([15612, 2])
We keep 1.14e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([29496, 2])
We keep 2.95e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([18169, 2])
We keep 1.50e+06/6.26e+07 =  2% of the original kernel matrix.

torch.Size([32133, 2])
We keep 3.30e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([11569, 2])
We keep 2.31e+06/2.48e+07 =  9% of the original kernel matrix.

torch.Size([25573, 2])
We keep 2.38e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([50592, 2])
We keep 2.10e+07/9.98e+08 =  2% of the original kernel matrix.

torch.Size([53043, 2])
We keep 9.78e+06/8.98e+08 =  1% of the original kernel matrix.

torch.Size([96684, 2])
We keep 1.50e+08/6.43e+09 =  2% of the original kernel matrix.

torch.Size([70983, 2])
We keep 2.09e+07/2.28e+09 =  0% of the original kernel matrix.

torch.Size([17590, 2])
We keep 1.76e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([31383, 2])
We keep 3.24e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([15704, 2])
We keep 1.99e+06/5.71e+07 =  3% of the original kernel matrix.

torch.Size([29357, 2])
We keep 3.23e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([4603, 2])
We keep 1.61e+05/3.15e+06 =  5% of the original kernel matrix.

torch.Size([17342, 2])
We keep 1.15e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([23638, 2])
We keep 3.43e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([36347, 2])
We keep 4.29e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([29845, 2])
We keep 8.17e+06/2.50e+08 =  3% of the original kernel matrix.

torch.Size([41151, 2])
We keep 5.48e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([248037, 2])
We keep 6.41e+08/4.14e+10 =  1% of the original kernel matrix.

torch.Size([116361, 2])
We keep 4.61e+07/5.78e+09 =  0% of the original kernel matrix.

torch.Size([14981, 2])
We keep 1.07e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([28786, 2])
We keep 2.82e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([395595, 2])
We keep 4.66e+08/5.87e+10 =  0% of the original kernel matrix.

torch.Size([151242, 2])
We keep 5.43e+07/6.89e+09 =  0% of the original kernel matrix.

torch.Size([123204, 2])
We keep 2.77e+08/6.89e+09 =  4% of the original kernel matrix.

torch.Size([81806, 2])
We keep 2.12e+07/2.36e+09 =  0% of the original kernel matrix.

torch.Size([84732, 2])
We keep 4.07e+07/2.56e+09 =  1% of the original kernel matrix.

torch.Size([67725, 2])
We keep 1.41e+07/1.44e+09 =  0% of the original kernel matrix.

torch.Size([210011, 2])
We keep 4.72e+09/3.75e+10 = 12% of the original kernel matrix.

torch.Size([108362, 2])
We keep 4.41e+07/5.50e+09 =  0% of the original kernel matrix.

torch.Size([10359, 2])
We keep 6.23e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([24543, 2])
We keep 2.07e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([15908, 2])
We keep 1.97e+06/5.77e+07 =  3% of the original kernel matrix.

torch.Size([29774, 2])
We keep 3.18e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([677363, 2])
We keep 1.13e+09/1.49e+11 =  0% of the original kernel matrix.

torch.Size([196804, 2])
We keep 8.31e+07/1.10e+10 =  0% of the original kernel matrix.

torch.Size([1207430, 2])
We keep 2.50e+09/4.01e+11 =  0% of the original kernel matrix.

torch.Size([275062, 2])
We keep 1.29e+08/1.80e+10 =  0% of the original kernel matrix.

torch.Size([15753, 2])
We keep 1.58e+06/4.87e+07 =  3% of the original kernel matrix.

torch.Size([29537, 2])
We keep 3.06e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([104732, 2])
We keep 4.36e+07/3.17e+09 =  1% of the original kernel matrix.

torch.Size([75208, 2])
We keep 1.55e+07/1.60e+09 =  0% of the original kernel matrix.

torch.Size([51924, 2])
We keep 4.44e+07/1.01e+09 =  4% of the original kernel matrix.

torch.Size([54356, 2])
We keep 9.75e+06/9.01e+08 =  1% of the original kernel matrix.

torch.Size([62470, 2])
We keep 4.03e+07/1.33e+09 =  3% of the original kernel matrix.

torch.Size([59637, 2])
We keep 1.09e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([155171, 2])
We keep 2.81e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([90277, 2])
We keep 3.12e+07/3.65e+09 =  0% of the original kernel matrix.

torch.Size([86289, 2])
We keep 2.24e+08/5.82e+09 =  3% of the original kernel matrix.

torch.Size([65081, 2])
We keep 2.03e+07/2.17e+09 =  0% of the original kernel matrix.

torch.Size([57236, 2])
We keep 2.30e+07/1.15e+09 =  2% of the original kernel matrix.

torch.Size([56877, 2])
We keep 1.04e+07/9.63e+08 =  1% of the original kernel matrix.

torch.Size([46902, 2])
We keep 1.47e+07/7.11e+08 =  2% of the original kernel matrix.

torch.Size([51867, 2])
We keep 8.51e+06/7.58e+08 =  1% of the original kernel matrix.

torch.Size([11862, 2])
We keep 9.12e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([25982, 2])
We keep 2.36e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([222562, 2])
We keep 6.61e+08/4.09e+10 =  1% of the original kernel matrix.

torch.Size([106099, 2])
We keep 4.55e+07/5.75e+09 =  0% of the original kernel matrix.

torch.Size([23018, 2])
We keep 4.43e+06/1.31e+08 =  3% of the original kernel matrix.

torch.Size([35588, 2])
We keep 4.39e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([13443, 2])
We keep 9.92e+05/3.41e+07 =  2% of the original kernel matrix.

torch.Size([27201, 2])
We keep 2.63e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([54692, 2])
We keep 1.69e+07/9.64e+08 =  1% of the original kernel matrix.

torch.Size([56017, 2])
We keep 9.43e+06/8.82e+08 =  1% of the original kernel matrix.

torch.Size([152632, 2])
We keep 2.01e+08/1.10e+10 =  1% of the original kernel matrix.

torch.Size([90102, 2])
We keep 2.63e+07/2.98e+09 =  0% of the original kernel matrix.

torch.Size([554229, 2])
We keep 8.00e+08/1.01e+11 =  0% of the original kernel matrix.

torch.Size([181361, 2])
We keep 7.02e+07/9.04e+09 =  0% of the original kernel matrix.

torch.Size([210555, 2])
We keep 1.20e+08/1.39e+10 =  0% of the original kernel matrix.

torch.Size([108895, 2])
We keep 2.90e+07/3.35e+09 =  0% of the original kernel matrix.

torch.Size([9129, 2])
We keep 4.17e+05/1.26e+07 =  3% of the original kernel matrix.

torch.Size([23041, 2])
We keep 1.83e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([89277, 2])
We keep 4.51e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([68842, 2])
We keep 1.48e+07/1.50e+09 =  0% of the original kernel matrix.

torch.Size([24884, 2])
We keep 2.97e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([37513, 2])
We keep 4.63e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([74632, 2])
We keep 4.23e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([63464, 2])
We keep 1.38e+07/1.39e+09 =  0% of the original kernel matrix.

torch.Size([165113, 2])
We keep 2.22e+08/8.51e+09 =  2% of the original kernel matrix.

torch.Size([95906, 2])
We keep 2.36e+07/2.62e+09 =  0% of the original kernel matrix.

torch.Size([43268, 2])
We keep 2.61e+07/7.15e+08 =  3% of the original kernel matrix.

torch.Size([49733, 2])
We keep 8.66e+06/7.60e+08 =  1% of the original kernel matrix.

torch.Size([186711, 2])
We keep 1.08e+08/1.10e+10 =  0% of the original kernel matrix.

torch.Size([101711, 2])
We keep 2.62e+07/2.99e+09 =  0% of the original kernel matrix.

torch.Size([3900, 2])
We keep 9.55e+04/2.12e+06 =  4% of the original kernel matrix.

torch.Size([16565, 2])
We keep 9.99e+05/4.14e+07 =  2% of the original kernel matrix.

torch.Size([24454, 2])
We keep 2.73e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([37077, 2])
We keep 4.38e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([194314, 2])
We keep 1.69e+08/1.55e+10 =  1% of the original kernel matrix.

torch.Size([102525, 2])
We keep 3.03e+07/3.54e+09 =  0% of the original kernel matrix.

torch.Size([126330, 2])
We keep 1.09e+08/6.94e+09 =  1% of the original kernel matrix.

torch.Size([82495, 2])
We keep 2.17e+07/2.37e+09 =  0% of the original kernel matrix.

torch.Size([184839, 2])
We keep 1.75e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([100654, 2])
We keep 2.90e+07/3.36e+09 =  0% of the original kernel matrix.

torch.Size([13152, 2])
We keep 9.68e+05/3.11e+07 =  3% of the original kernel matrix.

torch.Size([27158, 2])
We keep 2.53e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([556682, 2])
We keep 6.48e+08/1.11e+11 =  0% of the original kernel matrix.

torch.Size([180707, 2])
We keep 7.13e+07/9.46e+09 =  0% of the original kernel matrix.

torch.Size([40647, 2])
We keep 6.29e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([48858, 2])
We keep 7.16e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([73406, 2])
We keep 2.30e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([63961, 2])
We keep 1.21e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([196880, 2])
We keep 1.34e+08/1.34e+10 =  0% of the original kernel matrix.

torch.Size([105012, 2])
We keep 2.87e+07/3.29e+09 =  0% of the original kernel matrix.

torch.Size([10640, 2])
We keep 5.32e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([24727, 2])
We keep 2.06e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([239194, 2])
We keep 2.64e+08/2.12e+10 =  1% of the original kernel matrix.

torch.Size([115646, 2])
We keep 3.51e+07/4.14e+09 =  0% of the original kernel matrix.

torch.Size([503923, 2])
We keep 1.41e+09/9.97e+10 =  1% of the original kernel matrix.

torch.Size([171027, 2])
We keep 6.96e+07/8.98e+09 =  0% of the original kernel matrix.

torch.Size([1695867, 2])
We keep 7.96e+09/8.28e+11 =  0% of the original kernel matrix.

torch.Size([322914, 2])
We keep 1.82e+08/2.59e+10 =  0% of the original kernel matrix.

torch.Size([83925, 2])
We keep 3.03e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([67975, 2])
We keep 1.37e+07/1.37e+09 =  0% of the original kernel matrix.

torch.Size([71060, 2])
We keep 2.45e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([62353, 2])
We keep 1.20e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([20552, 2])
We keep 3.82e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([33630, 2])
We keep 4.29e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([19195, 2])
We keep 1.64e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([32365, 2])
We keep 3.57e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([314671, 2])
We keep 4.90e+08/4.51e+10 =  1% of the original kernel matrix.

torch.Size([133583, 2])
We keep 4.90e+07/6.04e+09 =  0% of the original kernel matrix.

torch.Size([27286, 2])
We keep 3.42e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([39502, 2])
We keep 5.05e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([65544, 2])
We keep 4.63e+07/1.76e+09 =  2% of the original kernel matrix.

torch.Size([59897, 2])
We keep 1.22e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([57073, 2])
We keep 3.12e+07/1.12e+09 =  2% of the original kernel matrix.

torch.Size([56923, 2])
We keep 9.56e+06/9.52e+08 =  1% of the original kernel matrix.

torch.Size([62387, 2])
We keep 4.52e+07/1.28e+09 =  3% of the original kernel matrix.

torch.Size([59247, 2])
We keep 1.06e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([35559, 2])
We keep 1.84e+07/6.08e+08 =  3% of the original kernel matrix.

torch.Size([44095, 2])
We keep 8.15e+06/7.01e+08 =  1% of the original kernel matrix.

torch.Size([17299, 2])
We keep 1.26e+06/5.59e+07 =  2% of the original kernel matrix.

torch.Size([31020, 2])
We keep 3.13e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([193191, 2])
We keep 6.13e+08/4.18e+10 =  1% of the original kernel matrix.

torch.Size([98453, 2])
We keep 4.76e+07/5.81e+09 =  0% of the original kernel matrix.

torch.Size([84342, 2])
We keep 4.75e+08/3.69e+09 = 12% of the original kernel matrix.

torch.Size([67339, 2])
We keep 1.68e+07/1.73e+09 =  0% of the original kernel matrix.

torch.Size([37609, 2])
We keep 4.35e+07/8.13e+08 =  5% of the original kernel matrix.

torch.Size([44627, 2])
We keep 9.05e+06/8.10e+08 =  1% of the original kernel matrix.

torch.Size([23131, 2])
We keep 2.40e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([36434, 2])
We keep 4.21e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([159866, 2])
We keep 2.03e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([94071, 2])
We keep 2.62e+07/3.05e+09 =  0% of the original kernel matrix.

torch.Size([44371, 2])
We keep 9.26e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([50842, 2])
We keep 7.82e+06/6.93e+08 =  1% of the original kernel matrix.

torch.Size([351019, 2])
We keep 8.77e+08/6.10e+10 =  1% of the original kernel matrix.

torch.Size([139262, 2])
We keep 5.57e+07/7.02e+09 =  0% of the original kernel matrix.

torch.Size([23015, 2])
We keep 4.34e+06/1.45e+08 =  3% of the original kernel matrix.

torch.Size([35880, 2])
We keep 4.31e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([63545, 2])
We keep 5.09e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([56972, 2])
We keep 1.44e+07/1.45e+09 =  0% of the original kernel matrix.

torch.Size([29383, 2])
We keep 3.90e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([40974, 2])
We keep 5.49e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([334079, 2])
We keep 7.68e+08/5.88e+10 =  1% of the original kernel matrix.

torch.Size([136268, 2])
We keep 5.47e+07/6.89e+09 =  0% of the original kernel matrix.

torch.Size([86417, 2])
We keep 3.14e+07/3.20e+09 =  0% of the original kernel matrix.

torch.Size([68817, 2])
We keep 1.54e+07/1.61e+09 =  0% of the original kernel matrix.

torch.Size([31106, 2])
We keep 4.29e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([42082, 2])
We keep 5.56e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([309045, 2])
We keep 1.98e+08/3.02e+10 =  0% of the original kernel matrix.

torch.Size([133839, 2])
We keep 4.06e+07/4.94e+09 =  0% of the original kernel matrix.

torch.Size([413591, 2])
We keep 3.81e+08/5.62e+10 =  0% of the original kernel matrix.

torch.Size([157390, 2])
We keep 5.36e+07/6.74e+09 =  0% of the original kernel matrix.

torch.Size([26226, 2])
We keep 5.67e+07/2.60e+08 = 21% of the original kernel matrix.

torch.Size([38128, 2])
We keep 5.52e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([294939, 2])
We keep 6.33e+08/3.19e+10 =  1% of the original kernel matrix.

torch.Size([130910, 2])
We keep 4.10e+07/5.08e+09 =  0% of the original kernel matrix.

torch.Size([337733, 2])
We keep 2.73e+08/4.10e+10 =  0% of the original kernel matrix.

torch.Size([142226, 2])
We keep 4.65e+07/5.76e+09 =  0% of the original kernel matrix.

torch.Size([976193, 2])
We keep 1.60e+09/2.72e+11 =  0% of the original kernel matrix.

torch.Size([247455, 2])
We keep 1.07e+08/1.48e+10 =  0% of the original kernel matrix.

torch.Size([45928, 2])
We keep 2.00e+07/6.97e+08 =  2% of the original kernel matrix.

torch.Size([51452, 2])
We keep 8.60e+06/7.50e+08 =  1% of the original kernel matrix.

torch.Size([10011, 2])
We keep 6.49e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([24023, 2])
We keep 2.06e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([34557, 2])
We keep 5.07e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([44609, 2])
We keep 6.18e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([25937, 2])
We keep 5.57e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([38180, 2])
We keep 5.02e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([60260, 2])
We keep 1.44e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([58533, 2])
We keep 9.92e+06/9.42e+08 =  1% of the original kernel matrix.

torch.Size([99971, 2])
We keep 9.03e+07/3.40e+09 =  2% of the original kernel matrix.

torch.Size([73391, 2])
We keep 1.60e+07/1.66e+09 =  0% of the original kernel matrix.

torch.Size([8767, 2])
We keep 3.82e+05/1.12e+07 =  3% of the original kernel matrix.

torch.Size([22901, 2])
We keep 1.79e+06/9.51e+07 =  1% of the original kernel matrix.

torch.Size([176468, 2])
We keep 1.23e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([98949, 2])
We keep 2.57e+07/2.88e+09 =  0% of the original kernel matrix.

torch.Size([401238, 2])
We keep 3.86e+08/5.52e+10 =  0% of the original kernel matrix.

torch.Size([154113, 2])
We keep 5.31e+07/6.68e+09 =  0% of the original kernel matrix.

torch.Size([22102, 2])
We keep 6.33e+07/3.76e+08 = 16% of the original kernel matrix.

torch.Size([34197, 2])
We keep 6.00e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([21579, 2])
We keep 3.10e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([34601, 2])
We keep 4.22e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([101706, 2])
We keep 3.94e+07/3.52e+09 =  1% of the original kernel matrix.

torch.Size([73908, 2])
We keep 1.62e+07/1.69e+09 =  0% of the original kernel matrix.

torch.Size([32275, 2])
We keep 4.53e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([43477, 2])
We keep 5.99e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([42398, 2])
We keep 1.91e+07/6.80e+08 =  2% of the original kernel matrix.

torch.Size([49078, 2])
We keep 8.42e+06/7.41e+08 =  1% of the original kernel matrix.

torch.Size([34512, 2])
We keep 4.92e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([44897, 2])
We keep 6.35e+06/5.29e+08 =  1% of the original kernel matrix.

torch.Size([21905, 2])
We keep 9.73e+06/1.08e+08 =  9% of the original kernel matrix.

torch.Size([34879, 2])
We keep 3.96e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([121972, 2])
We keep 1.86e+08/5.90e+09 =  3% of the original kernel matrix.

torch.Size([80753, 2])
We keep 1.97e+07/2.18e+09 =  0% of the original kernel matrix.

torch.Size([100345, 2])
We keep 3.00e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([73837, 2])
We keep 1.48e+07/1.52e+09 =  0% of the original kernel matrix.

torch.Size([47075, 2])
We keep 1.41e+07/7.59e+08 =  1% of the original kernel matrix.

torch.Size([50263, 2])
We keep 8.29e+06/7.83e+08 =  1% of the original kernel matrix.

torch.Size([109766, 2])
We keep 6.67e+07/4.53e+09 =  1% of the original kernel matrix.

torch.Size([77339, 2])
We keep 1.80e+07/1.91e+09 =  0% of the original kernel matrix.

torch.Size([23937, 2])
We keep 3.50e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([36616, 2])
We keep 4.66e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([2300896, 2])
We keep 2.02e+10/1.37e+12 =  1% of the original kernel matrix.

torch.Size([382125, 2])
We keep 2.29e+08/3.32e+10 =  0% of the original kernel matrix.

torch.Size([30279, 2])
We keep 3.97e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([41699, 2])
We keep 5.59e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([229572, 2])
We keep 1.60e+08/1.86e+10 =  0% of the original kernel matrix.

torch.Size([113793, 2])
We keep 3.28e+07/3.88e+09 =  0% of the original kernel matrix.

torch.Size([7781, 2])
We keep 3.00e+05/8.81e+06 =  3% of the original kernel matrix.

torch.Size([21685, 2])
We keep 1.63e+06/8.44e+07 =  1% of the original kernel matrix.

torch.Size([15577, 2])
We keep 1.21e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([29254, 2])
We keep 2.88e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([13940, 2])
We keep 8.99e+05/3.40e+07 =  2% of the original kernel matrix.

torch.Size([27806, 2])
We keep 2.63e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([30595, 2])
We keep 5.66e+06/2.49e+08 =  2% of the original kernel matrix.

torch.Size([41814, 2])
We keep 5.53e+06/4.48e+08 =  1% of the original kernel matrix.

torch.Size([18593, 2])
We keep 1.69e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([32096, 2])
We keep 3.40e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([188367, 2])
We keep 3.11e+08/1.50e+10 =  2% of the original kernel matrix.

torch.Size([102262, 2])
We keep 3.03e+07/3.48e+09 =  0% of the original kernel matrix.

torch.Size([14762, 2])
We keep 1.02e+06/3.97e+07 =  2% of the original kernel matrix.

torch.Size([28439, 2])
We keep 2.77e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([15738, 2])
We keep 1.13e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([29498, 2])
We keep 2.90e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([613702, 2])
We keep 9.11e+08/1.18e+11 =  0% of the original kernel matrix.

torch.Size([188696, 2])
We keep 7.49e+07/9.77e+09 =  0% of the original kernel matrix.

torch.Size([130096, 2])
We keep 8.26e+07/6.04e+09 =  1% of the original kernel matrix.

torch.Size([83742, 2])
We keep 2.03e+07/2.21e+09 =  0% of the original kernel matrix.

torch.Size([87861, 2])
We keep 3.54e+07/2.67e+09 =  1% of the original kernel matrix.

torch.Size([69172, 2])
We keep 1.44e+07/1.47e+09 =  0% of the original kernel matrix.

torch.Size([286045, 2])
We keep 2.37e+08/2.85e+10 =  0% of the original kernel matrix.

torch.Size([129201, 2])
We keep 3.96e+07/4.80e+09 =  0% of the original kernel matrix.

torch.Size([842475, 2])
We keep 1.19e+09/2.03e+11 =  0% of the original kernel matrix.

torch.Size([224244, 2])
We keep 9.54e+07/1.28e+10 =  0% of the original kernel matrix.

torch.Size([16753, 2])
We keep 1.25e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([30284, 2])
We keep 3.05e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([22350, 2])
We keep 2.18e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([35159, 2])
We keep 4.10e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([54817, 2])
We keep 3.16e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([55173, 2])
We keep 1.05e+07/9.97e+08 =  1% of the original kernel matrix.

torch.Size([9540, 2])
We keep 5.57e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([23503, 2])
We keep 2.00e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([20369, 2])
We keep 1.82e+06/9.06e+07 =  2% of the original kernel matrix.

torch.Size([33504, 2])
We keep 3.76e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([35982, 2])
We keep 7.25e+06/4.18e+08 =  1% of the original kernel matrix.

torch.Size([45264, 2])
We keep 6.77e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([16816, 2])
We keep 1.50e+06/5.62e+07 =  2% of the original kernel matrix.

torch.Size([30342, 2])
We keep 3.16e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([21851, 2])
We keep 2.14e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([34858, 2])
We keep 4.04e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([21654, 2])
We keep 2.20e+06/9.90e+07 =  2% of the original kernel matrix.

torch.Size([34633, 2])
We keep 3.84e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([25116, 2])
We keep 3.36e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([37444, 2])
We keep 4.73e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([846528, 2])
We keep 1.53e+09/2.26e+11 =  0% of the original kernel matrix.

torch.Size([224540, 2])
We keep 9.93e+07/1.35e+10 =  0% of the original kernel matrix.

torch.Size([33369, 2])
We keep 9.61e+06/4.47e+08 =  2% of the original kernel matrix.

torch.Size([43064, 2])
We keep 7.00e+06/6.01e+08 =  1% of the original kernel matrix.

torch.Size([187659, 2])
We keep 1.59e+08/1.29e+10 =  1% of the original kernel matrix.

torch.Size([102923, 2])
We keep 2.84e+07/3.23e+09 =  0% of the original kernel matrix.

torch.Size([18465, 2])
We keep 3.75e+06/9.17e+07 =  4% of the original kernel matrix.

torch.Size([32131, 2])
We keep 3.83e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([14004, 2])
We keep 1.16e+06/3.99e+07 =  2% of the original kernel matrix.

torch.Size([27899, 2])
We keep 2.78e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([176594, 2])
We keep 1.16e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([98655, 2])
We keep 2.55e+07/2.91e+09 =  0% of the original kernel matrix.

torch.Size([565513, 2])
We keep 1.62e+09/1.08e+11 =  1% of the original kernel matrix.

torch.Size([181312, 2])
We keep 6.94e+07/9.33e+09 =  0% of the original kernel matrix.

torch.Size([1165574, 2])
We keep 1.86e+09/3.70e+11 =  0% of the original kernel matrix.

torch.Size([269506, 2])
We keep 1.23e+08/1.73e+10 =  0% of the original kernel matrix.

torch.Size([50455, 2])
We keep 2.30e+07/7.55e+08 =  3% of the original kernel matrix.

torch.Size([51065, 2])
We keep 7.93e+06/7.81e+08 =  1% of the original kernel matrix.

torch.Size([103468, 2])
We keep 5.12e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([75033, 2])
We keep 1.58e+07/1.64e+09 =  0% of the original kernel matrix.

torch.Size([11374, 2])
We keep 6.11e+05/2.13e+07 =  2% of the original kernel matrix.

torch.Size([25399, 2])
We keep 2.20e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([27899, 2])
We keep 2.79e+07/3.16e+08 =  8% of the original kernel matrix.

torch.Size([39440, 2])
We keep 5.79e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([1725375, 2])
We keep 1.57e+10/1.41e+12 =  1% of the original kernel matrix.

torch.Size([315119, 2])
We keep 2.33e+08/3.38e+10 =  0% of the original kernel matrix.

torch.Size([50175, 2])
We keep 2.10e+07/8.11e+08 =  2% of the original kernel matrix.

torch.Size([53612, 2])
We keep 8.70e+06/8.09e+08 =  1% of the original kernel matrix.

torch.Size([23557, 2])
We keep 2.54e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([36134, 2])
We keep 4.33e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([96765, 2])
We keep 5.70e+07/3.58e+09 =  1% of the original kernel matrix.

torch.Size([72491, 2])
We keep 1.61e+07/1.70e+09 =  0% of the original kernel matrix.

torch.Size([1456312, 2])
We keep 5.00e+09/7.17e+11 =  0% of the original kernel matrix.

torch.Size([296138, 2])
We keep 1.70e+08/2.41e+10 =  0% of the original kernel matrix.

torch.Size([130243, 2])
We keep 6.51e+07/5.68e+09 =  1% of the original kernel matrix.

torch.Size([83092, 2])
We keep 1.98e+07/2.14e+09 =  0% of the original kernel matrix.

torch.Size([32879, 2])
We keep 6.03e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([43152, 2])
We keep 6.16e+06/5.14e+08 =  1% of the original kernel matrix.

torch.Size([41490, 2])
We keep 1.06e+07/5.35e+08 =  1% of the original kernel matrix.

torch.Size([49718, 2])
We keep 7.56e+06/6.57e+08 =  1% of the original kernel matrix.

torch.Size([29580, 2])
We keep 3.59e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([41183, 2])
We keep 5.42e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([14592, 2])
We keep 1.09e+06/3.73e+07 =  2% of the original kernel matrix.

torch.Size([28416, 2])
We keep 2.72e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([387315, 2])
We keep 4.50e+08/5.32e+10 =  0% of the original kernel matrix.

torch.Size([151332, 2])
We keep 5.18e+07/6.55e+09 =  0% of the original kernel matrix.

torch.Size([45705, 2])
We keep 1.38e+07/6.89e+08 =  2% of the original kernel matrix.

torch.Size([51669, 2])
We keep 8.50e+06/7.46e+08 =  1% of the original kernel matrix.

torch.Size([7741, 2])
We keep 3.93e+05/9.42e+06 =  4% of the original kernel matrix.

torch.Size([21569, 2])
We keep 1.67e+06/8.72e+07 =  1% of the original kernel matrix.

torch.Size([175608, 2])
We keep 1.14e+08/9.47e+09 =  1% of the original kernel matrix.

torch.Size([98837, 2])
We keep 2.42e+07/2.77e+09 =  0% of the original kernel matrix.

torch.Size([44794, 2])
We keep 7.71e+06/5.92e+08 =  1% of the original kernel matrix.

torch.Size([52179, 2])
We keep 7.87e+06/6.92e+08 =  1% of the original kernel matrix.

torch.Size([6466, 2])
We keep 2.38e+05/6.15e+06 =  3% of the original kernel matrix.

torch.Size([20053, 2])
We keep 1.44e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([164341, 2])
We keep 1.18e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([94675, 2])
We keep 2.51e+07/2.86e+09 =  0% of the original kernel matrix.

torch.Size([233978, 2])
We keep 1.76e+08/1.78e+10 =  0% of the original kernel matrix.

torch.Size([115324, 2])
We keep 3.22e+07/3.79e+09 =  0% of the original kernel matrix.

torch.Size([425286, 2])
We keep 1.02e+09/7.74e+10 =  1% of the original kernel matrix.

torch.Size([157698, 2])
We keep 6.27e+07/7.91e+09 =  0% of the original kernel matrix.

torch.Size([439956, 2])
We keep 8.49e+08/7.26e+10 =  1% of the original kernel matrix.

torch.Size([159191, 2])
We keep 6.08e+07/7.66e+09 =  0% of the original kernel matrix.

torch.Size([524336, 2])
We keep 8.90e+08/9.76e+10 =  0% of the original kernel matrix.

torch.Size([173580, 2])
We keep 6.86e+07/8.88e+09 =  0% of the original kernel matrix.

torch.Size([64313, 2])
We keep 2.62e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([59405, 2])
We keep 1.14e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([73671, 2])
We keep 5.01e+07/1.82e+09 =  2% of the original kernel matrix.

torch.Size([63888, 2])
We keep 1.23e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([310269, 2])
We keep 2.64e+08/3.24e+10 =  0% of the original kernel matrix.

torch.Size([134510, 2])
We keep 4.18e+07/5.12e+09 =  0% of the original kernel matrix.

torch.Size([522952, 2])
We keep 6.21e+08/8.39e+10 =  0% of the original kernel matrix.

torch.Size([175174, 2])
We keep 6.36e+07/8.23e+09 =  0% of the original kernel matrix.

torch.Size([32158, 2])
We keep 6.75e+06/3.18e+08 =  2% of the original kernel matrix.

torch.Size([42890, 2])
We keep 6.09e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([172772, 2])
We keep 1.33e+09/1.66e+10 =  7% of the original kernel matrix.

torch.Size([99217, 2])
We keep 3.11e+07/3.66e+09 =  0% of the original kernel matrix.

torch.Size([449459, 2])
We keep 5.43e+09/9.86e+10 =  5% of the original kernel matrix.

torch.Size([159200, 2])
We keep 6.90e+07/8.92e+09 =  0% of the original kernel matrix.

torch.Size([376133, 2])
We keep 7.88e+08/6.24e+10 =  1% of the original kernel matrix.

torch.Size([147173, 2])
We keep 5.64e+07/7.10e+09 =  0% of the original kernel matrix.

torch.Size([226282, 2])
We keep 3.05e+08/2.27e+10 =  1% of the original kernel matrix.

torch.Size([111323, 2])
We keep 3.58e+07/4.28e+09 =  0% of the original kernel matrix.

torch.Size([80416, 2])
We keep 1.37e+08/3.74e+09 =  3% of the original kernel matrix.

torch.Size([64678, 2])
We keep 1.60e+07/1.74e+09 =  0% of the original kernel matrix.

torch.Size([33000, 2])
We keep 7.56e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([43030, 2])
We keep 6.58e+06/5.53e+08 =  1% of the original kernel matrix.

torch.Size([19901, 2])
We keep 1.69e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([33264, 2])
We keep 3.59e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([52794, 2])
We keep 2.10e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([54339, 2])
We keep 9.84e+06/9.14e+08 =  1% of the original kernel matrix.

torch.Size([319498, 2])
We keep 2.24e+08/3.26e+10 =  0% of the original kernel matrix.

torch.Size([137201, 2])
We keep 4.20e+07/5.13e+09 =  0% of the original kernel matrix.

torch.Size([279086, 2])
We keep 1.73e+08/2.46e+10 =  0% of the original kernel matrix.

torch.Size([127408, 2])
We keep 3.69e+07/4.46e+09 =  0% of the original kernel matrix.

torch.Size([516509, 2])
We keep 1.20e+09/1.14e+11 =  1% of the original kernel matrix.

torch.Size([172189, 2])
We keep 7.34e+07/9.59e+09 =  0% of the original kernel matrix.

torch.Size([278240, 2])
We keep 2.82e+08/2.81e+10 =  1% of the original kernel matrix.

torch.Size([127434, 2])
We keep 3.94e+07/4.76e+09 =  0% of the original kernel matrix.

torch.Size([162176, 2])
We keep 9.81e+07/8.59e+09 =  1% of the original kernel matrix.

torch.Size([94812, 2])
We keep 2.35e+07/2.63e+09 =  0% of the original kernel matrix.

torch.Size([39314, 2])
We keep 7.60e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([46683, 2])
We keep 7.62e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([561019, 2])
We keep 8.69e+08/9.68e+10 =  0% of the original kernel matrix.

torch.Size([180535, 2])
We keep 6.88e+07/8.84e+09 =  0% of the original kernel matrix.

torch.Size([16387, 2])
We keep 3.06e+06/7.93e+07 =  3% of the original kernel matrix.

torch.Size([29740, 2])
We keep 3.62e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([29996, 2])
We keep 4.90e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([41287, 2])
We keep 5.84e+06/4.68e+08 =  1% of the original kernel matrix.

torch.Size([361725, 2])
We keep 3.23e+08/4.43e+10 =  0% of the original kernel matrix.

torch.Size([146038, 2])
We keep 4.78e+07/5.98e+09 =  0% of the original kernel matrix.

torch.Size([113905, 2])
We keep 5.66e+07/3.91e+09 =  1% of the original kernel matrix.

torch.Size([78392, 2])
We keep 1.68e+07/1.78e+09 =  0% of the original kernel matrix.

torch.Size([18206, 2])
We keep 1.60e+06/6.99e+07 =  2% of the original kernel matrix.

torch.Size([31599, 2])
We keep 3.45e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([47315, 2])
We keep 1.10e+07/7.36e+08 =  1% of the original kernel matrix.

torch.Size([49632, 2])
We keep 8.10e+06/7.71e+08 =  1% of the original kernel matrix.

torch.Size([948970, 2])
We keep 4.62e+09/3.78e+11 =  1% of the original kernel matrix.

torch.Size([235499, 2])
We keep 1.25e+08/1.75e+10 =  0% of the original kernel matrix.

torch.Size([49112, 2])
We keep 1.16e+07/7.86e+08 =  1% of the original kernel matrix.

torch.Size([50001, 2])
We keep 8.31e+06/7.97e+08 =  1% of the original kernel matrix.

torch.Size([264930, 2])
We keep 2.93e+08/2.47e+10 =  1% of the original kernel matrix.

torch.Size([123466, 2])
We keep 3.72e+07/4.47e+09 =  0% of the original kernel matrix.

torch.Size([16452, 2])
We keep 8.30e+06/6.42e+07 = 12% of the original kernel matrix.

torch.Size([29984, 2])
We keep 3.28e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([18252, 2])
We keep 2.64e+06/6.69e+07 =  3% of the original kernel matrix.

torch.Size([31680, 2])
We keep 3.28e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([16750, 2])
We keep 1.59e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([30223, 2])
We keep 3.36e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([19785, 2])
We keep 3.66e+06/1.09e+08 =  3% of the original kernel matrix.

torch.Size([33334, 2])
We keep 4.08e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([61699, 2])
We keep 1.57e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([59466, 2])
We keep 1.03e+07/9.78e+08 =  1% of the original kernel matrix.

torch.Size([161875, 2])
We keep 1.10e+08/9.24e+09 =  1% of the original kernel matrix.

torch.Size([93788, 2])
We keep 2.45e+07/2.73e+09 =  0% of the original kernel matrix.

torch.Size([19604, 2])
We keep 6.83e+06/9.17e+07 =  7% of the original kernel matrix.

torch.Size([32976, 2])
We keep 3.86e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([325838, 2])
We keep 3.43e+08/3.54e+10 =  0% of the original kernel matrix.

torch.Size([138073, 2])
We keep 4.35e+07/5.35e+09 =  0% of the original kernel matrix.

torch.Size([21726, 2])
We keep 3.31e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([34567, 2])
We keep 4.23e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([161589, 2])
We keep 1.28e+08/8.21e+09 =  1% of the original kernel matrix.

torch.Size([94506, 2])
We keep 2.35e+07/2.58e+09 =  0% of the original kernel matrix.

torch.Size([28443, 2])
We keep 6.35e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([39576, 2])
We keep 5.66e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([267989, 2])
We keep 1.99e+08/2.26e+10 =  0% of the original kernel matrix.

torch.Size([122927, 2])
We keep 3.56e+07/4.28e+09 =  0% of the original kernel matrix.

torch.Size([18883, 2])
We keep 2.85e+06/8.83e+07 =  3% of the original kernel matrix.

torch.Size([32251, 2])
We keep 3.79e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([52406, 2])
We keep 1.65e+07/9.25e+08 =  1% of the original kernel matrix.

torch.Size([54625, 2])
We keep 9.21e+06/8.64e+08 =  1% of the original kernel matrix.

torch.Size([70082, 2])
We keep 2.17e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([62820, 2])
We keep 1.17e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([59644, 2])
We keep 2.63e+07/1.31e+09 =  2% of the original kernel matrix.

torch.Size([57921, 2])
We keep 1.09e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([161844, 2])
We keep 1.06e+08/9.47e+09 =  1% of the original kernel matrix.

torch.Size([94871, 2])
We keep 2.48e+07/2.77e+09 =  0% of the original kernel matrix.

torch.Size([1034897, 2])
We keep 3.27e+09/3.70e+11 =  0% of the original kernel matrix.

torch.Size([250762, 2])
We keep 1.25e+08/1.73e+10 =  0% of the original kernel matrix.

torch.Size([26014, 2])
We keep 2.77e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([38702, 2])
We keep 4.66e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([19853, 2])
We keep 3.21e+06/9.66e+07 =  3% of the original kernel matrix.

torch.Size([33207, 2])
We keep 3.90e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([128392, 2])
We keep 9.16e+07/5.32e+09 =  1% of the original kernel matrix.

torch.Size([83631, 2])
We keep 1.89e+07/2.07e+09 =  0% of the original kernel matrix.

torch.Size([51041, 2])
We keep 1.23e+07/8.32e+08 =  1% of the original kernel matrix.

torch.Size([54630, 2])
We keep 9.18e+06/8.20e+08 =  1% of the original kernel matrix.

torch.Size([1222569, 2])
We keep 2.02e+09/4.21e+11 =  0% of the original kernel matrix.

torch.Size([277430, 2])
We keep 1.31e+08/1.84e+10 =  0% of the original kernel matrix.

torch.Size([10897, 2])
We keep 7.80e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([24850, 2])
We keep 2.22e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([272409, 2])
We keep 2.23e+08/2.69e+10 =  0% of the original kernel matrix.

torch.Size([124504, 2])
We keep 3.90e+07/4.66e+09 =  0% of the original kernel matrix.

torch.Size([675870, 2])
We keep 7.27e+08/1.30e+11 =  0% of the original kernel matrix.

torch.Size([199231, 2])
We keep 7.71e+07/1.03e+10 =  0% of the original kernel matrix.

torch.Size([17109, 2])
We keep 1.63e+06/5.98e+07 =  2% of the original kernel matrix.

torch.Size([30704, 2])
We keep 3.26e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([400958, 2])
We keep 8.08e+08/7.38e+10 =  1% of the original kernel matrix.

torch.Size([150202, 2])
We keep 6.02e+07/7.72e+09 =  0% of the original kernel matrix.

torch.Size([38562, 2])
We keep 1.90e+07/7.44e+08 =  2% of the original kernel matrix.

torch.Size([44475, 2])
We keep 8.39e+06/7.75e+08 =  1% of the original kernel matrix.

torch.Size([1221928, 2])
We keep 3.56e+09/4.70e+11 =  0% of the original kernel matrix.

torch.Size([276008, 2])
We keep 1.40e+08/1.95e+10 =  0% of the original kernel matrix.

torch.Size([35575, 2])
We keep 5.17e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([45468, 2])
We keep 6.52e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([56948, 2])
We keep 1.61e+07/9.86e+08 =  1% of the original kernel matrix.

torch.Size([56832, 2])
We keep 9.60e+06/8.92e+08 =  1% of the original kernel matrix.

torch.Size([20371, 2])
We keep 5.74e+06/1.65e+08 =  3% of the original kernel matrix.

torch.Size([32916, 2])
We keep 4.74e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([14678, 2])
We keep 1.33e+06/3.98e+07 =  3% of the original kernel matrix.

torch.Size([28824, 2])
We keep 2.79e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([20484, 2])
We keep 3.23e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([33618, 2])
We keep 4.25e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([99715, 2])
We keep 1.95e+08/3.52e+09 =  5% of the original kernel matrix.

torch.Size([74012, 2])
We keep 1.60e+07/1.69e+09 =  0% of the original kernel matrix.

torch.Size([19907, 2])
We keep 7.27e+06/9.09e+07 =  7% of the original kernel matrix.

torch.Size([33104, 2])
We keep 3.59e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([333529, 2])
We keep 3.01e+08/3.87e+10 =  0% of the original kernel matrix.

torch.Size([139908, 2])
We keep 4.52e+07/5.59e+09 =  0% of the original kernel matrix.

torch.Size([44509, 2])
We keep 7.88e+06/6.25e+08 =  1% of the original kernel matrix.

torch.Size([50977, 2])
We keep 8.04e+06/7.10e+08 =  1% of the original kernel matrix.

torch.Size([17749, 2])
We keep 2.15e+06/6.91e+07 =  3% of the original kernel matrix.

torch.Size([31354, 2])
We keep 3.45e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([56756, 2])
We keep 1.47e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([56557, 2])
We keep 1.02e+07/9.40e+08 =  1% of the original kernel matrix.

torch.Size([345856, 2])
We keep 7.96e+08/7.11e+10 =  1% of the original kernel matrix.

torch.Size([140051, 2])
We keep 5.97e+07/7.58e+09 =  0% of the original kernel matrix.

torch.Size([134355, 2])
We keep 2.60e+08/1.56e+10 =  1% of the original kernel matrix.

torch.Size([81227, 2])
We keep 3.08e+07/3.55e+09 =  0% of the original kernel matrix.

torch.Size([703105, 2])
We keep 9.65e+08/1.41e+11 =  0% of the original kernel matrix.

torch.Size([203208, 2])
We keep 8.06e+07/1.07e+10 =  0% of the original kernel matrix.

torch.Size([55186, 2])
We keep 3.46e+07/1.01e+09 =  3% of the original kernel matrix.

torch.Size([56299, 2])
We keep 9.38e+06/9.02e+08 =  1% of the original kernel matrix.

torch.Size([18881, 2])
We keep 2.79e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([32202, 2])
We keep 4.06e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([13586, 2])
We keep 9.35e+05/3.15e+07 =  2% of the original kernel matrix.

torch.Size([27946, 2])
We keep 2.56e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([15409, 2])
We keep 2.05e+06/5.76e+07 =  3% of the original kernel matrix.

torch.Size([29257, 2])
We keep 3.17e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([440491, 2])
We keep 1.12e+09/6.52e+10 =  1% of the original kernel matrix.

torch.Size([161812, 2])
We keep 5.63e+07/7.26e+09 =  0% of the original kernel matrix.

torch.Size([25447, 2])
We keep 3.22e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([37880, 2])
We keep 4.89e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([57373, 2])
We keep 1.60e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([57155, 2])
We keep 9.81e+06/9.11e+08 =  1% of the original kernel matrix.

torch.Size([88112, 2])
We keep 3.48e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([69227, 2])
We keep 1.42e+07/1.45e+09 =  0% of the original kernel matrix.

torch.Size([40727, 2])
We keep 2.25e+07/9.55e+08 =  2% of the original kernel matrix.

torch.Size([46677, 2])
We keep 9.58e+06/8.78e+08 =  1% of the original kernel matrix.

torch.Size([25480, 2])
We keep 5.29e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([37790, 2])
We keep 4.92e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([14409, 2])
We keep 1.09e+06/3.99e+07 =  2% of the original kernel matrix.

torch.Size([28317, 2])
We keep 2.82e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([472294, 2])
We keep 8.89e+08/9.05e+10 =  0% of the original kernel matrix.

torch.Size([164154, 2])
We keep 6.58e+07/8.55e+09 =  0% of the original kernel matrix.

torch.Size([107632, 2])
We keep 5.91e+07/3.83e+09 =  1% of the original kernel matrix.

torch.Size([76394, 2])
We keep 1.68e+07/1.76e+09 =  0% of the original kernel matrix.

torch.Size([29416, 2])
We keep 8.56e+06/2.78e+08 =  3% of the original kernel matrix.

torch.Size([40313, 2])
We keep 5.68e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([38288, 2])
We keep 1.60e+07/4.47e+08 =  3% of the original kernel matrix.

torch.Size([46739, 2])
We keep 6.85e+06/6.01e+08 =  1% of the original kernel matrix.

torch.Size([170493, 2])
We keep 5.08e+08/1.42e+10 =  3% of the original kernel matrix.

torch.Size([97592, 2])
We keep 2.96e+07/3.39e+09 =  0% of the original kernel matrix.

torch.Size([225493, 2])
We keep 1.99e+08/1.78e+10 =  1% of the original kernel matrix.

torch.Size([112897, 2])
We keep 3.23e+07/3.80e+09 =  0% of the original kernel matrix.

torch.Size([633294, 2])
We keep 2.65e+09/2.48e+11 =  1% of the original kernel matrix.

torch.Size([186071, 2])
We keep 1.04e+08/1.41e+10 =  0% of the original kernel matrix.

torch.Size([269355, 2])
We keep 2.27e+08/2.39e+10 =  0% of the original kernel matrix.

torch.Size([124721, 2])
We keep 3.68e+07/4.39e+09 =  0% of the original kernel matrix.

torch.Size([18160, 2])
We keep 5.06e+06/7.80e+07 =  6% of the original kernel matrix.

torch.Size([31923, 2])
We keep 3.54e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([88734, 2])
We keep 2.77e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([70067, 2])
We keep 1.39e+07/1.43e+09 =  0% of the original kernel matrix.

torch.Size([71982, 2])
We keep 9.33e+07/3.68e+09 =  2% of the original kernel matrix.

torch.Size([59943, 2])
We keep 1.67e+07/1.72e+09 =  0% of the original kernel matrix.

torch.Size([26787, 2])
We keep 4.23e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([38877, 2])
We keep 4.80e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([29081, 2])
We keep 5.64e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([39985, 2])
We keep 5.88e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([19888, 2])
We keep 1.79e+06/8.22e+07 =  2% of the original kernel matrix.

torch.Size([33225, 2])
We keep 3.64e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([50155, 2])
We keep 3.18e+07/8.76e+08 =  3% of the original kernel matrix.

torch.Size([53182, 2])
We keep 9.37e+06/8.41e+08 =  1% of the original kernel matrix.

torch.Size([61586, 2])
We keep 2.49e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([58622, 2])
We keep 1.13e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([12632, 2])
We keep 9.84e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([26563, 2])
We keep 2.46e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([39530, 2])
We keep 6.27e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([47862, 2])
We keep 7.00e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([16591, 2])
We keep 2.98e+06/8.48e+07 =  3% of the original kernel matrix.

torch.Size([29869, 2])
We keep 3.67e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([60050, 2])
We keep 3.42e+07/1.67e+09 =  2% of the original kernel matrix.

torch.Size([56814, 2])
We keep 1.17e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([17125, 2])
We keep 2.82e+06/9.17e+07 =  3% of the original kernel matrix.

torch.Size([30573, 2])
We keep 3.80e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([49693, 2])
We keep 2.63e+07/1.16e+09 =  2% of the original kernel matrix.

torch.Size([51745, 2])
We keep 1.04e+07/9.70e+08 =  1% of the original kernel matrix.

torch.Size([27482, 2])
We keep 6.58e+07/5.19e+08 = 12% of the original kernel matrix.

torch.Size([38433, 2])
We keep 7.49e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([42288, 2])
We keep 1.35e+07/6.09e+08 =  2% of the original kernel matrix.

torch.Size([49250, 2])
We keep 7.85e+06/7.01e+08 =  1% of the original kernel matrix.

torch.Size([18916, 2])
We keep 2.22e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([32253, 2])
We keep 3.59e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([35145, 2])
We keep 5.12e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([44968, 2])
We keep 6.39e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([52564, 2])
We keep 1.21e+07/8.22e+08 =  1% of the original kernel matrix.

torch.Size([56184, 2])
We keep 9.04e+06/8.15e+08 =  1% of the original kernel matrix.

torch.Size([285510, 2])
We keep 3.79e+08/2.91e+10 =  1% of the original kernel matrix.

torch.Size([127910, 2])
We keep 3.98e+07/4.85e+09 =  0% of the original kernel matrix.

torch.Size([25447, 2])
We keep 3.04e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([38175, 2])
We keep 4.68e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([7947, 2])
We keep 3.47e+05/9.90e+06 =  3% of the original kernel matrix.

torch.Size([21630, 2])
We keep 1.71e+06/8.94e+07 =  1% of the original kernel matrix.

torch.Size([19698, 2])
We keep 2.44e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([33087, 2])
We keep 3.83e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([238273, 2])
We keep 3.80e+08/2.42e+10 =  1% of the original kernel matrix.

torch.Size([116435, 2])
We keep 3.63e+07/4.43e+09 =  0% of the original kernel matrix.

torch.Size([1596144, 2])
We keep 5.20e+09/6.76e+11 =  0% of the original kernel matrix.

torch.Size([314839, 2])
We keep 1.66e+08/2.34e+10 =  0% of the original kernel matrix.

torch.Size([125736, 2])
We keep 9.79e+07/6.29e+09 =  1% of the original kernel matrix.

torch.Size([81691, 2])
We keep 2.05e+07/2.25e+09 =  0% of the original kernel matrix.

torch.Size([22196, 2])
We keep 2.58e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([35334, 2])
We keep 4.19e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([293767, 2])
We keep 6.16e+08/4.80e+10 =  1% of the original kernel matrix.

torch.Size([128821, 2])
We keep 5.01e+07/6.23e+09 =  0% of the original kernel matrix.

torch.Size([332604, 2])
We keep 2.44e+08/3.66e+10 =  0% of the original kernel matrix.

torch.Size([141403, 2])
We keep 4.42e+07/5.44e+09 =  0% of the original kernel matrix.

torch.Size([10979, 2])
We keep 6.23e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([24901, 2])
We keep 2.17e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([22656, 2])
We keep 3.49e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([35677, 2])
We keep 4.40e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([83999, 2])
We keep 2.04e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([68400, 2])
We keep 1.28e+07/1.28e+09 =  0% of the original kernel matrix.

torch.Size([190012, 2])
We keep 4.17e+08/2.28e+10 =  1% of the original kernel matrix.

torch.Size([100920, 2])
We keep 3.60e+07/4.29e+09 =  0% of the original kernel matrix.

torch.Size([565821, 2])
We keep 2.78e+09/2.00e+11 =  1% of the original kernel matrix.

torch.Size([175961, 2])
We keep 9.52e+07/1.27e+10 =  0% of the original kernel matrix.

torch.Size([466803, 2])
We keep 4.74e+08/7.21e+10 =  0% of the original kernel matrix.

torch.Size([165768, 2])
We keep 5.99e+07/7.63e+09 =  0% of the original kernel matrix.

torch.Size([53407, 2])
We keep 1.65e+07/8.69e+08 =  1% of the original kernel matrix.

torch.Size([55786, 2])
We keep 9.08e+06/8.38e+08 =  1% of the original kernel matrix.

torch.Size([14848, 2])
We keep 1.04e+06/3.97e+07 =  2% of the original kernel matrix.

torch.Size([28565, 2])
We keep 2.75e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([11280, 2])
We keep 6.09e+05/2.14e+07 =  2% of the original kernel matrix.

torch.Size([25225, 2])
We keep 2.22e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([155505, 2])
We keep 9.88e+07/8.48e+09 =  1% of the original kernel matrix.

torch.Size([92336, 2])
We keep 2.36e+07/2.62e+09 =  0% of the original kernel matrix.

torch.Size([34894, 2])
We keep 5.60e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([44918, 2])
We keep 6.26e+06/5.35e+08 =  1% of the original kernel matrix.

torch.Size([55259, 2])
We keep 5.57e+07/1.52e+09 =  3% of the original kernel matrix.

torch.Size([54038, 2])
We keep 1.13e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([333753, 2])
We keep 3.88e+08/4.33e+10 =  0% of the original kernel matrix.

torch.Size([139463, 2])
We keep 4.81e+07/5.91e+09 =  0% of the original kernel matrix.

torch.Size([1359826, 2])
We keep 2.54e+09/5.07e+11 =  0% of the original kernel matrix.

torch.Size([293600, 2])
We keep 1.44e+08/2.02e+10 =  0% of the original kernel matrix.

torch.Size([28260, 2])
We keep 4.97e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([39754, 2])
We keep 5.49e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([8774, 2])
We keep 4.24e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([22822, 2])
We keep 1.82e+06/9.81e+07 =  1% of the original kernel matrix.

torch.Size([60968, 2])
We keep 3.43e+07/1.27e+09 =  2% of the original kernel matrix.

torch.Size([58888, 2])
We keep 1.07e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([256253, 2])
We keep 8.09e+08/2.27e+10 =  3% of the original kernel matrix.

torch.Size([121154, 2])
We keep 3.55e+07/4.28e+09 =  0% of the original kernel matrix.

torch.Size([104744, 2])
We keep 4.98e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([75511, 2])
We keep 1.55e+07/1.61e+09 =  0% of the original kernel matrix.

torch.Size([18690, 2])
We keep 1.23e+07/1.14e+08 = 10% of the original kernel matrix.

torch.Size([31821, 2])
We keep 4.14e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([14547, 2])
We keep 1.32e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([28220, 2])
We keep 2.82e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([64523, 2])
We keep 1.85e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([59735, 2])
We keep 1.06e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([10241, 2])
We keep 9.00e+05/1.90e+07 =  4% of the original kernel matrix.

torch.Size([24212, 2])
We keep 2.14e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([90432, 2])
We keep 1.30e+08/5.00e+09 =  2% of the original kernel matrix.

torch.Size([68042, 2])
We keep 1.87e+07/2.01e+09 =  0% of the original kernel matrix.

torch.Size([18852, 2])
We keep 2.18e+06/8.27e+07 =  2% of the original kernel matrix.

torch.Size([32290, 2])
We keep 3.69e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([39046, 2])
We keep 5.93e+06/4.37e+08 =  1% of the original kernel matrix.

torch.Size([47857, 2])
We keep 6.90e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([26307, 2])
We keep 3.91e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([38910, 2])
We keep 5.05e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([20967, 2])
We keep 1.84e+06/9.46e+07 =  1% of the original kernel matrix.

torch.Size([34079, 2])
We keep 3.84e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([17167, 2])
We keep 1.66e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([30820, 2])
We keep 3.25e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([370699, 2])
We keep 3.30e+08/4.48e+10 =  0% of the original kernel matrix.

torch.Size([147486, 2])
We keep 4.81e+07/6.02e+09 =  0% of the original kernel matrix.

torch.Size([854779, 2])
We keep 1.44e+09/2.18e+11 =  0% of the original kernel matrix.

torch.Size([225087, 2])
We keep 9.83e+07/1.33e+10 =  0% of the original kernel matrix.

torch.Size([214691, 2])
We keep 3.26e+08/1.83e+10 =  1% of the original kernel matrix.

torch.Size([110251, 2])
We keep 3.28e+07/3.85e+09 =  0% of the original kernel matrix.

torch.Size([174920, 2])
We keep 2.86e+08/1.54e+10 =  1% of the original kernel matrix.

torch.Size([96933, 2])
We keep 3.06e+07/3.52e+09 =  0% of the original kernel matrix.

torch.Size([50923, 2])
We keep 9.92e+06/7.69e+08 =  1% of the original kernel matrix.

torch.Size([51800, 2])
We keep 8.26e+06/7.88e+08 =  1% of the original kernel matrix.

torch.Size([25234, 2])
We keep 5.07e+06/1.67e+08 =  3% of the original kernel matrix.

torch.Size([37525, 2])
We keep 4.61e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([42486, 2])
We keep 8.67e+06/5.87e+08 =  1% of the original kernel matrix.

torch.Size([49881, 2])
We keep 7.89e+06/6.89e+08 =  1% of the original kernel matrix.

torch.Size([350897, 2])
We keep 2.98e+08/4.18e+10 =  0% of the original kernel matrix.

torch.Size([144830, 2])
We keep 4.70e+07/5.81e+09 =  0% of the original kernel matrix.

torch.Size([60175, 2])
We keep 3.11e+07/1.47e+09 =  2% of the original kernel matrix.

torch.Size([58169, 2])
We keep 1.14e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([92588, 2])
We keep 1.61e+08/3.04e+09 =  5% of the original kernel matrix.

torch.Size([71086, 2])
We keep 1.52e+07/1.57e+09 =  0% of the original kernel matrix.

torch.Size([32546, 2])
We keep 5.42e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([43698, 2])
We keep 6.23e+06/5.15e+08 =  1% of the original kernel matrix.

torch.Size([3179496, 2])
We keep 1.19e+10/2.48e+12 =  0% of the original kernel matrix.

torch.Size([455420, 2])
We keep 3.02e+08/4.48e+10 =  0% of the original kernel matrix.

torch.Size([40148, 2])
We keep 7.41e+06/4.82e+08 =  1% of the original kernel matrix.

torch.Size([47997, 2])
We keep 7.22e+06/6.24e+08 =  1% of the original kernel matrix.

torch.Size([149651, 2])
We keep 7.23e+07/7.28e+09 =  0% of the original kernel matrix.

torch.Size([90631, 2])
We keep 2.20e+07/2.42e+09 =  0% of the original kernel matrix.

torch.Size([441915, 2])
We keep 6.88e+08/7.41e+10 =  0% of the original kernel matrix.

torch.Size([160921, 2])
We keep 6.05e+07/7.74e+09 =  0% of the original kernel matrix.

torch.Size([118308, 2])
We keep 4.95e+07/4.12e+09 =  1% of the original kernel matrix.

torch.Size([80353, 2])
We keep 1.73e+07/1.82e+09 =  0% of the original kernel matrix.

torch.Size([61604, 2])
We keep 2.03e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([58704, 2])
We keep 1.05e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([1824882, 2])
We keep 7.27e+09/1.02e+12 =  0% of the original kernel matrix.

torch.Size([334388, 2])
We keep 2.00e+08/2.86e+10 =  0% of the original kernel matrix.

torch.Size([310882, 2])
We keep 2.19e+08/3.27e+10 =  0% of the original kernel matrix.

torch.Size([135172, 2])
We keep 4.21e+07/5.14e+09 =  0% of the original kernel matrix.

torch.Size([162777, 2])
We keep 1.27e+08/8.89e+09 =  1% of the original kernel matrix.

torch.Size([94570, 2])
We keep 2.37e+07/2.68e+09 =  0% of the original kernel matrix.

torch.Size([20549, 2])
We keep 3.18e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([33692, 2])
We keep 3.92e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([90657, 2])
We keep 5.26e+07/2.82e+09 =  1% of the original kernel matrix.

torch.Size([70315, 2])
We keep 1.50e+07/1.51e+09 =  0% of the original kernel matrix.

torch.Size([32540, 2])
We keep 8.37e+06/3.54e+08 =  2% of the original kernel matrix.

torch.Size([42926, 2])
We keep 6.36e+06/5.35e+08 =  1% of the original kernel matrix.

torch.Size([379211, 2])
We keep 1.02e+09/5.05e+10 =  2% of the original kernel matrix.

torch.Size([150331, 2])
We keep 5.22e+07/6.39e+09 =  0% of the original kernel matrix.

torch.Size([2718109, 2])
We keep 1.01e+10/1.83e+12 =  0% of the original kernel matrix.

torch.Size([421164, 2])
We keep 2.61e+08/3.84e+10 =  0% of the original kernel matrix.

torch.Size([200087, 2])
We keep 4.82e+08/1.76e+10 =  2% of the original kernel matrix.

torch.Size([106228, 2])
We keep 3.28e+07/3.78e+09 =  0% of the original kernel matrix.

torch.Size([407758, 2])
We keep 5.38e+08/6.61e+10 =  0% of the original kernel matrix.

torch.Size([153027, 2])
We keep 5.78e+07/7.31e+09 =  0% of the original kernel matrix.

torch.Size([142806, 2])
We keep 3.29e+08/8.14e+09 =  4% of the original kernel matrix.

torch.Size([88160, 2])
We keep 2.33e+07/2.56e+09 =  0% of the original kernel matrix.

torch.Size([65827, 2])
We keep 4.56e+07/1.81e+09 =  2% of the original kernel matrix.

torch.Size([59368, 2])
We keep 1.24e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([89572, 2])
We keep 3.44e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([70876, 2])
We keep 1.44e+07/1.45e+09 =  0% of the original kernel matrix.

torch.Size([32090, 2])
We keep 7.97e+06/3.43e+08 =  2% of the original kernel matrix.

torch.Size([42608, 2])
We keep 6.26e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([451659, 2])
We keep 3.37e+09/1.64e+11 =  2% of the original kernel matrix.

torch.Size([155519, 2])
We keep 8.77e+07/1.15e+10 =  0% of the original kernel matrix.

torch.Size([939473, 2])
We keep 1.29e+09/2.56e+11 =  0% of the original kernel matrix.

torch.Size([240386, 2])
We keep 1.05e+08/1.44e+10 =  0% of the original kernel matrix.

torch.Size([103219, 2])
We keep 1.72e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([68028, 2])
We keep 2.54e+07/2.86e+09 =  0% of the original kernel matrix.

torch.Size([2453345, 2])
We keep 2.17e+10/2.34e+12 =  0% of the original kernel matrix.

torch.Size([376411, 2])
We keep 2.95e+08/4.35e+10 =  0% of the original kernel matrix.

torch.Size([290815, 2])
We keep 5.80e+08/4.15e+10 =  1% of the original kernel matrix.

torch.Size([129192, 2])
We keep 4.68e+07/5.79e+09 =  0% of the original kernel matrix.

torch.Size([103282, 2])
We keep 3.78e+07/3.43e+09 =  1% of the original kernel matrix.

torch.Size([75329, 2])
We keep 1.59e+07/1.66e+09 =  0% of the original kernel matrix.

torch.Size([18908, 2])
We keep 2.88e+06/9.00e+07 =  3% of the original kernel matrix.

torch.Size([32101, 2])
We keep 3.84e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([150202, 2])
We keep 1.29e+08/7.87e+09 =  1% of the original kernel matrix.

torch.Size([90915, 2])
We keep 2.29e+07/2.52e+09 =  0% of the original kernel matrix.

torch.Size([289916, 2])
We keep 1.95e+08/2.65e+10 =  0% of the original kernel matrix.

torch.Size([129611, 2])
We keep 3.82e+07/4.63e+09 =  0% of the original kernel matrix.

torch.Size([20272, 2])
We keep 4.48e+06/1.44e+08 =  3% of the original kernel matrix.

torch.Size([33479, 2])
We keep 4.50e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([1628016, 2])
We keep 3.12e+09/6.59e+11 =  0% of the original kernel matrix.

torch.Size([316378, 2])
We keep 1.62e+08/2.31e+10 =  0% of the original kernel matrix.

torch.Size([38630, 2])
We keep 8.15e+06/4.65e+08 =  1% of the original kernel matrix.

torch.Size([46563, 2])
We keep 6.71e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([64203, 2])
We keep 2.97e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([59538, 2])
We keep 1.18e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([19799, 2])
We keep 1.89e+06/8.27e+07 =  2% of the original kernel matrix.

torch.Size([33145, 2])
We keep 3.60e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([33160, 2])
We keep 4.68e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([43665, 2])
We keep 6.10e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([8834, 2])
We keep 4.41e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([22731, 2])
We keep 1.84e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([22567, 2])
We keep 4.08e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([35316, 2])
We keep 4.51e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([53933, 2])
We keep 1.82e+07/9.81e+08 =  1% of the original kernel matrix.

torch.Size([55378, 2])
We keep 9.78e+06/8.90e+08 =  1% of the original kernel matrix.

torch.Size([81519, 2])
We keep 4.96e+07/2.22e+09 =  2% of the original kernel matrix.

torch.Size([66822, 2])
We keep 1.32e+07/1.34e+09 =  0% of the original kernel matrix.

torch.Size([4994973, 2])
We keep 9.02e+10/1.32e+13 =  0% of the original kernel matrix.

torch.Size([492028, 2])
We keep 6.61e+08/1.03e+11 =  0% of the original kernel matrix.

torch.Size([35623, 2])
We keep 1.48e+07/4.96e+08 =  2% of the original kernel matrix.

torch.Size([44285, 2])
We keep 7.43e+06/6.33e+08 =  1% of the original kernel matrix.

torch.Size([378761, 2])
We keep 3.09e+08/4.77e+10 =  0% of the original kernel matrix.

torch.Size([149141, 2])
We keep 4.87e+07/6.21e+09 =  0% of the original kernel matrix.

torch.Size([21321, 2])
We keep 2.25e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([34429, 2])
We keep 3.97e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([56548, 2])
We keep 1.24e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([57305, 2])
We keep 9.96e+06/9.22e+08 =  1% of the original kernel matrix.

torch.Size([25591, 2])
We keep 3.86e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([37803, 2])
We keep 4.97e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([297549, 2])
We keep 9.61e+08/6.85e+10 =  1% of the original kernel matrix.

torch.Size([124682, 2])
We keep 5.76e+07/7.44e+09 =  0% of the original kernel matrix.

torch.Size([121385, 2])
We keep 1.64e+08/7.16e+09 =  2% of the original kernel matrix.

torch.Size([80452, 2])
We keep 2.20e+07/2.41e+09 =  0% of the original kernel matrix.

torch.Size([2383955, 2])
We keep 6.35e+09/1.38e+12 =  0% of the original kernel matrix.

torch.Size([390643, 2])
We keep 2.28e+08/3.34e+10 =  0% of the original kernel matrix.

torch.Size([23004, 2])
We keep 3.42e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([35979, 2])
We keep 4.27e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([391572, 2])
We keep 3.92e+08/5.12e+10 =  0% of the original kernel matrix.

torch.Size([154509, 2])
We keep 5.18e+07/6.43e+09 =  0% of the original kernel matrix.

torch.Size([59689, 2])
We keep 3.37e+07/1.26e+09 =  2% of the original kernel matrix.

torch.Size([58107, 2])
We keep 1.06e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([303248, 2])
We keep 3.30e+08/3.39e+10 =  0% of the original kernel matrix.

torch.Size([132522, 2])
We keep 4.27e+07/5.24e+09 =  0% of the original kernel matrix.

torch.Size([461077, 2])
We keep 7.56e+08/8.41e+10 =  0% of the original kernel matrix.

torch.Size([164029, 2])
We keep 6.38e+07/8.24e+09 =  0% of the original kernel matrix.

torch.Size([51947, 2])
We keep 1.22e+07/8.68e+08 =  1% of the original kernel matrix.

torch.Size([55225, 2])
We keep 9.37e+06/8.38e+08 =  1% of the original kernel matrix.

torch.Size([29851, 2])
We keep 5.48e+06/2.68e+08 =  2% of the original kernel matrix.

torch.Size([41138, 2])
We keep 5.74e+06/4.65e+08 =  1% of the original kernel matrix.

torch.Size([179884, 2])
We keep 8.38e+07/9.64e+09 =  0% of the original kernel matrix.

torch.Size([101434, 2])
We keep 2.48e+07/2.79e+09 =  0% of the original kernel matrix.

torch.Size([50140, 2])
We keep 4.00e+07/8.68e+08 =  4% of the original kernel matrix.

torch.Size([53588, 2])
We keep 9.02e+06/8.37e+08 =  1% of the original kernel matrix.

torch.Size([118459, 2])
We keep 8.98e+07/5.13e+09 =  1% of the original kernel matrix.

torch.Size([79716, 2])
We keep 1.91e+07/2.04e+09 =  0% of the original kernel matrix.

torch.Size([616758, 2])
We keep 1.09e+09/1.17e+11 =  0% of the original kernel matrix.

torch.Size([188323, 2])
We keep 7.46e+07/9.72e+09 =  0% of the original kernel matrix.

torch.Size([25909, 2])
We keep 6.36e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([37916, 2])
We keep 5.20e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([157292, 2])
We keep 1.54e+08/9.05e+09 =  1% of the original kernel matrix.

torch.Size([94120, 2])
We keep 2.44e+07/2.70e+09 =  0% of the original kernel matrix.

torch.Size([62216, 2])
We keep 2.27e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([58406, 2])
We keep 1.09e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([285977, 2])
We keep 2.59e+08/2.91e+10 =  0% of the original kernel matrix.

torch.Size([128658, 2])
We keep 4.00e+07/4.85e+09 =  0% of the original kernel matrix.

torch.Size([553900, 2])
We keep 9.98e+08/1.07e+11 =  0% of the original kernel matrix.

torch.Size([177839, 2])
We keep 7.08e+07/9.30e+09 =  0% of the original kernel matrix.

torch.Size([99207, 2])
We keep 4.86e+07/2.94e+09 =  1% of the original kernel matrix.

torch.Size([73384, 2])
We keep 1.49e+07/1.54e+09 =  0% of the original kernel matrix.

torch.Size([30807, 2])
We keep 5.79e+06/2.71e+08 =  2% of the original kernel matrix.

torch.Size([42318, 2])
We keep 5.80e+06/4.68e+08 =  1% of the original kernel matrix.

torch.Size([354275, 2])
We keep 4.74e+08/4.99e+10 =  0% of the original kernel matrix.

torch.Size([141210, 2])
We keep 5.01e+07/6.35e+09 =  0% of the original kernel matrix.

torch.Size([225743, 2])
We keep 2.03e+08/1.92e+10 =  1% of the original kernel matrix.

torch.Size([112008, 2])
We keep 3.37e+07/3.94e+09 =  0% of the original kernel matrix.

torch.Size([845098, 2])
We keep 7.51e+09/5.22e+11 =  1% of the original kernel matrix.

torch.Size([215057, 2])
We keep 1.48e+08/2.05e+10 =  0% of the original kernel matrix.

torch.Size([130240, 2])
We keep 7.91e+07/5.54e+09 =  1% of the original kernel matrix.

torch.Size([83559, 2])
We keep 1.96e+07/2.12e+09 =  0% of the original kernel matrix.

torch.Size([102916, 2])
We keep 6.84e+07/4.02e+09 =  1% of the original kernel matrix.

torch.Size([74258, 2])
We keep 1.70e+07/1.80e+09 =  0% of the original kernel matrix.

torch.Size([61375, 2])
We keep 3.62e+07/1.47e+09 =  2% of the original kernel matrix.

torch.Size([58314, 2])
We keep 1.10e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([240608, 2])
We keep 4.40e+08/3.08e+10 =  1% of the original kernel matrix.

torch.Size([113334, 2])
We keep 4.13e+07/4.98e+09 =  0% of the original kernel matrix.

torch.Size([322035, 2])
We keep 5.79e+08/5.36e+10 =  1% of the original kernel matrix.

torch.Size([133836, 2])
We keep 5.24e+07/6.58e+09 =  0% of the original kernel matrix.

torch.Size([68727, 2])
We keep 1.89e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([62055, 2])
We keep 1.15e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([19839, 2])
We keep 2.00e+06/8.78e+07 =  2% of the original kernel matrix.

torch.Size([33077, 2])
We keep 3.77e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([29343, 2])
We keep 3.97e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([41426, 2])
We keep 5.28e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([132424, 2])
We keep 1.61e+08/7.58e+09 =  2% of the original kernel matrix.

torch.Size([84380, 2])
We keep 2.19e+07/2.47e+09 =  0% of the original kernel matrix.

torch.Size([174529, 2])
We keep 3.04e+08/1.33e+10 =  2% of the original kernel matrix.

torch.Size([98177, 2])
We keep 2.84e+07/3.28e+09 =  0% of the original kernel matrix.

torch.Size([178230, 2])
We keep 9.41e+07/9.46e+09 =  0% of the original kernel matrix.

torch.Size([99559, 2])
We keep 2.46e+07/2.76e+09 =  0% of the original kernel matrix.

torch.Size([28128, 2])
We keep 3.54e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([39902, 2])
We keep 5.09e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([56329, 2])
We keep 1.17e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([56460, 2])
We keep 9.65e+06/9.05e+08 =  1% of the original kernel matrix.

torch.Size([105435, 2])
We keep 9.63e+07/3.79e+09 =  2% of the original kernel matrix.

torch.Size([75580, 2])
We keep 1.67e+07/1.75e+09 =  0% of the original kernel matrix.

torch.Size([106965, 2])
We keep 3.47e+07/3.39e+09 =  1% of the original kernel matrix.

torch.Size([76663, 2])
We keep 1.58e+07/1.66e+09 =  0% of the original kernel matrix.

torch.Size([64717, 2])
We keep 2.26e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([60639, 2])
We keep 1.08e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([145867, 2])
We keep 6.63e+07/6.54e+09 =  1% of the original kernel matrix.

torch.Size([88871, 2])
We keep 2.12e+07/2.30e+09 =  0% of the original kernel matrix.

torch.Size([36635, 2])
We keep 1.37e+07/5.42e+08 =  2% of the original kernel matrix.

torch.Size([45058, 2])
We keep 7.61e+06/6.62e+08 =  1% of the original kernel matrix.

torch.Size([63245, 2])
We keep 1.92e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([59770, 2])
We keep 1.06e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([9615, 2])
We keep 5.72e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([23304, 2])
We keep 2.02e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([25807, 2])
We keep 3.23e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([38646, 2])
We keep 4.71e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([326544, 2])
We keep 2.47e+08/3.44e+10 =  0% of the original kernel matrix.

torch.Size([139194, 2])
We keep 4.28e+07/5.27e+09 =  0% of the original kernel matrix.

torch.Size([46428, 2])
We keep 1.33e+07/7.52e+08 =  1% of the original kernel matrix.

torch.Size([48675, 2])
We keep 8.25e+06/7.79e+08 =  1% of the original kernel matrix.

torch.Size([178193, 2])
We keep 9.41e+07/9.58e+09 =  0% of the original kernel matrix.

torch.Size([99470, 2])
We keep 2.45e+07/2.78e+09 =  0% of the original kernel matrix.

torch.Size([8695, 2])
We keep 3.97e+05/1.12e+07 =  3% of the original kernel matrix.

torch.Size([22853, 2])
We keep 1.77e+06/9.51e+07 =  1% of the original kernel matrix.

torch.Size([19492, 2])
We keep 4.71e+06/8.36e+07 =  5% of the original kernel matrix.

torch.Size([32963, 2])
We keep 3.65e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([98576, 2])
We keep 1.00e+08/3.32e+09 =  3% of the original kernel matrix.

torch.Size([73141, 2])
We keep 1.55e+07/1.64e+09 =  0% of the original kernel matrix.

torch.Size([167442, 2])
We keep 8.46e+07/8.75e+09 =  0% of the original kernel matrix.

torch.Size([95659, 2])
We keep 2.37e+07/2.66e+09 =  0% of the original kernel matrix.

torch.Size([67585, 2])
We keep 1.90e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([61893, 2])
We keep 1.13e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([17504, 2])
We keep 1.75e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([30997, 2])
We keep 3.46e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([19792, 2])
We keep 2.61e+06/9.59e+07 =  2% of the original kernel matrix.

torch.Size([32993, 2])
We keep 3.84e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([84712, 2])
We keep 4.46e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([68179, 2])
We keep 1.34e+07/1.34e+09 =  0% of the original kernel matrix.

torch.Size([53435, 2])
We keep 1.33e+07/8.81e+08 =  1% of the original kernel matrix.

torch.Size([55052, 2])
We keep 9.04e+06/8.43e+08 =  1% of the original kernel matrix.

torch.Size([242628, 2])
We keep 3.58e+08/2.15e+10 =  1% of the original kernel matrix.

torch.Size([117423, 2])
We keep 3.57e+07/4.17e+09 =  0% of the original kernel matrix.

torch.Size([18335, 2])
We keep 2.06e+06/8.04e+07 =  2% of the original kernel matrix.

torch.Size([31757, 2])
We keep 3.64e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([25854, 2])
We keep 5.05e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([37724, 2])
We keep 5.15e+06/4.03e+08 =  1% of the original kernel matrix.

torch.Size([85745, 2])
We keep 7.09e+07/3.97e+09 =  1% of the original kernel matrix.

torch.Size([65427, 2])
We keep 1.72e+07/1.79e+09 =  0% of the original kernel matrix.

torch.Size([19370, 2])
We keep 2.05e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([32717, 2])
We keep 3.57e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([318827, 2])
We keep 5.62e+08/4.59e+10 =  1% of the original kernel matrix.

torch.Size([135777, 2])
We keep 4.80e+07/6.09e+09 =  0% of the original kernel matrix.

torch.Size([27381, 2])
We keep 2.15e+07/2.30e+08 =  9% of the original kernel matrix.

torch.Size([39096, 2])
We keep 5.49e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([194521, 2])
We keep 1.95e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([104084, 2])
We keep 3.09e+07/3.59e+09 =  0% of the original kernel matrix.

torch.Size([313029, 2])
We keep 2.63e+08/3.12e+10 =  0% of the original kernel matrix.

torch.Size([135498, 2])
We keep 4.14e+07/5.02e+09 =  0% of the original kernel matrix.

torch.Size([82423, 2])
We keep 7.52e+07/2.26e+09 =  3% of the original kernel matrix.

torch.Size([67462, 2])
We keep 1.29e+07/1.35e+09 =  0% of the original kernel matrix.

torch.Size([272034, 2])
We keep 3.52e+08/2.84e+10 =  1% of the original kernel matrix.

torch.Size([125508, 2])
We keep 3.99e+07/4.79e+09 =  0% of the original kernel matrix.

torch.Size([293008, 2])
We keep 2.25e+08/2.76e+10 =  0% of the original kernel matrix.

torch.Size([130598, 2])
We keep 3.92e+07/4.72e+09 =  0% of the original kernel matrix.

torch.Size([9778, 2])
We keep 4.52e+05/1.36e+07 =  3% of the original kernel matrix.

torch.Size([23869, 2])
We keep 1.88e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([131588, 2])
We keep 5.49e+07/5.03e+09 =  1% of the original kernel matrix.

torch.Size([84078, 2])
We keep 1.88e+07/2.02e+09 =  0% of the original kernel matrix.

torch.Size([45777, 2])
We keep 1.18e+07/6.20e+08 =  1% of the original kernel matrix.

torch.Size([51815, 2])
We keep 8.00e+06/7.08e+08 =  1% of the original kernel matrix.

torch.Size([304437, 2])
We keep 4.17e+08/3.24e+10 =  1% of the original kernel matrix.

torch.Size([133031, 2])
We keep 4.20e+07/5.12e+09 =  0% of the original kernel matrix.

torch.Size([246669, 2])
We keep 2.35e+08/2.37e+10 =  0% of the original kernel matrix.

torch.Size([118436, 2])
We keep 3.66e+07/4.37e+09 =  0% of the original kernel matrix.

torch.Size([63936, 2])
We keep 4.85e+07/1.95e+09 =  2% of the original kernel matrix.

torch.Size([58277, 2])
We keep 1.25e+07/1.25e+09 =  0% of the original kernel matrix.

torch.Size([175806, 2])
We keep 9.05e+07/9.83e+09 =  0% of the original kernel matrix.

torch.Size([98740, 2])
We keep 2.51e+07/2.82e+09 =  0% of the original kernel matrix.

torch.Size([51269, 2])
We keep 1.29e+07/8.09e+08 =  1% of the original kernel matrix.

torch.Size([54160, 2])
We keep 9.04e+06/8.09e+08 =  1% of the original kernel matrix.

torch.Size([48752, 2])
We keep 4.71e+07/1.78e+09 =  2% of the original kernel matrix.

torch.Size([50012, 2])
We keep 1.20e+07/1.20e+09 =  0% of the original kernel matrix.

torch.Size([17042, 2])
We keep 1.51e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([30813, 2])
We keep 3.13e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([24415, 2])
We keep 4.04e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([36946, 2])
We keep 4.83e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([455390, 2])
We keep 1.12e+09/7.01e+10 =  1% of the original kernel matrix.

torch.Size([163612, 2])
We keep 5.91e+07/7.53e+09 =  0% of the original kernel matrix.

torch.Size([1273738, 2])
We keep 3.29e+09/4.76e+11 =  0% of the original kernel matrix.

torch.Size([283602, 2])
We keep 1.40e+08/1.96e+10 =  0% of the original kernel matrix.

torch.Size([59421, 2])
We keep 2.10e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([56971, 2])
We keep 1.14e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([32503, 2])
We keep 5.03e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([43203, 2])
We keep 6.03e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([84390, 2])
We keep 2.71e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([68192, 2])
We keep 1.35e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([305387, 2])
We keep 5.50e+08/3.50e+10 =  1% of the original kernel matrix.

torch.Size([133658, 2])
We keep 4.28e+07/5.31e+09 =  0% of the original kernel matrix.

torch.Size([101697, 2])
We keep 3.55e+07/3.05e+09 =  1% of the original kernel matrix.

torch.Size([73754, 2])
We keep 1.52e+07/1.57e+09 =  0% of the original kernel matrix.

torch.Size([27805, 2])
We keep 3.84e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([39515, 2])
We keep 5.16e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([107434, 2])
We keep 4.74e+07/3.56e+09 =  1% of the original kernel matrix.

torch.Size([77116, 2])
We keep 1.62e+07/1.70e+09 =  0% of the original kernel matrix.

torch.Size([345454, 2])
We keep 3.70e+08/4.05e+10 =  0% of the original kernel matrix.

torch.Size([143216, 2])
We keep 4.63e+07/5.72e+09 =  0% of the original kernel matrix.

torch.Size([19752, 2])
We keep 3.49e+06/1.15e+08 =  3% of the original kernel matrix.

torch.Size([32961, 2])
We keep 4.09e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([92116, 2])
We keep 7.54e+07/2.99e+09 =  2% of the original kernel matrix.

torch.Size([70695, 2])
We keep 1.51e+07/1.55e+09 =  0% of the original kernel matrix.

torch.Size([51283, 2])
We keep 5.54e+07/1.02e+09 =  5% of the original kernel matrix.

torch.Size([53476, 2])
We keep 9.60e+06/9.07e+08 =  1% of the original kernel matrix.

time for making ranges is 9.180806159973145
Sorting X and nu_X
time for sorting X is 0.1067817211151123
Sorting Z and nu_Z
time for sorting Z is 0.0002720355987548828
Starting Optim
sum tnu_Z before tensor(56590544., device='cuda:0')
c= tensor(3270.6470, device='cuda:0')
c= tensor(220250.2344, device='cuda:0')
c= tensor(226662.0781, device='cuda:0')
c= tensor(475014.0625, device='cuda:0')
c= tensor(620785.5625, device='cuda:0')
c= tensor(1374702.7500, device='cuda:0')
c= tensor(2661509., device='cuda:0')
c= tensor(3456954.5000, device='cuda:0')
c= tensor(3753129.5000, device='cuda:0')
c= tensor(7301484., device='cuda:0')
c= tensor(7399151., device='cuda:0')
c= tensor(16622246., device='cuda:0')
c= tensor(16658112., device='cuda:0')
c= tensor(67607176., device='cuda:0')
c= tensor(67991240., device='cuda:0')
c= tensor(68955920., device='cuda:0')
c= tensor(70652536., device='cuda:0')
c= tensor(71366608., device='cuda:0')
c= tensor(83485984., device='cuda:0')
c= tensor(88645416., device='cuda:0')
c= tensor(90093688., device='cuda:0')
c= tensor(1.0357e+08, device='cuda:0')
c= tensor(1.0362e+08, device='cuda:0')
c= tensor(1.0592e+08, device='cuda:0')
c= tensor(1.0619e+08, device='cuda:0')
c= tensor(1.0815e+08, device='cuda:0')
c= tensor(1.1150e+08, device='cuda:0')
c= tensor(1.1160e+08, device='cuda:0')
c= tensor(1.3142e+08, device='cuda:0')
c= tensor(5.4338e+08, device='cuda:0')
c= tensor(5.4358e+08, device='cuda:0')
c= tensor(9.1410e+08, device='cuda:0')
c= tensor(9.1550e+08, device='cuda:0')
c= tensor(9.1560e+08, device='cuda:0')
c= tensor(9.1580e+08, device='cuda:0')
c= tensor(9.5579e+08, device='cuda:0')
c= tensor(9.5983e+08, device='cuda:0')
c= tensor(9.5983e+08, device='cuda:0')
c= tensor(9.5984e+08, device='cuda:0')
c= tensor(9.5985e+08, device='cuda:0')
c= tensor(9.5986e+08, device='cuda:0')
c= tensor(9.5989e+08, device='cuda:0')
c= tensor(9.5989e+08, device='cuda:0')
c= tensor(9.5991e+08, device='cuda:0')
c= tensor(9.5991e+08, device='cuda:0')
c= tensor(9.5991e+08, device='cuda:0')
c= tensor(9.5993e+08, device='cuda:0')
c= tensor(9.5994e+08, device='cuda:0')
c= tensor(9.5995e+08, device='cuda:0')
c= tensor(9.6019e+08, device='cuda:0')
c= tensor(9.6030e+08, device='cuda:0')
c= tensor(9.6031e+08, device='cuda:0')
c= tensor(9.6039e+08, device='cuda:0')
c= tensor(9.6040e+08, device='cuda:0')
c= tensor(9.6042e+08, device='cuda:0')
c= tensor(9.6047e+08, device='cuda:0')
c= tensor(9.6048e+08, device='cuda:0')
c= tensor(9.6050e+08, device='cuda:0')
c= tensor(9.6051e+08, device='cuda:0')
c= tensor(9.6052e+08, device='cuda:0')
c= tensor(9.6054e+08, device='cuda:0')
c= tensor(9.6054e+08, device='cuda:0')
c= tensor(9.6058e+08, device='cuda:0')
c= tensor(9.6066e+08, device='cuda:0')
c= tensor(9.6067e+08, device='cuda:0')
c= tensor(9.6068e+08, device='cuda:0')
c= tensor(9.6069e+08, device='cuda:0')
c= tensor(9.6073e+08, device='cuda:0')
c= tensor(9.6076e+08, device='cuda:0')
c= tensor(9.6077e+08, device='cuda:0')
c= tensor(9.6079e+08, device='cuda:0')
c= tensor(9.6080e+08, device='cuda:0')
c= tensor(9.6082e+08, device='cuda:0')
c= tensor(9.6083e+08, device='cuda:0')
c= tensor(9.6085e+08, device='cuda:0')
c= tensor(9.6090e+08, device='cuda:0')
c= tensor(9.6090e+08, device='cuda:0')
c= tensor(9.6091e+08, device='cuda:0')
c= tensor(9.6093e+08, device='cuda:0')
c= tensor(9.6108e+08, device='cuda:0')
c= tensor(9.6109e+08, device='cuda:0')
c= tensor(9.6110e+08, device='cuda:0')
c= tensor(9.6113e+08, device='cuda:0')
c= tensor(9.6113e+08, device='cuda:0')
c= tensor(9.6114e+08, device='cuda:0')
c= tensor(9.6114e+08, device='cuda:0')
c= tensor(9.6115e+08, device='cuda:0')
c= tensor(9.6115e+08, device='cuda:0')
c= tensor(9.6117e+08, device='cuda:0')
c= tensor(9.6118e+08, device='cuda:0')
c= tensor(9.6119e+08, device='cuda:0')
c= tensor(9.6120e+08, device='cuda:0')
c= tensor(9.6121e+08, device='cuda:0')
c= tensor(9.6123e+08, device='cuda:0')
c= tensor(9.6125e+08, device='cuda:0')
c= tensor(9.6127e+08, device='cuda:0')
c= tensor(9.6128e+08, device='cuda:0')
c= tensor(9.6132e+08, device='cuda:0')
c= tensor(9.6133e+08, device='cuda:0')
c= tensor(9.6135e+08, device='cuda:0')
c= tensor(9.6143e+08, device='cuda:0')
c= tensor(9.6144e+08, device='cuda:0')
c= tensor(9.6147e+08, device='cuda:0')
c= tensor(9.6148e+08, device='cuda:0')
c= tensor(9.6160e+08, device='cuda:0')
c= tensor(9.6162e+08, device='cuda:0')
c= tensor(9.6164e+08, device='cuda:0')
c= tensor(9.6164e+08, device='cuda:0')
c= tensor(9.6165e+08, device='cuda:0')
c= tensor(9.6166e+08, device='cuda:0')
c= tensor(9.6166e+08, device='cuda:0')
c= tensor(9.6167e+08, device='cuda:0')
c= tensor(9.6167e+08, device='cuda:0')
c= tensor(9.6169e+08, device='cuda:0')
c= tensor(9.6171e+08, device='cuda:0')
c= tensor(9.6174e+08, device='cuda:0')
c= tensor(9.6175e+08, device='cuda:0')
c= tensor(9.6175e+08, device='cuda:0')
c= tensor(9.6179e+08, device='cuda:0')
c= tensor(9.6179e+08, device='cuda:0')
c= tensor(9.6185e+08, device='cuda:0')
c= tensor(9.6186e+08, device='cuda:0')
c= tensor(9.6186e+08, device='cuda:0')
c= tensor(9.6187e+08, device='cuda:0')
c= tensor(9.6188e+08, device='cuda:0')
c= tensor(9.6188e+08, device='cuda:0')
c= tensor(9.6189e+08, device='cuda:0')
c= tensor(9.6191e+08, device='cuda:0')
c= tensor(9.6198e+08, device='cuda:0')
c= tensor(9.6199e+08, device='cuda:0')
c= tensor(9.6203e+08, device='cuda:0')
c= tensor(9.6204e+08, device='cuda:0')
c= tensor(9.6205e+08, device='cuda:0')
c= tensor(9.6205e+08, device='cuda:0')
c= tensor(9.6217e+08, device='cuda:0')
c= tensor(9.6218e+08, device='cuda:0')
c= tensor(9.6218e+08, device='cuda:0')
c= tensor(9.6218e+08, device='cuda:0')
c= tensor(9.6219e+08, device='cuda:0')
c= tensor(9.6220e+08, device='cuda:0')
c= tensor(9.6221e+08, device='cuda:0')
c= tensor(9.6222e+08, device='cuda:0')
c= tensor(9.6232e+08, device='cuda:0')
c= tensor(9.6235e+08, device='cuda:0')
c= tensor(9.6238e+08, device='cuda:0')
c= tensor(9.6238e+08, device='cuda:0')
c= tensor(9.6239e+08, device='cuda:0')
c= tensor(9.6239e+08, device='cuda:0')
c= tensor(9.6240e+08, device='cuda:0')
c= tensor(9.6241e+08, device='cuda:0')
c= tensor(9.6242e+08, device='cuda:0')
c= tensor(9.6243e+08, device='cuda:0')
c= tensor(9.6244e+08, device='cuda:0')
c= tensor(9.6251e+08, device='cuda:0')
c= tensor(9.6252e+08, device='cuda:0')
c= tensor(9.6281e+08, device='cuda:0')
c= tensor(9.6282e+08, device='cuda:0')
c= tensor(9.6284e+08, device='cuda:0')
c= tensor(9.6285e+08, device='cuda:0')
c= tensor(9.6285e+08, device='cuda:0')
c= tensor(9.6289e+08, device='cuda:0')
c= tensor(9.6290e+08, device='cuda:0')
c= tensor(9.6296e+08, device='cuda:0')
c= tensor(9.6296e+08, device='cuda:0')
c= tensor(9.6300e+08, device='cuda:0')
c= tensor(9.6300e+08, device='cuda:0')
c= tensor(9.6301e+08, device='cuda:0')
c= tensor(9.6302e+08, device='cuda:0')
c= tensor(9.6304e+08, device='cuda:0')
c= tensor(9.6304e+08, device='cuda:0')
c= tensor(9.6305e+08, device='cuda:0')
c= tensor(9.6305e+08, device='cuda:0')
c= tensor(9.6309e+08, device='cuda:0')
c= tensor(9.6310e+08, device='cuda:0')
c= tensor(9.6311e+08, device='cuda:0')
c= tensor(9.6580e+08, device='cuda:0')
c= tensor(9.6583e+08, device='cuda:0')
c= tensor(9.6583e+08, device='cuda:0')
c= tensor(9.6588e+08, device='cuda:0')
c= tensor(9.6589e+08, device='cuda:0')
c= tensor(9.6591e+08, device='cuda:0')
c= tensor(9.6592e+08, device='cuda:0')
c= tensor(9.6593e+08, device='cuda:0')
c= tensor(9.6594e+08, device='cuda:0')
c= tensor(9.6596e+08, device='cuda:0')
c= tensor(9.6598e+08, device='cuda:0')
c= tensor(9.6599e+08, device='cuda:0')
c= tensor(9.6601e+08, device='cuda:0')
c= tensor(9.6604e+08, device='cuda:0')
c= tensor(9.6609e+08, device='cuda:0')
c= tensor(9.6610e+08, device='cuda:0')
c= tensor(9.6610e+08, device='cuda:0')
c= tensor(9.6612e+08, device='cuda:0')
c= tensor(9.6617e+08, device='cuda:0')
c= tensor(9.6620e+08, device='cuda:0')
c= tensor(9.6621e+08, device='cuda:0')
c= tensor(9.6623e+08, device='cuda:0')
c= tensor(9.6623e+08, device='cuda:0')
c= tensor(9.6625e+08, device='cuda:0')
c= tensor(9.6625e+08, device='cuda:0')
c= tensor(9.6626e+08, device='cuda:0')
c= tensor(9.6627e+08, device='cuda:0')
c= tensor(9.6630e+08, device='cuda:0')
c= tensor(9.6631e+08, device='cuda:0')
c= tensor(9.6633e+08, device='cuda:0')
c= tensor(9.6634e+08, device='cuda:0')
c= tensor(9.6635e+08, device='cuda:0')
c= tensor(9.6637e+08, device='cuda:0')
c= tensor(9.6638e+08, device='cuda:0')
c= tensor(9.6644e+08, device='cuda:0')
c= tensor(9.6646e+08, device='cuda:0')
c= tensor(9.6655e+08, device='cuda:0')
c= tensor(9.6656e+08, device='cuda:0')
c= tensor(9.6657e+08, device='cuda:0')
c= tensor(9.6657e+08, device='cuda:0')
c= tensor(9.6658e+08, device='cuda:0')
c= tensor(9.6658e+08, device='cuda:0')
c= tensor(9.6658e+08, device='cuda:0')
c= tensor(9.6675e+08, device='cuda:0')
c= tensor(9.6676e+08, device='cuda:0')
c= tensor(9.6677e+08, device='cuda:0')
c= tensor(9.6677e+08, device='cuda:0')
c= tensor(9.6681e+08, device='cuda:0')
c= tensor(9.6684e+08, device='cuda:0')
c= tensor(9.6685e+08, device='cuda:0')
c= tensor(9.6687e+08, device='cuda:0')
c= tensor(9.6687e+08, device='cuda:0')
c= tensor(9.6687e+08, device='cuda:0')
c= tensor(9.6689e+08, device='cuda:0')
c= tensor(9.6690e+08, device='cuda:0')
c= tensor(9.6691e+08, device='cuda:0')
c= tensor(9.6692e+08, device='cuda:0')
c= tensor(9.6693e+08, device='cuda:0')
c= tensor(9.6695e+08, device='cuda:0')
c= tensor(9.6696e+08, device='cuda:0')
c= tensor(9.6699e+08, device='cuda:0')
c= tensor(9.6700e+08, device='cuda:0')
c= tensor(9.6702e+08, device='cuda:0')
c= tensor(9.6707e+08, device='cuda:0')
c= tensor(9.6752e+08, device='cuda:0')
c= tensor(9.7154e+08, device='cuda:0')
c= tensor(9.7158e+08, device='cuda:0')
c= tensor(9.7160e+08, device='cuda:0')
c= tensor(9.7160e+08, device='cuda:0')
c= tensor(9.7167e+08, device='cuda:0')
c= tensor(9.7190e+08, device='cuda:0')
c= tensor(9.9190e+08, device='cuda:0')
c= tensor(9.9191e+08, device='cuda:0')
c= tensor(1.0048e+09, device='cuda:0')
c= tensor(1.0116e+09, device='cuda:0')
c= tensor(1.0124e+09, device='cuda:0')
c= tensor(1.1721e+09, device='cuda:0')
c= tensor(1.1721e+09, device='cuda:0')
c= tensor(1.1722e+09, device='cuda:0')
c= tensor(1.2030e+09, device='cuda:0')
c= tensor(1.3109e+09, device='cuda:0')
c= tensor(1.3109e+09, device='cuda:0')
c= tensor(1.3116e+09, device='cuda:0')
c= tensor(1.3128e+09, device='cuda:0')
c= tensor(1.3139e+09, device='cuda:0')
c= tensor(1.3210e+09, device='cuda:0')
c= tensor(1.3257e+09, device='cuda:0')
c= tensor(1.3264e+09, device='cuda:0')
c= tensor(1.3268e+09, device='cuda:0')
c= tensor(1.3269e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3558e+09, device='cuda:0')
c= tensor(1.3849e+09, device='cuda:0')
c= tensor(1.3878e+09, device='cuda:0')
c= tensor(1.3878e+09, device='cuda:0')
c= tensor(1.3886e+09, device='cuda:0')
c= tensor(1.3886e+09, device='cuda:0')
c= tensor(1.3899e+09, device='cuda:0')
c= tensor(1.4005e+09, device='cuda:0')
c= tensor(1.4034e+09, device='cuda:0')
c= tensor(1.4063e+09, device='cuda:0')
c= tensor(1.4063e+09, device='cuda:0')
c= tensor(1.4063e+09, device='cuda:0')
c= tensor(1.4101e+09, device='cuda:0')
c= tensor(1.4131e+09, device='cuda:0')
c= tensor(1.4171e+09, device='cuda:0')
c= tensor(1.4172e+09, device='cuda:0')
c= tensor(1.4577e+09, device='cuda:0')
c= tensor(1.4578e+09, device='cuda:0')
c= tensor(1.4584e+09, device='cuda:0')
c= tensor(1.4618e+09, device='cuda:0')
c= tensor(1.4618e+09, device='cuda:0')
c= tensor(1.4677e+09, device='cuda:0')
c= tensor(1.5074e+09, device='cuda:0')
c= tensor(1.8106e+09, device='cuda:0')
c= tensor(1.8111e+09, device='cuda:0')
c= tensor(1.8116e+09, device='cuda:0')
c= tensor(1.8116e+09, device='cuda:0')
c= tensor(1.8116e+09, device='cuda:0')
c= tensor(1.8268e+09, device='cuda:0')
c= tensor(1.8269e+09, device='cuda:0')
c= tensor(1.8284e+09, device='cuda:0')
c= tensor(1.8309e+09, device='cuda:0')
c= tensor(1.8321e+09, device='cuda:0')
c= tensor(1.8326e+09, device='cuda:0')
c= tensor(1.8326e+09, device='cuda:0')
c= tensor(1.8537e+09, device='cuda:0')
c= tensor(1.8735e+09, device='cuda:0')
c= tensor(1.8752e+09, device='cuda:0')
c= tensor(1.8752e+09, device='cuda:0')
c= tensor(1.8827e+09, device='cuda:0')
c= tensor(1.8829e+09, device='cuda:0')
c= tensor(1.9071e+09, device='cuda:0')
c= tensor(1.9073e+09, device='cuda:0')
c= tensor(1.9117e+09, device='cuda:0')
c= tensor(1.9118e+09, device='cuda:0')
c= tensor(1.9346e+09, device='cuda:0')
c= tensor(1.9365e+09, device='cuda:0')
c= tensor(1.9366e+09, device='cuda:0')
c= tensor(1.9438e+09, device='cuda:0')
c= tensor(1.9556e+09, device='cuda:0')
c= tensor(1.9567e+09, device='cuda:0')
c= tensor(1.9715e+09, device='cuda:0')
c= tensor(1.9861e+09, device='cuda:0')
c= tensor(2.0425e+09, device='cuda:0')
c= tensor(2.0432e+09, device='cuda:0')
c= tensor(2.0432e+09, device='cuda:0')
c= tensor(2.0433e+09, device='cuda:0')
c= tensor(2.0437e+09, device='cuda:0')
c= tensor(2.0441e+09, device='cuda:0')
c= tensor(2.0457e+09, device='cuda:0')
c= tensor(2.0457e+09, device='cuda:0')
c= tensor(2.0486e+09, device='cuda:0')
c= tensor(2.0586e+09, device='cuda:0')
c= tensor(2.0601e+09, device='cuda:0')
c= tensor(2.0602e+09, device='cuda:0')
c= tensor(2.0619e+09, device='cuda:0')
c= tensor(2.0620e+09, device='cuda:0')
c= tensor(2.0623e+09, device='cuda:0')
c= tensor(2.0624e+09, device='cuda:0')
c= tensor(2.0626e+09, device='cuda:0')
c= tensor(2.0764e+09, device='cuda:0')
c= tensor(2.0772e+09, device='cuda:0')
c= tensor(2.0775e+09, device='cuda:0')
c= tensor(2.0793e+09, device='cuda:0')
c= tensor(2.0794e+09, device='cuda:0')
c= tensor(3.2741e+09, device='cuda:0')
c= tensor(3.2742e+09, device='cuda:0')
c= tensor(3.2810e+09, device='cuda:0')
c= tensor(3.2810e+09, device='cuda:0')
c= tensor(3.2810e+09, device='cuda:0')
c= tensor(3.2810e+09, device='cuda:0')
c= tensor(3.2811e+09, device='cuda:0')
c= tensor(3.2811e+09, device='cuda:0')
c= tensor(3.2887e+09, device='cuda:0')
c= tensor(3.2887e+09, device='cuda:0')
c= tensor(3.2887e+09, device='cuda:0')
c= tensor(3.3128e+09, device='cuda:0')
c= tensor(3.3146e+09, device='cuda:0')
c= tensor(3.3155e+09, device='cuda:0')
c= tensor(3.3215e+09, device='cuda:0')
c= tensor(3.3547e+09, device='cuda:0')
c= tensor(3.3547e+09, device='cuda:0')
c= tensor(3.3548e+09, device='cuda:0')
c= tensor(3.3553e+09, device='cuda:0')
c= tensor(3.3554e+09, device='cuda:0')
c= tensor(3.3554e+09, device='cuda:0')
c= tensor(3.3555e+09, device='cuda:0')
c= tensor(3.3555e+09, device='cuda:0')
c= tensor(3.3556e+09, device='cuda:0')
c= tensor(3.3556e+09, device='cuda:0')
c= tensor(3.3557e+09, device='cuda:0')
c= tensor(3.4139e+09, device='cuda:0')
c= tensor(3.4141e+09, device='cuda:0')
c= tensor(3.4183e+09, device='cuda:0')
c= tensor(3.4185e+09, device='cuda:0')
c= tensor(3.4185e+09, device='cuda:0')
c= tensor(3.4209e+09, device='cuda:0')
c= tensor(3.4785e+09, device='cuda:0')
c= tensor(3.5528e+09, device='cuda:0')
c= tensor(3.5534e+09, device='cuda:0')
c= tensor(3.5546e+09, device='cuda:0')
c= tensor(3.5546e+09, device='cuda:0')
c= tensor(3.5552e+09, device='cuda:0')
c= tensor(4.1385e+09, device='cuda:0')
c= tensor(4.1390e+09, device='cuda:0')
c= tensor(4.1390e+09, device='cuda:0')
c= tensor(4.1410e+09, device='cuda:0')
c= tensor(4.3089e+09, device='cuda:0')
c= tensor(4.3103e+09, device='cuda:0')
c= tensor(4.3105e+09, device='cuda:0')
c= tensor(4.3107e+09, device='cuda:0')
c= tensor(4.3107e+09, device='cuda:0')
c= tensor(4.3107e+09, device='cuda:0')
c= tensor(4.3239e+09, device='cuda:0')
c= tensor(4.3241e+09, device='cuda:0')
c= tensor(4.3242e+09, device='cuda:0')
c= tensor(4.3264e+09, device='cuda:0')
c= tensor(4.3266e+09, device='cuda:0')
c= tensor(4.3266e+09, device='cuda:0')
c= tensor(4.3291e+09, device='cuda:0')
c= tensor(4.3340e+09, device='cuda:0')
c= tensor(4.3621e+09, device='cuda:0')
c= tensor(4.3850e+09, device='cuda:0')
c= tensor(4.4092e+09, device='cuda:0')
c= tensor(4.4096e+09, device='cuda:0')
c= tensor(4.4112e+09, device='cuda:0')
c= tensor(4.4173e+09, device='cuda:0')
c= tensor(4.4348e+09, device='cuda:0')
c= tensor(4.4349e+09, device='cuda:0')
c= tensor(4.5083e+09, device='cuda:0')
c= tensor(4.8167e+09, device='cuda:0')
c= tensor(4.8433e+09, device='cuda:0')
c= tensor(4.8502e+09, device='cuda:0')
c= tensor(4.8540e+09, device='cuda:0')
c= tensor(4.8542e+09, device='cuda:0')
c= tensor(4.8542e+09, device='cuda:0')
c= tensor(4.8547e+09, device='cuda:0')
c= tensor(4.8605e+09, device='cuda:0')
c= tensor(4.8667e+09, device='cuda:0')
c= tensor(4.9129e+09, device='cuda:0')
c= tensor(4.9221e+09, device='cuda:0')
c= tensor(4.9253e+09, device='cuda:0')
c= tensor(4.9258e+09, device='cuda:0')
c= tensor(4.9485e+09, device='cuda:0')
c= tensor(4.9485e+09, device='cuda:0')
c= tensor(4.9486e+09, device='cuda:0')
c= tensor(4.9585e+09, device='cuda:0')
c= tensor(4.9595e+09, device='cuda:0')
c= tensor(4.9595e+09, device='cuda:0')
c= tensor(4.9599e+09, device='cuda:0')
c= tensor(5.1282e+09, device='cuda:0')
c= tensor(5.1284e+09, device='cuda:0')
c= tensor(5.1355e+09, device='cuda:0')
c= tensor(5.1356e+09, device='cuda:0')
c= tensor(5.1357e+09, device='cuda:0')
c= tensor(5.1357e+09, device='cuda:0')
c= tensor(5.1359e+09, device='cuda:0')
c= tensor(5.1362e+09, device='cuda:0')
c= tensor(5.1384e+09, device='cuda:0')
c= tensor(5.1386e+09, device='cuda:0')
c= tensor(5.1485e+09, device='cuda:0')
c= tensor(5.1486e+09, device='cuda:0')
c= tensor(5.1523e+09, device='cuda:0')
c= tensor(5.1525e+09, device='cuda:0')
c= tensor(5.1566e+09, device='cuda:0')
c= tensor(5.1567e+09, device='cuda:0')
c= tensor(5.1570e+09, device='cuda:0')
c= tensor(5.1574e+09, device='cuda:0')
c= tensor(5.1579e+09, device='cuda:0')
c= tensor(5.1614e+09, device='cuda:0')
c= tensor(5.3148e+09, device='cuda:0')
c= tensor(5.3149e+09, device='cuda:0')
c= tensor(5.3150e+09, device='cuda:0')
c= tensor(5.3178e+09, device='cuda:0')
c= tensor(5.3180e+09, device='cuda:0')
c= tensor(5.3967e+09, device='cuda:0')
c= tensor(5.3967e+09, device='cuda:0')
c= tensor(5.4022e+09, device='cuda:0')
c= tensor(5.4210e+09, device='cuda:0')
c= tensor(5.4210e+09, device='cuda:0')
c= tensor(5.4551e+09, device='cuda:0')
c= tensor(5.4564e+09, device='cuda:0')
c= tensor(5.5724e+09, device='cuda:0')
c= tensor(5.5725e+09, device='cuda:0')
c= tensor(5.5729e+09, device='cuda:0')
c= tensor(5.5731e+09, device='cuda:0')
c= tensor(5.5731e+09, device='cuda:0')
c= tensor(5.5731e+09, device='cuda:0')
c= tensor(5.5776e+09, device='cuda:0')
c= tensor(5.5778e+09, device='cuda:0')
c= tensor(5.5867e+09, device='cuda:0')
c= tensor(5.5868e+09, device='cuda:0')
c= tensor(5.5868e+09, device='cuda:0')
c= tensor(5.5871e+09, device='cuda:0')
c= tensor(5.6076e+09, device='cuda:0')
c= tensor(5.6134e+09, device='cuda:0')
c= tensor(5.6406e+09, device='cuda:0')
c= tensor(5.6413e+09, device='cuda:0')
c= tensor(5.6414e+09, device='cuda:0')
c= tensor(5.6414e+09, device='cuda:0')
c= tensor(5.6415e+09, device='cuda:0')
c= tensor(5.6872e+09, device='cuda:0')
c= tensor(5.6872e+09, device='cuda:0')
c= tensor(5.6875e+09, device='cuda:0')
c= tensor(5.6918e+09, device='cuda:0')
c= tensor(5.6932e+09, device='cuda:0')
c= tensor(5.6932e+09, device='cuda:0')
c= tensor(5.6933e+09, device='cuda:0')
c= tensor(5.7298e+09, device='cuda:0')
c= tensor(5.7309e+09, device='cuda:0')
c= tensor(5.7312e+09, device='cuda:0')
c= tensor(5.7316e+09, device='cuda:0')
c= tensor(5.7484e+09, device='cuda:0')
c= tensor(5.7532e+09, device='cuda:0')
c= tensor(5.8325e+09, device='cuda:0')
c= tensor(5.8386e+09, device='cuda:0')
c= tensor(5.8387e+09, device='cuda:0')
c= tensor(5.8397e+09, device='cuda:0')
c= tensor(5.8416e+09, device='cuda:0')
c= tensor(5.8417e+09, device='cuda:0')
c= tensor(5.8418e+09, device='cuda:0')
c= tensor(5.8418e+09, device='cuda:0')
c= tensor(5.8435e+09, device='cuda:0')
c= tensor(5.8439e+09, device='cuda:0')
c= tensor(5.8439e+09, device='cuda:0')
c= tensor(5.8440e+09, device='cuda:0')
c= tensor(5.8441e+09, device='cuda:0')
c= tensor(5.8455e+09, device='cuda:0')
c= tensor(5.8456e+09, device='cuda:0')
c= tensor(5.8461e+09, device='cuda:0')
c= tensor(5.8479e+09, device='cuda:0')
c= tensor(5.8483e+09, device='cuda:0')
c= tensor(5.8483e+09, device='cuda:0')
c= tensor(5.8484e+09, device='cuda:0')
c= tensor(5.8486e+09, device='cuda:0')
c= tensor(5.8638e+09, device='cuda:0')
c= tensor(5.8639e+09, device='cuda:0')
c= tensor(5.8639e+09, device='cuda:0')
c= tensor(5.8639e+09, device='cuda:0')
c= tensor(5.8758e+09, device='cuda:0')
c= tensor(6.0517e+09, device='cuda:0')
c= tensor(6.0538e+09, device='cuda:0')
c= tensor(6.0539e+09, device='cuda:0')
c= tensor(6.0702e+09, device='cuda:0')
c= tensor(6.0758e+09, device='cuda:0')
c= tensor(6.0759e+09, device='cuda:0')
c= tensor(6.0760e+09, device='cuda:0')
c= tensor(6.0763e+09, device='cuda:0')
c= tensor(6.0963e+09, device='cuda:0')
c= tensor(6.2056e+09, device='cuda:0')
c= tensor(6.2187e+09, device='cuda:0')
c= tensor(6.2190e+09, device='cuda:0')
c= tensor(6.2190e+09, device='cuda:0')
c= tensor(6.2191e+09, device='cuda:0')
c= tensor(6.2215e+09, device='cuda:0')
c= tensor(6.2216e+09, device='cuda:0')
c= tensor(6.2231e+09, device='cuda:0')
c= tensor(6.2338e+09, device='cuda:0')
c= tensor(6.3108e+09, device='cuda:0')
c= tensor(6.3108e+09, device='cuda:0')
c= tensor(6.3108e+09, device='cuda:0')
c= tensor(6.3115e+09, device='cuda:0')
c= tensor(6.3329e+09, device='cuda:0')
c= tensor(6.3344e+09, device='cuda:0')
c= tensor(6.3347e+09, device='cuda:0')
c= tensor(6.3347e+09, device='cuda:0')
c= tensor(6.3352e+09, device='cuda:0')
c= tensor(6.3352e+09, device='cuda:0')
c= tensor(6.3381e+09, device='cuda:0')
c= tensor(6.3382e+09, device='cuda:0')
c= tensor(6.3383e+09, device='cuda:0')
c= tensor(6.3383e+09, device='cuda:0')
c= tensor(6.3384e+09, device='cuda:0')
c= tensor(6.3384e+09, device='cuda:0')
c= tensor(6.3484e+09, device='cuda:0')
c= tensor(6.3962e+09, device='cuda:0')
c= tensor(6.4054e+09, device='cuda:0')
c= tensor(6.4150e+09, device='cuda:0')
c= tensor(6.4152e+09, device='cuda:0')
c= tensor(6.4153e+09, device='cuda:0')
c= tensor(6.4155e+09, device='cuda:0')
c= tensor(6.4224e+09, device='cuda:0')
c= tensor(6.4231e+09, device='cuda:0')
c= tensor(6.4261e+09, device='cuda:0')
c= tensor(6.4262e+09, device='cuda:0')
c= tensor(6.8823e+09, device='cuda:0')
c= tensor(6.8824e+09, device='cuda:0')
c= tensor(6.8839e+09, device='cuda:0')
c= tensor(6.9092e+09, device='cuda:0')
c= tensor(6.9104e+09, device='cuda:0')
c= tensor(6.9109e+09, device='cuda:0')
c= tensor(7.1612e+09, device='cuda:0')
c= tensor(7.1704e+09, device='cuda:0')
c= tensor(7.1729e+09, device='cuda:0')
c= tensor(7.1730e+09, device='cuda:0')
c= tensor(7.1740e+09, device='cuda:0')
c= tensor(7.1741e+09, device='cuda:0')
c= tensor(7.2005e+09, device='cuda:0')
c= tensor(7.5488e+09, device='cuda:0')
c= tensor(7.5626e+09, device='cuda:0')
c= tensor(7.5768e+09, device='cuda:0')
c= tensor(7.5836e+09, device='cuda:0')
c= tensor(7.5844e+09, device='cuda:0')
c= tensor(7.5852e+09, device='cuda:0')
c= tensor(7.5854e+09, device='cuda:0')
c= tensor(7.6761e+09, device='cuda:0')
c= tensor(7.7206e+09, device='cuda:0')
c= tensor(7.7240e+09, device='cuda:0')
c= tensor(8.5926e+09, device='cuda:0')
c= tensor(8.6111e+09, device='cuda:0')
c= tensor(8.6121e+09, device='cuda:0')
c= tensor(8.6121e+09, device='cuda:0')
c= tensor(8.6152e+09, device='cuda:0')
c= tensor(8.6200e+09, device='cuda:0')
c= tensor(8.6201e+09, device='cuda:0')
c= tensor(8.7214e+09, device='cuda:0')
c= tensor(8.7219e+09, device='cuda:0')
c= tensor(8.7225e+09, device='cuda:0')
c= tensor(8.7226e+09, device='cuda:0')
c= tensor(8.7226e+09, device='cuda:0')
c= tensor(8.7227e+09, device='cuda:0')
c= tensor(8.7227e+09, device='cuda:0')
c= tensor(8.7231e+09, device='cuda:0')
c= tensor(8.7241e+09, device='cuda:0')
c= tensor(1.2468e+10, device='cuda:0')
c= tensor(1.2470e+10, device='cuda:0')
c= tensor(1.2478e+10, device='cuda:0')
c= tensor(1.2478e+10, device='cuda:0')
c= tensor(1.2478e+10, device='cuda:0')
c= tensor(1.2478e+10, device='cuda:0')
c= tensor(1.2507e+10, device='cuda:0')
c= tensor(1.2511e+10, device='cuda:0')
c= tensor(1.2778e+10, device='cuda:0')
c= tensor(1.2778e+10, device='cuda:0')
c= tensor(1.2789e+10, device='cuda:0')
c= tensor(1.2789e+10, device='cuda:0')
c= tensor(1.2799e+10, device='cuda:0')
c= tensor(1.2830e+10, device='cuda:0')
c= tensor(1.2830e+10, device='cuda:0')
c= tensor(1.2830e+10, device='cuda:0')
c= tensor(1.2832e+10, device='cuda:0')
c= tensor(1.2833e+10, device='cuda:0')
c= tensor(1.2835e+10, device='cuda:0')
c= tensor(1.2865e+10, device='cuda:0')
c= tensor(1.2866e+10, device='cuda:0')
c= tensor(1.2870e+10, device='cuda:0')
c= tensor(1.2870e+10, device='cuda:0')
c= tensor(1.2877e+10, device='cuda:0')
c= tensor(1.2906e+10, device='cuda:0')
c= tensor(1.2906e+10, device='cuda:0')
c= tensor(1.2907e+10, device='cuda:0')
c= tensor(1.2925e+10, device='cuda:0')
c= tensor(1.2929e+10, device='cuda:0')
c= tensor(1.3155e+10, device='cuda:0')
c= tensor(1.3157e+10, device='cuda:0')
c= tensor(1.3158e+10, device='cuda:0')
c= tensor(1.3159e+10, device='cuda:0')
c= tensor(1.3183e+10, device='cuda:0')
c= tensor(1.3200e+10, device='cuda:0')
c= tensor(1.3201e+10, device='cuda:0')
c= tensor(1.3201e+10, device='cuda:0')
c= tensor(1.3201e+10, device='cuda:0')
c= tensor(1.3205e+10, device='cuda:0')
c= tensor(1.3212e+10, device='cuda:0')
c= tensor(1.3214e+10, device='cuda:0')
c= tensor(1.3214e+10, device='cuda:0')
c= tensor(1.3214e+10, device='cuda:0')
c= tensor(1.3217e+10, device='cuda:0')
c= tensor(1.3218e+10, device='cuda:0')
c= tensor(1.3218e+10, device='cuda:0')
c= tensor(1.3220e+10, device='cuda:0')
c= tensor(1.3221e+10, device='cuda:0')
c= tensor(1.3222e+10, device='cuda:0')
c= tensor(1.3222e+10, device='cuda:0')
c= tensor(1.3222e+10, device='cuda:0')
c= tensor(1.3230e+10, device='cuda:0')
c= tensor(1.3230e+10, device='cuda:0')
c= tensor(1.3233e+10, device='cuda:0')
c= tensor(1.3233e+10, device='cuda:0')
c= tensor(1.3233e+10, device='cuda:0')
c= tensor(1.3235e+10, device='cuda:0')
c= tensor(1.3237e+10, device='cuda:0')
c= tensor(1.3237e+10, device='cuda:0')
c= tensor(1.3237e+10, device='cuda:0')
c= tensor(1.3237e+10, device='cuda:0')
c= tensor(1.3238e+10, device='cuda:0')
c= tensor(1.3239e+10, device='cuda:0')
c= tensor(1.3249e+10, device='cuda:0')
c= tensor(1.3249e+10, device='cuda:0')
c= tensor(1.3250e+10, device='cuda:0')
c= tensor(1.3251e+10, device='cuda:0')
c= tensor(1.3251e+10, device='cuda:0')
c= tensor(1.3272e+10, device='cuda:0')
c= tensor(1.3272e+10, device='cuda:0')
c= tensor(1.3283e+10, device='cuda:0')
c= tensor(1.3290e+10, device='cuda:0')
c= tensor(1.3291e+10, device='cuda:0')
c= tensor(1.3299e+10, device='cuda:0')
c= tensor(1.3305e+10, device='cuda:0')
c= tensor(1.3305e+10, device='cuda:0')
c= tensor(1.3306e+10, device='cuda:0')
c= tensor(1.3307e+10, device='cuda:0')
c= tensor(1.3316e+10, device='cuda:0')
c= tensor(1.3324e+10, device='cuda:0')
c= tensor(1.3324e+10, device='cuda:0')
c= tensor(1.3327e+10, device='cuda:0')
c= tensor(1.3327e+10, device='cuda:0')
c= tensor(1.3332e+10, device='cuda:0')
c= tensor(1.3332e+10, device='cuda:0')
c= tensor(1.3333e+10, device='cuda:0')
c= tensor(1.3368e+10, device='cuda:0')
c= tensor(1.3485e+10, device='cuda:0')
c= tensor(1.3486e+10, device='cuda:0')
c= tensor(1.3486e+10, device='cuda:0')
c= tensor(1.3486e+10, device='cuda:0')
c= tensor(1.3502e+10, device='cuda:0')
c= tensor(1.3503e+10, device='cuda:0')
c= tensor(1.3503e+10, device='cuda:0')
c= tensor(1.3504e+10, device='cuda:0')
c= tensor(1.3514e+10, device='cuda:0')
c= tensor(1.3514e+10, device='cuda:0')
c= tensor(1.3515e+10, device='cuda:0')
c= tensor(1.3516e+10, device='cuda:0')
memory (bytes)
6364680192
time for making loss 2 is 10.784741163253784
p0 True
it  0 : 3921132032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 96% |
shape of L is 
torch.Size([])
memory (bytes)
6364946432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 42% |
memory (bytes)
6365347840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  126376710000.0
relative error loss 9.350273
shape of L is 
torch.Size([])
memory (bytes)
6435569664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 43% |
memory (bytes)
6435573760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  126376330000.0
relative error loss 9.350245
shape of L is 
torch.Size([])
memory (bytes)
6437933056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 43% |
memory (bytes)
6437933056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  126375810000.0
relative error loss 9.350207
shape of L is 
torch.Size([])
memory (bytes)
6439026688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 43% |
memory (bytes)
6439026688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  126371774000.0
relative error loss 9.349909
shape of L is 
torch.Size([])
memory (bytes)
6439997440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 43% |
memory (bytes)
6440128512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  126349470000.0
relative error loss 9.348258
shape of L is 
torch.Size([])
memory (bytes)
6441238528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
memory (bytes)
6441238528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 43% |
error is  126226510000.0
relative error loss 9.339161
shape of L is 
torch.Size([])
memory (bytes)
6442184704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
memory (bytes)
6442299392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  124881840000.0
relative error loss 9.239672
shape of L is 
torch.Size([])
memory (bytes)
6443360256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6443360256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  112109860000.0
relative error loss 8.294708
shape of L is 
torch.Size([])
memory (bytes)
6444429312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
memory (bytes)
6444429312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  64248123000.0
relative error loss 4.753546
shape of L is 
torch.Size([])
memory (bytes)
6445494272
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6445498368
| ID | GPU  | MEM |
-------------------
|  0 |   3% |  0% |
|  1 | 100% | 43% |
error is  27434045000.0
relative error loss 2.0297713
time to take a step is 180.8471384048462
it  1 : 4320194048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6446559232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
memory (bytes)
6446559232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  27434045000.0
relative error loss 2.0297713
shape of L is 
torch.Size([])
memory (bytes)
6447366144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6447628288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  18073129000.0
relative error loss 1.3371823
shape of L is 
torch.Size([])
memory (bytes)
6448689152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
memory (bytes)
6448689152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  14582431000.0
relative error loss 1.0789149
shape of L is 
torch.Size([])
memory (bytes)
6449590272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 43% |
memory (bytes)
6449590272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  73799434000.0
relative error loss 5.4602222
shape of L is 
torch.Size([])
memory (bytes)
6450610176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6450827264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  14259377000.0
relative error loss 1.0550131
shape of L is 
torch.Size([])
memory (bytes)
6451888128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 43% |
memory (bytes)
6451888128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  13977546000.0
relative error loss 1.0341611
shape of L is 
torch.Size([])
memory (bytes)
6452785152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
memory (bytes)
6452969472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  13138139000.0
relative error loss 0.9720557
shape of L is 
torch.Size([])
memory (bytes)
6454009856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
memory (bytes)
6454009856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  12409387000.0
relative error loss 0.91813725
shape of L is 
torch.Size([])
memory (bytes)
6455099392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
memory (bytes)
6455099392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  11914356000.0
relative error loss 0.8815112
shape of L is 
torch.Size([])
memory (bytes)
6455934976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 43% |
memory (bytes)
6455934976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  11592591000.0
relative error loss 0.8577047
time to take a step is 175.0262598991394
it  2 : 4639444480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6457073664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 43% |
memory (bytes)
6457073664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  11592591000.0
relative error loss 0.8577047
shape of L is 
torch.Size([])
memory (bytes)
6458290176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 43% |
memory (bytes)
6458290176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  165350280000.0
relative error loss 12.233823
shape of L is 
torch.Size([])
memory (bytes)
6459211776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 43% |
memory (bytes)
6459211776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  12253290000.0
relative error loss 0.9065881
shape of L is 
torch.Size([])
memory (bytes)
6461345792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6461345792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  11152458000.0
relative error loss 0.8251404
shape of L is 
torch.Size([])
memory (bytes)
6463610880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 43% |
memory (bytes)
6463610880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  10642835000.0
relative error loss 0.7874348
shape of L is 
torch.Size([])
memory (bytes)
6465720320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
memory (bytes)
6465720320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  9864390000.0
relative error loss 0.7298397
shape of L is 
torch.Size([])
memory (bytes)
6467690496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 43% |
memory (bytes)
6467690496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  8982055000.0
relative error loss 0.6645581
shape of L is 
torch.Size([])
memory (bytes)
6469976064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 43% |
memory (bytes)
6469976064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  8531912000.0
relative error loss 0.6312533
shape of L is 
torch.Size([])
memory (bytes)
6472105984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 43% |
memory (bytes)
6472105984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  8209015300.0
relative error loss 0.607363
shape of L is 
torch.Size([])
memory (bytes)
6474035200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 43% |
memory (bytes)
6474256384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  7881683000.0
relative error loss 0.58314455
time to take a step is 174.40529108047485
it  3 : 4639444480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6476402688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 43% |
memory (bytes)
6476402688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  7881683000.0
relative error loss 0.58314455
shape of L is 
torch.Size([])
memory (bytes)
6478331904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6478331904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  23172454000.0
relative error loss 1.7144676
shape of L is 
torch.Size([])
memory (bytes)
6480531456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 43% |
memory (bytes)
6480531456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  7543556600.0
relative error loss 0.5581275
shape of L is 
torch.Size([])
memory (bytes)
6482780160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 43% |
memory (bytes)
6482780160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  7198419000.0
relative error loss 0.5325917
shape of L is 
torch.Size([])
memory (bytes)
6484901888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 43% |
memory (bytes)
6484901888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  6944931000.0
relative error loss 0.5138368
shape of L is 
torch.Size([])
memory (bytes)
6486847488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
memory (bytes)
6487048192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  6623757000.0
relative error loss 0.49007398
shape of L is 
torch.Size([])
memory (bytes)
6489186304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
memory (bytes)
6489186304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  6297710000.0
relative error loss 0.46595067
shape of L is 
torch.Size([])
memory (bytes)
6491095040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 43% |
memory (bytes)
6491316224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  6143569000.0
relative error loss 0.45454618
shape of L is 
torch.Size([])
memory (bytes)
6493454336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6493454336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5946777000.0
relative error loss 0.43998608
shape of L is 
torch.Size([])
memory (bytes)
6495571968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6495571968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5590295600.0
relative error loss 0.41361097
time to take a step is 173.86717891693115
c= tensor(3270.6470, device='cuda:0')
c= tensor(220250.2344, device='cuda:0')
c= tensor(226662.0781, device='cuda:0')
c= tensor(475014.0625, device='cuda:0')
c= tensor(620785.5625, device='cuda:0')
c= tensor(1374702.7500, device='cuda:0')
c= tensor(2661509., device='cuda:0')
c= tensor(3456954.5000, device='cuda:0')
c= tensor(3753129.5000, device='cuda:0')
c= tensor(7301484., device='cuda:0')
c= tensor(7399151., device='cuda:0')
c= tensor(16622246., device='cuda:0')
c= tensor(16658112., device='cuda:0')
c= tensor(67607176., device='cuda:0')
c= tensor(67991240., device='cuda:0')
c= tensor(68955920., device='cuda:0')
c= tensor(70652536., device='cuda:0')
c= tensor(71366608., device='cuda:0')
c= tensor(83485984., device='cuda:0')
c= tensor(88645416., device='cuda:0')
c= tensor(90093688., device='cuda:0')
c= tensor(1.0357e+08, device='cuda:0')
c= tensor(1.0362e+08, device='cuda:0')
c= tensor(1.0592e+08, device='cuda:0')
c= tensor(1.0619e+08, device='cuda:0')
c= tensor(1.0815e+08, device='cuda:0')
c= tensor(1.1150e+08, device='cuda:0')
c= tensor(1.1160e+08, device='cuda:0')
c= tensor(1.3142e+08, device='cuda:0')
c= tensor(5.4338e+08, device='cuda:0')
c= tensor(5.4358e+08, device='cuda:0')
c= tensor(9.1410e+08, device='cuda:0')
c= tensor(9.1550e+08, device='cuda:0')
c= tensor(9.1560e+08, device='cuda:0')
c= tensor(9.1580e+08, device='cuda:0')
c= tensor(9.5579e+08, device='cuda:0')
c= tensor(9.5983e+08, device='cuda:0')
c= tensor(9.5983e+08, device='cuda:0')
c= tensor(9.5984e+08, device='cuda:0')
c= tensor(9.5985e+08, device='cuda:0')
c= tensor(9.5986e+08, device='cuda:0')
c= tensor(9.5989e+08, device='cuda:0')
c= tensor(9.5989e+08, device='cuda:0')
c= tensor(9.5991e+08, device='cuda:0')
c= tensor(9.5991e+08, device='cuda:0')
c= tensor(9.5991e+08, device='cuda:0')
c= tensor(9.5993e+08, device='cuda:0')
c= tensor(9.5994e+08, device='cuda:0')
c= tensor(9.5995e+08, device='cuda:0')
c= tensor(9.6019e+08, device='cuda:0')
c= tensor(9.6030e+08, device='cuda:0')
c= tensor(9.6031e+08, device='cuda:0')
c= tensor(9.6039e+08, device='cuda:0')
c= tensor(9.6040e+08, device='cuda:0')
c= tensor(9.6042e+08, device='cuda:0')
c= tensor(9.6047e+08, device='cuda:0')
c= tensor(9.6048e+08, device='cuda:0')
c= tensor(9.6050e+08, device='cuda:0')
c= tensor(9.6051e+08, device='cuda:0')
c= tensor(9.6052e+08, device='cuda:0')
c= tensor(9.6054e+08, device='cuda:0')
c= tensor(9.6054e+08, device='cuda:0')
c= tensor(9.6058e+08, device='cuda:0')
c= tensor(9.6066e+08, device='cuda:0')
c= tensor(9.6067e+08, device='cuda:0')
c= tensor(9.6068e+08, device='cuda:0')
c= tensor(9.6069e+08, device='cuda:0')
c= tensor(9.6073e+08, device='cuda:0')
c= tensor(9.6076e+08, device='cuda:0')
c= tensor(9.6077e+08, device='cuda:0')
c= tensor(9.6079e+08, device='cuda:0')
c= tensor(9.6080e+08, device='cuda:0')
c= tensor(9.6082e+08, device='cuda:0')
c= tensor(9.6083e+08, device='cuda:0')
c= tensor(9.6085e+08, device='cuda:0')
c= tensor(9.6090e+08, device='cuda:0')
c= tensor(9.6090e+08, device='cuda:0')
c= tensor(9.6091e+08, device='cuda:0')
c= tensor(9.6093e+08, device='cuda:0')
c= tensor(9.6108e+08, device='cuda:0')
c= tensor(9.6109e+08, device='cuda:0')
c= tensor(9.6110e+08, device='cuda:0')
c= tensor(9.6113e+08, device='cuda:0')
c= tensor(9.6113e+08, device='cuda:0')
c= tensor(9.6114e+08, device='cuda:0')
c= tensor(9.6114e+08, device='cuda:0')
c= tensor(9.6115e+08, device='cuda:0')
c= tensor(9.6115e+08, device='cuda:0')
c= tensor(9.6117e+08, device='cuda:0')
c= tensor(9.6118e+08, device='cuda:0')
c= tensor(9.6119e+08, device='cuda:0')
c= tensor(9.6120e+08, device='cuda:0')
c= tensor(9.6121e+08, device='cuda:0')
c= tensor(9.6123e+08, device='cuda:0')
c= tensor(9.6125e+08, device='cuda:0')
c= tensor(9.6127e+08, device='cuda:0')
c= tensor(9.6128e+08, device='cuda:0')
c= tensor(9.6132e+08, device='cuda:0')
c= tensor(9.6133e+08, device='cuda:0')
c= tensor(9.6135e+08, device='cuda:0')
c= tensor(9.6143e+08, device='cuda:0')
c= tensor(9.6144e+08, device='cuda:0')
c= tensor(9.6147e+08, device='cuda:0')
c= tensor(9.6148e+08, device='cuda:0')
c= tensor(9.6160e+08, device='cuda:0')
c= tensor(9.6162e+08, device='cuda:0')
c= tensor(9.6164e+08, device='cuda:0')
c= tensor(9.6164e+08, device='cuda:0')
c= tensor(9.6165e+08, device='cuda:0')
c= tensor(9.6166e+08, device='cuda:0')
c= tensor(9.6166e+08, device='cuda:0')
c= tensor(9.6167e+08, device='cuda:0')
c= tensor(9.6167e+08, device='cuda:0')
c= tensor(9.6169e+08, device='cuda:0')
c= tensor(9.6171e+08, device='cuda:0')
c= tensor(9.6174e+08, device='cuda:0')
c= tensor(9.6175e+08, device='cuda:0')
c= tensor(9.6175e+08, device='cuda:0')
c= tensor(9.6179e+08, device='cuda:0')
c= tensor(9.6179e+08, device='cuda:0')
c= tensor(9.6185e+08, device='cuda:0')
c= tensor(9.6186e+08, device='cuda:0')
c= tensor(9.6186e+08, device='cuda:0')
c= tensor(9.6187e+08, device='cuda:0')
c= tensor(9.6188e+08, device='cuda:0')
c= tensor(9.6188e+08, device='cuda:0')
c= tensor(9.6189e+08, device='cuda:0')
c= tensor(9.6191e+08, device='cuda:0')
c= tensor(9.6198e+08, device='cuda:0')
c= tensor(9.6199e+08, device='cuda:0')
c= tensor(9.6203e+08, device='cuda:0')
c= tensor(9.6204e+08, device='cuda:0')
c= tensor(9.6205e+08, device='cuda:0')
c= tensor(9.6205e+08, device='cuda:0')
c= tensor(9.6217e+08, device='cuda:0')
c= tensor(9.6218e+08, device='cuda:0')
c= tensor(9.6218e+08, device='cuda:0')
c= tensor(9.6218e+08, device='cuda:0')
c= tensor(9.6219e+08, device='cuda:0')
c= tensor(9.6220e+08, device='cuda:0')
c= tensor(9.6221e+08, device='cuda:0')
c= tensor(9.6222e+08, device='cuda:0')
c= tensor(9.6232e+08, device='cuda:0')
c= tensor(9.6235e+08, device='cuda:0')
c= tensor(9.6238e+08, device='cuda:0')
c= tensor(9.6238e+08, device='cuda:0')
c= tensor(9.6239e+08, device='cuda:0')
c= tensor(9.6239e+08, device='cuda:0')
c= tensor(9.6240e+08, device='cuda:0')
c= tensor(9.6241e+08, device='cuda:0')
c= tensor(9.6242e+08, device='cuda:0')
c= tensor(9.6243e+08, device='cuda:0')
c= tensor(9.6244e+08, device='cuda:0')
c= tensor(9.6251e+08, device='cuda:0')
c= tensor(9.6252e+08, device='cuda:0')
c= tensor(9.6281e+08, device='cuda:0')
c= tensor(9.6282e+08, device='cuda:0')
c= tensor(9.6284e+08, device='cuda:0')
c= tensor(9.6285e+08, device='cuda:0')
c= tensor(9.6285e+08, device='cuda:0')
c= tensor(9.6289e+08, device='cuda:0')
c= tensor(9.6290e+08, device='cuda:0')
c= tensor(9.6296e+08, device='cuda:0')
c= tensor(9.6296e+08, device='cuda:0')
c= tensor(9.6300e+08, device='cuda:0')
c= tensor(9.6300e+08, device='cuda:0')
c= tensor(9.6301e+08, device='cuda:0')
c= tensor(9.6302e+08, device='cuda:0')
c= tensor(9.6304e+08, device='cuda:0')
c= tensor(9.6304e+08, device='cuda:0')
c= tensor(9.6305e+08, device='cuda:0')
c= tensor(9.6305e+08, device='cuda:0')
c= tensor(9.6309e+08, device='cuda:0')
c= tensor(9.6310e+08, device='cuda:0')
c= tensor(9.6311e+08, device='cuda:0')
c= tensor(9.6580e+08, device='cuda:0')
c= tensor(9.6583e+08, device='cuda:0')
c= tensor(9.6583e+08, device='cuda:0')
c= tensor(9.6588e+08, device='cuda:0')
c= tensor(9.6589e+08, device='cuda:0')
c= tensor(9.6591e+08, device='cuda:0')
c= tensor(9.6592e+08, device='cuda:0')
c= tensor(9.6593e+08, device='cuda:0')
c= tensor(9.6594e+08, device='cuda:0')
c= tensor(9.6596e+08, device='cuda:0')
c= tensor(9.6598e+08, device='cuda:0')
c= tensor(9.6599e+08, device='cuda:0')
c= tensor(9.6601e+08, device='cuda:0')
c= tensor(9.6604e+08, device='cuda:0')
c= tensor(9.6609e+08, device='cuda:0')
c= tensor(9.6610e+08, device='cuda:0')
c= tensor(9.6610e+08, device='cuda:0')
c= tensor(9.6612e+08, device='cuda:0')
c= tensor(9.6617e+08, device='cuda:0')
c= tensor(9.6620e+08, device='cuda:0')
c= tensor(9.6621e+08, device='cuda:0')
c= tensor(9.6623e+08, device='cuda:0')
c= tensor(9.6623e+08, device='cuda:0')
c= tensor(9.6625e+08, device='cuda:0')
c= tensor(9.6625e+08, device='cuda:0')
c= tensor(9.6626e+08, device='cuda:0')
c= tensor(9.6627e+08, device='cuda:0')
c= tensor(9.6630e+08, device='cuda:0')
c= tensor(9.6631e+08, device='cuda:0')
c= tensor(9.6633e+08, device='cuda:0')
c= tensor(9.6634e+08, device='cuda:0')
c= tensor(9.6635e+08, device='cuda:0')
c= tensor(9.6637e+08, device='cuda:0')
c= tensor(9.6638e+08, device='cuda:0')
c= tensor(9.6644e+08, device='cuda:0')
c= tensor(9.6646e+08, device='cuda:0')
c= tensor(9.6655e+08, device='cuda:0')
c= tensor(9.6656e+08, device='cuda:0')
c= tensor(9.6657e+08, device='cuda:0')
c= tensor(9.6657e+08, device='cuda:0')
c= tensor(9.6658e+08, device='cuda:0')
c= tensor(9.6658e+08, device='cuda:0')
c= tensor(9.6658e+08, device='cuda:0')
c= tensor(9.6675e+08, device='cuda:0')
c= tensor(9.6676e+08, device='cuda:0')
c= tensor(9.6677e+08, device='cuda:0')
c= tensor(9.6677e+08, device='cuda:0')
c= tensor(9.6681e+08, device='cuda:0')
c= tensor(9.6684e+08, device='cuda:0')
c= tensor(9.6685e+08, device='cuda:0')
c= tensor(9.6687e+08, device='cuda:0')
c= tensor(9.6687e+08, device='cuda:0')
c= tensor(9.6687e+08, device='cuda:0')
c= tensor(9.6689e+08, device='cuda:0')
c= tensor(9.6690e+08, device='cuda:0')
c= tensor(9.6691e+08, device='cuda:0')
c= tensor(9.6692e+08, device='cuda:0')
c= tensor(9.6693e+08, device='cuda:0')
c= tensor(9.6695e+08, device='cuda:0')
c= tensor(9.6696e+08, device='cuda:0')
c= tensor(9.6699e+08, device='cuda:0')
c= tensor(9.6700e+08, device='cuda:0')
c= tensor(9.6702e+08, device='cuda:0')
c= tensor(9.6707e+08, device='cuda:0')
c= tensor(9.6752e+08, device='cuda:0')
c= tensor(9.7154e+08, device='cuda:0')
c= tensor(9.7158e+08, device='cuda:0')
c= tensor(9.7160e+08, device='cuda:0')
c= tensor(9.7160e+08, device='cuda:0')
c= tensor(9.7167e+08, device='cuda:0')
c= tensor(9.7190e+08, device='cuda:0')
c= tensor(9.9190e+08, device='cuda:0')
c= tensor(9.9191e+08, device='cuda:0')
c= tensor(1.0048e+09, device='cuda:0')
c= tensor(1.0116e+09, device='cuda:0')
c= tensor(1.0124e+09, device='cuda:0')
c= tensor(1.1721e+09, device='cuda:0')
c= tensor(1.1721e+09, device='cuda:0')
c= tensor(1.1722e+09, device='cuda:0')
c= tensor(1.2030e+09, device='cuda:0')
c= tensor(1.3109e+09, device='cuda:0')
c= tensor(1.3109e+09, device='cuda:0')
c= tensor(1.3116e+09, device='cuda:0')
c= tensor(1.3128e+09, device='cuda:0')
c= tensor(1.3139e+09, device='cuda:0')
c= tensor(1.3210e+09, device='cuda:0')
c= tensor(1.3257e+09, device='cuda:0')
c= tensor(1.3264e+09, device='cuda:0')
c= tensor(1.3268e+09, device='cuda:0')
c= tensor(1.3269e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3514e+09, device='cuda:0')
c= tensor(1.3517e+09, device='cuda:0')
c= tensor(1.3558e+09, device='cuda:0')
c= tensor(1.3849e+09, device='cuda:0')
c= tensor(1.3878e+09, device='cuda:0')
c= tensor(1.3878e+09, device='cuda:0')
c= tensor(1.3886e+09, device='cuda:0')
c= tensor(1.3886e+09, device='cuda:0')
c= tensor(1.3899e+09, device='cuda:0')
c= tensor(1.4005e+09, device='cuda:0')
c= tensor(1.4034e+09, device='cuda:0')
c= tensor(1.4063e+09, device='cuda:0')
c= tensor(1.4063e+09, device='cuda:0')
c= tensor(1.4063e+09, device='cuda:0')
c= tensor(1.4101e+09, device='cuda:0')
c= tensor(1.4131e+09, device='cuda:0')
c= tensor(1.4171e+09, device='cuda:0')
c= tensor(1.4172e+09, device='cuda:0')
c= tensor(1.4577e+09, device='cuda:0')
c= tensor(1.4578e+09, device='cuda:0')
c= tensor(1.4584e+09, device='cuda:0')
c= tensor(1.4618e+09, device='cuda:0')
c= tensor(1.4618e+09, device='cuda:0')
c= tensor(1.4677e+09, device='cuda:0')
c= tensor(1.5074e+09, device='cuda:0')
c= tensor(1.8106e+09, device='cuda:0')
c= tensor(1.8111e+09, device='cuda:0')
c= tensor(1.8116e+09, device='cuda:0')
c= tensor(1.8116e+09, device='cuda:0')
c= tensor(1.8116e+09, device='cuda:0')
c= tensor(1.8268e+09, device='cuda:0')
c= tensor(1.8269e+09, device='cuda:0')
c= tensor(1.8284e+09, device='cuda:0')
c= tensor(1.8309e+09, device='cuda:0')
c= tensor(1.8321e+09, device='cuda:0')
c= tensor(1.8326e+09, device='cuda:0')
c= tensor(1.8326e+09, device='cuda:0')
c= tensor(1.8537e+09, device='cuda:0')
c= tensor(1.8735e+09, device='cuda:0')
c= tensor(1.8752e+09, device='cuda:0')
c= tensor(1.8752e+09, device='cuda:0')
c= tensor(1.8827e+09, device='cuda:0')
c= tensor(1.8829e+09, device='cuda:0')
c= tensor(1.9071e+09, device='cuda:0')
c= tensor(1.9073e+09, device='cuda:0')
c= tensor(1.9117e+09, device='cuda:0')
c= tensor(1.9118e+09, device='cuda:0')
c= tensor(1.9346e+09, device='cuda:0')
c= tensor(1.9365e+09, device='cuda:0')
c= tensor(1.9366e+09, device='cuda:0')
c= tensor(1.9438e+09, device='cuda:0')
c= tensor(1.9556e+09, device='cuda:0')
c= tensor(1.9567e+09, device='cuda:0')
c= tensor(1.9715e+09, device='cuda:0')
c= tensor(1.9861e+09, device='cuda:0')
c= tensor(2.0425e+09, device='cuda:0')
c= tensor(2.0432e+09, device='cuda:0')
c= tensor(2.0432e+09, device='cuda:0')
c= tensor(2.0433e+09, device='cuda:0')
c= tensor(2.0437e+09, device='cuda:0')
c= tensor(2.0441e+09, device='cuda:0')
c= tensor(2.0457e+09, device='cuda:0')
c= tensor(2.0457e+09, device='cuda:0')
c= tensor(2.0486e+09, device='cuda:0')
c= tensor(2.0586e+09, device='cuda:0')
c= tensor(2.0601e+09, device='cuda:0')
c= tensor(2.0602e+09, device='cuda:0')
c= tensor(2.0619e+09, device='cuda:0')
c= tensor(2.0620e+09, device='cuda:0')
c= tensor(2.0623e+09, device='cuda:0')
c= tensor(2.0624e+09, device='cuda:0')
c= tensor(2.0626e+09, device='cuda:0')
c= tensor(2.0764e+09, device='cuda:0')
c= tensor(2.0772e+09, device='cuda:0')
c= tensor(2.0775e+09, device='cuda:0')
c= tensor(2.0793e+09, device='cuda:0')
c= tensor(2.0794e+09, device='cuda:0')
c= tensor(3.2741e+09, device='cuda:0')
c= tensor(3.2742e+09, device='cuda:0')
c= tensor(3.2810e+09, device='cuda:0')
c= tensor(3.2810e+09, device='cuda:0')
c= tensor(3.2810e+09, device='cuda:0')
c= tensor(3.2810e+09, device='cuda:0')
c= tensor(3.2811e+09, device='cuda:0')
c= tensor(3.2811e+09, device='cuda:0')
c= tensor(3.2887e+09, device='cuda:0')
c= tensor(3.2887e+09, device='cuda:0')
c= tensor(3.2887e+09, device='cuda:0')
c= tensor(3.3128e+09, device='cuda:0')
c= tensor(3.3146e+09, device='cuda:0')
c= tensor(3.3155e+09, device='cuda:0')
c= tensor(3.3215e+09, device='cuda:0')
c= tensor(3.3547e+09, device='cuda:0')
c= tensor(3.3547e+09, device='cuda:0')
c= tensor(3.3548e+09, device='cuda:0')
c= tensor(3.3553e+09, device='cuda:0')
c= tensor(3.3554e+09, device='cuda:0')
c= tensor(3.3554e+09, device='cuda:0')
c= tensor(3.3555e+09, device='cuda:0')
c= tensor(3.3555e+09, device='cuda:0')
c= tensor(3.3556e+09, device='cuda:0')
c= tensor(3.3556e+09, device='cuda:0')
c= tensor(3.3557e+09, device='cuda:0')
c= tensor(3.4139e+09, device='cuda:0')
c= tensor(3.4141e+09, device='cuda:0')
c= tensor(3.4183e+09, device='cuda:0')
c= tensor(3.4185e+09, device='cuda:0')
c= tensor(3.4185e+09, device='cuda:0')
c= tensor(3.4209e+09, device='cuda:0')
c= tensor(3.4785e+09, device='cuda:0')
c= tensor(3.5528e+09, device='cuda:0')
c= tensor(3.5534e+09, device='cuda:0')
c= tensor(3.5546e+09, device='cuda:0')
c= tensor(3.5546e+09, device='cuda:0')
c= tensor(3.5552e+09, device='cuda:0')
c= tensor(4.1385e+09, device='cuda:0')
c= tensor(4.1390e+09, device='cuda:0')
c= tensor(4.1390e+09, device='cuda:0')
c= tensor(4.1410e+09, device='cuda:0')
c= tensor(4.3089e+09, device='cuda:0')
c= tensor(4.3103e+09, device='cuda:0')
c= tensor(4.3105e+09, device='cuda:0')
c= tensor(4.3107e+09, device='cuda:0')
c= tensor(4.3107e+09, device='cuda:0')
c= tensor(4.3107e+09, device='cuda:0')
c= tensor(4.3239e+09, device='cuda:0')
c= tensor(4.3241e+09, device='cuda:0')
c= tensor(4.3242e+09, device='cuda:0')
c= tensor(4.3264e+09, device='cuda:0')
c= tensor(4.3266e+09, device='cuda:0')
c= tensor(4.3266e+09, device='cuda:0')
c= tensor(4.3291e+09, device='cuda:0')
c= tensor(4.3340e+09, device='cuda:0')
c= tensor(4.3621e+09, device='cuda:0')
c= tensor(4.3850e+09, device='cuda:0')
c= tensor(4.4092e+09, device='cuda:0')
c= tensor(4.4096e+09, device='cuda:0')
c= tensor(4.4112e+09, device='cuda:0')
c= tensor(4.4173e+09, device='cuda:0')
c= tensor(4.4348e+09, device='cuda:0')
c= tensor(4.4349e+09, device='cuda:0')
c= tensor(4.5083e+09, device='cuda:0')
c= tensor(4.8167e+09, device='cuda:0')
c= tensor(4.8433e+09, device='cuda:0')
c= tensor(4.8502e+09, device='cuda:0')
c= tensor(4.8540e+09, device='cuda:0')
c= tensor(4.8542e+09, device='cuda:0')
c= tensor(4.8542e+09, device='cuda:0')
c= tensor(4.8547e+09, device='cuda:0')
c= tensor(4.8605e+09, device='cuda:0')
c= tensor(4.8667e+09, device='cuda:0')
c= tensor(4.9129e+09, device='cuda:0')
c= tensor(4.9221e+09, device='cuda:0')
c= tensor(4.9253e+09, device='cuda:0')
c= tensor(4.9258e+09, device='cuda:0')
c= tensor(4.9485e+09, device='cuda:0')
c= tensor(4.9485e+09, device='cuda:0')
c= tensor(4.9486e+09, device='cuda:0')
c= tensor(4.9585e+09, device='cuda:0')
c= tensor(4.9595e+09, device='cuda:0')
c= tensor(4.9595e+09, device='cuda:0')
c= tensor(4.9599e+09, device='cuda:0')
c= tensor(5.1282e+09, device='cuda:0')
c= tensor(5.1284e+09, device='cuda:0')
c= tensor(5.1355e+09, device='cuda:0')
c= tensor(5.1356e+09, device='cuda:0')
c= tensor(5.1357e+09, device='cuda:0')
c= tensor(5.1357e+09, device='cuda:0')
c= tensor(5.1359e+09, device='cuda:0')
c= tensor(5.1362e+09, device='cuda:0')
c= tensor(5.1384e+09, device='cuda:0')
c= tensor(5.1386e+09, device='cuda:0')
c= tensor(5.1485e+09, device='cuda:0')
c= tensor(5.1486e+09, device='cuda:0')
c= tensor(5.1523e+09, device='cuda:0')
c= tensor(5.1525e+09, device='cuda:0')
c= tensor(5.1566e+09, device='cuda:0')
c= tensor(5.1567e+09, device='cuda:0')
c= tensor(5.1570e+09, device='cuda:0')
c= tensor(5.1574e+09, device='cuda:0')
c= tensor(5.1579e+09, device='cuda:0')
c= tensor(5.1614e+09, device='cuda:0')
c= tensor(5.3148e+09, device='cuda:0')
c= tensor(5.3149e+09, device='cuda:0')
c= tensor(5.3150e+09, device='cuda:0')
c= tensor(5.3178e+09, device='cuda:0')
c= tensor(5.3180e+09, device='cuda:0')
c= tensor(5.3967e+09, device='cuda:0')
c= tensor(5.3967e+09, device='cuda:0')
c= tensor(5.4022e+09, device='cuda:0')
c= tensor(5.4210e+09, device='cuda:0')
c= tensor(5.4210e+09, device='cuda:0')
c= tensor(5.4551e+09, device='cuda:0')
c= tensor(5.4564e+09, device='cuda:0')
c= tensor(5.5724e+09, device='cuda:0')
c= tensor(5.5725e+09, device='cuda:0')
c= tensor(5.5729e+09, device='cuda:0')
c= tensor(5.5731e+09, device='cuda:0')
c= tensor(5.5731e+09, device='cuda:0')
c= tensor(5.5731e+09, device='cuda:0')
c= tensor(5.5776e+09, device='cuda:0')
c= tensor(5.5778e+09, device='cuda:0')
c= tensor(5.5867e+09, device='cuda:0')
c= tensor(5.5868e+09, device='cuda:0')
c= tensor(5.5868e+09, device='cuda:0')
c= tensor(5.5871e+09, device='cuda:0')
c= tensor(5.6076e+09, device='cuda:0')
c= tensor(5.6134e+09, device='cuda:0')
c= tensor(5.6406e+09, device='cuda:0')
c= tensor(5.6413e+09, device='cuda:0')
c= tensor(5.6414e+09, device='cuda:0')
c= tensor(5.6414e+09, device='cuda:0')
c= tensor(5.6415e+09, device='cuda:0')
c= tensor(5.6872e+09, device='cuda:0')
c= tensor(5.6872e+09, device='cuda:0')
c= tensor(5.6875e+09, device='cuda:0')
c= tensor(5.6918e+09, device='cuda:0')
c= tensor(5.6932e+09, device='cuda:0')
c= tensor(5.6932e+09, device='cuda:0')
c= tensor(5.6933e+09, device='cuda:0')
c= tensor(5.7298e+09, device='cuda:0')
c= tensor(5.7309e+09, device='cuda:0')
c= tensor(5.7312e+09, device='cuda:0')
c= tensor(5.7316e+09, device='cuda:0')
c= tensor(5.7484e+09, device='cuda:0')
c= tensor(5.7532e+09, device='cuda:0')
c= tensor(5.8325e+09, device='cuda:0')
c= tensor(5.8386e+09, device='cuda:0')
c= tensor(5.8387e+09, device='cuda:0')
c= tensor(5.8397e+09, device='cuda:0')
c= tensor(5.8416e+09, device='cuda:0')
c= tensor(5.8417e+09, device='cuda:0')
c= tensor(5.8418e+09, device='cuda:0')
c= tensor(5.8418e+09, device='cuda:0')
c= tensor(5.8435e+09, device='cuda:0')
c= tensor(5.8439e+09, device='cuda:0')
c= tensor(5.8439e+09, device='cuda:0')
c= tensor(5.8440e+09, device='cuda:0')
c= tensor(5.8441e+09, device='cuda:0')
c= tensor(5.8455e+09, device='cuda:0')
c= tensor(5.8456e+09, device='cuda:0')
c= tensor(5.8461e+09, device='cuda:0')
c= tensor(5.8479e+09, device='cuda:0')
c= tensor(5.8483e+09, device='cuda:0')
c= tensor(5.8483e+09, device='cuda:0')
c= tensor(5.8484e+09, device='cuda:0')
c= tensor(5.8486e+09, device='cuda:0')
c= tensor(5.8638e+09, device='cuda:0')
c= tensor(5.8639e+09, device='cuda:0')
c= tensor(5.8639e+09, device='cuda:0')
c= tensor(5.8639e+09, device='cuda:0')
c= tensor(5.8758e+09, device='cuda:0')
c= tensor(6.0517e+09, device='cuda:0')
c= tensor(6.0538e+09, device='cuda:0')
c= tensor(6.0539e+09, device='cuda:0')
c= tensor(6.0702e+09, device='cuda:0')
c= tensor(6.0758e+09, device='cuda:0')
c= tensor(6.0759e+09, device='cuda:0')
c= tensor(6.0760e+09, device='cuda:0')
c= tensor(6.0763e+09, device='cuda:0')
c= tensor(6.0963e+09, device='cuda:0')
c= tensor(6.2056e+09, device='cuda:0')
c= tensor(6.2187e+09, device='cuda:0')
c= tensor(6.2190e+09, device='cuda:0')
c= tensor(6.2190e+09, device='cuda:0')
c= tensor(6.2191e+09, device='cuda:0')
c= tensor(6.2215e+09, device='cuda:0')
c= tensor(6.2216e+09, device='cuda:0')
c= tensor(6.2231e+09, device='cuda:0')
c= tensor(6.2338e+09, device='cuda:0')
c= tensor(6.3108e+09, device='cuda:0')
c= tensor(6.3108e+09, device='cuda:0')
c= tensor(6.3108e+09, device='cuda:0')
c= tensor(6.3115e+09, device='cuda:0')
c= tensor(6.3329e+09, device='cuda:0')
c= tensor(6.3344e+09, device='cuda:0')
c= tensor(6.3347e+09, device='cuda:0')
c= tensor(6.3347e+09, device='cuda:0')
c= tensor(6.3352e+09, device='cuda:0')
c= tensor(6.3352e+09, device='cuda:0')
c= tensor(6.3381e+09, device='cuda:0')
c= tensor(6.3382e+09, device='cuda:0')
c= tensor(6.3383e+09, device='cuda:0')
c= tensor(6.3383e+09, device='cuda:0')
c= tensor(6.3384e+09, device='cuda:0')
c= tensor(6.3384e+09, device='cuda:0')
c= tensor(6.3484e+09, device='cuda:0')
c= tensor(6.3962e+09, device='cuda:0')
c= tensor(6.4054e+09, device='cuda:0')
c= tensor(6.4150e+09, device='cuda:0')
c= tensor(6.4152e+09, device='cuda:0')
c= tensor(6.4153e+09, device='cuda:0')
c= tensor(6.4155e+09, device='cuda:0')
c= tensor(6.4224e+09, device='cuda:0')
c= tensor(6.4231e+09, device='cuda:0')
c= tensor(6.4261e+09, device='cuda:0')
c= tensor(6.4262e+09, device='cuda:0')
c= tensor(6.8823e+09, device='cuda:0')
c= tensor(6.8824e+09, device='cuda:0')
c= tensor(6.8839e+09, device='cuda:0')
c= tensor(6.9092e+09, device='cuda:0')
c= tensor(6.9104e+09, device='cuda:0')
c= tensor(6.9109e+09, device='cuda:0')
c= tensor(7.1612e+09, device='cuda:0')
c= tensor(7.1704e+09, device='cuda:0')
c= tensor(7.1729e+09, device='cuda:0')
c= tensor(7.1730e+09, device='cuda:0')
c= tensor(7.1740e+09, device='cuda:0')
c= tensor(7.1741e+09, device='cuda:0')
c= tensor(7.2005e+09, device='cuda:0')
c= tensor(7.5488e+09, device='cuda:0')
c= tensor(7.5626e+09, device='cuda:0')
c= tensor(7.5768e+09, device='cuda:0')
c= tensor(7.5836e+09, device='cuda:0')
c= tensor(7.5844e+09, device='cuda:0')
c= tensor(7.5852e+09, device='cuda:0')
c= tensor(7.5854e+09, device='cuda:0')
c= tensor(7.6761e+09, device='cuda:0')
c= tensor(7.7206e+09, device='cuda:0')
c= tensor(7.7240e+09, device='cuda:0')
c= tensor(8.5926e+09, device='cuda:0')
c= tensor(8.6111e+09, device='cuda:0')
c= tensor(8.6121e+09, device='cuda:0')
c= tensor(8.6121e+09, device='cuda:0')
c= tensor(8.6152e+09, device='cuda:0')
c= tensor(8.6200e+09, device='cuda:0')
c= tensor(8.6201e+09, device='cuda:0')
c= tensor(8.7214e+09, device='cuda:0')
c= tensor(8.7219e+09, device='cuda:0')
c= tensor(8.7225e+09, device='cuda:0')
c= tensor(8.7226e+09, device='cuda:0')
c= tensor(8.7226e+09, device='cuda:0')
c= tensor(8.7227e+09, device='cuda:0')
c= tensor(8.7227e+09, device='cuda:0')
c= tensor(8.7231e+09, device='cuda:0')
c= tensor(8.7241e+09, device='cuda:0')
c= tensor(1.2468e+10, device='cuda:0')
c= tensor(1.2470e+10, device='cuda:0')
c= tensor(1.2478e+10, device='cuda:0')
c= tensor(1.2478e+10, device='cuda:0')
c= tensor(1.2478e+10, device='cuda:0')
c= tensor(1.2478e+10, device='cuda:0')
c= tensor(1.2507e+10, device='cuda:0')
c= tensor(1.2511e+10, device='cuda:0')
c= tensor(1.2778e+10, device='cuda:0')
c= tensor(1.2778e+10, device='cuda:0')
c= tensor(1.2789e+10, device='cuda:0')
c= tensor(1.2789e+10, device='cuda:0')
c= tensor(1.2799e+10, device='cuda:0')
c= tensor(1.2830e+10, device='cuda:0')
c= tensor(1.2830e+10, device='cuda:0')
c= tensor(1.2830e+10, device='cuda:0')
c= tensor(1.2832e+10, device='cuda:0')
c= tensor(1.2833e+10, device='cuda:0')
c= tensor(1.2835e+10, device='cuda:0')
c= tensor(1.2865e+10, device='cuda:0')
c= tensor(1.2866e+10, device='cuda:0')
c= tensor(1.2870e+10, device='cuda:0')
c= tensor(1.2870e+10, device='cuda:0')
c= tensor(1.2877e+10, device='cuda:0')
c= tensor(1.2906e+10, device='cuda:0')
c= tensor(1.2906e+10, device='cuda:0')
c= tensor(1.2907e+10, device='cuda:0')
c= tensor(1.2925e+10, device='cuda:0')
c= tensor(1.2929e+10, device='cuda:0')
c= tensor(1.3155e+10, device='cuda:0')
c= tensor(1.3157e+10, device='cuda:0')
c= tensor(1.3158e+10, device='cuda:0')
c= tensor(1.3159e+10, device='cuda:0')
c= tensor(1.3183e+10, device='cuda:0')
c= tensor(1.3200e+10, device='cuda:0')
c= tensor(1.3201e+10, device='cuda:0')
c= tensor(1.3201e+10, device='cuda:0')
c= tensor(1.3201e+10, device='cuda:0')
c= tensor(1.3205e+10, device='cuda:0')
c= tensor(1.3212e+10, device='cuda:0')
c= tensor(1.3214e+10, device='cuda:0')
c= tensor(1.3214e+10, device='cuda:0')
c= tensor(1.3214e+10, device='cuda:0')
c= tensor(1.3217e+10, device='cuda:0')
c= tensor(1.3218e+10, device='cuda:0')
c= tensor(1.3218e+10, device='cuda:0')
c= tensor(1.3220e+10, device='cuda:0')
c= tensor(1.3221e+10, device='cuda:0')
c= tensor(1.3222e+10, device='cuda:0')
c= tensor(1.3222e+10, device='cuda:0')
c= tensor(1.3222e+10, device='cuda:0')
c= tensor(1.3230e+10, device='cuda:0')
c= tensor(1.3230e+10, device='cuda:0')
c= tensor(1.3233e+10, device='cuda:0')
c= tensor(1.3233e+10, device='cuda:0')
c= tensor(1.3233e+10, device='cuda:0')
c= tensor(1.3235e+10, device='cuda:0')
c= tensor(1.3237e+10, device='cuda:0')
c= tensor(1.3237e+10, device='cuda:0')
c= tensor(1.3237e+10, device='cuda:0')
c= tensor(1.3237e+10, device='cuda:0')
c= tensor(1.3238e+10, device='cuda:0')
c= tensor(1.3239e+10, device='cuda:0')
c= tensor(1.3249e+10, device='cuda:0')
c= tensor(1.3249e+10, device='cuda:0')
c= tensor(1.3250e+10, device='cuda:0')
c= tensor(1.3251e+10, device='cuda:0')
c= tensor(1.3251e+10, device='cuda:0')
c= tensor(1.3272e+10, device='cuda:0')
c= tensor(1.3272e+10, device='cuda:0')
c= tensor(1.3283e+10, device='cuda:0')
c= tensor(1.3290e+10, device='cuda:0')
c= tensor(1.3291e+10, device='cuda:0')
c= tensor(1.3299e+10, device='cuda:0')
c= tensor(1.3305e+10, device='cuda:0')
c= tensor(1.3305e+10, device='cuda:0')
c= tensor(1.3306e+10, device='cuda:0')
c= tensor(1.3307e+10, device='cuda:0')
c= tensor(1.3316e+10, device='cuda:0')
c= tensor(1.3324e+10, device='cuda:0')
c= tensor(1.3324e+10, device='cuda:0')
c= tensor(1.3327e+10, device='cuda:0')
c= tensor(1.3327e+10, device='cuda:0')
c= tensor(1.3332e+10, device='cuda:0')
c= tensor(1.3332e+10, device='cuda:0')
c= tensor(1.3333e+10, device='cuda:0')
c= tensor(1.3368e+10, device='cuda:0')
c= tensor(1.3485e+10, device='cuda:0')
c= tensor(1.3486e+10, device='cuda:0')
c= tensor(1.3486e+10, device='cuda:0')
c= tensor(1.3486e+10, device='cuda:0')
c= tensor(1.3502e+10, device='cuda:0')
c= tensor(1.3503e+10, device='cuda:0')
c= tensor(1.3503e+10, device='cuda:0')
c= tensor(1.3504e+10, device='cuda:0')
c= tensor(1.3514e+10, device='cuda:0')
c= tensor(1.3514e+10, device='cuda:0')
c= tensor(1.3515e+10, device='cuda:0')
c= tensor(1.3516e+10, device='cuda:0')
time to make c is 7.206730365753174
time for making loss is 7.2068772315979
p0 True
it  0 : 3921473536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6497697792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 43% |
memory (bytes)
6497873920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 43% |
error is  5590295600.0
relative error loss 0.41361097
shape of L is 
torch.Size([])
memory (bytes)
6523162624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 43% |
memory (bytes)
6523162624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5556045000.0
relative error loss 0.41107684
shape of L is 
torch.Size([])
memory (bytes)
6526705664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6526939136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5539262500.0
relative error loss 0.40983516
shape of L is 
torch.Size([])
memory (bytes)
6530203648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6530203648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5512532500.0
relative error loss 0.40785748
shape of L is 
torch.Size([])
memory (bytes)
6533255168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6533386240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5486482000.0
relative error loss 0.40593007
shape of L is 
torch.Size([])
memory (bytes)
6536597504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 43% |
memory (bytes)
6536597504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5467912700.0
relative error loss 0.40455619
shape of L is 
torch.Size([])
memory (bytes)
6539804672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 43% |
memory (bytes)
6539804672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 43% |
error is  5439371300.0
relative error loss 0.40244448
shape of L is 
torch.Size([])
memory (bytes)
6543007744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 43% |
memory (bytes)
6543007744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5420347400.0
relative error loss 0.40103695
shape of L is 
torch.Size([])
memory (bytes)
6546132992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 43% |
memory (bytes)
6546214912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5401311700.0
relative error loss 0.39962855
shape of L is 
torch.Size([])
memory (bytes)
6549413888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6549413888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5384950000.0
relative error loss 0.39841798
time to take a step is 226.63697481155396
it  1 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6552625152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6552625152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5384950000.0
relative error loss 0.39841798
shape of L is 
torch.Size([])
memory (bytes)
6555832320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6555832320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5363374600.0
relative error loss 0.39682168
shape of L is 
torch.Size([])
memory (bytes)
6558961664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6559031296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 43% |
error is  5353682000.0
relative error loss 0.39610454
shape of L is 
torch.Size([])
memory (bytes)
6562238464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6562238464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 43% |
error is  5337634300.0
relative error loss 0.39491722
shape of L is 
torch.Size([])
memory (bytes)
6565416960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6565449728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5331058700.0
relative error loss 0.39443073
shape of L is 
torch.Size([])
memory (bytes)
6568652800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6568652800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5319134700.0
relative error loss 0.3935485
shape of L is 
torch.Size([])
memory (bytes)
6571847680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 43% |
memory (bytes)
6571847680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5312801000.0
relative error loss 0.39307988
shape of L is 
torch.Size([])
memory (bytes)
6575054848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6575054848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5304809000.0
relative error loss 0.39248857
shape of L is 
torch.Size([])
memory (bytes)
6578278400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 43% |
memory (bytes)
6578278400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5295597600.0
relative error loss 0.39180705
shape of L is 
torch.Size([])
memory (bytes)
6581473280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6581473280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5305164300.0
relative error loss 0.39251485
shape of L is 
torch.Size([])
memory (bytes)
6584541184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6584672256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5290901500.0
relative error loss 0.3914596
time to take a step is 244.1685938835144
it  2 : 4642855424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6587875328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 43% |
memory (bytes)
6587875328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5290901500.0
relative error loss 0.3914596
shape of L is 
torch.Size([])
memory (bytes)
6590914560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6591078400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5285342700.0
relative error loss 0.3910483
shape of L is 
torch.Size([])
memory (bytes)
6594281472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6594281472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5279708000.0
relative error loss 0.39063144
shape of L is 
torch.Size([])
memory (bytes)
6597328896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6597488640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5273513000.0
relative error loss 0.39017308
shape of L is 
torch.Size([])
memory (bytes)
6600695808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6600695808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5267769000.0
relative error loss 0.38974807
shape of L is 
torch.Size([])
memory (bytes)
6603718656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6603898880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5264324600.0
relative error loss 0.38949326
shape of L is 
torch.Size([])
memory (bytes)
6607106048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6607106048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5260070400.0
relative error loss 0.38917848
shape of L is 
torch.Size([])
memory (bytes)
6610296832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6610296832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5260786700.0
relative error loss 0.38923147
shape of L is 
torch.Size([])
memory (bytes)
6613512192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6613512192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5257001000.0
relative error loss 0.3889514
shape of L is 
torch.Size([])
memory (bytes)
6616641536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 43% |
memory (bytes)
6616641536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5253301000.0
relative error loss 0.38867766
time to take a step is 221.7272207736969
it  3 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6619914240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6619914240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5253301000.0
relative error loss 0.38867766
shape of L is 
torch.Size([])
memory (bytes)
6622998528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
memory (bytes)
6623109120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5250721000.0
relative error loss 0.38848674
shape of L is 
torch.Size([])
memory (bytes)
6626324480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 43% |
memory (bytes)
6626324480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5248232000.0
relative error loss 0.3883026
shape of L is 
torch.Size([])
memory (bytes)
6629429248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
memory (bytes)
6629429248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5245934600.0
relative error loss 0.38813263
shape of L is 
torch.Size([])
memory (bytes)
6632734720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 43% |
memory (bytes)
6632734720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5243758600.0
relative error loss 0.3879716
shape of L is 
torch.Size([])
memory (bytes)
6635905024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6635905024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5240768500.0
relative error loss 0.3877504
shape of L is 
torch.Size([])
memory (bytes)
6639144960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 43% |
memory (bytes)
6639144960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 43% |
error is  5239357400.0
relative error loss 0.387646
shape of L is 
torch.Size([])
memory (bytes)
6642302976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6642302976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5237138000.0
relative error loss 0.38748178
shape of L is 
torch.Size([])
memory (bytes)
6645555200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 43% |
memory (bytes)
6645555200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5235619000.0
relative error loss 0.3873694
shape of L is 
torch.Size([])
memory (bytes)
6648602624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6648762368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 43% |
error is  5233946600.0
relative error loss 0.38724566
time to take a step is 223.62889099121094
it  4 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6651965440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 43% |
memory (bytes)
6651965440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5233946600.0
relative error loss 0.38724566
shape of L is 
torch.Size([])
memory (bytes)
6655045632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6655184896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5232431600.0
relative error loss 0.38713357
shape of L is 
torch.Size([])
memory (bytes)
6658396160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6658396160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5229643300.0
relative error loss 0.38692728
shape of L is 
torch.Size([])
memory (bytes)
6661607424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 43% |
memory (bytes)
6661607424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5227680000.0
relative error loss 0.386782
shape of L is 
torch.Size([])
memory (bytes)
6664814592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 43% |
memory (bytes)
6664814592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5225957400.0
relative error loss 0.38665456
shape of L is 
torch.Size([])
memory (bytes)
6667919360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6668013568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5222334500.0
relative error loss 0.3863865
shape of L is 
torch.Size([])
memory (bytes)
6671224832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 43% |
memory (bytes)
6671224832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5221537300.0
relative error loss 0.38632753
shape of L is 
torch.Size([])
memory (bytes)
6674345984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 43% |
memory (bytes)
6674345984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5217379300.0
relative error loss 0.3860199
shape of L is 
torch.Size([])
memory (bytes)
6677635072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 43% |
memory (bytes)
6677635072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5216386600.0
relative error loss 0.38594645
shape of L is 
torch.Size([])
memory (bytes)
6680764416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 43% |
memory (bytes)
6680858624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5213392400.0
relative error loss 0.3857249
time to take a step is 223.24436783790588
it  5 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6683996160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
memory (bytes)
6683996160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5213392400.0
relative error loss 0.3857249
shape of L is 
torch.Size([])
memory (bytes)
6687256576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 43% |
memory (bytes)
6687256576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5212009500.0
relative error loss 0.3856226
shape of L is 
torch.Size([])
memory (bytes)
6690439168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 43% |
memory (bytes)
6690439168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5209789400.0
relative error loss 0.38545835
shape of L is 
torch.Size([])
memory (bytes)
6693666816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
memory (bytes)
6693666816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5208368000.0
relative error loss 0.38535318
shape of L is 
torch.Size([])
memory (bytes)
6696804352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6696804352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5207453700.0
relative error loss 0.38528553
shape of L is 
torch.Size([])
memory (bytes)
6700085248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6700085248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5205656600.0
relative error loss 0.38515255
shape of L is 
torch.Size([])
memory (bytes)
6703255552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 43% |
memory (bytes)
6703255552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5204384300.0
relative error loss 0.38505843
shape of L is 
torch.Size([])
memory (bytes)
6706487296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6706487296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5203232300.0
relative error loss 0.3849732
shape of L is 
torch.Size([])
memory (bytes)
6709694464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6709694464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5202437000.0
relative error loss 0.38491437
shape of L is 
torch.Size([])
memory (bytes)
6712901632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6712901632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5201243000.0
relative error loss 0.384826
time to take a step is 229.1305902004242
it  6 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6716108800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 43% |
memory (bytes)
6716108800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5201243000.0
relative error loss 0.384826
shape of L is 
torch.Size([])
memory (bytes)
6719303680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6719303680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5200333000.0
relative error loss 0.38475865
shape of L is 
torch.Size([])
memory (bytes)
6722514944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 43% |
memory (bytes)
6722514944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 43% |
error is  5198792000.0
relative error loss 0.3846447
shape of L is 
torch.Size([])
memory (bytes)
6725627904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 43% |
memory (bytes)
6725627904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5197817300.0
relative error loss 0.38457257
shape of L is 
torch.Size([])
memory (bytes)
6728921088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 43% |
memory (bytes)
6728921088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5196715000.0
relative error loss 0.384491
shape of L is 
torch.Size([])
memory (bytes)
6732083200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6732083200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 43% |
error is  5195392500.0
relative error loss 0.38439316
shape of L is 
torch.Size([])
memory (bytes)
6735331328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6735331328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5193966600.0
relative error loss 0.38428766
shape of L is 
torch.Size([])
memory (bytes)
6738530304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 43% |
memory (bytes)
6738534400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5193256400.0
relative error loss 0.3842351
shape of L is 
torch.Size([])
memory (bytes)
6741741568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6741741568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5192543000.0
relative error loss 0.38418233
shape of L is 
torch.Size([])
memory (bytes)
6744915968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6744915968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5191810000.0
relative error loss 0.3841281
time to take a step is 238.89157819747925
it  7 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6748082176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6748151808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5191810000.0
relative error loss 0.3841281
shape of L is 
torch.Size([])
memory (bytes)
6751350784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 43% |
memory (bytes)
6751350784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5190901000.0
relative error loss 0.38406086
shape of L is 
torch.Size([])
memory (bytes)
6754553856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 43% |
memory (bytes)
6754553856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5190142500.0
relative error loss 0.3840047
shape of L is 
torch.Size([])
memory (bytes)
6757761024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 43% |
memory (bytes)
6757769216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5189541400.0
relative error loss 0.38396025
shape of L is 
torch.Size([])
memory (bytes)
6760828928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6760955904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5188473000.0
relative error loss 0.38388118
shape of L is 
torch.Size([])
memory (bytes)
6764171264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6764171264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5187191300.0
relative error loss 0.38378635
shape of L is 
torch.Size([])
memory (bytes)
6767370240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6767370240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5186627600.0
relative error loss 0.38374466
shape of L is 
torch.Size([])
memory (bytes)
6770581504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 43% |
memory (bytes)
6770581504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5185818000.0
relative error loss 0.38368475
shape of L is 
torch.Size([])
memory (bytes)
6773678080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 43% |
memory (bytes)
6773796864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5184963000.0
relative error loss 0.3836215
shape of L is 
torch.Size([])
memory (bytes)
6777008128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 43% |
memory (bytes)
6777008128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5184011000.0
relative error loss 0.38355103
time to take a step is 238.41698670387268
it  8 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6780141568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6780141568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5184011000.0
relative error loss 0.38355103
shape of L is 
torch.Size([])
memory (bytes)
6783410176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 43% |
memory (bytes)
6783410176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5183352000.0
relative error loss 0.38350227
shape of L is 
torch.Size([])
memory (bytes)
6786404352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 43% |
memory (bytes)
6786621440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5182759000.0
relative error loss 0.3834584
shape of L is 
torch.Size([])
memory (bytes)
6789824512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 43% |
memory (bytes)
6789824512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5181807000.0
relative error loss 0.38338798
shape of L is 
torch.Size([])
memory (bytes)
6793011200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6793011200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5181061600.0
relative error loss 0.38333285
shape of L is 
torch.Size([])
memory (bytes)
6796238848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6796238848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5180500000.0
relative error loss 0.38329127
shape of L is 
torch.Size([])
memory (bytes)
6799429632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6799429632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5179760600.0
relative error loss 0.3832366
shape of L is 
torch.Size([])
memory (bytes)
6802649088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6802649088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5179321000.0
relative error loss 0.38320404
shape of L is 
torch.Size([])
memory (bytes)
6805831680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6805831680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 43% |
error is  5178881500.0
relative error loss 0.38317153
shape of L is 
torch.Size([])
memory (bytes)
6809063424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 43% |
memory (bytes)
6809063424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5178636000.0
relative error loss 0.38315335
time to take a step is 224.09069991111755
it  9 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6812278784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 43% |
memory (bytes)
6812282880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5178636000.0
relative error loss 0.38315335
shape of L is 
torch.Size([])
memory (bytes)
6815481856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 43% |
memory (bytes)
6815481856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5178367500.0
relative error loss 0.3831335
shape of L is 
torch.Size([])
memory (bytes)
6818693120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6818693120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5177862700.0
relative error loss 0.38309616
shape of L is 
torch.Size([])
memory (bytes)
6821801984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 43% |
memory (bytes)
6821900288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5177542000.0
relative error loss 0.38307244
shape of L is 
torch.Size([])
memory (bytes)
6825103360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6825103360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5177094700.0
relative error loss 0.38303933
shape of L is 
torch.Size([])
memory (bytes)
6828175360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6828175360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5176884700.0
relative error loss 0.3830238
shape of L is 
torch.Size([])
memory (bytes)
6831509504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6831509504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5176368000.0
relative error loss 0.3829856
shape of L is 
torch.Size([])
memory (bytes)
6834589696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6834724864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 43% |
error is  5175906000.0
relative error loss 0.38295138
shape of L is 
torch.Size([])
memory (bytes)
6837927936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6837927936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5175361000.0
relative error loss 0.38291106
shape of L is 
torch.Size([])
memory (bytes)
6840963072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 43% |
memory (bytes)
6841126912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5174929400.0
relative error loss 0.38287914
time to take a step is 223.14046239852905
it  10 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6844342272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6844342272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5174929400.0
relative error loss 0.38287914
shape of L is 
torch.Size([])
memory (bytes)
6847512576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6847512576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5174556000.0
relative error loss 0.3828515
shape of L is 
torch.Size([])
memory (bytes)
6850756608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6850756608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5174007000.0
relative error loss 0.38281086
shape of L is 
torch.Size([])
memory (bytes)
6853820416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6853967872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 43% |
error is  5173491700.0
relative error loss 0.38277277
shape of L is 
torch.Size([])
memory (bytes)
6857175040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
memory (bytes)
6857175040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5173006000.0
relative error loss 0.3827368
shape of L is 
torch.Size([])
memory (bytes)
6860382208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 43% |
memory (bytes)
6860382208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 43% |
error is  5172569600.0
relative error loss 0.38270453
shape of L is 
torch.Size([])
memory (bytes)
6863585280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 43% |
memory (bytes)
6863585280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5172168700.0
relative error loss 0.38267487
shape of L is 
torch.Size([])
memory (bytes)
6866796544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6866796544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5171934700.0
relative error loss 0.38265756
shape of L is 
torch.Size([])
memory (bytes)
6869995520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 43% |
memory (bytes)
6869995520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5171784700.0
relative error loss 0.38264647
shape of L is 
torch.Size([])
memory (bytes)
6873206784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 43% |
memory (bytes)
6873206784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 43% |
error is  5171444000.0
relative error loss 0.3826213
time to take a step is 250.29488897323608
it  11 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6876401664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 43% |
memory (bytes)
6876401664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5171444000.0
relative error loss 0.3826213
shape of L is 
torch.Size([])
memory (bytes)
6879600640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6879600640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5171142700.0
relative error loss 0.38259897
shape of L is 
torch.Size([])
memory (bytes)
6882816000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6882816000
| ID | GPU  | MEM |
-------------------
|  0 |   1% |  0% |
|  1 | 100% | 43% |
error is  5170898000.0
relative error loss 0.38258085
shape of L is 
torch.Size([])
memory (bytes)
6885998592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 43% |
memory (bytes)
6885998592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5170695700.0
relative error loss 0.3825659
shape of L is 
torch.Size([])
memory (bytes)
6889213952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6889213952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5170488000.0
relative error loss 0.3825505
shape of L is 
torch.Size([])
memory (bytes)
6892425216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 43% |
memory (bytes)
6892425216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5170222000.0
relative error loss 0.38253084
shape of L is 
torch.Size([])
memory (bytes)
6895628288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6895628288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5169782300.0
relative error loss 0.38249832
shape of L is 
torch.Size([])
memory (bytes)
6898835456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 43% |
memory (bytes)
6898835456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5169116000.0
relative error loss 0.38244903
shape of L is 
torch.Size([])
memory (bytes)
6902042624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
memory (bytes)
6902042624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5169465300.0
relative error loss 0.38247487
shape of L is 
torch.Size([])
memory (bytes)
6905245696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
memory (bytes)
6905245696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5168682000.0
relative error loss 0.3824169
time to take a step is 276.52538228034973
it  12 : 4642855424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6908342272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6908456960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5168682000.0
relative error loss 0.3824169
shape of L is 
torch.Size([])
memory (bytes)
6911647744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6911647744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5168284700.0
relative error loss 0.38238752
shape of L is 
torch.Size([])
memory (bytes)
6914859008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 43% |
memory (bytes)
6914859008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5167937000.0
relative error loss 0.3823618
shape of L is 
torch.Size([])
memory (bytes)
6917959680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 43% |
memory (bytes)
6917959680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5167580700.0
relative error loss 0.38233542
shape of L is 
torch.Size([])
memory (bytes)
6921277440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6921277440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5167273500.0
relative error loss 0.3823127
shape of L is 
torch.Size([])
memory (bytes)
6924410880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6924480512
| ID | GPU  | MEM |
-------------------
|  0 |   1% |  0% |
|  1 | 100% | 43% |
error is  5166967000.0
relative error loss 0.38229
shape of L is 
torch.Size([])
memory (bytes)
6927683584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 43% |
memory (bytes)
6927683584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5166652400.0
relative error loss 0.38226673
shape of L is 
torch.Size([])
memory (bytes)
6930886656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6930886656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5166344700.0
relative error loss 0.382244
shape of L is 
torch.Size([])
memory (bytes)
6934024192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 43% |
memory (bytes)
6934024192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5166006300.0
relative error loss 0.38221893
shape of L is 
torch.Size([])
memory (bytes)
6937276416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 43% |
memory (bytes)
6937276416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5165382700.0
relative error loss 0.3821728
time to take a step is 277.4344801902771
it  13 : 4642854912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6940483584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 43% |
memory (bytes)
6940483584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5165382700.0
relative error loss 0.3821728
shape of L is 
torch.Size([])
memory (bytes)
6943682560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 43% |
memory (bytes)
6943682560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5165144000.0
relative error loss 0.38215515
shape of L is 
torch.Size([])
memory (bytes)
6946902016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 43% |
memory (bytes)
6946902016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 43% |
error is  5164573700.0
relative error loss 0.38211295
shape of L is 
torch.Size([])
memory (bytes)
6950105088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6950105088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5164022300.0
relative error loss 0.38207215
shape of L is 
torch.Size([])
memory (bytes)
6953308160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6953308160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5163694000.0
relative error loss 0.38204786
shape of L is 
torch.Size([])
memory (bytes)
6956515328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6956515328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5163416600.0
relative error loss 0.38202733
shape of L is 
torch.Size([])
memory (bytes)
6959607808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 43% |
memory (bytes)
6959607808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5163168000.0
relative error loss 0.3820089
shape of L is 
torch.Size([])
memory (bytes)
6962925568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 43% |
memory (bytes)
6962925568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 43% |
error is  5162641000.0
relative error loss 0.38196993
shape of L is 
torch.Size([])
memory (bytes)
6966116352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6966132736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 43% |
error is  5162806300.0
relative error loss 0.38198218
shape of L is 
torch.Size([])
memory (bytes)
6969331712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6969331712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5162449400.0
relative error loss 0.38195577
time to take a step is 281.5933005809784
it  14 : 4642855424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
shape of L is 
torch.Size([])
memory (bytes)
6972506112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6972506112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5162449400.0
relative error loss 0.38195577
shape of L is 
torch.Size([])
memory (bytes)
6975746048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 43% |
memory (bytes)
6975746048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 43% |
error is  5162179600.0
relative error loss 0.3819358
shape of L is 
torch.Size([])
memory (bytes)
6978949120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6978949120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5161768400.0
relative error loss 0.38190538
shape of L is 
torch.Size([])
memory (bytes)
6982168576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 43% |
memory (bytes)
6982168576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 43% |
error is  5161352000.0
relative error loss 0.3818746
shape of L is 
torch.Size([])
memory (bytes)
6985375744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 43% |
memory (bytes)
6985375744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5160870000.0
relative error loss 0.38183892
shape of L is 
torch.Size([])
memory (bytes)
6988578816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 43% |
memory (bytes)
6988587008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5160488000.0
relative error loss 0.38181064
shape of L is 
torch.Size([])
memory (bytes)
6991794176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 43% |
memory (bytes)
6991794176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5160185000.0
relative error loss 0.38178822
shape of L is 
torch.Size([])
memory (bytes)
6994956288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 43% |
memory (bytes)
6994997248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5159579000.0
relative error loss 0.3817434
shape of L is 
torch.Size([])
memory (bytes)
6998200320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 43% |
memory (bytes)
6998200320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 43% |
error is  5159235600.0
relative error loss 0.38171798
shape of L is 
torch.Size([])
memory (bytes)
7001411584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 43% |
memory (bytes)
7001411584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 43% |
error is  5158948000.0
relative error loss 0.3816967
time to take a step is 277.10140585899353
sum tnnu_Z after tensor(11227297., device='cuda:0')
shape of features
(7106,)
shape of features
(7106,)
number of orig particles 28423
number of new particles after remove low mass 28404
tnuZ shape should be parts x labs
torch.Size([28423, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  5590110000.0
relative error without small mass is  0.41359726
nnu_Z shape should be number of particles by maxV
(28423, 702)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
shape of features
(28423,)
Wed Feb 1 00:21:57 EST 2023
