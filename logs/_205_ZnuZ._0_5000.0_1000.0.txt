Tue Jan 31 10:34:49 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 30656136
numbers of Z: 18063
shape of features
(18063,)
shape of features
(18063,)
ZX	Vol	Parts	Cubes	Eps
Z	0.014379365226820352	18063	18.063	0.09267941366676778
X	0.013545099429976996	786	0.786	0.25829731028757563
X	0.013723361472153204	16855	16.855	0.0933778596988293
X	0.014201097871814896	2465	2.465	0.1792675427751703
X	0.013367019482169847	2546	2.546	0.17380358378171257
X	0.013790314275978448	17934	17.934	0.09161480595718452
X	0.013617612666628856	24745	24.745	0.0819479020494093
X	0.014090020347882624	47381	47.381	0.06674762222288604
X	0.013756476398629749	32645	32.645	0.07497161129284723
X	0.013662997514387874	14933	14.933	0.09708071689791312
X	0.013985622788056097	35675	35.675	0.073187898354903
X	0.013738818212626566	18735	18.735	0.09017757656858016
X	0.013584859655289109	36504	36.504	0.07192914534883287
X	0.01372664612986024	3880	3.88	0.15237401249792307
X	0.013772053343993946	166139	166.139	0.043602263456266575
X	0.013589702929787062	16424	16.424	0.09388084910939064
X	0.013713080900400137	21825	21.825	0.08564991347838731
X	0.013791449421101885	39319	39.319	0.0705238179557852
X	0.013788016755052665	40998	40.998	0.06954187197345829
X	0.01373536103060535	91041	91.041	0.053235461084744376
X	0.014175716800908962	115300	115.3	0.04972466970725659
X	0.013667955475969953	10879	10.879	0.10790416115175945
X	0.014033939330642111	133473	133.473	0.047198557334997095
X	0.013593873508016545	8951	8.951	0.11494512470385668
X	0.013868792677323504	13364	13.364	0.10124355734460351
X	0.013115352238422527	4182	4.182	0.14637446143069432
X	0.013742166001271988	47693	47.693	0.06604909170518282
X	0.014032403389434445	47576	47.576	0.06656532321705334
X	0.013608365841812239	7612	7.612	0.12136749795195481
X	0.01395900605336937	180343	180.343	0.04261721529959421
X	0.013787605025267723	588001	588.001	0.02862223806636285
X	0.013667157691129021	6673	6.673	0.12699472836807288
X	0.01424482595786153	959331	959.331	0.02457892529678339
X	0.013574592685606306	6803	6.803	0.12589508597032342
X	0.013752069897965586	11600	11.6	0.10583679843247329
X	0.013625688815906811	6832	6.832	0.12587424235768327
X	0.01403525291890797	321883	321.883	0.035197142050975315
X	0.014093132791880666	59950	59.95	0.06171718638743393
X	0.01367643284980772	1246	1.246	0.22223797186030442
X	0.01360128913936229	3771	3.771	0.15335849817253586
X	0.01347694217719461	2555	2.555	0.17407383080714625
X	0.013783317076537528	2483	2.483	0.17706212651737457
X	0.012658922594386243	1317	1.317	0.2126197760820055
X	0.013230045512882104	904	0.904	0.2446044211884712
X	0.013193091485158182	2257	2.257	0.1801378624300584
X	0.013539865370081866	533	0.533	0.29396505871374656
X	0.013545262161368728	1263	1.263	0.22052678944676413
X	0.01357075255473214	4236	4.236	0.1474175967045332
X	0.013617357917923834	4459	4.459	0.14508364568908977
X	0.013564115111749823	1275	1.275	0.21993470277198562
X	0.013469648317276612	5205	5.205	0.13729198187137806
X	0.013874350169345458	11613	11.613	0.10610994149584708
X	0.013376431590095033	1438	1.438	0.21031026095325694
X	0.013622741895858903	6172	6.172	0.13020056543986822
X	0.013585103731907197	3587	3.587	0.15587526114439232
X	0.01371164012003339	5296	5.296	0.13731368111155864
X	0.013584696004563397	3275	3.275	0.16067422235879464
X	0.013322263151763927	1752	1.752	0.1966442727036106
X	0.01353858859416388	2136	2.136	0.18506489605730758
X	0.013677553465476553	2340	2.34	0.1801347583316486
X	0.013500500075499296	2563	2.563	0.17399379064974171
X	0.013856808385734904	5812	5.812	0.1335916155713465
X	0.013621707996098456	1287	1.287	0.2195588798586517
X	0.014144185260525218	5500	5.5	0.13700563046048425
X	0.013628144646399915	6755	6.755	0.12635830562507466
X	0.013458011060364618	2246	2.246	0.18163116824627445
X	0.013472038058872607	1774	1.774	0.19655913330133123
X	0.013554576121715137	2391	2.391	0.17830716223799656
X	0.013302853201473943	2020	2.02	0.18744094986081455
X	0.013650054943660374	3024	3.024	0.16526626196476546
X	0.013523875047479664	1899	1.899	0.19239423815043086
X	0.013206466839016648	1550	1.55	0.20424510129162154
X	0.01326156539193169	1687	1.687	0.19883518429940736
X	0.013496936298808175	1405	1.405	0.2125786401812963
X	0.013161794428995281	1663	1.663	0.19928485028713655
X	0.01356795490760739	2641	2.641	0.17255014561065307
X	0.013670655236407118	13077	13.077	0.10149089188725985
X	0.0136828040836734	1635	1.635	0.20302637036760104
X	0.013444054356752319	682	0.682	0.2701356870474148
X	0.013660042978312423	1979	1.979	0.19040079490256143
X	0.013913577776840621	7382	7.382	0.12352501815812295
X	0.013596803600330167	2010	2.01	0.18912412090482023
X	0.013562137321421282	803	0.803	0.25656900939175153
X	0.013953265563874726	2489	2.489	0.1776439089981755
X	0.013049807377429794	843	0.843	0.24922532760878838
X	0.013688544273332248	2016	2.016	0.18936029103054483
X	0.01340553019101353	1222	1.222	0.22219669578017293
X	0.013600907403159359	2537	2.537	0.17501790148549498
X	0.013607319822957278	1337	1.337	0.21671072001297398
X	0.01358072856623666	2229	2.229	0.18264354736174593
X	0.013454800202840877	3394	3.394	0.15826630812615017
X	0.013635914714349365	1629	1.629	0.20304286398429652
X	0.013316113708445132	1442	1.442	0.20979932095721832
X	0.01332056400531149	1306	1.306	0.21686683712057425
X	0.013639777515280128	8130	8.13	0.11882438638889424
X	0.013414170236302656	2161	2.161	0.18378201783956546
X	0.013562014686829271	4173	4.173	0.14812394673069076
X	0.01359241604633714	3281	3.281	0.1606066325420381
X	0.013594349591701489	2657	2.657	0.17231468608776299
X	0.013390399228287875	2379	2.379	0.17788241944419977
X	0.013667878726963288	3368	3.368	0.1595057504163704
X	0.013682144719685043	12155	12.155	0.10402388566416171
X	0.013585649492937666	3078	3.078	0.16403527966798273
X	0.013520771256165381	3771	3.771	0.15305527817718503
X	0.013530304108373189	2655	2.655	0.1720868470227607
X	0.013590196409820033	5246	5.246	0.13734066598735528
X	0.013578050249699951	2795	2.795	0.1693629981257469
X	0.01362050198479355	3524	3.524	0.15693473246428757
X	0.013650808144725555	1982	1.982	0.19026178676172556
X	0.013310981620419314	2014	2.014	0.18766511063718602
X	0.01338517660807509	830	0.83	0.2526476935100902
X	0.013488839761408	3088	3.088	0.16346788161823955
X	0.013504191250052672	1297	1.297	0.21836156652876726
X	0.013187681146284486	927	0.927	0.2423052931228147
X	0.013429139901542502	1438	1.438	0.21058613349798447
X	0.013326651913884094	3278	3.278	0.1596016468669968
X	0.013396624949215338	1256	1.256	0.2201246034260952
X	0.013701502472635501	1206	1.206	0.22480551530762827
X	0.013268404222191959	949	0.949	0.24090778087785
X	0.013966774409223687	6844	6.844	0.12684169566842166
X	0.01358708998147337	1084	1.084	0.23229100330774866
X	0.01365288340719384	6745	6.745	0.126497169878671
X	0.013580725621354598	2038	2.038	0.18817976930528485
X	0.01318011318563292	1875	1.875	0.19156049734384825
X	0.013645412193217315	3566	3.566	0.15641141325100164
X	0.013592418261973218	3062	3.062	0.16434778221868795
X	0.013493574639763311	1331	1.331	0.2164294321969896
X	0.013346052871655242	2246	2.246	0.18112609736783272
X	0.013553714349418853	1528	1.528	0.20700380323996154
X	0.013801796484918015	8130	8.13	0.11929301760913977
X	0.013341126854800565	2456	2.456	0.1757875245448772
X	0.013521397206914873	3634	3.634	0.15495736258650994
X	0.013261203568055668	1187	1.187	0.22355140701002965
X	0.013361753689277792	1628	1.628	0.20171414498849216
X	0.013275061035258237	1449	1.449	0.20924546151394863
X	0.01378174477661398	1806	1.806	0.19687722787572354
X	0.013574177731934294	1721	1.721	0.1990570454429127
X	0.01315167011390453	905	0.905	0.24403049969263754
X	0.013519685859201571	1780	1.78	0.19656922832207105
X	0.013675061598575027	1422	1.422	0.21265547327374137
X	0.013694458455695608	1416	1.416	0.21305604939740033
X	0.013630089912304709	2625	2.625	0.17316354615302282
X	0.013290038489451572	2289	2.289	0.1797325828311735
X	0.013966787789402533	7830	7.83	0.12127694753295817
X	0.013679630333762918	9291	9.291	0.1137637677778133
X	0.013923062419512921	3646	3.646	0.15630493923810884
X	0.01360640668909896	1100	1.1	0.23126873284750724
X	0.013657257115398093	1552	1.552	0.2064542953308149
X	0.01403986948207149	1297	1.297	0.22121149872398022
X	0.013070738157674342	1422	1.422	0.20947563040742945
X	0.013462705988800132	1347	1.347	0.21540457440265273
X	0.01342106254648028	1721	1.721	0.1983057665755766
X	0.013907655191511454	5609	5.609	0.1353493449526197
X	0.013510383352438987	2533	2.533	0.17472061945937037
X	0.013635177917375148	4420	4.42	0.1455725579269047
X	0.01357729437194543	1991	1.991	0.18963303305951298
X	0.013600174367639455	7421	7.421	0.12237536917468665
X	0.01349209198697913	2524	2.524	0.1748490650110001
X	0.01358181311256522	2540	2.54	0.17486706112562272
X	0.013465592445044808	3696	3.696	0.15387373694050369
X	0.01329280175249747	372	0.372	0.32937542729671626
X	0.014213744947756448	3063	3.063	0.16679659235800273
X	0.013265046669118975	1077	1.077	0.23093921911752285
X	0.013589295148235515	6311	6.311	0.12913171443305016
X	0.013131741611243593	798	0.798	0.25435472661874714
X	0.01359676512706749	2542	2.542	0.17488531748548483
X	0.013584997639576817	2534	2.534	0.17501864569280237
X	0.013681436073310854	1992	1.992	0.19008482638781454
X	0.013561832020096197	1766	1.766	0.1972918842189325
X	0.013946827006675109	4405	4.405	0.14683959131387664
X	0.013219120937783338	588	0.588	0.2822334058153228
X	0.013506057134574704	1604	1.604	0.2034423698614513
X	0.013568908962137806	2533	2.533	0.1749725466988846
X	0.013757308374352356	3580	3.58	0.15663307782652627
X	0.013630053325116697	976	0.976	0.2408144699674326
X	0.013785486734844839	4384	4.384	0.14650434993378406
X	0.013699060477880774	2307	2.307	0.18108441015418872
X	0.01320253964218944	3072	3.072	0.1625843564407058
X	0.01351624725915259	2885	2.885	0.16732857150881156
X	0.013456240530879847	2593	2.593	0.1731305631159519
X	0.013552617688273066	2011	2.011	0.18888771180269406
X	0.013670033391559713	4161	4.161	0.1486587990051898
X	0.013327355313139493	1805	1.805	0.19472524400051464
X	0.013558718112180781	1996	1.996	0.18938810487691335
X	0.013396195686131571	1689	1.689	0.19942699461731053
X	0.013536112661902423	2899	2.899	0.1671405838915287
X	0.013915314301973713	4382	4.382	0.1469851801314094
X	0.013706033491127636	2054	2.054	0.18826537331823603
X	0.013373469494372193	2838	2.838	0.1676527056914755
X	0.013469878387458945	4956	4.956	0.13955459016490085
X	0.013588394507101276	8143	8.143	0.11861180204456559
X	0.012986176097765242	1268	1.268	0.21716375613445119
X	0.012843231015969105	526	0.526	0.29011010089610983
X	0.013666369329145972	2219	2.219	0.18330118967416495
X	0.013665749823443735	2552	2.552	0.17495147108707992
X	0.013175728949140736	1851	1.851	0.1923635328039541
X	0.013613027799785826	2810	2.81	0.16920614736088152
X	0.01349956386126848	3244	3.244	0.1608469995860127
X	0.013691328797516547	1144	1.144	0.22873881062417928
X	0.01374795974262198	4009	4.009	0.15079978829355678
X	0.013626173076172405	1041	1.041	0.23567185695402396
X	0.013853540395922948	3235	3.235	0.1623910775046273
X	0.01321568819492549	1623	1.623	0.20118260220063405
X	0.013696570283425783	5820	5.82	0.13301367171824152
X	0.013610057940942274	4355	4.355	0.14620333240382624
X	0.013680652154935287	5493	5.493	0.13554985807797199
X	0.013608977135899612	1165	1.165	0.22689931582900402
X	0.014009447076401688	3216	3.216	0.163318350504151
X	0.013650006535890411	2740	2.74	0.17078934206818333
X	0.013702701444142497	2296	2.296	0.1813892049389404
X	0.013837096246910086	6291	6.291	0.13004942176082526
X	0.013130549047799481	2405	2.405	0.17608509738292485
X	0.013415573426894296	2335	2.335	0.1791048935451183
X	0.013680547697769758	1942	1.942	0.19169822267574668
X	0.013416025913970729	1896	1.896	0.19198259241408158
X	0.013497507791567212	587	0.587	0.28436217577034567
X	0.013684002873198294	639	0.639	0.27769673026037855
X	0.013626027103121078	695	0.695	0.26964651681460594
X	0.013331927662643196	1002	1.002	0.23696001882485854
X	0.013562206597052432	9823	9.823	0.11135132877832334
X	0.013574458946044745	1533	1.533	0.20688394602432178
X	0.012891859595531118	1162	1.162	0.2230333527554562
X	0.013441917026818129	929	0.929	0.24367735133783944
X	0.013893907975513636	3175	3.175	0.1635661994805599
X	0.013485155082236938	1267	1.267	0.21996813710023172
X	0.013580391307730293	3874	3.874	0.1519092510368537
X	0.013393137744149651	1793	1.793	0.19547925804236252
X	0.01351469605767611	1364	1.364	0.21478171419968148
X	0.01343003842173692	947	0.947	0.24205223219725655
X	0.013867353462051589	2474	2.474	0.1776361274078004
X	0.012708032786181312	521	0.521	0.2900103747054411
X	0.01353184779883236	1006	1.006	0.23782255535901398
X	0.013274916975582241	1476	1.476	0.20796096245181464
X	0.013738591171828341	2711	2.711	0.17176614187365327
X	0.01394259927633653	3024	3.024	0.1664385740020282
X	0.013540786488855826	2817	2.817	0.16876628133174526
X	0.01352797107778291	1351	1.351	0.21553895471414436
X	0.013947714188194802	2564	2.564	0.17587131311042437
X	0.01391568987268203	4746	4.746	0.14312835147338354
X	0.013438136727608874	754	0.754	0.26120966160771647
X	0.013654418140799623	7110	7.11	0.12429906614497661
X	0.01373464791350293	21909	21.909	0.08558513206285334
X	0.013734988348963592	3966	3.966	0.1512952158060921
X	0.01357012798032195	3653	3.653	0.15487384852038735
X	0.01308381990605434	1304	1.304	0.21568452619498604
X	0.013523788297365934	3485	3.485	0.1571442617671021
X	0.013713774830358532	5087	5.087	0.13917623772035098
X	0.013742118474283185	46061	46.061	0.06682004677644118
X	0.013713048677362701	2649	2.649	0.17298854788956844
X	0.013858466716397518	89617	89.617	0.053675355216287156
X	0.013621581815804244	39878	39.878	0.06990336110652083
X	0.013575410585928944	13906	13.906	0.0992011978313001
X	0.013635062010370682	26312	26.312	0.08032198246708155
X	0.013456994938374395	1224	1.224	0.2223594325561177
X	0.013695704069152709	2427	2.427	0.17803478414786245
X	0.01391803682636214	327258	327.258	0.03490566393938292
X	0.013733608725923498	192588	192.588	0.041468514894758224
X	0.013754044690100001	3659	3.659	0.15548530165319885
X	0.013641092446352535	22895	22.895	0.08414652160619655
X	0.013850914999476067	106618	106.618	0.05064648515147741
X	0.013736563183663347	16113	16.113	0.09482013304088174
X	0.01368122517391363	20247	20.247	0.08775154453579279
X	0.013893017566496723	19911	19.911	0.08869541514942253
X	0.013723397014518657	11938	11.938	0.1047554638770916
X	0.013974908811706004	11220	11.22	0.1075933364883056
X	0.01358625915482688	1942	1.942	0.1912568019718653
X	0.013937218929307814	56629	56.629	0.06266798678531683
X	0.013559809334359544	4466	4.466	0.14480324289104923
X	0.01375083460169785	3168	3.168	0.16312278534069735
X	0.01365140929146921	14495	14.495	0.09802114155555013
X	0.013635880515883044	14400	14.4	0.09819896226507104
X	0.014196567251902053	142674	142.674	0.04633894787844265
X	0.01358125235315255	30269	30.269	0.07655623763645776
X	0.01342913211177442	1316	1.316	0.2169022309660808
X	0.013710012201409879	18032	18.032	0.09127068588502292
X	0.013556196281298932	5836	5.836	0.13243644722103415
X	0.013626082629370375	10702	10.702	0.10838487735750907
X	0.01376380731698564	42283	42.283	0.06878983435644434
X	0.013984404753908329	27867	27.867	0.07946659217698525
X	0.01374505437394344	46179	46.179	0.06676783785716121
X	0.013372955591921106	873	0.873	0.24835393924346846
X	0.013637092906153843	4277	4.277	0.14718408409937234
X	0.01362712473364088	44973	44.973	0.06716620192542602
X	0.013646602090808235	20528	20.528	0.08727555712708952
X	0.01310626226184306	19200	19.2	0.08804929341167396
X	0.013740811580385863	3251	3.251	0.1616832901887153
X	0.01395966156595981	166971	166.971	0.043726489656279684
X	0.013772604086007315	8811	8.811	0.11605493713425596
X	0.013586733830782743	15116	15.116	0.0965071287344065
X	0.013772178672853444	79018	79.018	0.055858877612981445
X	0.013418491746541085	2318	2.318	0.17955469156954598
X	0.013603197969314175	22825	22.825	0.08415438375345764
X	0.014264659756677086	85738	85.738	0.05500000000858157
X	0.014043126803367311	183751	183.751	0.04243700266861804
X	0.013566414552480284	13873	13.873	0.0992578580796694
X	0.01356997220954191	15013	15.013	0.09668753565981963
X	0.0135195885449498	5221	5.221	0.1373208850235354
X	0.013659965071535808	3454	3.454	0.15814024909506164
X	0.014017737431002278	39926	39.926	0.07054626728353258
X	0.013764815982956641	5751	5.751	0.1337649548856366
X	0.013537210292944855	9301	9.301	0.11332694384204832
X	0.014051181923979804	49998	49.998	0.06550182617325917
X	0.013791881463916513	13999	13.999	0.09950437299866619
X	0.013551343859946817	6054	6.054	0.13081171804924607
X	0.014286909136386116	6177	6.177	0.13224732425767358
X	0.013788543469598956	238228	238.228	0.03868196514767136
X	0.013794485239438638	5597	5.597	0.13507761916691505
X	0.013535950502933148	4169	4.169	0.14807632274682814
X	0.014293475004871901	14466	14.466	0.09960086770345174
X	0.014074379984145681	96363	96.363	0.05266308785251008
X	0.013556904315305443	10088	10.088	0.11035325619370874
X	0.013825900038133382	20427	20.427	0.08780035554322768
X	0.013567501072764778	4353	4.353	0.14607315148597352
X	0.01331010498275589	13371	13.371	0.0998479602727864
X	0.013888855363628431	11540	11.54	0.10637025870059386
X	0.014257452596684072	386035	386.035	0.033302110036101555
X	0.013709945637489729	29539	29.539	0.07742478023342315
X	0.013576505304470913	5361	5.361	0.13630571402813796
X	0.013786951522753707	92336	92.336	0.05305167053265429
X	0.0140272436160383	102170	102.17	0.051588113746370116
X	0.013824021946787982	2416	2.416	0.1788596987275144
X	0.013738598054787242	55566	55.566	0.06276406946279282
X	0.014042761886242542	86314	86.314	0.05459134103164429
X	0.014093175451174576	409765	409.765	0.032520558795333734
X	0.013741711135119888	12232	12.232	0.10395557402565164
X	0.013508172188825432	1461	1.461	0.2098853490947622
X	0.013546556174316902	8311	8.311	0.1176861096593334
X	0.013773860394883082	8814	8.814	0.11604529678399442
X	0.013712558556305831	14763	14.763	0.09756962259774656
X	0.01382359507161493	8582	8.582	0.11722246001208883
X	0.013598809460540784	1924	1.924	0.19191044258171205
X	0.014018240885921924	26376	26.376	0.08100183575489335
X	0.014074546443650575	78998	78.998	0.05626946302155749
X	0.013140943201234786	33407	33.407	0.07327058972776232
X	0.013760820917979399	3309	3.309	0.16081103925886167
X	0.013640908871053263	34182	34.182	0.0736232999028694
X	0.01367184917934707	12670	12.67	0.10256917947997116
X	0.01390352949086581	15720	15.72	0.09598959815302198
X	0.013747141164220784	9009	9.009	0.11512737481279942
X	0.013642281951753831	3096	3.096	0.1639439355635709
X	0.013988633777728366	130452	130.452	0.04750888663530087
X	0.013740722896276881	30606	30.606	0.0765715906911179
X	0.013762933417870563	8367	8.367	0.11804485959630497
X	0.013919692442667546	27685	27.685	0.07951731440765987
X	0.013496577535806195	3704	3.704	0.15388072554414878
X	0.013774576879469062	142053	142.053	0.045941935290723664
X	0.013726984166114828	9525	9.525	0.11295445976795167
X	0.013601068569345781	62243	62.243	0.06023203055829159
X	0.013707529606212826	1637	1.637	0.20306582398678522
X	0.013535549928815513	3481	3.481	0.1572499902905805
X	0.013649466230644183	3463	3.463	0.1579626432285532
X	0.013790672419076595	9571	9.571	0.11294734901852867
X	0.013654435618904967	3137	3.137	0.16327501981072465
X	0.01375352228485575	17417	17.417	0.09243012234238614
X	0.013550993994933022	2711	2.711	0.17098074763153484
X	0.013578685503561387	2665	2.665	0.1720759485053069
X	0.013809076759391983	130630	130.63	0.047283241175730246
X	0.013747314877076736	28185	28.185	0.07871665188851054
X	0.013843795399227287	27593	27.593	0.07946064458426585
X	0.014087183104600887	98053	98.053	0.052374644876853095
X	0.014336818470371286	222872	222.872	0.040068131645608046
X	0.013631346556478685	2611	2.611	0.17347782259408862
X	0.013570511424969244	4247	4.247	0.14728934064196417
X	0.0135703127401687	7424	7.424	0.12226926393810598
X	0.013389667896736061	817	0.817	0.2540090847342021
X	0.013277124999940677	3559	3.559	0.1550929364173107
X	0.013717572430479443	10523	10.523	0.1092393809924829
X	0.013298203959479957	6307	6.307	0.1282301212345971
X	0.013585609902486438	3939	3.939	0.15108837598657368
X	0.013460550745819994	5090	5.09	0.1382871042181059
X	0.013829141865256321	11289	11.289	0.10699904946124413
X	0.014050249242051727	669607	669.607	0.027581720154034328
X	0.013305692615271616	3536	3.536	0.15553987418007573
X	0.013644202292448964	40102	40.102	0.0698115647519094
X	0.013670337941188894	8107	8.107	0.11902541176856105
X	0.013645521728195404	2300	2.3	0.181031480478238
X	0.01364980145306282	33121	33.121	0.0744173666421255
X	0.014329677856741151	170723	170.723	0.043783935749291514
X	0.01391221619982562	321569	321.569	0.03510540984615275
X	0.013792807769714102	14841	14.841	0.0975880199120428
X	0.01375292776769955	19462	19.462	0.08907091176199095
X	0.013561223351620239	2487	2.487	0.17601149341829297
X	0.013554368740005884	21438	21.438	0.08582853043423695
X	0.014026853374337445	787690	787.69	0.02611369385995094
X	0.013868866294013366	25872	25.872	0.08123384056212331
X	0.01361965127254049	6266	6.266	0.1295364126765006
X	0.013609759806375629	40026	40.026	0.0697968928963849
X	0.014206156174875632	271606	271.606	0.03739777304655477
X	0.013590052370976146	30213	30.213	0.07662004920560721
X	0.013566784837991192	5024	5.024	0.13925445637467382
X	0.01359200500978182	4944	4.944	0.1400882428685708
X	0.01364216724446109	8004	8.004	0.11945163443005326
X	0.01372263056470131	6445	6.445	0.12864852659844586
X	0.01385779139080622	227836	227.836	0.03932697637987674
X	0.013751653740974501	10079	10.079	0.11091215946434017
X	0.013340101659494175	1052	1.052	0.23319243988192778
X	0.01369798724526054	31961	31.961	0.07539550702371817
X	0.013754246204012804	14416	14.416	0.09844583345836702
X	0.013522005988761526	1657	1.657	0.2013289391657435
X	0.013658034390075947	74247	74.247	0.05687260185560005
X	0.013965323582699621	47677	47.677	0.06641212258823688
X	0.013973128776091692	56144	56.144	0.0629018526445954
X	0.01407083020439276	91212	91.212	0.053631825708554265
X	0.013470047631315476	94056	94.056	0.05231917358547196
X	0.013606103069913372	15974	15.974	0.09479236501777326
X	0.01361096129249919	39797	39.797	0.06993256987584416
X	0.013803928655890557	63118	63.118	0.06024899759598969
X	0.013752565867380242	155609	155.609	0.04454336344673422
X	0.013729843935349647	4627	4.627	0.14369953641544658
X	0.014021430952411585	67903	67.903	0.059106393192679574
X	0.013956843891917916	80039	80.039	0.05586783677782199
X	0.013954921349256234	42785	42.785	0.06883541772637028
X	0.013881296052839468	28522	28.522	0.0786592884952775
X	0.013422275236448391	16951	16.951	0.09251459929123344
X	0.013722251157390754	9533	9.533	0.11290987422739648
X	0.013565968485976566	3961	3.961	0.1507354217733716
X	0.014015004259263551	56221	56.221	0.06293586747419187
X	0.014095275674263776	102785	102.785	0.051568117531237995
X	0.013749213157116037	82087	82.087	0.055123208352033964
X	0.0138373692405032	90841	90.841	0.053406060226856795
X	0.013882016263941019	84873	84.873	0.054688288464099004
X	0.013650039863889168	60850	60.85	0.06076093464598812
X	0.01331619408015589	9582	9.582	0.11159412122410402
X	0.0137854085645911	207276	207.276	0.040515718122706226
X	0.013607041211803861	1879	1.879	0.19346940974886795
X	0.013310390371573113	10566	10.566	0.10800073940726881
X	0.013712817617031695	158193	158.193	0.04425677998751856
X	0.01356772120131696	11459	11.459	0.1057921387466771
X	0.0135968186458249	2556	2.556	0.17456566150733932
X	0.013751816636412237	13225	13.225	0.10131057900250767
X	0.013918601243074188	118752	118.752	0.048938601371693924
X	0.013465484065775956	15974	15.974	0.09446467398415972
X	0.0139936959246195	52070	52.07	0.06453296483553314
X	0.013588288729873978	2982	2.982	0.1657877264172324
X	0.013692346865553576	3032	3.032	0.16529113627294245
X	0.013725685150319836	3205	3.205	0.1623935090253317
X	0.013590908043198634	14102	14.102	0.09877702032231353
X	0.013897395689871476	23805	23.805	0.08357731451745005
X	0.013648230337300164	51105	51.105	0.0643977929738167
X	0.013635441301474979	2972	2.972	0.16616522157827476
X	0.013784877138824415	51665	51.665	0.06437770473190589
X	0.013577335370726439	4722	4.722	0.14219888387152
X	0.013985427442406374	25227	25.227	0.08214920768159463
X	0.013646207500840926	12514	12.514	0.10292920222810707
X	0.013744832891617523	76067	76.067	0.05653460886804168
X	0.013686455785989305	3147	3.147	0.1632292876355066
X	0.013632137940721052	25415	25.415	0.08125023379348888
X	0.013741958020486729	13411	13.411	0.10081592789668209
X	0.013709978883169587	12227	12.227	0.10388965193021868
X	0.013803833781576487	32413	32.413	0.07523619625300282
X	0.013631995352646774	138597	138.597	0.046160290486359425
X	0.013678868357685335	4065	4.065	0.1498522287884608
X	0.013385818572303716	7344	7.344	0.12215298630191458
X	0.013810179696677213	76883	76.883	0.056423021087818624
X	0.013723066854303287	10637	10.637	0.1088622584565591
X	0.014342373094568348	463206	463.206	0.03140139533108355
X	0.013316889211290085	2047	2.047	0.18667879176000124
X	0.014032572412055534	85250	85.25	0.05480425790748832
X	0.014034394383521625	195112	195.112	0.041588178432297285
X	0.01374357358836526	3114	3.114	0.1640314112243056
X	0.013983260696824654	57773	57.773	0.0623200574277
X	0.013673431873842122	10816	10.816	0.10812769644129572
X	0.013807655613390807	188528	188.528	0.04183900637679753
X	0.013703961975106857	14314	14.314	0.09855871759755436
X	0.013667090798651648	13295	13.295	0.10092433876358833
X	0.013607420758317909	1784	1.784	0.1968461784937948
X	0.01353558685576557	2910	2.91	0.16692755553348984
X	0.013597608311790771	5502	5.502	0.13520123133893458
X	0.014074823747312181	16730	16.73	0.09440224910884008
X	0.013748680375378524	12911	12.911	0.10211754800770229
X	0.013982868975903973	69766	69.766	0.05852177474743743
X	0.01362018290758273	15986	15.986	0.09480131839108155
X	0.013641641555528867	2112	2.112	0.18623340053558723
X	0.01361998519663237	9924	9.924	0.1111296534371263
X	0.014330404071217323	44997	44.997	0.06829018545720818
X	0.013812374216220585	10138	10.138	0.11085927049399706
X	0.014135608075448279	112334	112.334	0.0501111562851898
X	0.01386578588385769	24978	24.978	0.08218557716368459
X	0.013706646811390307	4176	4.176	0.14861303746909335
X	0.013339720106789162	2069	2.069	0.18612101355264846
X	0.013577257445026018	4398	4.398	0.14560812494117797
X	0.014109046687311961	290688	290.688	0.0364773703242182
X	0.0136488511168556	7368	7.368	0.12281426866198973
X	0.013646374655228957	9637	9.637	0.1122944977789332
X	0.013656372713053342	46661	46.661	0.06639373586781212
X	0.013389149927978506	7512	7.512	0.12124555015433501
X	0.0135765956347314	2191	2.191	0.18367476880828273
X	0.01356458702794884	4480	4.48	0.14466923582228544
X	0.014199447235765092	104902	104.902	0.051344731839573776
X	0.013641625333407683	30517	30.517	0.07646125580033654
X	0.013531278018322351	11441	11.441	0.10575273562238657
X	0.013780141312124683	7073	7.073	0.12489642411922246
X	0.013879946142314005	60430	60.43	0.06124139322700626
X	0.013960995332874801	60299	60.299	0.06140476741137246
X	0.013645558190504822	67768	67.768	0.05861231554623333
X	0.013769343593870076	73840	73.84	0.057131266240961126
X	0.013652664887559462	2964	2.964	0.16638458085924576
X	0.014049275251308242	86619	86.619	0.054535619624604154
X	0.013612055706665743	5386	5.386	0.13621317714868678
X	0.01360189094701897	26914	26.914	0.07965389431314461
X	0.013601289784998732	9259	9.259	0.11367684279061023
X	0.013901251317744183	3492	3.492	0.15848684499200177
X	0.013643843512548636	10006	10.006	0.11088992175493105
X	0.013597998848636082	20474	20.474	0.08724839242343273
X	0.01301744117160742	3219	3.219	0.15931923619915497
X	0.013693225853580893	18779	18.779	0.09000730739819858
X	0.013781389832947197	6133	6.133	0.13098051218321827
X	0.013690386892789806	106719	106.719	0.05043414447018759
X	0.013714919219185575	2820	2.82	0.16942651660671207
X	0.013389195427888634	4849	4.849	0.140292385137817
X	0.01322028266302876	4434	4.434	0.14392901416016787
X	0.013649041606779716	16140	16.14	0.09456553322547404
X	0.013736656506599094	2437	2.437	0.17796796581966517
X	0.01356172643781521	10292	10.292	0.1096322685355337
X	0.013761262932558131	16208	16.208	0.094691200287143
X	0.014080815341591852	54384	54.384	0.06373610974526744
X	0.013475705599348313	3474	3.474	0.15712329093062052
X	0.01341664486928453	1206	1.206	0.22323667169828665
X	0.013448130475518873	1795	1.795	0.19567371345645565
X	0.01374083672017856	142337	142.337	0.045873842972046205
X	0.01434229692168319	852884	852.884	0.025619830217932918
X	0.013626012228551034	12912	12.912	0.10181030824289684
X	0.013617600804769862	4957	4.957	0.14005347639439464
X	0.01387802787300111	57433	57.433	0.0622857575736932
X	0.014009743093708614	76750	76.75	0.05672623098988962
X	0.013238181701680153	1540	1.54	0.2048499547232292
X	0.013668597062359341	9952	9.952	0.11115726638212889
X	0.013790611183274104	21189	21.189	0.086661130675322
X	0.014078530663683347	370275	370.275	0.033626188781298263
X	0.01387836525576078	431390	431.39	0.03180456157542845
X	0.01373221335289383	127687	127.687	0.04755519348249563
X	0.013696986041940716	8165	8.165	0.11882001641504863
X	0.013602038968501654	1957	1.957	0.19084072730952498
X	0.013435373302037031	2394	2.394	0.1777086348194839
X	0.013671902484895283	42750	42.75	0.06838553953471875
X	0.013808207692993303	8456	8.456	0.11775809702083329
X	0.013673801040630163	15029	15.029	0.09689909568537845
X	0.013949818572310483	105460	105.46	0.05095188599109434
X	0.013960285184877073	295038	295.038	0.036169189611916064
X	0.01388654798729906	5175	5.175	0.1389615495914637
X	0.013423466244924446	2111	2.111	0.18526447379281732
X	0.01409848527800927	35868	35.868	0.07325238422142734
X	0.013775513419772243	87824	87.824	0.05393013741932544
X	0.013621066328237194	28382	28.382	0.07829295844717574
X	0.013680193250180455	1424	1.424	0.21258245313264956
X	0.013579923515779269	2145	2.145	0.1849935880321922
X	0.013767879615275883	9342	9.342	0.11380003588010063
X	0.01372939106289132	1864	1.864	0.19456667438621747
X	0.013527662362964278	23094	23.094	0.08367091748595117
X	0.013870575929342608	2972	2.972	0.16711492164646455
X	0.013669734928251244	9860	9.86	0.1115050130201045
X	0.013629606123983062	7176	7.176	0.12384171524737828
X	0.013601056839993846	4003	4.003	0.1503357799779329
X	0.013533744935168707	1396	1.396	0.21322797851920516
X	0.014029310874985197	114938	114.938	0.049604860693792115
X	0.013944055840932458	254454	254.454	0.03798335101670391
X	0.013784222590704845	53481	53.481	0.06363962122639076
X	0.013431480026156977	12336	12.336	0.10287657645503127
X	0.013739209171930557	12880	12.88	0.10217593584032623
X	0.013653525403589765	4402	4.402	0.1458360596197107
X	0.013581876588641566	12657	12.657	0.1023787132049673
X	0.014015829092800944	104245	104.245	0.05122962613526179
X	0.014206843479708702	9688	9.688	0.11361120213923936
X	0.013605236245530056	34477	34.477	0.07334866371803234
X	0.01374076460837673	15493	15.493	0.09607825934191157
X	0.014295246305535357	702278	702.278	0.02730407865324201
X	0.01363861055716409	7446	7.446	0.12235330328104353
X	0.013833719821080214	45248	45.248	0.06736689475974682
X	0.01406381795019988	139081	139.081	0.046588473111292085
X	0.013898521472697946	45247	45.247	0.06747241749451424
X	0.013701298614584726	17842	17.842	0.0915741154265746
X	0.01417293718113166	229425	229.425	0.03953117354738616
X	0.01362142310790515	47243	47.243	0.06606351415853864
X	0.013743488455867479	16545	16.545	0.09400336572646543
X	0.013457651755594907	4426	4.426	0.1448725013817782
X	0.013752700553880873	13851	13.851	0.09976287414038426
X	0.013726468129717022	8459	8.459	0.11751138015665522
X	0.013998671332251085	56121	56.121	0.06294875407929432
X	0.014322838340515372	391390	391.39	0.033200130309148375
X	0.013947508903552213	59417	59.417	0.06168723566497457
X	0.014070460928663387	96620	96.62	0.052611469275368555
X	0.01378627748107574	12725	12.725	0.10270614016222832
X	0.013720776462739075	11112	11.112	0.10728248355296742
X	0.014044847868485607	21476	21.476	0.08680029770084546
X	0.01350046236700321	4957	4.957	0.13965073996158953
X	0.01407111966254876	315946	315.946	0.035446383481993436
X	0.014102124456993081	235517	235.517	0.03912197406482574
X	0.013293678347225462	6526	6.526	0.12676500142787384
X	0.014005717772596077	899559	899.559	0.024970385043865208
X	0.013645413585206184	69221	69.221	0.05819910126398718
X	0.013555243215780964	21512	21.512	0.08573184597533742
X	0.013551457646681813	2391	2.391	0.1782934869282217
X	0.013844110318758148	33174	33.174	0.07472898273235926
X	0.013880830142596662	71726	71.726	0.05784239621309184
X	0.013649574785175958	1892	1.892	0.19322619273216515
X	0.014049450535668296	497291	497.291	0.03045670476129249
X	0.013796132182104705	18046	18.046	0.09143773603920595
X	0.013766904624611892	15385	15.385	0.09636358183780752
X	0.01374779818536506	5707	5.707	0.13405255375191258
X	0.013645094516346525	9231	9.231	0.11391369321833204
X	0.013548002807954285	1393	1.393	0.2134558461745863
X	0.013697795712482624	12460	12.46	0.10320741031679626
X	0.01358224998389484	10162	10.162	0.11015329930255846
X	0.013693639356682602	11840	11.84	0.10496771125395231
X	0.014288021203730022	690216	690.216	0.027457586125143624
X	0.013638185570799273	14252	14.252	0.09854326040656788
X	0.013740582892286489	71950	71.95	0.057587043961758545
X	0.013719754667430426	2755	2.755	0.17076869022287117
X	0.013534043670286792	11935	11.935	0.10428016669143295
X	0.0136593714809459	6951	6.951	0.12525482828814746
X	0.013715429340341695	47692	47.692	0.06600669034716428
X	0.013481835197371013	107130	107.13	0.05011248851154005
X	0.014113087650749399	647436	647.436	0.027934553334960217
X	0.013690942951132283	12495	12.495	0.10309375701537292
X	0.014293888025953248	124978	124.978	0.04854065157918256
X	0.01379098413694969	12330	12.33	0.10380319652999988
X	0.013696375975425779	81848	81.848	0.055106039500541
X	0.01358744501173204	101550	101.55	0.05114689696359878
X	0.013707648441142967	12415	12.415	0.10335672616967988
X	0.013625554212578671	3686	3.686	0.15462021816455565
X	0.013783858008575133	43078	43.078	0.06839721517620712
X	0.013675912223638653	13071	13.071	0.10151942828907498
X	0.01364777357119031	61162	61.162	0.06065408330056185
X	0.013681134294864446	166099	166.099	0.04350959386045869
X	0.013736907874318438	7749	7.749	0.12102667675794863
X	0.013788569862682929	32884	32.884	0.07484765439281159
X	0.01358548141983987	10498	10.498	0.10897398220057257
X	0.013696530833186442	120092	120.092	0.04849520884627366
X	0.013802930156790114	176165	176.165	0.042790814595921316
X	0.013525792912567702	17328	17.328	0.09207423054547012
X	0.013598379835376182	5963	5.963	0.13162572799157252
X	0.013991082808948998	231872	231.872	0.03922241582834646
X	0.01391030559256624	70411	70.411	0.05824143311584416
X	0.013986182806189563	388065	388.065	0.03303174940587579
X	0.0137650576873087	44175	44.175	0.06779544138011226
X	0.013654640713290617	43818	43.818	0.06779680723997404
X	0.01395621493966261	16618	16.618	0.09434733376078835
X	0.01363681417529354	33755	33.755	0.07392504599438586
X	0.013979242056835667	213486	213.486	0.04030615849112039
X	0.013759501965278857	31836	31.836	0.07560689556190726
X	0.013574813323053244	5403	5.403	0.1359459579977579
X	0.013814280245251822	8708	8.708	0.11662812149420548
X	0.013229692697020796	21689	21.689	0.08480798458162707
X	0.013702905507853427	50901	50.901	0.06456970210771448
X	0.013729448760801318	40461	40.461	0.06974913554965355
X	0.013844807396135332	6782	6.782	0.1268556196307526
X	0.013541141867449378	14156	14.156	0.09853070223582933
X	0.013836360587085816	70598	70.598	0.05808667151343557
X	0.013769031714019801	32479	32.479	0.0751219611176609
X	0.013715457570669025	36520	36.52	0.07214836782195101
X	0.01377154482638701	44178	44.178	0.06780455495216797
X	0.013676280082051259	21284	21.284	0.08629220741442828
X	0.01374168608852572	13613	13.613	0.10031411814037626
X	0.013507482757754383	2475	2.475	0.17606231427029365
X	0.014093172890342034	5631	5.631	0.13577122948465667
X	0.013870346645246437	85414	85.414	0.05455728519896442
X	0.01355481281967687	10387	10.387	0.10927843439876452
X	0.0137673540319989	46015	46.015	0.06688319607274612
X	0.013728549340528588	1404	1.404	0.21383847424833238
X	0.013902994640123414	4036	4.036	0.15102623619836883
X	0.013809600423743549	58274	58.274	0.061882629734350804
X	0.013889310436573054	56602	56.602	0.0626060498376066
X	0.013749617833852068	16790	16.79	0.09355779486517456
X	0.013695852218920047	2985	2.985	0.16616832491685693
X	0.013776934704547987	1786	1.786	0.19758640000032543
X	0.013990283912661262	15760	15.76	0.09610738742842534
X	0.013846818927676014	16598	16.598	0.09413795309887903
X	0.013899445284837296	62697	62.697	0.06052250605142102
X	0.013483008662500568	4441	4.441	0.1448000394303459
X	0.013622485929751465	10776	10.776	0.10812669497020162
X	0.013475866087446397	7702	7.702	0.12049926303067887
X	0.01351220882238122	11767	11.767	0.10471772493561571
X	0.01395477118177761	117444	117.444	0.049162122932440794
X	0.013718082216267254	4182	4.182	0.14858322858729822
X	0.013617942985900524	36224	36.224	0.07217249218164896
X	0.014073062967221021	170751	170.751	0.04351862098481877
X	0.013643667914228446	25985	25.985	0.08067447347964984
X	0.013838208919231128	38551	38.551	0.07106919752715565
X	0.013817356744932442	73770	73.77	0.05721567980932406
X	0.013244815568071557	1404	1.404	0.21129680773726134
X	0.013557508442714705	22054	22.054	0.08502843214349193
X	0.013718706629613213	8608	8.608	0.11680738558462743
X	0.013773080572140368	161457	161.457	0.044020814087774536
X	0.013787799419346825	46997	46.997	0.06644693304156447
X	0.013399240641489023	15213	15.213	0.09585654552648758
X	0.013992462393943177	57767	57.767	0.06233588238703717
X	0.013667618387861494	11551	11.551	0.10576884296663785
X	0.013597282561680536	4171	4.171	0.1482759257682792
X	0.013695417255800802	3080	3.08	0.16444027104602194
X	0.01362020949394914	7364	7.364	0.12275051868203296
X	0.01397462832499915	79839	79.839	0.05593818777378753
X	0.014329269331080264	431291	431.291	0.03214779695819712
X	0.01367383118164224	40578	40.578	0.06958781368009087
X	0.013762370943281822	9422	9.422	0.11346190034148575
X	0.014096328178576128	18176	18.176	0.09187610230541844
X	0.01425148905103422	254157	254.157	0.0382753692674299
X	0.013711250490043786	10835	10.835	0.1081639942630387
X	0.01367583387896603	8003	8.003	0.11955479523341665
X	0.013767647880696676	41951	41.951	0.06897723966630688
X	0.01402637678047822	63897	63.897	0.060323751773306594
X	0.013440846918026467	5537	5.537	0.13439528639242154
X	0.013720538545457534	11528	11.528	0.10597556243422722
X	0.013754535330958498	16405	16.405	0.09429526734670504
time for making epsilon is 1.7250306606292725
epsilons are
[0.25829731028757563, 0.0933778596988293, 0.1792675427751703, 0.17380358378171257, 0.09161480595718452, 0.0819479020494093, 0.06674762222288604, 0.07497161129284723, 0.09708071689791312, 0.073187898354903, 0.09017757656858016, 0.07192914534883287, 0.15237401249792307, 0.043602263456266575, 0.09388084910939064, 0.08564991347838731, 0.0705238179557852, 0.06954187197345829, 0.053235461084744376, 0.04972466970725659, 0.10790416115175945, 0.047198557334997095, 0.11494512470385668, 0.10124355734460351, 0.14637446143069432, 0.06604909170518282, 0.06656532321705334, 0.12136749795195481, 0.04261721529959421, 0.02862223806636285, 0.12699472836807288, 0.02457892529678339, 0.12589508597032342, 0.10583679843247329, 0.12587424235768327, 0.035197142050975315, 0.06171718638743393, 0.22223797186030442, 0.15335849817253586, 0.17407383080714625, 0.17706212651737457, 0.2126197760820055, 0.2446044211884712, 0.1801378624300584, 0.29396505871374656, 0.22052678944676413, 0.1474175967045332, 0.14508364568908977, 0.21993470277198562, 0.13729198187137806, 0.10610994149584708, 0.21031026095325694, 0.13020056543986822, 0.15587526114439232, 0.13731368111155864, 0.16067422235879464, 0.1966442727036106, 0.18506489605730758, 0.1801347583316486, 0.17399379064974171, 0.1335916155713465, 0.2195588798586517, 0.13700563046048425, 0.12635830562507466, 0.18163116824627445, 0.19655913330133123, 0.17830716223799656, 0.18744094986081455, 0.16526626196476546, 0.19239423815043086, 0.20424510129162154, 0.19883518429940736, 0.2125786401812963, 0.19928485028713655, 0.17255014561065307, 0.10149089188725985, 0.20302637036760104, 0.2701356870474148, 0.19040079490256143, 0.12352501815812295, 0.18912412090482023, 0.25656900939175153, 0.1776439089981755, 0.24922532760878838, 0.18936029103054483, 0.22219669578017293, 0.17501790148549498, 0.21671072001297398, 0.18264354736174593, 0.15826630812615017, 0.20304286398429652, 0.20979932095721832, 0.21686683712057425, 0.11882438638889424, 0.18378201783956546, 0.14812394673069076, 0.1606066325420381, 0.17231468608776299, 0.17788241944419977, 0.1595057504163704, 0.10402388566416171, 0.16403527966798273, 0.15305527817718503, 0.1720868470227607, 0.13734066598735528, 0.1693629981257469, 0.15693473246428757, 0.19026178676172556, 0.18766511063718602, 0.2526476935100902, 0.16346788161823955, 0.21836156652876726, 0.2423052931228147, 0.21058613349798447, 0.1596016468669968, 0.2201246034260952, 0.22480551530762827, 0.24090778087785, 0.12684169566842166, 0.23229100330774866, 0.126497169878671, 0.18817976930528485, 0.19156049734384825, 0.15641141325100164, 0.16434778221868795, 0.2164294321969896, 0.18112609736783272, 0.20700380323996154, 0.11929301760913977, 0.1757875245448772, 0.15495736258650994, 0.22355140701002965, 0.20171414498849216, 0.20924546151394863, 0.19687722787572354, 0.1990570454429127, 0.24403049969263754, 0.19656922832207105, 0.21265547327374137, 0.21305604939740033, 0.17316354615302282, 0.1797325828311735, 0.12127694753295817, 0.1137637677778133, 0.15630493923810884, 0.23126873284750724, 0.2064542953308149, 0.22121149872398022, 0.20947563040742945, 0.21540457440265273, 0.1983057665755766, 0.1353493449526197, 0.17472061945937037, 0.1455725579269047, 0.18963303305951298, 0.12237536917468665, 0.1748490650110001, 0.17486706112562272, 0.15387373694050369, 0.32937542729671626, 0.16679659235800273, 0.23093921911752285, 0.12913171443305016, 0.25435472661874714, 0.17488531748548483, 0.17501864569280237, 0.19008482638781454, 0.1972918842189325, 0.14683959131387664, 0.2822334058153228, 0.2034423698614513, 0.1749725466988846, 0.15663307782652627, 0.2408144699674326, 0.14650434993378406, 0.18108441015418872, 0.1625843564407058, 0.16732857150881156, 0.1731305631159519, 0.18888771180269406, 0.1486587990051898, 0.19472524400051464, 0.18938810487691335, 0.19942699461731053, 0.1671405838915287, 0.1469851801314094, 0.18826537331823603, 0.1676527056914755, 0.13955459016490085, 0.11861180204456559, 0.21716375613445119, 0.29011010089610983, 0.18330118967416495, 0.17495147108707992, 0.1923635328039541, 0.16920614736088152, 0.1608469995860127, 0.22873881062417928, 0.15079978829355678, 0.23567185695402396, 0.1623910775046273, 0.20118260220063405, 0.13301367171824152, 0.14620333240382624, 0.13554985807797199, 0.22689931582900402, 0.163318350504151, 0.17078934206818333, 0.1813892049389404, 0.13004942176082526, 0.17608509738292485, 0.1791048935451183, 0.19169822267574668, 0.19198259241408158, 0.28436217577034567, 0.27769673026037855, 0.26964651681460594, 0.23696001882485854, 0.11135132877832334, 0.20688394602432178, 0.2230333527554562, 0.24367735133783944, 0.1635661994805599, 0.21996813710023172, 0.1519092510368537, 0.19547925804236252, 0.21478171419968148, 0.24205223219725655, 0.1776361274078004, 0.2900103747054411, 0.23782255535901398, 0.20796096245181464, 0.17176614187365327, 0.1664385740020282, 0.16876628133174526, 0.21553895471414436, 0.17587131311042437, 0.14312835147338354, 0.26120966160771647, 0.12429906614497661, 0.08558513206285334, 0.1512952158060921, 0.15487384852038735, 0.21568452619498604, 0.1571442617671021, 0.13917623772035098, 0.06682004677644118, 0.17298854788956844, 0.053675355216287156, 0.06990336110652083, 0.0992011978313001, 0.08032198246708155, 0.2223594325561177, 0.17803478414786245, 0.03490566393938292, 0.041468514894758224, 0.15548530165319885, 0.08414652160619655, 0.05064648515147741, 0.09482013304088174, 0.08775154453579279, 0.08869541514942253, 0.1047554638770916, 0.1075933364883056, 0.1912568019718653, 0.06266798678531683, 0.14480324289104923, 0.16312278534069735, 0.09802114155555013, 0.09819896226507104, 0.04633894787844265, 0.07655623763645776, 0.2169022309660808, 0.09127068588502292, 0.13243644722103415, 0.10838487735750907, 0.06878983435644434, 0.07946659217698525, 0.06676783785716121, 0.24835393924346846, 0.14718408409937234, 0.06716620192542602, 0.08727555712708952, 0.08804929341167396, 0.1616832901887153, 0.043726489656279684, 0.11605493713425596, 0.0965071287344065, 0.055858877612981445, 0.17955469156954598, 0.08415438375345764, 0.05500000000858157, 0.04243700266861804, 0.0992578580796694, 0.09668753565981963, 0.1373208850235354, 0.15814024909506164, 0.07054626728353258, 0.1337649548856366, 0.11332694384204832, 0.06550182617325917, 0.09950437299866619, 0.13081171804924607, 0.13224732425767358, 0.03868196514767136, 0.13507761916691505, 0.14807632274682814, 0.09960086770345174, 0.05266308785251008, 0.11035325619370874, 0.08780035554322768, 0.14607315148597352, 0.0998479602727864, 0.10637025870059386, 0.033302110036101555, 0.07742478023342315, 0.13630571402813796, 0.05305167053265429, 0.051588113746370116, 0.1788596987275144, 0.06276406946279282, 0.05459134103164429, 0.032520558795333734, 0.10395557402565164, 0.2098853490947622, 0.1176861096593334, 0.11604529678399442, 0.09756962259774656, 0.11722246001208883, 0.19191044258171205, 0.08100183575489335, 0.05626946302155749, 0.07327058972776232, 0.16081103925886167, 0.0736232999028694, 0.10256917947997116, 0.09598959815302198, 0.11512737481279942, 0.1639439355635709, 0.04750888663530087, 0.0765715906911179, 0.11804485959630497, 0.07951731440765987, 0.15388072554414878, 0.045941935290723664, 0.11295445976795167, 0.06023203055829159, 0.20306582398678522, 0.1572499902905805, 0.1579626432285532, 0.11294734901852867, 0.16327501981072465, 0.09243012234238614, 0.17098074763153484, 0.1720759485053069, 0.047283241175730246, 0.07871665188851054, 0.07946064458426585, 0.052374644876853095, 0.040068131645608046, 0.17347782259408862, 0.14728934064196417, 0.12226926393810598, 0.2540090847342021, 0.1550929364173107, 0.1092393809924829, 0.1282301212345971, 0.15108837598657368, 0.1382871042181059, 0.10699904946124413, 0.027581720154034328, 0.15553987418007573, 0.0698115647519094, 0.11902541176856105, 0.181031480478238, 0.0744173666421255, 0.043783935749291514, 0.03510540984615275, 0.0975880199120428, 0.08907091176199095, 0.17601149341829297, 0.08582853043423695, 0.02611369385995094, 0.08123384056212331, 0.1295364126765006, 0.0697968928963849, 0.03739777304655477, 0.07662004920560721, 0.13925445637467382, 0.1400882428685708, 0.11945163443005326, 0.12864852659844586, 0.03932697637987674, 0.11091215946434017, 0.23319243988192778, 0.07539550702371817, 0.09844583345836702, 0.2013289391657435, 0.05687260185560005, 0.06641212258823688, 0.0629018526445954, 0.053631825708554265, 0.05231917358547196, 0.09479236501777326, 0.06993256987584416, 0.06024899759598969, 0.04454336344673422, 0.14369953641544658, 0.059106393192679574, 0.05586783677782199, 0.06883541772637028, 0.0786592884952775, 0.09251459929123344, 0.11290987422739648, 0.1507354217733716, 0.06293586747419187, 0.051568117531237995, 0.055123208352033964, 0.053406060226856795, 0.054688288464099004, 0.06076093464598812, 0.11159412122410402, 0.040515718122706226, 0.19346940974886795, 0.10800073940726881, 0.04425677998751856, 0.1057921387466771, 0.17456566150733932, 0.10131057900250767, 0.048938601371693924, 0.09446467398415972, 0.06453296483553314, 0.1657877264172324, 0.16529113627294245, 0.1623935090253317, 0.09877702032231353, 0.08357731451745005, 0.0643977929738167, 0.16616522157827476, 0.06437770473190589, 0.14219888387152, 0.08214920768159463, 0.10292920222810707, 0.05653460886804168, 0.1632292876355066, 0.08125023379348888, 0.10081592789668209, 0.10388965193021868, 0.07523619625300282, 0.046160290486359425, 0.1498522287884608, 0.12215298630191458, 0.056423021087818624, 0.1088622584565591, 0.03140139533108355, 0.18667879176000124, 0.05480425790748832, 0.041588178432297285, 0.1640314112243056, 0.0623200574277, 0.10812769644129572, 0.04183900637679753, 0.09855871759755436, 0.10092433876358833, 0.1968461784937948, 0.16692755553348984, 0.13520123133893458, 0.09440224910884008, 0.10211754800770229, 0.05852177474743743, 0.09480131839108155, 0.18623340053558723, 0.1111296534371263, 0.06829018545720818, 0.11085927049399706, 0.0501111562851898, 0.08218557716368459, 0.14861303746909335, 0.18612101355264846, 0.14560812494117797, 0.0364773703242182, 0.12281426866198973, 0.1122944977789332, 0.06639373586781212, 0.12124555015433501, 0.18367476880828273, 0.14466923582228544, 0.051344731839573776, 0.07646125580033654, 0.10575273562238657, 0.12489642411922246, 0.06124139322700626, 0.06140476741137246, 0.05861231554623333, 0.057131266240961126, 0.16638458085924576, 0.054535619624604154, 0.13621317714868678, 0.07965389431314461, 0.11367684279061023, 0.15848684499200177, 0.11088992175493105, 0.08724839242343273, 0.15931923619915497, 0.09000730739819858, 0.13098051218321827, 0.05043414447018759, 0.16942651660671207, 0.140292385137817, 0.14392901416016787, 0.09456553322547404, 0.17796796581966517, 0.1096322685355337, 0.094691200287143, 0.06373610974526744, 0.15712329093062052, 0.22323667169828665, 0.19567371345645565, 0.045873842972046205, 0.025619830217932918, 0.10181030824289684, 0.14005347639439464, 0.0622857575736932, 0.05672623098988962, 0.2048499547232292, 0.11115726638212889, 0.086661130675322, 0.033626188781298263, 0.03180456157542845, 0.04755519348249563, 0.11882001641504863, 0.19084072730952498, 0.1777086348194839, 0.06838553953471875, 0.11775809702083329, 0.09689909568537845, 0.05095188599109434, 0.036169189611916064, 0.1389615495914637, 0.18526447379281732, 0.07325238422142734, 0.05393013741932544, 0.07829295844717574, 0.21258245313264956, 0.1849935880321922, 0.11380003588010063, 0.19456667438621747, 0.08367091748595117, 0.16711492164646455, 0.1115050130201045, 0.12384171524737828, 0.1503357799779329, 0.21322797851920516, 0.049604860693792115, 0.03798335101670391, 0.06363962122639076, 0.10287657645503127, 0.10217593584032623, 0.1458360596197107, 0.1023787132049673, 0.05122962613526179, 0.11361120213923936, 0.07334866371803234, 0.09607825934191157, 0.02730407865324201, 0.12235330328104353, 0.06736689475974682, 0.046588473111292085, 0.06747241749451424, 0.0915741154265746, 0.03953117354738616, 0.06606351415853864, 0.09400336572646543, 0.1448725013817782, 0.09976287414038426, 0.11751138015665522, 0.06294875407929432, 0.033200130309148375, 0.06168723566497457, 0.052611469275368555, 0.10270614016222832, 0.10728248355296742, 0.08680029770084546, 0.13965073996158953, 0.035446383481993436, 0.03912197406482574, 0.12676500142787384, 0.024970385043865208, 0.05819910126398718, 0.08573184597533742, 0.1782934869282217, 0.07472898273235926, 0.05784239621309184, 0.19322619273216515, 0.03045670476129249, 0.09143773603920595, 0.09636358183780752, 0.13405255375191258, 0.11391369321833204, 0.2134558461745863, 0.10320741031679626, 0.11015329930255846, 0.10496771125395231, 0.027457586125143624, 0.09854326040656788, 0.057587043961758545, 0.17076869022287117, 0.10428016669143295, 0.12525482828814746, 0.06600669034716428, 0.05011248851154005, 0.027934553334960217, 0.10309375701537292, 0.04854065157918256, 0.10380319652999988, 0.055106039500541, 0.05114689696359878, 0.10335672616967988, 0.15462021816455565, 0.06839721517620712, 0.10151942828907498, 0.06065408330056185, 0.04350959386045869, 0.12102667675794863, 0.07484765439281159, 0.10897398220057257, 0.04849520884627366, 0.042790814595921316, 0.09207423054547012, 0.13162572799157252, 0.03922241582834646, 0.05824143311584416, 0.03303174940587579, 0.06779544138011226, 0.06779680723997404, 0.09434733376078835, 0.07392504599438586, 0.04030615849112039, 0.07560689556190726, 0.1359459579977579, 0.11662812149420548, 0.08480798458162707, 0.06456970210771448, 0.06974913554965355, 0.1268556196307526, 0.09853070223582933, 0.05808667151343557, 0.0751219611176609, 0.07214836782195101, 0.06780455495216797, 0.08629220741442828, 0.10031411814037626, 0.17606231427029365, 0.13577122948465667, 0.05455728519896442, 0.10927843439876452, 0.06688319607274612, 0.21383847424833238, 0.15102623619836883, 0.061882629734350804, 0.0626060498376066, 0.09355779486517456, 0.16616832491685693, 0.19758640000032543, 0.09610738742842534, 0.09413795309887903, 0.06052250605142102, 0.1448000394303459, 0.10812669497020162, 0.12049926303067887, 0.10471772493561571, 0.049162122932440794, 0.14858322858729822, 0.07217249218164896, 0.04351862098481877, 0.08067447347964984, 0.07106919752715565, 0.05721567980932406, 0.21129680773726134, 0.08502843214349193, 0.11680738558462743, 0.044020814087774536, 0.06644693304156447, 0.09585654552648758, 0.06233588238703717, 0.10576884296663785, 0.1482759257682792, 0.16444027104602194, 0.12275051868203296, 0.05593818777378753, 0.03214779695819712, 0.06958781368009087, 0.11346190034148575, 0.09187610230541844, 0.0382753692674299, 0.1081639942630387, 0.11955479523341665, 0.06897723966630688, 0.060323751773306594, 0.13439528639242154, 0.10597556243422722, 0.09429526734670504]
0.09267941366676778
Making ranges
torch.Size([30311, 2])
We keep 5.25e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([2016, 2])
We keep 5.29e+04/6.18e+05 =  8% of the original kernel matrix.

torch.Size([9265, 2])
We keep 5.96e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([27582, 2])
We keep 6.05e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([29639, 2])
We keep 5.33e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([5607, 2])
We keep 3.06e+05/6.08e+06 =  5% of the original kernel matrix.

torch.Size([13875, 2])
We keep 1.29e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([5618, 2])
We keep 5.44e+05/6.48e+06 =  8% of the original kernel matrix.

torch.Size([13777, 2])
We keep 1.31e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([29031, 2])
We keep 6.62e+06/3.22e+08 =  2% of the original kernel matrix.

torch.Size([31481, 2])
We keep 5.85e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([40493, 2])
We keep 1.09e+07/6.12e+08 =  1% of the original kernel matrix.

torch.Size([36383, 2])
We keep 7.46e+06/4.47e+08 =  1% of the original kernel matrix.

torch.Size([77706, 2])
We keep 3.29e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([49158, 2])
We keep 1.28e+07/8.56e+08 =  1% of the original kernel matrix.

torch.Size([54637, 2])
We keep 1.63e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([41825, 2])
We keep 9.36e+06/5.90e+08 =  1% of the original kernel matrix.

torch.Size([22065, 2])
We keep 1.82e+07/2.23e+08 =  8% of the original kernel matrix.

torch.Size([26695, 2])
We keep 5.15e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([37764, 2])
We keep 1.41e+08/1.27e+09 = 11% of the original kernel matrix.

torch.Size([34379, 2])
We keep 1.01e+07/6.44e+08 =  1% of the original kernel matrix.

torch.Size([29904, 2])
We keep 7.84e+06/3.51e+08 =  2% of the original kernel matrix.

torch.Size([31610, 2])
We keep 5.98e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([56192, 2])
We keep 2.89e+07/1.33e+09 =  2% of the original kernel matrix.

torch.Size([41901, 2])
We keep 1.03e+07/6.59e+08 =  1% of the original kernel matrix.

torch.Size([8728, 2])
We keep 6.20e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([16747, 2])
We keep 1.78e+06/7.01e+07 =  2% of the original kernel matrix.

torch.Size([266931, 2])
We keep 3.26e+08/2.76e+10 =  1% of the original kernel matrix.

torch.Size([96135, 2])
We keep 3.80e+07/3.00e+09 =  1% of the original kernel matrix.

torch.Size([27682, 2])
We keep 5.25e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([30296, 2])
We keep 5.37e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([35675, 2])
We keep 9.17e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([34467, 2])
We keep 6.69e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([65879, 2])
We keep 2.47e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([45512, 2])
We keep 1.09e+07/7.10e+08 =  1% of the original kernel matrix.

torch.Size([64746, 2])
We keep 3.24e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([45134, 2])
We keep 1.15e+07/7.41e+08 =  1% of the original kernel matrix.

torch.Size([147038, 2])
We keep 1.22e+08/8.29e+09 =  1% of the original kernel matrix.

torch.Size([69569, 2])
We keep 2.26e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([181466, 2])
We keep 1.62e+08/1.33e+10 =  1% of the original kernel matrix.

torch.Size([78092, 2])
We keep 2.78e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([18226, 2])
We keep 3.04e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([24298, 2])
We keep 3.82e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([210076, 2])
We keep 1.86e+08/1.78e+10 =  1% of the original kernel matrix.

torch.Size([84922, 2])
We keep 3.05e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([16839, 2])
We keep 2.33e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([23285, 2])
We keep 3.30e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([22303, 2])
We keep 6.31e+06/1.79e+08 =  3% of the original kernel matrix.

torch.Size([27197, 2])
We keep 4.47e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([6556, 2])
We keep 1.28e+06/1.75e+07 =  7% of the original kernel matrix.

torch.Size([14377, 2])
We keep 1.93e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([79934, 2])
We keep 3.31e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([50142, 2])
We keep 1.28e+07/8.61e+08 =  1% of the original kernel matrix.

torch.Size([79120, 2])
We keep 3.65e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([49838, 2])
We keep 1.29e+07/8.59e+08 =  1% of the original kernel matrix.

torch.Size([14357, 2])
We keep 1.71e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([21414, 2])
We keep 2.92e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([224743, 2])
We keep 4.41e+08/3.25e+10 =  1% of the original kernel matrix.

torch.Size([85525, 2])
We keep 4.10e+07/3.26e+09 =  1% of the original kernel matrix.

torch.Size([1091740, 2])
We keep 2.64e+09/3.46e+11 =  0% of the original kernel matrix.

torch.Size([198289, 2])
We keep 1.19e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([12676, 2])
We keep 1.67e+06/4.45e+07 =  3% of the original kernel matrix.

torch.Size([20094, 2])
We keep 2.64e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([1742465, 2])
We keep 7.31e+09/9.20e+11 =  0% of the original kernel matrix.

torch.Size([259526, 2])
We keep 1.88e+08/1.73e+10 =  1% of the original kernel matrix.

torch.Size([12628, 2])
We keep 1.71e+06/4.63e+07 =  3% of the original kernel matrix.

torch.Size([20078, 2])
We keep 2.72e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([19926, 2])
We keep 4.16e+06/1.35e+08 =  3% of the original kernel matrix.

torch.Size([25601, 2])
We keep 4.04e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([12951, 2])
We keep 1.63e+06/4.67e+07 =  3% of the original kernel matrix.

torch.Size([20271, 2])
We keep 2.73e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([532471, 2])
We keep 1.09e+09/1.04e+11 =  1% of the original kernel matrix.

torch.Size([135681, 2])
We keep 6.90e+07/5.81e+09 =  1% of the original kernel matrix.

torch.Size([98733, 2])
We keep 6.58e+07/3.59e+09 =  1% of the original kernel matrix.

torch.Size([56297, 2])
We keep 1.58e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([3251, 2])
We keep 1.01e+05/1.55e+06 =  6% of the original kernel matrix.

torch.Size([11181, 2])
We keep 8.05e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([8453, 2])
We keep 5.66e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([16462, 2])
We keep 1.74e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([5878, 2])
We keep 3.30e+05/6.53e+06 =  5% of the original kernel matrix.

torch.Size([14078, 2])
We keep 1.32e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([5505, 2])
We keep 2.95e+05/6.17e+06 =  4% of the original kernel matrix.

torch.Size([13655, 2])
We keep 1.28e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([2351, 2])
We keep 3.27e+05/1.73e+06 = 18% of the original kernel matrix.

torch.Size([9221, 2])
We keep 8.34e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([2410, 2])
We keep 6.26e+04/8.17e+05 =  7% of the original kernel matrix.

torch.Size([9800, 2])
We keep 6.46e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([5198, 2])
We keep 2.92e+05/5.09e+06 =  5% of the original kernel matrix.

torch.Size([13266, 2])
We keep 1.20e+06/4.08e+07 =  2% of the original kernel matrix.

torch.Size([1438, 2])
We keep 2.79e+04/2.84e+05 =  9% of the original kernel matrix.

torch.Size([8146, 2])
We keep 4.52e+05/9.63e+06 =  4% of the original kernel matrix.

torch.Size([3282, 2])
We keep 1.02e+05/1.60e+06 =  6% of the original kernel matrix.

torch.Size([11090, 2])
We keep 7.96e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([9671, 2])
We keep 7.42e+05/1.79e+07 =  4% of the original kernel matrix.

torch.Size([17534, 2])
We keep 1.89e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([9397, 2])
We keep 7.47e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([17247, 2])
We keep 1.97e+06/8.05e+07 =  2% of the original kernel matrix.

torch.Size([3245, 2])
We keep 1.02e+05/1.63e+06 =  6% of the original kernel matrix.

torch.Size([11135, 2])
We keep 8.15e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([9409, 2])
We keep 2.17e+06/2.71e+07 =  8% of the original kernel matrix.

torch.Size([17220, 2])
We keep 2.22e+06/9.40e+07 =  2% of the original kernel matrix.

torch.Size([20015, 2])
We keep 3.17e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([25604, 2])
We keep 4.08e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([3450, 2])
We keep 1.25e+05/2.07e+06 =  6% of the original kernel matrix.

torch.Size([11252, 2])
We keep 8.77e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([11773, 2])
We keep 1.36e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([19279, 2])
We keep 2.48e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([7300, 2])
We keep 6.90e+05/1.29e+07 =  5% of the original kernel matrix.

torch.Size([15305, 2])
We keep 1.66e+06/6.48e+07 =  2% of the original kernel matrix.

torch.Size([10747, 2])
We keep 9.86e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([18345, 2])
We keep 2.23e+06/9.57e+07 =  2% of the original kernel matrix.

torch.Size([7031, 2])
We keep 6.00e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([15039, 2])
We keep 1.56e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([4374, 2])
We keep 1.67e+05/3.07e+06 =  5% of the original kernel matrix.

torch.Size([12519, 2])
We keep 9.91e+05/3.16e+07 =  3% of the original kernel matrix.

torch.Size([5270, 2])
We keep 2.37e+05/4.56e+06 =  5% of the original kernel matrix.

torch.Size([13462, 2])
We keep 1.16e+06/3.86e+07 =  2% of the original kernel matrix.

torch.Size([5493, 2])
We keep 2.75e+05/5.48e+06 =  5% of the original kernel matrix.

torch.Size([13655, 2])
We keep 1.23e+06/4.23e+07 =  2% of the original kernel matrix.

torch.Size([5809, 2])
We keep 3.32e+05/6.57e+06 =  5% of the original kernel matrix.

torch.Size([13865, 2])
We keep 1.32e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([11700, 2])
We keep 1.11e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([19269, 2])
We keep 2.39e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([3281, 2])
We keep 1.03e+05/1.66e+06 =  6% of the original kernel matrix.

torch.Size([11186, 2])
We keep 8.07e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([10918, 2])
We keep 1.09e+06/3.02e+07 =  3% of the original kernel matrix.

torch.Size([18642, 2])
We keep 2.33e+06/9.93e+07 =  2% of the original kernel matrix.

torch.Size([13252, 2])
We keep 1.44e+06/4.56e+07 =  3% of the original kernel matrix.

torch.Size([20443, 2])
We keep 2.68e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([5317, 2])
We keep 2.57e+05/5.04e+06 =  5% of the original kernel matrix.

torch.Size([13529, 2])
We keep 1.18e+06/4.06e+07 =  2% of the original kernel matrix.

torch.Size([4469, 2])
We keep 1.74e+05/3.15e+06 =  5% of the original kernel matrix.

torch.Size([12577, 2])
We keep 1.02e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([5488, 2])
We keep 2.88e+05/5.72e+06 =  5% of the original kernel matrix.

torch.Size([13613, 2])
We keep 1.25e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([4741, 2])
We keep 2.52e+05/4.08e+06 =  6% of the original kernel matrix.

torch.Size([12728, 2])
We keep 1.11e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([7030, 2])
We keep 4.13e+05/9.14e+06 =  4% of the original kernel matrix.

torch.Size([15230, 2])
We keep 1.47e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([4499, 2])
We keep 2.15e+05/3.61e+06 =  5% of the original kernel matrix.

torch.Size([12567, 2])
We keep 1.06e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([3574, 2])
We keep 1.64e+05/2.40e+06 =  6% of the original kernel matrix.

torch.Size([11295, 2])
We keep 9.22e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([3957, 2])
We keep 2.19e+05/2.85e+06 =  7% of the original kernel matrix.

torch.Size([11865, 2])
We keep 9.97e+05/3.05e+07 =  3% of the original kernel matrix.

torch.Size([3579, 2])
We keep 1.29e+05/1.97e+06 =  6% of the original kernel matrix.

torch.Size([11558, 2])
We keep 8.65e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([3798, 2])
We keep 1.74e+05/2.77e+06 =  6% of the original kernel matrix.

torch.Size([11644, 2])
We keep 9.63e+05/3.00e+07 =  3% of the original kernel matrix.

torch.Size([5995, 2])
We keep 3.53e+05/6.97e+06 =  5% of the original kernel matrix.

torch.Size([14112, 2])
We keep 1.34e+06/4.77e+07 =  2% of the original kernel matrix.

torch.Size([21871, 2])
We keep 4.47e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([26913, 2])
We keep 4.43e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([3853, 2])
We keep 1.66e+05/2.67e+06 =  6% of the original kernel matrix.

torch.Size([11793, 2])
We keep 9.64e+05/2.95e+07 =  3% of the original kernel matrix.

torch.Size([1967, 2])
We keep 3.63e+04/4.65e+05 =  7% of the original kernel matrix.

torch.Size([9337, 2])
We keep 5.31e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([4577, 2])
We keep 2.13e+05/3.92e+06 =  5% of the original kernel matrix.

torch.Size([12603, 2])
We keep 1.09e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([13883, 2])
We keep 1.65e+06/5.45e+07 =  3% of the original kernel matrix.

torch.Size([20954, 2])
We keep 2.90e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([4736, 2])
We keep 2.20e+05/4.04e+06 =  5% of the original kernel matrix.

torch.Size([12842, 2])
We keep 1.12e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([1865, 2])
We keep 6.47e+04/6.45e+05 = 10% of the original kernel matrix.

torch.Size([8969, 2])
We keep 6.01e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([5528, 2])
We keep 3.71e+05/6.20e+06 =  5% of the original kernel matrix.

torch.Size([13706, 2])
We keep 1.29e+06/4.50e+07 =  2% of the original kernel matrix.

torch.Size([2168, 2])
We keep 5.60e+04/7.11e+05 =  7% of the original kernel matrix.

torch.Size([9492, 2])
We keep 6.09e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([4661, 2])
We keep 2.25e+05/4.06e+06 =  5% of the original kernel matrix.

torch.Size([12748, 2])
We keep 1.11e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([3267, 2])
We keep 9.30e+04/1.49e+06 =  6% of the original kernel matrix.

torch.Size([11167, 2])
We keep 7.78e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([6142, 2])
We keep 3.46e+05/6.44e+06 =  5% of the original kernel matrix.

torch.Size([14320, 2])
We keep 1.31e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([3161, 2])
We keep 1.18e+05/1.79e+06 =  6% of the original kernel matrix.

torch.Size([10896, 2])
We keep 8.36e+05/2.42e+07 =  3% of the original kernel matrix.

torch.Size([5278, 2])
We keep 2.49e+05/4.97e+06 =  5% of the original kernel matrix.

torch.Size([13458, 2])
We keep 1.19e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([7470, 2])
We keep 4.86e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([15474, 2])
We keep 1.60e+06/6.13e+07 =  2% of the original kernel matrix.

torch.Size([4081, 2])
We keep 1.49e+05/2.65e+06 =  5% of the original kernel matrix.

torch.Size([12086, 2])
We keep 9.50e+05/2.94e+07 =  3% of the original kernel matrix.

torch.Size([3689, 2])
We keep 1.26e+05/2.08e+06 =  6% of the original kernel matrix.

torch.Size([11550, 2])
We keep 8.70e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([3137, 2])
We keep 1.05e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([10903, 2])
We keep 8.04e+05/2.36e+07 =  3% of the original kernel matrix.

torch.Size([15103, 2])
We keep 2.00e+06/6.61e+07 =  3% of the original kernel matrix.

torch.Size([22104, 2])
We keep 3.08e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([4972, 2])
We keep 2.48e+05/4.67e+06 =  5% of the original kernel matrix.

torch.Size([13012, 2])
We keep 1.15e+06/3.90e+07 =  2% of the original kernel matrix.

torch.Size([9139, 2])
We keep 7.08e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([17189, 2])
We keep 1.85e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([7281, 2])
We keep 4.58e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([15335, 2])
We keep 1.56e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([5191, 2])
We keep 6.38e+05/7.06e+06 =  9% of the original kernel matrix.

torch.Size([13147, 2])
We keep 1.36e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([5433, 2])
We keep 2.77e+05/5.66e+06 =  4% of the original kernel matrix.

torch.Size([13526, 2])
We keep 1.25e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([7184, 2])
We keep 4.97e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([15223, 2])
We keep 1.57e+06/6.08e+07 =  2% of the original kernel matrix.

torch.Size([20437, 2])
We keep 3.25e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([25888, 2])
We keep 4.19e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([6999, 2])
We keep 4.80e+05/9.47e+06 =  5% of the original kernel matrix.

torch.Size([15150, 2])
We keep 1.50e+06/5.56e+07 =  2% of the original kernel matrix.

torch.Size([8504, 2])
We keep 5.83e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([16541, 2])
We keep 1.74e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([5812, 2])
We keep 3.74e+05/7.05e+06 =  5% of the original kernel matrix.

torch.Size([13882, 2])
We keep 1.35e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([9500, 2])
We keep 1.20e+06/2.75e+07 =  4% of the original kernel matrix.

torch.Size([17204, 2])
We keep 2.21e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([6383, 2])
We keep 3.78e+05/7.81e+06 =  4% of the original kernel matrix.

torch.Size([14465, 2])
We keep 1.39e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([8205, 2])
We keep 5.27e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([16224, 2])
We keep 1.65e+06/6.37e+07 =  2% of the original kernel matrix.

torch.Size([4112, 2])
We keep 2.55e+05/3.93e+06 =  6% of the original kernel matrix.

torch.Size([11915, 2])
We keep 1.08e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([4816, 2])
We keep 2.37e+05/4.06e+06 =  5% of the original kernel matrix.

torch.Size([12935, 2])
We keep 1.11e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([2161, 2])
We keep 5.33e+04/6.89e+05 =  7% of the original kernel matrix.

torch.Size([9565, 2])
We keep 5.96e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([6823, 2])
We keep 4.76e+05/9.54e+06 =  4% of the original kernel matrix.

torch.Size([14945, 2])
We keep 1.50e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([3184, 2])
We keep 1.19e+05/1.68e+06 =  7% of the original kernel matrix.

torch.Size([10890, 2])
We keep 8.27e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([2448, 2])
We keep 6.31e+04/8.59e+05 =  7% of the original kernel matrix.

torch.Size([9903, 2])
We keep 6.44e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([3630, 2])
We keep 1.32e+05/2.07e+06 =  6% of the original kernel matrix.

torch.Size([11679, 2])
We keep 8.87e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([7017, 2])
We keep 5.50e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([15103, 2])
We keep 1.57e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([2971, 2])
We keep 1.22e+05/1.58e+06 =  7% of the original kernel matrix.

torch.Size([10643, 2])
We keep 8.05e+05/2.27e+07 =  3% of the original kernel matrix.

torch.Size([3180, 2])
We keep 1.09e+05/1.45e+06 =  7% of the original kernel matrix.

torch.Size([11081, 2])
We keep 7.82e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([2596, 2])
We keep 6.63e+04/9.01e+05 =  7% of the original kernel matrix.

torch.Size([10204, 2])
We keep 6.60e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([12829, 2])
We keep 1.51e+06/4.68e+07 =  3% of the original kernel matrix.

torch.Size([20149, 2])
We keep 2.71e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([2669, 2])
We keep 8.85e+04/1.18e+06 =  7% of the original kernel matrix.

torch.Size([10202, 2])
We keep 7.31e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([13363, 2])
We keep 1.38e+06/4.55e+07 =  3% of the original kernel matrix.

torch.Size([20559, 2])
We keep 2.67e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([4675, 2])
We keep 2.27e+05/4.15e+06 =  5% of the original kernel matrix.

torch.Size([12712, 2])
We keep 1.12e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([3926, 2])
We keep 2.19e+05/3.52e+06 =  6% of the original kernel matrix.

torch.Size([11597, 2])
We keep 1.06e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([7558, 2])
We keep 5.96e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([15754, 2])
We keep 1.67e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([6661, 2])
We keep 4.70e+05/9.38e+06 =  5% of the original kernel matrix.

torch.Size([14662, 2])
We keep 1.48e+06/5.53e+07 =  2% of the original kernel matrix.

torch.Size([3196, 2])
We keep 1.16e+05/1.77e+06 =  6% of the original kernel matrix.

torch.Size([10917, 2])
We keep 8.32e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([5526, 2])
We keep 2.72e+05/5.04e+06 =  5% of the original kernel matrix.

torch.Size([13597, 2])
We keep 1.19e+06/4.06e+07 =  2% of the original kernel matrix.

torch.Size([3787, 2])
We keep 1.34e+05/2.33e+06 =  5% of the original kernel matrix.

torch.Size([11803, 2])
We keep 9.10e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([15818, 2])
We keep 1.97e+06/6.61e+07 =  2% of the original kernel matrix.

torch.Size([22622, 2])
We keep 3.09e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([5728, 2])
We keep 2.91e+05/6.03e+06 =  4% of the original kernel matrix.

torch.Size([13916, 2])
We keep 1.26e+06/4.44e+07 =  2% of the original kernel matrix.

torch.Size([7784, 2])
We keep 5.78e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([15791, 2])
We keep 1.68e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([2903, 2])
We keep 1.32e+05/1.41e+06 =  9% of the original kernel matrix.

torch.Size([10512, 2])
We keep 7.61e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([3825, 2])
We keep 1.78e+05/2.65e+06 =  6% of the original kernel matrix.

torch.Size([11761, 2])
We keep 9.44e+05/2.94e+07 =  3% of the original kernel matrix.

torch.Size([3410, 2])
We keep 1.79e+05/2.10e+06 =  8% of the original kernel matrix.

torch.Size([11239, 2])
We keep 8.93e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([4481, 2])
We keep 1.93e+05/3.26e+06 =  5% of the original kernel matrix.

torch.Size([12648, 2])
We keep 1.03e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([4106, 2])
We keep 1.68e+05/2.96e+06 =  5% of the original kernel matrix.

torch.Size([12131, 2])
We keep 9.91e+05/3.11e+07 =  3% of the original kernel matrix.

torch.Size([2404, 2])
We keep 7.03e+04/8.19e+05 =  8% of the original kernel matrix.

torch.Size([9827, 2])
We keep 6.43e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([4269, 2])
We keep 1.78e+05/3.17e+06 =  5% of the original kernel matrix.

torch.Size([12370, 2])
We keep 1.03e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([3413, 2])
We keep 1.25e+05/2.02e+06 =  6% of the original kernel matrix.

torch.Size([11305, 2])
We keep 8.61e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([3532, 2])
We keep 1.26e+05/2.01e+06 =  6% of the original kernel matrix.

torch.Size([11553, 2])
We keep 8.78e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([6247, 2])
We keep 3.54e+05/6.89e+06 =  5% of the original kernel matrix.

torch.Size([14492, 2])
We keep 1.35e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([5035, 2])
We keep 3.18e+05/5.24e+06 =  6% of the original kernel matrix.

torch.Size([13082, 2])
We keep 1.21e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([13925, 2])
We keep 1.83e+06/6.13e+07 =  2% of the original kernel matrix.

torch.Size([21117, 2])
We keep 3.01e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([17107, 2])
We keep 2.18e+06/8.63e+07 =  2% of the original kernel matrix.

torch.Size([23662, 2])
We keep 3.44e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([8177, 2])
We keep 5.66e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([16366, 2])
We keep 1.71e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([2768, 2])
We keep 8.24e+04/1.21e+06 =  6% of the original kernel matrix.

torch.Size([10431, 2])
We keep 7.29e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([3641, 2])
We keep 1.39e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([11571, 2])
We keep 9.17e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([2940, 2])
We keep 1.38e+05/1.68e+06 =  8% of the original kernel matrix.

torch.Size([10472, 2])
We keep 8.11e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([3574, 2])
We keep 1.35e+05/2.02e+06 =  6% of the original kernel matrix.

torch.Size([11521, 2])
We keep 8.66e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([3151, 2])
We keep 1.17e+05/1.81e+06 =  6% of the original kernel matrix.

torch.Size([11006, 2])
We keep 8.43e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([3915, 2])
We keep 1.87e+05/2.96e+06 =  6% of the original kernel matrix.

torch.Size([11835, 2])
We keep 1.00e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([11743, 2])
We keep 1.06e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([19331, 2])
We keep 2.34e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([5926, 2])
We keep 3.35e+05/6.42e+06 =  5% of the original kernel matrix.

torch.Size([14140, 2])
We keep 1.30e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([8170, 2])
We keep 1.81e+06/1.95e+07 =  9% of the original kernel matrix.

torch.Size([16126, 2])
We keep 1.99e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([4556, 2])
We keep 2.05e+05/3.96e+06 =  5% of the original kernel matrix.

torch.Size([12637, 2])
We keep 1.09e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([13680, 2])
We keep 2.45e+06/5.51e+07 =  4% of the original kernel matrix.

torch.Size([20847, 2])
We keep 2.91e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([6194, 2])
We keep 3.18e+05/6.37e+06 =  4% of the original kernel matrix.

torch.Size([14422, 2])
We keep 1.30e+06/4.56e+07 =  2% of the original kernel matrix.

torch.Size([5933, 2])
We keep 3.23e+05/6.45e+06 =  5% of the original kernel matrix.

torch.Size([14130, 2])
We keep 1.31e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([8237, 2])
We keep 5.60e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([16243, 2])
We keep 1.72e+06/6.68e+07 =  2% of the original kernel matrix.

torch.Size([1099, 2])
We keep 1.71e+04/1.38e+05 = 12% of the original kernel matrix.

torch.Size([7436, 2])
We keep 3.65e+05/6.72e+06 =  5% of the original kernel matrix.

torch.Size([6271, 2])
We keep 4.72e+05/9.38e+06 =  5% of the original kernel matrix.

torch.Size([14429, 2])
We keep 1.49e+06/5.53e+07 =  2% of the original kernel matrix.

torch.Size([2674, 2])
We keep 1.03e+05/1.16e+06 =  8% of the original kernel matrix.

torch.Size([10248, 2])
We keep 7.27e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([12194, 2])
We keep 1.52e+06/3.98e+07 =  3% of the original kernel matrix.

torch.Size([19637, 2])
We keep 2.53e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([2222, 2])
We keep 5.01e+04/6.37e+05 =  7% of the original kernel matrix.

torch.Size([9673, 2])
We keep 5.89e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([5656, 2])
We keep 3.31e+05/6.46e+06 =  5% of the original kernel matrix.

torch.Size([13854, 2])
We keep 1.29e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([5607, 2])
We keep 3.65e+05/6.42e+06 =  5% of the original kernel matrix.

torch.Size([13684, 2])
We keep 1.32e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([4484, 2])
We keep 2.30e+05/3.97e+06 =  5% of the original kernel matrix.

torch.Size([12567, 2])
We keep 1.09e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([4346, 2])
We keep 1.91e+05/3.12e+06 =  6% of the original kernel matrix.

torch.Size([12483, 2])
We keep 1.02e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([9347, 2])
We keep 8.02e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([17157, 2])
We keep 1.96e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([1587, 2])
We keep 3.08e+04/3.46e+05 =  8% of the original kernel matrix.

torch.Size([8511, 2])
We keep 4.81e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([3515, 2])
We keep 1.70e+05/2.57e+06 =  6% of the original kernel matrix.

torch.Size([11355, 2])
We keep 9.52e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([5781, 2])
We keep 3.47e+05/6.42e+06 =  5% of the original kernel matrix.

torch.Size([14006, 2])
We keep 1.30e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([8154, 2])
We keep 5.58e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([16261, 2])
We keep 1.65e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([2377, 2])
We keep 9.53e+04/9.53e+05 = 10% of the original kernel matrix.

torch.Size([9890, 2])
We keep 6.74e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([8959, 2])
We keep 8.19e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([16889, 2])
We keep 1.93e+06/7.92e+07 =  2% of the original kernel matrix.

torch.Size([5355, 2])
We keep 2.68e+05/5.32e+06 =  5% of the original kernel matrix.

torch.Size([13541, 2])
We keep 1.21e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([7199, 2])
We keep 4.21e+05/9.44e+06 =  4% of the original kernel matrix.

torch.Size([15294, 2])
We keep 1.48e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([6435, 2])
We keep 4.03e+05/8.32e+06 =  4% of the original kernel matrix.

torch.Size([14452, 2])
We keep 1.43e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([5912, 2])
We keep 5.32e+05/6.72e+06 =  7% of the original kernel matrix.

torch.Size([14100, 2])
We keep 1.34e+06/4.68e+07 =  2% of the original kernel matrix.

torch.Size([4747, 2])
We keep 2.20e+05/4.04e+06 =  5% of the original kernel matrix.

torch.Size([12868, 2])
We keep 1.11e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([8716, 2])
We keep 7.31e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([16787, 2])
We keep 1.86e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([4600, 2])
We keep 1.87e+05/3.26e+06 =  5% of the original kernel matrix.

torch.Size([12834, 2])
We keep 1.02e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([4371, 2])
We keep 3.91e+05/3.98e+06 =  9% of the original kernel matrix.

torch.Size([12321, 2])
We keep 1.12e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([4027, 2])
We keep 1.70e+05/2.85e+06 =  5% of the original kernel matrix.

torch.Size([12004, 2])
We keep 9.71e+05/3.05e+07 =  3% of the original kernel matrix.

torch.Size([6745, 2])
We keep 3.98e+05/8.40e+06 =  4% of the original kernel matrix.

torch.Size([14807, 2])
We keep 1.43e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([9439, 2])
We keep 7.37e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([17411, 2])
We keep 1.93e+06/7.92e+07 =  2% of the original kernel matrix.

torch.Size([4692, 2])
We keep 2.24e+05/4.22e+06 =  5% of the original kernel matrix.

torch.Size([12652, 2])
We keep 1.12e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([6551, 2])
We keep 4.11e+05/8.05e+06 =  5% of the original kernel matrix.

torch.Size([14705, 2])
We keep 1.41e+06/5.13e+07 =  2% of the original kernel matrix.

torch.Size([10743, 2])
We keep 8.64e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([18445, 2])
We keep 2.10e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([15256, 2])
We keep 1.85e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([22127, 2])
We keep 3.10e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([3071, 2])
We keep 1.01e+05/1.61e+06 =  6% of the original kernel matrix.

torch.Size([10853, 2])
We keep 8.00e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([1577, 2])
We keep 2.55e+04/2.77e+05 =  9% of the original kernel matrix.

torch.Size([8390, 2])
We keep 4.42e+05/9.50e+06 =  4% of the original kernel matrix.

torch.Size([5201, 2])
We keep 2.68e+05/4.92e+06 =  5% of the original kernel matrix.

torch.Size([13385, 2])
We keep 1.19e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([5928, 2])
We keep 3.26e+05/6.51e+06 =  4% of the original kernel matrix.

torch.Size([14129, 2])
We keep 1.31e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([4067, 2])
We keep 3.60e+05/3.43e+06 = 10% of the original kernel matrix.

torch.Size([12002, 2])
We keep 1.04e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([6472, 2])
We keep 3.68e+05/7.90e+06 =  4% of the original kernel matrix.

torch.Size([14623, 2])
We keep 1.40e+06/5.08e+07 =  2% of the original kernel matrix.

torch.Size([6895, 2])
We keep 5.00e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([14878, 2])
We keep 1.55e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([2971, 2])
We keep 9.64e+04/1.31e+06 =  7% of the original kernel matrix.

torch.Size([10794, 2])
We keep 7.55e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([8071, 2])
We keep 7.28e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([15986, 2])
We keep 1.82e+06/7.24e+07 =  2% of the original kernel matrix.

torch.Size([2550, 2])
We keep 7.63e+04/1.08e+06 =  7% of the original kernel matrix.

torch.Size([10143, 2])
We keep 6.95e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([7454, 2])
We keep 4.36e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([15559, 2])
We keep 1.55e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([4053, 2])
We keep 1.56e+05/2.63e+06 =  5% of the original kernel matrix.

torch.Size([12049, 2])
We keep 9.44e+05/2.93e+07 =  3% of the original kernel matrix.

torch.Size([12238, 2])
We keep 1.09e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([19644, 2])
We keep 2.39e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([9420, 2])
We keep 7.23e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([17259, 2])
We keep 1.92e+06/7.87e+07 =  2% of the original kernel matrix.

torch.Size([11302, 2])
We keep 1.14e+06/3.02e+07 =  3% of the original kernel matrix.

torch.Size([18907, 2])
We keep 2.26e+06/9.92e+07 =  2% of the original kernel matrix.

torch.Size([2934, 2])
We keep 8.92e+04/1.36e+06 =  6% of the original kernel matrix.

torch.Size([10697, 2])
We keep 7.51e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([7277, 2])
We keep 4.86e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([15656, 2])
We keep 1.57e+06/5.81e+07 =  2% of the original kernel matrix.

torch.Size([6152, 2])
We keep 3.62e+05/7.51e+06 =  4% of the original kernel matrix.

torch.Size([14267, 2])
We keep 1.37e+06/4.95e+07 =  2% of the original kernel matrix.

torch.Size([5448, 2])
We keep 2.82e+05/5.27e+06 =  5% of the original kernel matrix.

torch.Size([13660, 2])
We keep 1.21e+06/4.15e+07 =  2% of the original kernel matrix.

torch.Size([12632, 2])
We keep 1.26e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([20070, 2])
We keep 2.54e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([5433, 2])
We keep 2.97e+05/5.78e+06 =  5% of the original kernel matrix.

torch.Size([13539, 2])
We keep 1.24e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([4684, 2])
We keep 5.13e+05/5.45e+06 =  9% of the original kernel matrix.

torch.Size([12628, 2])
We keep 1.24e+06/4.22e+07 =  2% of the original kernel matrix.

torch.Size([4642, 2])
We keep 2.10e+05/3.77e+06 =  5% of the original kernel matrix.

torch.Size([12715, 2])
We keep 1.08e+06/3.51e+07 =  3% of the original kernel matrix.

torch.Size([4442, 2])
We keep 2.01e+05/3.59e+06 =  5% of the original kernel matrix.

torch.Size([12480, 2])
We keep 1.06e+06/3.42e+07 =  3% of the original kernel matrix.

torch.Size([1589, 2])
We keep 3.12e+04/3.45e+05 =  9% of the original kernel matrix.

torch.Size([8510, 2])
We keep 4.89e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([1547, 2])
We keep 5.33e+04/4.08e+05 = 13% of the original kernel matrix.

torch.Size([8347, 2])
We keep 5.21e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([1963, 2])
We keep 4.13e+04/4.83e+05 =  8% of the original kernel matrix.

torch.Size([9256, 2])
We keep 5.47e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([2422, 2])
We keep 8.36e+04/1.00e+06 =  8% of the original kernel matrix.

torch.Size([9745, 2])
We keep 6.95e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([15976, 2])
We keep 3.50e+06/9.65e+07 =  3% of the original kernel matrix.

torch.Size([22560, 2])
We keep 3.55e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([3790, 2])
We keep 1.37e+05/2.35e+06 =  5% of the original kernel matrix.

torch.Size([11767, 2])
We keep 9.12e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([2588, 2])
We keep 1.51e+05/1.35e+06 = 11% of the original kernel matrix.

torch.Size([9913, 2])
We keep 7.57e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([2533, 2])
We keep 6.53e+04/8.63e+05 =  7% of the original kernel matrix.

torch.Size([10177, 2])
We keep 6.50e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([6975, 2])
We keep 4.83e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([15155, 2])
We keep 1.52e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([2675, 2])
We keep 2.06e+05/1.61e+06 = 12% of the original kernel matrix.

torch.Size([10140, 2])
We keep 8.14e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([8032, 2])
We keep 6.41e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([16051, 2])
We keep 1.77e+06/7.00e+07 =  2% of the original kernel matrix.

torch.Size([4250, 2])
We keep 1.77e+05/3.21e+06 =  5% of the original kernel matrix.

torch.Size([12367, 2])
We keep 1.02e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([3115, 2])
We keep 1.13e+05/1.86e+06 =  6% of the original kernel matrix.

torch.Size([10764, 2])
We keep 8.44e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([2379, 2])
We keep 7.44e+04/8.97e+05 =  8% of the original kernel matrix.

torch.Size([9870, 2])
We keep 6.70e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([5407, 2])
We keep 3.00e+05/6.12e+06 =  4% of the original kernel matrix.

torch.Size([13522, 2])
We keep 1.29e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([1575, 2])
We keep 2.48e+04/2.71e+05 =  9% of the original kernel matrix.

torch.Size([8451, 2])
We keep 4.37e+05/9.41e+06 =  4% of the original kernel matrix.

torch.Size([2546, 2])
We keep 7.45e+04/1.01e+06 =  7% of the original kernel matrix.

torch.Size([10182, 2])
We keep 6.87e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([3734, 2])
We keep 1.52e+05/2.18e+06 =  6% of the original kernel matrix.

torch.Size([11675, 2])
We keep 8.94e+05/2.67e+07 =  3% of the original kernel matrix.

torch.Size([6324, 2])
We keep 3.73e+05/7.35e+06 =  5% of the original kernel matrix.

torch.Size([14468, 2])
We keep 1.38e+06/4.90e+07 =  2% of the original kernel matrix.

torch.Size([6647, 2])
We keep 4.10e+05/9.14e+06 =  4% of the original kernel matrix.

torch.Size([14842, 2])
We keep 1.48e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([6449, 2])
We keep 3.86e+05/7.94e+06 =  4% of the original kernel matrix.

torch.Size([14604, 2])
We keep 1.40e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([3126, 2])
We keep 1.30e+05/1.83e+06 =  7% of the original kernel matrix.

torch.Size([10858, 2])
We keep 8.37e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([5863, 2])
We keep 3.15e+05/6.57e+06 =  4% of the original kernel matrix.

torch.Size([14063, 2])
We keep 1.32e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([9867, 2])
We keep 8.97e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([17715, 2])
We keep 2.03e+06/8.57e+07 =  2% of the original kernel matrix.

torch.Size([2054, 2])
We keep 4.50e+04/5.69e+05 =  7% of the original kernel matrix.

torch.Size([9395, 2])
We keep 5.64e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([13385, 2])
We keep 2.32e+06/5.06e+07 =  4% of the original kernel matrix.

torch.Size([20695, 2])
We keep 2.78e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([27776, 2])
We keep 4.10e+07/4.80e+08 =  8% of the original kernel matrix.

torch.Size([29602, 2])
We keep 6.72e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([7586, 2])
We keep 1.16e+06/1.57e+07 =  7% of the original kernel matrix.

torch.Size([15828, 2])
We keep 1.82e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([8112, 2])
We keep 5.72e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([16268, 2])
We keep 1.69e+06/6.60e+07 =  2% of the original kernel matrix.

torch.Size([2759, 2])
We keep 1.28e+05/1.70e+06 =  7% of the original kernel matrix.

torch.Size([10137, 2])
We keep 8.13e+05/2.36e+07 =  3% of the original kernel matrix.

torch.Size([7811, 2])
We keep 6.99e+05/1.21e+07 =  5% of the original kernel matrix.

torch.Size([15917, 2])
We keep 1.65e+06/6.29e+07 =  2% of the original kernel matrix.

torch.Size([10530, 2])
We keep 1.07e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([18279, 2])
We keep 2.18e+06/9.19e+07 =  2% of the original kernel matrix.

torch.Size([54735, 2])
We keep 1.71e+08/2.12e+09 =  8% of the original kernel matrix.

torch.Size([41216, 2])
We keep 1.24e+07/8.32e+08 =  1% of the original kernel matrix.

torch.Size([6133, 2])
We keep 3.49e+05/7.02e+06 =  4% of the original kernel matrix.

torch.Size([14330, 2])
We keep 1.35e+06/4.78e+07 =  2% of the original kernel matrix.

torch.Size([139094, 2])
We keep 1.13e+08/8.03e+09 =  1% of the original kernel matrix.

torch.Size([67501, 2])
We keep 2.22e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([55402, 2])
We keep 3.66e+07/1.59e+09 =  2% of the original kernel matrix.

torch.Size([41458, 2])
We keep 1.12e+07/7.20e+08 =  1% of the original kernel matrix.

torch.Size([21993, 2])
We keep 6.68e+06/1.93e+08 =  3% of the original kernel matrix.

torch.Size([26753, 2])
We keep 4.78e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([41801, 2])
We keep 2.86e+07/6.92e+08 =  4% of the original kernel matrix.

torch.Size([36723, 2])
We keep 7.88e+06/4.75e+08 =  1% of the original kernel matrix.

torch.Size([3105, 2])
We keep 9.68e+04/1.50e+06 =  6% of the original kernel matrix.

torch.Size([10862, 2])
We keep 7.77e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([5309, 2])
We keep 3.37e+05/5.89e+06 =  5% of the original kernel matrix.

torch.Size([13365, 2])
We keep 1.28e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([545256, 2])
We keep 1.16e+09/1.07e+11 =  1% of the original kernel matrix.

torch.Size([136733, 2])
We keep 7.00e+07/5.91e+09 =  1% of the original kernel matrix.

torch.Size([321857, 2])
We keep 4.31e+08/3.71e+10 =  1% of the original kernel matrix.

torch.Size([105146, 2])
We keep 4.29e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([7914, 2])
We keep 7.38e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([16045, 2])
We keep 1.69e+06/6.61e+07 =  2% of the original kernel matrix.

torch.Size([36997, 2])
We keep 9.32e+06/5.24e+08 =  1% of the original kernel matrix.

torch.Size([34993, 2])
We keep 6.96e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([120627, 2])
We keep 3.49e+08/1.14e+10 =  3% of the original kernel matrix.

torch.Size([60505, 2])
We keep 2.59e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([26113, 2])
We keep 1.24e+07/2.60e+08 =  4% of the original kernel matrix.

torch.Size([29340, 2])
We keep 5.35e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([29158, 2])
We keep 1.88e+07/4.10e+08 =  4% of the original kernel matrix.

torch.Size([30679, 2])
We keep 6.48e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([25107, 2])
We keep 1.42e+07/3.96e+08 =  3% of the original kernel matrix.

torch.Size([27828, 2])
We keep 6.26e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([19915, 2])
We keep 7.52e+06/1.43e+08 =  5% of the original kernel matrix.

torch.Size([25422, 2])
We keep 4.14e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([18544, 2])
We keep 5.93e+06/1.26e+08 =  4% of the original kernel matrix.

torch.Size([24541, 2])
We keep 4.00e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([4643, 2])
We keep 2.50e+05/3.77e+06 =  6% of the original kernel matrix.

torch.Size([12790, 2])
We keep 1.09e+06/3.51e+07 =  3% of the original kernel matrix.

torch.Size([53265, 2])
We keep 1.83e+08/3.21e+09 =  5% of the original kernel matrix.

torch.Size([40255, 2])
We keep 1.50e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([8603, 2])
We keep 1.32e+06/1.99e+07 =  6% of the original kernel matrix.

torch.Size([16496, 2])
We keep 1.97e+06/8.07e+07 =  2% of the original kernel matrix.

torch.Size([7080, 2])
We keep 4.51e+05/1.00e+07 =  4% of the original kernel matrix.

torch.Size([15342, 2])
We keep 1.52e+06/5.72e+07 =  2% of the original kernel matrix.

torch.Size([24177, 2])
We keep 6.73e+06/2.10e+08 =  3% of the original kernel matrix.

torch.Size([28261, 2])
We keep 4.93e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([22363, 2])
We keep 1.09e+07/2.07e+08 =  5% of the original kernel matrix.

torch.Size([26671, 2])
We keep 4.84e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([217011, 2])
We keep 2.76e+08/2.04e+10 =  1% of the original kernel matrix.

torch.Size([86812, 2])
We keep 3.37e+07/2.58e+09 =  1% of the original kernel matrix.

torch.Size([50877, 2])
We keep 1.66e+07/9.16e+08 =  1% of the original kernel matrix.

torch.Size([40443, 2])
We keep 8.76e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([3230, 2])
We keep 1.07e+05/1.73e+06 =  6% of the original kernel matrix.

torch.Size([11056, 2])
We keep 8.20e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([28631, 2])
We keep 7.34e+06/3.25e+08 =  2% of the original kernel matrix.

torch.Size([31145, 2])
We keep 5.89e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([11754, 2])
We keep 1.27e+06/3.41e+07 =  3% of the original kernel matrix.

torch.Size([19322, 2])
We keep 2.41e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([18088, 2])
We keep 3.71e+06/1.15e+08 =  3% of the original kernel matrix.

torch.Size([24168, 2])
We keep 3.80e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([68737, 2])
We keep 2.98e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([46844, 2])
We keep 1.15e+07/7.64e+08 =  1% of the original kernel matrix.

torch.Size([36581, 2])
We keep 2.35e+07/7.77e+08 =  3% of the original kernel matrix.

torch.Size([34167, 2])
We keep 8.22e+06/5.03e+08 =  1% of the original kernel matrix.

torch.Size([74221, 2])
We keep 4.01e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([47931, 2])
We keep 1.24e+07/8.34e+08 =  1% of the original kernel matrix.

torch.Size([2374, 2])
We keep 5.66e+04/7.62e+05 =  7% of the original kernel matrix.

torch.Size([9840, 2])
We keep 6.27e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([9587, 2])
We keep 7.33e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([17499, 2])
We keep 1.91e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([70133, 2])
We keep 3.89e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([46517, 2])
We keep 1.23e+07/8.12e+08 =  1% of the original kernel matrix.

torch.Size([31536, 2])
We keep 1.12e+07/4.21e+08 =  2% of the original kernel matrix.

torch.Size([31838, 2])
We keep 6.39e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([30651, 2])
We keep 1.17e+07/3.69e+08 =  3% of the original kernel matrix.

torch.Size([31353, 2])
We keep 6.08e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([7082, 2])
We keep 4.90e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([15125, 2])
We keep 1.54e+06/5.87e+07 =  2% of the original kernel matrix.

torch.Size([235683, 2])
We keep 2.70e+08/2.79e+10 =  0% of the original kernel matrix.

torch.Size([89298, 2])
We keep 3.78e+07/3.02e+09 =  1% of the original kernel matrix.

torch.Size([16181, 2])
We keep 1.96e+06/7.76e+07 =  2% of the original kernel matrix.

torch.Size([22926, 2])
We keep 3.25e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([26074, 2])
We keep 4.57e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([29274, 2])
We keep 4.97e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([123215, 2])
We keep 1.05e+08/6.24e+09 =  1% of the original kernel matrix.

torch.Size([63255, 2])
We keep 2.00e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([5322, 2])
We keep 2.79e+05/5.37e+06 =  5% of the original kernel matrix.

torch.Size([13375, 2])
We keep 1.23e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([37230, 2])
We keep 1.11e+07/5.21e+08 =  2% of the original kernel matrix.

torch.Size([34799, 2])
We keep 6.92e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([120880, 2])
We keep 2.05e+08/7.35e+09 =  2% of the original kernel matrix.

torch.Size([63005, 2])
We keep 2.11e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([273150, 2])
We keep 6.04e+08/3.38e+10 =  1% of the original kernel matrix.

torch.Size([96058, 2])
We keep 4.11e+07/3.32e+09 =  1% of the original kernel matrix.

torch.Size([23302, 2])
We keep 4.21e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([27720, 2])
We keep 4.61e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([25044, 2])
We keep 5.31e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([28481, 2])
We keep 4.91e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([9351, 2])
We keep 1.42e+06/2.73e+07 =  5% of the original kernel matrix.

torch.Size([17049, 2])
We keep 2.23e+06/9.43e+07 =  2% of the original kernel matrix.

torch.Size([7822, 2])
We keep 5.01e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([15804, 2])
We keep 1.63e+06/6.24e+07 =  2% of the original kernel matrix.

torch.Size([53970, 2])
We keep 5.67e+07/1.59e+09 =  3% of the original kernel matrix.

torch.Size([41197, 2])
We keep 1.08e+07/7.21e+08 =  1% of the original kernel matrix.

torch.Size([11427, 2])
We keep 1.72e+06/3.31e+07 =  5% of the original kernel matrix.

torch.Size([19088, 2])
We keep 2.37e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([15724, 2])
We keep 2.64e+06/8.65e+07 =  3% of the original kernel matrix.

torch.Size([22528, 2])
We keep 3.40e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([65638, 2])
We keep 1.47e+08/2.50e+09 =  5% of the original kernel matrix.

torch.Size([45692, 2])
We keep 1.26e+07/9.03e+08 =  1% of the original kernel matrix.

torch.Size([23766, 2])
We keep 6.40e+06/1.96e+08 =  3% of the original kernel matrix.

torch.Size([28065, 2])
We keep 4.63e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([9995, 2])
We keep 2.92e+06/3.67e+07 =  7% of the original kernel matrix.

torch.Size([17765, 2])
We keep 2.54e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([12278, 2])
We keep 1.23e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([19722, 2])
We keep 2.51e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([234492, 2])
We keep 8.90e+08/5.68e+10 =  1% of the original kernel matrix.

torch.Size([84059, 2])
We keep 5.29e+07/4.30e+09 =  1% of the original kernel matrix.

torch.Size([11029, 2])
We keep 1.96e+06/3.13e+07 =  6% of the original kernel matrix.

torch.Size([18868, 2])
We keep 2.27e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([7277, 2])
We keep 3.30e+06/1.74e+07 = 19% of the original kernel matrix.

torch.Size([15420, 2])
We keep 1.90e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([24271, 2])
We keep 4.46e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([28571, 2])
We keep 4.86e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([107539, 2])
We keep 2.73e+08/9.29e+09 =  2% of the original kernel matrix.

torch.Size([58315, 2])
We keep 2.32e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([18211, 2])
We keep 3.36e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([24292, 2])
We keep 3.56e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([25524, 2])
We keep 2.86e+07/4.17e+08 =  6% of the original kernel matrix.

torch.Size([28385, 2])
We keep 6.38e+06/3.69e+08 =  1% of the original kernel matrix.

torch.Size([8633, 2])
We keep 1.94e+06/1.89e+07 = 10% of the original kernel matrix.

torch.Size([16756, 2])
We keep 1.95e+06/7.86e+07 =  2% of the original kernel matrix.

torch.Size([17351, 2])
We keep 1.12e+07/1.79e+08 =  6% of the original kernel matrix.

torch.Size([22999, 2])
We keep 4.63e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([19136, 2])
We keep 9.28e+06/1.33e+08 =  6% of the original kernel matrix.

torch.Size([24929, 2])
We keep 4.08e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([504263, 2])
We keep 1.97e+09/1.49e+11 =  1% of the original kernel matrix.

torch.Size([130848, 2])
We keep 8.19e+07/6.97e+09 =  1% of the original kernel matrix.

torch.Size([38967, 2])
We keep 1.52e+07/8.73e+08 =  1% of the original kernel matrix.

torch.Size([35160, 2])
We keep 8.63e+06/5.34e+08 =  1% of the original kernel matrix.

torch.Size([11284, 2])
We keep 1.03e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([18939, 2])
We keep 2.24e+06/9.68e+07 =  2% of the original kernel matrix.

torch.Size([148856, 2])
We keep 9.72e+07/8.53e+09 =  1% of the original kernel matrix.

torch.Size([70287, 2])
We keep 2.28e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([164461, 2])
We keep 1.11e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([74201, 2])
We keep 2.47e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([5283, 2])
We keep 3.38e+05/5.84e+06 =  5% of the original kernel matrix.

torch.Size([13522, 2])
We keep 1.24e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([90146, 2])
We keep 6.45e+07/3.09e+09 =  2% of the original kernel matrix.

torch.Size([52972, 2])
We keep 1.46e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([134519, 2])
We keep 8.35e+07/7.45e+09 =  1% of the original kernel matrix.

torch.Size([66460, 2])
We keep 2.13e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([692851, 2])
We keep 2.04e+09/1.68e+11 =  1% of the original kernel matrix.

torch.Size([161602, 2])
We keep 8.40e+07/7.40e+09 =  1% of the original kernel matrix.

torch.Size([19414, 2])
We keep 8.69e+06/1.50e+08 =  5% of the original kernel matrix.

torch.Size([25084, 2])
We keep 4.14e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([3702, 2])
We keep 1.49e+05/2.13e+06 =  6% of the original kernel matrix.

torch.Size([11661, 2])
We keep 8.85e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([15430, 2])
We keep 1.99e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([22279, 2])
We keep 3.14e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([13772, 2])
We keep 6.88e+06/7.77e+07 =  8% of the original kernel matrix.

torch.Size([21066, 2])
We keep 3.31e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([24146, 2])
We keep 4.91e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([28229, 2])
We keep 4.90e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([15890, 2])
We keep 2.52e+06/7.37e+07 =  3% of the original kernel matrix.

torch.Size([22649, 2])
We keep 3.22e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([4362, 2])
We keep 1.98e+05/3.70e+06 =  5% of the original kernel matrix.

torch.Size([12324, 2])
We keep 1.07e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([42381, 2])
We keep 1.75e+07/6.96e+08 =  2% of the original kernel matrix.

torch.Size([37165, 2])
We keep 7.88e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([125129, 2])
We keep 7.66e+07/6.24e+09 =  1% of the original kernel matrix.

torch.Size([63747, 2])
We keep 1.99e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([32665, 2])
We keep 5.27e+07/1.12e+09 =  4% of the original kernel matrix.

torch.Size([30164, 2])
We keep 9.57e+06/6.03e+08 =  1% of the original kernel matrix.

torch.Size([6877, 2])
We keep 6.95e+05/1.09e+07 =  6% of the original kernel matrix.

torch.Size([15012, 2])
We keep 1.58e+06/5.98e+07 =  2% of the original kernel matrix.

torch.Size([54209, 2])
We keep 2.91e+07/1.17e+09 =  2% of the original kernel matrix.

torch.Size([41169, 2])
We keep 9.64e+06/6.17e+08 =  1% of the original kernel matrix.

torch.Size([22369, 2])
We keep 3.51e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([27448, 2])
We keep 4.36e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([24930, 2])
We keep 6.39e+06/2.47e+08 =  2% of the original kernel matrix.

torch.Size([28750, 2])
We keep 5.15e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([16539, 2])
We keep 2.16e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([23063, 2])
We keep 3.36e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([7354, 2])
We keep 4.26e+05/9.59e+06 =  4% of the original kernel matrix.

torch.Size([15603, 2])
We keep 1.49e+06/5.59e+07 =  2% of the original kernel matrix.

torch.Size([159808, 2])
We keep 5.17e+08/1.70e+10 =  3% of the original kernel matrix.

torch.Size([71634, 2])
We keep 2.99e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([51293, 2])
We keep 1.58e+07/9.37e+08 =  1% of the original kernel matrix.

torch.Size([40543, 2])
We keep 8.81e+06/5.53e+08 =  1% of the original kernel matrix.

torch.Size([14980, 2])
We keep 3.67e+06/7.00e+07 =  5% of the original kernel matrix.

torch.Size([22109, 2])
We keep 3.16e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([39927, 2])
We keep 1.98e+07/7.66e+08 =  2% of the original kernel matrix.

torch.Size([35756, 2])
We keep 8.19e+06/5.00e+08 =  1% of the original kernel matrix.

torch.Size([8117, 2])
We keep 9.35e+05/1.37e+07 =  6% of the original kernel matrix.

torch.Size([16248, 2])
We keep 1.72e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([214895, 2])
We keep 2.74e+08/2.02e+10 =  1% of the original kernel matrix.

torch.Size([85129, 2])
We keep 3.27e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([17013, 2])
We keep 2.45e+06/9.07e+07 =  2% of the original kernel matrix.

torch.Size([23452, 2])
We keep 3.48e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([99698, 2])
We keep 6.25e+07/3.87e+09 =  1% of the original kernel matrix.

torch.Size([56169, 2])
We keep 1.61e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([3790, 2])
We keep 1.53e+05/2.68e+06 =  5% of the original kernel matrix.

torch.Size([11850, 2])
We keep 9.51e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([8100, 2])
We keep 5.53e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([16224, 2])
We keep 1.64e+06/6.29e+07 =  2% of the original kernel matrix.

torch.Size([7762, 2])
We keep 5.12e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([15887, 2])
We keep 1.64e+06/6.26e+07 =  2% of the original kernel matrix.

torch.Size([17122, 2])
We keep 4.79e+06/9.16e+07 =  5% of the original kernel matrix.

torch.Size([23619, 2])
We keep 3.44e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([7141, 2])
We keep 5.88e+05/9.84e+06 =  5% of the original kernel matrix.

torch.Size([15362, 2])
We keep 1.54e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([27661, 2])
We keep 2.25e+07/3.03e+08 =  7% of the original kernel matrix.

torch.Size([28821, 2])
We keep 5.18e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([6232, 2])
We keep 3.44e+05/7.35e+06 =  4% of the original kernel matrix.

torch.Size([14325, 2])
We keep 1.36e+06/4.90e+07 =  2% of the original kernel matrix.

torch.Size([6203, 2])
We keep 3.43e+05/7.10e+06 =  4% of the original kernel matrix.

torch.Size([14304, 2])
We keep 1.36e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([206078, 2])
We keep 2.03e+08/1.71e+10 =  1% of the original kernel matrix.

torch.Size([84131, 2])
We keep 3.07e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([44995, 2])
We keep 1.56e+07/7.94e+08 =  1% of the original kernel matrix.

torch.Size([37952, 2])
We keep 8.32e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([44380, 2])
We keep 1.44e+07/7.61e+08 =  1% of the original kernel matrix.

torch.Size([38011, 2])
We keep 8.25e+06/4.98e+08 =  1% of the original kernel matrix.

torch.Size([157380, 2])
We keep 1.34e+08/9.61e+09 =  1% of the original kernel matrix.

torch.Size([72812, 2])
We keep 2.40e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([368504, 2])
We keep 4.80e+08/4.97e+10 =  0% of the original kernel matrix.

torch.Size([112983, 2])
We keep 4.96e+07/4.03e+09 =  1% of the original kernel matrix.

torch.Size([6246, 2])
We keep 3.53e+05/6.82e+06 =  5% of the original kernel matrix.

torch.Size([14453, 2])
We keep 1.33e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([9439, 2])
We keep 7.12e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([17371, 2])
We keep 1.90e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([12655, 2])
We keep 5.81e+06/5.51e+07 = 10% of the original kernel matrix.

torch.Size([20144, 2])
We keep 2.93e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([1922, 2])
We keep 5.94e+04/6.67e+05 =  8% of the original kernel matrix.

torch.Size([8893, 2])
We keep 6.01e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([7867, 2])
We keep 5.56e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([15816, 2])
We keep 1.65e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([17848, 2])
We keep 3.84e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([24029, 2])
We keep 3.77e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([12390, 2])
We keep 1.48e+06/3.98e+07 =  3% of the original kernel matrix.

torch.Size([19868, 2])
We keep 2.51e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([8494, 2])
We keep 6.13e+05/1.55e+07 =  3% of the original kernel matrix.

torch.Size([16474, 2])
We keep 1.79e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([10282, 2])
We keep 1.03e+06/2.59e+07 =  3% of the original kernel matrix.

torch.Size([17977, 2])
We keep 2.14e+06/9.19e+07 =  2% of the original kernel matrix.

torch.Size([19061, 2])
We keep 3.32e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([24890, 2])
We keep 3.96e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([1132716, 2])
We keep 4.21e+09/4.48e+11 =  0% of the original kernel matrix.

torch.Size([201602, 2])
We keep 1.34e+08/1.21e+10 =  1% of the original kernel matrix.

torch.Size([6640, 2])
We keep 1.26e+06/1.25e+07 = 10% of the original kernel matrix.

torch.Size([14780, 2])
We keep 1.69e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([59917, 2])
We keep 6.50e+07/1.61e+09 =  4% of the original kernel matrix.

torch.Size([43027, 2])
We keep 1.13e+07/7.24e+08 =  1% of the original kernel matrix.

torch.Size([13761, 2])
We keep 2.86e+06/6.57e+07 =  4% of the original kernel matrix.

torch.Size([21036, 2])
We keep 3.06e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([5360, 2])
We keep 3.23e+05/5.29e+06 =  6% of the original kernel matrix.

torch.Size([13602, 2])
We keep 1.23e+06/4.15e+07 =  2% of the original kernel matrix.

torch.Size([53931, 2])
We keep 1.95e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([41268, 2])
We keep 9.49e+06/5.98e+08 =  1% of the original kernel matrix.

torch.Size([212995, 2])
We keep 9.40e+08/2.91e+10 =  3% of the original kernel matrix.

torch.Size([84881, 2])
We keep 3.68e+07/3.08e+09 =  1% of the original kernel matrix.

torch.Size([543223, 2])
We keep 9.07e+08/1.03e+11 =  0% of the original kernel matrix.

torch.Size([138108, 2])
We keep 6.81e+07/5.81e+09 =  1% of the original kernel matrix.

torch.Size([23965, 2])
We keep 6.76e+06/2.20e+08 =  3% of the original kernel matrix.

torch.Size([28051, 2])
We keep 4.87e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([31591, 2])
We keep 9.25e+06/3.79e+08 =  2% of the original kernel matrix.

torch.Size([32364, 2])
We keep 6.16e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([5843, 2])
We keep 2.99e+05/6.19e+06 =  4% of the original kernel matrix.

torch.Size([13991, 2])
We keep 1.28e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([29859, 2])
We keep 1.40e+07/4.60e+08 =  3% of the original kernel matrix.

torch.Size([30605, 2])
We keep 6.72e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([1108450, 2])
We keep 6.48e+09/6.20e+11 =  1% of the original kernel matrix.

torch.Size([195130, 2])
We keep 1.56e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([36657, 2])
We keep 2.82e+07/6.69e+08 =  4% of the original kernel matrix.

torch.Size([34463, 2])
We keep 7.68e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([12822, 2])
We keep 1.30e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([20096, 2])
We keep 2.54e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([50819, 2])
We keep 5.44e+07/1.60e+09 =  3% of the original kernel matrix.

torch.Size([39733, 2])
We keep 1.13e+07/7.23e+08 =  1% of the original kernel matrix.

torch.Size([433071, 2])
We keep 7.79e+08/7.38e+10 =  1% of the original kernel matrix.

torch.Size([121817, 2])
We keep 5.92e+07/4.91e+09 =  1% of the original kernel matrix.

torch.Size([47897, 2])
We keep 1.74e+07/9.13e+08 =  1% of the original kernel matrix.

torch.Size([38778, 2])
We keep 8.78e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([10074, 2])
We keep 1.00e+06/2.52e+07 =  3% of the original kernel matrix.

torch.Size([17853, 2])
We keep 2.12e+06/9.07e+07 =  2% of the original kernel matrix.

torch.Size([10561, 2])
We keep 1.17e+06/2.44e+07 =  4% of the original kernel matrix.

torch.Size([18471, 2])
We keep 2.12e+06/8.93e+07 =  2% of the original kernel matrix.

torch.Size([15486, 2])
We keep 1.74e+06/6.41e+07 =  2% of the original kernel matrix.

torch.Size([22254, 2])
We keep 3.04e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([11681, 2])
We keep 3.65e+06/4.15e+07 =  8% of the original kernel matrix.

torch.Size([19464, 2])
We keep 2.53e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([367358, 2])
We keep 7.22e+08/5.19e+10 =  1% of the original kernel matrix.

torch.Size([111787, 2])
We keep 5.01e+07/4.12e+09 =  1% of the original kernel matrix.

torch.Size([17760, 2])
We keep 3.02e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([23875, 2])
We keep 3.63e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([2608, 2])
We keep 8.32e+04/1.11e+06 =  7% of the original kernel matrix.

torch.Size([10118, 2])
We keep 7.10e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([50930, 2])
We keep 3.23e+07/1.02e+09 =  3% of the original kernel matrix.

torch.Size([40359, 2])
We keep 8.89e+06/5.77e+08 =  1% of the original kernel matrix.

torch.Size([24707, 2])
We keep 5.99e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([28705, 2])
We keep 4.81e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([3846, 2])
We keep 1.64e+05/2.75e+06 =  5% of the original kernel matrix.

torch.Size([11709, 2])
We keep 9.75e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([114526, 2])
We keep 9.42e+07/5.51e+09 =  1% of the original kernel matrix.

torch.Size([60723, 2])
We keep 1.90e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([75406, 2])
We keep 8.38e+07/2.27e+09 =  3% of the original kernel matrix.

torch.Size([48933, 2])
We keep 1.30e+07/8.61e+08 =  1% of the original kernel matrix.

torch.Size([88346, 2])
We keep 1.02e+08/3.15e+09 =  3% of the original kernel matrix.

torch.Size([52868, 2])
We keep 1.46e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([141392, 2])
We keep 1.34e+08/8.32e+09 =  1% of the original kernel matrix.

torch.Size([68073, 2])
We keep 2.26e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([149292, 2])
We keep 1.60e+08/8.85e+09 =  1% of the original kernel matrix.

torch.Size([69900, 2])
We keep 2.31e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([25794, 2])
We keep 5.80e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([29322, 2])
We keep 5.30e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([63841, 2])
We keep 3.10e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([44646, 2])
We keep 1.11e+07/7.19e+08 =  1% of the original kernel matrix.

torch.Size([104312, 2])
We keep 5.57e+07/3.98e+09 =  1% of the original kernel matrix.

torch.Size([57635, 2])
We keep 1.62e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([247683, 2])
We keep 2.44e+08/2.42e+10 =  1% of the original kernel matrix.

torch.Size([92812, 2])
We keep 3.56e+07/2.81e+09 =  1% of the original kernel matrix.

torch.Size([9509, 2])
We keep 9.27e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([17384, 2])
We keep 2.01e+06/8.36e+07 =  2% of the original kernel matrix.

torch.Size([79548, 2])
We keep 2.68e+08/4.61e+09 =  5% of the original kernel matrix.

torch.Size([50770, 2])
We keep 1.67e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([98648, 2])
We keep 1.93e+08/6.41e+09 =  3% of the original kernel matrix.

torch.Size([55977, 2])
We keep 1.93e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([64231, 2])
We keep 3.87e+07/1.83e+09 =  2% of the original kernel matrix.

torch.Size([45102, 2])
We keep 1.18e+07/7.73e+08 =  1% of the original kernel matrix.

torch.Size([42544, 2])
We keep 2.23e+07/8.14e+08 =  2% of the original kernel matrix.

torch.Size([36823, 2])
We keep 8.37e+06/5.15e+08 =  1% of the original kernel matrix.

torch.Size([21361, 2])
We keep 5.37e+07/2.87e+08 = 18% of the original kernel matrix.

torch.Size([24812, 2])
We keep 5.32e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([17101, 2])
We keep 2.43e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([23630, 2])
We keep 3.47e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([8180, 2])
We keep 6.58e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([16125, 2])
We keep 1.81e+06/7.15e+07 =  2% of the original kernel matrix.

torch.Size([77690, 2])
We keep 9.92e+07/3.16e+09 =  3% of the original kernel matrix.

torch.Size([49163, 2])
We keep 1.47e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([164641, 2])
We keep 1.72e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([74105, 2])
We keep 2.47e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([134274, 2])
We keep 7.76e+07/6.74e+09 =  1% of the original kernel matrix.

torch.Size([66312, 2])
We keep 2.03e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([126587, 2])
We keep 2.24e+08/8.25e+09 =  2% of the original kernel matrix.

torch.Size([64046, 2])
We keep 2.27e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([130475, 2])
We keep 1.13e+08/7.20e+09 =  1% of the original kernel matrix.

torch.Size([65295, 2])
We keep 2.11e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([90271, 2])
We keep 8.32e+07/3.70e+09 =  2% of the original kernel matrix.

torch.Size([53364, 2])
We keep 1.57e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([15506, 2])
We keep 2.31e+06/9.18e+07 =  2% of the original kernel matrix.

torch.Size([22283, 2])
We keep 3.44e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([351888, 2])
We keep 4.76e+08/4.30e+10 =  1% of the original kernel matrix.

torch.Size([109741, 2])
We keep 4.62e+07/3.74e+09 =  1% of the original kernel matrix.

torch.Size([4301, 2])
We keep 2.05e+05/3.53e+06 =  5% of the original kernel matrix.

torch.Size([12309, 2])
We keep 1.06e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([17362, 2])
We keep 4.32e+06/1.12e+08 =  3% of the original kernel matrix.

torch.Size([23618, 2])
We keep 3.81e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([250319, 2])
We keep 2.71e+08/2.50e+10 =  1% of the original kernel matrix.

torch.Size([92487, 2])
We keep 3.67e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([19834, 2])
We keep 3.37e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([25545, 2])
We keep 4.04e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([6055, 2])
We keep 3.68e+05/6.53e+06 =  5% of the original kernel matrix.

torch.Size([14209, 2])
We keep 1.32e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([22055, 2])
We keep 4.39e+06/1.75e+08 =  2% of the original kernel matrix.

torch.Size([26911, 2])
We keep 4.43e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([167216, 2])
We keep 2.17e+08/1.41e+10 =  1% of the original kernel matrix.

torch.Size([74885, 2])
We keep 2.85e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([26632, 2])
We keep 5.89e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([29943, 2])
We keep 5.35e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([85790, 2])
We keep 4.88e+07/2.71e+09 =  1% of the original kernel matrix.

torch.Size([52035, 2])
We keep 1.39e+07/9.41e+08 =  1% of the original kernel matrix.

torch.Size([6537, 2])
We keep 4.39e+05/8.89e+06 =  4% of the original kernel matrix.

torch.Size([14675, 2])
We keep 1.46e+06/5.39e+07 =  2% of the original kernel matrix.

torch.Size([6684, 2])
We keep 5.33e+05/9.19e+06 =  5% of the original kernel matrix.

torch.Size([14840, 2])
We keep 1.50e+06/5.48e+07 =  2% of the original kernel matrix.

torch.Size([7196, 2])
We keep 4.75e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([15283, 2])
We keep 1.55e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([21912, 2])
We keep 6.84e+06/1.99e+08 =  3% of the original kernel matrix.

torch.Size([26502, 2])
We keep 4.80e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([38260, 2])
We keep 1.14e+07/5.67e+08 =  2% of the original kernel matrix.

torch.Size([35589, 2])
We keep 7.27e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([81543, 2])
We keep 9.92e+07/2.61e+09 =  3% of the original kernel matrix.

torch.Size([50031, 2])
We keep 1.39e+07/9.23e+08 =  1% of the original kernel matrix.

torch.Size([6498, 2])
We keep 4.15e+05/8.83e+06 =  4% of the original kernel matrix.

torch.Size([14618, 2])
We keep 1.46e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([82295, 2])
We keep 4.95e+07/2.67e+09 =  1% of the original kernel matrix.

torch.Size([50962, 2])
We keep 1.37e+07/9.33e+08 =  1% of the original kernel matrix.

torch.Size([9055, 2])
We keep 1.08e+06/2.23e+07 =  4% of the original kernel matrix.

torch.Size([17051, 2])
We keep 2.06e+06/8.53e+07 =  2% of the original kernel matrix.

torch.Size([41454, 2])
We keep 1.36e+07/6.36e+08 =  2% of the original kernel matrix.

torch.Size([37048, 2])
We keep 7.52e+06/4.56e+08 =  1% of the original kernel matrix.

torch.Size([14792, 2])
We keep 2.06e+07/1.57e+08 = 13% of the original kernel matrix.

torch.Size([21276, 2])
We keep 4.23e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([124500, 2])
We keep 7.32e+07/5.79e+09 =  1% of the original kernel matrix.

torch.Size([63744, 2])
We keep 1.91e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([6551, 2])
We keep 8.05e+05/9.90e+06 =  8% of the original kernel matrix.

torch.Size([14607, 2])
We keep 1.57e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([39304, 2])
We keep 1.72e+07/6.46e+08 =  2% of the original kernel matrix.

torch.Size([35425, 2])
We keep 7.56e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([22033, 2])
We keep 4.12e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([27144, 2])
We keep 4.53e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([20528, 2])
We keep 6.08e+06/1.49e+08 =  4% of the original kernel matrix.

torch.Size([25924, 2])
We keep 4.29e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([51233, 2])
We keep 1.81e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([40634, 2])
We keep 9.38e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([207768, 2])
We keep 2.86e+08/1.92e+10 =  1% of the original kernel matrix.

torch.Size([83656, 2])
We keep 3.22e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([9010, 2])
We keep 6.67e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([16907, 2])
We keep 1.82e+06/7.34e+07 =  2% of the original kernel matrix.

torch.Size([11839, 2])
We keep 2.40e+06/5.39e+07 =  4% of the original kernel matrix.

torch.Size([19365, 2])
We keep 2.86e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([121150, 2])
We keep 1.40e+08/5.91e+09 =  2% of the original kernel matrix.

torch.Size([62748, 2])
We keep 1.94e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([18512, 2])
We keep 2.90e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([24577, 2])
We keep 3.83e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([772760, 2])
We keep 1.63e+09/2.15e+11 =  0% of the original kernel matrix.

torch.Size([168699, 2])
We keep 9.54e+07/8.37e+09 =  1% of the original kernel matrix.

torch.Size([4831, 2])
We keep 2.35e+05/4.19e+06 =  5% of the original kernel matrix.

torch.Size([13022, 2])
We keep 1.12e+06/3.70e+07 =  3% of the original kernel matrix.

torch.Size([130691, 2])
We keep 1.00e+08/7.27e+09 =  1% of the original kernel matrix.

torch.Size([65093, 2])
We keep 2.12e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([318627, 2])
We keep 3.39e+08/3.81e+10 =  0% of the original kernel matrix.

torch.Size([105230, 2])
We keep 4.37e+07/3.52e+09 =  1% of the original kernel matrix.

torch.Size([7110, 2])
We keep 4.44e+05/9.70e+06 =  4% of the original kernel matrix.

torch.Size([15270, 2])
We keep 1.51e+06/5.62e+07 =  2% of the original kernel matrix.

torch.Size([67786, 2])
We keep 7.25e+07/3.34e+09 =  2% of the original kernel matrix.

torch.Size([45657, 2])
We keep 1.51e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([14183, 2])
We keep 3.85e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([20964, 2])
We keep 3.90e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([302784, 2])
We keep 6.49e+08/3.55e+10 =  1% of the original kernel matrix.

torch.Size([101689, 2])
We keep 4.29e+07/3.41e+09 =  1% of the original kernel matrix.

torch.Size([24310, 2])
We keep 4.76e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([28565, 2])
We keep 4.80e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([21645, 2])
We keep 4.95e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([26535, 2])
We keep 4.57e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([3890, 2])
We keep 2.20e+05/3.18e+06 =  6% of the original kernel matrix.

torch.Size([11770, 2])
We keep 1.03e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([6514, 2])
We keep 4.14e+05/8.47e+06 =  4% of the original kernel matrix.

torch.Size([14588, 2])
We keep 1.43e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([10987, 2])
We keep 1.50e+06/3.03e+07 =  4% of the original kernel matrix.

torch.Size([18701, 2])
We keep 2.29e+06/9.94e+07 =  2% of the original kernel matrix.

torch.Size([26238, 2])
We keep 6.28e+06/2.80e+08 =  2% of the original kernel matrix.

torch.Size([29390, 2])
We keep 5.33e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([19791, 2])
We keep 1.19e+07/1.67e+08 =  7% of the original kernel matrix.

torch.Size([25291, 2])
We keep 4.34e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([110507, 2])
We keep 7.87e+07/4.87e+09 =  1% of the original kernel matrix.

torch.Size([59934, 2])
We keep 1.78e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([26773, 2])
We keep 5.29e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([30292, 2])
We keep 5.30e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([4939, 2])
We keep 2.45e+05/4.46e+06 =  5% of the original kernel matrix.

torch.Size([13042, 2])
We keep 1.15e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([17969, 2])
We keep 2.62e+06/9.85e+07 =  2% of the original kernel matrix.

torch.Size([24064, 2])
We keep 3.55e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([55335, 2])
We keep 1.33e+08/2.02e+09 =  6% of the original kernel matrix.

torch.Size([41668, 2])
We keep 1.24e+07/8.13e+08 =  1% of the original kernel matrix.

torch.Size([13626, 2])
We keep 1.37e+07/1.03e+08 = 13% of the original kernel matrix.

torch.Size([20951, 2])
We keep 3.62e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([179507, 2])
We keep 1.51e+08/1.26e+10 =  1% of the original kernel matrix.

torch.Size([78051, 2])
We keep 2.69e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([38659, 2])
We keep 2.78e+07/6.24e+08 =  4% of the original kernel matrix.

torch.Size([35393, 2])
We keep 7.23e+06/4.51e+08 =  1% of the original kernel matrix.

torch.Size([7695, 2])
We keep 9.58e+05/1.74e+07 =  5% of the original kernel matrix.

torch.Size([15911, 2])
We keep 1.90e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([5033, 2])
We keep 2.46e+05/4.28e+06 =  5% of the original kernel matrix.

torch.Size([13244, 2])
We keep 1.14e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([8537, 2])
We keep 1.19e+06/1.93e+07 =  6% of the original kernel matrix.

torch.Size([16581, 2])
We keep 1.98e+06/7.94e+07 =  2% of the original kernel matrix.

torch.Size([488239, 2])
We keep 1.10e+09/8.45e+10 =  1% of the original kernel matrix.

torch.Size([129963, 2])
We keep 6.27e+07/5.25e+09 =  1% of the original kernel matrix.

torch.Size([14147, 2])
We keep 1.68e+06/5.43e+07 =  3% of the original kernel matrix.

torch.Size([21297, 2])
We keep 2.85e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([17214, 2])
We keep 2.96e+06/9.29e+07 =  3% of the original kernel matrix.

torch.Size([23624, 2])
We keep 3.46e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([73976, 2])
We keep 3.91e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([47924, 2])
We keep 1.27e+07/8.43e+08 =  1% of the original kernel matrix.

torch.Size([12609, 2])
We keep 3.91e+06/5.64e+07 =  6% of the original kernel matrix.

torch.Size([19883, 2])
We keep 2.90e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([5296, 2])
We keep 2.69e+05/4.80e+06 =  5% of the original kernel matrix.

torch.Size([13463, 2])
We keep 1.18e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([8893, 2])
We keep 9.28e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([16771, 2])
We keep 1.97e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([138183, 2])
We keep 3.55e+08/1.10e+10 =  3% of the original kernel matrix.

torch.Size([67321, 2])
We keep 2.59e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([48929, 2])
We keep 5.13e+07/9.31e+08 =  5% of the original kernel matrix.

torch.Size([39569, 2])
We keep 8.81e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([18046, 2])
We keep 4.54e+06/1.31e+08 =  3% of the original kernel matrix.

torch.Size([23887, 2])
We keep 4.10e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([13406, 2])
We keep 2.34e+06/5.00e+07 =  4% of the original kernel matrix.

torch.Size([20811, 2])
We keep 2.77e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([70003, 2])
We keep 5.60e+08/3.65e+09 = 15% of the original kernel matrix.

torch.Size([47112, 2])
We keep 1.60e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([95732, 2])
We keep 6.05e+07/3.64e+09 =  1% of the original kernel matrix.

torch.Size([55254, 2])
We keep 1.58e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([58253, 2])
We keep 4.32e+08/4.59e+09 =  9% of the original kernel matrix.

torch.Size([41401, 2])
We keep 1.77e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([120340, 2])
We keep 8.48e+07/5.45e+09 =  1% of the original kernel matrix.

torch.Size([62591, 2])
We keep 1.84e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([6275, 2])
We keep 4.81e+05/8.79e+06 =  5% of the original kernel matrix.

torch.Size([14517, 2])
We keep 1.45e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([124131, 2])
We keep 1.57e+08/7.50e+09 =  2% of the original kernel matrix.

torch.Size([63616, 2])
We keep 2.15e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([10410, 2])
We keep 1.92e+06/2.90e+07 =  6% of the original kernel matrix.

torch.Size([18347, 2])
We keep 2.28e+06/9.73e+07 =  2% of the original kernel matrix.

torch.Size([41536, 2])
We keep 2.68e+07/7.24e+08 =  3% of the original kernel matrix.

torch.Size([36310, 2])
We keep 7.98e+06/4.86e+08 =  1% of the original kernel matrix.

torch.Size([13897, 2])
We keep 3.01e+06/8.57e+07 =  3% of the original kernel matrix.

torch.Size([20975, 2])
We keep 3.34e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([7508, 2])
We keep 5.32e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([15607, 2])
We keep 1.64e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([17305, 2])
We keep 4.35e+06/1.00e+08 =  4% of the original kernel matrix.

torch.Size([23615, 2])
We keep 3.64e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([31895, 2])
We keep 9.36e+06/4.19e+08 =  2% of the original kernel matrix.

torch.Size([32060, 2])
We keep 6.39e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([6642, 2])
We keep 5.54e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([14534, 2])
We keep 1.54e+06/5.81e+07 =  2% of the original kernel matrix.

torch.Size([30569, 2])
We keep 7.31e+06/3.53e+08 =  2% of the original kernel matrix.

torch.Size([32035, 2])
We keep 6.07e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([9124, 2])
We keep 5.24e+06/3.76e+07 = 13% of the original kernel matrix.

torch.Size([17018, 2])
We keep 2.48e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([127625, 2])
We keep 3.15e+08/1.14e+10 =  2% of the original kernel matrix.

torch.Size([63930, 2])
We keep 2.54e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([5918, 2])
We keep 7.98e+05/7.95e+06 = 10% of the original kernel matrix.

torch.Size([14051, 2])
We keep 1.45e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([8006, 2])
We keep 1.51e+06/2.35e+07 =  6% of the original kernel matrix.

torch.Size([15762, 2])
We keep 2.00e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([7510, 2])
We keep 1.54e+06/1.97e+07 =  7% of the original kernel matrix.

torch.Size([15365, 2])
We keep 1.95e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([26461, 2])
We keep 7.07e+06/2.60e+08 =  2% of the original kernel matrix.

torch.Size([29947, 2])
We keep 5.34e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([5296, 2])
We keep 3.12e+05/5.94e+06 =  5% of the original kernel matrix.

torch.Size([13415, 2])
We keep 1.28e+06/4.40e+07 =  2% of the original kernel matrix.

torch.Size([18726, 2])
We keep 2.51e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([24700, 2])
We keep 3.68e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([27177, 2])
We keep 5.71e+06/2.63e+08 =  2% of the original kernel matrix.

torch.Size([30580, 2])
We keep 5.31e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([83144, 2])
We keep 6.08e+07/2.96e+09 =  2% of the original kernel matrix.

torch.Size([51478, 2])
We keep 1.45e+07/9.82e+08 =  1% of the original kernel matrix.

torch.Size([7907, 2])
We keep 5.96e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([16084, 2])
We keep 1.64e+06/6.28e+07 =  2% of the original kernel matrix.

torch.Size([3262, 2])
We keep 9.57e+04/1.45e+06 =  6% of the original kernel matrix.

torch.Size([11184, 2])
We keep 7.74e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([4337, 2])
We keep 2.73e+05/3.22e+06 =  8% of the original kernel matrix.

torch.Size([12381, 2])
We keep 1.03e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([187550, 2])
We keep 4.31e+08/2.03e+10 =  2% of the original kernel matrix.

torch.Size([78918, 2])
We keep 3.29e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([1473520, 2])
We keep 7.41e+09/7.27e+11 =  1% of the original kernel matrix.

torch.Size([237588, 2])
We keep 1.68e+08/1.54e+10 =  1% of the original kernel matrix.

torch.Size([21084, 2])
We keep 4.84e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([26144, 2])
We keep 4.51e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([10301, 2])
We keep 9.58e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([18106, 2])
We keep 2.14e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([83652, 2])
We keep 9.24e+07/3.30e+09 =  2% of the original kernel matrix.

torch.Size([51208, 2])
We keep 1.53e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([124480, 2])
We keep 7.06e+07/5.89e+09 =  1% of the original kernel matrix.

torch.Size([63930, 2])
We keep 1.92e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([3640, 2])
We keep 1.48e+05/2.37e+06 =  6% of the original kernel matrix.

torch.Size([11441, 2])
We keep 9.20e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([16985, 2])
We keep 4.82e+06/9.90e+07 =  4% of the original kernel matrix.

torch.Size([23296, 2])
We keep 3.64e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([34609, 2])
We keep 7.81e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([34063, 2])
We keep 6.54e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([388771, 2])
We keep 2.51e+09/1.37e+11 =  1% of the original kernel matrix.

torch.Size([110643, 2])
We keep 7.87e+07/6.69e+09 =  1% of the original kernel matrix.

torch.Size([528582, 2])
We keep 2.40e+09/1.86e+11 =  1% of the original kernel matrix.

torch.Size([130796, 2])
We keep 9.02e+07/7.79e+09 =  1% of the original kernel matrix.

torch.Size([203475, 2])
We keep 1.78e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([83171, 2])
We keep 3.01e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([15587, 2])
We keep 2.35e+06/6.67e+07 =  3% of the original kernel matrix.

torch.Size([22433, 2])
We keep 3.10e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([4438, 2])
We keep 2.18e+05/3.83e+06 =  5% of the original kernel matrix.

torch.Size([12514, 2])
We keep 1.07e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([5276, 2])
We keep 3.54e+05/5.73e+06 =  6% of the original kernel matrix.

torch.Size([13373, 2])
We keep 1.25e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([65038, 2])
We keep 3.95e+07/1.83e+09 =  2% of the original kernel matrix.

torch.Size([44848, 2])
We keep 1.17e+07/7.72e+08 =  1% of the original kernel matrix.

torch.Size([15634, 2])
We keep 2.02e+06/7.15e+07 =  2% of the original kernel matrix.

torch.Size([22449, 2])
We keep 3.19e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([18586, 2])
We keep 1.25e+07/2.26e+08 =  5% of the original kernel matrix.

torch.Size([23953, 2])
We keep 4.79e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([157676, 2])
We keep 2.15e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([72133, 2])
We keep 2.56e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([514828, 2])
We keep 7.69e+08/8.70e+10 =  0% of the original kernel matrix.

torch.Size([133387, 2])
We keep 6.35e+07/5.33e+09 =  1% of the original kernel matrix.

torch.Size([10359, 2])
We keep 9.69e+05/2.68e+07 =  3% of the original kernel matrix.

torch.Size([18080, 2])
We keep 2.20e+06/9.35e+07 =  2% of the original kernel matrix.

torch.Size([5136, 2])
We keep 2.47e+05/4.46e+06 =  5% of the original kernel matrix.

torch.Size([13339, 2])
We keep 1.15e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([55851, 2])
We keep 7.02e+07/1.29e+09 =  5% of the original kernel matrix.

torch.Size([42146, 2])
We keep 9.83e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([128506, 2])
We keep 5.53e+08/7.71e+09 =  7% of the original kernel matrix.

torch.Size([65064, 2])
We keep 2.09e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([46727, 2])
We keep 2.68e+07/8.06e+08 =  3% of the original kernel matrix.

torch.Size([38713, 2])
We keep 8.38e+06/5.13e+08 =  1% of the original kernel matrix.

torch.Size([3302, 2])
We keep 2.03e+05/2.03e+06 = 10% of the original kernel matrix.

torch.Size([11035, 2])
We keep 8.69e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([4890, 2])
We keep 5.09e+05/4.60e+06 = 11% of the original kernel matrix.

torch.Size([13066, 2])
We keep 1.19e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([17052, 2])
We keep 2.44e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([23485, 2])
We keep 3.43e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([4488, 2])
We keep 1.89e+05/3.47e+06 =  5% of the original kernel matrix.

torch.Size([12729, 2])
We keep 1.05e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([24994, 2])
We keep 5.73e+07/5.33e+08 = 10% of the original kernel matrix.

torch.Size([27944, 2])
We keep 7.10e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([6570, 2])
We keep 4.09e+05/8.83e+06 =  4% of the original kernel matrix.

torch.Size([14711, 2])
We keep 1.46e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([17753, 2])
We keep 2.50e+06/9.72e+07 =  2% of the original kernel matrix.

torch.Size([24082, 2])
We keep 3.57e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([13941, 2])
We keep 1.62e+06/5.15e+07 =  3% of the original kernel matrix.

torch.Size([21112, 2])
We keep 2.81e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([8425, 2])
We keep 6.39e+05/1.60e+07 =  3% of the original kernel matrix.

torch.Size([16315, 2])
We keep 1.82e+06/7.23e+07 =  2% of the original kernel matrix.

torch.Size([3526, 2])
We keep 1.25e+05/1.95e+06 =  6% of the original kernel matrix.

torch.Size([11492, 2])
We keep 8.60e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([180870, 2])
We keep 1.60e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([78100, 2])
We keep 2.74e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([437026, 2])
We keep 8.14e+08/6.47e+10 =  1% of the original kernel matrix.

torch.Size([121942, 2])
We keep 5.59e+07/4.60e+09 =  1% of the original kernel matrix.

torch.Size([79553, 2])
We keep 1.12e+08/2.86e+09 =  3% of the original kernel matrix.

torch.Size([50259, 2])
We keep 1.43e+07/9.66e+08 =  1% of the original kernel matrix.

torch.Size([19381, 2])
We keep 9.62e+06/1.52e+08 =  6% of the original kernel matrix.

torch.Size([24867, 2])
We keep 4.36e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([22435, 2])
We keep 3.63e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([27281, 2])
We keep 4.40e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([8810, 2])
We keep 1.17e+06/1.94e+07 =  6% of the original kernel matrix.

torch.Size([16839, 2])
We keep 1.99e+06/7.95e+07 =  2% of the original kernel matrix.

torch.Size([21638, 2])
We keep 4.66e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([26564, 2])
We keep 4.40e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([167731, 2])
We keep 1.32e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([74649, 2])
We keep 2.53e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([17759, 2])
We keep 2.74e+06/9.39e+07 =  2% of the original kernel matrix.

torch.Size([24352, 2])
We keep 3.56e+06/1.75e+08 =  2% of the original kernel matrix.

torch.Size([51498, 2])
We keep 7.23e+07/1.19e+09 =  6% of the original kernel matrix.

torch.Size([40379, 2])
We keep 9.39e+06/6.23e+08 =  1% of the original kernel matrix.

torch.Size([26053, 2])
We keep 5.92e+06/2.40e+08 =  2% of the original kernel matrix.

torch.Size([29471, 2])
We keep 5.18e+06/2.80e+08 =  1% of the original kernel matrix.

torch.Size([1254752, 2])
We keep 4.42e+09/4.93e+11 =  0% of the original kernel matrix.

torch.Size([216617, 2])
We keep 1.41e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([14508, 2])
We keep 2.11e+06/5.54e+07 =  3% of the original kernel matrix.

torch.Size([21582, 2])
We keep 2.90e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([74200, 2])
We keep 3.35e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([48157, 2])
We keep 1.24e+07/8.17e+08 =  1% of the original kernel matrix.

torch.Size([213201, 2])
We keep 3.04e+08/1.93e+10 =  1% of the original kernel matrix.

torch.Size([85169, 2])
We keep 3.24e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([76464, 2])
We keep 4.88e+07/2.05e+09 =  2% of the original kernel matrix.

torch.Size([49168, 2])
We keep 1.22e+07/8.17e+08 =  1% of the original kernel matrix.

torch.Size([27936, 2])
We keep 9.35e+06/3.18e+08 =  2% of the original kernel matrix.

torch.Size([30556, 2])
We keep 5.82e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([364447, 2])
We keep 7.01e+08/5.26e+10 =  1% of the original kernel matrix.

torch.Size([112330, 2])
We keep 5.07e+07/4.14e+09 =  1% of the original kernel matrix.

torch.Size([76955, 2])
We keep 3.16e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([49330, 2])
We keep 1.27e+07/8.53e+08 =  1% of the original kernel matrix.

torch.Size([26975, 2])
We keep 5.82e+06/2.74e+08 =  2% of the original kernel matrix.

torch.Size([30701, 2])
We keep 5.48e+06/2.99e+08 =  1% of the original kernel matrix.

torch.Size([8805, 2])
We keep 1.14e+06/1.96e+07 =  5% of the original kernel matrix.

torch.Size([16969, 2])
We keep 1.96e+06/7.99e+07 =  2% of the original kernel matrix.

torch.Size([22650, 2])
We keep 5.86e+06/1.92e+08 =  3% of the original kernel matrix.

torch.Size([27228, 2])
We keep 4.70e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([14577, 2])
We keep 3.19e+06/7.16e+07 =  4% of the original kernel matrix.

torch.Size([21656, 2])
We keep 3.20e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([92642, 2])
We keep 4.59e+07/3.15e+09 =  1% of the original kernel matrix.

torch.Size([53764, 2])
We keep 1.47e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([637617, 2])
We keep 1.64e+09/1.53e+11 =  1% of the original kernel matrix.

torch.Size([153617, 2])
We keep 8.26e+07/7.07e+09 =  1% of the original kernel matrix.

torch.Size([89503, 2])
We keep 1.96e+08/3.53e+09 =  5% of the original kernel matrix.

torch.Size([53486, 2])
We keep 1.56e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([142247, 2])
We keep 1.52e+08/9.34e+09 =  1% of the original kernel matrix.

torch.Size([68059, 2])
We keep 2.39e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([22218, 2])
We keep 3.94e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([27126, 2])
We keep 4.37e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([15457, 2])
We keep 1.26e+07/1.23e+08 = 10% of the original kernel matrix.

torch.Size([22135, 2])
We keep 3.81e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([32843, 2])
We keep 9.95e+06/4.61e+08 =  2% of the original kernel matrix.

torch.Size([32937, 2])
We keep 6.70e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([9054, 2])
We keep 1.58e+06/2.46e+07 =  6% of the original kernel matrix.

torch.Size([16917, 2])
We keep 2.13e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([449526, 2])
We keep 1.40e+09/9.98e+10 =  1% of the original kernel matrix.

torch.Size([124435, 2])
We keep 6.73e+07/5.71e+09 =  1% of the original kernel matrix.

torch.Size([394025, 2])
We keep 4.86e+08/5.55e+10 =  0% of the original kernel matrix.

torch.Size([116741, 2])
We keep 5.17e+07/4.25e+09 =  1% of the original kernel matrix.

torch.Size([9150, 2])
We keep 6.08e+06/4.26e+07 = 14% of the original kernel matrix.

torch.Size([16891, 2])
We keep 2.59e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([1122124, 2])
We keep 9.64e+09/8.09e+11 =  1% of the original kernel matrix.

torch.Size([191777, 2])
We keep 1.77e+08/1.62e+10 =  1% of the original kernel matrix.

torch.Size([81883, 2])
We keep 5.38e+08/4.79e+09 = 11% of the original kernel matrix.

torch.Size([51023, 2])
We keep 1.81e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([35502, 2])
We keep 8.50e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([33812, 2])
We keep 6.56e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([5388, 2])
We keep 3.15e+05/5.72e+06 =  5% of the original kernel matrix.

torch.Size([13424, 2])
We keep 1.26e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([53291, 2])
We keep 3.46e+07/1.10e+09 =  3% of the original kernel matrix.

torch.Size([41317, 2])
We keep 9.39e+06/5.99e+08 =  1% of the original kernel matrix.

torch.Size([115914, 2])
We keep 7.23e+07/5.14e+09 =  1% of the original kernel matrix.

torch.Size([61201, 2])
We keep 1.83e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([4585, 2])
We keep 2.21e+05/3.58e+06 =  6% of the original kernel matrix.

torch.Size([12661, 2])
We keep 1.06e+06/3.42e+07 =  3% of the original kernel matrix.

torch.Size([860836, 2])
We keep 1.77e+09/2.47e+11 =  0% of the original kernel matrix.

torch.Size([178538, 2])
We keep 1.02e+08/8.98e+09 =  1% of the original kernel matrix.

torch.Size([26621, 2])
We keep 8.57e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([29519, 2])
We keep 5.54e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([22616, 2])
We keep 8.22e+06/2.37e+08 =  3% of the original kernel matrix.

torch.Size([26957, 2])
We keep 5.05e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([10735, 2])
We keep 1.28e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([18598, 2])
We keep 2.33e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([16929, 2])
We keep 2.40e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([23416, 2])
We keep 3.43e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([3488, 2])
We keep 1.25e+05/1.94e+06 =  6% of the original kernel matrix.

torch.Size([11420, 2])
We keep 8.65e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([19438, 2])
We keep 4.33e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([24976, 2])
We keep 4.32e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([17944, 2])
We keep 3.95e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([24091, 2])
We keep 3.67e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([19958, 2])
We keep 3.79e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([25563, 2])
We keep 4.18e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([372673, 2])
We keep 1.99e+10/4.76e+11 =  4% of the original kernel matrix.

torch.Size([92493, 2])
We keep 1.41e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([19359, 2])
We keep 6.79e+06/2.03e+08 =  3% of the original kernel matrix.

torch.Size([24724, 2])
We keep 4.73e+06/2.57e+08 =  1% of the original kernel matrix.

torch.Size([118115, 2])
We keep 6.68e+07/5.18e+09 =  1% of the original kernel matrix.

torch.Size([61658, 2])
We keep 1.81e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([6116, 2])
We keep 3.81e+05/7.59e+06 =  5% of the original kernel matrix.

torch.Size([14284, 2])
We keep 1.37e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([20416, 2])
We keep 3.36e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([25886, 2])
We keep 4.17e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([13041, 2])
We keep 1.55e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([20362, 2])
We keep 2.74e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([40845, 2])
We keep 2.66e+08/2.27e+09 = 11% of the original kernel matrix.

torch.Size([34744, 2])
We keep 1.32e+07/8.61e+08 =  1% of the original kernel matrix.

torch.Size([133854, 2])
We keep 2.11e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([64633, 2])
We keep 2.59e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([1173941, 2])
We keep 3.12e+09/4.19e+11 =  0% of the original kernel matrix.

torch.Size([207082, 2])
We keep 1.30e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([19892, 2])
We keep 6.25e+06/1.56e+08 =  4% of the original kernel matrix.

torch.Size([25386, 2])
We keep 4.28e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([191364, 2])
We keep 1.81e+08/1.56e+10 =  1% of the original kernel matrix.

torch.Size([80687, 2])
We keep 2.96e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([17993, 2])
We keep 1.46e+07/1.52e+08 =  9% of the original kernel matrix.

torch.Size([24011, 2])
We keep 4.26e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([127641, 2])
We keep 9.47e+07/6.70e+09 =  1% of the original kernel matrix.

torch.Size([64337, 2])
We keep 2.05e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([141802, 2])
We keep 1.48e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([68312, 2])
We keep 2.45e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([20747, 2])
We keep 4.39e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([26029, 2])
We keep 4.24e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([7885, 2])
We keep 7.22e+05/1.36e+07 =  5% of the original kernel matrix.

torch.Size([15912, 2])
We keep 1.71e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([71685, 2])
We keep 2.96e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([47327, 2])
We keep 1.18e+07/7.78e+08 =  1% of the original kernel matrix.

torch.Size([21240, 2])
We keep 1.06e+07/1.71e+08 =  6% of the original kernel matrix.

torch.Size([26375, 2])
We keep 4.42e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([91795, 2])
We keep 6.53e+07/3.74e+09 =  1% of the original kernel matrix.

torch.Size([53237, 2])
We keep 1.59e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([253299, 2])
We keep 3.42e+08/2.76e+10 =  1% of the original kernel matrix.

torch.Size([92221, 2])
We keep 3.80e+07/3.00e+09 =  1% of the original kernel matrix.

torch.Size([11704, 2])
We keep 3.35e+06/6.00e+07 =  5% of the original kernel matrix.

torch.Size([19392, 2])
We keep 2.98e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([50766, 2])
We keep 5.88e+07/1.08e+09 =  5% of the original kernel matrix.

torch.Size([39889, 2])
We keep 9.56e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([18164, 2])
We keep 3.22e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([24148, 2])
We keep 3.78e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([187698, 2])
We keep 1.67e+08/1.44e+10 =  1% of the original kernel matrix.

torch.Size([79326, 2])
We keep 2.84e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([276513, 2])
We keep 3.87e+08/3.10e+10 =  1% of the original kernel matrix.

torch.Size([97576, 2])
We keep 3.98e+07/3.18e+09 =  1% of the original kernel matrix.

torch.Size([28610, 2])
We keep 6.99e+06/3.00e+08 =  2% of the original kernel matrix.

torch.Size([29931, 2])
We keep 5.44e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([11251, 2])
We keep 1.36e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([18908, 2])
We keep 2.44e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([325066, 2])
We keep 6.77e+08/5.38e+10 =  1% of the original kernel matrix.

torch.Size([103367, 2])
We keep 5.12e+07/4.19e+09 =  1% of the original kernel matrix.

torch.Size([104489, 2])
We keep 9.13e+07/4.96e+09 =  1% of the original kernel matrix.

torch.Size([57391, 2])
We keep 1.80e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([535362, 2])
We keep 2.02e+09/1.51e+11 =  1% of the original kernel matrix.

torch.Size([136006, 2])
We keep 8.13e+07/7.01e+09 =  1% of the original kernel matrix.

torch.Size([68224, 2])
We keep 4.02e+07/1.95e+09 =  2% of the original kernel matrix.

torch.Size([45712, 2])
We keep 1.22e+07/7.98e+08 =  1% of the original kernel matrix.

torch.Size([64216, 2])
We keep 4.53e+07/1.92e+09 =  2% of the original kernel matrix.

torch.Size([44532, 2])
We keep 1.22e+07/7.91e+08 =  1% of the original kernel matrix.

torch.Size([23421, 2])
We keep 1.77e+07/2.76e+08 =  6% of the original kernel matrix.

torch.Size([27766, 2])
We keep 5.62e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([48474, 2])
We keep 3.15e+07/1.14e+09 =  2% of the original kernel matrix.

torch.Size([38401, 2])
We keep 9.78e+06/6.10e+08 =  1% of the original kernel matrix.

torch.Size([292330, 2])
We keep 8.19e+08/4.56e+10 =  1% of the original kernel matrix.

torch.Size([98100, 2])
We keep 4.75e+07/3.86e+09 =  1% of the original kernel matrix.

torch.Size([51703, 2])
We keep 1.90e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([40697, 2])
We keep 9.27e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([10775, 2])
We keep 1.12e+06/2.92e+07 =  3% of the original kernel matrix.

torch.Size([18443, 2])
We keep 2.27e+06/9.76e+07 =  2% of the original kernel matrix.

torch.Size([14983, 2])
We keep 2.54e+06/7.58e+07 =  3% of the original kernel matrix.

torch.Size([21895, 2])
We keep 3.25e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([33294, 2])
We keep 3.76e+07/4.70e+08 =  7% of the original kernel matrix.

torch.Size([32633, 2])
We keep 6.77e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([79867, 2])
We keep 1.02e+08/2.59e+09 =  3% of the original kernel matrix.

torch.Size([49702, 2])
We keep 1.39e+07/9.19e+08 =  1% of the original kernel matrix.

torch.Size([68744, 2])
We keep 3.03e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([46467, 2])
We keep 1.12e+07/7.31e+08 =  1% of the original kernel matrix.

torch.Size([12892, 2])
We keep 1.45e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([20199, 2])
We keep 2.70e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([24348, 2])
We keep 4.12e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([28164, 2])
We keep 4.70e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([106585, 2])
We keep 1.35e+08/4.98e+09 =  2% of the original kernel matrix.

torch.Size([58359, 2])
We keep 1.80e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([53631, 2])
We keep 1.76e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([41487, 2])
We keep 9.29e+06/5.87e+08 =  1% of the original kernel matrix.

torch.Size([59680, 2])
We keep 3.51e+07/1.33e+09 =  2% of the original kernel matrix.

torch.Size([43497, 2])
We keep 1.04e+07/6.60e+08 =  1% of the original kernel matrix.

torch.Size([74140, 2])
We keep 3.70e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([48165, 2])
We keep 1.20e+07/7.98e+08 =  1% of the original kernel matrix.

torch.Size([32542, 2])
We keep 1.15e+07/4.53e+08 =  2% of the original kernel matrix.

torch.Size([32446, 2])
We keep 6.64e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([23060, 2])
We keep 5.19e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([27607, 2])
We keep 4.53e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([5482, 2])
We keep 3.33e+05/6.13e+06 =  5% of the original kernel matrix.

torch.Size([13644, 2])
We keep 1.28e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([11392, 2])
We keep 1.16e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([18977, 2])
We keep 2.35e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([139971, 2])
We keep 8.66e+07/7.30e+09 =  1% of the original kernel matrix.

torch.Size([67835, 2])
We keep 2.11e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([17882, 2])
We keep 4.34e+06/1.08e+08 =  4% of the original kernel matrix.

torch.Size([24081, 2])
We keep 3.76e+06/1.88e+08 =  2% of the original kernel matrix.

torch.Size([76988, 2])
We keep 3.18e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([49001, 2])
We keep 1.25e+07/8.31e+08 =  1% of the original kernel matrix.

torch.Size([3412, 2])
We keep 1.17e+05/1.97e+06 =  5% of the original kernel matrix.

torch.Size([11320, 2])
We keep 8.56e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([8635, 2])
We keep 6.26e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([16607, 2])
We keep 1.83e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([82560, 2])
We keep 8.90e+07/3.40e+09 =  2% of the original kernel matrix.

torch.Size([50727, 2])
We keep 1.51e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([90744, 2])
We keep 5.22e+07/3.20e+09 =  1% of the original kernel matrix.

torch.Size([53243, 2])
We keep 1.49e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([28084, 2])
We keep 5.97e+06/2.82e+08 =  2% of the original kernel matrix.

torch.Size([30548, 2])
We keep 5.45e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([6815, 2])
We keep 4.11e+05/8.91e+06 =  4% of the original kernel matrix.

torch.Size([14953, 2])
We keep 1.47e+06/5.39e+07 =  2% of the original kernel matrix.

torch.Size([3987, 2])
We keep 3.16e+05/3.19e+06 =  9% of the original kernel matrix.

torch.Size([11970, 2])
We keep 1.03e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([26113, 2])
We keep 5.82e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([29654, 2])
We keep 5.24e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([26570, 2])
We keep 1.13e+07/2.75e+08 =  4% of the original kernel matrix.

torch.Size([30062, 2])
We keep 5.53e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([99288, 2])
We keep 6.48e+07/3.93e+09 =  1% of the original kernel matrix.

torch.Size([56179, 2])
We keep 1.65e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([8542, 2])
We keep 1.10e+06/1.97e+07 =  5% of the original kernel matrix.

torch.Size([16563, 2])
We keep 1.94e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([16227, 2])
We keep 4.13e+06/1.16e+08 =  3% of the original kernel matrix.

torch.Size([22682, 2])
We keep 3.83e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([12762, 2])
We keep 2.87e+06/5.93e+07 =  4% of the original kernel matrix.

torch.Size([20012, 2])
We keep 2.98e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([18941, 2])
We keep 9.27e+06/1.38e+08 =  6% of the original kernel matrix.

torch.Size([24561, 2])
We keep 4.12e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([177563, 2])
We keep 2.52e+08/1.38e+10 =  1% of the original kernel matrix.

torch.Size([77420, 2])
We keep 2.80e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([8802, 2])
We keep 1.39e+06/1.75e+07 =  7% of the original kernel matrix.

torch.Size([16943, 2])
We keep 1.82e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([55930, 2])
We keep 2.50e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([42057, 2])
We keep 1.01e+07/6.54e+08 =  1% of the original kernel matrix.

torch.Size([270902, 2])
We keep 4.21e+08/2.92e+10 =  1% of the original kernel matrix.

torch.Size([97205, 2])
We keep 3.91e+07/3.08e+09 =  1% of the original kernel matrix.

torch.Size([43202, 2])
We keep 1.37e+07/6.75e+08 =  2% of the original kernel matrix.

torch.Size([37635, 2])
We keep 7.77e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([64358, 2])
We keep 4.49e+07/1.49e+09 =  3% of the original kernel matrix.

torch.Size([45438, 2])
We keep 1.08e+07/6.96e+08 =  1% of the original kernel matrix.

torch.Size([121880, 2])
We keep 6.63e+07/5.44e+09 =  1% of the original kernel matrix.

torch.Size([63198, 2])
We keep 1.87e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([3795, 2])
We keep 1.20e+05/1.97e+06 =  6% of the original kernel matrix.

torch.Size([11863, 2])
We keep 8.56e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([37020, 2])
We keep 1.31e+07/4.86e+08 =  2% of the original kernel matrix.

torch.Size([35101, 2])
We keep 6.88e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([16364, 2])
We keep 2.02e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([23036, 2])
We keep 3.23e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([249969, 2])
We keep 3.33e+08/2.61e+10 =  1% of the original kernel matrix.

torch.Size([93708, 2])
We keep 3.74e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([71040, 2])
We keep 4.25e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([47424, 2])
We keep 1.27e+07/8.49e+08 =  1% of the original kernel matrix.

torch.Size([23291, 2])
We keep 7.86e+06/2.31e+08 =  3% of the original kernel matrix.

torch.Size([27436, 2])
We keep 5.11e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([95130, 2])
We keep 6.51e+07/3.34e+09 =  1% of the original kernel matrix.

torch.Size([55097, 2])
We keep 1.50e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([19966, 2])
We keep 4.51e+06/1.33e+08 =  3% of the original kernel matrix.

torch.Size([25578, 2])
We keep 4.04e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([7155, 2])
We keep 1.73e+06/1.74e+07 =  9% of the original kernel matrix.

torch.Size([15379, 2])
We keep 1.82e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([6969, 2])
We keep 4.28e+05/9.49e+06 =  4% of the original kernel matrix.

torch.Size([15185, 2])
We keep 1.51e+06/5.56e+07 =  2% of the original kernel matrix.

torch.Size([14127, 2])
We keep 1.77e+06/5.42e+07 =  3% of the original kernel matrix.

torch.Size([21237, 2])
We keep 2.89e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([127773, 2])
We keep 1.07e+08/6.37e+09 =  1% of the original kernel matrix.

torch.Size([64818, 2])
We keep 2.00e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([706388, 2])
We keep 1.67e+09/1.86e+11 =  0% of the original kernel matrix.

torch.Size([163220, 2])
We keep 8.99e+07/7.79e+09 =  1% of the original kernel matrix.

torch.Size([55670, 2])
We keep 4.91e+07/1.65e+09 =  2% of the original kernel matrix.

torch.Size([40720, 2])
We keep 1.11e+07/7.33e+08 =  1% of the original kernel matrix.

torch.Size([16973, 2])
We keep 2.81e+06/8.88e+07 =  3% of the original kernel matrix.

torch.Size([23498, 2])
We keep 3.41e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([28296, 2])
We keep 7.29e+06/3.30e+08 =  2% of the original kernel matrix.

torch.Size([30480, 2])
We keep 5.75e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([399009, 2])
We keep 7.58e+08/6.46e+10 =  1% of the original kernel matrix.

torch.Size([116568, 2])
We keep 5.56e+07/4.59e+09 =  1% of the original kernel matrix.

torch.Size([18954, 2])
We keep 2.90e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([25024, 2])
We keep 3.85e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([14766, 2])
We keep 2.23e+06/6.40e+07 =  3% of the original kernel matrix.

torch.Size([21651, 2])
We keep 3.01e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([69693, 2])
We keep 2.93e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([46725, 2])
We keep 1.16e+07/7.58e+08 =  1% of the original kernel matrix.

torch.Size([103093, 2])
We keep 7.89e+07/4.08e+09 =  1% of the original kernel matrix.

torch.Size([57323, 2])
We keep 1.64e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([10708, 2])
We keep 1.54e+06/3.07e+07 =  5% of the original kernel matrix.

torch.Size([18388, 2])
We keep 2.27e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([20115, 2])
We keep 4.45e+06/1.33e+08 =  3% of the original kernel matrix.

torch.Size([25751, 2])
We keep 3.99e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([22288, 2])
We keep 1.91e+07/2.69e+08 =  7% of the original kernel matrix.

torch.Size([27078, 2])
We keep 5.32e+06/2.96e+08 =  1% of the original kernel matrix.

time for making ranges is 3.3680193424224854
Sorting X and nu_X
time for sorting X is 0.08167505264282227
Sorting Z and nu_Z
time for sorting Z is 0.00026345252990722656
Starting Optim
sum tnu_Z before tensor(30420312., device='cuda:0')
c= tensor(1091.8049, device='cuda:0')
c= tensor(112519.6719, device='cuda:0')
c= tensor(117344.9609, device='cuda:0')
c= tensor(126535.7656, device='cuda:0')
c= tensor(284913.8125, device='cuda:0')
c= tensor(479531.2188, device='cuda:0')
c= tensor(1148577., device='cuda:0')
c= tensor(1433327.3750, device='cuda:0')
c= tensor(1971488.5000, device='cuda:0')
c= tensor(13578008., device='cuda:0')
c= tensor(13729682., device='cuda:0')
c= tensor(14908808., device='cuda:0')
c= tensor(14917908., device='cuda:0')
c= tensor(26010304., device='cuda:0')
c= tensor(26150408., device='cuda:0')
c= tensor(26297264., device='cuda:0')
c= tensor(26809028., device='cuda:0')
c= tensor(27848012., device='cuda:0')
c= tensor(31170636., device='cuda:0')
c= tensor(35240076., device='cuda:0')
c= tensor(35293952., device='cuda:0')
c= tensor(42166252., device='cuda:0')
c= tensor(42209944., device='cuda:0')
c= tensor(42352276., device='cuda:0')
c= tensor(42420640., device='cuda:0')
c= tensor(43183104., device='cuda:0')
c= tensor(44228424., device='cuda:0')
c= tensor(44254900., device='cuda:0')
c= tensor(59359356., device='cuda:0')
c= tensor(1.5162e+08, device='cuda:0')
c= tensor(1.5165e+08, device='cuda:0')
c= tensor(4.6328e+08, device='cuda:0')
c= tensor(4.6334e+08, device='cuda:0')
c= tensor(4.6344e+08, device='cuda:0')
c= tensor(4.6347e+08, device='cuda:0')
c= tensor(4.9770e+08, device='cuda:0')
c= tensor(4.9925e+08, device='cuda:0')
c= tensor(4.9925e+08, device='cuda:0')
c= tensor(4.9926e+08, device='cuda:0')
c= tensor(4.9926e+08, device='cuda:0')
c= tensor(4.9927e+08, device='cuda:0')
c= tensor(4.9927e+08, device='cuda:0')
c= tensor(4.9927e+08, device='cuda:0')
c= tensor(4.9928e+08, device='cuda:0')
c= tensor(4.9928e+08, device='cuda:0')
c= tensor(4.9928e+08, device='cuda:0')
c= tensor(4.9929e+08, device='cuda:0')
c= tensor(4.9930e+08, device='cuda:0')
c= tensor(4.9930e+08, device='cuda:0')
c= tensor(4.9933e+08, device='cuda:0')
c= tensor(4.9938e+08, device='cuda:0')
c= tensor(4.9939e+08, device='cuda:0')
c= tensor(4.9941e+08, device='cuda:0')
c= tensor(4.9942e+08, device='cuda:0')
c= tensor(4.9943e+08, device='cuda:0')
c= tensor(4.9944e+08, device='cuda:0')
c= tensor(4.9944e+08, device='cuda:0')
c= tensor(4.9945e+08, device='cuda:0')
c= tensor(4.9945e+08, device='cuda:0')
c= tensor(4.9946e+08, device='cuda:0')
c= tensor(4.9947e+08, device='cuda:0')
c= tensor(4.9947e+08, device='cuda:0')
c= tensor(4.9949e+08, device='cuda:0')
c= tensor(4.9951e+08, device='cuda:0')
c= tensor(4.9951e+08, device='cuda:0')
c= tensor(4.9952e+08, device='cuda:0')
c= tensor(4.9952e+08, device='cuda:0')
c= tensor(4.9952e+08, device='cuda:0')
c= tensor(4.9953e+08, device='cuda:0')
c= tensor(4.9953e+08, device='cuda:0')
c= tensor(4.9954e+08, device='cuda:0')
c= tensor(4.9954e+08, device='cuda:0')
c= tensor(4.9954e+08, device='cuda:0')
c= tensor(4.9954e+08, device='cuda:0')
c= tensor(4.9955e+08, device='cuda:0')
c= tensor(4.9963e+08, device='cuda:0')
c= tensor(4.9963e+08, device='cuda:0')
c= tensor(4.9963e+08, device='cuda:0')
c= tensor(4.9964e+08, device='cuda:0')
c= tensor(4.9966e+08, device='cuda:0')
c= tensor(4.9966e+08, device='cuda:0')
c= tensor(4.9966e+08, device='cuda:0')
c= tensor(4.9967e+08, device='cuda:0')
c= tensor(4.9967e+08, device='cuda:0')
c= tensor(4.9967e+08, device='cuda:0')
c= tensor(4.9968e+08, device='cuda:0')
c= tensor(4.9968e+08, device='cuda:0')
c= tensor(4.9968e+08, device='cuda:0')
c= tensor(4.9969e+08, device='cuda:0')
c= tensor(4.9969e+08, device='cuda:0')
c= tensor(4.9970e+08, device='cuda:0')
c= tensor(4.9970e+08, device='cuda:0')
c= tensor(4.9970e+08, device='cuda:0')
c= tensor(4.9973e+08, device='cuda:0')
c= tensor(4.9973e+08, device='cuda:0')
c= tensor(4.9975e+08, device='cuda:0')
c= tensor(4.9975e+08, device='cuda:0')
c= tensor(4.9976e+08, device='cuda:0')
c= tensor(4.9976e+08, device='cuda:0')
c= tensor(4.9977e+08, device='cuda:0')
c= tensor(4.9983e+08, device='cuda:0')
c= tensor(4.9983e+08, device='cuda:0')
c= tensor(4.9984e+08, device='cuda:0')
c= tensor(4.9985e+08, device='cuda:0')
c= tensor(4.9986e+08, device='cuda:0')
c= tensor(4.9987e+08, device='cuda:0')
c= tensor(4.9988e+08, device='cuda:0')
c= tensor(4.9988e+08, device='cuda:0')
c= tensor(4.9988e+08, device='cuda:0')
c= tensor(4.9988e+08, device='cuda:0')
c= tensor(4.9989e+08, device='cuda:0')
c= tensor(4.9989e+08, device='cuda:0')
c= tensor(4.9989e+08, device='cuda:0')
c= tensor(4.9990e+08, device='cuda:0')
c= tensor(4.9990e+08, device='cuda:0')
c= tensor(4.9991e+08, device='cuda:0')
c= tensor(4.9991e+08, device='cuda:0')
c= tensor(4.9991e+08, device='cuda:0')
c= tensor(4.9993e+08, device='cuda:0')
c= tensor(4.9993e+08, device='cuda:0')
c= tensor(4.9995e+08, device='cuda:0')
c= tensor(4.9996e+08, device='cuda:0')
c= tensor(4.9996e+08, device='cuda:0')
c= tensor(4.9997e+08, device='cuda:0')
c= tensor(4.9997e+08, device='cuda:0')
c= tensor(4.9998e+08, device='cuda:0')
c= tensor(4.9998e+08, device='cuda:0')
c= tensor(4.9998e+08, device='cuda:0')
c= tensor(5.0001e+08, device='cuda:0')
c= tensor(5.0001e+08, device='cuda:0')
c= tensor(5.0002e+08, device='cuda:0')
c= tensor(5.0002e+08, device='cuda:0')
c= tensor(5.0003e+08, device='cuda:0')
c= tensor(5.0003e+08, device='cuda:0')
c= tensor(5.0003e+08, device='cuda:0')
c= tensor(5.0003e+08, device='cuda:0')
c= tensor(5.0004e+08, device='cuda:0')
c= tensor(5.0004e+08, device='cuda:0')
c= tensor(5.0004e+08, device='cuda:0')
c= tensor(5.0004e+08, device='cuda:0')
c= tensor(5.0005e+08, device='cuda:0')
c= tensor(5.0005e+08, device='cuda:0')
c= tensor(5.0008e+08, device='cuda:0')
c= tensor(5.0012e+08, device='cuda:0')
c= tensor(5.0013e+08, device='cuda:0')
c= tensor(5.0013e+08, device='cuda:0')
c= tensor(5.0013e+08, device='cuda:0')
c= tensor(5.0013e+08, device='cuda:0')
c= tensor(5.0013e+08, device='cuda:0')
c= tensor(5.0014e+08, device='cuda:0')
c= tensor(5.0014e+08, device='cuda:0')
c= tensor(5.0015e+08, device='cuda:0')
c= tensor(5.0016e+08, device='cuda:0')
c= tensor(5.0018e+08, device='cuda:0')
c= tensor(5.0019e+08, device='cuda:0')
c= tensor(5.0023e+08, device='cuda:0')
c= tensor(5.0023e+08, device='cuda:0')
c= tensor(5.0024e+08, device='cuda:0')
c= tensor(5.0024e+08, device='cuda:0')
c= tensor(5.0025e+08, device='cuda:0')
c= tensor(5.0025e+08, device='cuda:0')
c= tensor(5.0025e+08, device='cuda:0')
c= tensor(5.0028e+08, device='cuda:0')
c= tensor(5.0028e+08, device='cuda:0')
c= tensor(5.0029e+08, device='cuda:0')
c= tensor(5.0029e+08, device='cuda:0')
c= tensor(5.0030e+08, device='cuda:0')
c= tensor(5.0030e+08, device='cuda:0')
c= tensor(5.0031e+08, device='cuda:0')
c= tensor(5.0031e+08, device='cuda:0')
c= tensor(5.0032e+08, device='cuda:0')
c= tensor(5.0032e+08, device='cuda:0')
c= tensor(5.0033e+08, device='cuda:0')
c= tensor(5.0033e+08, device='cuda:0')
c= tensor(5.0034e+08, device='cuda:0')
c= tensor(5.0035e+08, device='cuda:0')
c= tensor(5.0035e+08, device='cuda:0')
c= tensor(5.0036e+08, device='cuda:0')
c= tensor(5.0036e+08, device='cuda:0')
c= tensor(5.0037e+08, device='cuda:0')
c= tensor(5.0038e+08, device='cuda:0')
c= tensor(5.0038e+08, device='cuda:0')
c= tensor(5.0039e+08, device='cuda:0')
c= tensor(5.0039e+08, device='cuda:0')
c= tensor(5.0040e+08, device='cuda:0')
c= tensor(5.0040e+08, device='cuda:0')
c= tensor(5.0041e+08, device='cuda:0')
c= tensor(5.0041e+08, device='cuda:0')
c= tensor(5.0043e+08, device='cuda:0')
c= tensor(5.0046e+08, device='cuda:0')
c= tensor(5.0046e+08, device='cuda:0')
c= tensor(5.0046e+08, device='cuda:0')
c= tensor(5.0046e+08, device='cuda:0')
c= tensor(5.0047e+08, device='cuda:0')
c= tensor(5.0047e+08, device='cuda:0')
c= tensor(5.0048e+08, device='cuda:0')
c= tensor(5.0049e+08, device='cuda:0')
c= tensor(5.0049e+08, device='cuda:0')
c= tensor(5.0050e+08, device='cuda:0')
c= tensor(5.0050e+08, device='cuda:0')
c= tensor(5.0050e+08, device='cuda:0')
c= tensor(5.0051e+08, device='cuda:0')
c= tensor(5.0052e+08, device='cuda:0')
c= tensor(5.0053e+08, device='cuda:0')
c= tensor(5.0055e+08, device='cuda:0')
c= tensor(5.0055e+08, device='cuda:0')
c= tensor(5.0056e+08, device='cuda:0')
c= tensor(5.0056e+08, device='cuda:0')
c= tensor(5.0057e+08, device='cuda:0')
c= tensor(5.0059e+08, device='cuda:0')
c= tensor(5.0059e+08, device='cuda:0')
c= tensor(5.0060e+08, device='cuda:0')
c= tensor(5.0060e+08, device='cuda:0')
c= tensor(5.0060e+08, device='cuda:0')
c= tensor(5.0060e+08, device='cuda:0')
c= tensor(5.0061e+08, device='cuda:0')
c= tensor(5.0061e+08, device='cuda:0')
c= tensor(5.0061e+08, device='cuda:0')
c= tensor(5.0067e+08, device='cuda:0')
c= tensor(5.0067e+08, device='cuda:0')
c= tensor(5.0067e+08, device='cuda:0')
c= tensor(5.0067e+08, device='cuda:0')
c= tensor(5.0068e+08, device='cuda:0')
c= tensor(5.0068e+08, device='cuda:0')
c= tensor(5.0069e+08, device='cuda:0')
c= tensor(5.0070e+08, device='cuda:0')
c= tensor(5.0070e+08, device='cuda:0')
c= tensor(5.0070e+08, device='cuda:0')
c= tensor(5.0070e+08, device='cuda:0')
c= tensor(5.0070e+08, device='cuda:0')
c= tensor(5.0071e+08, device='cuda:0')
c= tensor(5.0071e+08, device='cuda:0')
c= tensor(5.0071e+08, device='cuda:0')
c= tensor(5.0072e+08, device='cuda:0')
c= tensor(5.0072e+08, device='cuda:0')
c= tensor(5.0073e+08, device='cuda:0')
c= tensor(5.0073e+08, device='cuda:0')
c= tensor(5.0075e+08, device='cuda:0')
c= tensor(5.0075e+08, device='cuda:0')
c= tensor(5.0079e+08, device='cuda:0')
c= tensor(5.0178e+08, device='cuda:0')
c= tensor(5.0183e+08, device='cuda:0')
c= tensor(5.0184e+08, device='cuda:0')
c= tensor(5.0184e+08, device='cuda:0')
c= tensor(5.0186e+08, device='cuda:0')
c= tensor(5.0189e+08, device='cuda:0')
c= tensor(5.0595e+08, device='cuda:0')
c= tensor(5.0596e+08, device='cuda:0')
c= tensor(5.0931e+08, device='cuda:0')
c= tensor(5.1036e+08, device='cuda:0')
c= tensor(5.1047e+08, device='cuda:0')
c= tensor(5.1171e+08, device='cuda:0')
c= tensor(5.1171e+08, device='cuda:0')
c= tensor(5.1171e+08, device='cuda:0')
c= tensor(5.4517e+08, device='cuda:0')
c= tensor(5.6113e+08, device='cuda:0')
c= tensor(5.6114e+08, device='cuda:0')
c= tensor(5.6129e+08, device='cuda:0')
c= tensor(5.7225e+08, device='cuda:0')
c= tensor(5.7258e+08, device='cuda:0')
c= tensor(5.7305e+08, device='cuda:0')
c= tensor(5.7344e+08, device='cuda:0')
c= tensor(5.7364e+08, device='cuda:0')
c= tensor(5.7380e+08, device='cuda:0')
c= tensor(5.7381e+08, device='cuda:0')
c= tensor(5.8075e+08, device='cuda:0')
c= tensor(5.8078e+08, device='cuda:0')
c= tensor(5.8079e+08, device='cuda:0')
c= tensor(5.8091e+08, device='cuda:0')
c= tensor(5.8112e+08, device='cuda:0')
c= tensor(5.9179e+08, device='cuda:0')
c= tensor(5.9214e+08, device='cuda:0')
c= tensor(5.9214e+08, device='cuda:0')
c= tensor(5.9229e+08, device='cuda:0')
c= tensor(5.9231e+08, device='cuda:0')
c= tensor(5.9249e+08, device='cuda:0')
c= tensor(5.9354e+08, device='cuda:0')
c= tensor(5.9684e+08, device='cuda:0')
c= tensor(5.9773e+08, device='cuda:0')
c= tensor(5.9773e+08, device='cuda:0')
c= tensor(5.9774e+08, device='cuda:0')
c= tensor(5.9867e+08, device='cuda:0')
c= tensor(5.9902e+08, device='cuda:0')
c= tensor(5.9927e+08, device='cuda:0')
c= tensor(5.9928e+08, device='cuda:0')
c= tensor(6.1759e+08, device='cuda:0')
c= tensor(6.1762e+08, device='cuda:0')
c= tensor(6.1774e+08, device='cuda:0')
c= tensor(6.2065e+08, device='cuda:0')
c= tensor(6.2066e+08, device='cuda:0')
c= tensor(6.2094e+08, device='cuda:0')
c= tensor(6.2606e+08, device='cuda:0')
c= tensor(6.4822e+08, device='cuda:0')
c= tensor(6.4829e+08, device='cuda:0')
c= tensor(6.4840e+08, device='cuda:0')
c= tensor(6.4842e+08, device='cuda:0')
c= tensor(6.4843e+08, device='cuda:0')
c= tensor(6.4974e+08, device='cuda:0')
c= tensor(6.4977e+08, device='cuda:0')
c= tensor(6.4989e+08, device='cuda:0')
c= tensor(6.6041e+08, device='cuda:0')
c= tensor(6.6060e+08, device='cuda:0')
c= tensor(6.6067e+08, device='cuda:0')
c= tensor(6.6069e+08, device='cuda:0')
c= tensor(6.9268e+08, device='cuda:0')
c= tensor(6.9277e+08, device='cuda:0')
c= tensor(6.9283e+08, device='cuda:0')
c= tensor(6.9293e+08, device='cuda:0')
c= tensor(7.0150e+08, device='cuda:0')
c= tensor(7.0155e+08, device='cuda:0')
c= tensor(7.0218e+08, device='cuda:0')
c= tensor(7.0224e+08, device='cuda:0')
c= tensor(7.0311e+08, device='cuda:0')
c= tensor(7.0351e+08, device='cuda:0')
c= tensor(7.6657e+08, device='cuda:0')
c= tensor(7.6751e+08, device='cuda:0')
c= tensor(7.6753e+08, device='cuda:0')
c= tensor(7.7098e+08, device='cuda:0')
c= tensor(7.7372e+08, device='cuda:0')
c= tensor(7.7373e+08, device='cuda:0')
c= tensor(7.7525e+08, device='cuda:0')
c= tensor(7.7948e+08, device='cuda:0')
c= tensor(8.7214e+08, device='cuda:0')
c= tensor(8.7295e+08, device='cuda:0')
c= tensor(8.7295e+08, device='cuda:0')
c= tensor(8.7298e+08, device='cuda:0')
c= tensor(8.7324e+08, device='cuda:0')
c= tensor(8.7336e+08, device='cuda:0')
c= tensor(8.7339e+08, device='cuda:0')
c= tensor(8.7340e+08, device='cuda:0')
c= tensor(8.7386e+08, device='cuda:0')
c= tensor(8.7560e+08, device='cuda:0')
c= tensor(8.7713e+08, device='cuda:0')
c= tensor(8.7714e+08, device='cuda:0')
c= tensor(8.7823e+08, device='cuda:0')
c= tensor(8.7829e+08, device='cuda:0')
c= tensor(8.7840e+08, device='cuda:0')
c= tensor(8.7843e+08, device='cuda:0')
c= tensor(8.7844e+08, device='cuda:0')
c= tensor(9.0709e+08, device='cuda:0')
c= tensor(9.0753e+08, device='cuda:0')
c= tensor(9.0761e+08, device='cuda:0')
c= tensor(9.0821e+08, device='cuda:0')
c= tensor(9.0822e+08, device='cuda:0')
c= tensor(9.2114e+08, device='cuda:0')
c= tensor(9.2118e+08, device='cuda:0')
c= tensor(9.2328e+08, device='cuda:0')
c= tensor(9.2328e+08, device='cuda:0')
c= tensor(9.2329e+08, device='cuda:0')
c= tensor(9.2329e+08, device='cuda:0')
c= tensor(9.2349e+08, device='cuda:0')
c= tensor(9.2350e+08, device='cuda:0')
c= tensor(9.2394e+08, device='cuda:0')
c= tensor(9.2394e+08, device='cuda:0')
c= tensor(9.2395e+08, device='cuda:0')
c= tensor(9.2947e+08, device='cuda:0')
c= tensor(9.3004e+08, device='cuda:0')
c= tensor(9.3048e+08, device='cuda:0')
c= tensor(9.3418e+08, device='cuda:0')
c= tensor(9.4794e+08, device='cuda:0')
c= tensor(9.4794e+08, device='cuda:0')
c= tensor(9.4795e+08, device='cuda:0')
c= tensor(9.4805e+08, device='cuda:0')
c= tensor(9.4805e+08, device='cuda:0')
c= tensor(9.4806e+08, device='cuda:0')
c= tensor(9.4813e+08, device='cuda:0')
c= tensor(9.4815e+08, device='cuda:0')
c= tensor(9.4816e+08, device='cuda:0')
c= tensor(9.4818e+08, device='cuda:0')
c= tensor(9.4823e+08, device='cuda:0')
c= tensor(1.1095e+09, device='cuda:0')
c= tensor(1.1095e+09, device='cuda:0')
c= tensor(1.1111e+09, device='cuda:0')
c= tensor(1.1112e+09, device='cuda:0')
c= tensor(1.1112e+09, device='cuda:0')
c= tensor(1.1116e+09, device='cuda:0')
c= tensor(1.1815e+09, device='cuda:0')
c= tensor(1.2156e+09, device='cuda:0')
c= tensor(1.2158e+09, device='cuda:0')
c= tensor(1.2160e+09, device='cuda:0')
c= tensor(1.2160e+09, device='cuda:0')
c= tensor(1.2165e+09, device='cuda:0')
c= tensor(1.4653e+09, device='cuda:0')
c= tensor(1.4659e+09, device='cuda:0')
c= tensor(1.4659e+09, device='cuda:0')
c= tensor(1.4683e+09, device='cuda:0')
c= tensor(1.4954e+09, device='cuda:0')
c= tensor(1.4958e+09, device='cuda:0')
c= tensor(1.4958e+09, device='cuda:0')
c= tensor(1.4959e+09, device='cuda:0')
c= tensor(1.4959e+09, device='cuda:0')
c= tensor(1.4960e+09, device='cuda:0')
c= tensor(1.5207e+09, device='cuda:0')
c= tensor(1.5208e+09, device='cuda:0')
c= tensor(1.5208e+09, device='cuda:0')
c= tensor(1.5213e+09, device='cuda:0')
c= tensor(1.5215e+09, device='cuda:0')
c= tensor(1.5215e+09, device='cuda:0')
c= tensor(1.5236e+09, device='cuda:0')
c= tensor(1.5257e+09, device='cuda:0')
c= tensor(1.5281e+09, device='cuda:0')
c= tensor(1.5313e+09, device='cuda:0')
c= tensor(1.5358e+09, device='cuda:0')
c= tensor(1.5359e+09, device='cuda:0')
c= tensor(1.5366e+09, device='cuda:0')
c= tensor(1.5378e+09, device='cuda:0')
c= tensor(1.5444e+09, device='cuda:0')
c= tensor(1.5444e+09, device='cuda:0')
c= tensor(1.5667e+09, device='cuda:0')
c= tensor(1.5803e+09, device='cuda:0')
c= tensor(1.5826e+09, device='cuda:0')
c= tensor(1.5833e+09, device='cuda:0')
c= tensor(1.5852e+09, device='cuda:0')
c= tensor(1.5853e+09, device='cuda:0')
c= tensor(1.5853e+09, device='cuda:0')
c= tensor(1.5893e+09, device='cuda:0')
c= tensor(1.5953e+09, device='cuda:0')
c= tensor(1.5980e+09, device='cuda:0')
c= tensor(1.6067e+09, device='cuda:0')
c= tensor(1.6097e+09, device='cuda:0')
c= tensor(1.6117e+09, device='cuda:0')
c= tensor(1.6118e+09, device='cuda:0')
c= tensor(1.6250e+09, device='cuda:0')
c= tensor(1.6250e+09, device='cuda:0')
c= tensor(1.6250e+09, device='cuda:0')
c= tensor(1.6333e+09, device='cuda:0')
c= tensor(1.6333e+09, device='cuda:0')
c= tensor(1.6333e+09, device='cuda:0')
c= tensor(1.6335e+09, device='cuda:0')
c= tensor(1.6408e+09, device='cuda:0')
c= tensor(1.6410e+09, device='cuda:0')
c= tensor(1.6420e+09, device='cuda:0')
c= tensor(1.6420e+09, device='cuda:0')
c= tensor(1.6420e+09, device='cuda:0')
c= tensor(1.6420e+09, device='cuda:0')
c= tensor(1.6424e+09, device='cuda:0')
c= tensor(1.6426e+09, device='cuda:0')
c= tensor(1.6450e+09, device='cuda:0')
c= tensor(1.6450e+09, device='cuda:0')
c= tensor(1.6464e+09, device='cuda:0')
c= tensor(1.6464e+09, device='cuda:0')
c= tensor(1.6466e+09, device='cuda:0')
c= tensor(1.6475e+09, device='cuda:0')
c= tensor(1.6490e+09, device='cuda:0')
c= tensor(1.6491e+09, device='cuda:0')
c= tensor(1.6494e+09, device='cuda:0')
c= tensor(1.6495e+09, device='cuda:0')
c= tensor(1.6496e+09, device='cuda:0')
c= tensor(1.6503e+09, device='cuda:0')
c= tensor(1.6648e+09, device='cuda:0')
c= tensor(1.6649e+09, device='cuda:0')
c= tensor(1.6649e+09, device='cuda:0')
c= tensor(1.6708e+09, device='cuda:0')
c= tensor(1.6708e+09, device='cuda:0')
c= tensor(1.7333e+09, device='cuda:0')
c= tensor(1.7333e+09, device='cuda:0')
c= tensor(1.7363e+09, device='cuda:0')
c= tensor(1.7447e+09, device='cuda:0')
c= tensor(1.7447e+09, device='cuda:0')
c= tensor(1.7479e+09, device='cuda:0')
c= tensor(1.7482e+09, device='cuda:0')
c= tensor(1.7672e+09, device='cuda:0')
c= tensor(1.7672e+09, device='cuda:0')
c= tensor(1.7673e+09, device='cuda:0')
c= tensor(1.7674e+09, device='cuda:0')
c= tensor(1.7674e+09, device='cuda:0')
c= tensor(1.7674e+09, device='cuda:0')
c= tensor(1.7678e+09, device='cuda:0')
c= tensor(1.7681e+09, device='cuda:0')
c= tensor(1.7704e+09, device='cuda:0')
c= tensor(1.7705e+09, device='cuda:0')
c= tensor(1.7705e+09, device='cuda:0')
c= tensor(1.7705e+09, device='cuda:0')
c= tensor(1.7733e+09, device='cuda:0')
c= tensor(1.7735e+09, device='cuda:0')
c= tensor(1.7778e+09, device='cuda:0')
c= tensor(1.7785e+09, device='cuda:0')
c= tensor(1.7785e+09, device='cuda:0')
c= tensor(1.7785e+09, device='cuda:0')
c= tensor(1.7785e+09, device='cuda:0')
c= tensor(1.8172e+09, device='cuda:0')
c= tensor(1.8172e+09, device='cuda:0')
c= tensor(1.8173e+09, device='cuda:0')
c= tensor(1.8199e+09, device='cuda:0')
c= tensor(1.8201e+09, device='cuda:0')
c= tensor(1.8201e+09, device='cuda:0')
c= tensor(1.8201e+09, device='cuda:0')
c= tensor(1.8347e+09, device='cuda:0')
c= tensor(1.8370e+09, device='cuda:0')
c= tensor(1.8372e+09, device='cuda:0')
c= tensor(1.8372e+09, device='cuda:0')
c= tensor(1.8508e+09, device='cuda:0')
c= tensor(1.8526e+09, device='cuda:0')
c= tensor(1.8647e+09, device='cuda:0')
c= tensor(1.8666e+09, device='cuda:0')
c= tensor(1.8666e+09, device='cuda:0')
c= tensor(1.8705e+09, device='cuda:0')
c= tensor(1.8705e+09, device='cuda:0')
c= tensor(1.8712e+09, device='cuda:0')
c= tensor(1.8712e+09, device='cuda:0')
c= tensor(1.8712e+09, device='cuda:0')
c= tensor(1.8714e+09, device='cuda:0')
c= tensor(1.8716e+09, device='cuda:0')
c= tensor(1.8716e+09, device='cuda:0')
c= tensor(1.8717e+09, device='cuda:0')
c= tensor(1.8719e+09, device='cuda:0')
c= tensor(1.8857e+09, device='cuda:0')
c= tensor(1.8857e+09, device='cuda:0')
c= tensor(1.8858e+09, device='cuda:0')
c= tensor(1.8858e+09, device='cuda:0')
c= tensor(1.8860e+09, device='cuda:0')
c= tensor(1.8860e+09, device='cuda:0')
c= tensor(1.8860e+09, device='cuda:0')
c= tensor(1.8861e+09, device='cuda:0')
c= tensor(1.8886e+09, device='cuda:0')
c= tensor(1.8886e+09, device='cuda:0')
c= tensor(1.8886e+09, device='cuda:0')
c= tensor(1.8886e+09, device='cuda:0')
c= tensor(1.9091e+09, device='cuda:0')
c= tensor(2.2179e+09, device='cuda:0')
c= tensor(2.2180e+09, device='cuda:0')
c= tensor(2.2180e+09, device='cuda:0')
c= tensor(2.2212e+09, device='cuda:0')
c= tensor(2.2226e+09, device='cuda:0')
c= tensor(2.2226e+09, device='cuda:0')
c= tensor(2.2229e+09, device='cuda:0')
c= tensor(2.2230e+09, device='cuda:0')
c= tensor(2.3322e+09, device='cuda:0')
c= tensor(2.4388e+09, device='cuda:0')
c= tensor(2.4438e+09, device='cuda:0')
c= tensor(2.4439e+09, device='cuda:0')
c= tensor(2.4439e+09, device='cuda:0')
c= tensor(2.4439e+09, device='cuda:0')
c= tensor(2.4451e+09, device='cuda:0')
c= tensor(2.4452e+09, device='cuda:0')
c= tensor(2.4455e+09, device='cuda:0')
c= tensor(2.4514e+09, device='cuda:0')
c= tensor(2.4752e+09, device='cuda:0')
c= tensor(2.4752e+09, device='cuda:0')
c= tensor(2.4752e+09, device='cuda:0')
c= tensor(2.4765e+09, device='cuda:0')
c= tensor(2.4910e+09, device='cuda:0')
c= tensor(2.4922e+09, device='cuda:0')
c= tensor(2.4922e+09, device='cuda:0')
c= tensor(2.4922e+09, device='cuda:0')
c= tensor(2.4923e+09, device='cuda:0')
c= tensor(2.4923e+09, device='cuda:0')
c= tensor(2.4934e+09, device='cuda:0')
c= tensor(2.4934e+09, device='cuda:0')
c= tensor(2.4934e+09, device='cuda:0')
c= tensor(2.4935e+09, device='cuda:0')
c= tensor(2.4935e+09, device='cuda:0')
c= tensor(2.4935e+09, device='cuda:0')
c= tensor(2.4982e+09, device='cuda:0')
c= tensor(2.5248e+09, device='cuda:0')
c= tensor(2.5278e+09, device='cuda:0')
c= tensor(2.5281e+09, device='cuda:0')
c= tensor(2.5282e+09, device='cuda:0')
c= tensor(2.5282e+09, device='cuda:0')
c= tensor(2.5283e+09, device='cuda:0')
c= tensor(2.5313e+09, device='cuda:0')
c= tensor(2.5314e+09, device='cuda:0')
c= tensor(2.5328e+09, device='cuda:0')
c= tensor(2.5330e+09, device='cuda:0')
c= tensor(2.7077e+09, device='cuda:0')
c= tensor(2.7078e+09, device='cuda:0')
c= tensor(2.7085e+09, device='cuda:0')
c= tensor(2.7185e+09, device='cuda:0')
c= tensor(2.7204e+09, device='cuda:0')
c= tensor(2.7206e+09, device='cuda:0')
c= tensor(2.7422e+09, device='cuda:0')
c= tensor(2.7434e+09, device='cuda:0')
c= tensor(2.7435e+09, device='cuda:0')
c= tensor(2.7436e+09, device='cuda:0')
c= tensor(2.7437e+09, device='cuda:0')
c= tensor(2.7438e+09, device='cuda:0')
c= tensor(2.7447e+09, device='cuda:0')
c= tensor(2.7980e+09, device='cuda:0')
c= tensor(2.8060e+09, device='cuda:0')
c= tensor(2.8102e+09, device='cuda:0')
c= tensor(2.8103e+09, device='cuda:0')
c= tensor(2.8105e+09, device='cuda:0')
c= tensor(2.8107e+09, device='cuda:0')
c= tensor(2.8107e+09, device='cuda:0')
c= tensor(2.8541e+09, device='cuda:0')
c= tensor(2.8698e+09, device='cuda:0')
c= tensor(2.8699e+09, device='cuda:0')
c= tensor(3.2571e+09, device='cuda:0')
c= tensor(3.2757e+09, device='cuda:0')
c= tensor(3.2760e+09, device='cuda:0')
c= tensor(3.2760e+09, device='cuda:0')
c= tensor(3.2767e+09, device='cuda:0')
c= tensor(3.2783e+09, device='cuda:0')
c= tensor(3.2783e+09, device='cuda:0')
c= tensor(3.3357e+09, device='cuda:0')
c= tensor(3.3361e+09, device='cuda:0')
c= tensor(3.3363e+09, device='cuda:0')
c= tensor(3.3364e+09, device='cuda:0')
c= tensor(3.3364e+09, device='cuda:0')
c= tensor(3.3364e+09, device='cuda:0')
c= tensor(3.3365e+09, device='cuda:0')
c= tensor(3.3366e+09, device='cuda:0')
c= tensor(3.3366e+09, device='cuda:0')
c= tensor(4.0886e+09, device='cuda:0')
c= tensor(4.0897e+09, device='cuda:0')
c= tensor(4.0911e+09, device='cuda:0')
c= tensor(4.0911e+09, device='cuda:0')
c= tensor(4.0911e+09, device='cuda:0')
c= tensor(4.0911e+09, device='cuda:0')
c= tensor(4.1004e+09, device='cuda:0')
c= tensor(4.1073e+09, device='cuda:0')
c= tensor(4.2192e+09, device='cuda:0')
c= tensor(4.2194e+09, device='cuda:0')
c= tensor(4.2246e+09, device='cuda:0')
c= tensor(4.2249e+09, device='cuda:0')
c= tensor(4.2274e+09, device='cuda:0')
c= tensor(4.2342e+09, device='cuda:0')
c= tensor(4.2343e+09, device='cuda:0')
c= tensor(4.2343e+09, device='cuda:0')
c= tensor(4.2350e+09, device='cuda:0')
c= tensor(4.2352e+09, device='cuda:0')
c= tensor(4.2368e+09, device='cuda:0')
c= tensor(4.2461e+09, device='cuda:0')
c= tensor(4.2463e+09, device='cuda:0')
c= tensor(4.2478e+09, device='cuda:0')
c= tensor(4.2479e+09, device='cuda:0')
c= tensor(4.2528e+09, device='cuda:0')
c= tensor(4.2634e+09, device='cuda:0')
c= tensor(4.2636e+09, device='cuda:0')
c= tensor(4.2636e+09, device='cuda:0')
c= tensor(4.2875e+09, device='cuda:0')
c= tensor(4.2894e+09, device='cuda:0')
c= tensor(4.3496e+09, device='cuda:0')
c= tensor(4.3504e+09, device='cuda:0')
c= tensor(4.3516e+09, device='cuda:0')
c= tensor(4.3519e+09, device='cuda:0')
c= tensor(4.3534e+09, device='cuda:0')
c= tensor(4.3892e+09, device='cuda:0')
c= tensor(4.3895e+09, device='cuda:0')
c= tensor(4.3895e+09, device='cuda:0')
c= tensor(4.3897e+09, device='cuda:0')
c= tensor(4.3907e+09, device='cuda:0')
c= tensor(4.3933e+09, device='cuda:0')
c= tensor(4.3940e+09, device='cuda:0')
c= tensor(4.3940e+09, device='cuda:0')
c= tensor(4.3942e+09, device='cuda:0')
c= tensor(4.3998e+09, device='cuda:0')
c= tensor(4.4004e+09, device='cuda:0')
c= tensor(4.4012e+09, device='cuda:0')
c= tensor(4.4026e+09, device='cuda:0')
c= tensor(4.4036e+09, device='cuda:0')
c= tensor(4.4037e+09, device='cuda:0')
c= tensor(4.4037e+09, device='cuda:0')
c= tensor(4.4037e+09, device='cuda:0')
c= tensor(4.4062e+09, device='cuda:0')
c= tensor(4.4063e+09, device='cuda:0')
c= tensor(4.4070e+09, device='cuda:0')
c= tensor(4.4070e+09, device='cuda:0')
c= tensor(4.4070e+09, device='cuda:0')
c= tensor(4.4093e+09, device='cuda:0')
c= tensor(4.4105e+09, device='cuda:0')
c= tensor(4.4106e+09, device='cuda:0')
c= tensor(4.4106e+09, device='cuda:0')
c= tensor(4.4106e+09, device='cuda:0')
c= tensor(4.4107e+09, device='cuda:0')
c= tensor(4.4109e+09, device='cuda:0')
c= tensor(4.4129e+09, device='cuda:0')
c= tensor(4.4129e+09, device='cuda:0')
c= tensor(4.4132e+09, device='cuda:0')
c= tensor(4.4133e+09, device='cuda:0')
c= tensor(4.4134e+09, device='cuda:0')
c= tensor(4.4220e+09, device='cuda:0')
c= tensor(4.4221e+09, device='cuda:0')
c= tensor(4.4255e+09, device='cuda:0')
c= tensor(4.4413e+09, device='cuda:0')
c= tensor(4.4416e+09, device='cuda:0')
c= tensor(4.4426e+09, device='cuda:0')
c= tensor(4.4441e+09, device='cuda:0')
c= tensor(4.4441e+09, device='cuda:0')
c= tensor(4.4444e+09, device='cuda:0')
c= tensor(4.4444e+09, device='cuda:0')
c= tensor(4.4526e+09, device='cuda:0')
c= tensor(4.4540e+09, device='cuda:0')
c= tensor(4.4541e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4564e+09, device='cuda:0')
c= tensor(4.4566e+09, device='cuda:0')
c= tensor(4.4566e+09, device='cuda:0')
c= tensor(4.4567e+09, device='cuda:0')
c= tensor(4.4598e+09, device='cuda:0')
c= tensor(4.5212e+09, device='cuda:0')
c= tensor(4.5230e+09, device='cuda:0')
c= tensor(4.5231e+09, device='cuda:0')
c= tensor(4.5232e+09, device='cuda:0')
c= tensor(4.5475e+09, device='cuda:0')
c= tensor(4.5476e+09, device='cuda:0')
c= tensor(4.5476e+09, device='cuda:0')
c= tensor(4.5482e+09, device='cuda:0')
c= tensor(4.5500e+09, device='cuda:0')
c= tensor(4.5500e+09, device='cuda:0')
c= tensor(4.5501e+09, device='cuda:0')
c= tensor(4.5504e+09, device='cuda:0')
memory (bytes)
4520112128
time for making loss 2 is 14.086746215820312
p0 True
it  0 : 1811803648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 31% |
shape of L is 
torch.Size([])
memory (bytes)
4520439808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 20% |
memory (bytes)
4520939520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 21% |
error is  55461650000.0
relative error loss 12.188395
shape of L is 
torch.Size([])
memory (bytes)
4697948160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 21% |
memory (bytes)
4697997312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  55461368000.0
relative error loss 12.188332
shape of L is 
torch.Size([])
memory (bytes)
4701028352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 21% |
memory (bytes)
4701069312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  55460585000.0
relative error loss 12.18816
shape of L is 
torch.Size([])
memory (bytes)
4703199232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 21% |
memory (bytes)
4703199232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  55455790000.0
relative error loss 12.187106
shape of L is 
torch.Size([])
memory (bytes)
4705308672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 21% |
memory (bytes)
4705353728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  55424418000.0
relative error loss 12.180212
shape of L is 
torch.Size([])
memory (bytes)
4707459072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4707487744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  55256990000.0
relative error loss 12.143417
shape of L is 
torch.Size([])
memory (bytes)
4709597184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 21% |
memory (bytes)
4709597184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  53443867000.0
relative error loss 11.744961
shape of L is 
torch.Size([])
memory (bytes)
4711702528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4711702528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 21% |
error is  45110776000.0
relative error loss 9.913659
shape of L is 
torch.Size([])
memory (bytes)
4713799680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
4713828352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  13270477000.0
relative error loss 2.916354
shape of L is 
torch.Size([])
memory (bytes)
4715945984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4715974656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  7700637700.0
relative error loss 1.6923119
time to take a step is 236.6475315093994
it  1 : 2166855168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
4718092288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 21% |
memory (bytes)
4718092288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 21% |
error is  7700637700.0
relative error loss 1.6923119
shape of L is 
torch.Size([])
memory (bytes)
4720214016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4720242688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  5395324000.0
relative error loss 1.1856902
shape of L is 
torch.Size([])
memory (bytes)
4722352128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4722380800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  4578201600.0
relative error loss 1.0061173
shape of L is 
torch.Size([])
memory (bytes)
4724482048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 21% |
memory (bytes)
4724502528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  32160164000.0
relative error loss 7.0676
shape of L is 
torch.Size([])
memory (bytes)
4726624256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4726652928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  4399916500.0
relative error loss 0.9669369
shape of L is 
torch.Size([])
memory (bytes)
4728754176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 21% |
memory (bytes)
4728782848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 21% |
error is  4287160300.0
relative error loss 0.9421573
shape of L is 
torch.Size([])
memory (bytes)
4730900480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4730900480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 21% |
error is  4126939000.0
relative error loss 0.90694666
shape of L is 
torch.Size([])
memory (bytes)
4732985344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
4732985344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  3952100600.0
relative error loss 0.8685237
shape of L is 
torch.Size([])
memory (bytes)
4735086592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
4735115264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  3836844800.0
relative error loss 0.84319484
shape of L is 
torch.Size([])
memory (bytes)
4737191936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4737220608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  4040047900.0
relative error loss 0.88785124
shape of L is 
torch.Size([])
memory (bytes)
4739309568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4739338240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  3633442300.0
relative error loss 0.7984946
time to take a step is 248.56910824775696
it  2 : 2268299264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
4741431296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 21% |
memory (bytes)
4741459968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  3633442300.0
relative error loss 0.7984946
shape of L is 
torch.Size([])
memory (bytes)
4743573504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4743573504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 21% |
error is  3424184300.0
relative error loss 0.7525075
shape of L is 
torch.Size([])
memory (bytes)
4745506816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4745506816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  3132642800.0
relative error loss 0.6884376
shape of L is 
torch.Size([])
memory (bytes)
4747812864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4747812864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  2898511400.0
relative error loss 0.6369843
shape of L is 
torch.Size([])
memory (bytes)
4749914112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
4749942784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  2730318800.0
relative error loss 0.60002184
shape of L is 
torch.Size([])
memory (bytes)
4752044032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4752072704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  2449686000.0
relative error loss 0.5383493
shape of L is 
torch.Size([])
memory (bytes)
4754182144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
4754210816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  2219733200.0
relative error loss 0.48781425
shape of L is 
torch.Size([])
memory (bytes)
4756324352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4756324352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  1990881500.0
relative error loss 0.43752122
shape of L is 
torch.Size([])
memory (bytes)
4758458368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4758458368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  1808022800.0
relative error loss 0.3973357
shape of L is 
torch.Size([])
memory (bytes)
4760637440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4760637440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  1625776900.0
relative error loss 0.35728487
time to take a step is 219.68912363052368
it  3 : 2268298752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
4762427392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4762427392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 21% |
error is  1625776900.0
relative error loss 0.35728487
shape of L is 
torch.Size([])
memory (bytes)
4764700672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4764700672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  1837875200.0
relative error loss 0.40389615
shape of L is 
torch.Size([])
memory (bytes)
4767064064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4767092736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  1482546200.0
relative error loss 0.32580814
shape of L is 
torch.Size([])
memory (bytes)
4769202176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4769230848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  1349615900.0
relative error loss 0.29659504
shape of L is 
torch.Size([])
memory (bytes)
4771336192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4771364864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  1262792000.0
relative error loss 0.2775144
shape of L is 
torch.Size([])
memory (bytes)
4773490688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
4773490688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  1136080900.0
relative error loss 0.24966805
shape of L is 
torch.Size([])
memory (bytes)
4775620608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 21% |
memory (bytes)
4775649280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  1070679550.0
relative error loss 0.23529528
shape of L is 
torch.Size([])
memory (bytes)
4777758720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4777787392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  995559400.0
relative error loss 0.21878669
shape of L is 
torch.Size([])
memory (bytes)
4779909120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 21% |
memory (bytes)
4779925504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  959909400.0
relative error loss 0.21095213
shape of L is 
torch.Size([])
memory (bytes)
4782075904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4782075904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  1281801000.0
relative error loss 0.28169185
shape of L is 
torch.Size([])
memory (bytes)
4784189440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
4784218112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  889393400.0
relative error loss 0.19545537
time to take a step is 241.2667269706726
c= tensor(1091.8049, device='cuda:0')
c= tensor(112519.6719, device='cuda:0')
c= tensor(117344.9609, device='cuda:0')
c= tensor(126535.7656, device='cuda:0')
c= tensor(284913.8125, device='cuda:0')
c= tensor(479531.2188, device='cuda:0')
c= tensor(1148577., device='cuda:0')
c= tensor(1433327.3750, device='cuda:0')
c= tensor(1971488.5000, device='cuda:0')
c= tensor(13578008., device='cuda:0')
c= tensor(13729682., device='cuda:0')
c= tensor(14908808., device='cuda:0')
c= tensor(14917908., device='cuda:0')
c= tensor(26010304., device='cuda:0')
c= tensor(26150408., device='cuda:0')
c= tensor(26297264., device='cuda:0')
c= tensor(26809028., device='cuda:0')
c= tensor(27848012., device='cuda:0')
c= tensor(31170636., device='cuda:0')
c= tensor(35240076., device='cuda:0')
c= tensor(35293952., device='cuda:0')
c= tensor(42166252., device='cuda:0')
c= tensor(42209944., device='cuda:0')
c= tensor(42352276., device='cuda:0')
c= tensor(42420640., device='cuda:0')
c= tensor(43183104., device='cuda:0')
c= tensor(44228424., device='cuda:0')
c= tensor(44254900., device='cuda:0')
c= tensor(59359356., device='cuda:0')
c= tensor(1.5162e+08, device='cuda:0')
c= tensor(1.5165e+08, device='cuda:0')
c= tensor(4.6328e+08, device='cuda:0')
c= tensor(4.6334e+08, device='cuda:0')
c= tensor(4.6344e+08, device='cuda:0')
c= tensor(4.6347e+08, device='cuda:0')
c= tensor(4.9770e+08, device='cuda:0')
c= tensor(4.9925e+08, device='cuda:0')
c= tensor(4.9925e+08, device='cuda:0')
c= tensor(4.9926e+08, device='cuda:0')
c= tensor(4.9926e+08, device='cuda:0')
c= tensor(4.9927e+08, device='cuda:0')
c= tensor(4.9927e+08, device='cuda:0')
c= tensor(4.9927e+08, device='cuda:0')
c= tensor(4.9928e+08, device='cuda:0')
c= tensor(4.9928e+08, device='cuda:0')
c= tensor(4.9928e+08, device='cuda:0')
c= tensor(4.9929e+08, device='cuda:0')
c= tensor(4.9930e+08, device='cuda:0')
c= tensor(4.9930e+08, device='cuda:0')
c= tensor(4.9933e+08, device='cuda:0')
c= tensor(4.9938e+08, device='cuda:0')
c= tensor(4.9939e+08, device='cuda:0')
c= tensor(4.9941e+08, device='cuda:0')
c= tensor(4.9942e+08, device='cuda:0')
c= tensor(4.9943e+08, device='cuda:0')
c= tensor(4.9944e+08, device='cuda:0')
c= tensor(4.9944e+08, device='cuda:0')
c= tensor(4.9945e+08, device='cuda:0')
c= tensor(4.9945e+08, device='cuda:0')
c= tensor(4.9946e+08, device='cuda:0')
c= tensor(4.9947e+08, device='cuda:0')
c= tensor(4.9947e+08, device='cuda:0')
c= tensor(4.9949e+08, device='cuda:0')
c= tensor(4.9951e+08, device='cuda:0')
c= tensor(4.9951e+08, device='cuda:0')
c= tensor(4.9952e+08, device='cuda:0')
c= tensor(4.9952e+08, device='cuda:0')
c= tensor(4.9952e+08, device='cuda:0')
c= tensor(4.9953e+08, device='cuda:0')
c= tensor(4.9953e+08, device='cuda:0')
c= tensor(4.9954e+08, device='cuda:0')
c= tensor(4.9954e+08, device='cuda:0')
c= tensor(4.9954e+08, device='cuda:0')
c= tensor(4.9954e+08, device='cuda:0')
c= tensor(4.9955e+08, device='cuda:0')
c= tensor(4.9963e+08, device='cuda:0')
c= tensor(4.9963e+08, device='cuda:0')
c= tensor(4.9963e+08, device='cuda:0')
c= tensor(4.9964e+08, device='cuda:0')
c= tensor(4.9966e+08, device='cuda:0')
c= tensor(4.9966e+08, device='cuda:0')
c= tensor(4.9966e+08, device='cuda:0')
c= tensor(4.9967e+08, device='cuda:0')
c= tensor(4.9967e+08, device='cuda:0')
c= tensor(4.9967e+08, device='cuda:0')
c= tensor(4.9968e+08, device='cuda:0')
c= tensor(4.9968e+08, device='cuda:0')
c= tensor(4.9968e+08, device='cuda:0')
c= tensor(4.9969e+08, device='cuda:0')
c= tensor(4.9969e+08, device='cuda:0')
c= tensor(4.9970e+08, device='cuda:0')
c= tensor(4.9970e+08, device='cuda:0')
c= tensor(4.9970e+08, device='cuda:0')
c= tensor(4.9973e+08, device='cuda:0')
c= tensor(4.9973e+08, device='cuda:0')
c= tensor(4.9975e+08, device='cuda:0')
c= tensor(4.9975e+08, device='cuda:0')
c= tensor(4.9976e+08, device='cuda:0')
c= tensor(4.9976e+08, device='cuda:0')
c= tensor(4.9977e+08, device='cuda:0')
c= tensor(4.9983e+08, device='cuda:0')
c= tensor(4.9983e+08, device='cuda:0')
c= tensor(4.9984e+08, device='cuda:0')
c= tensor(4.9985e+08, device='cuda:0')
c= tensor(4.9986e+08, device='cuda:0')
c= tensor(4.9987e+08, device='cuda:0')
c= tensor(4.9988e+08, device='cuda:0')
c= tensor(4.9988e+08, device='cuda:0')
c= tensor(4.9988e+08, device='cuda:0')
c= tensor(4.9988e+08, device='cuda:0')
c= tensor(4.9989e+08, device='cuda:0')
c= tensor(4.9989e+08, device='cuda:0')
c= tensor(4.9989e+08, device='cuda:0')
c= tensor(4.9990e+08, device='cuda:0')
c= tensor(4.9990e+08, device='cuda:0')
c= tensor(4.9991e+08, device='cuda:0')
c= tensor(4.9991e+08, device='cuda:0')
c= tensor(4.9991e+08, device='cuda:0')
c= tensor(4.9993e+08, device='cuda:0')
c= tensor(4.9993e+08, device='cuda:0')
c= tensor(4.9995e+08, device='cuda:0')
c= tensor(4.9996e+08, device='cuda:0')
c= tensor(4.9996e+08, device='cuda:0')
c= tensor(4.9997e+08, device='cuda:0')
c= tensor(4.9997e+08, device='cuda:0')
c= tensor(4.9998e+08, device='cuda:0')
c= tensor(4.9998e+08, device='cuda:0')
c= tensor(4.9998e+08, device='cuda:0')
c= tensor(5.0001e+08, device='cuda:0')
c= tensor(5.0001e+08, device='cuda:0')
c= tensor(5.0002e+08, device='cuda:0')
c= tensor(5.0002e+08, device='cuda:0')
c= tensor(5.0003e+08, device='cuda:0')
c= tensor(5.0003e+08, device='cuda:0')
c= tensor(5.0003e+08, device='cuda:0')
c= tensor(5.0003e+08, device='cuda:0')
c= tensor(5.0004e+08, device='cuda:0')
c= tensor(5.0004e+08, device='cuda:0')
c= tensor(5.0004e+08, device='cuda:0')
c= tensor(5.0004e+08, device='cuda:0')
c= tensor(5.0005e+08, device='cuda:0')
c= tensor(5.0005e+08, device='cuda:0')
c= tensor(5.0008e+08, device='cuda:0')
c= tensor(5.0012e+08, device='cuda:0')
c= tensor(5.0013e+08, device='cuda:0')
c= tensor(5.0013e+08, device='cuda:0')
c= tensor(5.0013e+08, device='cuda:0')
c= tensor(5.0013e+08, device='cuda:0')
c= tensor(5.0013e+08, device='cuda:0')
c= tensor(5.0014e+08, device='cuda:0')
c= tensor(5.0014e+08, device='cuda:0')
c= tensor(5.0015e+08, device='cuda:0')
c= tensor(5.0016e+08, device='cuda:0')
c= tensor(5.0018e+08, device='cuda:0')
c= tensor(5.0019e+08, device='cuda:0')
c= tensor(5.0023e+08, device='cuda:0')
c= tensor(5.0023e+08, device='cuda:0')
c= tensor(5.0024e+08, device='cuda:0')
c= tensor(5.0024e+08, device='cuda:0')
c= tensor(5.0025e+08, device='cuda:0')
c= tensor(5.0025e+08, device='cuda:0')
c= tensor(5.0025e+08, device='cuda:0')
c= tensor(5.0028e+08, device='cuda:0')
c= tensor(5.0028e+08, device='cuda:0')
c= tensor(5.0029e+08, device='cuda:0')
c= tensor(5.0029e+08, device='cuda:0')
c= tensor(5.0030e+08, device='cuda:0')
c= tensor(5.0030e+08, device='cuda:0')
c= tensor(5.0031e+08, device='cuda:0')
c= tensor(5.0031e+08, device='cuda:0')
c= tensor(5.0032e+08, device='cuda:0')
c= tensor(5.0032e+08, device='cuda:0')
c= tensor(5.0033e+08, device='cuda:0')
c= tensor(5.0033e+08, device='cuda:0')
c= tensor(5.0034e+08, device='cuda:0')
c= tensor(5.0035e+08, device='cuda:0')
c= tensor(5.0035e+08, device='cuda:0')
c= tensor(5.0036e+08, device='cuda:0')
c= tensor(5.0036e+08, device='cuda:0')
c= tensor(5.0037e+08, device='cuda:0')
c= tensor(5.0038e+08, device='cuda:0')
c= tensor(5.0038e+08, device='cuda:0')
c= tensor(5.0039e+08, device='cuda:0')
c= tensor(5.0039e+08, device='cuda:0')
c= tensor(5.0040e+08, device='cuda:0')
c= tensor(5.0040e+08, device='cuda:0')
c= tensor(5.0041e+08, device='cuda:0')
c= tensor(5.0041e+08, device='cuda:0')
c= tensor(5.0043e+08, device='cuda:0')
c= tensor(5.0046e+08, device='cuda:0')
c= tensor(5.0046e+08, device='cuda:0')
c= tensor(5.0046e+08, device='cuda:0')
c= tensor(5.0046e+08, device='cuda:0')
c= tensor(5.0047e+08, device='cuda:0')
c= tensor(5.0047e+08, device='cuda:0')
c= tensor(5.0048e+08, device='cuda:0')
c= tensor(5.0049e+08, device='cuda:0')
c= tensor(5.0049e+08, device='cuda:0')
c= tensor(5.0050e+08, device='cuda:0')
c= tensor(5.0050e+08, device='cuda:0')
c= tensor(5.0050e+08, device='cuda:0')
c= tensor(5.0051e+08, device='cuda:0')
c= tensor(5.0052e+08, device='cuda:0')
c= tensor(5.0053e+08, device='cuda:0')
c= tensor(5.0055e+08, device='cuda:0')
c= tensor(5.0055e+08, device='cuda:0')
c= tensor(5.0056e+08, device='cuda:0')
c= tensor(5.0056e+08, device='cuda:0')
c= tensor(5.0057e+08, device='cuda:0')
c= tensor(5.0059e+08, device='cuda:0')
c= tensor(5.0059e+08, device='cuda:0')
c= tensor(5.0060e+08, device='cuda:0')
c= tensor(5.0060e+08, device='cuda:0')
c= tensor(5.0060e+08, device='cuda:0')
c= tensor(5.0060e+08, device='cuda:0')
c= tensor(5.0061e+08, device='cuda:0')
c= tensor(5.0061e+08, device='cuda:0')
c= tensor(5.0061e+08, device='cuda:0')
c= tensor(5.0067e+08, device='cuda:0')
c= tensor(5.0067e+08, device='cuda:0')
c= tensor(5.0067e+08, device='cuda:0')
c= tensor(5.0067e+08, device='cuda:0')
c= tensor(5.0068e+08, device='cuda:0')
c= tensor(5.0068e+08, device='cuda:0')
c= tensor(5.0069e+08, device='cuda:0')
c= tensor(5.0070e+08, device='cuda:0')
c= tensor(5.0070e+08, device='cuda:0')
c= tensor(5.0070e+08, device='cuda:0')
c= tensor(5.0070e+08, device='cuda:0')
c= tensor(5.0070e+08, device='cuda:0')
c= tensor(5.0071e+08, device='cuda:0')
c= tensor(5.0071e+08, device='cuda:0')
c= tensor(5.0071e+08, device='cuda:0')
c= tensor(5.0072e+08, device='cuda:0')
c= tensor(5.0072e+08, device='cuda:0')
c= tensor(5.0073e+08, device='cuda:0')
c= tensor(5.0073e+08, device='cuda:0')
c= tensor(5.0075e+08, device='cuda:0')
c= tensor(5.0075e+08, device='cuda:0')
c= tensor(5.0079e+08, device='cuda:0')
c= tensor(5.0178e+08, device='cuda:0')
c= tensor(5.0183e+08, device='cuda:0')
c= tensor(5.0184e+08, device='cuda:0')
c= tensor(5.0184e+08, device='cuda:0')
c= tensor(5.0186e+08, device='cuda:0')
c= tensor(5.0189e+08, device='cuda:0')
c= tensor(5.0595e+08, device='cuda:0')
c= tensor(5.0596e+08, device='cuda:0')
c= tensor(5.0931e+08, device='cuda:0')
c= tensor(5.1036e+08, device='cuda:0')
c= tensor(5.1047e+08, device='cuda:0')
c= tensor(5.1171e+08, device='cuda:0')
c= tensor(5.1171e+08, device='cuda:0')
c= tensor(5.1171e+08, device='cuda:0')
c= tensor(5.4517e+08, device='cuda:0')
c= tensor(5.6113e+08, device='cuda:0')
c= tensor(5.6114e+08, device='cuda:0')
c= tensor(5.6129e+08, device='cuda:0')
c= tensor(5.7225e+08, device='cuda:0')
c= tensor(5.7258e+08, device='cuda:0')
c= tensor(5.7305e+08, device='cuda:0')
c= tensor(5.7344e+08, device='cuda:0')
c= tensor(5.7364e+08, device='cuda:0')
c= tensor(5.7380e+08, device='cuda:0')
c= tensor(5.7381e+08, device='cuda:0')
c= tensor(5.8075e+08, device='cuda:0')
c= tensor(5.8078e+08, device='cuda:0')
c= tensor(5.8079e+08, device='cuda:0')
c= tensor(5.8091e+08, device='cuda:0')
c= tensor(5.8112e+08, device='cuda:0')
c= tensor(5.9179e+08, device='cuda:0')
c= tensor(5.9214e+08, device='cuda:0')
c= tensor(5.9214e+08, device='cuda:0')
c= tensor(5.9229e+08, device='cuda:0')
c= tensor(5.9231e+08, device='cuda:0')
c= tensor(5.9249e+08, device='cuda:0')
c= tensor(5.9354e+08, device='cuda:0')
c= tensor(5.9684e+08, device='cuda:0')
c= tensor(5.9773e+08, device='cuda:0')
c= tensor(5.9773e+08, device='cuda:0')
c= tensor(5.9774e+08, device='cuda:0')
c= tensor(5.9867e+08, device='cuda:0')
c= tensor(5.9902e+08, device='cuda:0')
c= tensor(5.9927e+08, device='cuda:0')
c= tensor(5.9928e+08, device='cuda:0')
c= tensor(6.1759e+08, device='cuda:0')
c= tensor(6.1762e+08, device='cuda:0')
c= tensor(6.1774e+08, device='cuda:0')
c= tensor(6.2065e+08, device='cuda:0')
c= tensor(6.2066e+08, device='cuda:0')
c= tensor(6.2094e+08, device='cuda:0')
c= tensor(6.2606e+08, device='cuda:0')
c= tensor(6.4822e+08, device='cuda:0')
c= tensor(6.4829e+08, device='cuda:0')
c= tensor(6.4840e+08, device='cuda:0')
c= tensor(6.4842e+08, device='cuda:0')
c= tensor(6.4843e+08, device='cuda:0')
c= tensor(6.4974e+08, device='cuda:0')
c= tensor(6.4977e+08, device='cuda:0')
c= tensor(6.4989e+08, device='cuda:0')
c= tensor(6.6041e+08, device='cuda:0')
c= tensor(6.6060e+08, device='cuda:0')
c= tensor(6.6067e+08, device='cuda:0')
c= tensor(6.6069e+08, device='cuda:0')
c= tensor(6.9268e+08, device='cuda:0')
c= tensor(6.9277e+08, device='cuda:0')
c= tensor(6.9283e+08, device='cuda:0')
c= tensor(6.9293e+08, device='cuda:0')
c= tensor(7.0150e+08, device='cuda:0')
c= tensor(7.0155e+08, device='cuda:0')
c= tensor(7.0218e+08, device='cuda:0')
c= tensor(7.0224e+08, device='cuda:0')
c= tensor(7.0311e+08, device='cuda:0')
c= tensor(7.0351e+08, device='cuda:0')
c= tensor(7.6657e+08, device='cuda:0')
c= tensor(7.6751e+08, device='cuda:0')
c= tensor(7.6753e+08, device='cuda:0')
c= tensor(7.7098e+08, device='cuda:0')
c= tensor(7.7372e+08, device='cuda:0')
c= tensor(7.7373e+08, device='cuda:0')
c= tensor(7.7525e+08, device='cuda:0')
c= tensor(7.7948e+08, device='cuda:0')
c= tensor(8.7214e+08, device='cuda:0')
c= tensor(8.7295e+08, device='cuda:0')
c= tensor(8.7295e+08, device='cuda:0')
c= tensor(8.7298e+08, device='cuda:0')
c= tensor(8.7324e+08, device='cuda:0')
c= tensor(8.7336e+08, device='cuda:0')
c= tensor(8.7339e+08, device='cuda:0')
c= tensor(8.7340e+08, device='cuda:0')
c= tensor(8.7386e+08, device='cuda:0')
c= tensor(8.7560e+08, device='cuda:0')
c= tensor(8.7713e+08, device='cuda:0')
c= tensor(8.7714e+08, device='cuda:0')
c= tensor(8.7823e+08, device='cuda:0')
c= tensor(8.7829e+08, device='cuda:0')
c= tensor(8.7840e+08, device='cuda:0')
c= tensor(8.7843e+08, device='cuda:0')
c= tensor(8.7844e+08, device='cuda:0')
c= tensor(9.0709e+08, device='cuda:0')
c= tensor(9.0753e+08, device='cuda:0')
c= tensor(9.0761e+08, device='cuda:0')
c= tensor(9.0821e+08, device='cuda:0')
c= tensor(9.0822e+08, device='cuda:0')
c= tensor(9.2114e+08, device='cuda:0')
c= tensor(9.2118e+08, device='cuda:0')
c= tensor(9.2328e+08, device='cuda:0')
c= tensor(9.2328e+08, device='cuda:0')
c= tensor(9.2329e+08, device='cuda:0')
c= tensor(9.2329e+08, device='cuda:0')
c= tensor(9.2349e+08, device='cuda:0')
c= tensor(9.2350e+08, device='cuda:0')
c= tensor(9.2394e+08, device='cuda:0')
c= tensor(9.2394e+08, device='cuda:0')
c= tensor(9.2395e+08, device='cuda:0')
c= tensor(9.2947e+08, device='cuda:0')
c= tensor(9.3004e+08, device='cuda:0')
c= tensor(9.3048e+08, device='cuda:0')
c= tensor(9.3418e+08, device='cuda:0')
c= tensor(9.4794e+08, device='cuda:0')
c= tensor(9.4794e+08, device='cuda:0')
c= tensor(9.4795e+08, device='cuda:0')
c= tensor(9.4805e+08, device='cuda:0')
c= tensor(9.4805e+08, device='cuda:0')
c= tensor(9.4806e+08, device='cuda:0')
c= tensor(9.4813e+08, device='cuda:0')
c= tensor(9.4815e+08, device='cuda:0')
c= tensor(9.4816e+08, device='cuda:0')
c= tensor(9.4818e+08, device='cuda:0')
c= tensor(9.4823e+08, device='cuda:0')
c= tensor(1.1095e+09, device='cuda:0')
c= tensor(1.1095e+09, device='cuda:0')
c= tensor(1.1111e+09, device='cuda:0')
c= tensor(1.1112e+09, device='cuda:0')
c= tensor(1.1112e+09, device='cuda:0')
c= tensor(1.1116e+09, device='cuda:0')
c= tensor(1.1815e+09, device='cuda:0')
c= tensor(1.2156e+09, device='cuda:0')
c= tensor(1.2158e+09, device='cuda:0')
c= tensor(1.2160e+09, device='cuda:0')
c= tensor(1.2160e+09, device='cuda:0')
c= tensor(1.2165e+09, device='cuda:0')
c= tensor(1.4653e+09, device='cuda:0')
c= tensor(1.4659e+09, device='cuda:0')
c= tensor(1.4659e+09, device='cuda:0')
c= tensor(1.4683e+09, device='cuda:0')
c= tensor(1.4954e+09, device='cuda:0')
c= tensor(1.4958e+09, device='cuda:0')
c= tensor(1.4958e+09, device='cuda:0')
c= tensor(1.4959e+09, device='cuda:0')
c= tensor(1.4959e+09, device='cuda:0')
c= tensor(1.4960e+09, device='cuda:0')
c= tensor(1.5207e+09, device='cuda:0')
c= tensor(1.5208e+09, device='cuda:0')
c= tensor(1.5208e+09, device='cuda:0')
c= tensor(1.5213e+09, device='cuda:0')
c= tensor(1.5215e+09, device='cuda:0')
c= tensor(1.5215e+09, device='cuda:0')
c= tensor(1.5236e+09, device='cuda:0')
c= tensor(1.5257e+09, device='cuda:0')
c= tensor(1.5281e+09, device='cuda:0')
c= tensor(1.5313e+09, device='cuda:0')
c= tensor(1.5358e+09, device='cuda:0')
c= tensor(1.5359e+09, device='cuda:0')
c= tensor(1.5366e+09, device='cuda:0')
c= tensor(1.5378e+09, device='cuda:0')
c= tensor(1.5444e+09, device='cuda:0')
c= tensor(1.5444e+09, device='cuda:0')
c= tensor(1.5667e+09, device='cuda:0')
c= tensor(1.5803e+09, device='cuda:0')
c= tensor(1.5826e+09, device='cuda:0')
c= tensor(1.5833e+09, device='cuda:0')
c= tensor(1.5852e+09, device='cuda:0')
c= tensor(1.5853e+09, device='cuda:0')
c= tensor(1.5853e+09, device='cuda:0')
c= tensor(1.5893e+09, device='cuda:0')
c= tensor(1.5953e+09, device='cuda:0')
c= tensor(1.5980e+09, device='cuda:0')
c= tensor(1.6067e+09, device='cuda:0')
c= tensor(1.6097e+09, device='cuda:0')
c= tensor(1.6117e+09, device='cuda:0')
c= tensor(1.6118e+09, device='cuda:0')
c= tensor(1.6250e+09, device='cuda:0')
c= tensor(1.6250e+09, device='cuda:0')
c= tensor(1.6250e+09, device='cuda:0')
c= tensor(1.6333e+09, device='cuda:0')
c= tensor(1.6333e+09, device='cuda:0')
c= tensor(1.6333e+09, device='cuda:0')
c= tensor(1.6335e+09, device='cuda:0')
c= tensor(1.6408e+09, device='cuda:0')
c= tensor(1.6410e+09, device='cuda:0')
c= tensor(1.6420e+09, device='cuda:0')
c= tensor(1.6420e+09, device='cuda:0')
c= tensor(1.6420e+09, device='cuda:0')
c= tensor(1.6420e+09, device='cuda:0')
c= tensor(1.6424e+09, device='cuda:0')
c= tensor(1.6426e+09, device='cuda:0')
c= tensor(1.6450e+09, device='cuda:0')
c= tensor(1.6450e+09, device='cuda:0')
c= tensor(1.6464e+09, device='cuda:0')
c= tensor(1.6464e+09, device='cuda:0')
c= tensor(1.6466e+09, device='cuda:0')
c= tensor(1.6475e+09, device='cuda:0')
c= tensor(1.6490e+09, device='cuda:0')
c= tensor(1.6491e+09, device='cuda:0')
c= tensor(1.6494e+09, device='cuda:0')
c= tensor(1.6495e+09, device='cuda:0')
c= tensor(1.6496e+09, device='cuda:0')
c= tensor(1.6503e+09, device='cuda:0')
c= tensor(1.6648e+09, device='cuda:0')
c= tensor(1.6649e+09, device='cuda:0')
c= tensor(1.6649e+09, device='cuda:0')
c= tensor(1.6708e+09, device='cuda:0')
c= tensor(1.6708e+09, device='cuda:0')
c= tensor(1.7333e+09, device='cuda:0')
c= tensor(1.7333e+09, device='cuda:0')
c= tensor(1.7363e+09, device='cuda:0')
c= tensor(1.7447e+09, device='cuda:0')
c= tensor(1.7447e+09, device='cuda:0')
c= tensor(1.7479e+09, device='cuda:0')
c= tensor(1.7482e+09, device='cuda:0')
c= tensor(1.7672e+09, device='cuda:0')
c= tensor(1.7672e+09, device='cuda:0')
c= tensor(1.7673e+09, device='cuda:0')
c= tensor(1.7674e+09, device='cuda:0')
c= tensor(1.7674e+09, device='cuda:0')
c= tensor(1.7674e+09, device='cuda:0')
c= tensor(1.7678e+09, device='cuda:0')
c= tensor(1.7681e+09, device='cuda:0')
c= tensor(1.7704e+09, device='cuda:0')
c= tensor(1.7705e+09, device='cuda:0')
c= tensor(1.7705e+09, device='cuda:0')
c= tensor(1.7705e+09, device='cuda:0')
c= tensor(1.7733e+09, device='cuda:0')
c= tensor(1.7735e+09, device='cuda:0')
c= tensor(1.7778e+09, device='cuda:0')
c= tensor(1.7785e+09, device='cuda:0')
c= tensor(1.7785e+09, device='cuda:0')
c= tensor(1.7785e+09, device='cuda:0')
c= tensor(1.7785e+09, device='cuda:0')
c= tensor(1.8172e+09, device='cuda:0')
c= tensor(1.8172e+09, device='cuda:0')
c= tensor(1.8173e+09, device='cuda:0')
c= tensor(1.8199e+09, device='cuda:0')
c= tensor(1.8201e+09, device='cuda:0')
c= tensor(1.8201e+09, device='cuda:0')
c= tensor(1.8201e+09, device='cuda:0')
c= tensor(1.8347e+09, device='cuda:0')
c= tensor(1.8370e+09, device='cuda:0')
c= tensor(1.8372e+09, device='cuda:0')
c= tensor(1.8372e+09, device='cuda:0')
c= tensor(1.8508e+09, device='cuda:0')
c= tensor(1.8526e+09, device='cuda:0')
c= tensor(1.8647e+09, device='cuda:0')
c= tensor(1.8666e+09, device='cuda:0')
c= tensor(1.8666e+09, device='cuda:0')
c= tensor(1.8705e+09, device='cuda:0')
c= tensor(1.8705e+09, device='cuda:0')
c= tensor(1.8712e+09, device='cuda:0')
c= tensor(1.8712e+09, device='cuda:0')
c= tensor(1.8712e+09, device='cuda:0')
c= tensor(1.8714e+09, device='cuda:0')
c= tensor(1.8716e+09, device='cuda:0')
c= tensor(1.8716e+09, device='cuda:0')
c= tensor(1.8717e+09, device='cuda:0')
c= tensor(1.8719e+09, device='cuda:0')
c= tensor(1.8857e+09, device='cuda:0')
c= tensor(1.8857e+09, device='cuda:0')
c= tensor(1.8858e+09, device='cuda:0')
c= tensor(1.8858e+09, device='cuda:0')
c= tensor(1.8860e+09, device='cuda:0')
c= tensor(1.8860e+09, device='cuda:0')
c= tensor(1.8860e+09, device='cuda:0')
c= tensor(1.8861e+09, device='cuda:0')
c= tensor(1.8886e+09, device='cuda:0')
c= tensor(1.8886e+09, device='cuda:0')
c= tensor(1.8886e+09, device='cuda:0')
c= tensor(1.8886e+09, device='cuda:0')
c= tensor(1.9091e+09, device='cuda:0')
c= tensor(2.2179e+09, device='cuda:0')
c= tensor(2.2180e+09, device='cuda:0')
c= tensor(2.2180e+09, device='cuda:0')
c= tensor(2.2212e+09, device='cuda:0')
c= tensor(2.2226e+09, device='cuda:0')
c= tensor(2.2226e+09, device='cuda:0')
c= tensor(2.2229e+09, device='cuda:0')
c= tensor(2.2230e+09, device='cuda:0')
c= tensor(2.3322e+09, device='cuda:0')
c= tensor(2.4388e+09, device='cuda:0')
c= tensor(2.4438e+09, device='cuda:0')
c= tensor(2.4439e+09, device='cuda:0')
c= tensor(2.4439e+09, device='cuda:0')
c= tensor(2.4439e+09, device='cuda:0')
c= tensor(2.4451e+09, device='cuda:0')
c= tensor(2.4452e+09, device='cuda:0')
c= tensor(2.4455e+09, device='cuda:0')
c= tensor(2.4514e+09, device='cuda:0')
c= tensor(2.4752e+09, device='cuda:0')
c= tensor(2.4752e+09, device='cuda:0')
c= tensor(2.4752e+09, device='cuda:0')
c= tensor(2.4765e+09, device='cuda:0')
c= tensor(2.4910e+09, device='cuda:0')
c= tensor(2.4922e+09, device='cuda:0')
c= tensor(2.4922e+09, device='cuda:0')
c= tensor(2.4922e+09, device='cuda:0')
c= tensor(2.4923e+09, device='cuda:0')
c= tensor(2.4923e+09, device='cuda:0')
c= tensor(2.4934e+09, device='cuda:0')
c= tensor(2.4934e+09, device='cuda:0')
c= tensor(2.4934e+09, device='cuda:0')
c= tensor(2.4935e+09, device='cuda:0')
c= tensor(2.4935e+09, device='cuda:0')
c= tensor(2.4935e+09, device='cuda:0')
c= tensor(2.4982e+09, device='cuda:0')
c= tensor(2.5248e+09, device='cuda:0')
c= tensor(2.5278e+09, device='cuda:0')
c= tensor(2.5281e+09, device='cuda:0')
c= tensor(2.5282e+09, device='cuda:0')
c= tensor(2.5282e+09, device='cuda:0')
c= tensor(2.5283e+09, device='cuda:0')
c= tensor(2.5313e+09, device='cuda:0')
c= tensor(2.5314e+09, device='cuda:0')
c= tensor(2.5328e+09, device='cuda:0')
c= tensor(2.5330e+09, device='cuda:0')
c= tensor(2.7077e+09, device='cuda:0')
c= tensor(2.7078e+09, device='cuda:0')
c= tensor(2.7085e+09, device='cuda:0')
c= tensor(2.7185e+09, device='cuda:0')
c= tensor(2.7204e+09, device='cuda:0')
c= tensor(2.7206e+09, device='cuda:0')
c= tensor(2.7422e+09, device='cuda:0')
c= tensor(2.7434e+09, device='cuda:0')
c= tensor(2.7435e+09, device='cuda:0')
c= tensor(2.7436e+09, device='cuda:0')
c= tensor(2.7437e+09, device='cuda:0')
c= tensor(2.7438e+09, device='cuda:0')
c= tensor(2.7447e+09, device='cuda:0')
c= tensor(2.7980e+09, device='cuda:0')
c= tensor(2.8060e+09, device='cuda:0')
c= tensor(2.8102e+09, device='cuda:0')
c= tensor(2.8103e+09, device='cuda:0')
c= tensor(2.8105e+09, device='cuda:0')
c= tensor(2.8107e+09, device='cuda:0')
c= tensor(2.8107e+09, device='cuda:0')
c= tensor(2.8541e+09, device='cuda:0')
c= tensor(2.8698e+09, device='cuda:0')
c= tensor(2.8699e+09, device='cuda:0')
c= tensor(3.2571e+09, device='cuda:0')
c= tensor(3.2757e+09, device='cuda:0')
c= tensor(3.2760e+09, device='cuda:0')
c= tensor(3.2760e+09, device='cuda:0')
c= tensor(3.2767e+09, device='cuda:0')
c= tensor(3.2783e+09, device='cuda:0')
c= tensor(3.2783e+09, device='cuda:0')
c= tensor(3.3357e+09, device='cuda:0')
c= tensor(3.3361e+09, device='cuda:0')
c= tensor(3.3363e+09, device='cuda:0')
c= tensor(3.3364e+09, device='cuda:0')
c= tensor(3.3364e+09, device='cuda:0')
c= tensor(3.3364e+09, device='cuda:0')
c= tensor(3.3365e+09, device='cuda:0')
c= tensor(3.3366e+09, device='cuda:0')
c= tensor(3.3366e+09, device='cuda:0')
c= tensor(4.0886e+09, device='cuda:0')
c= tensor(4.0897e+09, device='cuda:0')
c= tensor(4.0911e+09, device='cuda:0')
c= tensor(4.0911e+09, device='cuda:0')
c= tensor(4.0911e+09, device='cuda:0')
c= tensor(4.0911e+09, device='cuda:0')
c= tensor(4.1004e+09, device='cuda:0')
c= tensor(4.1073e+09, device='cuda:0')
c= tensor(4.2192e+09, device='cuda:0')
c= tensor(4.2194e+09, device='cuda:0')
c= tensor(4.2246e+09, device='cuda:0')
c= tensor(4.2249e+09, device='cuda:0')
c= tensor(4.2274e+09, device='cuda:0')
c= tensor(4.2342e+09, device='cuda:0')
c= tensor(4.2343e+09, device='cuda:0')
c= tensor(4.2343e+09, device='cuda:0')
c= tensor(4.2350e+09, device='cuda:0')
c= tensor(4.2352e+09, device='cuda:0')
c= tensor(4.2368e+09, device='cuda:0')
c= tensor(4.2461e+09, device='cuda:0')
c= tensor(4.2463e+09, device='cuda:0')
c= tensor(4.2478e+09, device='cuda:0')
c= tensor(4.2479e+09, device='cuda:0')
c= tensor(4.2528e+09, device='cuda:0')
c= tensor(4.2634e+09, device='cuda:0')
c= tensor(4.2636e+09, device='cuda:0')
c= tensor(4.2636e+09, device='cuda:0')
c= tensor(4.2875e+09, device='cuda:0')
c= tensor(4.2894e+09, device='cuda:0')
c= tensor(4.3496e+09, device='cuda:0')
c= tensor(4.3504e+09, device='cuda:0')
c= tensor(4.3516e+09, device='cuda:0')
c= tensor(4.3519e+09, device='cuda:0')
c= tensor(4.3534e+09, device='cuda:0')
c= tensor(4.3892e+09, device='cuda:0')
c= tensor(4.3895e+09, device='cuda:0')
c= tensor(4.3895e+09, device='cuda:0')
c= tensor(4.3897e+09, device='cuda:0')
c= tensor(4.3907e+09, device='cuda:0')
c= tensor(4.3933e+09, device='cuda:0')
c= tensor(4.3940e+09, device='cuda:0')
c= tensor(4.3940e+09, device='cuda:0')
c= tensor(4.3942e+09, device='cuda:0')
c= tensor(4.3998e+09, device='cuda:0')
c= tensor(4.4004e+09, device='cuda:0')
c= tensor(4.4012e+09, device='cuda:0')
c= tensor(4.4026e+09, device='cuda:0')
c= tensor(4.4036e+09, device='cuda:0')
c= tensor(4.4037e+09, device='cuda:0')
c= tensor(4.4037e+09, device='cuda:0')
c= tensor(4.4037e+09, device='cuda:0')
c= tensor(4.4062e+09, device='cuda:0')
c= tensor(4.4063e+09, device='cuda:0')
c= tensor(4.4070e+09, device='cuda:0')
c= tensor(4.4070e+09, device='cuda:0')
c= tensor(4.4070e+09, device='cuda:0')
c= tensor(4.4093e+09, device='cuda:0')
c= tensor(4.4105e+09, device='cuda:0')
c= tensor(4.4106e+09, device='cuda:0')
c= tensor(4.4106e+09, device='cuda:0')
c= tensor(4.4106e+09, device='cuda:0')
c= tensor(4.4107e+09, device='cuda:0')
c= tensor(4.4109e+09, device='cuda:0')
c= tensor(4.4129e+09, device='cuda:0')
c= tensor(4.4129e+09, device='cuda:0')
c= tensor(4.4132e+09, device='cuda:0')
c= tensor(4.4133e+09, device='cuda:0')
c= tensor(4.4134e+09, device='cuda:0')
c= tensor(4.4220e+09, device='cuda:0')
c= tensor(4.4221e+09, device='cuda:0')
c= tensor(4.4255e+09, device='cuda:0')
c= tensor(4.4413e+09, device='cuda:0')
c= tensor(4.4416e+09, device='cuda:0')
c= tensor(4.4426e+09, device='cuda:0')
c= tensor(4.4441e+09, device='cuda:0')
c= tensor(4.4441e+09, device='cuda:0')
c= tensor(4.4444e+09, device='cuda:0')
c= tensor(4.4444e+09, device='cuda:0')
c= tensor(4.4526e+09, device='cuda:0')
c= tensor(4.4540e+09, device='cuda:0')
c= tensor(4.4541e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4564e+09, device='cuda:0')
c= tensor(4.4566e+09, device='cuda:0')
c= tensor(4.4566e+09, device='cuda:0')
c= tensor(4.4567e+09, device='cuda:0')
c= tensor(4.4598e+09, device='cuda:0')
c= tensor(4.5212e+09, device='cuda:0')
c= tensor(4.5230e+09, device='cuda:0')
c= tensor(4.5231e+09, device='cuda:0')
c= tensor(4.5232e+09, device='cuda:0')
c= tensor(4.5475e+09, device='cuda:0')
c= tensor(4.5476e+09, device='cuda:0')
c= tensor(4.5476e+09, device='cuda:0')
c= tensor(4.5482e+09, device='cuda:0')
c= tensor(4.5500e+09, device='cuda:0')
c= tensor(4.5500e+09, device='cuda:0')
c= tensor(4.5501e+09, device='cuda:0')
c= tensor(4.5504e+09, device='cuda:0')
time to make c is 12.159328699111938
time for making loss is 12.159354209899902
p0 True
it  0 : 1812020736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
4786380800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 21% |
memory (bytes)
4786589696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  889393400.0
relative error loss 0.19545537
shape of L is 
torch.Size([])
memory (bytes)
4813742080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
4813774848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  877868000.0
relative error loss 0.19292252
shape of L is 
torch.Size([])
memory (bytes)
4817248256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4817248256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  840993540.0
relative error loss 0.1848189
shape of L is 
torch.Size([])
memory (bytes)
4820549632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4820549632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  825317900.0
relative error loss 0.18137397
shape of L is 
torch.Size([])
memory (bytes)
4823756800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4823756800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  812946940.0
relative error loss 0.1786553
shape of L is 
torch.Size([])
memory (bytes)
4826923008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4826972160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  803614460.0
relative error loss 0.17660438
shape of L is 
torch.Size([])
memory (bytes)
4830158848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
4830158848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  795853600.0
relative error loss 0.17489882
shape of L is 
torch.Size([])
memory (bytes)
4833177600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4833406976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  789958660.0
relative error loss 0.17360334
shape of L is 
torch.Size([])
memory (bytes)
4836597760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4836618240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  784664800.0
relative error loss 0.17243995
shape of L is 
torch.Size([])
memory (bytes)
4839841792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 21% |
memory (bytes)
4839841792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  780823300.0
relative error loss 0.17159572
time to take a step is 296.9485011100769
it  1 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
4842872832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4843057152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  780823300.0
relative error loss 0.17159572
shape of L is 
torch.Size([])
memory (bytes)
4846268416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4846272512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 21% |
error is  778057200.0
relative error loss 0.17098784
shape of L is 
torch.Size([])
memory (bytes)
4849504256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
4849504256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  775573500.0
relative error loss 0.17044201
shape of L is 
torch.Size([])
memory (bytes)
4852690944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4852731904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  773188600.0
relative error loss 0.16991791
shape of L is 
torch.Size([])
memory (bytes)
4855943168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
4855947264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  770783000.0
relative error loss 0.16938923
shape of L is 
torch.Size([])
memory (bytes)
4859166720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 21% |
memory (bytes)
4859166720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  769142500.0
relative error loss 0.16902873
shape of L is 
torch.Size([])
memory (bytes)
4862345216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4862382080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  767594240.0
relative error loss 0.16868848
shape of L is 
torch.Size([])
memory (bytes)
4865597440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 21% |
memory (bytes)
4865597440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  765817340.0
relative error loss 0.16829798
shape of L is 
torch.Size([])
memory (bytes)
4868812800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 21% |
memory (bytes)
4868812800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  764221700.0
relative error loss 0.1679473
shape of L is 
torch.Size([])
memory (bytes)
4871987200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
4872028160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  762701300.0
relative error loss 0.1676132
time to take a step is 303.799565076828
it  2 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
4875243520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4875243520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  762701300.0
relative error loss 0.1676132
shape of L is 
torch.Size([])
memory (bytes)
4878454784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
4878454784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  761571600.0
relative error loss 0.16736493
shape of L is 
torch.Size([])
memory (bytes)
4881641472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
4881682432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  760566500.0
relative error loss 0.16714405
shape of L is 
torch.Size([])
memory (bytes)
4884889600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4884893696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  759168000.0
relative error loss 0.1668367
shape of L is 
torch.Size([])
memory (bytes)
4888117248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
4888117248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  758358800.0
relative error loss 0.16665886
shape of L is 
torch.Size([])
memory (bytes)
4891324416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
4891332608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  757469440.0
relative error loss 0.16646342
shape of L is 
torch.Size([])
memory (bytes)
4894552064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 21% |
memory (bytes)
4894552064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  756675840.0
relative error loss 0.16628902
shape of L is 
torch.Size([])
memory (bytes)
4897730560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
4897767424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  755908100.0
relative error loss 0.16612029
shape of L is 
torch.Size([])
memory (bytes)
4900982784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4900982784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  754994200.0
relative error loss 0.16591945
shape of L is 
torch.Size([])
memory (bytes)
4904161280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4904202240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 21% |
error is  754334700.0
relative error loss 0.16577452
time to take a step is 292.7228374481201
it  3 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
4907421696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 21% |
memory (bytes)
4907421696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  754334700.0
relative error loss 0.16577452
shape of L is 
torch.Size([])
memory (bytes)
4910596096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4910632960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  753684000.0
relative error loss 0.16563152
shape of L is 
torch.Size([])
memory (bytes)
4913819648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 21% |
memory (bytes)
4913856512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  753201400.0
relative error loss 0.16552547
shape of L is 
torch.Size([])
memory (bytes)
4917071872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
4917075968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  752759800.0
relative error loss 0.16542841
shape of L is 
torch.Size([])
memory (bytes)
4920250368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 21% |
memory (bytes)
4920287232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  752463900.0
relative error loss 0.16536339
shape of L is 
torch.Size([])
memory (bytes)
4923469824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 21% |
memory (bytes)
4923469824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  752228100.0
relative error loss 0.16531157
shape of L is 
torch.Size([])
memory (bytes)
4926726144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4926726144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 21% |
error is  751897340.0
relative error loss 0.16523889
shape of L is 
torch.Size([])
memory (bytes)
4929945600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4929945600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 21% |
error is  751952400.0
relative error loss 0.16525097
shape of L is 
torch.Size([])
memory (bytes)
4933148672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4933148672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 21% |
error is  751771400.0
relative error loss 0.1652112
shape of L is 
torch.Size([])
memory (bytes)
4936355840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 21% |
memory (bytes)
4936368128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  751630100.0
relative error loss 0.16518015
time to take a step is 303.4913799762726
it  4 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
4939550720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4939587584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 21% |
error is  751630100.0
relative error loss 0.16518015
shape of L is 
torch.Size([])
memory (bytes)
4942790656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
4942790656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  751376900.0
relative error loss 0.1651245
shape of L is 
torch.Size([])
memory (bytes)
4945997824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 21% |
memory (bytes)
4945997824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  751140860.0
relative error loss 0.16507263
shape of L is 
torch.Size([])
memory (bytes)
4949213184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4949217280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  750871040.0
relative error loss 0.16501334
shape of L is 
torch.Size([])
memory (bytes)
4952424448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4952440832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  750568450.0
relative error loss 0.16494684
shape of L is 
torch.Size([])
memory (bytes)
4955648000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
4955652096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  750154000.0
relative error loss 0.16485576
shape of L is 
torch.Size([])
memory (bytes)
4958871552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
4958871552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 21% |
error is  749836540.0
relative error loss 0.164786
shape of L is 
torch.Size([])
memory (bytes)
4962091008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 21% |
memory (bytes)
4962091008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  749297900.0
relative error loss 0.16466762
shape of L is 
torch.Size([])
memory (bytes)
4965273600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 21% |
memory (bytes)
4965310464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  749120800.0
relative error loss 0.1646287
shape of L is 
torch.Size([])
memory (bytes)
4968521728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
4968521728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  748626940.0
relative error loss 0.16452017
time to take a step is 299.6021685600281
it  5 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
4971741184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 21% |
memory (bytes)
4971741184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  748626940.0
relative error loss 0.16452017
shape of L is 
torch.Size([])
memory (bytes)
4974927872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 21% |
memory (bytes)
4974956544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  748440600.0
relative error loss 0.16447921
shape of L is 
torch.Size([])
memory (bytes)
4978180096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4978180096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  748167400.0
relative error loss 0.16441919
shape of L is 
torch.Size([])
memory (bytes)
4981399552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
4981399552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  747877900.0
relative error loss 0.16435556
shape of L is 
torch.Size([])
memory (bytes)
4984606720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4984610816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 21% |
error is  747494140.0
relative error loss 0.16427122
shape of L is 
torch.Size([])
memory (bytes)
4987813888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4987834368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  747154200.0
relative error loss 0.1641965
shape of L is 
torch.Size([])
memory (bytes)
4990959616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
4990959616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  746808060.0
relative error loss 0.16412045
shape of L is 
torch.Size([])
memory (bytes)
4994228224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
4994269184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  746552600.0
relative error loss 0.1640643
shape of L is 
torch.Size([])
memory (bytes)
4997443584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
4997484544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  746262300.0
relative error loss 0.16400051
shape of L is 
torch.Size([])
memory (bytes)
5000708096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
5000708096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 21% |
error is  746077950.0
relative error loss 0.16396
time to take a step is 297.71419644355774
it  6 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
5003915264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5003915264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  746077950.0
relative error loss 0.16396
shape of L is 
torch.Size([])
memory (bytes)
5007118336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
5007118336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  745905150.0
relative error loss 0.16392203
shape of L is 
torch.Size([])
memory (bytes)
5010358272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
5010358272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  745733900.0
relative error loss 0.16388439
shape of L is 
torch.Size([])
memory (bytes)
5013577728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 21% |
memory (bytes)
5013577728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  745417700.0
relative error loss 0.1638149
shape of L is 
torch.Size([])
memory (bytes)
5016756224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
5016797184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  745308400.0
relative error loss 0.16379088
shape of L is 
torch.Size([])
memory (bytes)
5020012544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 21% |
memory (bytes)
5020012544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  745163800.0
relative error loss 0.1637591
shape of L is 
torch.Size([])
memory (bytes)
5023236096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
5023236096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  744929800.0
relative error loss 0.16370767
shape of L is 
torch.Size([])
memory (bytes)
5026459648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
5026459648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 21% |
error is  744745500.0
relative error loss 0.16366717
shape of L is 
torch.Size([])
memory (bytes)
5029613568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5029666816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 21% |
error is  744593900.0
relative error loss 0.16363387
shape of L is 
torch.Size([])
memory (bytes)
5032882176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 21% |
memory (bytes)
5032882176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  744473600.0
relative error loss 0.16360742
time to take a step is 296.5041928291321
it  7 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
5036060672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
5036101632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  744473600.0
relative error loss 0.16360742
shape of L is 
torch.Size([])
memory (bytes)
5039308800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5039308800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  744312300.0
relative error loss 0.16357198
shape of L is 
torch.Size([])
memory (bytes)
5042528256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 21% |
memory (bytes)
5042528256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  744220700.0
relative error loss 0.16355184
shape of L is 
torch.Size([])
memory (bytes)
5045747712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 21% |
memory (bytes)
5045747712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  744060400.0
relative error loss 0.16351663
shape of L is 
torch.Size([])
memory (bytes)
5048918016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5048958976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 21% |
error is  743941600.0
relative error loss 0.16349052
shape of L is 
torch.Size([])
memory (bytes)
5052157952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5052170240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  743837950.0
relative error loss 0.16346774
shape of L is 
torch.Size([])
memory (bytes)
5055340544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
5055381504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  743654140.0
relative error loss 0.16342734
shape of L is 
torch.Size([])
memory (bytes)
5058588672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
5058588672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  743506940.0
relative error loss 0.16339499
shape of L is 
torch.Size([])
memory (bytes)
5061808128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 21% |
memory (bytes)
5061808128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  743402240.0
relative error loss 0.16337198
shape of L is 
torch.Size([])
memory (bytes)
5065019392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5065023488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  743277600.0
relative error loss 0.16334458
time to take a step is 302.74429655075073
it  8 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
5068197888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
5068238848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  743277600.0
relative error loss 0.16334458
shape of L is 
torch.Size([])
memory (bytes)
5071421440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5071458304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 21% |
error is  743137000.0
relative error loss 0.16331369
shape of L is 
torch.Size([])
memory (bytes)
5074669568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 21% |
memory (bytes)
5074669568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  742998500.0
relative error loss 0.16328326
shape of L is 
torch.Size([])
memory (bytes)
5077901312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
5077901312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  742855940.0
relative error loss 0.16325192
shape of L is 
torch.Size([])
memory (bytes)
5081120768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
memory (bytes)
5081120768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  742772200.0
relative error loss 0.16323352
shape of L is 
torch.Size([])
memory (bytes)
5084336128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5084336128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  742673900.0
relative error loss 0.16321193
shape of L is 
torch.Size([])
memory (bytes)
5087518720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5087559680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  742710000.0
relative error loss 0.16321985
shape of L is 
torch.Size([])
memory (bytes)
5090779136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 21% |
memory (bytes)
5090779136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  742595100.0
relative error loss 0.1631946
shape of L is 
torch.Size([])
memory (bytes)
5093994496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5093998592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  742489100.0
relative error loss 0.1631713
shape of L is 
torch.Size([])
memory (bytes)
5097168896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 21% |
memory (bytes)
5097168896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  742396400.0
relative error loss 0.16315094
time to take a step is 296.038950920105
it  9 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
5100388352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
5100425216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 21% |
error is  742396400.0
relative error loss 0.16315094
shape of L is 
torch.Size([])
memory (bytes)
5103644672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5103644672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  742298600.0
relative error loss 0.16312945
shape of L is 
torch.Size([])
memory (bytes)
5106814976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 21% |
memory (bytes)
5106851840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  742185500.0
relative error loss 0.16310458
shape of L is 
torch.Size([])
memory (bytes)
5110079488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 21% |
memory (bytes)
5110079488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  742081300.0
relative error loss 0.16308168
shape of L is 
torch.Size([])
memory (bytes)
5113245696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
5113282560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 21% |
error is  741948400.0
relative error loss 0.16305248
shape of L is 
torch.Size([])
memory (bytes)
5116485632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
5116485632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 21% |
error is  741826050.0
relative error loss 0.16302559
shape of L is 
torch.Size([])
memory (bytes)
5119705088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
5119705088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  741761800.0
relative error loss 0.16301146
shape of L is 
torch.Size([])
memory (bytes)
5122916352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
5122916352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  741675260.0
relative error loss 0.16299245
shape of L is 
torch.Size([])
memory (bytes)
5126135808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5126135808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  741594600.0
relative error loss 0.16297473
shape of L is 
torch.Size([])
memory (bytes)
5129310208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
5129351168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  741538800.0
relative error loss 0.16296247
time to take a step is 302.2172763347626
it  10 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
5132537856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 21% |
memory (bytes)
5132578816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  741538800.0
relative error loss 0.16296247
shape of L is 
torch.Size([])
memory (bytes)
5135753216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5135790080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  741387800.0
relative error loss 0.16292927
shape of L is 
torch.Size([])
memory (bytes)
5138948096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
5138948096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  741336600.0
relative error loss 0.16291802
shape of L is 
torch.Size([])
memory (bytes)
5142196224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 21% |
memory (bytes)
5142237184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  741248500.0
relative error loss 0.16289867
shape of L is 
torch.Size([])
memory (bytes)
5145415680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5145456640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 21% |
error is  741181700.0
relative error loss 0.16288398
shape of L is 
torch.Size([])
memory (bytes)
5148680192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
5148680192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  741112800.0
relative error loss 0.16286886
shape of L is 
torch.Size([])
memory (bytes)
5151891456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5151891456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  741089300.0
relative error loss 0.16286367
shape of L is 
torch.Size([])
memory (bytes)
5155102720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
5155102720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  741010700.0
relative error loss 0.1628464
shape of L is 
torch.Size([])
memory (bytes)
5158313984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
5158313984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740973600.0
relative error loss 0.16283825
shape of L is 
torch.Size([])
memory (bytes)
5161369600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
5161533440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740923900.0
relative error loss 0.16282733
time to take a step is 293.81140899658203
it  11 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
5164756992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5164756992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740923900.0
relative error loss 0.16282733
shape of L is 
torch.Size([])
memory (bytes)
5167964160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5167964160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740853760.0
relative error loss 0.16281192
shape of L is 
torch.Size([])
memory (bytes)
5171187712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
5171187712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740924400.0
relative error loss 0.16282745
shape of L is 
torch.Size([])
memory (bytes)
5174415360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 21% |
memory (bytes)
5174415360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740802560.0
relative error loss 0.16280067
shape of L is 
torch.Size([])
memory (bytes)
5177634816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 21% |
memory (bytes)
5177634816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 21% |
error is  740711940.0
relative error loss 0.16278075
shape of L is 
torch.Size([])
memory (bytes)
5180846080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
5180846080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740633100.0
relative error loss 0.16276342
shape of L is 
torch.Size([])
memory (bytes)
5184028672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5184069632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740579840.0
relative error loss 0.16275172
shape of L is 
torch.Size([])
memory (bytes)
5187289088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 21% |
memory (bytes)
5187289088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740505340.0
relative error loss 0.16273534
shape of L is 
torch.Size([])
memory (bytes)
5190508544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
5190508544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740468500.0
relative error loss 0.16272725
shape of L is 
torch.Size([])
memory (bytes)
5193723904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5193723904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740418800.0
relative error loss 0.16271633
time to take a step is 304.1292552947998
it  12 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
5196943360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
5196943360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 21% |
error is  740418800.0
relative error loss 0.16271633
shape of L is 
torch.Size([])
memory (bytes)
5200158720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5200158720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740332540.0
relative error loss 0.16269737
shape of L is 
torch.Size([])
memory (bytes)
5203365888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5203365888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740459500.0
relative error loss 0.16272528
shape of L is 
torch.Size([])
memory (bytes)
5206564864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 21% |
memory (bytes)
5206564864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740290800.0
relative error loss 0.1626882
shape of L is 
torch.Size([])
memory (bytes)
5209788416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 21% |
memory (bytes)
5209788416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740214800.0
relative error loss 0.16267149
shape of L is 
torch.Size([])
memory (bytes)
5212995584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5212995584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740157440.0
relative error loss 0.16265889
shape of L is 
torch.Size([])
memory (bytes)
5216174080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5216215040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740142600.0
relative error loss 0.16265562
shape of L is 
torch.Size([])
memory (bytes)
5219430400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5219430400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740083460.0
relative error loss 0.16264263
shape of L is 
torch.Size([])
memory (bytes)
5222641664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 21% |
memory (bytes)
5222641664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740066560.0
relative error loss 0.16263892
shape of L is 
torch.Size([])
memory (bytes)
5225852928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 21% |
memory (bytes)
5225857024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740040200.0
relative error loss 0.16263312
time to take a step is 305.64223885536194
it  13 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
5229064192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5229064192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  740040200.0
relative error loss 0.16263312
shape of L is 
torch.Size([])
memory (bytes)
5232279552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
5232283648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  740007700.0
relative error loss 0.16262598
shape of L is 
torch.Size([])
memory (bytes)
5235511296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 21% |
memory (bytes)
5235511296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  739990800.0
relative error loss 0.16262227
shape of L is 
torch.Size([])
memory (bytes)
5238595584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
5238730752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  739939840.0
relative error loss 0.16261107
shape of L is 
torch.Size([])
memory (bytes)
5241950208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
5241950208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  739919900.0
relative error loss 0.16260669
shape of L is 
torch.Size([])
memory (bytes)
5245165568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
5245165568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  739889400.0
relative error loss 0.16259998
shape of L is 
torch.Size([])
memory (bytes)
5248368640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
memory (bytes)
5248372736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  739857400.0
relative error loss 0.16259296
shape of L is 
torch.Size([])
memory (bytes)
5251588096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5251588096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  739818750.0
relative error loss 0.16258445
shape of L is 
torch.Size([])
memory (bytes)
5254758400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
5254799360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  739790850.0
relative error loss 0.16257833
shape of L is 
torch.Size([])
memory (bytes)
5258006528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 21% |
memory (bytes)
5258006528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  739751940.0
relative error loss 0.16256978
time to take a step is 304.9628493785858
it  14 : 2270465024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
5261221888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
5261221888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  739751940.0
relative error loss 0.16256978
shape of L is 
torch.Size([])
memory (bytes)
5264433152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
5264433152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  739743200.0
relative error loss 0.16256787
shape of L is 
torch.Size([])
memory (bytes)
5267644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 21% |
memory (bytes)
5267644416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  739693060.0
relative error loss 0.16255684
shape of L is 
torch.Size([])
memory (bytes)
5270863872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 21% |
memory (bytes)
5270863872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  739671040.0
relative error loss 0.162552
shape of L is 
torch.Size([])
memory (bytes)
5274042368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 21% |
memory (bytes)
5274083328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 21% |
error is  739639550.0
relative error loss 0.16254508
shape of L is 
torch.Size([])
memory (bytes)
5277298688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 21% |
memory (bytes)
5277298688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 21% |
error is  739593700.0
relative error loss 0.16253501
shape of L is 
torch.Size([])
memory (bytes)
5280501760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 21% |
memory (bytes)
5280501760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  739569150.0
relative error loss 0.1625296
shape of L is 
torch.Size([])
memory (bytes)
5283713024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 21% |
memory (bytes)
5283713024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  739524860.0
relative error loss 0.16251987
shape of L is 
torch.Size([])
memory (bytes)
5286920192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 21% |
memory (bytes)
5286924288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  739505660.0
relative error loss 0.16251566
shape of L is 
torch.Size([])
memory (bytes)
5290106880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 21% |
memory (bytes)
5290143744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 21% |
error is  739455200.0
relative error loss 0.16250457
time to take a step is 267.32551670074463
sum tnnu_Z after tensor(12234627., device='cuda:0')
shape of features
(4516,)
shape of features
(4516,)
number of orig particles 18063
number of new particles after remove low mass 16729
tnuZ shape should be parts x labs
torch.Size([18063, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  889325200.0
relative error without small mass is  0.19544038
nnu_Z shape should be number of particles by maxV
(18063, 702)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
shape of features
(18063,)
Tue Jan 31 12:07:05 EST 2023
