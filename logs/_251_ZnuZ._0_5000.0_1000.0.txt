Thu Feb 2 21:06:52 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 35413434
numbers of Z: 18313
shape of features
(18313,)
shape of features
(18313,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01474050528254128	18313	18.313	0.09302170421255773
X	0.01258244889039653	1394	1.394	0.20820962425245632
X	0.01316826085071595	20849	20.849	0.08579877038607024
X	0.013654330896107476	1218	1.218	0.22380736717838132
X	0.013431889139449888	6040	6.04	0.13052690891612095
X	0.014427195254453483	102774	102.774	0.05197161547777684
X	0.013342275407879101	63040	63.04	0.05959427840543141
X	0.012935961116222352	29064	29.064	0.07635094889550144
X	0.013108725886532882	30020	30.02	0.07586647947038726
X	0.013103366937470653	9393	9.393	0.1117359331168103
X	0.013835727499651965	23044	23.044	0.08436222868340437
X	0.012860301075459936	6514	6.514	0.12544917048493875
X	0.012800698051218135	44762	44.762	0.06588320583235326
X	0.012607722492878217	6113	6.113	0.1272899399931346
X	0.013856887719352354	248094	248.094	0.03822520403519092
X	0.012829592652892857	17699	17.699	0.08982999580721956
X	0.014605563487895682	69791	69.791	0.05937080879180914
X	0.013528498344372718	38934	38.934	0.0703029457619669
X	0.012834044612605853	33171	33.171	0.07286768855503198
X	0.014296490554969824	208269	208.269	0.04094506862153246
X	0.013509103090369895	64847	64.847	0.05928054789011486
X	0.012856609784185038	63992	63.992	0.05856888095856576
X	0.014451947198857115	348533	348.533	0.03461206313470033
X	0.012917881316864678	13338	13.338	0.09893885020845627
X	0.013290534482932216	15019	15.019	0.09600646441535854
X	0.012815125177428355	20740	20.74	0.08517354046306881
X	0.013124328456232163	46255	46.255	0.06571121062899447
X	0.013238706581784509	25619	25.619	0.08024681316919645
X	0.013067413673639118	11936	11.936	0.10306477904415118
X	0.013562805975336613	57641	57.641	0.061736122276378515
X	0.014443950938292687	1864245	1864.245	0.019787655429388023
X	0.012699536182160786	15576	15.576	0.09342088426747135
X	0.014586622939155532	309500	309.5	0.03612141053595573
X	0.013934974592229431	11056	11.056	0.10801963788411208
X	0.012800112817029224	10063	10.063	0.10834996488677241
X	0.013759687485703035	57434	57.434	0.06210785095341049
X	0.013817845399780874	94911	94.911	0.05260672428833288
X	0.013677843685778185	59447	59.447	0.0612767746496468
X	0.013046089087933442	1392	1.392	0.21083710042149353
X	0.013884148009686178	3337	3.337	0.16083763408783022
X	0.01264839452417358	1695	1.695	0.1954138016115345
X	0.012794436539774261	2253	2.253	0.1784103277076013
X	0.012638465983796523	797	0.797	0.2512341549451053
X	0.0123487209779745	1164	1.164	0.2197301568947346
X	0.012420782717280915	1503	1.503	0.202176183770603
X	0.012592989454627694	698	0.698	0.26227554955021065
X	0.012172471407684345	556	0.556	0.2797486442662036
X	0.012430031857470231	1341	1.341	0.21006215729579092
X	0.013112115218030494	3127	3.127	0.16125565680748513
X	0.012718250180866265	4942	4.942	0.13703816698358626
X	0.013203209641263109	5651	5.651	0.1326940360642607
X	0.0139623808235997	9617	9.617	0.11323307144889133
X	0.012965000799538542	2456	2.456	0.174119760775925
X	0.012765517455010615	1980	1.98	0.18611915745009497
X	0.012553434864398597	1379	1.379	0.20880109367167712
X	0.013711578811339407	3438	3.438	0.15858442455271463
X	0.012804224305408625	9909	9.909	0.10892004573953167
X	0.012501849216330215	952	0.952	0.23592779172536918
X	0.01254061763550222	3507	3.507	0.15291886062501914
X	0.012873497916110636	5177	5.177	0.135479255278858
X	0.012237237288881337	1226	1.226	0.2153100509591997
X	0.013460502278221688	7994	7.994	0.11896860631789363
X	0.01250901018621821	1228	1.228	0.21677449997240975
X	0.013842329051628539	5195	5.195	0.1386355274209499
X	0.012834048373503007	10377	10.377	0.1073405834869548
X	0.012838634511782222	2744	2.744	0.16725466618665838
X	0.012563547226358409	1602	1.602	0.1986780650150839
X	0.012494571981352934	2394	2.394	0.17345989497381295
X	0.012755813836782244	3559	3.559	0.15303592566586227
X	0.013001646541283919	4531	4.531	0.14210240593598497
X	0.012801705942459291	3399	3.399	0.15558661331292464
X	0.01261514286273224	9147	9.147	0.11131093514274983
X	0.013467762671943934	2986	2.986	0.1652222569735948
X	0.012769583336364295	6607	6.607	0.12456350116226456
X	0.01248228665114583	1500	1.5	0.2026442557674053
X	0.012780363204689577	6691	6.691	0.12407493473908601
X	0.01359200876679182	3198	3.198	0.16198261116325943
X	0.012719283063077473	750	0.75	0.2569215044084736
X	0.012492696058916267	723	0.723	0.2585270311369917
X	0.012605035243642149	4393	4.393	0.14210006841180547
X	0.012741704023202342	7359	7.359	0.12007969327714633
X	0.012308005886360511	1557	1.557	0.19920475199760843
X	0.012563854195076192	1838	1.838	0.1897837950702862
X	0.012924047252046402	2814	2.814	0.16622313975924463
X	0.01257695759083412	1309	1.309	0.21259121628720204
X	0.012681738455063481	1058	1.058	0.22885709178805685
X	0.012577905659427312	885	0.885	0.2422265186566353
X	0.012385622766222596	1527	1.527	0.2009214346187372
X	0.012258242889345088	938	0.938	0.23554567158322803
X	0.013669605102754532	3314	3.314	0.16037419452374502
X	0.012796036532114334	2175	2.175	0.18052557488029233
X	0.012880046953399902	4252	4.252	0.14469095955627248
X	0.012778612432008816	2209	2.209	0.179513036924229
X	0.01248252283160729	1070	1.07	0.22679816190003924
X	0.01368690272094798	4069	4.069	0.1498324324887419
X	0.01260262704302435	4377	4.377	0.14226394442860205
X	0.012722954926428987	3201	3.201	0.1584044719752647
X	0.013348177142167844	2497	2.497	0.17485087355301343
X	0.012501805695059204	4306	4.306	0.1426591306303603
X	0.013687796616827468	7179	7.179	0.1240004315840956
X	0.01281688225232822	5215	5.215	0.13495097519672977
X	0.012933349784735386	8013	8.013	0.11730196667477658
X	0.012771023560407265	4507	4.507	0.1415074753290205
X	0.01252752548066928	1749	1.749	0.19276365916182064
X	0.012592359100891909	1215	1.215	0.21802699750177199
X	0.012638272588212542	17229	17.229	0.09018671100912065
X	0.012329432006547347	1303	1.303	0.21151096405672903
X	0.013428200022468197	3331	3.331	0.15915283886340825
X	0.012726824055556465	409	0.409	0.3145325365919701
X	0.012721469399720499	1772	1.772	0.1929113824731283
X	0.012665537543243482	1259	1.259	0.21587348307067325
X	0.012343310026251622	1380	1.38	0.20757936717305944
X	0.012607848707036377	1626	1.626	0.19792780837448434
X	0.012614788419941209	2157	2.157	0.18016756481023588
X	0.012853110076426904	3065	3.065	0.16125972543660647
X	0.012594188247181188	2635	2.635	0.16844696364527556
X	0.012508355719596386	2221	2.221	0.17791690768374951
X	0.013256875100431692	2294	2.294	0.17945239147690173
X	0.012735057314166487	964	0.964	0.23639663484511447
X	0.013099749609448484	6265	6.265	0.1278735257394455
X	0.012180924952989253	711	0.711	0.2577924667933977
X	0.012803033886258551	9615	9.615	0.11001566682975669
X	0.012557529610840178	1073	1.073	0.22703953901392016
X	0.012614525658631623	1711	1.711	0.19462874024842886
X	0.01280377669496182	2064	2.064	0.18374243441889854
X	0.012439072998816796	1963	1.963	0.185050063714447
X	0.0127584143436741	3551	3.551	0.15316117048554356
X	0.012409275111915866	915	0.915	0.23847440448711352
X	0.012881569735916601	2113	2.113	0.18267947884738575
X	0.012876547467783854	6048	6.048	0.12864588043152808
X	0.0125850096851605	2352	2.352	0.17490628368809413
X	0.012938671905580934	18228	18.228	0.08920384206951837
X	0.012542675872698316	1352	1.352	0.21012182745694083
X	0.013653196912626607	3798	3.798	0.1531886061184096
X	0.012319678937954643	1189	1.189	0.21800804067303484
X	0.012370460804399517	2199	2.199	0.17784986890617288
X	0.012415509588922957	1496	1.496	0.2024623711070407
X	0.012577940940373306	1944	1.944	0.18633928734397548
X	0.012476304172707377	2009	2.009	0.18380980424185425
X	0.012372322724721857	1667	1.667	0.1950616439164094
X	0.01271538227618568	3156	3.156	0.1591222125653452
X	0.012748300697960666	2023	2.023	0.18470790054485747
X	0.012665505976571423	679	0.679	0.2652065565219658
X	0.012718876586949935	5279	5.279	0.1340599430115085
X	0.01463487414149404	8332	8.332	0.12065534807575404
X	0.013999540139233694	3628	3.628	0.15684914797635768
X	0.012500325248734921	1634	1.634	0.197040917154906
X	0.012742854748399595	1822	1.822	0.1912373806978479
X	0.012513082802202386	1735	1.735	0.19320644657237163
X	0.012901975446315411	1586	1.586	0.2011180313963049
X	0.012720188968445026	2226	2.226	0.17878159233757634
X	0.012613652535002014	2045	2.045	0.18339289955482665
X	0.013392153900665938	3079	3.079	0.16323511174666705
X	0.012792551824038396	1826	1.826	0.19134574270069857
X	0.012532198600364	6317	6.317	0.12565278839303592
X	0.013224586282019354	4521	4.521	0.1430153091173717
X	0.012584739453050508	8011	8.011	0.1162480916838013
X	0.012847679839854605	4425	4.425	0.14266052245914707
X	0.012788809681756443	1849	1.849	0.19053045492580276
X	0.012392956378162357	1921	1.921	0.18615834904085315
X	0.01261792959044332	1007	1.007	0.2322663042892845
X	0.012820957174664731	6862	6.862	0.12316573756994806
X	0.012517865472780283	964	0.964	0.23504503514361472
X	0.012773580684772433	2861	2.861	0.1646638501913687
X	0.012718708151038106	481	0.481	0.2979197593677227
X	0.012612428576217293	4341	4.341	0.14269310161712326
X	0.012822839387312516	2444	2.444	0.17376447811105558
X	0.012688103412902468	1930	1.93	0.1873325019922726
X	0.012716176007595455	3927	3.927	0.14794440415403062
X	0.01329799517849796	2239	2.239	0.18109676504480135
X	0.01249627062880743	575	0.575	0.27906424256790496
X	0.012309061741091889	588	0.588	0.2756021119929598
X	0.012963043769186432	1485	1.485	0.20590181643660685
X	0.012777285138761103	6322	6.322	0.12643326843160443
X	0.012782099513383289	3018	3.018	0.1617934688253559
X	0.013255530536294058	1829	1.829	0.1935208678721452
X	0.012853815908281137	1529	1.529	0.2033331537643558
X	0.012452251431462563	3791	3.791	0.14864992654351844
X	0.012724511713889273	2719	2.719	0.16726714315611907
X	0.012651307868914463	3356	3.356	0.15563400401635635
X	0.012466417262330469	2046	2.046	0.1826467756816965
X	0.013450620557259919	5077	5.077	0.1383709914257501
X	0.012501085450575065	1691	1.691	0.19480557077569205
X	0.012689417372874304	4818	4.818	0.13809934821074257
X	0.012678754102140968	1186	1.186	0.22029127506030763
X	0.013719694361209374	4131	4.131	0.14919802716185823
X	0.012827239119129773	2873	2.873	0.16466419989125736
X	0.01261700665152841	2432	2.432	0.17311349340818752
X	0.013350104077725883	2798	2.798	0.16834968719476387
X	0.013740338163791478	3220	3.22	0.1621986305774042
X	0.012836417010963528	4551	4.551	0.14129029350327657
X	0.012657842524494587	2415	2.415	0.17370574589448742
X	0.012796503541363441	968	0.968	0.23644963864001137
X	0.012776263813229764	2026	2.026	0.1847515723817981
X	0.012495901328090775	1541	1.541	0.20090387626562897
X	0.012766682518405535	6128	6.128	0.12771829159271136
X	0.012352552165115502	985	0.985	0.23233081828185062
X	0.0127518757463377	949	0.949	0.23774019794419782
X	0.012576082065579324	1328	1.328	0.21156756901548238
X	0.012700915944531176	4644	4.644	0.13984521477219836
X	0.012505123434549214	1006	1.006	0.2316487711614145
X	0.013642514334873243	9342	9.342	0.11345357498062551
X	0.012831670676860318	2356	2.356	0.17594191161528203
X	0.013470081393847165	19133	19.133	0.08896019294141587
X	0.012626398258432233	1568	1.568	0.20043696054643415
X	0.012623596201623472	3651	3.651	0.15121344938871595
X	0.012643257208902501	972	0.972	0.2351784747372238
X	0.013642335296524324	2010	2.01	0.189334992715838
X	0.012435388218370333	1619	1.619	0.1973047313609593
X	0.013149456129481728	1182	1.182	0.22323559179865152
X	0.013855929751198509	5030	5.03	0.14018102437021615
X	0.012872037943586531	10157	10.157	0.10821661694454765
X	0.012523159749770203	1509	1.509	0.2024610851667043
X	0.012574066443574585	1806	1.806	0.19094985590315638
X	0.012849080417037214	9888	9.888	0.10912423105971292
X	0.012533901590653984	852	0.852	0.2450279135304645
X	0.012511950303566896	861	0.861	0.2440285389761838
X	0.01265676647218153	640	0.64	0.27042547281115675
X	0.0124402478220378	1474	1.474	0.2035997230136751
X	0.012559438502781182	2486	2.486	0.17158910137587574
X	0.012639258364420201	2895	2.895	0.16343974441859122
X	0.012784580638348836	3737	3.737	0.15067960150852439
X	0.012528158776642803	1507	1.507	0.2025775582811287
X	0.012652305214810785	3894	3.894	0.14811233804287627
X	0.012801219779917297	3687	3.687	0.1514233112069003
X	0.012832829433981362	1924	1.924	0.18823734328934216
X	0.013680791203927374	2343	2.343	0.18007204998621368
X	0.012427765476840965	1662	1.662	0.1955482842995489
X	0.012451512313681981	1434	1.434	0.2055374592472617
X	0.012761201766240298	3356	3.356	0.15608333682856218
X	0.012458350580479925	1260	1.26	0.2146330848453796
X	0.012699552622199444	1723	1.723	0.19461114813653504
X	0.01362755534933621	2379	2.379	0.17892643303448558
X	0.013302336873366365	988	0.988	0.23789774882874073
X	0.013726123238770463	2952	2.952	0.16690801039972752
X	0.012295820062972881	1506	1.506	0.20136199672580088
X	0.012583856544801772	1857	1.857	0.18923463128605464
X	0.012781177376422488	5440	5.44	0.13294050395180176
X	0.013950456181049186	3203	3.203	0.16330913941052624
X	0.012576069131510497	3496	3.496	0.1532231891376796
X	0.01338269265057506	9244	9.244	0.11312567642853302
X	0.012806059529555101	30328	30.328	0.0750223357951229
X	0.014538057227772457	33655	33.655	0.07559369830780613
X	0.012572982580121805	14961	14.961	0.09436824903574183
X	0.012440768336129846	1130	1.13	0.22246221300650154
X	0.012732490230934711	7009	7.009	0.12201665001930337
X	0.013463860891293781	12608	12.608	0.10221339304539097
X	0.01356312878384564	102905	102.905	0.05089101773903512
X	0.012874139579078837	2487	2.487	0.17298726911916418
X	0.01351534998561001	121893	121.893	0.0480414577332777
X	0.013111973966737562	28265	28.265	0.07741165685424144
X	0.013528178645396774	76568	76.568	0.05611306726018841
X	0.01457909575846647	131515	131.515	0.0480380201222095
X	0.01276189417474465	1988	1.988	0.1858515780402349
X	0.012845599110543976	9401	9.401	0.11096689579977277
X	0.013645426627385718	203611	203.611	0.04061894387563494
X	0.014437470005990265	367546	367.546	0.033993284688596095
X	0.013751478384004642	4673	4.673	0.1433016540217638
X	0.013265135084441399	33401	33.401	0.07350508810862884
X	0.013093611090216364	18710	18.71	0.08878274120368419
X	0.013901984532767593	52469	52.469	0.06422804031339781
X	0.01301138611572616	93287	93.287	0.05186019506561383
X	0.013405992479722828	25865	25.865	0.08032711381968381
X	0.014213765131624772	19883	19.883	0.08941473095666351
X	0.01286317620725781	10334	10.334	0.10757051649998198
X	0.013287141515482777	2695	2.695	0.17019923236627205
X	0.014669724960361443	157729	157.729	0.04530750431741095
X	0.013394762805634891	22420	22.42	0.0842234718243133
X	0.012839070936752925	4112	4.112	0.14615942667855508
X	0.012957993007773267	62948	62.948	0.059045291141508235
X	0.012969189024535227	81879	81.879	0.05410616634543246
X	0.01453617290955219	199021	199.021	0.041800716442309614
X	0.013356726183000657	92375	92.375	0.05248661003933058
X	0.013334317682354279	18661	18.661	0.08940160074433384
X	0.012920468078698288	30997	30.997	0.074699815307937
X	0.013690762834136448	2721	2.721	0.1713561523148833
X	0.01285550365995639	19511	19.511	0.08701681620040233
X	0.013731058889622063	51891	51.891	0.06420034026700087
X	0.013256280375143571	11783	11.783	0.10400525131860106
X	0.012922068446994017	66603	66.603	0.057891226203683976
X	0.012026864796770845	2221	2.221	0.17560408860515506
X	0.012820116697011079	9925	9.925	0.1089065050002314
X	0.012954878006100094	51273	51.273	0.06321908409623322
X	0.013045406577141433	84540	84.54	0.053637079792127736
X	0.013033088732194203	78140	78.14	0.05504585837046906
X	0.012815183969141832	7234	7.234	0.12099909262937035
X	0.013420874972553405	149705	149.705	0.04475568154267417
X	0.012905431872700848	11004	11.004	0.10545665383324399
X	0.013163333993353793	17346	17.346	0.09121274369574359
X	0.012896211690489998	55748	55.748	0.061387171157168664
X	0.012767779237752318	3302	3.302	0.15695654562554434
X	0.013668146882626818	149318	149.318	0.04506774458250614
X	0.014042737904624533	101230	101.23	0.051766346834760675
X	0.014695122413255738	247809	247.809	0.03899587871765113
X	0.01391332224243386	95791	95.791	0.052565650791605155
X	0.012946842580164129	53045	53.045	0.062494213185413774
X	0.012744375988016348	54512	54.512	0.0616039868962055
X	0.012671097929831527	6463	6.463	0.12515840804696862
X	0.013371503715821732	50867	50.867	0.06405917995964967
X	0.0142493801916823	7620	7.62	0.12320083962792969
X	0.012933489390305072	15409	15.409	0.0943294028875909
X	0.013040017032535841	29598	29.598	0.07609192963118194
X	0.01287579165800482	17150	17.15	0.09088731468311112
X	0.012599442420684072	10119	10.119	0.10758157797109295
X	0.013783743294973668	2507	2.507	0.17649711632622198
X	0.01289456267482166	22018	22.018	0.08366482686697828
X	0.013779091664794206	9622	9.622	0.11271587296069624
X	0.01346110556406738	29212	29.212	0.07723956383654244
X	0.013888391874449391	7384	7.384	0.123439292727247
X	0.014319364968206323	60101	60.101	0.061993670892790995
X	0.013965398685112576	28801	28.801	0.07856249313943639
X	0.013990181262637002	78946	78.946	0.05616913515848459
X	0.013334418837830769	5611	5.611	0.13344775577301146
X	0.013179570678318474	25209	25.209	0.08055921303842038
X	0.01286437968260255	8927	8.927	0.11295213342553659
X	0.01372257059070638	85549	85.549	0.05433422940489105
X	0.012809920335323527	15660	15.66	0.09352295567944477
X	0.013571987376624092	12106	12.106	0.10388374379630026
X	0.012885906333001953	82643	82.643	0.05382321454350775
X	0.014452099554680324	84751	84.751	0.05545346708044602
X	0.01409207434738233	16549	16.549	0.09478385976129693
X	0.013530540743145924	130583	130.583	0.04696880424590341
X	0.014579738310587889	134615	134.615	0.047667102958165924
X	0.014559110196554691	267932	267.932	0.03787652105936366
X	0.013124270575323192	24579	24.579	0.08112791635510798
X	0.012624780647733806	1491	1.491	0.20382090826155458
X	0.012688666303265808	6483	6.483	0.12508732873362188
X	0.012894022738795596	30516	30.516	0.07503898341616407
X	0.01338854203938808	30128	30.128	0.07631109077357338
X	0.014632706779886342	65434	65.434	0.06069790945416099
X	0.01254707354606688	2091	2.091	0.18171724209586781
X	0.014586467747075864	52272	52.272	0.06534720341016355
X	0.013578980427938741	51384	51.384	0.06417212455605097
X	0.012869670294914209	43107	43.107	0.06683539829206914
X	0.013537206053125245	5238	5.238	0.13723172268636966
X	0.013521681726270935	50040	50.04	0.06465038806557012
X	0.013784905824033189	10591	10.591	0.10918314798585037
X	0.013834268496468634	14569	14.569	0.09828988319861506
X	0.01394898017094342	17612	17.612	0.09252191079058834
X	0.013257481780766639	10284	10.284	0.10883443415850481
X	0.0132143255732133	23965	23.965	0.08200163715644847
X	0.013134481252057252	37291	37.291	0.0706213960437666
X	0.013715362454912085	18183	18.183	0.09102917012706897
X	0.013456010730063103	19407	19.407	0.08850870224464169
X	0.01280629424221192	19906	19.906	0.08632696886686222
X	0.01422508690255778	488992	488.992	0.030755141162668406
X	0.013716122470612057	50670	50.67	0.06468846065296745
X	0.013269900356567173	146328	146.328	0.04492761953440925
X	0.012842214470987528	1867	1.867	0.19018015639457275
X	0.01338645226826054	47154	47.154	0.06572273856816051
X	0.013966947391740337	4623	4.623	0.1445636820773839
X	0.013454747532760134	29218	29.218	0.07722211451655027
X	0.012607690001293078	2638	2.638	0.16844323627668809
X	0.01345219409318887	55879	55.879	0.062208276101714206
X	0.012903468829585355	18181	18.181	0.08919959929093889
X	0.012859525867265704	21068	21.068	0.0848269559079181
X	0.014277049274168268	212752	212.752	0.040636996573008816
X	0.014303351933339323	43748	43.748	0.0688905851634926
X	0.013647256320880055	29109	29.109	0.07768539067664917
X	0.013636710122627384	95176	95.176	0.05232718677305252
X	0.014603575875165663	211285	211.285	0.04103900744690325
X	0.012828769345357387	7215	7.215	0.12114799207235977
X	0.013605272843450398	11232	11.232	0.10659824474471702
X	0.012770136614034986	5611	5.611	0.1315381586987828
X	0.012710191470453791	2189	2.189	0.17973615374730342
X	0.013279967218908822	29121	29.121	0.07697155130793344
X	0.01284160930311876	17597	17.597	0.09003131763396507
X	0.012622375466099374	3630	3.63	0.15149960127860804
X	0.012928462576463379	17929	17.929	0.08967339352296347
X	0.013471690007670982	54305	54.305	0.06283391028606297
X	0.012798089899873107	6438	6.438	0.12573747304708227
X	0.014031292176250014	298134	298.134	0.036104559752833144
X	0.012523404248758137	7643	7.643	0.11789250072322507
X	0.013998485885788862	64567	64.567	0.06007449820551274
X	0.012836891074975425	6425	6.425	0.12594924680504757
X	0.01328942499684053	33079	33.079	0.07378780750388679
X	0.014634119085401149	180585	180.585	0.04327412585000451
X	0.014701455592434947	363969	363.969	0.0343108897493721
X	0.01416607961083619	566139	566.139	0.02924885491424013
X	0.012928408957588032	28064	28.064	0.07723221727388092
X	0.01387984112309675	32744	32.744	0.07511919428283914
X	0.01286658383642172	13670	13.67	0.09800124204075578
X	0.012769357052223268	11898	11.898	0.10238390473123042
X	0.012940256263396734	64716	64.716	0.05847592093507607
X	0.014449016659599467	32107	32.107	0.07663247542201564
X	0.01271831491026774	7754	7.754	0.11793279528618332
X	0.013136980410306663	65173	65.173	0.058633067164087636
X	0.013814385025900093	246355	246.355	0.03827572200921756
X	0.01272961972734313	46764	46.764	0.06480889142274905
X	0.013374547516605046	15747	15.747	0.09470227750321464
X	0.01437474169246699	10380	10.38	0.11146389935158083
X	0.013549897945806216	16955	16.955	0.09279959411326029
X	0.012818485107694035	5791	5.791	0.1303249937418774
X	0.014332319093722854	123590	123.59	0.04876531630234743
X	0.012916820130926087	20950	20.95	0.08511194754601696
X	0.013449719851332842	8609	8.609	0.11603442207047986
X	0.01348633351394632	56395	56.395	0.06207038390383508
X	0.014650225288991903	9006	9.006	0.1176081594935277
X	0.012255106322618808	594	0.594	0.2742690987874374
X	0.013119024307648832	47632	47.632	0.06506302328939982
X	0.013640047515007022	63953	63.953	0.05974730499459438
X	0.014569172971355447	97627	97.627	0.05304221503181774
X	0.014642656225395461	162515	162.515	0.044830694967469546
X	0.013387752770314567	90656	90.656	0.052857146050281155
X	0.012635795038920438	12258	12.258	0.10101696554556311
X	0.013559587809067453	85534	85.534	0.05412142652873407
X	0.012964044053185502	126546	126.546	0.04679118299542981
X	0.01444564630369946	158914	158.914	0.044963309794046426
X	0.012975770481375435	19273	19.273	0.08764516128176024
X	0.013384755020907128	87303	87.303	0.05352135332888154
X	0.01374525555150522	57583	57.583	0.06203253231328569
X	0.014298005662786427	48109	48.109	0.06673439255385719
X	0.013441929745647858	34121	34.121	0.07330719860458282
X	0.012846489998683751	19336	19.336	0.08725813945198645
X	0.01309462325939279	17885	17.885	0.09012971985501456
X	0.013339904041177521	11896	11.896	0.10389243294894515
X	0.012570697000972436	2638	2.638	0.16827832832192582
X	0.014002966475874339	168104	168.104	0.04367309803470412
X	0.013604684733342568	98251	98.251	0.05173492624479457
X	0.01345944324686952	234468	234.468	0.03857576128547223
X	0.014432066190157416	58471	58.471	0.06272817660060145
X	0.014081847551186194	79581	79.581	0.056141422877301186
X	0.012793313786471973	8780	8.78	0.1133694862931504
X	0.014669056632778734	85049	85.049	0.05566441610780662
X	0.01270569333696637	2483	2.483	0.1723219299071552
X	0.012851417747633515	22682	22.682	0.08274782074152266
X	0.01357309934381767	81596	81.596	0.05499671120133832
X	0.013227063769896496	61885	61.885	0.059789650857660685
X	0.012829563294989285	2535	2.535	0.17168985611806864
X	0.012812810880629624	7983	7.983	0.11708270896155923
X	0.014358016718473404	138579	138.579	0.04696766941077072
X	0.012997208756922028	17269	17.269	0.09096220266663088
X	0.014144863898589445	100605	100.605	0.05199876981299428
X	0.012567503228691039	13231	13.231	0.09829968180135654
X	0.012828843047122361	12863	12.863	0.09991140679598554
X	0.012593320070411473	2153	2.153	0.18017674514657492
X	0.013223648594703008	3156	3.156	0.16121474787552567
X	0.014483657600750047	44362	44.362	0.06885820711326707
X	0.014390077976601573	58937	58.937	0.06250168232747562
X	0.012542803372868895	1967	1.967	0.18543715317531245
X	0.013359137437284913	102409	102.409	0.050716206812588815
X	0.012652912098733706	15442	15.442	0.09357553402843405
X	0.013648476931838282	24198	24.198	0.08262311654740476
X	0.012662712876032784	6147	6.147	0.1272392739011909
X	0.013433747224371986	289747	289.747	0.03592466618900351
X	0.012751833791148054	3313	3.313	0.15671734632223858
X	0.013693111058562893	51502	51.502	0.06430222666808308
X	0.013211597835501878	46274	46.274	0.06584752173449283
X	0.013753355714940565	43879	43.879	0.06792828521858403
X	0.012924767002382811	34820	34.82	0.07186729827829236
X	0.014299185926730875	456313	456.313	0.031527012854593604
X	0.013652059570456102	38138	38.138	0.07100355389831621
X	0.012781112684833113	9923	9.923	0.10880325568804894
X	0.013103901799229671	41139	41.139	0.06829396060451236
X	0.012860556687033961	23054	23.054	0.08231983534209379
X	0.014379083809609228	278209	278.209	0.03724946274880296
X	0.013467598201617158	32262	32.262	0.07473653428766705
X	0.014378428363629405	139086	139.086	0.04693274978445222
X	0.01473034509119277	369476	369.476	0.03416191669784427
X	0.0127744806950258	14654	14.654	0.0955276342144659
X	0.013428334207476435	133495	133.495	0.046507077278771784
X	0.012711613463112633	16984	16.984	0.09079325073181131
X	0.014641548759236444	367560	367.56	0.0341522712985068
X	0.013240895816524368	22474	22.474	0.08383248321013086
X	0.013702192003542647	30573	30.573	0.07652746560505046
X	0.014242899850525766	10077	10.077	0.11222485527108249
X	0.012838362362311149	3847	3.847	0.1494384936027833
X	0.012827229568803954	5352	5.352	0.13382551423551625
X	0.012891355315967992	32725	32.725	0.07330602002619116
X	0.012894553590998078	17238	17.238	0.09077643960623447
X	0.014749416336675703	157581	157.581	0.0454036040351472
X	0.01280791060966456	10178	10.178	0.10796225501143322
X	0.013802444684172499	11449	11.449	0.10642969558808998
X	0.012773683959680215	7220	7.22	0.12094641218515059
X	0.014548153574456713	84929	84.929	0.055537196827605585
X	0.014558682331341522	135993	135.993	0.04748267437632793
X	0.01463405720779511	219449	219.449	0.04055182282693574
X	0.014602530592248412	31114	31.114	0.0777124783038091
X	0.012627708391284359	3375	3.375	0.1552447500199785
X	0.013856482953183783	4987	4.987	0.14058464261569248
X	0.012709411771578971	6465	6.465	0.12527150849144666
X	0.01473535697556447	424086	424.086	0.032631390106348065
X	0.01274187297245217	4767	4.767	0.13878079566372753
X	0.012865465267847676	17184	17.184	0.09080304467474434
X	0.012924289334781599	56526	56.526	0.061148546632275126
X	0.013118720533536037	22850	22.85	0.0831128979405307
X	0.014088150175901733	30299	30.299	0.07747148708565339
X	0.012798628539494502	8980	8.98	0.1125370947599251
X	0.013487805537166858	129878	129.878	0.047004045123055206
X	0.014488409194103526	51670	51.67	0.06545267701290813
X	0.012524120593856256	9681	9.681	0.1089621523975339
X	0.012893068844877217	22701	22.701	0.0828140017124773
X	0.013542385035414987	121360	121.36	0.04814374423736106
X	0.013338948340454907	53011	53.011	0.06313233677852571
X	0.014680830208187617	330301	330.301	0.035422584293030336
X	0.013149706404304186	79323	79.323	0.0549337156492242
X	0.013762100052952586	5325	5.325	0.13723188630845326
X	0.012868246618189312	31676	31.676	0.07406215541395143
X	0.012763266744893385	12201	12.201	0.10151311148560113
X	0.014401280771705457	7763	7.763	0.12287320236193824
X	0.01281447525753339	15698	15.698	0.09345850595728669
X	0.012666985920886463	7202	7.202	0.12070911252803827
X	0.013138466214490684	49579	49.579	0.06423163375303566
X	0.013628924660515039	81407	81.407	0.05511459411388911
X	0.01237244116679877	8455	8.455	0.11353086021092154
X	0.013075146607200754	34465	34.465	0.07239180879261682
X	0.013353867648520052	4943	4.943	0.13927467457535617
X	0.013672173218047649	30948	30.948	0.07616141214916297
X	0.013147662913715465	4686	4.686	0.14104209780910673
X	0.013211945040575237	9249	9.249	0.11262219643506892
X	0.01369955958049613	36273	36.273	0.07228381125237648
X	0.013719657644730515	10314	10.314	0.10997788980007149
X	0.01287682848334817	3700	3.7	0.15154294714800315
X	0.012674264734868119	11251	11.251	0.10405043092631473
X	0.013439919519868487	16907	16.907	0.09263534113134902
X	0.01451269616879849	107574	107.574	0.051287743889405504
X	0.013395293688148962	3340	3.34	0.15887976581361507
X	0.01227440893043402	1831	1.831	0.18855472269445195
X	0.012867905507681509	18735	18.735	0.08823036080746387
X	0.01299282349056856	74962	74.962	0.05575546700744069
X	0.014601110235436686	197364	197.364	0.041979698368988586
X	0.013481848651623734	108709	108.709	0.04986869256390852
X	0.01368037513499321	7893	7.893	0.12012091579275218
X	0.014486610095246703	173547	173.547	0.04370353367636142
X	0.014721448569452993	137426	137.426	0.0474927386147337
X	0.012791068368249302	2280	2.28	0.17768768304161123
X	0.012793266068827719	4648	4.648	0.1401431164542937
X	0.013194231072428609	53109	53.109	0.06286447898778023
X	0.012841223916894483	20836	20.836	0.08510022142177517
X	0.012817513961903946	40393	40.393	0.06820756777244529
X	0.013130078691000122	84233	84.233	0.053818099023800615
X	0.013096621043572655	38876	38.876	0.06958130385983423
X	0.012797516855436888	21381	21.381	0.08427509140620461
X	0.013278822284420021	2754	2.754	0.16893976052621815
X	0.013570667263530442	32701	32.701	0.07458990537488795
X	0.013835377583554102	40979	40.979	0.06963216310139135
X	0.012750006123591135	14215	14.215	0.09643940222842591
X	0.014510082023234524	44928	44.928	0.06860950096032264
X	0.014479500453315409	263849	263.849	0.03800138064430908
X	0.012808378475479545	7355	7.355	0.12031058125753462
X	0.012472113456739901	2475	2.475	0.1714437338368792
X	0.012769232866287361	6823	6.823	0.12323379190307815
X	0.013302573123678312	24682	24.682	0.08138014056165356
X	0.013025380882840146	23271	23.271	0.08241225452444913
X	0.01282654783980618	12676	12.676	0.10039432981580139
X	0.012806642927091636	1888	1.888	0.1892973075785485
X	0.013241121114974759	32128	32.128	0.07441840466120207
X	0.012882493698070506	14675	14.675	0.09575040279245778
X	0.013318202739998156	26444	26.444	0.07956208537752661
X	0.013355472750299661	2684	2.684	0.170723085557395
X	0.013418772264472238	8254	8.254	0.11758432739836834
X	0.013880274881152807	12262	12.262	0.10421868402864827
X	0.012853510502063821	7549	7.549	0.1194109381692235
X	0.013392139030495074	25548	25.548	0.08063018952037725
X	0.01315061432087832	107400	107.4	0.04965701237209685
X	0.013373671645064611	177862	177.862	0.04220745455298798
X	0.012885165683035216	76293	76.293	0.05527580866794288
X	0.014050537922793925	61144	61.144	0.06125097905060742
X	0.012851805353903008	12872	12.872	0.09994767656848036
X	0.014241812156069546	13252	13.252	0.1024301814453266
X	0.014122654284014982	16008	16.008	0.09590908567247886
X	0.013873657845649034	90310	90.31	0.053557263552771676
X	0.013645791819539472	9133	9.133	0.11432163524041793
X	0.01286973442082305	15730	15.73	0.09352914281479494
X	0.013677927271129889	9358	9.358	0.1134869043765351
X	0.014655634460659055	472594	472.594	0.03141753736423799
X	0.01292802291304875	38063	38.063	0.06977122788311622
X	0.014629485384213731	31655	31.655	0.07731473004795716
X	0.014247954075097827	127563	127.563	0.048158830006865924
X	0.01386758240248878	31279	31.279	0.07625165923596534
X	0.013291507967907486	25527	25.527	0.08044977352714962
X	0.01443510445889721	300130	300.13	0.036366667647522326
X	0.01418021533209531	181746	181.746	0.042730632518170907
X	0.01296228990941394	79858	79.858	0.05454912228828143
X	0.012843612863066184	31967	31.967	0.0737895919023158
X	0.013825767309429486	46257	46.257	0.06686064985083481
X	0.012793355936478863	3545	3.545	0.1533873041126821
X	0.0138232783388821	122958	122.958	0.0482634193571157
X	0.014230273409856645	653754	653.754	0.027921128138904763
X	0.014304089277653517	50989	50.989	0.06546279929129865
X	0.0134152703373621	48718	48.718	0.06505839214262589
X	0.013370158547822692	19554	19.554	0.08809818925742664
X	0.012927246290720241	31251	31.251	0.07450990930012458
X	0.014628182050388933	90201	90.201	0.05453303491916414
X	0.012746356354847563	11563	11.563	0.10330115486991145
X	0.01366365827743941	15631	15.631	0.09561515710417984
X	0.01455524444981882	337975	337.975	0.03505190631889903
X	0.013322559934455679	38381	38.381	0.07027871286751883
X	0.012944013485820046	150099	150.099	0.04418047552324883
X	0.012786967311132963	48506	48.506	0.06411958421563252
X	0.01287372783264216	57456	57.456	0.06073741605753634
X	0.013242429956076407	36236	36.236	0.07149502315717568
X	0.013387605011997572	88596	88.596	0.053263486632994544
X	0.013148105512447675	165486	165.486	0.042989986359222146
X	0.012598409974561488	6283	6.283	0.12610033076620705
X	0.014617549489518421	554533	554.533	0.029761105472318253
X	0.013421458180496556	23732	23.732	0.0826967523351091
X	0.014669435130802152	45283	45.283	0.06867933266520922
X	0.01283968742426076	2335	2.335	0.1765045297414132
X	0.013493484089067384	55993	55.993	0.062229568294486955
X	0.01240774004564037	2539	2.539	0.16969797569647768
X	0.0121989466107064	3000	3.0	0.1596125460572816
X	0.013444526957582784	12418	12.418	0.10268286010892833
X	0.014333908004497513	297026	297.026	0.03640743214061664
X	0.014592261237942859	1101881	1101.881	0.02365897727841224
X	0.012780384417096672	5935	5.935	0.12913413702168347
X	0.014475709130256711	344433	344.433	0.03476789236890408
X	0.012768446941693372	8972	8.972	0.11248197599273146
X	0.013413962279478732	13944	13.944	0.09871653482173612
X	0.013869034674252262	9257	9.257	0.11442615989615519
X	0.013199009088491824	58659	58.659	0.060823147575748394
X	0.01281330092182049	8950	8.95	0.11270573013135196
X	0.014725524927118006	841859	841.859	0.025958353481000396
X	0.012596098388120947	3880	3.88	0.1480703511156589
X	0.013943035853845426	101180	101.18	0.05165204923841701
X	0.013809120454250328	14920	14.92	0.09745386434830053
X	0.013926506679788871	65722	65.722	0.05961796397118682
X	0.01330909884832593	114636	114.636	0.04878382797826965
X	0.012839453877060902	23531	23.531	0.08171505096221707
X	0.012842199727210561	10998	10.998	0.10530328065034854
X	0.01437306213900171	91885	91.885	0.0538809065936712
X	0.013515783605953633	6423	6.423	0.12814483394058462
X	0.012652548630577798	6056	6.056	0.1278392015747391
X	0.014277536204455371	219963	219.963	0.040188445876245524
X	0.012890465674825938	31109	31.109	0.07455226662959116
X	0.01403603815847394	32022	32.022	0.07596240315627623
X	0.012630560984027869	22050	22.05	0.0830496899488275
X	0.01468863081938316	109365	109.365	0.051211518407768095
X	0.013791248354015564	158487	158.487	0.04431356037836849
X	0.013712364624762	95422	95.422	0.05237868761889917
X	0.013894079573507453	6062	6.062	0.13184731294064242
X	0.012798298966386651	87729	87.729	0.052642482004517266
X	0.013099514262751645	39838	39.838	0.06902173759213565
X	0.014135559587568205	34801	34.801	0.0740583037655175
X	0.013476330386936684	73470	73.47	0.05681816495177188
X	0.012790866174036424	34544	34.544	0.07180848707205965
X	0.014424437002287013	42481	42.481	0.06976453650878148
X	0.013104662489149616	268595	268.595	0.03654062506256107
X	0.01372718345696795	32712	32.712	0.07486717513419067
X	0.012651871187854208	21335	21.335	0.08401445957686138
X	0.012475446454263066	3066	3.066	0.1596472034325897
X	0.01401293289093853	14804	14.804	0.09818609516005906
X	0.012958673409566156	50341	50.341	0.0636130524367363
X	0.014707228858612897	112325	112.325	0.05077907760962691
X	0.01332834836169858	55375	55.375	0.06220436561932992
X	0.012791133088753756	7637	7.637	0.11875778141802694
X	0.013290952724580791	20161	20.161	0.08703247777634074
X	0.01437311217358395	36476	36.476	0.07331303566355053
X	0.013792835100812433	111957	111.957	0.049758517263799436
X	0.0136861330529066	32971	32.971	0.07459611551543117
X	0.014183751714143456	44424	44.424	0.06834779161279206
X	0.012913896379308242	14697	14.697	0.09578030177658926
X	0.013770420854536228	31304	31.304	0.07605290243673357
X	0.01284098526919107	5466	5.466	0.13293609181184676
X	0.013718177329579266	9222	9.222	0.11415381537632922
X	0.014537838920453095	210651	210.651	0.04101840565893252
X	0.012800857822641216	23098	23.098	0.08214003791473734
X	0.013674901237800444	53219	53.219	0.06357492105639863
X	0.013465455991608172	25491	25.491	0.0808372251422152
X	0.013040548607170424	2190	2.19	0.18125246257874011
X	0.013320816867275452	107317	107.317	0.04988317768581597
X	0.01364885584174335	100892	100.892	0.05133495336931849
X	0.013992472353340836	45162	45.162	0.06766650781990287
X	0.012827662273674153	3182	3.182	0.159153348608014
X	0.012852850101005358	16667	16.667	0.09170239303773557
X	0.013438459685528493	44727	44.727	0.066977136254236
X	0.013845089784456227	30964	30.964	0.07646797201773224
X	0.012826919341641452	44796	44.796	0.06591147682187984
X	0.012823394909402326	3475	3.475	0.1545311616667585
X	0.01261494743267041	9176	9.176	0.11119297415088045
X	0.013083469089466905	71287	71.287	0.05682911045081646
X	0.012918113762091725	14109	14.109	0.09710336730809747
X	0.014374192730170404	158979	158.979	0.04488293350503452
X	0.012856174472484007	9807	9.807	0.10944397564666554
X	0.013402800171269586	46319	46.319	0.06614222331912409
X	0.014183370248475923	72376	72.376	0.0580846826516332
X	0.013684687178436215	102379	102.379	0.05112986707499506
X	0.013411368259441689	107864	107.864	0.04991129697888096
X	0.01350341774295965	86719	86.719	0.05379928212897142
X	0.012504635087899682	1449	1.449	0.20511663305374409
X	0.012966905187945053	53097	53.097	0.0625060589396001
X	0.01291660206470021	19259	19.259	0.08753294002059155
X	0.01450798793061632	219792	219.792	0.040413993589202044
X	0.012708376425106986	46750	46.75	0.06477928522319053
X	0.013542801129352355	97471	97.471	0.05179379018219236
X	0.01456040248302725	40288	40.288	0.0712304832160601
X	0.013229673665510786	16828	16.828	0.0922937324053881
X	0.012997911771934045	32416	32.416	0.0737402694737495
X	0.01280823814172699	5490	5.49	0.13262915920129148
X	0.013447677373211332	29782	29.782	0.07671809448299291
X	0.013675436942267367	120530	120.53	0.04841151624445955
X	0.014494502029846385	106472	106.472	0.05144256667243011
X	0.013060088140830076	5055	5.055	0.1372171251084708
X	0.012895171329683458	16679	16.679	0.0917809121060602
X	0.012877394574806236	13360	13.36	0.09878109778668295
X	0.01454926805124533	191255	191.255	0.04237172215642983
X	0.012969011591580572	61948	61.948	0.05937813182865626
X	0.012712653707907466	8572	8.572	0.11403846201391299
X	0.014102521907926198	89512	89.512	0.05400970942254266
X	0.013658915830785896	66739	66.739	0.05893123075694305
X	0.01268652536707529	9736	9.736	0.10922466188577148
X	0.01429305753407927	59123	59.123	0.062295435513380445
X	0.01288430436438446	19405	19.405	0.08724002395528613
time for making epsilon is 1.8853087425231934
epsilons are
[0.20820962425245632, 0.08579877038607024, 0.22380736717838132, 0.13052690891612095, 0.05197161547777684, 0.05959427840543141, 0.07635094889550144, 0.07586647947038726, 0.1117359331168103, 0.08436222868340437, 0.12544917048493875, 0.06588320583235326, 0.1272899399931346, 0.03822520403519092, 0.08982999580721956, 0.05937080879180914, 0.0703029457619669, 0.07286768855503198, 0.04094506862153246, 0.05928054789011486, 0.05856888095856576, 0.03461206313470033, 0.09893885020845627, 0.09600646441535854, 0.08517354046306881, 0.06571121062899447, 0.08024681316919645, 0.10306477904415118, 0.061736122276378515, 0.019787655429388023, 0.09342088426747135, 0.03612141053595573, 0.10801963788411208, 0.10834996488677241, 0.06210785095341049, 0.05260672428833288, 0.0612767746496468, 0.21083710042149353, 0.16083763408783022, 0.1954138016115345, 0.1784103277076013, 0.2512341549451053, 0.2197301568947346, 0.202176183770603, 0.26227554955021065, 0.2797486442662036, 0.21006215729579092, 0.16125565680748513, 0.13703816698358626, 0.1326940360642607, 0.11323307144889133, 0.174119760775925, 0.18611915745009497, 0.20880109367167712, 0.15858442455271463, 0.10892004573953167, 0.23592779172536918, 0.15291886062501914, 0.135479255278858, 0.2153100509591997, 0.11896860631789363, 0.21677449997240975, 0.1386355274209499, 0.1073405834869548, 0.16725466618665838, 0.1986780650150839, 0.17345989497381295, 0.15303592566586227, 0.14210240593598497, 0.15558661331292464, 0.11131093514274983, 0.1652222569735948, 0.12456350116226456, 0.2026442557674053, 0.12407493473908601, 0.16198261116325943, 0.2569215044084736, 0.2585270311369917, 0.14210006841180547, 0.12007969327714633, 0.19920475199760843, 0.1897837950702862, 0.16622313975924463, 0.21259121628720204, 0.22885709178805685, 0.2422265186566353, 0.2009214346187372, 0.23554567158322803, 0.16037419452374502, 0.18052557488029233, 0.14469095955627248, 0.179513036924229, 0.22679816190003924, 0.1498324324887419, 0.14226394442860205, 0.1584044719752647, 0.17485087355301343, 0.1426591306303603, 0.1240004315840956, 0.13495097519672977, 0.11730196667477658, 0.1415074753290205, 0.19276365916182064, 0.21802699750177199, 0.09018671100912065, 0.21151096405672903, 0.15915283886340825, 0.3145325365919701, 0.1929113824731283, 0.21587348307067325, 0.20757936717305944, 0.19792780837448434, 0.18016756481023588, 0.16125972543660647, 0.16844696364527556, 0.17791690768374951, 0.17945239147690173, 0.23639663484511447, 0.1278735257394455, 0.2577924667933977, 0.11001566682975669, 0.22703953901392016, 0.19462874024842886, 0.18374243441889854, 0.185050063714447, 0.15316117048554356, 0.23847440448711352, 0.18267947884738575, 0.12864588043152808, 0.17490628368809413, 0.08920384206951837, 0.21012182745694083, 0.1531886061184096, 0.21800804067303484, 0.17784986890617288, 0.2024623711070407, 0.18633928734397548, 0.18380980424185425, 0.1950616439164094, 0.1591222125653452, 0.18470790054485747, 0.2652065565219658, 0.1340599430115085, 0.12065534807575404, 0.15684914797635768, 0.197040917154906, 0.1912373806978479, 0.19320644657237163, 0.2011180313963049, 0.17878159233757634, 0.18339289955482665, 0.16323511174666705, 0.19134574270069857, 0.12565278839303592, 0.1430153091173717, 0.1162480916838013, 0.14266052245914707, 0.19053045492580276, 0.18615834904085315, 0.2322663042892845, 0.12316573756994806, 0.23504503514361472, 0.1646638501913687, 0.2979197593677227, 0.14269310161712326, 0.17376447811105558, 0.1873325019922726, 0.14794440415403062, 0.18109676504480135, 0.27906424256790496, 0.2756021119929598, 0.20590181643660685, 0.12643326843160443, 0.1617934688253559, 0.1935208678721452, 0.2033331537643558, 0.14864992654351844, 0.16726714315611907, 0.15563400401635635, 0.1826467756816965, 0.1383709914257501, 0.19480557077569205, 0.13809934821074257, 0.22029127506030763, 0.14919802716185823, 0.16466419989125736, 0.17311349340818752, 0.16834968719476387, 0.1621986305774042, 0.14129029350327657, 0.17370574589448742, 0.23644963864001137, 0.1847515723817981, 0.20090387626562897, 0.12771829159271136, 0.23233081828185062, 0.23774019794419782, 0.21156756901548238, 0.13984521477219836, 0.2316487711614145, 0.11345357498062551, 0.17594191161528203, 0.08896019294141587, 0.20043696054643415, 0.15121344938871595, 0.2351784747372238, 0.189334992715838, 0.1973047313609593, 0.22323559179865152, 0.14018102437021615, 0.10821661694454765, 0.2024610851667043, 0.19094985590315638, 0.10912423105971292, 0.2450279135304645, 0.2440285389761838, 0.27042547281115675, 0.2035997230136751, 0.17158910137587574, 0.16343974441859122, 0.15067960150852439, 0.2025775582811287, 0.14811233804287627, 0.1514233112069003, 0.18823734328934216, 0.18007204998621368, 0.1955482842995489, 0.2055374592472617, 0.15608333682856218, 0.2146330848453796, 0.19461114813653504, 0.17892643303448558, 0.23789774882874073, 0.16690801039972752, 0.20136199672580088, 0.18923463128605464, 0.13294050395180176, 0.16330913941052624, 0.1532231891376796, 0.11312567642853302, 0.0750223357951229, 0.07559369830780613, 0.09436824903574183, 0.22246221300650154, 0.12201665001930337, 0.10221339304539097, 0.05089101773903512, 0.17298726911916418, 0.0480414577332777, 0.07741165685424144, 0.05611306726018841, 0.0480380201222095, 0.1858515780402349, 0.11096689579977277, 0.04061894387563494, 0.033993284688596095, 0.1433016540217638, 0.07350508810862884, 0.08878274120368419, 0.06422804031339781, 0.05186019506561383, 0.08032711381968381, 0.08941473095666351, 0.10757051649998198, 0.17019923236627205, 0.04530750431741095, 0.0842234718243133, 0.14615942667855508, 0.059045291141508235, 0.05410616634543246, 0.041800716442309614, 0.05248661003933058, 0.08940160074433384, 0.074699815307937, 0.1713561523148833, 0.08701681620040233, 0.06420034026700087, 0.10400525131860106, 0.057891226203683976, 0.17560408860515506, 0.1089065050002314, 0.06321908409623322, 0.053637079792127736, 0.05504585837046906, 0.12099909262937035, 0.04475568154267417, 0.10545665383324399, 0.09121274369574359, 0.061387171157168664, 0.15695654562554434, 0.04506774458250614, 0.051766346834760675, 0.03899587871765113, 0.052565650791605155, 0.062494213185413774, 0.0616039868962055, 0.12515840804696862, 0.06405917995964967, 0.12320083962792969, 0.0943294028875909, 0.07609192963118194, 0.09088731468311112, 0.10758157797109295, 0.17649711632622198, 0.08366482686697828, 0.11271587296069624, 0.07723956383654244, 0.123439292727247, 0.061993670892790995, 0.07856249313943639, 0.05616913515848459, 0.13344775577301146, 0.08055921303842038, 0.11295213342553659, 0.05433422940489105, 0.09352295567944477, 0.10388374379630026, 0.05382321454350775, 0.05545346708044602, 0.09478385976129693, 0.04696880424590341, 0.047667102958165924, 0.03787652105936366, 0.08112791635510798, 0.20382090826155458, 0.12508732873362188, 0.07503898341616407, 0.07631109077357338, 0.06069790945416099, 0.18171724209586781, 0.06534720341016355, 0.06417212455605097, 0.06683539829206914, 0.13723172268636966, 0.06465038806557012, 0.10918314798585037, 0.09828988319861506, 0.09252191079058834, 0.10883443415850481, 0.08200163715644847, 0.0706213960437666, 0.09102917012706897, 0.08850870224464169, 0.08632696886686222, 0.030755141162668406, 0.06468846065296745, 0.04492761953440925, 0.19018015639457275, 0.06572273856816051, 0.1445636820773839, 0.07722211451655027, 0.16844323627668809, 0.062208276101714206, 0.08919959929093889, 0.0848269559079181, 0.040636996573008816, 0.0688905851634926, 0.07768539067664917, 0.05232718677305252, 0.04103900744690325, 0.12114799207235977, 0.10659824474471702, 0.1315381586987828, 0.17973615374730342, 0.07697155130793344, 0.09003131763396507, 0.15149960127860804, 0.08967339352296347, 0.06283391028606297, 0.12573747304708227, 0.036104559752833144, 0.11789250072322507, 0.06007449820551274, 0.12594924680504757, 0.07378780750388679, 0.04327412585000451, 0.0343108897493721, 0.02924885491424013, 0.07723221727388092, 0.07511919428283914, 0.09800124204075578, 0.10238390473123042, 0.05847592093507607, 0.07663247542201564, 0.11793279528618332, 0.058633067164087636, 0.03827572200921756, 0.06480889142274905, 0.09470227750321464, 0.11146389935158083, 0.09279959411326029, 0.1303249937418774, 0.04876531630234743, 0.08511194754601696, 0.11603442207047986, 0.06207038390383508, 0.1176081594935277, 0.2742690987874374, 0.06506302328939982, 0.05974730499459438, 0.05304221503181774, 0.044830694967469546, 0.052857146050281155, 0.10101696554556311, 0.05412142652873407, 0.04679118299542981, 0.044963309794046426, 0.08764516128176024, 0.05352135332888154, 0.06203253231328569, 0.06673439255385719, 0.07330719860458282, 0.08725813945198645, 0.09012971985501456, 0.10389243294894515, 0.16827832832192582, 0.04367309803470412, 0.05173492624479457, 0.03857576128547223, 0.06272817660060145, 0.056141422877301186, 0.1133694862931504, 0.05566441610780662, 0.1723219299071552, 0.08274782074152266, 0.05499671120133832, 0.059789650857660685, 0.17168985611806864, 0.11708270896155923, 0.04696766941077072, 0.09096220266663088, 0.05199876981299428, 0.09829968180135654, 0.09991140679598554, 0.18017674514657492, 0.16121474787552567, 0.06885820711326707, 0.06250168232747562, 0.18543715317531245, 0.050716206812588815, 0.09357553402843405, 0.08262311654740476, 0.1272392739011909, 0.03592466618900351, 0.15671734632223858, 0.06430222666808308, 0.06584752173449283, 0.06792828521858403, 0.07186729827829236, 0.031527012854593604, 0.07100355389831621, 0.10880325568804894, 0.06829396060451236, 0.08231983534209379, 0.03724946274880296, 0.07473653428766705, 0.04693274978445222, 0.03416191669784427, 0.0955276342144659, 0.046507077278771784, 0.09079325073181131, 0.0341522712985068, 0.08383248321013086, 0.07652746560505046, 0.11222485527108249, 0.1494384936027833, 0.13382551423551625, 0.07330602002619116, 0.09077643960623447, 0.0454036040351472, 0.10796225501143322, 0.10642969558808998, 0.12094641218515059, 0.055537196827605585, 0.04748267437632793, 0.04055182282693574, 0.0777124783038091, 0.1552447500199785, 0.14058464261569248, 0.12527150849144666, 0.032631390106348065, 0.13878079566372753, 0.09080304467474434, 0.061148546632275126, 0.0831128979405307, 0.07747148708565339, 0.1125370947599251, 0.047004045123055206, 0.06545267701290813, 0.1089621523975339, 0.0828140017124773, 0.04814374423736106, 0.06313233677852571, 0.035422584293030336, 0.0549337156492242, 0.13723188630845326, 0.07406215541395143, 0.10151311148560113, 0.12287320236193824, 0.09345850595728669, 0.12070911252803827, 0.06423163375303566, 0.05511459411388911, 0.11353086021092154, 0.07239180879261682, 0.13927467457535617, 0.07616141214916297, 0.14104209780910673, 0.11262219643506892, 0.07228381125237648, 0.10997788980007149, 0.15154294714800315, 0.10405043092631473, 0.09263534113134902, 0.051287743889405504, 0.15887976581361507, 0.18855472269445195, 0.08823036080746387, 0.05575546700744069, 0.041979698368988586, 0.04986869256390852, 0.12012091579275218, 0.04370353367636142, 0.0474927386147337, 0.17768768304161123, 0.1401431164542937, 0.06286447898778023, 0.08510022142177517, 0.06820756777244529, 0.053818099023800615, 0.06958130385983423, 0.08427509140620461, 0.16893976052621815, 0.07458990537488795, 0.06963216310139135, 0.09643940222842591, 0.06860950096032264, 0.03800138064430908, 0.12031058125753462, 0.1714437338368792, 0.12323379190307815, 0.08138014056165356, 0.08241225452444913, 0.10039432981580139, 0.1892973075785485, 0.07441840466120207, 0.09575040279245778, 0.07956208537752661, 0.170723085557395, 0.11758432739836834, 0.10421868402864827, 0.1194109381692235, 0.08063018952037725, 0.04965701237209685, 0.04220745455298798, 0.05527580866794288, 0.06125097905060742, 0.09994767656848036, 0.1024301814453266, 0.09590908567247886, 0.053557263552771676, 0.11432163524041793, 0.09352914281479494, 0.1134869043765351, 0.03141753736423799, 0.06977122788311622, 0.07731473004795716, 0.048158830006865924, 0.07625165923596534, 0.08044977352714962, 0.036366667647522326, 0.042730632518170907, 0.05454912228828143, 0.0737895919023158, 0.06686064985083481, 0.1533873041126821, 0.0482634193571157, 0.027921128138904763, 0.06546279929129865, 0.06505839214262589, 0.08809818925742664, 0.07450990930012458, 0.05453303491916414, 0.10330115486991145, 0.09561515710417984, 0.03505190631889903, 0.07027871286751883, 0.04418047552324883, 0.06411958421563252, 0.06073741605753634, 0.07149502315717568, 0.053263486632994544, 0.042989986359222146, 0.12610033076620705, 0.029761105472318253, 0.0826967523351091, 0.06867933266520922, 0.1765045297414132, 0.062229568294486955, 0.16969797569647768, 0.1596125460572816, 0.10268286010892833, 0.03640743214061664, 0.02365897727841224, 0.12913413702168347, 0.03476789236890408, 0.11248197599273146, 0.09871653482173612, 0.11442615989615519, 0.060823147575748394, 0.11270573013135196, 0.025958353481000396, 0.1480703511156589, 0.05165204923841701, 0.09745386434830053, 0.05961796397118682, 0.04878382797826965, 0.08171505096221707, 0.10530328065034854, 0.0538809065936712, 0.12814483394058462, 0.1278392015747391, 0.040188445876245524, 0.07455226662959116, 0.07596240315627623, 0.0830496899488275, 0.051211518407768095, 0.04431356037836849, 0.05237868761889917, 0.13184731294064242, 0.052642482004517266, 0.06902173759213565, 0.0740583037655175, 0.05681816495177188, 0.07180848707205965, 0.06976453650878148, 0.03654062506256107, 0.07486717513419067, 0.08401445957686138, 0.1596472034325897, 0.09818609516005906, 0.0636130524367363, 0.05077907760962691, 0.06220436561932992, 0.11875778141802694, 0.08703247777634074, 0.07331303566355053, 0.049758517263799436, 0.07459611551543117, 0.06834779161279206, 0.09578030177658926, 0.07605290243673357, 0.13293609181184676, 0.11415381537632922, 0.04101840565893252, 0.08214003791473734, 0.06357492105639863, 0.0808372251422152, 0.18125246257874011, 0.04988317768581597, 0.05133495336931849, 0.06766650781990287, 0.159153348608014, 0.09170239303773557, 0.066977136254236, 0.07646797201773224, 0.06591147682187984, 0.1545311616667585, 0.11119297415088045, 0.05682911045081646, 0.09710336730809747, 0.04488293350503452, 0.10944397564666554, 0.06614222331912409, 0.0580846826516332, 0.05112986707499506, 0.04991129697888096, 0.05379928212897142, 0.20511663305374409, 0.0625060589396001, 0.08753294002059155, 0.040413993589202044, 0.06477928522319053, 0.05179379018219236, 0.0712304832160601, 0.0922937324053881, 0.0737402694737495, 0.13262915920129148, 0.07671809448299291, 0.04841151624445955, 0.05144256667243011, 0.1372171251084708, 0.0917809121060602, 0.09878109778668295, 0.04237172215642983, 0.05937813182865626, 0.11403846201391299, 0.05400970942254266, 0.05893123075694305, 0.10922466188577148, 0.062295435513380445, 0.08724002395528613]
0.09302170421255773
Making ranges
torch.Size([30465, 2])
We keep 5.29e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([3516, 2])
We keep 1.25e+05/1.94e+06 =  6% of the original kernel matrix.

torch.Size([11198, 2])
We keep 8.59e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([33200, 2])
We keep 8.64e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([32510, 2])
We keep 6.53e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([2967, 2])
We keep 1.06e+05/1.48e+06 =  7% of the original kernel matrix.

torch.Size([10610, 2])
We keep 7.88e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([10512, 2])
We keep 3.21e+06/3.65e+07 =  8% of the original kernel matrix.

torch.Size([18147, 2])
We keep 2.54e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([154264, 2])
We keep 2.00e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([70824, 2])
We keep 2.51e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([102727, 2])
We keep 5.56e+07/3.97e+09 =  1% of the original kernel matrix.

torch.Size([56203, 2])
We keep 1.65e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([47511, 2])
We keep 1.44e+07/8.45e+08 =  1% of the original kernel matrix.

torch.Size([38380, 2])
We keep 8.57e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([51051, 2])
We keep 1.50e+07/9.01e+08 =  1% of the original kernel matrix.

torch.Size([39771, 2])
We keep 8.74e+06/5.50e+08 =  1% of the original kernel matrix.

torch.Size([17304, 2])
We keep 3.30e+06/8.82e+07 =  3% of the original kernel matrix.

torch.Size([23332, 2])
We keep 3.50e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([33636, 2])
We keep 2.16e+07/5.31e+08 =  4% of the original kernel matrix.

torch.Size([32282, 2])
We keep 7.09e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([12784, 2])
We keep 1.46e+06/4.24e+07 =  3% of the original kernel matrix.

torch.Size([19731, 2])
We keep 2.62e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([68855, 2])
We keep 4.25e+07/2.00e+09 =  2% of the original kernel matrix.

torch.Size([44205, 2])
We keep 1.22e+07/8.20e+08 =  1% of the original kernel matrix.

torch.Size([12108, 2])
We keep 1.22e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([19169, 2])
We keep 2.45e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([403797, 2])
We keep 6.42e+08/6.16e+10 =  1% of the original kernel matrix.

torch.Size([114728, 2])
We keep 5.52e+07/4.54e+09 =  1% of the original kernel matrix.

torch.Size([29032, 2])
We keep 5.94e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([30244, 2])
We keep 5.69e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([106053, 2])
We keep 6.84e+07/4.87e+09 =  1% of the original kernel matrix.

torch.Size([57634, 2])
We keep 1.83e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([65013, 2])
We keep 2.53e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([44554, 2])
We keep 1.10e+07/7.13e+08 =  1% of the original kernel matrix.

torch.Size([53448, 2])
We keep 2.69e+07/1.10e+09 =  2% of the original kernel matrix.

torch.Size([40257, 2])
We keep 9.54e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([316266, 2])
We keep 5.50e+08/4.34e+10 =  1% of the original kernel matrix.

torch.Size([101567, 2])
We keep 4.72e+07/3.81e+09 =  1% of the original kernel matrix.

torch.Size([105473, 2])
We keep 6.80e+07/4.21e+09 =  1% of the original kernel matrix.

torch.Size([57297, 2])
We keep 1.68e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([97578, 2])
We keep 1.05e+08/4.09e+09 =  2% of the original kernel matrix.

torch.Size([53949, 2])
We keep 1.68e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([553717, 2])
We keep 1.19e+09/1.21e+11 =  0% of the original kernel matrix.

torch.Size([137375, 2])
We keep 7.44e+07/6.38e+09 =  1% of the original kernel matrix.

torch.Size([22157, 2])
We keep 4.09e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([26910, 2])
We keep 4.54e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([24539, 2])
We keep 1.04e+07/2.26e+08 =  4% of the original kernel matrix.

torch.Size([28369, 2])
We keep 5.10e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([23700, 2])
We keep 3.04e+07/4.30e+08 =  7% of the original kernel matrix.

torch.Size([26241, 2])
We keep 6.21e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([79747, 2])
We keep 3.06e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([49221, 2])
We keep 1.26e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([25345, 2])
We keep 2.47e+07/6.56e+08 =  3% of the original kernel matrix.

torch.Size([26505, 2])
We keep 7.79e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([20332, 2])
We keep 3.52e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([25450, 2])
We keep 4.11e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([77169, 2])
We keep 1.10e+08/3.32e+09 =  3% of the original kernel matrix.

torch.Size([48074, 2])
We keep 1.52e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([3556697, 2])
We keep 2.17e+10/3.48e+12 =  0% of the original kernel matrix.

torch.Size([365888, 2])
We keep 3.53e+08/3.41e+10 =  1% of the original kernel matrix.

torch.Size([24452, 2])
We keep 9.38e+06/2.43e+08 =  3% of the original kernel matrix.

torch.Size([26441, 2])
We keep 4.87e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([480621, 2])
We keep 1.31e+09/9.58e+10 =  1% of the original kernel matrix.

torch.Size([127664, 2])
We keep 6.82e+07/5.67e+09 =  1% of the original kernel matrix.

torch.Size([18105, 2])
We keep 4.75e+06/1.22e+08 =  3% of the original kernel matrix.

torch.Size([23946, 2])
We keep 3.99e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([18737, 2])
We keep 3.14e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([24437, 2])
We keep 3.63e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([85468, 2])
We keep 7.33e+07/3.30e+09 =  2% of the original kernel matrix.

torch.Size([50849, 2])
We keep 1.54e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([137981, 2])
We keep 2.50e+08/9.01e+09 =  2% of the original kernel matrix.

torch.Size([66202, 2])
We keep 2.37e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([95644, 2])
We keep 5.62e+07/3.53e+09 =  1% of the original kernel matrix.

torch.Size([54347, 2])
We keep 1.57e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([3559, 2])
We keep 1.29e+05/1.94e+06 =  6% of the original kernel matrix.

torch.Size([11253, 2])
We keep 8.67e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([6749, 2])
We keep 5.05e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([14534, 2])
We keep 1.63e+06/6.11e+07 =  2% of the original kernel matrix.

torch.Size([4189, 2])
We keep 2.13e+05/2.87e+06 =  7% of the original kernel matrix.

torch.Size([11938, 2])
We keep 9.92e+05/3.10e+07 =  3% of the original kernel matrix.

torch.Size([5514, 2])
We keep 2.59e+05/5.08e+06 =  5% of the original kernel matrix.

torch.Size([13500, 2])
We keep 1.19e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([1550, 2])
We keep 7.67e+04/6.35e+05 = 12% of the original kernel matrix.

torch.Size([7824, 2])
We keep 5.97e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([2897, 2])
We keep 9.37e+04/1.35e+06 =  6% of the original kernel matrix.

torch.Size([10263, 2])
We keep 7.56e+05/2.13e+07 =  3% of the original kernel matrix.

torch.Size([3739, 2])
We keep 1.48e+05/2.26e+06 =  6% of the original kernel matrix.

torch.Size([11357, 2])
We keep 9.03e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([1995, 2])
We keep 4.24e+04/4.87e+05 =  8% of the original kernel matrix.

torch.Size([9213, 2])
We keep 5.48e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([1632, 2])
We keep 2.95e+04/3.09e+05 =  9% of the original kernel matrix.

torch.Size([8475, 2])
We keep 4.66e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([3233, 2])
We keep 1.49e+05/1.80e+06 =  8% of the original kernel matrix.

torch.Size([10770, 2])
We keep 8.33e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([6948, 2])
We keep 4.52e+05/9.78e+06 =  4% of the original kernel matrix.

torch.Size([14716, 2])
We keep 1.52e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([10477, 2])
We keep 9.53e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([17857, 2])
We keep 2.13e+06/9.05e+07 =  2% of the original kernel matrix.

torch.Size([10241, 2])
We keep 1.80e+06/3.19e+07 =  5% of the original kernel matrix.

torch.Size([17572, 2])
We keep 2.38e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([15961, 2])
We keep 2.60e+06/9.25e+07 =  2% of the original kernel matrix.

torch.Size([22447, 2])
We keep 3.59e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([5672, 2])
We keep 3.32e+05/6.03e+06 =  5% of the original kernel matrix.

torch.Size([13449, 2])
We keep 1.28e+06/4.50e+07 =  2% of the original kernel matrix.

torch.Size([4874, 2])
We keep 2.15e+05/3.92e+06 =  5% of the original kernel matrix.

torch.Size([12698, 2])
We keep 1.09e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([3169, 2])
We keep 1.51e+05/1.90e+06 =  7% of the original kernel matrix.

torch.Size([10567, 2])
We keep 8.48e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([7554, 2])
We keep 5.35e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([15389, 2])
We keep 1.65e+06/6.30e+07 =  2% of the original kernel matrix.

torch.Size([17124, 2])
We keep 3.19e+06/9.82e+07 =  3% of the original kernel matrix.

torch.Size([23147, 2])
We keep 3.62e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([2376, 2])
We keep 6.97e+04/9.06e+05 =  7% of the original kernel matrix.

torch.Size([9685, 2])
We keep 6.61e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([7894, 2])
We keep 5.58e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([15656, 2])
We keep 1.65e+06/6.42e+07 =  2% of the original kernel matrix.

torch.Size([11028, 2])
We keep 9.37e+05/2.68e+07 =  3% of the original kernel matrix.

torch.Size([18430, 2])
We keep 2.20e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([3049, 2])
We keep 1.20e+05/1.50e+06 =  7% of the original kernel matrix.

torch.Size([10456, 2])
We keep 7.91e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([14689, 2])
We keep 1.83e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([21571, 2])
We keep 3.07e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([3204, 2])
We keep 9.95e+04/1.51e+06 =  6% of the original kernel matrix.

torch.Size([10778, 2])
We keep 7.82e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([10086, 2])
We keep 1.14e+06/2.70e+07 =  4% of the original kernel matrix.

torch.Size([17695, 2])
We keep 2.24e+06/9.51e+07 =  2% of the original kernel matrix.

torch.Size([18659, 2])
We keep 2.61e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([24312, 2])
We keep 3.74e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([6465, 2])
We keep 3.70e+05/7.53e+06 =  4% of the original kernel matrix.

torch.Size([14384, 2])
We keep 1.38e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([4141, 2])
We keep 1.53e+05/2.57e+06 =  5% of the original kernel matrix.

torch.Size([11983, 2])
We keep 9.46e+05/2.93e+07 =  3% of the original kernel matrix.

torch.Size([5683, 2])
We keep 3.10e+05/5.73e+06 =  5% of the original kernel matrix.

torch.Size([13527, 2])
We keep 1.25e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([7870, 2])
We keep 5.68e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([15494, 2])
We keep 1.67e+06/6.52e+07 =  2% of the original kernel matrix.

torch.Size([9673, 2])
We keep 8.10e+05/2.05e+07 =  3% of the original kernel matrix.

torch.Size([17255, 2])
We keep 2.01e+06/8.30e+07 =  2% of the original kernel matrix.

torch.Size([7555, 2])
We keep 5.32e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([15311, 2])
We keep 1.61e+06/6.22e+07 =  2% of the original kernel matrix.

torch.Size([15660, 2])
We keep 3.24e+06/8.37e+07 =  3% of the original kernel matrix.

torch.Size([21572, 2])
We keep 3.35e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([6516, 2])
We keep 4.31e+05/8.92e+06 =  4% of the original kernel matrix.

torch.Size([14321, 2])
We keep 1.49e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([13137, 2])
We keep 1.33e+06/4.37e+07 =  3% of the original kernel matrix.

torch.Size([20068, 2])
We keep 2.63e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([3505, 2])
We keep 1.57e+05/2.25e+06 =  6% of the original kernel matrix.

torch.Size([11010, 2])
We keep 9.02e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([13000, 2])
We keep 1.46e+06/4.48e+07 =  3% of the original kernel matrix.

torch.Size([19983, 2])
We keep 2.67e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([6435, 2])
We keep 5.56e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([14314, 2])
We keep 1.57e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([1993, 2])
We keep 4.60e+04/5.62e+05 =  8% of the original kernel matrix.

torch.Size([9039, 2])
We keep 5.67e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([1929, 2])
We keep 4.33e+04/5.23e+05 =  8% of the original kernel matrix.

torch.Size([8969, 2])
We keep 5.58e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([9463, 2])
We keep 7.48e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([17073, 2])
We keep 1.94e+06/8.04e+07 =  2% of the original kernel matrix.

torch.Size([13870, 2])
We keep 1.71e+06/5.42e+07 =  3% of the original kernel matrix.

torch.Size([20505, 2])
We keep 2.85e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([3814, 2])
We keep 1.54e+05/2.42e+06 =  6% of the original kernel matrix.

torch.Size([11456, 2])
We keep 9.25e+05/2.85e+07 =  3% of the original kernel matrix.

torch.Size([4164, 2])
We keep 2.18e+05/3.38e+06 =  6% of the original kernel matrix.

torch.Size([11695, 2])
We keep 1.05e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([6607, 2])
We keep 3.93e+05/7.92e+06 =  4% of the original kernel matrix.

torch.Size([14473, 2])
We keep 1.40e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([3421, 2])
We keep 1.15e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([11023, 2])
We keep 8.22e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([2746, 2])
We keep 7.74e+04/1.12e+06 =  6% of the original kernel matrix.

torch.Size([10232, 2])
We keep 7.07e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([2346, 2])
We keep 6.05e+04/7.83e+05 =  7% of the original kernel matrix.

torch.Size([9665, 2])
We keep 6.19e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([3547, 2])
We keep 2.12e+05/2.33e+06 =  9% of the original kernel matrix.

torch.Size([11131, 2])
We keep 9.23e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([2293, 2])
We keep 6.30e+04/8.80e+05 =  7% of the original kernel matrix.

torch.Size([9474, 2])
We keep 6.50e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([6822, 2])
We keep 5.00e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([14596, 2])
We keep 1.60e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([5104, 2])
We keep 2.48e+05/4.73e+06 =  5% of the original kernel matrix.

torch.Size([12930, 2])
We keep 1.17e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([9359, 2])
We keep 7.00e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([16977, 2])
We keep 1.90e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([5029, 2])
We keep 2.57e+05/4.88e+06 =  5% of the original kernel matrix.

torch.Size([12783, 2])
We keep 1.18e+06/4.05e+07 =  2% of the original kernel matrix.

torch.Size([2741, 2])
We keep 8.69e+04/1.14e+06 =  7% of the original kernel matrix.

torch.Size([10279, 2])
We keep 7.21e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([8430, 2])
We keep 7.17e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([16180, 2])
We keep 1.86e+06/7.45e+07 =  2% of the original kernel matrix.

torch.Size([9345, 2])
We keep 7.58e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([16932, 2])
We keep 1.93e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([7442, 2])
We keep 4.61e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([15289, 2])
We keep 1.54e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([5604, 2])
We keep 3.48e+05/6.24e+06 =  5% of the original kernel matrix.

torch.Size([13394, 2])
We keep 1.31e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([8891, 2])
We keep 8.49e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([16322, 2])
We keep 1.91e+06/7.89e+07 =  2% of the original kernel matrix.

torch.Size([13154, 2])
We keep 1.67e+06/5.15e+07 =  3% of the original kernel matrix.

torch.Size([20147, 2])
We keep 2.86e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([11005, 2])
We keep 9.65e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([18322, 2])
We keep 2.20e+06/9.55e+07 =  2% of the original kernel matrix.

torch.Size([14452, 2])
We keep 1.93e+06/6.42e+07 =  3% of the original kernel matrix.

torch.Size([21208, 2])
We keep 3.09e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([9627, 2])
We keep 8.94e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([17103, 2])
We keep 1.99e+06/8.25e+07 =  2% of the original kernel matrix.

torch.Size([4280, 2])
We keep 1.78e+05/3.06e+06 =  5% of the original kernel matrix.

torch.Size([12108, 2])
We keep 1.00e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([2706, 2])
We keep 1.09e+05/1.48e+06 =  7% of the original kernel matrix.

torch.Size([9881, 2])
We keep 7.85e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([27875, 2])
We keep 6.06e+06/2.97e+08 =  2% of the original kernel matrix.

torch.Size([29872, 2])
We keep 5.64e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([3331, 2])
We keep 1.27e+05/1.70e+06 =  7% of the original kernel matrix.

torch.Size([10841, 2])
We keep 8.08e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([7443, 2])
We keep 5.15e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([15187, 2])
We keep 1.60e+06/6.10e+07 =  2% of the original kernel matrix.

torch.Size([1183, 2])
We keep 1.69e+04/1.67e+05 = 10% of the original kernel matrix.

torch.Size([7639, 2])
We keep 3.84e+05/7.49e+06 =  5% of the original kernel matrix.

torch.Size([4195, 2])
We keep 1.86e+05/3.14e+06 =  5% of the original kernel matrix.

torch.Size([11934, 2])
We keep 1.02e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([3221, 2])
We keep 1.09e+05/1.59e+06 =  6% of the original kernel matrix.

torch.Size([10915, 2])
We keep 8.09e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([3505, 2])
We keep 1.18e+05/1.90e+06 =  6% of the original kernel matrix.

torch.Size([11291, 2])
We keep 8.44e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([3725, 2])
We keep 1.85e+05/2.64e+06 =  7% of the original kernel matrix.

torch.Size([11182, 2])
We keep 9.61e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([4982, 2])
We keep 2.74e+05/4.65e+06 =  5% of the original kernel matrix.

torch.Size([12812, 2])
We keep 1.17e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([6775, 2])
We keep 4.43e+05/9.39e+06 =  4% of the original kernel matrix.

torch.Size([14599, 2])
We keep 1.51e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([6230, 2])
We keep 3.54e+05/6.94e+06 =  5% of the original kernel matrix.

torch.Size([14180, 2])
We keep 1.34e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([4816, 2])
We keep 2.83e+05/4.93e+06 =  5% of the original kernel matrix.

torch.Size([12556, 2])
We keep 1.19e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([5162, 2])
We keep 2.80e+05/5.26e+06 =  5% of the original kernel matrix.

torch.Size([13003, 2])
We keep 1.23e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([2324, 2])
We keep 6.89e+04/9.29e+05 =  7% of the original kernel matrix.

torch.Size([9571, 2])
We keep 6.79e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([12315, 2])
We keep 1.39e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([19505, 2])
We keep 2.56e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([1899, 2])
We keep 4.21e+04/5.06e+05 =  8% of the original kernel matrix.

torch.Size([8896, 2])
We keep 5.47e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([17068, 2])
We keep 2.66e+06/9.24e+07 =  2% of the original kernel matrix.

torch.Size([23202, 2])
We keep 3.48e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([2812, 2])
We keep 8.12e+04/1.15e+06 =  7% of the original kernel matrix.

torch.Size([10280, 2])
We keep 7.25e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([4092, 2])
We keep 1.82e+05/2.93e+06 =  6% of the original kernel matrix.

torch.Size([11909, 2])
We keep 9.93e+05/3.13e+07 =  3% of the original kernel matrix.

torch.Size([4822, 2])
We keep 2.38e+05/4.26e+06 =  5% of the original kernel matrix.

torch.Size([12662, 2])
We keep 1.12e+06/3.78e+07 =  2% of the original kernel matrix.

torch.Size([4789, 2])
We keep 2.15e+05/3.85e+06 =  5% of the original kernel matrix.

torch.Size([12637, 2])
We keep 1.08e+06/3.59e+07 =  3% of the original kernel matrix.

torch.Size([7869, 2])
We keep 5.41e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([15513, 2])
We keep 1.65e+06/6.50e+07 =  2% of the original kernel matrix.

torch.Size([2421, 2])
We keep 6.43e+04/8.37e+05 =  7% of the original kernel matrix.

torch.Size([9690, 2])
We keep 6.47e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([5061, 2])
We keep 2.45e+05/4.46e+06 =  5% of the original kernel matrix.

torch.Size([12972, 2])
We keep 1.15e+06/3.87e+07 =  2% of the original kernel matrix.

torch.Size([12036, 2])
We keep 1.31e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([19197, 2])
We keep 2.47e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([5795, 2])
We keep 2.77e+05/5.53e+06 =  5% of the original kernel matrix.

torch.Size([13787, 2])
We keep 1.22e+06/4.31e+07 =  2% of the original kernel matrix.

torch.Size([30390, 2])
We keep 6.54e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([31019, 2])
We keep 5.81e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([3553, 2])
We keep 1.28e+05/1.83e+06 =  6% of the original kernel matrix.

torch.Size([11430, 2])
We keep 8.42e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([7957, 2])
We keep 6.10e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([15747, 2])
We keep 1.77e+06/6.96e+07 =  2% of the original kernel matrix.

torch.Size([2689, 2])
We keep 1.13e+05/1.41e+06 =  8% of the original kernel matrix.

torch.Size([9923, 2])
We keep 7.68e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([5254, 2])
We keep 2.60e+05/4.84e+06 =  5% of the original kernel matrix.

torch.Size([13014, 2])
We keep 1.17e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([3742, 2])
We keep 1.47e+05/2.24e+06 =  6% of the original kernel matrix.

torch.Size([11476, 2])
We keep 9.00e+05/2.74e+07 =  3% of the original kernel matrix.

torch.Size([4865, 2])
We keep 2.11e+05/3.78e+06 =  5% of the original kernel matrix.

torch.Size([12725, 2])
We keep 1.08e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([4770, 2])
We keep 2.46e+05/4.04e+06 =  6% of the original kernel matrix.

torch.Size([12509, 2])
We keep 1.10e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([4296, 2])
We keep 1.62e+05/2.78e+06 =  5% of the original kernel matrix.

torch.Size([12162, 2])
We keep 9.72e+05/3.05e+07 =  3% of the original kernel matrix.

torch.Size([7224, 2])
We keep 4.59e+05/9.96e+06 =  4% of the original kernel matrix.

torch.Size([15045, 2])
We keep 1.53e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([4671, 2])
We keep 2.18e+05/4.09e+06 =  5% of the original kernel matrix.

torch.Size([12424, 2])
We keep 1.11e+06/3.70e+07 =  2% of the original kernel matrix.

torch.Size([1965, 2])
We keep 4.08e+04/4.61e+05 =  8% of the original kernel matrix.

torch.Size([9021, 2])
We keep 5.34e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([10731, 2])
We keep 9.57e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([18145, 2])
We keep 2.22e+06/9.67e+07 =  2% of the original kernel matrix.

torch.Size([13822, 2])
We keep 2.10e+06/6.94e+07 =  3% of the original kernel matrix.

torch.Size([20579, 2])
We keep 3.23e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([7620, 2])
We keep 6.10e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([15490, 2])
We keep 1.72e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([4203, 2])
We keep 1.63e+05/2.67e+06 =  6% of the original kernel matrix.

torch.Size([11895, 2])
We keep 9.55e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([4290, 2])
We keep 1.88e+05/3.32e+06 =  5% of the original kernel matrix.

torch.Size([12183, 2])
We keep 1.02e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([4041, 2])
We keep 1.96e+05/3.01e+06 =  6% of the original kernel matrix.

torch.Size([11711, 2])
We keep 1.01e+06/3.18e+07 =  3% of the original kernel matrix.

torch.Size([3928, 2])
We keep 1.54e+05/2.52e+06 =  6% of the original kernel matrix.

torch.Size([11665, 2])
We keep 9.43e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([5139, 2])
We keep 2.60e+05/4.96e+06 =  5% of the original kernel matrix.

torch.Size([13125, 2])
We keep 1.19e+06/4.08e+07 =  2% of the original kernel matrix.

torch.Size([4920, 2])
We keep 2.25e+05/4.18e+06 =  5% of the original kernel matrix.

torch.Size([12772, 2])
We keep 1.11e+06/3.75e+07 =  2% of the original kernel matrix.

torch.Size([6680, 2])
We keep 4.78e+05/9.48e+06 =  5% of the original kernel matrix.

torch.Size([14571, 2])
We keep 1.51e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([4202, 2])
We keep 2.26e+05/3.33e+06 =  6% of the original kernel matrix.

torch.Size([11972, 2])
We keep 1.04e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([11372, 2])
We keep 2.16e+06/3.99e+07 =  5% of the original kernel matrix.

torch.Size([18418, 2])
We keep 2.55e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([9687, 2])
We keep 7.93e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([17253, 2])
We keep 2.01e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([15069, 2])
We keep 2.45e+06/6.42e+07 =  3% of the original kernel matrix.

torch.Size([21419, 2])
We keep 3.03e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([9642, 2])
We keep 7.44e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([17157, 2])
We keep 1.96e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([4230, 2])
We keep 2.25e+05/3.42e+06 =  6% of the original kernel matrix.

torch.Size([12057, 2])
We keep 1.05e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([4793, 2])
We keep 2.05e+05/3.69e+06 =  5% of the original kernel matrix.

torch.Size([12709, 2])
We keep 1.08e+06/3.52e+07 =  3% of the original kernel matrix.

torch.Size([2647, 2])
We keep 7.31e+04/1.01e+06 =  7% of the original kernel matrix.

torch.Size([10108, 2])
We keep 6.99e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([13077, 2])
We keep 1.74e+06/4.71e+07 =  3% of the original kernel matrix.

torch.Size([20203, 2])
We keep 2.74e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([2295, 2])
We keep 6.69e+04/9.29e+05 =  7% of the original kernel matrix.

torch.Size([9414, 2])
We keep 6.65e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([6617, 2])
We keep 3.87e+05/8.19e+06 =  4% of the original kernel matrix.

torch.Size([14486, 2])
We keep 1.42e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([1260, 2])
We keep 2.67e+04/2.31e+05 = 11% of the original kernel matrix.

torch.Size([7657, 2])
We keep 4.34e+05/8.81e+06 =  4% of the original kernel matrix.

torch.Size([9257, 2])
We keep 7.90e+05/1.88e+07 =  4% of the original kernel matrix.

torch.Size([16863, 2])
We keep 1.93e+06/7.95e+07 =  2% of the original kernel matrix.

torch.Size([5976, 2])
We keep 2.98e+05/5.97e+06 =  4% of the original kernel matrix.

torch.Size([13924, 2])
We keep 1.27e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([4814, 2])
We keep 2.08e+05/3.72e+06 =  5% of the original kernel matrix.

torch.Size([12775, 2])
We keep 1.08e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([8801, 2])
We keep 6.63e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([16450, 2])
We keep 1.79e+06/7.19e+07 =  2% of the original kernel matrix.

torch.Size([5094, 2])
We keep 2.73e+05/5.01e+06 =  5% of the original kernel matrix.

torch.Size([12928, 2])
We keep 1.21e+06/4.10e+07 =  2% of the original kernel matrix.

torch.Size([1518, 2])
We keep 3.04e+04/3.31e+05 =  9% of the original kernel matrix.

torch.Size([8327, 2])
We keep 4.75e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([1550, 2])
We keep 3.34e+04/3.46e+05 =  9% of the original kernel matrix.

torch.Size([8103, 2])
We keep 4.87e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([3453, 2])
We keep 1.38e+05/2.21e+06 =  6% of the original kernel matrix.

torch.Size([10891, 2])
We keep 8.96e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([12758, 2])
We keep 1.34e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([19751, 2])
We keep 2.55e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([6819, 2])
We keep 4.18e+05/9.11e+06 =  4% of the original kernel matrix.

torch.Size([14630, 2])
We keep 1.48e+06/5.53e+07 =  2% of the original kernel matrix.

torch.Size([4178, 2])
We keep 2.55e+05/3.35e+06 =  7% of the original kernel matrix.

torch.Size([12055, 2])
We keep 1.05e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([3713, 2])
We keep 1.60e+05/2.34e+06 =  6% of the original kernel matrix.

torch.Size([11458, 2])
We keep 9.11e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([8296, 2])
We keep 6.08e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([15866, 2])
We keep 1.74e+06/6.94e+07 =  2% of the original kernel matrix.

torch.Size([6333, 2])
We keep 3.59e+05/7.39e+06 =  4% of the original kernel matrix.

torch.Size([14113, 2])
We keep 1.37e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([7359, 2])
We keep 5.43e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([15061, 2])
We keep 1.59e+06/6.15e+07 =  2% of the original kernel matrix.

torch.Size([4880, 2])
We keep 2.34e+05/4.19e+06 =  5% of the original kernel matrix.

torch.Size([12630, 2])
We keep 1.12e+06/3.75e+07 =  2% of the original kernel matrix.

torch.Size([10255, 2])
We keep 9.69e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([17825, 2])
We keep 2.20e+06/9.30e+07 =  2% of the original kernel matrix.

torch.Size([4139, 2])
We keep 1.66e+05/2.86e+06 =  5% of the original kernel matrix.

torch.Size([11840, 2])
We keep 9.78e+05/3.10e+07 =  3% of the original kernel matrix.

torch.Size([10282, 2])
We keep 9.45e+05/2.32e+07 =  4% of the original kernel matrix.

torch.Size([17711, 2])
We keep 2.08e+06/8.82e+07 =  2% of the original kernel matrix.

torch.Size([3072, 2])
We keep 9.13e+04/1.41e+06 =  6% of the original kernel matrix.

torch.Size([10651, 2])
We keep 7.67e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([8947, 2])
We keep 7.18e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([16687, 2])
We keep 1.91e+06/7.57e+07 =  2% of the original kernel matrix.

torch.Size([6588, 2])
We keep 3.99e+05/8.25e+06 =  4% of the original kernel matrix.

torch.Size([14445, 2])
We keep 1.43e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([5633, 2])
We keep 2.96e+05/5.91e+06 =  4% of the original kernel matrix.

torch.Size([13378, 2])
We keep 1.26e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([5998, 2])
We keep 4.47e+05/7.83e+06 =  5% of the original kernel matrix.

torch.Size([13755, 2])
We keep 1.43e+06/5.12e+07 =  2% of the original kernel matrix.

torch.Size([6895, 2])
We keep 4.86e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([14927, 2])
We keep 1.58e+06/5.90e+07 =  2% of the original kernel matrix.

torch.Size([9293, 2])
We keep 9.55e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([16960, 2])
We keep 1.99e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([5787, 2])
We keep 2.97e+05/5.83e+06 =  5% of the original kernel matrix.

torch.Size([13726, 2])
We keep 1.26e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([2447, 2])
We keep 7.05e+04/9.37e+05 =  7% of the original kernel matrix.

torch.Size([9732, 2])
We keep 6.75e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([5010, 2])
We keep 2.28e+05/4.10e+06 =  5% of the original kernel matrix.

torch.Size([12884, 2])
We keep 1.13e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([3949, 2])
We keep 1.37e+05/2.37e+06 =  5% of the original kernel matrix.

torch.Size([11702, 2])
We keep 9.02e+05/2.82e+07 =  3% of the original kernel matrix.

torch.Size([12019, 2])
We keep 1.44e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([19221, 2])
We keep 2.50e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([2657, 2])
We keep 6.96e+04/9.70e+05 =  7% of the original kernel matrix.

torch.Size([10058, 2])
We keep 6.73e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([2437, 2])
We keep 6.73e+04/9.01e+05 =  7% of the original kernel matrix.

torch.Size([9724, 2])
We keep 6.61e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([3280, 2])
We keep 1.20e+05/1.76e+06 =  6% of the original kernel matrix.

torch.Size([10994, 2])
We keep 8.28e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([9812, 2])
We keep 8.28e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([17240, 2])
We keep 2.00e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([2770, 2])
We keep 7.10e+04/1.01e+06 =  7% of the original kernel matrix.

torch.Size([10258, 2])
We keep 6.82e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([16399, 2])
We keep 2.35e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([22890, 2])
We keep 3.47e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([5717, 2])
We keep 2.87e+05/5.55e+06 =  5% of the original kernel matrix.

torch.Size([13564, 2])
We keep 1.23e+06/4.31e+07 =  2% of the original kernel matrix.

torch.Size([30886, 2])
We keep 6.88e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([31601, 2])
We keep 6.07e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([4073, 2])
We keep 1.52e+05/2.46e+06 =  6% of the original kernel matrix.

torch.Size([11917, 2])
We keep 9.28e+05/2.87e+07 =  3% of the original kernel matrix.

torch.Size([8106, 2])
We keep 5.64e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([15945, 2])
We keep 1.71e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([2485, 2])
We keep 6.75e+04/9.45e+05 =  7% of the original kernel matrix.

torch.Size([9819, 2])
We keep 6.69e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([4355, 2])
We keep 3.13e+05/4.04e+06 =  7% of the original kernel matrix.

torch.Size([12191, 2])
We keep 1.13e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([4053, 2])
We keep 1.68e+05/2.62e+06 =  6% of the original kernel matrix.

torch.Size([11817, 2])
We keep 9.38e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([2907, 2])
We keep 1.10e+05/1.40e+06 =  7% of the original kernel matrix.

torch.Size([10387, 2])
We keep 7.86e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([10032, 2])
We keep 9.41e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([17491, 2])
We keep 2.18e+06/9.21e+07 =  2% of the original kernel matrix.

torch.Size([18311, 2])
We keep 2.70e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([23992, 2])
We keep 3.68e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([3872, 2])
We keep 1.42e+05/2.28e+06 =  6% of the original kernel matrix.

torch.Size([11719, 2])
We keep 9.08e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([4266, 2])
We keep 1.90e+05/3.26e+06 =  5% of the original kernel matrix.

torch.Size([12010, 2])
We keep 1.02e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([17689, 2])
We keep 2.52e+06/9.78e+07 =  2% of the original kernel matrix.

torch.Size([23662, 2])
We keep 3.59e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([2307, 2])
We keep 6.34e+04/7.26e+05 =  8% of the original kernel matrix.

torch.Size([9563, 2])
We keep 6.22e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([2199, 2])
We keep 6.81e+04/7.41e+05 =  9% of the original kernel matrix.

torch.Size([9226, 2])
We keep 6.29e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([1736, 2])
We keep 3.54e+04/4.10e+05 =  8% of the original kernel matrix.

torch.Size([8695, 2])
We keep 5.13e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([3656, 2])
We keep 1.37e+05/2.17e+06 =  6% of the original kernel matrix.

torch.Size([11160, 2])
We keep 8.90e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([5436, 2])
We keep 3.51e+05/6.18e+06 =  5% of the original kernel matrix.

torch.Size([13132, 2])
We keep 1.29e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([6818, 2])
We keep 3.95e+05/8.38e+06 =  4% of the original kernel matrix.

torch.Size([14667, 2])
We keep 1.44e+06/5.30e+07 =  2% of the original kernel matrix.

torch.Size([8264, 2])
We keep 6.48e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([15977, 2])
We keep 1.73e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([3913, 2])
We keep 1.35e+05/2.27e+06 =  5% of the original kernel matrix.

torch.Size([11734, 2])
We keep 9.06e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([8495, 2])
We keep 6.50e+05/1.52e+07 =  4% of the original kernel matrix.

torch.Size([16100, 2])
We keep 1.78e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([7983, 2])
We keep 6.08e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([15711, 2])
We keep 1.72e+06/6.75e+07 =  2% of the original kernel matrix.

torch.Size([4322, 2])
We keep 2.31e+05/3.70e+06 =  6% of the original kernel matrix.

torch.Size([11967, 2])
We keep 1.09e+06/3.52e+07 =  3% of the original kernel matrix.

torch.Size([5160, 2])
We keep 3.27e+05/5.49e+06 =  5% of the original kernel matrix.

torch.Size([13113, 2])
We keep 1.26e+06/4.29e+07 =  2% of the original kernel matrix.

torch.Size([4218, 2])
We keep 1.67e+05/2.76e+06 =  6% of the original kernel matrix.

torch.Size([12044, 2])
We keep 9.61e+05/3.04e+07 =  3% of the original kernel matrix.

torch.Size([3568, 2])
We keep 1.37e+05/2.06e+06 =  6% of the original kernel matrix.

torch.Size([11246, 2])
We keep 8.76e+05/2.63e+07 =  3% of the original kernel matrix.

torch.Size([7613, 2])
We keep 4.98e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([15496, 2])
We keep 1.60e+06/6.15e+07 =  2% of the original kernel matrix.

torch.Size([3244, 2])
We keep 1.07e+05/1.59e+06 =  6% of the original kernel matrix.

torch.Size([10799, 2])
We keep 7.97e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([4317, 2])
We keep 1.74e+05/2.97e+06 =  5% of the original kernel matrix.

torch.Size([12210, 2])
We keep 1.00e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([5074, 2])
We keep 3.18e+05/5.66e+06 =  5% of the original kernel matrix.

torch.Size([12965, 2])
We keep 1.29e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([2332, 2])
We keep 8.64e+04/9.76e+05 =  8% of the original kernel matrix.

torch.Size([9548, 2])
We keep 6.97e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([6784, 2])
We keep 4.17e+05/8.71e+06 =  4% of the original kernel matrix.

torch.Size([14871, 2])
We keep 1.48e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([3882, 2])
We keep 1.41e+05/2.27e+06 =  6% of the original kernel matrix.

torch.Size([11637, 2])
We keep 8.99e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([4513, 2])
We keep 2.27e+05/3.45e+06 =  6% of the original kernel matrix.

torch.Size([12301, 2])
We keep 1.05e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([11240, 2])
We keep 1.05e+06/2.96e+07 =  3% of the original kernel matrix.

torch.Size([18498, 2])
We keep 2.27e+06/9.96e+07 =  2% of the original kernel matrix.

torch.Size([6906, 2])
We keep 4.75e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([14921, 2])
We keep 1.57e+06/5.87e+07 =  2% of the original kernel matrix.

torch.Size([7933, 2])
We keep 5.52e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([15549, 2])
We keep 1.64e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([15233, 2])
We keep 3.40e+06/8.55e+07 =  3% of the original kernel matrix.

torch.Size([21855, 2])
We keep 3.46e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([41522, 2])
We keep 3.60e+07/9.20e+08 =  3% of the original kernel matrix.

torch.Size([35151, 2])
We keep 8.85e+06/5.55e+08 =  1% of the original kernel matrix.

torch.Size([39033, 2])
We keep 4.71e+07/1.13e+09 =  4% of the original kernel matrix.

torch.Size([34224, 2])
We keep 9.64e+06/6.16e+08 =  1% of the original kernel matrix.

torch.Size([24919, 2])
We keep 8.32e+06/2.24e+08 =  3% of the original kernel matrix.

torch.Size([28300, 2])
We keep 5.13e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([2865, 2])
We keep 9.76e+04/1.28e+06 =  7% of the original kernel matrix.

torch.Size([10248, 2])
We keep 7.38e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([13383, 2])
We keep 1.54e+06/4.91e+07 =  3% of the original kernel matrix.

torch.Size([20241, 2])
We keep 2.77e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([19929, 2])
We keep 8.77e+06/1.59e+08 =  5% of the original kernel matrix.

torch.Size([24898, 2])
We keep 4.38e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([133014, 2])
We keep 3.05e+08/1.06e+10 =  2% of the original kernel matrix.

torch.Size([64781, 2])
We keep 2.56e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([5864, 2])
We keep 3.08e+05/6.19e+06 =  4% of the original kernel matrix.

torch.Size([13881, 2])
We keep 1.28e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([184421, 2])
We keep 2.41e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([77652, 2])
We keep 2.95e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([43894, 2])
We keep 2.05e+07/7.99e+08 =  2% of the original kernel matrix.

torch.Size([36596, 2])
We keep 8.30e+06/5.18e+08 =  1% of the original kernel matrix.

torch.Size([104039, 2])
We keep 1.56e+08/5.86e+09 =  2% of the original kernel matrix.

torch.Size([57352, 2])
We keep 1.97e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([172886, 2])
We keep 3.90e+08/1.73e+10 =  2% of the original kernel matrix.

torch.Size([75562, 2])
We keep 3.16e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([4915, 2])
We keep 2.03e+05/3.95e+06 =  5% of the original kernel matrix.

torch.Size([13013, 2])
We keep 1.10e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([16951, 2])
We keep 5.61e+06/8.84e+07 =  6% of the original kernel matrix.

torch.Size([22943, 2])
We keep 3.33e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([321519, 2])
We keep 5.21e+08/4.15e+10 =  1% of the original kernel matrix.

torch.Size([102030, 2])
We keep 4.61e+07/3.73e+09 =  1% of the original kernel matrix.

torch.Size([576822, 2])
We keep 1.34e+09/1.35e+11 =  0% of the original kernel matrix.

torch.Size([141156, 2])
We keep 7.88e+07/6.73e+09 =  1% of the original kernel matrix.

torch.Size([9392, 2])
We keep 9.20e+05/2.18e+07 =  4% of the original kernel matrix.

torch.Size([17078, 2])
We keep 2.08e+06/8.56e+07 =  2% of the original kernel matrix.

torch.Size([56245, 2])
We keep 2.10e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([41726, 2])
We keep 9.56e+06/6.12e+08 =  1% of the original kernel matrix.

torch.Size([29849, 2])
We keep 1.27e+07/3.50e+08 =  3% of the original kernel matrix.

torch.Size([30678, 2])
We keep 5.96e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([79248, 2])
We keep 2.39e+08/2.75e+09 =  8% of the original kernel matrix.

torch.Size([48921, 2])
We keep 1.33e+07/9.61e+08 =  1% of the original kernel matrix.

torch.Size([132065, 2])
We keep 2.18e+08/8.70e+09 =  2% of the original kernel matrix.

torch.Size([63981, 2])
We keep 2.31e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([35415, 2])
We keep 1.92e+07/6.69e+08 =  2% of the original kernel matrix.

torch.Size([32366, 2])
We keep 7.79e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([29477, 2])
We keep 1.41e+07/3.95e+08 =  3% of the original kernel matrix.

torch.Size([30578, 2])
We keep 6.32e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([18279, 2])
We keep 3.84e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([24000, 2])
We keep 3.73e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([5792, 2])
We keep 3.94e+05/7.26e+06 =  5% of the original kernel matrix.

torch.Size([13566, 2])
We keep 1.36e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([151900, 2])
We keep 6.38e+08/2.49e+10 =  2% of the original kernel matrix.

torch.Size([66841, 2])
We keep 3.72e+07/2.89e+09 =  1% of the original kernel matrix.

torch.Size([35571, 2])
We keep 1.72e+07/5.03e+08 =  3% of the original kernel matrix.

torch.Size([33342, 2])
We keep 6.68e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([8477, 2])
We keep 1.20e+06/1.69e+07 =  7% of the original kernel matrix.

torch.Size([16395, 2])
We keep 1.80e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([94755, 2])
We keep 9.11e+07/3.96e+09 =  2% of the original kernel matrix.

torch.Size([53525, 2])
We keep 1.64e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([117850, 2])
We keep 1.59e+08/6.70e+09 =  2% of the original kernel matrix.

torch.Size([59698, 2])
We keep 2.07e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([290832, 2])
We keep 4.79e+08/3.96e+10 =  1% of the original kernel matrix.

torch.Size([97802, 2])
We keep 4.55e+07/3.64e+09 =  1% of the original kernel matrix.

torch.Size([148983, 2])
We keep 1.07e+08/8.53e+09 =  1% of the original kernel matrix.

torch.Size([68743, 2])
We keep 2.29e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([29203, 2])
We keep 7.91e+06/3.48e+08 =  2% of the original kernel matrix.

torch.Size([30289, 2])
We keep 6.03e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([48796, 2])
We keep 2.84e+07/9.61e+08 =  2% of the original kernel matrix.

torch.Size([38433, 2])
We keep 9.10e+06/5.68e+08 =  1% of the original kernel matrix.

torch.Size([5212, 2])
We keep 6.55e+05/7.40e+06 =  8% of the original kernel matrix.

torch.Size([13000, 2])
We keep 1.40e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([31178, 2])
We keep 1.40e+07/3.81e+08 =  3% of the original kernel matrix.

torch.Size([31199, 2])
We keep 6.17e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([80805, 2])
We keep 4.46e+07/2.69e+09 =  1% of the original kernel matrix.

torch.Size([49785, 2])
We keep 1.39e+07/9.50e+08 =  1% of the original kernel matrix.

torch.Size([18227, 2])
We keep 2.99e+07/1.39e+08 = 21% of the original kernel matrix.

torch.Size([23524, 2])
We keep 3.48e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([108655, 2])
We keep 6.46e+07/4.44e+09 =  1% of the original kernel matrix.

torch.Size([58059, 2])
We keep 1.72e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([4743, 2])
We keep 5.98e+05/4.93e+06 = 12% of the original kernel matrix.

torch.Size([12279, 2])
We keep 1.07e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([17029, 2])
We keep 3.73e+06/9.85e+07 =  3% of the original kernel matrix.

torch.Size([23215, 2])
We keep 3.63e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([65599, 2])
We keep 7.57e+07/2.63e+09 =  2% of the original kernel matrix.

torch.Size([43619, 2])
We keep 1.39e+07/9.39e+08 =  1% of the original kernel matrix.

torch.Size([125387, 2])
We keep 1.22e+08/7.15e+09 =  1% of the original kernel matrix.

torch.Size([61726, 2])
We keep 2.11e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([118121, 2])
We keep 9.42e+07/6.11e+09 =  1% of the original kernel matrix.

torch.Size([59955, 2])
We keep 1.98e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([10827, 2])
We keep 5.05e+06/5.23e+07 =  9% of the original kernel matrix.

torch.Size([18026, 2])
We keep 2.74e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([222781, 2])
We keep 2.23e+08/2.24e+10 =  0% of the original kernel matrix.

torch.Size([84715, 2])
We keep 3.48e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([19249, 2])
We keep 2.72e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([24890, 2])
We keep 3.87e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([28536, 2])
We keep 5.82e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([30337, 2])
We keep 5.71e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([92575, 2])
We keep 4.65e+07/3.11e+09 =  1% of the original kernel matrix.

torch.Size([53175, 2])
We keep 1.47e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([7391, 2])
We keep 5.86e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([15266, 2])
We keep 1.57e+06/6.05e+07 =  2% of the original kernel matrix.

torch.Size([223228, 2])
We keep 3.48e+08/2.23e+10 =  1% of the original kernel matrix.

torch.Size([85759, 2])
We keep 3.51e+07/2.73e+09 =  1% of the original kernel matrix.

torch.Size([136193, 2])
We keep 2.61e+08/1.02e+10 =  2% of the original kernel matrix.

torch.Size([65477, 2])
We keep 2.52e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([361124, 2])
We keep 7.16e+08/6.14e+10 =  1% of the original kernel matrix.

torch.Size([107965, 2])
We keep 5.56e+07/4.54e+09 =  1% of the original kernel matrix.

torch.Size([139098, 2])
We keep 1.43e+08/9.18e+09 =  1% of the original kernel matrix.

torch.Size([65871, 2])
We keep 2.40e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([84030, 2])
We keep 4.99e+07/2.81e+09 =  1% of the original kernel matrix.

torch.Size([50111, 2])
We keep 1.42e+07/9.71e+08 =  1% of the original kernel matrix.

torch.Size([84425, 2])
We keep 6.59e+07/2.97e+09 =  2% of the original kernel matrix.

torch.Size([49477, 2])
We keep 1.45e+07/9.98e+08 =  1% of the original kernel matrix.

torch.Size([13141, 2])
We keep 1.67e+06/4.18e+07 =  3% of the original kernel matrix.

torch.Size([20013, 2])
We keep 2.58e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([74236, 2])
We keep 1.10e+08/2.59e+09 =  4% of the original kernel matrix.

torch.Size([47569, 2])
We keep 1.35e+07/9.32e+08 =  1% of the original kernel matrix.

torch.Size([13250, 2])
We keep 2.07e+06/5.81e+07 =  3% of the original kernel matrix.

torch.Size([20246, 2])
We keep 3.05e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([24366, 2])
We keep 8.62e+06/2.37e+08 =  3% of the original kernel matrix.

torch.Size([27975, 2])
We keep 5.22e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([41110, 2])
We keep 5.68e+07/8.76e+08 =  6% of the original kernel matrix.

torch.Size([35612, 2])
We keep 8.44e+06/5.42e+08 =  1% of the original kernel matrix.

torch.Size([28146, 2])
We keep 2.57e+07/2.94e+08 =  8% of the original kernel matrix.

torch.Size([30065, 2])
We keep 5.70e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([15864, 2])
We keep 4.64e+06/1.02e+08 =  4% of the original kernel matrix.

torch.Size([21848, 2])
We keep 3.71e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([5463, 2])
We keep 3.40e+05/6.29e+06 =  5% of the original kernel matrix.

torch.Size([13414, 2])
We keep 1.31e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([22907, 2])
We keep 2.09e+07/4.85e+08 =  4% of the original kernel matrix.

torch.Size([25955, 2])
We keep 6.81e+06/4.03e+08 =  1% of the original kernel matrix.

torch.Size([16371, 2])
We keep 2.60e+06/9.26e+07 =  2% of the original kernel matrix.

torch.Size([22680, 2])
We keep 3.55e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([29934, 2])
We keep 8.43e+07/8.53e+08 =  9% of the original kernel matrix.

torch.Size([28832, 2])
We keep 8.60e+06/5.35e+08 =  1% of the original kernel matrix.

torch.Size([13771, 2])
We keep 1.82e+06/5.45e+07 =  3% of the original kernel matrix.

torch.Size([20936, 2])
We keep 2.92e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([81765, 2])
We keep 9.98e+07/3.61e+09 =  2% of the original kernel matrix.

torch.Size([50207, 2])
We keep 1.59e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([43917, 2])
We keep 2.09e+07/8.29e+08 =  2% of the original kernel matrix.

torch.Size([37291, 2])
We keep 8.69e+06/5.27e+08 =  1% of the original kernel matrix.

torch.Size([109099, 2])
We keep 1.36e+08/6.23e+09 =  2% of the original kernel matrix.

torch.Size([58096, 2])
We keep 2.03e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([9994, 2])
We keep 1.62e+06/3.15e+07 =  5% of the original kernel matrix.

torch.Size([17643, 2])
We keep 2.38e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([26169, 2])
We keep 3.10e+07/6.35e+08 =  4% of the original kernel matrix.

torch.Size([26703, 2])
We keep 7.71e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([16226, 2])
We keep 3.09e+06/7.97e+07 =  3% of the original kernel matrix.

torch.Size([22554, 2])
We keep 3.26e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([67311, 2])
We keep 3.68e+08/7.32e+09 =  5% of the original kernel matrix.

torch.Size([40804, 2])
We keep 2.17e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([24819, 2])
We keep 4.97e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([27153, 2])
We keep 5.02e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([20865, 2])
We keep 3.43e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([25759, 2])
We keep 4.22e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([134640, 2])
We keep 7.71e+07/6.83e+09 =  1% of the original kernel matrix.

torch.Size([65169, 2])
We keep 2.06e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([126142, 2])
We keep 8.72e+07/7.18e+09 =  1% of the original kernel matrix.

torch.Size([63274, 2])
We keep 2.13e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([24257, 2])
We keep 2.23e+07/2.74e+08 =  8% of the original kernel matrix.

torch.Size([28107, 2])
We keep 5.61e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([200755, 2])
We keep 3.32e+08/1.71e+10 =  1% of the original kernel matrix.

torch.Size([81272, 2])
We keep 3.11e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([194797, 2])
We keep 1.90e+08/1.81e+10 =  1% of the original kernel matrix.

torch.Size([80143, 2])
We keep 3.21e+07/2.47e+09 =  1% of the original kernel matrix.

torch.Size([428209, 2])
We keep 7.09e+08/7.18e+10 =  0% of the original kernel matrix.

torch.Size([120047, 2])
We keep 5.94e+07/4.91e+09 =  1% of the original kernel matrix.

torch.Size([34217, 2])
We keep 2.07e+07/6.04e+08 =  3% of the original kernel matrix.

torch.Size([32184, 2])
We keep 7.46e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([3599, 2])
We keep 1.50e+05/2.22e+06 =  6% of the original kernel matrix.

torch.Size([11369, 2])
We keep 9.02e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([13170, 2])
We keep 1.27e+06/4.20e+07 =  3% of the original kernel matrix.

torch.Size([20103, 2])
We keep 2.59e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([34922, 2])
We keep 3.40e+07/9.31e+08 =  3% of the original kernel matrix.

torch.Size([32215, 2])
We keep 8.67e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([45535, 2])
We keep 4.49e+07/9.08e+08 =  4% of the original kernel matrix.

torch.Size([37242, 2])
We keep 8.36e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([96267, 2])
We keep 1.72e+08/4.28e+09 =  4% of the original kernel matrix.

torch.Size([55125, 2])
We keep 1.74e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([5104, 2])
We keep 2.46e+05/4.37e+06 =  5% of the original kernel matrix.

torch.Size([12947, 2])
We keep 1.13e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([80164, 2])
We keep 5.21e+07/2.73e+09 =  1% of the original kernel matrix.

torch.Size([49298, 2])
We keep 1.42e+07/9.57e+08 =  1% of the original kernel matrix.

torch.Size([84225, 2])
We keep 4.13e+07/2.64e+09 =  1% of the original kernel matrix.

torch.Size([50161, 2])
We keep 1.39e+07/9.41e+08 =  1% of the original kernel matrix.

torch.Size([30670, 2])
We keep 1.22e+08/1.86e+09 =  6% of the original kernel matrix.

torch.Size([26604, 2])
We keep 1.14e+07/7.89e+08 =  1% of the original kernel matrix.

torch.Size([10550, 2])
We keep 1.09e+06/2.74e+07 =  3% of the original kernel matrix.

torch.Size([17999, 2])
We keep 2.26e+06/9.59e+07 =  2% of the original kernel matrix.

torch.Size([80944, 2])
We keep 4.53e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([49501, 2])
We keep 1.36e+07/9.16e+08 =  1% of the original kernel matrix.

torch.Size([17874, 2])
We keep 2.95e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([23967, 2])
We keep 3.80e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([23189, 2])
We keep 4.72e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([27816, 2])
We keep 4.98e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([26664, 2])
We keep 6.73e+06/3.10e+08 =  2% of the original kernel matrix.

torch.Size([28184, 2])
We keep 5.56e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([15366, 2])
We keep 5.09e+06/1.06e+08 =  4% of the original kernel matrix.

torch.Size([21932, 2])
We keep 3.74e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([38373, 2])
We keep 1.66e+07/5.74e+08 =  2% of the original kernel matrix.

torch.Size([34392, 2])
We keep 7.20e+06/4.39e+08 =  1% of the original kernel matrix.

torch.Size([63058, 2])
We keep 2.17e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([43694, 2])
We keep 1.05e+07/6.83e+08 =  1% of the original kernel matrix.

torch.Size([29435, 2])
We keep 6.98e+06/3.31e+08 =  2% of the original kernel matrix.

torch.Size([31164, 2])
We keep 5.90e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([27916, 2])
We keep 1.91e+07/3.77e+08 =  5% of the original kernel matrix.

torch.Size([29551, 2])
We keep 6.40e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([33419, 2])
We keep 8.75e+06/3.96e+08 =  2% of the original kernel matrix.

torch.Size([32592, 2])
We keep 6.29e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([783585, 2])
We keep 2.11e+09/2.39e+11 =  0% of the original kernel matrix.

torch.Size([165190, 2])
We keep 1.02e+08/8.95e+09 =  1% of the original kernel matrix.

torch.Size([79788, 2])
We keep 5.17e+07/2.57e+09 =  2% of the original kernel matrix.

torch.Size([48567, 2])
We keep 1.37e+07/9.28e+08 =  1% of the original kernel matrix.

torch.Size([229680, 2])
We keep 2.26e+08/2.14e+10 =  1% of the original kernel matrix.

torch.Size([87193, 2])
We keep 3.40e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([4294, 2])
We keep 2.15e+05/3.49e+06 =  6% of the original kernel matrix.

torch.Size([12017, 2])
We keep 1.06e+06/3.42e+07 =  3% of the original kernel matrix.

torch.Size([77785, 2])
We keep 7.21e+07/2.22e+09 =  3% of the original kernel matrix.

torch.Size([48386, 2])
We keep 1.30e+07/8.64e+08 =  1% of the original kernel matrix.

torch.Size([9554, 2])
We keep 8.56e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([17090, 2])
We keep 2.07e+06/8.47e+07 =  2% of the original kernel matrix.

torch.Size([47608, 2])
We keep 1.86e+07/8.54e+08 =  2% of the original kernel matrix.

torch.Size([38622, 2])
We keep 8.51e+06/5.35e+08 =  1% of the original kernel matrix.

torch.Size([6259, 2])
We keep 3.43e+05/6.96e+06 =  4% of the original kernel matrix.

torch.Size([14110, 2])
We keep 1.34e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([53440, 2])
We keep 1.78e+08/3.12e+09 =  5% of the original kernel matrix.

torch.Size([38124, 2])
We keep 1.51e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([29858, 2])
We keep 1.02e+07/3.31e+08 =  3% of the original kernel matrix.

torch.Size([30559, 2])
We keep 5.68e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([34082, 2])
We keep 8.65e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([32907, 2])
We keep 6.59e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([322661, 2])
We keep 5.49e+08/4.53e+10 =  1% of the original kernel matrix.

torch.Size([102997, 2])
We keep 4.81e+07/3.90e+09 =  1% of the original kernel matrix.

torch.Size([64041, 2])
We keep 4.47e+07/1.91e+09 =  2% of the original kernel matrix.

torch.Size([44395, 2])
We keep 1.23e+07/8.01e+08 =  1% of the original kernel matrix.

torch.Size([40368, 2])
We keep 3.19e+07/8.47e+08 =  3% of the original kernel matrix.

torch.Size([35216, 2])
We keep 8.79e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([149974, 2])
We keep 2.01e+08/9.06e+09 =  2% of the original kernel matrix.

torch.Size([69656, 2])
We keep 2.38e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([205019, 2])
We keep 8.85e+08/4.46e+10 =  1% of the original kernel matrix.

torch.Size([78487, 2])
We keep 4.84e+07/3.87e+09 =  1% of the original kernel matrix.

torch.Size([14009, 2])
We keep 1.63e+06/5.21e+07 =  3% of the original kernel matrix.

torch.Size([20699, 2])
We keep 2.81e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([19228, 2])
We keep 3.04e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([24827, 2])
We keep 4.01e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([11399, 2])
We keep 1.38e+06/3.15e+07 =  4% of the original kernel matrix.

torch.Size([18681, 2])
We keep 2.35e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([4156, 2])
We keep 4.53e+05/4.79e+06 =  9% of the original kernel matrix.

torch.Size([11820, 2])
We keep 1.18e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([44774, 2])
We keep 1.97e+07/8.48e+08 =  2% of the original kernel matrix.

torch.Size([36735, 2])
We keep 8.53e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([28284, 2])
We keep 7.41e+06/3.10e+08 =  2% of the original kernel matrix.

torch.Size([30124, 2])
We keep 5.75e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([7769, 2])
We keep 6.16e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([15665, 2])
We keep 1.69e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([28585, 2])
We keep 6.37e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([30125, 2])
We keep 5.79e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([88790, 2])
We keep 4.98e+07/2.95e+09 =  1% of the original kernel matrix.

torch.Size([51608, 2])
We keep 1.44e+07/9.94e+08 =  1% of the original kernel matrix.

torch.Size([12718, 2])
We keep 1.59e+06/4.14e+07 =  3% of the original kernel matrix.

torch.Size([19780, 2])
We keep 2.58e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([454038, 2])
We keep 1.19e+09/8.89e+10 =  1% of the original kernel matrix.

torch.Size([122891, 2])
We keep 6.47e+07/5.46e+09 =  1% of the original kernel matrix.

torch.Size([12408, 2])
We keep 3.04e+06/5.84e+07 =  5% of the original kernel matrix.

torch.Size([19304, 2])
We keep 2.94e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([95694, 2])
We keep 8.59e+07/4.17e+09 =  2% of the original kernel matrix.

torch.Size([54060, 2])
We keep 1.71e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([10482, 2])
We keep 2.70e+06/4.13e+07 =  6% of the original kernel matrix.

torch.Size([17821, 2])
We keep 2.58e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([39367, 2])
We keep 4.08e+07/1.09e+09 =  3% of the original kernel matrix.

torch.Size([33681, 2])
We keep 9.52e+06/6.06e+08 =  1% of the original kernel matrix.

torch.Size([205523, 2])
We keep 6.39e+08/3.26e+10 =  1% of the original kernel matrix.

torch.Size([80722, 2])
We keep 4.22e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([487630, 2])
We keep 3.18e+09/1.32e+11 =  2% of the original kernel matrix.

torch.Size([129460, 2])
We keep 7.68e+07/6.67e+09 =  1% of the original kernel matrix.

torch.Size([955696, 2])
We keep 2.58e+09/3.21e+11 =  0% of the original kernel matrix.

torch.Size([181973, 2])
We keep 1.16e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([47272, 2])
We keep 1.50e+07/7.88e+08 =  1% of the original kernel matrix.

torch.Size([38281, 2])
We keep 8.24e+06/5.14e+08 =  1% of the original kernel matrix.

torch.Size([51392, 2])
We keep 2.82e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([39711, 2])
We keep 9.50e+06/6.00e+08 =  1% of the original kernel matrix.

torch.Size([24247, 2])
We keep 3.99e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([28339, 2])
We keep 4.66e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([16826, 2])
We keep 1.08e+07/1.42e+08 =  7% of the original kernel matrix.

torch.Size([22443, 2])
We keep 4.09e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([92324, 2])
We keep 1.25e+08/4.19e+09 =  2% of the original kernel matrix.

torch.Size([53280, 2])
We keep 1.68e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([48375, 2])
We keep 3.52e+07/1.03e+09 =  3% of the original kernel matrix.

torch.Size([39103, 2])
We keep 9.35e+06/5.88e+08 =  1% of the original kernel matrix.

torch.Size([14444, 2])
We keep 1.86e+06/6.01e+07 =  3% of the original kernel matrix.

torch.Size([21120, 2])
We keep 3.00e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([100372, 2])
We keep 7.11e+07/4.25e+09 =  1% of the original kernel matrix.

torch.Size([55452, 2])
We keep 1.70e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([376066, 2])
We keep 7.86e+08/6.07e+10 =  1% of the original kernel matrix.

torch.Size([109862, 2])
We keep 5.53e+07/4.51e+09 =  1% of the original kernel matrix.

torch.Size([73349, 2])
We keep 4.22e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([46053, 2])
We keep 1.27e+07/8.56e+08 =  1% of the original kernel matrix.

torch.Size([23571, 2])
We keep 6.51e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([27290, 2])
We keep 5.30e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([17289, 2])
We keep 3.98e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([23474, 2])
We keep 3.80e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([26790, 2])
We keep 5.95e+06/2.87e+08 =  2% of the original kernel matrix.

torch.Size([27690, 2])
We keep 5.19e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([11957, 2])
We keep 1.11e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([19076, 2])
We keep 2.38e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([181354, 2])
We keep 2.38e+08/1.53e+10 =  1% of the original kernel matrix.

torch.Size([77047, 2])
We keep 2.99e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([31039, 2])
We keep 1.36e+07/4.39e+08 =  3% of the original kernel matrix.

torch.Size([30794, 2])
We keep 6.54e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([13561, 2])
We keep 2.88e+06/7.41e+07 =  3% of the original kernel matrix.

torch.Size([20199, 2])
We keep 3.22e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([93592, 2])
We keep 6.03e+07/3.18e+09 =  1% of the original kernel matrix.

torch.Size([53731, 2])
We keep 1.48e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([15088, 2])
We keep 2.71e+06/8.11e+07 =  3% of the original kernel matrix.

torch.Size([22040, 2])
We keep 3.47e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([1518, 2])
We keep 4.11e+04/3.53e+05 = 11% of the original kernel matrix.

torch.Size([7973, 2])
We keep 4.89e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([73503, 2])
We keep 7.00e+07/2.27e+09 =  3% of the original kernel matrix.

torch.Size([46968, 2])
We keep 1.29e+07/8.72e+08 =  1% of the original kernel matrix.

torch.Size([99571, 2])
We keep 5.99e+07/4.09e+09 =  1% of the original kernel matrix.

torch.Size([55243, 2])
We keep 1.67e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([143041, 2])
We keep 1.92e+08/9.53e+09 =  2% of the original kernel matrix.

torch.Size([67899, 2])
We keep 2.44e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([221908, 2])
We keep 6.60e+08/2.64e+10 =  2% of the original kernel matrix.

torch.Size([86180, 2])
We keep 3.81e+07/2.98e+09 =  1% of the original kernel matrix.

torch.Size([134926, 2])
We keep 1.96e+08/8.22e+09 =  2% of the original kernel matrix.

torch.Size([64677, 2])
We keep 2.27e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([19307, 2])
We keep 7.84e+06/1.50e+08 =  5% of the original kernel matrix.

torch.Size([24069, 2])
We keep 4.29e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([121160, 2])
We keep 1.56e+08/7.32e+09 =  2% of the original kernel matrix.

torch.Size([60856, 2])
We keep 2.14e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([208389, 2])
We keep 1.76e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([83237, 2])
We keep 3.00e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([230336, 2])
We keep 2.50e+08/2.53e+10 =  0% of the original kernel matrix.

torch.Size([88416, 2])
We keep 3.69e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([31704, 2])
We keep 8.85e+06/3.71e+08 =  2% of the original kernel matrix.

torch.Size([31745, 2])
We keep 6.08e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([100012, 2])
We keep 4.08e+08/7.62e+09 =  5% of the original kernel matrix.

torch.Size([56282, 2])
We keep 2.13e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([90354, 2])
We keep 7.93e+07/3.32e+09 =  2% of the original kernel matrix.

torch.Size([52674, 2])
We keep 1.50e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([71716, 2])
We keep 7.56e+07/2.31e+09 =  3% of the original kernel matrix.

torch.Size([46789, 2])
We keep 1.33e+07/8.81e+08 =  1% of the original kernel matrix.

torch.Size([45344, 2])
We keep 4.67e+07/1.16e+09 =  4% of the original kernel matrix.

torch.Size([36991, 2])
We keep 9.92e+06/6.25e+08 =  1% of the original kernel matrix.

torch.Size([23613, 2])
We keep 2.93e+07/3.74e+08 =  7% of the original kernel matrix.

torch.Size([26583, 2])
We keep 6.25e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([26140, 2])
We keep 8.48e+06/3.20e+08 =  2% of the original kernel matrix.

torch.Size([28351, 2])
We keep 5.76e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([20348, 2])
We keep 3.48e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([25306, 2])
We keep 4.14e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([5848, 2])
We keep 4.44e+05/6.96e+06 =  6% of the original kernel matrix.

torch.Size([13737, 2])
We keep 1.28e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([252588, 2])
We keep 2.77e+08/2.83e+10 =  0% of the original kernel matrix.

torch.Size([92407, 2])
We keep 3.87e+07/3.08e+09 =  1% of the original kernel matrix.

torch.Size([151066, 2])
We keep 1.11e+08/9.65e+09 =  1% of the original kernel matrix.

torch.Size([69613, 2])
We keep 2.40e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([375932, 2])
We keep 6.12e+08/5.50e+10 =  1% of the original kernel matrix.

torch.Size([110236, 2])
We keep 5.21e+07/4.29e+09 =  1% of the original kernel matrix.

torch.Size([83003, 2])
We keep 1.01e+08/3.42e+09 =  2% of the original kernel matrix.

torch.Size([50016, 2])
We keep 1.58e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([119656, 2])
We keep 1.00e+08/6.33e+09 =  1% of the original kernel matrix.

torch.Size([61440, 2])
We keep 2.04e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([14576, 2])
We keep 2.15e+06/7.71e+07 =  2% of the original kernel matrix.

torch.Size([21170, 2])
We keep 3.25e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([124223, 2])
We keep 9.98e+07/7.23e+09 =  1% of the original kernel matrix.

torch.Size([63018, 2])
We keep 2.16e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([5335, 2])
We keep 4.75e+05/6.17e+06 =  7% of the original kernel matrix.

torch.Size([12989, 2])
We keep 1.30e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([35859, 2])
We keep 1.13e+07/5.14e+08 =  2% of the original kernel matrix.

torch.Size([33492, 2])
We keep 6.96e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([127100, 2])
We keep 9.46e+07/6.66e+09 =  1% of the original kernel matrix.

torch.Size([63093, 2])
We keep 2.07e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([98094, 2])
We keep 6.61e+07/3.83e+09 =  1% of the original kernel matrix.

torch.Size([54991, 2])
We keep 1.63e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([5698, 2])
We keep 3.73e+05/6.43e+06 =  5% of the original kernel matrix.

torch.Size([13585, 2])
We keep 1.31e+06/4.64e+07 =  2% of the original kernel matrix.

torch.Size([14942, 2])
We keep 1.82e+06/6.37e+07 =  2% of the original kernel matrix.

torch.Size([21591, 2])
We keep 3.03e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([174492, 2])
We keep 5.13e+08/1.92e+10 =  2% of the original kernel matrix.

torch.Size([74804, 2])
We keep 3.34e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([27701, 2])
We keep 8.19e+06/2.98e+08 =  2% of the original kernel matrix.

torch.Size([29624, 2])
We keep 5.48e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([149713, 2])
We keep 1.72e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([69531, 2])
We keep 2.51e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([19648, 2])
We keep 8.30e+06/1.75e+08 =  4% of the original kernel matrix.

torch.Size([24607, 2])
We keep 4.52e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([19549, 2])
We keep 6.76e+06/1.65e+08 =  4% of the original kernel matrix.

torch.Size([24608, 2])
We keep 4.40e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([4885, 2])
We keep 2.91e+05/4.64e+06 =  6% of the original kernel matrix.

torch.Size([12788, 2])
We keep 1.16e+06/3.94e+07 =  2% of the original kernel matrix.

torch.Size([6874, 2])
We keep 4.56e+05/9.96e+06 =  4% of the original kernel matrix.

torch.Size([14755, 2])
We keep 1.53e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([68542, 2])
We keep 5.64e+07/1.97e+09 =  2% of the original kernel matrix.

torch.Size([45842, 2])
We keep 1.24e+07/8.12e+08 =  1% of the original kernel matrix.

torch.Size([82363, 2])
We keep 1.16e+08/3.47e+09 =  3% of the original kernel matrix.

torch.Size([49658, 2])
We keep 1.58e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([4307, 2])
We keep 2.90e+05/3.87e+06 =  7% of the original kernel matrix.

torch.Size([12015, 2])
We keep 1.04e+06/3.60e+07 =  2% of the original kernel matrix.

torch.Size([157052, 2])
We keep 1.53e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([70383, 2])
We keep 2.49e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([23275, 2])
We keep 9.81e+06/2.38e+08 =  4% of the original kernel matrix.

torch.Size([26129, 2])
We keep 4.92e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([37595, 2])
We keep 1.26e+07/5.86e+08 =  2% of the original kernel matrix.

torch.Size([34703, 2])
We keep 7.42e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([11294, 2])
We keep 1.49e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([18490, 2])
We keep 2.51e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([490135, 2])
We keep 8.11e+08/8.40e+10 =  0% of the original kernel matrix.

torch.Size([126499, 2])
We keep 6.33e+07/5.31e+09 =  1% of the original kernel matrix.

torch.Size([6705, 2])
We keep 1.14e+06/1.10e+07 = 10% of the original kernel matrix.

torch.Size([14465, 2])
We keep 1.56e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([78845, 2])
We keep 8.65e+07/2.65e+09 =  3% of the original kernel matrix.

torch.Size([48861, 2])
We keep 1.40e+07/9.43e+08 =  1% of the original kernel matrix.

torch.Size([76503, 2])
We keep 3.93e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([48179, 2])
We keep 1.27e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([63025, 2])
We keep 4.52e+07/1.93e+09 =  2% of the original kernel matrix.

torch.Size([43047, 2])
We keep 1.22e+07/8.04e+08 =  1% of the original kernel matrix.

torch.Size([58126, 2])
We keep 2.64e+07/1.21e+09 =  2% of the original kernel matrix.

torch.Size([42177, 2])
We keep 9.98e+06/6.38e+08 =  1% of the original kernel matrix.

torch.Size([643026, 2])
We keep 2.38e+09/2.08e+11 =  1% of the original kernel matrix.

torch.Size([147034, 2])
We keep 9.58e+07/8.36e+09 =  1% of the original kernel matrix.

torch.Size([61657, 2])
We keep 2.44e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([43242, 2])
We keep 1.07e+07/6.98e+08 =  1% of the original kernel matrix.

torch.Size([16212, 2])
We keep 3.43e+06/9.85e+07 =  3% of the original kernel matrix.

torch.Size([22460, 2])
We keep 3.65e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([67588, 2])
We keep 3.98e+07/1.69e+09 =  2% of the original kernel matrix.

torch.Size([45062, 2])
We keep 1.14e+07/7.53e+08 =  1% of the original kernel matrix.

torch.Size([36781, 2])
We keep 1.39e+07/5.31e+08 =  2% of the original kernel matrix.

torch.Size([33698, 2])
We keep 7.11e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([434124, 2])
We keep 7.35e+08/7.74e+10 =  0% of the original kernel matrix.

torch.Size([119712, 2])
We keep 6.13e+07/5.09e+09 =  1% of the original kernel matrix.

torch.Size([47634, 2])
We keep 2.61e+07/1.04e+09 =  2% of the original kernel matrix.

torch.Size([37920, 2])
We keep 9.26e+06/5.91e+08 =  1% of the original kernel matrix.

torch.Size([198255, 2])
We keep 2.37e+08/1.93e+10 =  1% of the original kernel matrix.

torch.Size([80368, 2])
We keep 3.30e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([584170, 2])
We keep 1.05e+09/1.37e+11 =  0% of the original kernel matrix.

torch.Size([142727, 2])
We keep 7.90e+07/6.77e+09 =  1% of the original kernel matrix.

torch.Size([23147, 2])
We keep 5.21e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([27105, 2])
We keep 4.95e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([179917, 2])
We keep 2.80e+08/1.78e+10 =  1% of the original kernel matrix.

torch.Size([75006, 2])
We keep 3.19e+07/2.44e+09 =  1% of the original kernel matrix.

torch.Size([18948, 2])
We keep 1.88e+07/2.88e+08 =  6% of the original kernel matrix.

torch.Size([23216, 2])
We keep 5.56e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([562007, 2])
We keep 1.93e+09/1.35e+11 =  1% of the original kernel matrix.

torch.Size([138620, 2])
We keep 7.93e+07/6.73e+09 =  1% of the original kernel matrix.

torch.Size([36128, 2])
We keep 9.72e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([33940, 2])
We keep 6.95e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([43199, 2])
We keep 2.81e+07/9.35e+08 =  3% of the original kernel matrix.

torch.Size([35955, 2])
We keep 8.89e+06/5.60e+08 =  1% of the original kernel matrix.

torch.Size([11091, 2])
We keep 1.47e+07/1.02e+08 = 14% of the original kernel matrix.

torch.Size([17939, 2])
We keep 3.79e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([8623, 2])
We keep 6.30e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([16335, 2])
We keep 1.75e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([10165, 2])
We keep 1.23e+06/2.86e+07 =  4% of the original kernel matrix.

torch.Size([17501, 2])
We keep 2.25e+06/9.80e+07 =  2% of the original kernel matrix.

torch.Size([55534, 2])
We keep 2.37e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([41399, 2])
We keep 9.23e+06/5.99e+08 =  1% of the original kernel matrix.

torch.Size([26397, 2])
We keep 1.00e+07/2.97e+08 =  3% of the original kernel matrix.

torch.Size([28836, 2])
We keep 5.45e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([225434, 2])
We keep 4.18e+08/2.48e+10 =  1% of the original kernel matrix.

torch.Size([87362, 2])
We keep 3.70e+07/2.89e+09 =  1% of the original kernel matrix.

torch.Size([18297, 2])
We keep 2.56e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([24043, 2])
We keep 3.67e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([18275, 2])
We keep 3.74e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([23845, 2])
We keep 4.10e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([13030, 2])
We keep 2.48e+06/5.21e+07 =  4% of the original kernel matrix.

torch.Size([19970, 2])
We keep 2.84e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([95245, 2])
We keep 1.83e+08/7.21e+09 =  2% of the original kernel matrix.

torch.Size([54344, 2])
We keep 2.17e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([154451, 2])
We keep 5.14e+08/1.85e+10 =  2% of the original kernel matrix.

torch.Size([71016, 2])
We keep 3.30e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([339434, 2])
We keep 5.37e+08/4.82e+10 =  1% of the original kernel matrix.

torch.Size([106591, 2])
We keep 4.96e+07/4.02e+09 =  1% of the original kernel matrix.

torch.Size([46472, 2])
We keep 2.81e+07/9.68e+08 =  2% of the original kernel matrix.

torch.Size([38160, 2])
We keep 9.22e+06/5.70e+08 =  1% of the original kernel matrix.

torch.Size([6565, 2])
We keep 8.38e+05/1.14e+07 =  7% of the original kernel matrix.

torch.Size([14364, 2])
We keep 1.60e+06/6.18e+07 =  2% of the original kernel matrix.

torch.Size([10139, 2])
We keep 9.29e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([17613, 2])
We keep 2.17e+06/9.13e+07 =  2% of the original kernel matrix.

torch.Size([11241, 2])
We keep 3.22e+06/4.18e+07 =  7% of the original kernel matrix.

torch.Size([18388, 2])
We keep 2.55e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([650495, 2])
We keep 2.16e+09/1.80e+11 =  1% of the original kernel matrix.

torch.Size([153334, 2])
We keep 8.83e+07/7.77e+09 =  1% of the original kernel matrix.

torch.Size([9833, 2])
We keep 1.02e+06/2.27e+07 =  4% of the original kernel matrix.

torch.Size([17301, 2])
We keep 2.06e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([28743, 2])
We keep 6.54e+06/2.95e+08 =  2% of the original kernel matrix.

torch.Size([30876, 2])
We keep 5.67e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([89324, 2])
We keep 6.58e+07/3.20e+09 =  2% of the original kernel matrix.

torch.Size([51926, 2])
We keep 1.49e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([30126, 2])
We keep 2.08e+07/5.22e+08 =  3% of the original kernel matrix.

torch.Size([29542, 2])
We keep 7.05e+06/4.18e+08 =  1% of the original kernel matrix.

torch.Size([44466, 2])
We keep 2.62e+07/9.18e+08 =  2% of the original kernel matrix.

torch.Size([37054, 2])
We keep 8.99e+06/5.55e+08 =  1% of the original kernel matrix.

torch.Size([15706, 2])
We keep 2.46e+06/8.06e+07 =  3% of the original kernel matrix.

torch.Size([22008, 2])
We keep 3.33e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([175831, 2])
We keep 2.95e+08/1.69e+10 =  1% of the original kernel matrix.

torch.Size([74897, 2])
We keep 3.14e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([75421, 2])
We keep 8.17e+07/2.67e+09 =  3% of the original kernel matrix.

torch.Size([47861, 2])
We keep 1.42e+07/9.46e+08 =  1% of the original kernel matrix.

torch.Size([15818, 2])
We keep 6.90e+06/9.37e+07 =  7% of the original kernel matrix.

torch.Size([22129, 2])
We keep 3.54e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([36875, 2])
We keep 1.38e+07/5.15e+08 =  2% of the original kernel matrix.

torch.Size([34170, 2])
We keep 6.93e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([108279, 2])
We keep 7.79e+08/1.47e+10 =  5% of the original kernel matrix.

torch.Size([53590, 2])
We keep 2.94e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([77232, 2])
We keep 8.09e+07/2.81e+09 =  2% of the original kernel matrix.

torch.Size([48337, 2])
We keep 1.43e+07/9.71e+08 =  1% of the original kernel matrix.

torch.Size([501561, 2])
We keep 1.33e+09/1.09e+11 =  1% of the original kernel matrix.

torch.Size([130695, 2])
We keep 7.22e+07/6.05e+09 =  1% of the original kernel matrix.

torch.Size([127991, 2])
We keep 9.19e+07/6.29e+09 =  1% of the original kernel matrix.

torch.Size([63384, 2])
We keep 2.00e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([10258, 2])
We keep 1.57e+06/2.84e+07 =  5% of the original kernel matrix.

torch.Size([17888, 2])
We keep 2.29e+06/9.75e+07 =  2% of the original kernel matrix.

torch.Size([52208, 2])
We keep 2.27e+07/1.00e+09 =  2% of the original kernel matrix.

torch.Size([40058, 2])
We keep 8.94e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([18998, 2])
We keep 7.22e+06/1.49e+08 =  4% of the original kernel matrix.

torch.Size([24339, 2])
We keep 4.31e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([12829, 2])
We keep 3.23e+06/6.03e+07 =  5% of the original kernel matrix.

torch.Size([20004, 2])
We keep 3.05e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([25170, 2])
We keep 5.33e+06/2.46e+08 =  2% of the original kernel matrix.

torch.Size([26993, 2])
We keep 4.97e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([14076, 2])
We keep 1.55e+06/5.19e+07 =  2% of the original kernel matrix.

torch.Size([20776, 2])
We keep 2.79e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([75923, 2])
We keep 7.93e+07/2.46e+09 =  3% of the original kernel matrix.

torch.Size([47797, 2])
We keep 1.34e+07/9.08e+08 =  1% of the original kernel matrix.

torch.Size([117877, 2])
We keep 1.18e+08/6.63e+09 =  1% of the original kernel matrix.

torch.Size([59911, 2])
We keep 2.04e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([13114, 2])
We keep 3.02e+06/7.15e+07 =  4% of the original kernel matrix.

torch.Size([19622, 2])
We keep 3.06e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([56351, 2])
We keep 2.49e+07/1.19e+09 =  2% of the original kernel matrix.

torch.Size([41137, 2])
We keep 9.87e+06/6.31e+08 =  1% of the original kernel matrix.

torch.Size([7025, 2])
We keep 2.51e+06/2.44e+07 = 10% of the original kernel matrix.

torch.Size([14567, 2])
We keep 2.08e+06/9.05e+07 =  2% of the original kernel matrix.

torch.Size([39844, 2])
We keep 3.50e+07/9.58e+08 =  3% of the original kernel matrix.

torch.Size([34647, 2])
We keep 9.19e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([9189, 2])
We keep 1.04e+06/2.20e+07 =  4% of the original kernel matrix.

torch.Size([16830, 2])
We keep 2.04e+06/8.58e+07 =  2% of the original kernel matrix.

torch.Size([14650, 2])
We keep 3.03e+06/8.55e+07 =  3% of the original kernel matrix.

torch.Size([21038, 2])
We keep 3.39e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([39789, 2])
We keep 7.99e+07/1.32e+09 =  6% of the original kernel matrix.

torch.Size([33374, 2])
We keep 1.02e+07/6.64e+08 =  1% of the original kernel matrix.

torch.Size([16686, 2])
We keep 3.67e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([22833, 2])
We keep 3.77e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([8239, 2])
We keep 7.85e+05/1.37e+07 =  5% of the original kernel matrix.

torch.Size([16124, 2])
We keep 1.72e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([19982, 2])
We keep 3.19e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([25161, 2])
We keep 3.96e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([27037, 2])
We keep 5.94e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([28113, 2])
We keep 5.15e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([151460, 2])
We keep 2.25e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([69358, 2])
We keep 2.66e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([7445, 2])
We keep 5.80e+05/1.12e+07 =  5% of the original kernel matrix.

torch.Size([15382, 2])
We keep 1.61e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([4231, 2])
We keep 2.05e+05/3.35e+06 =  6% of the original kernel matrix.

torch.Size([11953, 2])
We keep 1.02e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([30278, 2])
We keep 8.59e+06/3.51e+08 =  2% of the original kernel matrix.

torch.Size([31113, 2])
We keep 6.06e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([120655, 2])
We keep 9.23e+07/5.62e+09 =  1% of the original kernel matrix.

torch.Size([61224, 2])
We keep 1.91e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([296437, 2])
We keep 4.52e+08/3.90e+10 =  1% of the original kernel matrix.

torch.Size([100274, 2])
We keep 4.49e+07/3.61e+09 =  1% of the original kernel matrix.

torch.Size([151855, 2])
We keep 2.01e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([68510, 2])
We keep 2.67e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([13589, 2])
We keep 3.40e+06/6.23e+07 =  5% of the original kernel matrix.

torch.Size([20520, 2])
We keep 3.10e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([228343, 2])
We keep 6.28e+08/3.01e+10 =  2% of the original kernel matrix.

torch.Size([87323, 2])
We keep 4.05e+07/3.18e+09 =  1% of the original kernel matrix.

torch.Size([202519, 2])
We keep 1.92e+08/1.89e+10 =  1% of the original kernel matrix.

torch.Size([82914, 2])
We keep 3.28e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([5453, 2])
We keep 3.11e+05/5.20e+06 =  5% of the original kernel matrix.

torch.Size([13205, 2])
We keep 1.22e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([9243, 2])
We keep 8.98e+05/2.16e+07 =  4% of the original kernel matrix.

torch.Size([17000, 2])
We keep 2.02e+06/8.51e+07 =  2% of the original kernel matrix.

torch.Size([90909, 2])
We keep 3.83e+07/2.82e+09 =  1% of the original kernel matrix.

torch.Size([52568, 2])
We keep 1.41e+07/9.73e+08 =  1% of the original kernel matrix.

torch.Size([22367, 2])
We keep 2.82e+07/4.34e+08 =  6% of the original kernel matrix.

torch.Size([25482, 2])
We keep 6.54e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([58328, 2])
We keep 5.92e+07/1.63e+09 =  3% of the original kernel matrix.

torch.Size([41864, 2])
We keep 1.11e+07/7.40e+08 =  1% of the original kernel matrix.

torch.Size([136033, 2])
We keep 1.06e+08/7.10e+09 =  1% of the original kernel matrix.

torch.Size([65659, 2])
We keep 2.13e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([66170, 2])
We keep 2.60e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([44638, 2])
We keep 1.08e+07/7.12e+08 =  1% of the original kernel matrix.

torch.Size([35418, 2])
We keep 2.16e+07/4.57e+08 =  4% of the original kernel matrix.

torch.Size([33134, 2])
We keep 6.71e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([6197, 2])
We keep 4.07e+05/7.58e+06 =  5% of the original kernel matrix.

torch.Size([13987, 2])
We keep 1.40e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([51160, 2])
We keep 2.53e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([39626, 2])
We keep 9.48e+06/5.99e+08 =  1% of the original kernel matrix.

torch.Size([64398, 2])
We keep 2.78e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([43891, 2])
We keep 1.15e+07/7.50e+08 =  1% of the original kernel matrix.

torch.Size([24163, 2])
We keep 5.03e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([27579, 2])
We keep 4.74e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([67385, 2])
We keep 5.93e+07/2.02e+09 =  2% of the original kernel matrix.

torch.Size([45052, 2])
We keep 1.26e+07/8.23e+08 =  1% of the original kernel matrix.

torch.Size([421562, 2])
We keep 6.47e+08/6.96e+10 =  0% of the original kernel matrix.

torch.Size([118001, 2])
We keep 5.85e+07/4.83e+09 =  1% of the original kernel matrix.

torch.Size([14022, 2])
We keep 1.75e+06/5.41e+07 =  3% of the original kernel matrix.

torch.Size([20606, 2])
We keep 2.81e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([5398, 2])
We keep 3.28e+05/6.13e+06 =  5% of the original kernel matrix.

torch.Size([13186, 2])
We keep 1.24e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([13289, 2])
We keep 1.41e+06/4.66e+07 =  3% of the original kernel matrix.

torch.Size([20189, 2])
We keep 2.69e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([40659, 2])
We keep 1.04e+07/6.09e+08 =  1% of the original kernel matrix.

torch.Size([35879, 2])
We keep 7.43e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([35791, 2])
We keep 1.08e+07/5.42e+08 =  2% of the original kernel matrix.

torch.Size([33563, 2])
We keep 7.11e+06/4.26e+08 =  1% of the original kernel matrix.

torch.Size([18487, 2])
We keep 9.43e+06/1.61e+08 =  5% of the original kernel matrix.

torch.Size([23496, 2])
We keep 4.35e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([4254, 2])
We keep 2.52e+05/3.56e+06 =  7% of the original kernel matrix.

torch.Size([11921, 2])
We keep 1.06e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([51660, 2])
We keep 1.95e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([40060, 2])
We keep 9.27e+06/5.88e+08 =  1% of the original kernel matrix.

torch.Size([23765, 2])
We keep 5.54e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([27754, 2])
We keep 4.97e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([38421, 2])
We keep 2.09e+07/6.99e+08 =  2% of the original kernel matrix.

torch.Size([34025, 2])
We keep 7.86e+06/4.84e+08 =  1% of the original kernel matrix.

torch.Size([5869, 2])
We keep 3.85e+05/7.20e+06 =  5% of the original kernel matrix.

torch.Size([13772, 2])
We keep 1.38e+06/4.92e+07 =  2% of the original kernel matrix.

torch.Size([14948, 2])
We keep 1.84e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([21707, 2])
We keep 3.16e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([20152, 2])
We keep 4.38e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([25322, 2])
We keep 4.26e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([14759, 2])
We keep 1.57e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([21480, 2])
We keep 2.89e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([40524, 2])
We keep 1.41e+07/6.53e+08 =  2% of the original kernel matrix.

torch.Size([35673, 2])
We keep 7.74e+06/4.68e+08 =  1% of the original kernel matrix.

torch.Size([173699, 2])
We keep 1.34e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([74950, 2])
We keep 2.60e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([288462, 2])
We keep 3.36e+08/3.16e+10 =  1% of the original kernel matrix.

torch.Size([97768, 2])
We keep 4.08e+07/3.26e+09 =  1% of the original kernel matrix.

torch.Size([126017, 2])
We keep 1.37e+08/5.82e+09 =  2% of the original kernel matrix.

torch.Size([62820, 2])
We keep 1.93e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([91618, 2])
We keep 7.40e+07/3.74e+09 =  1% of the original kernel matrix.

torch.Size([52872, 2])
We keep 1.62e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([22622, 2])
We keep 3.62e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([26837, 2])
We keep 4.35e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([20393, 2])
We keep 6.80e+06/1.76e+08 =  3% of the original kernel matrix.

torch.Size([25413, 2])
We keep 4.65e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([23458, 2])
We keep 9.41e+06/2.56e+08 =  3% of the original kernel matrix.

torch.Size([27463, 2])
We keep 5.43e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([139065, 2])
We keep 9.40e+07/8.16e+09 =  1% of the original kernel matrix.

torch.Size([66664, 2])
We keep 2.25e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([15832, 2])
We keep 2.62e+06/8.34e+07 =  3% of the original kernel matrix.

torch.Size([22347, 2])
We keep 3.44e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([26634, 2])
We keep 9.06e+06/2.47e+08 =  3% of the original kernel matrix.

torch.Size([28260, 2])
We keep 4.94e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([15927, 2])
We keep 2.36e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([22367, 2])
We keep 3.50e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([572201, 2])
We keep 4.51e+09/2.23e+11 =  2% of the original kernel matrix.

torch.Size([139694, 2])
We keep 1.00e+08/8.65e+09 =  1% of the original kernel matrix.

torch.Size([59760, 2])
We keep 3.87e+07/1.45e+09 =  2% of the original kernel matrix.

torch.Size([42109, 2])
We keep 1.07e+07/6.97e+08 =  1% of the original kernel matrix.

torch.Size([45939, 2])
We keep 2.70e+07/1.00e+09 =  2% of the original kernel matrix.

torch.Size([37852, 2])
We keep 9.49e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([187235, 2])
We keep 2.47e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([78386, 2])
We keep 3.04e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([48547, 2])
We keep 1.66e+07/9.78e+08 =  1% of the original kernel matrix.

torch.Size([39139, 2])
We keep 9.11e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([39746, 2])
We keep 1.57e+07/6.52e+08 =  2% of the original kernel matrix.

torch.Size([35392, 2])
We keep 7.68e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([448747, 2])
We keep 1.96e+09/9.01e+10 =  2% of the original kernel matrix.

torch.Size([123311, 2])
We keep 6.63e+07/5.50e+09 =  1% of the original kernel matrix.

torch.Size([274749, 2])
We keep 3.20e+08/3.30e+10 =  0% of the original kernel matrix.

torch.Size([95079, 2])
We keep 4.18e+07/3.33e+09 =  1% of the original kernel matrix.

torch.Size([129072, 2])
We keep 9.76e+07/6.38e+09 =  1% of the original kernel matrix.

torch.Size([64047, 2])
We keep 2.03e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([45378, 2])
We keep 2.39e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([36721, 2])
We keep 9.20e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([72122, 2])
We keep 4.47e+07/2.14e+09 =  2% of the original kernel matrix.

torch.Size([46542, 2])
We keep 1.27e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([7756, 2])
We keep 5.74e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([15747, 2])
We keep 1.65e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([191290, 2])
We keep 2.51e+08/1.51e+10 =  1% of the original kernel matrix.

torch.Size([79591, 2])
We keep 2.96e+07/2.25e+09 =  1% of the original kernel matrix.

torch.Size([1133558, 2])
We keep 3.48e+09/4.27e+11 =  0% of the original kernel matrix.

torch.Size([198678, 2])
We keep 1.34e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([75828, 2])
We keep 8.20e+07/2.60e+09 =  3% of the original kernel matrix.

torch.Size([48142, 2])
We keep 1.40e+07/9.34e+08 =  1% of the original kernel matrix.

torch.Size([59551, 2])
We keep 7.47e+07/2.37e+09 =  3% of the original kernel matrix.

torch.Size([41005, 2])
We keep 1.34e+07/8.92e+08 =  1% of the original kernel matrix.

torch.Size([30852, 2])
We keep 1.97e+07/3.82e+08 =  5% of the original kernel matrix.

torch.Size([31521, 2])
We keep 6.37e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([41106, 2])
We keep 3.99e+07/9.77e+08 =  4% of the original kernel matrix.

torch.Size([35039, 2])
We keep 8.95e+06/5.72e+08 =  1% of the original kernel matrix.

torch.Size([106621, 2])
We keep 4.66e+08/8.14e+09 =  5% of the original kernel matrix.

torch.Size([57444, 2])
We keep 2.30e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([19681, 2])
We keep 3.96e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([24712, 2])
We keep 4.08e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([24355, 2])
We keep 6.67e+06/2.44e+08 =  2% of the original kernel matrix.

torch.Size([28428, 2])
We keep 5.18e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([529919, 2])
We keep 9.17e+08/1.14e+11 =  0% of the original kernel matrix.

torch.Size([134207, 2])
We keep 7.30e+07/6.19e+09 =  1% of the original kernel matrix.

torch.Size([38127, 2])
We keep 5.64e+07/1.47e+09 =  3% of the original kernel matrix.

torch.Size([31816, 2])
We keep 1.10e+07/7.03e+08 =  1% of the original kernel matrix.

torch.Size([187694, 2])
We keep 5.08e+08/2.25e+10 =  2% of the original kernel matrix.

torch.Size([75546, 2])
We keep 3.51e+07/2.75e+09 =  1% of the original kernel matrix.

torch.Size([77229, 2])
We keep 8.97e+07/2.35e+09 =  3% of the original kernel matrix.

torch.Size([48156, 2])
We keep 1.33e+07/8.88e+08 =  1% of the original kernel matrix.

torch.Size([94888, 2])
We keep 5.63e+07/3.30e+09 =  1% of the original kernel matrix.

torch.Size([53853, 2])
We keep 1.51e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([52427, 2])
We keep 4.99e+07/1.31e+09 =  3% of the original kernel matrix.

torch.Size([39483, 2])
We keep 1.03e+07/6.64e+08 =  1% of the original kernel matrix.

torch.Size([137982, 2])
We keep 1.30e+08/7.85e+09 =  1% of the original kernel matrix.

torch.Size([65967, 2])
We keep 2.21e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([265447, 2])
We keep 2.72e+08/2.74e+10 =  0% of the original kernel matrix.

torch.Size([94166, 2])
We keep 3.79e+07/3.03e+09 =  1% of the original kernel matrix.

torch.Size([12262, 2])
We keep 2.22e+06/3.95e+07 =  5% of the original kernel matrix.

torch.Size([19422, 2])
We keep 2.56e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([909977, 2])
We keep 2.56e+09/3.08e+11 =  0% of the original kernel matrix.

torch.Size([176950, 2])
We keep 1.14e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([35845, 2])
We keep 1.65e+07/5.63e+08 =  2% of the original kernel matrix.

torch.Size([33578, 2])
We keep 7.14e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([63326, 2])
We keep 6.10e+07/2.05e+09 =  2% of the original kernel matrix.

torch.Size([43616, 2])
We keep 1.27e+07/8.29e+08 =  1% of the original kernel matrix.

torch.Size([5366, 2])
We keep 3.58e+05/5.45e+06 =  6% of the original kernel matrix.

torch.Size([13457, 2])
We keep 1.22e+06/4.28e+07 =  2% of the original kernel matrix.

torch.Size([89352, 2])
We keep 5.08e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([51734, 2])
We keep 1.49e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([5642, 2])
We keep 3.77e+05/6.45e+06 =  5% of the original kernel matrix.

torch.Size([13410, 2])
We keep 1.30e+06/4.65e+07 =  2% of the original kernel matrix.

torch.Size([6184, 2])
We keep 8.15e+05/9.00e+06 =  9% of the original kernel matrix.

torch.Size([13808, 2])
We keep 1.46e+06/5.49e+07 =  2% of the original kernel matrix.

torch.Size([20458, 2])
We keep 8.14e+06/1.54e+08 =  5% of the original kernel matrix.

torch.Size([25501, 2])
We keep 4.28e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([405731, 2])
We keep 1.48e+09/8.82e+10 =  1% of the original kernel matrix.

torch.Size([113948, 2])
We keep 6.46e+07/5.44e+09 =  1% of the original kernel matrix.

torch.Size([965982, 2])
We keep 1.74e+10/1.21e+12 =  1% of the original kernel matrix.

torch.Size([148848, 2])
We keep 2.20e+08/2.02e+10 =  1% of the original kernel matrix.

torch.Size([11193, 2])
We keep 2.00e+06/3.52e+07 =  5% of the original kernel matrix.

torch.Size([18539, 2])
We keep 2.44e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([549237, 2])
We keep 1.09e+09/1.19e+11 =  0% of the original kernel matrix.

torch.Size([136701, 2])
We keep 7.43e+07/6.31e+09 =  1% of the original kernel matrix.

torch.Size([16275, 2])
We keep 3.45e+06/8.05e+07 =  4% of the original kernel matrix.

torch.Size([22523, 2])
We keep 3.37e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([22344, 2])
We keep 5.18e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([26884, 2])
We keep 4.75e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([14862, 2])
We keep 4.92e+06/8.57e+07 =  5% of the original kernel matrix.

torch.Size([21363, 2])
We keep 3.49e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([86684, 2])
We keep 9.22e+07/3.44e+09 =  2% of the original kernel matrix.

torch.Size([50986, 2])
We keep 1.57e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([15499, 2])
We keep 3.12e+06/8.01e+07 =  3% of the original kernel matrix.

torch.Size([21812, 2])
We keep 3.29e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([1416375, 2])
We keep 5.11e+09/7.09e+11 =  0% of the original kernel matrix.

torch.Size([225233, 2])
We keep 1.69e+08/1.54e+10 =  1% of the original kernel matrix.

torch.Size([8648, 2])
We keep 6.08e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([16263, 2])
We keep 1.75e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([155795, 2])
We keep 1.90e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([71178, 2])
We keep 2.52e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([23187, 2])
We keep 1.13e+07/2.23e+08 =  5% of the original kernel matrix.

torch.Size([27225, 2])
We keep 5.16e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([99319, 2])
We keep 7.34e+07/4.32e+09 =  1% of the original kernel matrix.

torch.Size([55039, 2])
We keep 1.73e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([163105, 2])
We keep 1.77e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([72002, 2])
We keep 2.75e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([38898, 2])
We keep 1.11e+07/5.54e+08 =  2% of the original kernel matrix.

torch.Size([34728, 2])
We keep 7.09e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([18540, 2])
We keep 4.44e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([24228, 2])
We keep 3.93e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([138102, 2])
We keep 1.01e+08/8.44e+09 =  1% of the original kernel matrix.

torch.Size([66583, 2])
We keep 2.29e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([12267, 2])
We keep 1.57e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([19506, 2])
We keep 2.63e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([8521, 2])
We keep 2.29e+06/3.67e+07 =  6% of the original kernel matrix.

torch.Size([15810, 2])
We keep 2.48e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([326555, 2])
We keep 5.20e+08/4.84e+10 =  1% of the original kernel matrix.

torch.Size([102555, 2])
We keep 4.96e+07/4.03e+09 =  1% of the original kernel matrix.

torch.Size([42800, 2])
We keep 5.44e+07/9.68e+08 =  5% of the original kernel matrix.

torch.Size([35980, 2])
We keep 8.56e+06/5.70e+08 =  1% of the original kernel matrix.

torch.Size([46988, 2])
We keep 3.04e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([37932, 2])
We keep 9.45e+06/5.86e+08 =  1% of the original kernel matrix.

torch.Size([33821, 2])
We keep 1.18e+07/4.86e+08 =  2% of the original kernel matrix.

torch.Size([31911, 2])
We keep 6.77e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([153311, 2])
We keep 2.58e+08/1.20e+10 =  2% of the original kernel matrix.

torch.Size([69910, 2])
We keep 2.69e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([221770, 2])
We keep 5.07e+08/2.51e+10 =  2% of the original kernel matrix.

torch.Size([85737, 2])
We keep 3.70e+07/2.90e+09 =  1% of the original kernel matrix.

torch.Size([150629, 2])
We keep 1.28e+08/9.11e+09 =  1% of the original kernel matrix.

torch.Size([69911, 2])
We keep 2.39e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([11749, 2])
We keep 1.51e+06/3.67e+07 =  4% of the original kernel matrix.

torch.Size([19070, 2])
We keep 2.52e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([123768, 2])
We keep 1.75e+08/7.70e+09 =  2% of the original kernel matrix.

torch.Size([61797, 2])
We keep 2.14e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([63085, 2])
We keep 3.40e+07/1.59e+09 =  2% of the original kernel matrix.

torch.Size([42988, 2])
We keep 1.12e+07/7.30e+08 =  1% of the original kernel matrix.

torch.Size([50150, 2])
We keep 3.31e+07/1.21e+09 =  2% of the original kernel matrix.

torch.Size([38837, 2])
We keep 9.99e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([111795, 2])
We keep 8.71e+07/5.40e+09 =  1% of the original kernel matrix.

torch.Size([58890, 2])
We keep 1.89e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([51626, 2])
We keep 3.03e+07/1.19e+09 =  2% of the original kernel matrix.

torch.Size([39072, 2])
We keep 9.77e+06/6.33e+08 =  1% of the original kernel matrix.

torch.Size([59099, 2])
We keep 5.76e+07/1.80e+09 =  3% of the original kernel matrix.

torch.Size([42381, 2])
We keep 1.20e+07/7.78e+08 =  1% of the original kernel matrix.

torch.Size([349538, 2])
We keep 1.16e+09/7.21e+10 =  1% of the original kernel matrix.

torch.Size([102988, 2])
We keep 5.86e+07/4.92e+09 =  1% of the original kernel matrix.

torch.Size([49119, 2])
We keep 2.96e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([38678, 2])
We keep 9.60e+06/5.99e+08 =  1% of the original kernel matrix.

torch.Size([35621, 2])
We keep 9.10e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([33433, 2])
We keep 6.50e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([6871, 2])
We keep 4.59e+05/9.40e+06 =  4% of the original kernel matrix.

torch.Size([14644, 2])
We keep 1.48e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([23077, 2])
We keep 8.78e+06/2.19e+08 =  4% of the original kernel matrix.

torch.Size([27464, 2])
We keep 5.03e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([72881, 2])
We keep 6.36e+07/2.53e+09 =  2% of the original kernel matrix.

torch.Size([46548, 2])
We keep 1.37e+07/9.22e+08 =  1% of the original kernel matrix.

torch.Size([154088, 2])
We keep 2.12e+08/1.26e+10 =  1% of the original kernel matrix.

torch.Size([70279, 2])
We keep 2.75e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([91243, 2])
We keep 4.71e+07/3.07e+09 =  1% of the original kernel matrix.

torch.Size([52600, 2])
We keep 1.46e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([14128, 2])
We keep 2.04e+06/5.83e+07 =  3% of the original kernel matrix.

torch.Size([20943, 2])
We keep 2.97e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([32219, 2])
We keep 7.42e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([31854, 2])
We keep 6.36e+06/3.69e+08 =  1% of the original kernel matrix.

torch.Size([54202, 2])
We keep 2.92e+07/1.33e+09 =  2% of the original kernel matrix.

torch.Size([40702, 2])
We keep 1.05e+07/6.68e+08 =  1% of the original kernel matrix.

torch.Size([170340, 2])
We keep 4.10e+08/1.25e+10 =  3% of the original kernel matrix.

torch.Size([74397, 2])
We keep 2.76e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([50436, 2])
We keep 2.61e+07/1.09e+09 =  2% of the original kernel matrix.

torch.Size([39539, 2])
We keep 9.59e+06/6.04e+08 =  1% of the original kernel matrix.

torch.Size([71176, 2])
We keep 3.00e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([46232, 2])
We keep 1.23e+07/8.14e+08 =  1% of the original kernel matrix.

torch.Size([19827, 2])
We keep 3.32e+07/2.16e+08 = 15% of the original kernel matrix.

torch.Size([24632, 2])
We keep 4.28e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([49393, 2])
We keep 1.71e+07/9.80e+08 =  1% of the original kernel matrix.

torch.Size([39211, 2])
We keep 9.14e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([10796, 2])
We keep 1.23e+06/2.99e+07 =  4% of the original kernel matrix.

torch.Size([18173, 2])
We keep 2.29e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([15729, 2])
We keep 3.04e+06/8.50e+07 =  3% of the original kernel matrix.

torch.Size([22204, 2])
We keep 3.46e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([322023, 2])
We keep 4.22e+08/4.44e+10 =  0% of the original kernel matrix.

torch.Size([103095, 2])
We keep 4.74e+07/3.86e+09 =  1% of the original kernel matrix.

torch.Size([35957, 2])
We keep 1.74e+07/5.34e+08 =  3% of the original kernel matrix.

torch.Size([33176, 2])
We keep 7.01e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([86258, 2])
We keep 3.91e+07/2.83e+09 =  1% of the original kernel matrix.

torch.Size([50994, 2])
We keep 1.42e+07/9.75e+08 =  1% of the original kernel matrix.

torch.Size([40707, 2])
We keep 1.30e+07/6.50e+08 =  2% of the original kernel matrix.

torch.Size([35872, 2])
We keep 7.67e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([4969, 2])
We keep 2.97e+05/4.80e+06 =  6% of the original kernel matrix.

torch.Size([12887, 2])
We keep 1.19e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([163278, 2])
We keep 1.91e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([71821, 2])
We keep 2.58e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([153427, 2])
We keep 1.25e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([69647, 2])
We keep 2.49e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([72253, 2])
We keep 3.63e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([46543, 2])
We keep 1.24e+07/8.27e+08 =  1% of the original kernel matrix.

torch.Size([7323, 2])
We keep 4.62e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([15249, 2])
We keep 1.52e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([28041, 2])
We keep 5.95e+06/2.78e+08 =  2% of the original kernel matrix.

torch.Size([30385, 2])
We keep 5.54e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([73276, 2])
We keep 5.00e+07/2.00e+09 =  2% of the original kernel matrix.

torch.Size([47026, 2])
We keep 1.24e+07/8.19e+08 =  1% of the original kernel matrix.

torch.Size([48351, 2])
We keep 1.83e+07/9.59e+08 =  1% of the original kernel matrix.

torch.Size([39023, 2])
We keep 9.18e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([73867, 2])
We keep 5.70e+07/2.01e+09 =  2% of the original kernel matrix.

torch.Size([47080, 2])
We keep 1.23e+07/8.20e+08 =  1% of the original kernel matrix.

torch.Size([7468, 2])
We keep 6.04e+05/1.21e+07 =  5% of the original kernel matrix.

torch.Size([15335, 2])
We keep 1.62e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([14878, 2])
We keep 1.63e+07/8.42e+07 = 19% of the original kernel matrix.

torch.Size([21146, 2])
We keep 3.31e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([106086, 2])
We keep 1.19e+08/5.08e+09 =  2% of the original kernel matrix.

torch.Size([56733, 2])
We keep 1.84e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([22580, 2])
We keep 7.62e+06/1.99e+08 =  3% of the original kernel matrix.

torch.Size([26327, 2])
We keep 4.64e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([231263, 2])
We keep 3.12e+08/2.53e+10 =  1% of the original kernel matrix.

torch.Size([88408, 2])
We keep 3.72e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([17804, 2])
We keep 4.36e+06/9.62e+07 =  4% of the original kernel matrix.

torch.Size([23769, 2])
We keep 3.48e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([73508, 2])
We keep 4.16e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([46947, 2])
We keep 1.28e+07/8.48e+08 =  1% of the original kernel matrix.

torch.Size([111791, 2])
We keep 7.68e+07/5.24e+09 =  1% of the original kernel matrix.

torch.Size([59408, 2])
We keep 1.88e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([154655, 2])
We keep 3.30e+08/1.05e+10 =  3% of the original kernel matrix.

torch.Size([70049, 2])
We keep 2.53e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([167596, 2])
We keep 2.36e+08/1.16e+10 =  2% of the original kernel matrix.

torch.Size([73705, 2])
We keep 2.65e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([136956, 2])
We keep 1.02e+08/7.52e+09 =  1% of the original kernel matrix.

torch.Size([65826, 2])
We keep 2.16e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([3587, 2])
We keep 1.33e+05/2.10e+06 =  6% of the original kernel matrix.

torch.Size([11241, 2])
We keep 8.84e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([89825, 2])
We keep 4.60e+07/2.82e+09 =  1% of the original kernel matrix.

torch.Size([52217, 2])
We keep 1.42e+07/9.72e+08 =  1% of the original kernel matrix.

torch.Size([31698, 2])
We keep 7.04e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([31949, 2])
We keep 6.02e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([332697, 2])
We keep 7.39e+08/4.83e+10 =  1% of the original kernel matrix.

torch.Size([103628, 2])
We keep 4.98e+07/4.03e+09 =  1% of the original kernel matrix.

torch.Size([72063, 2])
We keep 4.50e+07/2.19e+09 =  2% of the original kernel matrix.

torch.Size([46327, 2])
We keep 1.26e+07/8.56e+08 =  1% of the original kernel matrix.

torch.Size([145288, 2])
We keep 1.45e+08/9.50e+09 =  1% of the original kernel matrix.

torch.Size([67448, 2])
We keep 2.42e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([60896, 2])
We keep 2.69e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([43135, 2])
We keep 1.14e+07/7.38e+08 =  1% of the original kernel matrix.

torch.Size([26858, 2])
We keep 6.37e+06/2.83e+08 =  2% of the original kernel matrix.

torch.Size([28731, 2])
We keep 5.37e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([34439, 2])
We keep 4.14e+07/1.05e+09 =  3% of the original kernel matrix.

torch.Size([30857, 2])
We keep 9.25e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([10990, 2])
We keep 1.65e+06/3.01e+07 =  5% of the original kernel matrix.

torch.Size([18328, 2])
We keep 2.19e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([41870, 2])
We keep 3.96e+07/8.87e+08 =  4% of the original kernel matrix.

torch.Size([35216, 2])
We keep 8.73e+06/5.45e+08 =  1% of the original kernel matrix.

torch.Size([185226, 2])
We keep 2.69e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([78117, 2])
We keep 2.91e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([152548, 2])
We keep 2.53e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([70447, 2])
We keep 2.64e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([9410, 2])
We keep 1.31e+06/2.56e+07 =  5% of the original kernel matrix.

torch.Size([16995, 2])
We keep 2.15e+06/9.26e+07 =  2% of the original kernel matrix.

torch.Size([28578, 2])
We keep 6.25e+06/2.78e+08 =  2% of the original kernel matrix.

torch.Size([30654, 2])
We keep 5.47e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([22664, 2])
We keep 4.94e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([27198, 2])
We keep 4.61e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([261350, 2])
We keep 5.94e+08/3.66e+10 =  1% of the original kernel matrix.

torch.Size([93172, 2])
We keep 4.35e+07/3.50e+09 =  1% of the original kernel matrix.

torch.Size([104841, 2])
We keep 5.35e+07/3.84e+09 =  1% of the original kernel matrix.

torch.Size([56890, 2])
We keep 1.62e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([15492, 2])
We keep 2.48e+06/7.35e+07 =  3% of the original kernel matrix.

torch.Size([22016, 2])
We keep 3.20e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([118446, 2])
We keep 1.77e+08/8.01e+09 =  2% of the original kernel matrix.

torch.Size([61097, 2])
We keep 2.22e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([106232, 2])
We keep 7.29e+07/4.45e+09 =  1% of the original kernel matrix.

torch.Size([57744, 2])
We keep 1.74e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([16929, 2])
We keep 2.83e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([22887, 2])
We keep 3.55e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([86282, 2])
We keep 8.34e+07/3.50e+09 =  2% of the original kernel matrix.

torch.Size([51345, 2])
We keep 1.58e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([28548, 2])
We keep 2.89e+07/3.77e+08 =  7% of the original kernel matrix.

torch.Size([29774, 2])
We keep 5.90e+06/3.55e+08 =  1% of the original kernel matrix.

time for making ranges is 3.5829994678497314
Sorting X and nu_X
time for sorting X is 0.08381295204162598
Sorting Z and nu_Z
time for sorting Z is 0.0002651214599609375
Starting Optim
sum tnu_Z before tensor(33367860., device='cuda:0')
c= tensor(1974.9509, device='cuda:0')
c= tensor(148600.8906, device='cuda:0')
c= tensor(150406.2500, device='cuda:0')
c= tensor(184395.2188, device='cuda:0')
c= tensor(5065907., device='cuda:0')
c= tensor(6204178.5000, device='cuda:0')
c= tensor(6455609.5000, device='cuda:0')
c= tensor(6707924., device='cuda:0')
c= tensor(6761268., device='cuda:0')
c= tensor(10474562., device='cuda:0')
c= tensor(10494570., device='cuda:0')
c= tensor(11375768., device='cuda:0')
c= tensor(11391508., device='cuda:0')
c= tensor(29366408., device='cuda:0')
c= tensor(29525006., device='cuda:0')
c= tensor(30831248., device='cuda:0')
c= tensor(31325462., device='cuda:0')
c= tensor(31890384., device='cuda:0')
c= tensor(46748028., device='cuda:0')
c= tensor(48248192., device='cuda:0')
c= tensor(50480248., device='cuda:0')
c= tensor(93944800., device='cuda:0')
c= tensor(94005440., device='cuda:0')
c= tensor(94198744., device='cuda:0')
c= tensor(95450392., device='cuda:0')
c= tensor(96058536., device='cuda:0')
c= tensor(96581288., device='cuda:0')
c= tensor(96661720., device='cuda:0')
c= tensor(99494800., device='cuda:0')
c= tensor(9.7350e+08, device='cuda:0')
c= tensor(9.7366e+08, device='cuda:0')
c= tensor(1.0123e+09, device='cuda:0')
c= tensor(1.0124e+09, device='cuda:0')
c= tensor(1.0125e+09, device='cuda:0')
c= tensor(1.0139e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0338e+09, device='cuda:0')
c= tensor(1.0338e+09, device='cuda:0')
c= tensor(1.0399e+09, device='cuda:0')
c= tensor(1.0404e+09, device='cuda:0')
c= tensor(1.0450e+09, device='cuda:0')
c= tensor(1.0579e+09, device='cuda:0')
c= tensor(1.0579e+09, device='cuda:0')
c= tensor(1.0581e+09, device='cuda:0')
c= tensor(1.0721e+09, device='cuda:0')
c= tensor(1.1170e+09, device='cuda:0')
c= tensor(1.1170e+09, device='cuda:0')
c= tensor(1.1175e+09, device='cuda:0')
c= tensor(1.1180e+09, device='cuda:0')
c= tensor(1.1275e+09, device='cuda:0')
c= tensor(1.1342e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1350e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1591e+09, device='cuda:0')
c= tensor(1.1598e+09, device='cuda:0')
c= tensor(1.1599e+09, device='cuda:0')
c= tensor(1.1618e+09, device='cuda:0')
c= tensor(1.1657e+09, device='cuda:0')
c= tensor(1.1825e+09, device='cuda:0')
c= tensor(1.1849e+09, device='cuda:0')
c= tensor(1.1850e+09, device='cuda:0')
c= tensor(1.1856e+09, device='cuda:0')
c= tensor(1.1856e+09, device='cuda:0')
c= tensor(1.1860e+09, device='cuda:0')
c= tensor(1.1872e+09, device='cuda:0')
c= tensor(1.1887e+09, device='cuda:0')
c= tensor(1.1902e+09, device='cuda:0')
c= tensor(1.1902e+09, device='cuda:0')
c= tensor(1.1903e+09, device='cuda:0')
c= tensor(1.1918e+09, device='cuda:0')
c= tensor(1.1954e+09, device='cuda:0')
c= tensor(1.1978e+09, device='cuda:0')
c= tensor(1.1979e+09, device='cuda:0')
c= tensor(1.2105e+09, device='cuda:0')
c= tensor(1.2106e+09, device='cuda:0')
c= tensor(1.2107e+09, device='cuda:0')
c= tensor(1.2118e+09, device='cuda:0')
c= tensor(1.2118e+09, device='cuda:0')
c= tensor(1.2211e+09, device='cuda:0')
c= tensor(1.2272e+09, device='cuda:0')
c= tensor(1.2537e+09, device='cuda:0')
c= tensor(1.2572e+09, device='cuda:0')
c= tensor(1.2584e+09, device='cuda:0')
c= tensor(1.2598e+09, device='cuda:0')
c= tensor(1.2598e+09, device='cuda:0')
c= tensor(1.2634e+09, device='cuda:0')
c= tensor(1.2634e+09, device='cuda:0')
c= tensor(1.2637e+09, device='cuda:0')
c= tensor(1.2671e+09, device='cuda:0')
c= tensor(1.2679e+09, device='cuda:0')
c= tensor(1.2680e+09, device='cuda:0')
c= tensor(1.2680e+09, device='cuda:0')
c= tensor(1.2688e+09, device='cuda:0')
c= tensor(1.2689e+09, device='cuda:0')
c= tensor(1.2708e+09, device='cuda:0')
c= tensor(1.2708e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2775e+09, device='cuda:0')
c= tensor(1.2776e+09, device='cuda:0')
c= tensor(1.2791e+09, device='cuda:0')
c= tensor(1.2792e+09, device='cuda:0')
c= tensor(1.2879e+09, device='cuda:0')
c= tensor(1.2882e+09, device='cuda:0')
c= tensor(1.2882e+09, device='cuda:0')
c= tensor(1.2912e+09, device='cuda:0')
c= tensor(1.2932e+09, device='cuda:0')
c= tensor(1.2937e+09, device='cuda:0')
c= tensor(1.3021e+09, device='cuda:0')
c= tensor(1.3111e+09, device='cuda:0')
c= tensor(1.3317e+09, device='cuda:0')
c= tensor(1.3323e+09, device='cuda:0')
c= tensor(1.3323e+09, device='cuda:0')
c= tensor(1.3324e+09, device='cuda:0')
c= tensor(1.3346e+09, device='cuda:0')
c= tensor(1.3367e+09, device='cuda:0')
c= tensor(1.3405e+09, device='cuda:0')
c= tensor(1.3405e+09, device='cuda:0')
c= tensor(1.3415e+09, device='cuda:0')
c= tensor(1.3422e+09, device='cuda:0')
c= tensor(1.3475e+09, device='cuda:0')
c= tensor(1.3475e+09, device='cuda:0')
c= tensor(1.3493e+09, device='cuda:0')
c= tensor(1.3494e+09, device='cuda:0')
c= tensor(1.3495e+09, device='cuda:0')
c= tensor(1.3496e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3503e+09, device='cuda:0')
c= tensor(1.3507e+09, device='cuda:0')
c= tensor(1.3509e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.4351e+09, device='cuda:0')
c= tensor(1.4360e+09, device='cuda:0')
c= tensor(1.4443e+09, device='cuda:0')
c= tensor(1.4443e+09, device='cuda:0')
c= tensor(1.4461e+09, device='cuda:0')
c= tensor(1.4461e+09, device='cuda:0')
c= tensor(1.4467e+09, device='cuda:0')
c= tensor(1.4467e+09, device='cuda:0')
c= tensor(1.4504e+09, device='cuda:0')
c= tensor(1.4506e+09, device='cuda:0')
c= tensor(1.4507e+09, device='cuda:0')
c= tensor(1.4652e+09, device='cuda:0')
c= tensor(1.4662e+09, device='cuda:0')
c= tensor(1.4670e+09, device='cuda:0')
c= tensor(1.4717e+09, device='cuda:0')
c= tensor(1.4936e+09, device='cuda:0')
c= tensor(1.4937e+09, device='cuda:0')
c= tensor(1.4937e+09, device='cuda:0')
c= tensor(1.4937e+09, device='cuda:0')
c= tensor(1.4937e+09, device='cuda:0')
c= tensor(1.4941e+09, device='cuda:0')
c= tensor(1.4943e+09, device='cuda:0')
c= tensor(1.4943e+09, device='cuda:0')
c= tensor(1.4944e+09, device='cuda:0')
c= tensor(1.4953e+09, device='cuda:0')
c= tensor(1.4953e+09, device='cuda:0')
c= tensor(1.5409e+09, device='cuda:0')
c= tensor(1.5410e+09, device='cuda:0')
c= tensor(1.5428e+09, device='cuda:0')
c= tensor(1.5428e+09, device='cuda:0')
c= tensor(1.5438e+09, device='cuda:0')
c= tensor(1.5621e+09, device='cuda:0')
c= tensor(1.8090e+09, device='cuda:0')
c= tensor(1.9026e+09, device='cuda:0')
c= tensor(1.9030e+09, device='cuda:0')
c= tensor(1.9035e+09, device='cuda:0')
c= tensor(1.9036e+09, device='cuda:0')
c= tensor(1.9039e+09, device='cuda:0')
c= tensor(1.9070e+09, device='cuda:0')
c= tensor(1.9077e+09, device='cuda:0')
c= tensor(1.9077e+09, device='cuda:0')
c= tensor(1.9101e+09, device='cuda:0')
c= tensor(1.9326e+09, device='cuda:0')
c= tensor(1.9335e+09, device='cuda:0')
c= tensor(1.9336e+09, device='cuda:0')
c= tensor(1.9337e+09, device='cuda:0')
c= tensor(1.9338e+09, device='cuda:0')
c= tensor(1.9338e+09, device='cuda:0')
c= tensor(1.9404e+09, device='cuda:0')
c= tensor(1.9406e+09, device='cuda:0')
c= tensor(1.9407e+09, device='cuda:0')
c= tensor(1.9418e+09, device='cuda:0')
c= tensor(1.9418e+09, device='cuda:0')
c= tensor(1.9418e+09, device='cuda:0')
c= tensor(1.9431e+09, device='cuda:0')
c= tensor(1.9445e+09, device='cuda:0')
c= tensor(1.9490e+09, device='cuda:0')
c= tensor(1.9658e+09, device='cuda:0')
c= tensor(1.9702e+09, device='cuda:0')
c= tensor(1.9703e+09, device='cuda:0')
c= tensor(1.9746e+09, device='cuda:0')
c= tensor(1.9792e+09, device='cuda:0')
c= tensor(1.9862e+09, device='cuda:0')
c= tensor(1.9864e+09, device='cuda:0')
c= tensor(2.0190e+09, device='cuda:0')
c= tensor(2.0227e+09, device='cuda:0')
c= tensor(2.0242e+09, device='cuda:0')
c= tensor(2.0252e+09, device='cuda:0')
c= tensor(2.0257e+09, device='cuda:0')
c= tensor(2.0260e+09, device='cuda:0')
c= tensor(2.0260e+09, device='cuda:0')
c= tensor(2.0261e+09, device='cuda:0')
c= tensor(2.0329e+09, device='cuda:0')
c= tensor(2.0369e+09, device='cuda:0')
c= tensor(2.0609e+09, device='cuda:0')
c= tensor(2.0634e+09, device='cuda:0')
c= tensor(2.0658e+09, device='cuda:0')
c= tensor(2.0659e+09, device='cuda:0')
c= tensor(2.0681e+09, device='cuda:0')
c= tensor(2.0681e+09, device='cuda:0')
c= tensor(2.0683e+09, device='cuda:0')
c= tensor(2.0706e+09, device='cuda:0')
c= tensor(2.0722e+09, device='cuda:0')
c= tensor(2.0722e+09, device='cuda:0')
c= tensor(2.0722e+09, device='cuda:0')
c= tensor(2.0885e+09, device='cuda:0')
c= tensor(2.0887e+09, device='cuda:0')
c= tensor(2.0928e+09, device='cuda:0')
c= tensor(2.0930e+09, device='cuda:0')
c= tensor(2.0931e+09, device='cuda:0')
c= tensor(2.0931e+09, device='cuda:0')
c= tensor(2.0931e+09, device='cuda:0')
c= tensor(2.0942e+09, device='cuda:0')
c= tensor(2.0963e+09, device='cuda:0')
c= tensor(2.0963e+09, device='cuda:0')
c= tensor(2.0998e+09, device='cuda:0')
c= tensor(2.1001e+09, device='cuda:0')
c= tensor(2.1003e+09, device='cuda:0')
c= tensor(2.1004e+09, device='cuda:0')
c= tensor(2.1213e+09, device='cuda:0')
c= tensor(2.1213e+09, device='cuda:0')
c= tensor(2.1233e+09, device='cuda:0')
c= tensor(2.1243e+09, device='cuda:0')
c= tensor(2.1253e+09, device='cuda:0')
c= tensor(2.1259e+09, device='cuda:0')
c= tensor(2.2242e+09, device='cuda:0')
c= tensor(2.2246e+09, device='cuda:0')
c= tensor(2.2247e+09, device='cuda:0')
c= tensor(2.2256e+09, device='cuda:0')
c= tensor(2.2258e+09, device='cuda:0')
c= tensor(2.2494e+09, device='cuda:0')
c= tensor(2.2500e+09, device='cuda:0')
c= tensor(2.2561e+09, device='cuda:0')
c= tensor(2.2859e+09, device='cuda:0')
c= tensor(2.2860e+09, device='cuda:0')
c= tensor(2.2950e+09, device='cuda:0')
c= tensor(2.2957e+09, device='cuda:0')
c= tensor(2.3501e+09, device='cuda:0')
c= tensor(2.3503e+09, device='cuda:0')
c= tensor(2.3509e+09, device='cuda:0')
c= tensor(2.3512e+09, device='cuda:0')
c= tensor(2.3512e+09, device='cuda:0')
c= tensor(2.3512e+09, device='cuda:0')
c= tensor(2.3521e+09, device='cuda:0')
c= tensor(2.3524e+09, device='cuda:0')
c= tensor(2.3642e+09, device='cuda:0')
c= tensor(2.3643e+09, device='cuda:0')
c= tensor(2.3643e+09, device='cuda:0')
c= tensor(2.3644e+09, device='cuda:0')
c= tensor(2.3681e+09, device='cuda:0')
c= tensor(2.3811e+09, device='cuda:0')
c= tensor(2.3949e+09, device='cuda:0')
c= tensor(2.3954e+09, device='cuda:0')
c= tensor(2.3954e+09, device='cuda:0')
c= tensor(2.3955e+09, device='cuda:0')
c= tensor(2.3955e+09, device='cuda:0')
c= tensor(2.4914e+09, device='cuda:0')
c= tensor(2.4915e+09, device='cuda:0')
c= tensor(2.4916e+09, device='cuda:0')
c= tensor(2.4934e+09, device='cuda:0')
c= tensor(2.4940e+09, device='cuda:0')
c= tensor(2.4946e+09, device='cuda:0')
c= tensor(2.4947e+09, device='cuda:0')
c= tensor(2.5061e+09, device='cuda:0')
c= tensor(2.5077e+09, device='cuda:0')
c= tensor(2.5079e+09, device='cuda:0')
c= tensor(2.5082e+09, device='cuda:0')
c= tensor(2.5269e+09, device='cuda:0')
c= tensor(2.5286e+09, device='cuda:0')
c= tensor(2.5693e+09, device='cuda:0')
c= tensor(2.5714e+09, device='cuda:0')
c= tensor(2.5714e+09, device='cuda:0')
c= tensor(2.5719e+09, device='cuda:0')
c= tensor(2.5720e+09, device='cuda:0')
c= tensor(2.5721e+09, device='cuda:0')
c= tensor(2.5722e+09, device='cuda:0')
c= tensor(2.5722e+09, device='cuda:0')
c= tensor(2.5746e+09, device='cuda:0')
c= tensor(2.5778e+09, device='cuda:0')
c= tensor(2.5779e+09, device='cuda:0')
c= tensor(2.5784e+09, device='cuda:0')
c= tensor(2.5785e+09, device='cuda:0')
c= tensor(2.5795e+09, device='cuda:0')
c= tensor(2.5796e+09, device='cuda:0')
c= tensor(2.5796e+09, device='cuda:0')
c= tensor(2.5813e+09, device='cuda:0')
c= tensor(2.5814e+09, device='cuda:0')
c= tensor(2.5814e+09, device='cuda:0')
c= tensor(2.5814e+09, device='cuda:0')
c= tensor(2.5815e+09, device='cuda:0')
c= tensor(2.5872e+09, device='cuda:0')
c= tensor(2.5872e+09, device='cuda:0')
c= tensor(2.5872e+09, device='cuda:0')
c= tensor(2.5874e+09, device='cuda:0')
c= tensor(2.5897e+09, device='cuda:0')
c= tensor(2.6040e+09, device='cuda:0')
c= tensor(2.6096e+09, device='cuda:0')
c= tensor(2.6096e+09, device='cuda:0')
c= tensor(2.6276e+09, device='cuda:0')
c= tensor(2.6319e+09, device='cuda:0')
c= tensor(2.6319e+09, device='cuda:0')
c= tensor(2.6320e+09, device='cuda:0')
c= tensor(2.6327e+09, device='cuda:0')
c= tensor(2.6343e+09, device='cuda:0')
c= tensor(2.6366e+09, device='cuda:0')
c= tensor(2.6390e+09, device='cuda:0')
c= tensor(2.6394e+09, device='cuda:0')
c= tensor(2.6399e+09, device='cuda:0')
c= tensor(2.6399e+09, device='cuda:0')
c= tensor(2.6405e+09, device='cuda:0')
c= tensor(2.6410e+09, device='cuda:0')
c= tensor(2.6412e+09, device='cuda:0')
c= tensor(2.6424e+09, device='cuda:0')
c= tensor(2.6585e+09, device='cuda:0')
c= tensor(2.6585e+09, device='cuda:0')
c= tensor(2.6585e+09, device='cuda:0')
c= tensor(2.6585e+09, device='cuda:0')
c= tensor(2.6588e+09, device='cuda:0')
c= tensor(2.6590e+09, device='cuda:0')
c= tensor(2.6592e+09, device='cuda:0')
c= tensor(2.6592e+09, device='cuda:0')
c= tensor(2.6596e+09, device='cuda:0')
c= tensor(2.6597e+09, device='cuda:0')
c= tensor(2.6602e+09, device='cuda:0')
c= tensor(2.6602e+09, device='cuda:0')
c= tensor(2.6602e+09, device='cuda:0')
c= tensor(2.6603e+09, device='cuda:0')
c= tensor(2.6604e+09, device='cuda:0')
c= tensor(2.6606e+09, device='cuda:0')
c= tensor(2.6641e+09, device='cuda:0')
c= tensor(2.6740e+09, device='cuda:0')
c= tensor(2.6772e+09, device='cuda:0')
c= tensor(2.6792e+09, device='cuda:0')
c= tensor(2.6793e+09, device='cuda:0')
c= tensor(2.6794e+09, device='cuda:0')
c= tensor(2.6796e+09, device='cuda:0')
c= tensor(2.6814e+09, device='cuda:0')
c= tensor(2.6815e+09, device='cuda:0')
c= tensor(2.6816e+09, device='cuda:0')
c= tensor(2.6817e+09, device='cuda:0')
c= tensor(2.8297e+09, device='cuda:0')
c= tensor(2.8305e+09, device='cuda:0')
c= tensor(2.8310e+09, device='cuda:0')
c= tensor(2.8419e+09, device='cuda:0')
c= tensor(2.8422e+09, device='cuda:0')
c= tensor(2.8426e+09, device='cuda:0')
c= tensor(2.9005e+09, device='cuda:0')
c= tensor(2.9157e+09, device='cuda:0')
c= tensor(2.9179e+09, device='cuda:0')
c= tensor(2.9184e+09, device='cuda:0')
c= tensor(2.9192e+09, device='cuda:0')
c= tensor(2.9192e+09, device='cuda:0')
c= tensor(2.9257e+09, device='cuda:0')
c= tensor(3.0355e+09, device='cuda:0')
c= tensor(3.0372e+09, device='cuda:0')
c= tensor(3.0385e+09, device='cuda:0')
c= tensor(3.0388e+09, device='cuda:0')
c= tensor(3.0397e+09, device='cuda:0')
c= tensor(3.0511e+09, device='cuda:0')
c= tensor(3.0512e+09, device='cuda:0')
c= tensor(3.0513e+09, device='cuda:0')
c= tensor(3.0806e+09, device='cuda:0')
c= tensor(3.0816e+09, device='cuda:0')
c= tensor(3.1033e+09, device='cuda:0')
c= tensor(3.1053e+09, device='cuda:0')
c= tensor(3.1068e+09, device='cuda:0')
c= tensor(3.1078e+09, device='cuda:0')
c= tensor(3.1114e+09, device='cuda:0')
c= tensor(3.1186e+09, device='cuda:0')
c= tensor(3.1187e+09, device='cuda:0')
c= tensor(3.1945e+09, device='cuda:0')
c= tensor(3.1952e+09, device='cuda:0')
c= tensor(3.1965e+09, device='cuda:0')
c= tensor(3.1965e+09, device='cuda:0')
c= tensor(3.1974e+09, device='cuda:0')
c= tensor(3.1974e+09, device='cuda:0')
c= tensor(3.1974e+09, device='cuda:0')
c= tensor(3.1976e+09, device='cuda:0')
c= tensor(3.2504e+09, device='cuda:0')
c= tensor(3.8869e+09, device='cuda:0')
c= tensor(3.8870e+09, device='cuda:0')
c= tensor(3.9185e+09, device='cuda:0')
c= tensor(3.9185e+09, device='cuda:0')
c= tensor(3.9186e+09, device='cuda:0')
c= tensor(3.9187e+09, device='cuda:0')
c= tensor(3.9206e+09, device='cuda:0')
c= tensor(3.9207e+09, device='cuda:0')
c= tensor(4.1095e+09, device='cuda:0')
c= tensor(4.1095e+09, device='cuda:0')
c= tensor(4.1140e+09, device='cuda:0')
c= tensor(4.1143e+09, device='cuda:0')
c= tensor(4.1161e+09, device='cuda:0')
c= tensor(4.1217e+09, device='cuda:0')
c= tensor(4.1219e+09, device='cuda:0')
c= tensor(4.1220e+09, device='cuda:0')
c= tensor(4.1243e+09, device='cuda:0')
c= tensor(4.1243e+09, device='cuda:0')
c= tensor(4.1243e+09, device='cuda:0')
c= tensor(4.1390e+09, device='cuda:0')
c= tensor(4.1431e+09, device='cuda:0')
c= tensor(4.1437e+09, device='cuda:0')
c= tensor(4.1439e+09, device='cuda:0')
c= tensor(4.1498e+09, device='cuda:0')
c= tensor(4.1656e+09, device='cuda:0')
c= tensor(4.1687e+09, device='cuda:0')
c= tensor(4.1687e+09, device='cuda:0')
c= tensor(4.1769e+09, device='cuda:0')
c= tensor(4.1775e+09, device='cuda:0')
c= tensor(4.1781e+09, device='cuda:0')
c= tensor(4.1807e+09, device='cuda:0')
c= tensor(4.1815e+09, device='cuda:0')
c= tensor(4.1828e+09, device='cuda:0')
c= tensor(4.2362e+09, device='cuda:0')
c= tensor(4.2368e+09, device='cuda:0')
c= tensor(4.2370e+09, device='cuda:0')
c= tensor(4.2370e+09, device='cuda:0')
c= tensor(4.2372e+09, device='cuda:0')
c= tensor(4.2387e+09, device='cuda:0')
c= tensor(4.2441e+09, device='cuda:0')
c= tensor(4.2452e+09, device='cuda:0')
c= tensor(4.2452e+09, device='cuda:0')
c= tensor(4.2454e+09, device='cuda:0')
c= tensor(4.2460e+09, device='cuda:0')
c= tensor(4.2562e+09, device='cuda:0')
c= tensor(4.2566e+09, device='cuda:0')
c= tensor(4.2572e+09, device='cuda:0')
c= tensor(4.2597e+09, device='cuda:0')
c= tensor(4.2601e+09, device='cuda:0')
c= tensor(4.2601e+09, device='cuda:0')
c= tensor(4.2601e+09, device='cuda:0')
c= tensor(4.2738e+09, device='cuda:0')
c= tensor(4.2743e+09, device='cuda:0')
c= tensor(4.2751e+09, device='cuda:0')
c= tensor(4.2754e+09, device='cuda:0')
c= tensor(4.2754e+09, device='cuda:0')
c= tensor(4.2803e+09, device='cuda:0')
c= tensor(4.2833e+09, device='cuda:0')
c= tensor(4.2840e+09, device='cuda:0')
c= tensor(4.2840e+09, device='cuda:0')
c= tensor(4.2841e+09, device='cuda:0')
c= tensor(4.2850e+09, device='cuda:0')
c= tensor(4.2854e+09, device='cuda:0')
c= tensor(4.2871e+09, device='cuda:0')
c= tensor(4.2872e+09, device='cuda:0')
c= tensor(4.2878e+09, device='cuda:0')
c= tensor(4.2906e+09, device='cuda:0')
c= tensor(4.2908e+09, device='cuda:0')
c= tensor(4.3027e+09, device='cuda:0')
c= tensor(4.3028e+09, device='cuda:0')
c= tensor(4.3037e+09, device='cuda:0')
c= tensor(4.3052e+09, device='cuda:0')
c= tensor(4.3122e+09, device='cuda:0')
c= tensor(4.3174e+09, device='cuda:0')
c= tensor(4.3197e+09, device='cuda:0')
c= tensor(4.3197e+09, device='cuda:0')
c= tensor(4.3207e+09, device='cuda:0')
c= tensor(4.3208e+09, device='cuda:0')
c= tensor(4.3381e+09, device='cuda:0')
c= tensor(4.3391e+09, device='cuda:0')
c= tensor(4.3428e+09, device='cuda:0')
c= tensor(4.3434e+09, device='cuda:0')
c= tensor(4.3436e+09, device='cuda:0')
c= tensor(4.3477e+09, device='cuda:0')
c= tensor(4.3477e+09, device='cuda:0')
c= tensor(4.3485e+09, device='cuda:0')
c= tensor(4.3555e+09, device='cuda:0')
c= tensor(4.3619e+09, device='cuda:0')
c= tensor(4.3619e+09, device='cuda:0')
c= tensor(4.3620e+09, device='cuda:0')
c= tensor(4.3621e+09, device='cuda:0')
c= tensor(4.3793e+09, device='cuda:0')
c= tensor(4.3803e+09, device='cuda:0')
c= tensor(4.3803e+09, device='cuda:0')
c= tensor(4.3842e+09, device='cuda:0')
c= tensor(4.3856e+09, device='cuda:0')
c= tensor(4.3856e+09, device='cuda:0')
c= tensor(4.3873e+09, device='cuda:0')
c= tensor(4.3879e+09, device='cuda:0')
memory (bytes)
4770955264
time for making loss 2 is 15.829652070999146
p0 True
it  0 : 2035846144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
4771221504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4771876864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 10% |
error is  64433316000.0
relative error loss 14.6842575
shape of L is 
torch.Size([])
memory (bytes)
4921552896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 11% |
memory (bytes)
4921708544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  64433005000.0
relative error loss 14.684187
shape of L is 
torch.Size([])
memory (bytes)
4927414272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4927578112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  64431047000.0
relative error loss 14.683741
shape of L is 
torch.Size([])
memory (bytes)
4929617920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4929617920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  64410657000.0
relative error loss 14.679093
shape of L is 
torch.Size([])
memory (bytes)
4931764224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
4931788800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  64308564000.0
relative error loss 14.655827
shape of L is 
torch.Size([])
memory (bytes)
4933926912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4933926912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  63195010000.0
relative error loss 14.402049
shape of L is 
torch.Size([])
memory (bytes)
4936044544
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4936065024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  57859730000.0
relative error loss 13.186148
shape of L is 
torch.Size([])
memory (bytes)
4938100736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4938100736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  34821130000.0
relative error loss 7.9356837
shape of L is 
torch.Size([])
memory (bytes)
4940292096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4940292096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  12367928000.0
relative error loss 2.8186326
shape of L is 
torch.Size([])
memory (bytes)
4942319616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4942319616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  7025752000.0
relative error loss 1.6011585
time to take a step is 225.74762845039368
it  1 : 2396816896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4944576512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4944601088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  7025752000.0
relative error loss 1.6011585
shape of L is 
torch.Size([])
memory (bytes)
4946653184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
4946653184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  5130266600.0
relative error loss 1.1691802
shape of L is 
torch.Size([])
memory (bytes)
4948779008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4948901888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4527431700.0
relative error loss 1.031795
shape of L is 
torch.Size([])
memory (bytes)
4951019520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4951044096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  6433122300.0
relative error loss 1.4660991
shape of L is 
torch.Size([])
memory (bytes)
4952997888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
4952997888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 11% |
error is  4258152200.0
relative error loss 0.9704266
shape of L is 
torch.Size([])
memory (bytes)
4955250688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4955275264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  4038444800.0
relative error loss 0.9203556
shape of L is 
torch.Size([])
memory (bytes)
4957335552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4957343744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3796834300.0
relative error loss 0.86529297
shape of L is 
torch.Size([])
memory (bytes)
4959268864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4959268864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3520025300.0
relative error loss 0.8022086
shape of L is 
torch.Size([])
memory (bytes)
4961644544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4961644544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3381076200.0
relative error loss 0.77054226
shape of L is 
torch.Size([])
memory (bytes)
4963708928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
4963708928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% | 11% |
error is  3281452000.0
relative error loss 0.7478381
time to take a step is 216.67139863967896
it  2 : 2499664896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4965707776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4965707776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3281452000.0
relative error loss 0.7478381
shape of L is 
torch.Size([])
memory (bytes)
4967940096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4967968768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2971180500.0
relative error loss 0.67712766
shape of L is 
torch.Size([])
memory (bytes)
4970082304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4970082304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2828171800.0
relative error loss 0.6445362
shape of L is 
torch.Size([])
memory (bytes)
4972146688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
4972146688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2646962700.0
relative error loss 0.6032389
shape of L is 
torch.Size([])
memory (bytes)
4974297088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4974321664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2415665000.0
relative error loss 0.55052644
shape of L is 
torch.Size([])
memory (bytes)
4976422912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4976422912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2263924000.0
relative error loss 0.51594496
shape of L is 
torch.Size([])
memory (bytes)
4978401280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4978401280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2084387300.0
relative error loss 0.47502878
shape of L is 
torch.Size([])
memory (bytes)
4980666368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4980666368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2014742000.0
relative error loss 0.45915672
shape of L is 
torch.Size([])
memory (bytes)
4982726656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
4982726656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1845855700.0
relative error loss 0.4206678
shape of L is 
torch.Size([])
memory (bytes)
4984729600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4984729600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1701311500.0
relative error loss 0.38772637
time to take a step is 216.77137422561646
it  3 : 2498658816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4987047936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4987060224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1701311500.0
relative error loss 0.38772637
shape of L is 
torch.Size([])
memory (bytes)
4989173760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4989173760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1576942600.0
relative error loss 0.3593829
shape of L is 
torch.Size([])
memory (bytes)
4991320064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4991344640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1435470100.0
relative error loss 0.32714152
shape of L is 
torch.Size([])
memory (bytes)
4993462272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
4993462272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1367916000.0
relative error loss 0.31174606
shape of L is 
torch.Size([])
memory (bytes)
4995575808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4995600384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1242782000.0
relative error loss 0.28322816
shape of L is 
torch.Size([])
memory (bytes)
4997705728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4997705728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1178732500.0
relative error loss 0.2686314
shape of L is 
torch.Size([])
memory (bytes)
4999868416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
4999868416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1123552300.0
relative error loss 0.2560559
shape of L is 
torch.Size([])
memory (bytes)
5001793536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
5001793536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1044211460.0
relative error loss 0.23797426
shape of L is 
torch.Size([])
memory (bytes)
5004140544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
5004144640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1000407300.0
relative error loss 0.22799134
shape of L is 
torch.Size([])
memory (bytes)
5006217216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5006217216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1674421000.0
relative error loss 0.3815981
shape of L is 
torch.Size([])
memory (bytes)
5008379904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5008379904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  968178400.0
relative error loss 0.22064644
time to take a step is 237.94251036643982
c= tensor(1974.9509, device='cuda:0')
c= tensor(148600.8906, device='cuda:0')
c= tensor(150406.2500, device='cuda:0')
c= tensor(184395.2188, device='cuda:0')
c= tensor(5065907., device='cuda:0')
c= tensor(6204178.5000, device='cuda:0')
c= tensor(6455609.5000, device='cuda:0')
c= tensor(6707924., device='cuda:0')
c= tensor(6761268., device='cuda:0')
c= tensor(10474562., device='cuda:0')
c= tensor(10494570., device='cuda:0')
c= tensor(11375768., device='cuda:0')
c= tensor(11391508., device='cuda:0')
c= tensor(29366408., device='cuda:0')
c= tensor(29525006., device='cuda:0')
c= tensor(30831248., device='cuda:0')
c= tensor(31325462., device='cuda:0')
c= tensor(31890384., device='cuda:0')
c= tensor(46748028., device='cuda:0')
c= tensor(48248192., device='cuda:0')
c= tensor(50480248., device='cuda:0')
c= tensor(93944800., device='cuda:0')
c= tensor(94005440., device='cuda:0')
c= tensor(94198744., device='cuda:0')
c= tensor(95450392., device='cuda:0')
c= tensor(96058536., device='cuda:0')
c= tensor(96581288., device='cuda:0')
c= tensor(96661720., device='cuda:0')
c= tensor(99494800., device='cuda:0')
c= tensor(9.7350e+08, device='cuda:0')
c= tensor(9.7366e+08, device='cuda:0')
c= tensor(1.0123e+09, device='cuda:0')
c= tensor(1.0124e+09, device='cuda:0')
c= tensor(1.0125e+09, device='cuda:0')
c= tensor(1.0139e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0208e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0210e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0212e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0219e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0221e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0223e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0224e+09, device='cuda:0')
c= tensor(1.0225e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0338e+09, device='cuda:0')
c= tensor(1.0338e+09, device='cuda:0')
c= tensor(1.0399e+09, device='cuda:0')
c= tensor(1.0404e+09, device='cuda:0')
c= tensor(1.0450e+09, device='cuda:0')
c= tensor(1.0579e+09, device='cuda:0')
c= tensor(1.0579e+09, device='cuda:0')
c= tensor(1.0581e+09, device='cuda:0')
c= tensor(1.0721e+09, device='cuda:0')
c= tensor(1.1170e+09, device='cuda:0')
c= tensor(1.1170e+09, device='cuda:0')
c= tensor(1.1175e+09, device='cuda:0')
c= tensor(1.1180e+09, device='cuda:0')
c= tensor(1.1275e+09, device='cuda:0')
c= tensor(1.1342e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1350e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1591e+09, device='cuda:0')
c= tensor(1.1598e+09, device='cuda:0')
c= tensor(1.1599e+09, device='cuda:0')
c= tensor(1.1618e+09, device='cuda:0')
c= tensor(1.1657e+09, device='cuda:0')
c= tensor(1.1825e+09, device='cuda:0')
c= tensor(1.1849e+09, device='cuda:0')
c= tensor(1.1850e+09, device='cuda:0')
c= tensor(1.1856e+09, device='cuda:0')
c= tensor(1.1856e+09, device='cuda:0')
c= tensor(1.1860e+09, device='cuda:0')
c= tensor(1.1872e+09, device='cuda:0')
c= tensor(1.1887e+09, device='cuda:0')
c= tensor(1.1902e+09, device='cuda:0')
c= tensor(1.1902e+09, device='cuda:0')
c= tensor(1.1903e+09, device='cuda:0')
c= tensor(1.1918e+09, device='cuda:0')
c= tensor(1.1954e+09, device='cuda:0')
c= tensor(1.1978e+09, device='cuda:0')
c= tensor(1.1979e+09, device='cuda:0')
c= tensor(1.2105e+09, device='cuda:0')
c= tensor(1.2106e+09, device='cuda:0')
c= tensor(1.2107e+09, device='cuda:0')
c= tensor(1.2118e+09, device='cuda:0')
c= tensor(1.2118e+09, device='cuda:0')
c= tensor(1.2211e+09, device='cuda:0')
c= tensor(1.2272e+09, device='cuda:0')
c= tensor(1.2537e+09, device='cuda:0')
c= tensor(1.2572e+09, device='cuda:0')
c= tensor(1.2584e+09, device='cuda:0')
c= tensor(1.2598e+09, device='cuda:0')
c= tensor(1.2598e+09, device='cuda:0')
c= tensor(1.2634e+09, device='cuda:0')
c= tensor(1.2634e+09, device='cuda:0')
c= tensor(1.2637e+09, device='cuda:0')
c= tensor(1.2671e+09, device='cuda:0')
c= tensor(1.2679e+09, device='cuda:0')
c= tensor(1.2680e+09, device='cuda:0')
c= tensor(1.2680e+09, device='cuda:0')
c= tensor(1.2688e+09, device='cuda:0')
c= tensor(1.2689e+09, device='cuda:0')
c= tensor(1.2708e+09, device='cuda:0')
c= tensor(1.2708e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2775e+09, device='cuda:0')
c= tensor(1.2776e+09, device='cuda:0')
c= tensor(1.2791e+09, device='cuda:0')
c= tensor(1.2792e+09, device='cuda:0')
c= tensor(1.2879e+09, device='cuda:0')
c= tensor(1.2882e+09, device='cuda:0')
c= tensor(1.2882e+09, device='cuda:0')
c= tensor(1.2912e+09, device='cuda:0')
c= tensor(1.2932e+09, device='cuda:0')
c= tensor(1.2937e+09, device='cuda:0')
c= tensor(1.3021e+09, device='cuda:0')
c= tensor(1.3111e+09, device='cuda:0')
c= tensor(1.3317e+09, device='cuda:0')
c= tensor(1.3323e+09, device='cuda:0')
c= tensor(1.3323e+09, device='cuda:0')
c= tensor(1.3324e+09, device='cuda:0')
c= tensor(1.3346e+09, device='cuda:0')
c= tensor(1.3367e+09, device='cuda:0')
c= tensor(1.3405e+09, device='cuda:0')
c= tensor(1.3405e+09, device='cuda:0')
c= tensor(1.3415e+09, device='cuda:0')
c= tensor(1.3422e+09, device='cuda:0')
c= tensor(1.3475e+09, device='cuda:0')
c= tensor(1.3475e+09, device='cuda:0')
c= tensor(1.3493e+09, device='cuda:0')
c= tensor(1.3494e+09, device='cuda:0')
c= tensor(1.3495e+09, device='cuda:0')
c= tensor(1.3496e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3503e+09, device='cuda:0')
c= tensor(1.3507e+09, device='cuda:0')
c= tensor(1.3509e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.4351e+09, device='cuda:0')
c= tensor(1.4360e+09, device='cuda:0')
c= tensor(1.4443e+09, device='cuda:0')
c= tensor(1.4443e+09, device='cuda:0')
c= tensor(1.4461e+09, device='cuda:0')
c= tensor(1.4461e+09, device='cuda:0')
c= tensor(1.4467e+09, device='cuda:0')
c= tensor(1.4467e+09, device='cuda:0')
c= tensor(1.4504e+09, device='cuda:0')
c= tensor(1.4506e+09, device='cuda:0')
c= tensor(1.4507e+09, device='cuda:0')
c= tensor(1.4652e+09, device='cuda:0')
c= tensor(1.4662e+09, device='cuda:0')
c= tensor(1.4670e+09, device='cuda:0')
c= tensor(1.4717e+09, device='cuda:0')
c= tensor(1.4936e+09, device='cuda:0')
c= tensor(1.4937e+09, device='cuda:0')
c= tensor(1.4937e+09, device='cuda:0')
c= tensor(1.4937e+09, device='cuda:0')
c= tensor(1.4937e+09, device='cuda:0')
c= tensor(1.4941e+09, device='cuda:0')
c= tensor(1.4943e+09, device='cuda:0')
c= tensor(1.4943e+09, device='cuda:0')
c= tensor(1.4944e+09, device='cuda:0')
c= tensor(1.4953e+09, device='cuda:0')
c= tensor(1.4953e+09, device='cuda:0')
c= tensor(1.5409e+09, device='cuda:0')
c= tensor(1.5410e+09, device='cuda:0')
c= tensor(1.5428e+09, device='cuda:0')
c= tensor(1.5428e+09, device='cuda:0')
c= tensor(1.5438e+09, device='cuda:0')
c= tensor(1.5621e+09, device='cuda:0')
c= tensor(1.8090e+09, device='cuda:0')
c= tensor(1.9026e+09, device='cuda:0')
c= tensor(1.9030e+09, device='cuda:0')
c= tensor(1.9035e+09, device='cuda:0')
c= tensor(1.9036e+09, device='cuda:0')
c= tensor(1.9039e+09, device='cuda:0')
c= tensor(1.9070e+09, device='cuda:0')
c= tensor(1.9077e+09, device='cuda:0')
c= tensor(1.9077e+09, device='cuda:0')
c= tensor(1.9101e+09, device='cuda:0')
c= tensor(1.9326e+09, device='cuda:0')
c= tensor(1.9335e+09, device='cuda:0')
c= tensor(1.9336e+09, device='cuda:0')
c= tensor(1.9337e+09, device='cuda:0')
c= tensor(1.9338e+09, device='cuda:0')
c= tensor(1.9338e+09, device='cuda:0')
c= tensor(1.9404e+09, device='cuda:0')
c= tensor(1.9406e+09, device='cuda:0')
c= tensor(1.9407e+09, device='cuda:0')
c= tensor(1.9418e+09, device='cuda:0')
c= tensor(1.9418e+09, device='cuda:0')
c= tensor(1.9418e+09, device='cuda:0')
c= tensor(1.9431e+09, device='cuda:0')
c= tensor(1.9445e+09, device='cuda:0')
c= tensor(1.9490e+09, device='cuda:0')
c= tensor(1.9658e+09, device='cuda:0')
c= tensor(1.9702e+09, device='cuda:0')
c= tensor(1.9703e+09, device='cuda:0')
c= tensor(1.9746e+09, device='cuda:0')
c= tensor(1.9792e+09, device='cuda:0')
c= tensor(1.9862e+09, device='cuda:0')
c= tensor(1.9864e+09, device='cuda:0')
c= tensor(2.0190e+09, device='cuda:0')
c= tensor(2.0227e+09, device='cuda:0')
c= tensor(2.0242e+09, device='cuda:0')
c= tensor(2.0252e+09, device='cuda:0')
c= tensor(2.0257e+09, device='cuda:0')
c= tensor(2.0260e+09, device='cuda:0')
c= tensor(2.0260e+09, device='cuda:0')
c= tensor(2.0261e+09, device='cuda:0')
c= tensor(2.0329e+09, device='cuda:0')
c= tensor(2.0369e+09, device='cuda:0')
c= tensor(2.0609e+09, device='cuda:0')
c= tensor(2.0634e+09, device='cuda:0')
c= tensor(2.0658e+09, device='cuda:0')
c= tensor(2.0659e+09, device='cuda:0')
c= tensor(2.0681e+09, device='cuda:0')
c= tensor(2.0681e+09, device='cuda:0')
c= tensor(2.0683e+09, device='cuda:0')
c= tensor(2.0706e+09, device='cuda:0')
c= tensor(2.0722e+09, device='cuda:0')
c= tensor(2.0722e+09, device='cuda:0')
c= tensor(2.0722e+09, device='cuda:0')
c= tensor(2.0885e+09, device='cuda:0')
c= tensor(2.0887e+09, device='cuda:0')
c= tensor(2.0928e+09, device='cuda:0')
c= tensor(2.0930e+09, device='cuda:0')
c= tensor(2.0931e+09, device='cuda:0')
c= tensor(2.0931e+09, device='cuda:0')
c= tensor(2.0931e+09, device='cuda:0')
c= tensor(2.0942e+09, device='cuda:0')
c= tensor(2.0963e+09, device='cuda:0')
c= tensor(2.0963e+09, device='cuda:0')
c= tensor(2.0998e+09, device='cuda:0')
c= tensor(2.1001e+09, device='cuda:0')
c= tensor(2.1003e+09, device='cuda:0')
c= tensor(2.1004e+09, device='cuda:0')
c= tensor(2.1213e+09, device='cuda:0')
c= tensor(2.1213e+09, device='cuda:0')
c= tensor(2.1233e+09, device='cuda:0')
c= tensor(2.1243e+09, device='cuda:0')
c= tensor(2.1253e+09, device='cuda:0')
c= tensor(2.1259e+09, device='cuda:0')
c= tensor(2.2242e+09, device='cuda:0')
c= tensor(2.2246e+09, device='cuda:0')
c= tensor(2.2247e+09, device='cuda:0')
c= tensor(2.2256e+09, device='cuda:0')
c= tensor(2.2258e+09, device='cuda:0')
c= tensor(2.2494e+09, device='cuda:0')
c= tensor(2.2500e+09, device='cuda:0')
c= tensor(2.2561e+09, device='cuda:0')
c= tensor(2.2859e+09, device='cuda:0')
c= tensor(2.2860e+09, device='cuda:0')
c= tensor(2.2950e+09, device='cuda:0')
c= tensor(2.2957e+09, device='cuda:0')
c= tensor(2.3501e+09, device='cuda:0')
c= tensor(2.3503e+09, device='cuda:0')
c= tensor(2.3509e+09, device='cuda:0')
c= tensor(2.3512e+09, device='cuda:0')
c= tensor(2.3512e+09, device='cuda:0')
c= tensor(2.3512e+09, device='cuda:0')
c= tensor(2.3521e+09, device='cuda:0')
c= tensor(2.3524e+09, device='cuda:0')
c= tensor(2.3642e+09, device='cuda:0')
c= tensor(2.3643e+09, device='cuda:0')
c= tensor(2.3643e+09, device='cuda:0')
c= tensor(2.3644e+09, device='cuda:0')
c= tensor(2.3681e+09, device='cuda:0')
c= tensor(2.3811e+09, device='cuda:0')
c= tensor(2.3949e+09, device='cuda:0')
c= tensor(2.3954e+09, device='cuda:0')
c= tensor(2.3954e+09, device='cuda:0')
c= tensor(2.3955e+09, device='cuda:0')
c= tensor(2.3955e+09, device='cuda:0')
c= tensor(2.4914e+09, device='cuda:0')
c= tensor(2.4915e+09, device='cuda:0')
c= tensor(2.4916e+09, device='cuda:0')
c= tensor(2.4934e+09, device='cuda:0')
c= tensor(2.4940e+09, device='cuda:0')
c= tensor(2.4946e+09, device='cuda:0')
c= tensor(2.4947e+09, device='cuda:0')
c= tensor(2.5061e+09, device='cuda:0')
c= tensor(2.5077e+09, device='cuda:0')
c= tensor(2.5079e+09, device='cuda:0')
c= tensor(2.5082e+09, device='cuda:0')
c= tensor(2.5269e+09, device='cuda:0')
c= tensor(2.5286e+09, device='cuda:0')
c= tensor(2.5693e+09, device='cuda:0')
c= tensor(2.5714e+09, device='cuda:0')
c= tensor(2.5714e+09, device='cuda:0')
c= tensor(2.5719e+09, device='cuda:0')
c= tensor(2.5720e+09, device='cuda:0')
c= tensor(2.5721e+09, device='cuda:0')
c= tensor(2.5722e+09, device='cuda:0')
c= tensor(2.5722e+09, device='cuda:0')
c= tensor(2.5746e+09, device='cuda:0')
c= tensor(2.5778e+09, device='cuda:0')
c= tensor(2.5779e+09, device='cuda:0')
c= tensor(2.5784e+09, device='cuda:0')
c= tensor(2.5785e+09, device='cuda:0')
c= tensor(2.5795e+09, device='cuda:0')
c= tensor(2.5796e+09, device='cuda:0')
c= tensor(2.5796e+09, device='cuda:0')
c= tensor(2.5813e+09, device='cuda:0')
c= tensor(2.5814e+09, device='cuda:0')
c= tensor(2.5814e+09, device='cuda:0')
c= tensor(2.5814e+09, device='cuda:0')
c= tensor(2.5815e+09, device='cuda:0')
c= tensor(2.5872e+09, device='cuda:0')
c= tensor(2.5872e+09, device='cuda:0')
c= tensor(2.5872e+09, device='cuda:0')
c= tensor(2.5874e+09, device='cuda:0')
c= tensor(2.5897e+09, device='cuda:0')
c= tensor(2.6040e+09, device='cuda:0')
c= tensor(2.6096e+09, device='cuda:0')
c= tensor(2.6096e+09, device='cuda:0')
c= tensor(2.6276e+09, device='cuda:0')
c= tensor(2.6319e+09, device='cuda:0')
c= tensor(2.6319e+09, device='cuda:0')
c= tensor(2.6320e+09, device='cuda:0')
c= tensor(2.6327e+09, device='cuda:0')
c= tensor(2.6343e+09, device='cuda:0')
c= tensor(2.6366e+09, device='cuda:0')
c= tensor(2.6390e+09, device='cuda:0')
c= tensor(2.6394e+09, device='cuda:0')
c= tensor(2.6399e+09, device='cuda:0')
c= tensor(2.6399e+09, device='cuda:0')
c= tensor(2.6405e+09, device='cuda:0')
c= tensor(2.6410e+09, device='cuda:0')
c= tensor(2.6412e+09, device='cuda:0')
c= tensor(2.6424e+09, device='cuda:0')
c= tensor(2.6585e+09, device='cuda:0')
c= tensor(2.6585e+09, device='cuda:0')
c= tensor(2.6585e+09, device='cuda:0')
c= tensor(2.6585e+09, device='cuda:0')
c= tensor(2.6588e+09, device='cuda:0')
c= tensor(2.6590e+09, device='cuda:0')
c= tensor(2.6592e+09, device='cuda:0')
c= tensor(2.6592e+09, device='cuda:0')
c= tensor(2.6596e+09, device='cuda:0')
c= tensor(2.6597e+09, device='cuda:0')
c= tensor(2.6602e+09, device='cuda:0')
c= tensor(2.6602e+09, device='cuda:0')
c= tensor(2.6602e+09, device='cuda:0')
c= tensor(2.6603e+09, device='cuda:0')
c= tensor(2.6604e+09, device='cuda:0')
c= tensor(2.6606e+09, device='cuda:0')
c= tensor(2.6641e+09, device='cuda:0')
c= tensor(2.6740e+09, device='cuda:0')
c= tensor(2.6772e+09, device='cuda:0')
c= tensor(2.6792e+09, device='cuda:0')
c= tensor(2.6793e+09, device='cuda:0')
c= tensor(2.6794e+09, device='cuda:0')
c= tensor(2.6796e+09, device='cuda:0')
c= tensor(2.6814e+09, device='cuda:0')
c= tensor(2.6815e+09, device='cuda:0')
c= tensor(2.6816e+09, device='cuda:0')
c= tensor(2.6817e+09, device='cuda:0')
c= tensor(2.8297e+09, device='cuda:0')
c= tensor(2.8305e+09, device='cuda:0')
c= tensor(2.8310e+09, device='cuda:0')
c= tensor(2.8419e+09, device='cuda:0')
c= tensor(2.8422e+09, device='cuda:0')
c= tensor(2.8426e+09, device='cuda:0')
c= tensor(2.9005e+09, device='cuda:0')
c= tensor(2.9157e+09, device='cuda:0')
c= tensor(2.9179e+09, device='cuda:0')
c= tensor(2.9184e+09, device='cuda:0')
c= tensor(2.9192e+09, device='cuda:0')
c= tensor(2.9192e+09, device='cuda:0')
c= tensor(2.9257e+09, device='cuda:0')
c= tensor(3.0355e+09, device='cuda:0')
c= tensor(3.0372e+09, device='cuda:0')
c= tensor(3.0385e+09, device='cuda:0')
c= tensor(3.0388e+09, device='cuda:0')
c= tensor(3.0397e+09, device='cuda:0')
c= tensor(3.0511e+09, device='cuda:0')
c= tensor(3.0512e+09, device='cuda:0')
c= tensor(3.0513e+09, device='cuda:0')
c= tensor(3.0806e+09, device='cuda:0')
c= tensor(3.0816e+09, device='cuda:0')
c= tensor(3.1033e+09, device='cuda:0')
c= tensor(3.1053e+09, device='cuda:0')
c= tensor(3.1068e+09, device='cuda:0')
c= tensor(3.1078e+09, device='cuda:0')
c= tensor(3.1114e+09, device='cuda:0')
c= tensor(3.1186e+09, device='cuda:0')
c= tensor(3.1187e+09, device='cuda:0')
c= tensor(3.1945e+09, device='cuda:0')
c= tensor(3.1952e+09, device='cuda:0')
c= tensor(3.1965e+09, device='cuda:0')
c= tensor(3.1965e+09, device='cuda:0')
c= tensor(3.1974e+09, device='cuda:0')
c= tensor(3.1974e+09, device='cuda:0')
c= tensor(3.1974e+09, device='cuda:0')
c= tensor(3.1976e+09, device='cuda:0')
c= tensor(3.2504e+09, device='cuda:0')
c= tensor(3.8869e+09, device='cuda:0')
c= tensor(3.8870e+09, device='cuda:0')
c= tensor(3.9185e+09, device='cuda:0')
c= tensor(3.9185e+09, device='cuda:0')
c= tensor(3.9186e+09, device='cuda:0')
c= tensor(3.9187e+09, device='cuda:0')
c= tensor(3.9206e+09, device='cuda:0')
c= tensor(3.9207e+09, device='cuda:0')
c= tensor(4.1095e+09, device='cuda:0')
c= tensor(4.1095e+09, device='cuda:0')
c= tensor(4.1140e+09, device='cuda:0')
c= tensor(4.1143e+09, device='cuda:0')
c= tensor(4.1161e+09, device='cuda:0')
c= tensor(4.1217e+09, device='cuda:0')
c= tensor(4.1219e+09, device='cuda:0')
c= tensor(4.1220e+09, device='cuda:0')
c= tensor(4.1243e+09, device='cuda:0')
c= tensor(4.1243e+09, device='cuda:0')
c= tensor(4.1243e+09, device='cuda:0')
c= tensor(4.1390e+09, device='cuda:0')
c= tensor(4.1431e+09, device='cuda:0')
c= tensor(4.1437e+09, device='cuda:0')
c= tensor(4.1439e+09, device='cuda:0')
c= tensor(4.1498e+09, device='cuda:0')
c= tensor(4.1656e+09, device='cuda:0')
c= tensor(4.1687e+09, device='cuda:0')
c= tensor(4.1687e+09, device='cuda:0')
c= tensor(4.1769e+09, device='cuda:0')
c= tensor(4.1775e+09, device='cuda:0')
c= tensor(4.1781e+09, device='cuda:0')
c= tensor(4.1807e+09, device='cuda:0')
c= tensor(4.1815e+09, device='cuda:0')
c= tensor(4.1828e+09, device='cuda:0')
c= tensor(4.2362e+09, device='cuda:0')
c= tensor(4.2368e+09, device='cuda:0')
c= tensor(4.2370e+09, device='cuda:0')
c= tensor(4.2370e+09, device='cuda:0')
c= tensor(4.2372e+09, device='cuda:0')
c= tensor(4.2387e+09, device='cuda:0')
c= tensor(4.2441e+09, device='cuda:0')
c= tensor(4.2452e+09, device='cuda:0')
c= tensor(4.2452e+09, device='cuda:0')
c= tensor(4.2454e+09, device='cuda:0')
c= tensor(4.2460e+09, device='cuda:0')
c= tensor(4.2562e+09, device='cuda:0')
c= tensor(4.2566e+09, device='cuda:0')
c= tensor(4.2572e+09, device='cuda:0')
c= tensor(4.2597e+09, device='cuda:0')
c= tensor(4.2601e+09, device='cuda:0')
c= tensor(4.2601e+09, device='cuda:0')
c= tensor(4.2601e+09, device='cuda:0')
c= tensor(4.2738e+09, device='cuda:0')
c= tensor(4.2743e+09, device='cuda:0')
c= tensor(4.2751e+09, device='cuda:0')
c= tensor(4.2754e+09, device='cuda:0')
c= tensor(4.2754e+09, device='cuda:0')
c= tensor(4.2803e+09, device='cuda:0')
c= tensor(4.2833e+09, device='cuda:0')
c= tensor(4.2840e+09, device='cuda:0')
c= tensor(4.2840e+09, device='cuda:0')
c= tensor(4.2841e+09, device='cuda:0')
c= tensor(4.2850e+09, device='cuda:0')
c= tensor(4.2854e+09, device='cuda:0')
c= tensor(4.2871e+09, device='cuda:0')
c= tensor(4.2872e+09, device='cuda:0')
c= tensor(4.2878e+09, device='cuda:0')
c= tensor(4.2906e+09, device='cuda:0')
c= tensor(4.2908e+09, device='cuda:0')
c= tensor(4.3027e+09, device='cuda:0')
c= tensor(4.3028e+09, device='cuda:0')
c= tensor(4.3037e+09, device='cuda:0')
c= tensor(4.3052e+09, device='cuda:0')
c= tensor(4.3122e+09, device='cuda:0')
c= tensor(4.3174e+09, device='cuda:0')
c= tensor(4.3197e+09, device='cuda:0')
c= tensor(4.3197e+09, device='cuda:0')
c= tensor(4.3207e+09, device='cuda:0')
c= tensor(4.3208e+09, device='cuda:0')
c= tensor(4.3381e+09, device='cuda:0')
c= tensor(4.3391e+09, device='cuda:0')
c= tensor(4.3428e+09, device='cuda:0')
c= tensor(4.3434e+09, device='cuda:0')
c= tensor(4.3436e+09, device='cuda:0')
c= tensor(4.3477e+09, device='cuda:0')
c= tensor(4.3477e+09, device='cuda:0')
c= tensor(4.3485e+09, device='cuda:0')
c= tensor(4.3555e+09, device='cuda:0')
c= tensor(4.3619e+09, device='cuda:0')
c= tensor(4.3619e+09, device='cuda:0')
c= tensor(4.3620e+09, device='cuda:0')
c= tensor(4.3621e+09, device='cuda:0')
c= tensor(4.3793e+09, device='cuda:0')
c= tensor(4.3803e+09, device='cuda:0')
c= tensor(4.3803e+09, device='cuda:0')
c= tensor(4.3842e+09, device='cuda:0')
c= tensor(4.3856e+09, device='cuda:0')
c= tensor(4.3856e+09, device='cuda:0')
c= tensor(4.3873e+09, device='cuda:0')
c= tensor(4.3879e+09, device='cuda:0')
time to make c is 9.759129285812378
time for making loss is 9.759142398834229
p0 True
it  0 : 2036066304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5010391040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5010874368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  968178400.0
relative error loss 0.22064644
shape of L is 
torch.Size([])
memory (bytes)
5037170688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5037170688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  960667900.0
relative error loss 0.2189348
shape of L is 
torch.Size([])
memory (bytes)
5040689152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5040726016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  937999900.0
relative error loss 0.21376878
shape of L is 
torch.Size([])
memory (bytes)
5043818496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5043818496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  915615200.0
relative error loss 0.20866737
shape of L is 
torch.Size([])
memory (bytes)
5047173120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5047173120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  910817300.0
relative error loss 0.20757392
shape of L is 
torch.Size([])
memory (bytes)
5050204160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5050204160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  898169100.0
relative error loss 0.20469141
shape of L is 
torch.Size([])
memory (bytes)
5053308928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5053579264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  894373100.0
relative error loss 0.20382631
shape of L is 
torch.Size([])
memory (bytes)
5056782336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5056782336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 11% |
error is  890596100.0
relative error loss 0.20296554
shape of L is 
torch.Size([])
memory (bytes)
5059788800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5059821568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  883569150.0
relative error loss 0.2013641
shape of L is 
torch.Size([])
memory (bytes)
5063188480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
5063188480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  881851400.0
relative error loss 0.20097263
time to take a step is 274.7031960487366
it  1 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5066317824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5066317824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  881851400.0
relative error loss 0.20097263
shape of L is 
torch.Size([])
memory (bytes)
5069557760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5069594624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  876933600.0
relative error loss 0.19985189
shape of L is 
torch.Size([])
memory (bytes)
5072621568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5072621568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  873444100.0
relative error loss 0.19905663
shape of L is 
torch.Size([])
memory (bytes)
5075931136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5075996672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  871214850.0
relative error loss 0.19854859
shape of L is 
torch.Size([])
memory (bytes)
5079166976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5079203840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  866849000.0
relative error loss 0.19755362
shape of L is 
torch.Size([])
memory (bytes)
5082386432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5082386432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  865144060.0
relative error loss 0.19716506
shape of L is 
torch.Size([])
memory (bytes)
5085614080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5085614080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  863371260.0
relative error loss 0.19676104
shape of L is 
torch.Size([])
memory (bytes)
5088714752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5088714752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  861938940.0
relative error loss 0.19643462
shape of L is 
torch.Size([])
memory (bytes)
5091991552
| ID | GPU | MEM |
------------------
|  0 | 25% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5092028416
| ID | GPU | MEM |
------------------
|  0 | 22% |  0% |
|  1 | 94% | 11% |
error is  859954400.0
relative error loss 0.19598235
shape of L is 
torch.Size([])
memory (bytes)
5095243776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5095243776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  858942700.0
relative error loss 0.19575179
time to take a step is 260.8636794090271
it  2 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5098377216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5098442752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 11% |
error is  858942700.0
relative error loss 0.19575179
shape of L is 
torch.Size([])
memory (bytes)
5101666304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5101666304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  857282050.0
relative error loss 0.19537331
shape of L is 
torch.Size([])
memory (bytes)
5104734208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5104734208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  856335100.0
relative error loss 0.19515751
shape of L is 
torch.Size([])
memory (bytes)
5108076544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5108076544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  855449340.0
relative error loss 0.19495565
shape of L is 
torch.Size([])
memory (bytes)
5111193600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5111193600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  853838600.0
relative error loss 0.19458856
shape of L is 
torch.Size([])
memory (bytes)
5114482688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5114482688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  854729500.0
relative error loss 0.19479159
shape of L is 
torch.Size([])
memory (bytes)
5117526016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5117689856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  852838400.0
relative error loss 0.19436061
shape of L is 
torch.Size([])
memory (bytes)
5120647168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5120897024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  851729400.0
relative error loss 0.19410788
shape of L is 
torch.Size([])
memory (bytes)
5124063232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5124091904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  850606100.0
relative error loss 0.19385187
shape of L is 
torch.Size([])
memory (bytes)
5127307264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5127307264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 11% |
error is  850368000.0
relative error loss 0.19379762
time to take a step is 269.47640585899353
it  3 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5130506240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5130506240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  850368000.0
relative error loss 0.19379762
shape of L is 
torch.Size([])
memory (bytes)
5133639680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5133709312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  849431550.0
relative error loss 0.1935842
shape of L is 
torch.Size([])
memory (bytes)
5136855040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5136920576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  849045760.0
relative error loss 0.19349627
shape of L is 
torch.Size([])
memory (bytes)
5140086784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5140086784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  848679700.0
relative error loss 0.19341284
shape of L is 
torch.Size([])
memory (bytes)
5143265280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5143334912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  847973600.0
relative error loss 0.19325194
shape of L is 
torch.Size([])
memory (bytes)
5146419200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5146419200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 11% |
error is  848477950.0
relative error loss 0.19336687
shape of L is 
torch.Size([])
memory (bytes)
5149741056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5149745152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  847578900.0
relative error loss 0.19316198
shape of L is 
torch.Size([])
memory (bytes)
5152784384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5152956416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  847050750.0
relative error loss 0.19304162
shape of L is 
torch.Size([])
memory (bytes)
5156163584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5156163584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  846627600.0
relative error loss 0.19294518
shape of L is 
torch.Size([])
memory (bytes)
5159251968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5159374848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  846339100.0
relative error loss 0.19287942
time to take a step is 258.0686550140381
it  4 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5162418176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5162418176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  846339100.0
relative error loss 0.19287942
shape of L is 
torch.Size([])
memory (bytes)
5165772800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5165772800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  845905150.0
relative error loss 0.19278054
shape of L is 
torch.Size([])
memory (bytes)
5168795648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5168992256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  845704700.0
relative error loss 0.19273485
shape of L is 
torch.Size([])
memory (bytes)
5172195328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5172195328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  845352960.0
relative error loss 0.1926547
shape of L is 
torch.Size([])
memory (bytes)
5175341056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5175341056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  845236000.0
relative error loss 0.19262803
shape of L is 
torch.Size([])
memory (bytes)
5178568704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5178605568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  844714500.0
relative error loss 0.19250919
shape of L is 
torch.Size([])
memory (bytes)
5181767680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5181767680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  844415200.0
relative error loss 0.19244099
shape of L is 
torch.Size([])
memory (bytes)
5185019904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5185019904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  844112900.0
relative error loss 0.19237208
shape of L is 
torch.Size([])
memory (bytes)
5188173824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5188173824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  843652100.0
relative error loss 0.19226708
shape of L is 
torch.Size([])
memory (bytes)
5191360512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5191360512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  843664400.0
relative error loss 0.19226986
shape of L is 
torch.Size([])
memory (bytes)
5194575872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5194637312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  843288060.0
relative error loss 0.1921841
time to take a step is 284.1084816455841
it  5 : 2500856320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5197676544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
5197676544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  843288060.0
relative error loss 0.1921841
shape of L is 
torch.Size([])
memory (bytes)
5201051648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5201055744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  842843650.0
relative error loss 0.19208282
shape of L is 
torch.Size([])
memory (bytes)
5204246528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5204246528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  842537200.0
relative error loss 0.192013
shape of L is 
torch.Size([])
memory (bytes)
5207404544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5207404544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  841974300.0
relative error loss 0.1918847
shape of L is 
torch.Size([])
memory (bytes)
5210628096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5210664960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  842225660.0
relative error loss 0.19194199
shape of L is 
torch.Size([])
memory (bytes)
5213794304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5213794304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  841754100.0
relative error loss 0.19183452
shape of L is 
torch.Size([])
memory (bytes)
5217087488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5217087488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  841494500.0
relative error loss 0.19177537
shape of L is 
torch.Size([])
memory (bytes)
5220229120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5220229120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  841208060.0
relative error loss 0.19171007
shape of L is 
torch.Size([])
memory (bytes)
5223493632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5223497728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  841139460.0
relative error loss 0.19169444
shape of L is 
torch.Size([])
memory (bytes)
5226631168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5226696704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  840917500.0
relative error loss 0.19164386
time to take a step is 258.75076508522034
it  6 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5229801472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5229891584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  840917500.0
relative error loss 0.19164386
shape of L is 
torch.Size([])
memory (bytes)
5233106944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5233106944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  840825340.0
relative error loss 0.19162285
shape of L is 
torch.Size([])
memory (bytes)
5236264960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5236305920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  840725000.0
relative error loss 0.19159998
shape of L is 
torch.Size([])
memory (bytes)
5239472128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5239508992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  840543200.0
relative error loss 0.19155857
shape of L is 
torch.Size([])
memory (bytes)
5242679296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5242716160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  840783600.0
relative error loss 0.19161335
shape of L is 
torch.Size([])
memory (bytes)
5245865984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5245931520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  840441340.0
relative error loss 0.19153534
shape of L is 
torch.Size([])
memory (bytes)
5249126400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5249126400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  840270600.0
relative error loss 0.19149643
shape of L is 
torch.Size([])
memory (bytes)
5252296704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5252296704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  840127500.0
relative error loss 0.19146381
shape of L is 
torch.Size([])
memory (bytes)
5255540736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5255540736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  839883260.0
relative error loss 0.19140816
shape of L is 
torch.Size([])
memory (bytes)
5258690560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5258690560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  839717900.0
relative error loss 0.19137047
time to take a step is 331.05930757522583
it  7 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5261926400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5261967360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  839717900.0
relative error loss 0.19137047
shape of L is 
torch.Size([])
memory (bytes)
5265080320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5265080320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  839540500.0
relative error loss 0.19133003
shape of L is 
torch.Size([])
memory (bytes)
5268312064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5268377600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  839312400.0
relative error loss 0.19127806
shape of L is 
torch.Size([])
memory (bytes)
5271584768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5271584768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  839138560.0
relative error loss 0.19123843
shape of L is 
torch.Size([])
memory (bytes)
5274734592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5274800128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  838860300.0
relative error loss 0.19117503
shape of L is 
torch.Size([])
memory (bytes)
5278015488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5278015488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  838669600.0
relative error loss 0.19113156
shape of L is 
torch.Size([])
memory (bytes)
5281144832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5281144832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  838392800.0
relative error loss 0.19106849
shape of L is 
torch.Size([])
memory (bytes)
5284409344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5284409344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  838288640.0
relative error loss 0.19104475
shape of L is 
torch.Size([])
memory (bytes)
5287460864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5287616512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  838192900.0
relative error loss 0.19102292
shape of L is 
torch.Size([])
memory (bytes)
5290795008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5290835968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  838040060.0
relative error loss 0.1909881
time to take a step is 338.2780969142914
it  8 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5294047232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5294047232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  838040060.0
relative error loss 0.1909881
shape of L is 
torch.Size([])
memory (bytes)
5297197056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5297238016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  837841660.0
relative error loss 0.19094288
shape of L is 
torch.Size([])
memory (bytes)
5300453376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5300453376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 11% |
error is  837738240.0
relative error loss 0.19091931
shape of L is 
torch.Size([])
memory (bytes)
5303623680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
5303656448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  837650200.0
relative error loss 0.19089924
shape of L is 
torch.Size([])
memory (bytes)
5306859520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5306859520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  837548000.0
relative error loss 0.19087596
shape of L is 
torch.Size([])
memory (bytes)
5310066688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5310066688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  837400600.0
relative error loss 0.19084236
shape of L is 
torch.Size([])
memory (bytes)
5313212416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5313277952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  837209340.0
relative error loss 0.19079877
shape of L is 
torch.Size([])
memory (bytes)
5316481024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5316481024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  837200640.0
relative error loss 0.19079679
shape of L is 
torch.Size([])
memory (bytes)
5319688192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5319688192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  837104640.0
relative error loss 0.19077492
shape of L is 
torch.Size([])
memory (bytes)
5322899456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5322899456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  836942340.0
relative error loss 0.19073792
time to take a step is 338.7727527618408
it  9 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5326057472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5326057472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  836942340.0
relative error loss 0.19073792
shape of L is 
torch.Size([])
memory (bytes)
5329305600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5329305600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  836796700.0
relative error loss 0.19070473
shape of L is 
torch.Size([])
memory (bytes)
5332504576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5332504576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  836662800.0
relative error loss 0.19067422
shape of L is 
torch.Size([])
memory (bytes)
5335642112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5335711744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  836527900.0
relative error loss 0.19064347
shape of L is 
torch.Size([])
memory (bytes)
5338914816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5338914816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  836438000.0
relative error loss 0.19062299
shape of L is 
torch.Size([])
memory (bytes)
5341949952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5342113792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  836350700.0
relative error loss 0.19060309
shape of L is 
torch.Size([])
memory (bytes)
5345292288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5345329152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  836286700.0
relative error loss 0.1905885
shape of L is 
torch.Size([])
memory (bytes)
5348515840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5348515840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  836247040.0
relative error loss 0.19057947
shape of L is 
torch.Size([])
memory (bytes)
5351714816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5351751680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  836219900.0
relative error loss 0.19057329
shape of L is 
torch.Size([])
memory (bytes)
5354958848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5354958848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  836193800.0
relative error loss 0.19056733
time to take a step is 339.251273393631
it  10 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5358125056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5358161920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  836193800.0
relative error loss 0.19056733
shape of L is 
torch.Size([])
memory (bytes)
5361364992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
5361364992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  836149760.0
relative error loss 0.1905573
shape of L is 
torch.Size([])
memory (bytes)
5364445184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5364445184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 11% |
error is  836101100.0
relative error loss 0.19054621
shape of L is 
torch.Size([])
memory (bytes)
5367742464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5367779328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  836062200.0
relative error loss 0.19053735
shape of L is 
torch.Size([])
memory (bytes)
5370904576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5370904576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 11% |
error is  835971100.0
relative error loss 0.19051658
shape of L is 
torch.Size([])
memory (bytes)
5374156800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5374193664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835921660.0
relative error loss 0.19050531
shape of L is 
torch.Size([])
memory (bytes)
5377388544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5377388544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835856100.0
relative error loss 0.19049038
shape of L is 
torch.Size([])
memory (bytes)
5380591616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5380591616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835780600.0
relative error loss 0.19047317
shape of L is 
torch.Size([])
memory (bytes)
5383798784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5383798784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835696900.0
relative error loss 0.1904541
shape of L is 
torch.Size([])
memory (bytes)
5386973184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5386973184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835563500.0
relative error loss 0.1904237
time to take a step is 339.6495599746704
it  11 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5390217216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5390217216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835563500.0
relative error loss 0.1904237
shape of L is 
torch.Size([])
memory (bytes)
5393428480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5393428480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835440400.0
relative error loss 0.19039564
shape of L is 
torch.Size([])
memory (bytes)
5396602880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5396639744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835359500.0
relative error loss 0.19037719
shape of L is 
torch.Size([])
memory (bytes)
5399838720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5399838720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835313150.0
relative error loss 0.19036664
shape of L is 
torch.Size([])
memory (bytes)
5402943488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5403054080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835181600.0
relative error loss 0.19033664
shape of L is 
torch.Size([])
memory (bytes)
5406269440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5406269440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835117300.0
relative error loss 0.190322
shape of L is 
torch.Size([])
memory (bytes)
5409476608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5409476608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  835067650.0
relative error loss 0.19031069
shape of L is 
torch.Size([])
memory (bytes)
5412675584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5412675584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834970600.0
relative error loss 0.19028857
shape of L is 
torch.Size([])
memory (bytes)
5415878656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5415878656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834891260.0
relative error loss 0.19027048
shape of L is 
torch.Size([])
memory (bytes)
5419077632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5419081728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834848260.0
relative error loss 0.1902607
time to take a step is 303.56896901130676
it  12 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5422284800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5422284800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  834848260.0
relative error loss 0.1902607
shape of L is 
torch.Size([])
memory (bytes)
5425291264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5425483776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834796540.0
relative error loss 0.1902489
shape of L is 
torch.Size([])
memory (bytes)
5428658176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5428686848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  834746400.0
relative error loss 0.19023746
shape of L is 
torch.Size([])
memory (bytes)
5431853056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5431853056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  834643200.0
relative error loss 0.19021395
shape of L is 
torch.Size([])
memory (bytes)
5435101184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5435101184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834612500.0
relative error loss 0.19020696
shape of L is 
torch.Size([])
memory (bytes)
5438291968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5438291968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  834541060.0
relative error loss 0.19019067
shape of L is 
torch.Size([])
memory (bytes)
5441515520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
5441515520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  834474240.0
relative error loss 0.19017544
shape of L is 
torch.Size([])
memory (bytes)
5444599808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5444599808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834419460.0
relative error loss 0.19016296
shape of L is 
torch.Size([])
memory (bytes)
5447749632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5447938048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834341600.0
relative error loss 0.19014522
shape of L is 
torch.Size([])
memory (bytes)
5451141120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5451145216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 11% |
error is  834255360.0
relative error loss 0.19012557
time to take a step is 266.33794021606445
it  13 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5454147584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
5454327808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  834255360.0
relative error loss 0.19012557
shape of L is 
torch.Size([])
memory (bytes)
5457559552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5457559552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834218750.0
relative error loss 0.19011723
shape of L is 
torch.Size([])
memory (bytes)
5460754432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5460754432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  834182400.0
relative error loss 0.19010894
shape of L is 
torch.Size([])
memory (bytes)
5463969792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5463969792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834122500.0
relative error loss 0.19009529
shape of L is 
torch.Size([])
memory (bytes)
5467148288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5467185152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834081540.0
relative error loss 0.19008595
shape of L is 
torch.Size([])
memory (bytes)
5470355456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5470355456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834040300.0
relative error loss 0.19007656
shape of L is 
torch.Size([])
memory (bytes)
5473591296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5473591296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  834017300.0
relative error loss 0.19007131
shape of L is 
torch.Size([])
memory (bytes)
5476622336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5476798464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  833956600.0
relative error loss 0.19005749
shape of L is 
torch.Size([])
memory (bytes)
5480009728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5480009728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 11% |
error is  833949440.0
relative error loss 0.19005585
shape of L is 
torch.Size([])
memory (bytes)
5483163648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5483163648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  833905900.0
relative error loss 0.19004592
time to take a step is 264.9023594856262
it  14 : 2500855808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5486366720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5486432256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  833905900.0
relative error loss 0.19004592
shape of L is 
torch.Size([])
memory (bytes)
5489627136
| ID | GPU | MEM |
------------------
|  0 |  3% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5489631232
| ID | GPU | MEM |
------------------
|  0 |  5% |  0% |
|  1 | 99% | 11% |
error is  833881600.0
relative error loss 0.19004038
shape of L is 
torch.Size([])
memory (bytes)
5492793344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5492793344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  833844500.0
relative error loss 0.19003193
shape of L is 
torch.Size([])
memory (bytes)
5496061952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5496061952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  833809400.0
relative error loss 0.19002393
shape of L is 
torch.Size([])
memory (bytes)
5499265024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5499265024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  833753100.0
relative error loss 0.1900111
shape of L is 
torch.Size([])
memory (bytes)
5502423040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5502423040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  833696800.0
relative error loss 0.18999827
shape of L is 
torch.Size([])
memory (bytes)
5505683456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5505683456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  833666050.0
relative error loss 0.18999127
shape of L is 
torch.Size([])
memory (bytes)
5508833280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5508833280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  833628400.0
relative error loss 0.18998268
shape of L is 
torch.Size([])
memory (bytes)
5512110080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5512110080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  833591300.0
relative error loss 0.18997423
shape of L is 
torch.Size([])
memory (bytes)
5515300864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5515300864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  833528300.0
relative error loss 0.18995987
time to take a step is 258.74819684028625
sum tnnu_Z after tensor(13384786., device='cuda:0')
shape of features
(4578,)
shape of features
(4578,)
number of orig particles 18313
number of new particles after remove low mass 16211
tnuZ shape should be parts x labs
torch.Size([18313, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  967999170.0
relative error without small mass is  0.22060558
nnu_Z shape should be number of particles by maxV
(18313, 702)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
shape of features
(18313,)
Thu Feb 2 22:37:11 EST 2023
