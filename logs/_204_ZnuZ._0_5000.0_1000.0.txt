Tue Jan 31 09:06:41 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 30113172
numbers of Z: 20336
shape of features
(20336,)
shape of features
(20336,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01600731991030941	20336	20.336	0.09233174222749402
X	0.012778497170526293	904	0.904	0.24178932168336298
X	0.012867552823305253	16069	16.069	0.09286150180095717
X	0.014578513694715104	1650	1.65	0.2067329816157069
X	0.013783013974032955	11786	11.786	0.10535599505507345
X	0.01545317236274373	30140	30.14	0.0800371225819065
X	0.013460473933685681	22182	22.182	0.08466161526122937
X	0.013457100594290752	42689	42.689	0.06805789395922898
X	0.013822265564151602	31017	31.017	0.07638237316177562
X	0.01432095373288451	14178	14.178	0.10033496955131345
X	0.014620423156477772	12508	12.508	0.10533936352982139
X	0.013401910671206132	7560	7.56	0.1210268252519482
X	0.012873591531938785	57807	57.807	0.06061402139377452
X	0.013084102282787944	6332	6.332	0.12737015425282497
X	0.014886331608964383	251462	251.462	0.03897371227043496
X	0.0128797998754815	19071	19.071	0.08773614528818055
X	0.013181741131004811	19680	19.68	0.08749486421049131
X	0.012981864177497486	52034	52.034	0.06295302682643276
X	0.014579985836771246	24530	24.53	0.08407880257348038
X	0.015042605437265624	99270	99.27	0.0533132323230542
X	0.01451192210979757	83430	83.43	0.05582142236359578
X	0.012647151709047323	15031	15.031	0.09440643930149782
X	0.01555452518179954	129427	129.427	0.04934887893632655
X	0.014632086332680908	8053	8.053	0.12202519732140267
X	0.01312843590400762	30547	30.547	0.0754654456570562
X	0.012981432724377259	2535	2.535	0.1723646576463634
X	0.014309205786738985	45099	45.099	0.06820499882382916
X	0.015223114330102022	48308	48.308	0.06805000328138294
X	0.014591409529544075	7924	7.924	0.12257002084618725
X	0.014982496807691918	106870	106.87	0.05194887275044113
X	0.014393402401323353	727406	727.406	0.02704760794829302
X	0.012864548587325544	8265	8.265	0.11589122795140078
X	0.015887283355354723	845989	845.989	0.026580407928200364
X	0.015041501563869245	14956	14.956	0.10019020046694249
X	0.0129922628585516	7813	7.813	0.11847380170815215
X	0.012853106026346716	6964	6.964	0.1226638079043084
X	0.015242336349131445	204051	204.051	0.04211507752424497
X	0.015248067022782398	70283	70.283	0.0600880578532192
X	0.012859111324674788	980	0.98	0.23586398228461744
X	0.01284935394285207	3680	3.68	0.15170893438056
X	0.012837968585250164	2944	2.944	0.16337521332096344
X	0.014429317719976844	2492	2.492	0.17956950525496682
X	0.012191792015665125	1713	1.713	0.19235496210051214
X	0.012606363953066744	627	0.627	0.27191975910072125
X	0.012427546511060962	3251	3.251	0.15635897300169727
X	0.012814110943175623	678	0.678	0.2663705834409965
X	0.012790768360648859	881	0.881	0.24395341866772613
X	0.014131262987072365	2269	2.269	0.18398489148580519
X	0.01334380196039943	3620	3.62	0.15447457383982693
X	0.012715977127506122	2826	2.826	0.16509193590136798
X	0.013062906746319183	8363	8.363	0.11602704808741603
X	0.014561150484602929	12867	12.867	0.10420921628104529
X	0.012426187089659244	1151	1.151	0.22101454459949352
X	0.014433915316376987	5021	5.021	0.14218855603779
X	0.014515450195507847	2314	2.314	0.18442610112860822
X	0.012839030841194596	5458	5.458	0.1329942619171745
X	0.013432640961805246	5369	5.369	0.13575505215393316
X	0.012831453158949279	1407	1.407	0.2089266858203336
X	0.01282045281796359	3095	3.095	0.1606007505054939
X	0.014433165478819822	2480	2.48	0.17987465336888925
X	0.013291543373514056	3059	3.059	0.16317939249360697
X	0.014604853392005613	5892	5.892	0.13533542398542966
X	0.012853789026613379	1474	1.474	0.20583121041058142
X	0.014586224484076168	5172	5.172	0.1412845889106581
X	0.012846655907767141	6357	6.357	0.12642878378594197
X	0.012826071595873487	2463	2.463	0.173331069204789
X	0.012791654522492688	1620	1.62	0.19913024846563152
X	0.012752638575546241	1504	1.504	0.20391573468589583
X	0.012823802670481521	2838	2.838	0.1653235840644541
X	0.012859144894640186	3138	3.138	0.16002446486393712
X	0.012711215339006328	3364	3.364	0.15575560939436348
X	0.012570843404527151	3877	3.877	0.1480094819215315
X	0.012786994883301747	3437	3.437	0.15495169087919106
X	0.01457822185441793	1776	1.776	0.20172227830934414
X	0.012441599213272007	2595	2.595	0.168621499995842
X	0.014598763234258055	4097	4.097	0.15273903194523125
X	0.014425921220199379	9847	9.847	0.11357424873677205
X	0.012712814405854877	772	0.772	0.2544142710205487
X	0.012615490538649226	664	0.664	0.2668365729629578
X	0.014578067791108781	2133	2.133	0.18977385072289205
X	0.01503336379192894	7375	7.375	0.1267938086609313
X	0.013205546828098949	1855	1.855	0.19237006506772517
X	0.012735183507199313	1475	1.475	0.20514978319141133
X	0.012821056395808921	2957	2.957	0.1630637761685321
X	0.012993379968901854	1721	1.721	0.19617654856520983
X	0.012823155214429838	2046	2.046	0.18437261831650964
X	0.01265927404038462	863	0.863	0.24479319189975043
X	0.012830212945195799	2863	2.863	0.16486843121786482
X	0.012741505100296448	849	0.849	0.2466631920582631
X	0.012711486136653037	2540	2.54	0.17104913036091368
X	0.01369491921102316	3510	3.51	0.15742877619357662
X	0.012798741472318208	2140	2.14	0.18151721980846441
X	0.012818360292132356	1352	1.352	0.2116501566510599
X	0.012762957583051184	1001	1.001	0.23361770298256454
X	0.013093220890003154	7818	7.818	0.11875455489401346
X	0.014408752356671469	3198	3.198	0.1651642207171944
X	0.012805179669519983	2816	2.816	0.16567272234331207
X	0.014582607204486337	2307	2.307	0.184896714651838
X	0.012819091155528506	7262	7.262	0.1208556607384637
X	0.014556235522452735	5916	5.916	0.13500203121885715
X	0.014421072595230844	2505	2.505	0.1792241836781127
X	0.014469638431247678	9808	9.808	0.11383934998617885
X	0.012846852138208673	3752	3.752	0.15072245705134596
X	0.012712694096079679	3287	3.287	0.1569685443615261
X	0.012399435964106288	1520	1.52	0.20130417433291634
X	0.01283871181380345	4163	4.163	0.14555875977200775
X	0.012804069872167863	2056	2.056	0.18398184725278752
X	0.014397220015810614	3530	3.53	0.15977223559137838
X	0.012781495965059052	835	0.835	0.2482933400063797
X	0.012615171030323976	1781	1.781	0.1920479789389946
X	0.01452410212848939	850	0.85	0.25756694259631385
X	0.012662075467532806	1960	1.96	0.18624427943189176
X	0.014527154402944052	2389	2.389	0.18252462614756515
X	0.012370203477215558	1657	1.657	0.19544209605246715
X	0.01283754884073235	2183	2.183	0.18049955036670967
X	0.014327708245172713	4053	4.053	0.15233512100728588
X	0.013580135296127971	2174	2.174	0.1841682885725995
X	0.012828928386037759	1634	1.634	0.19875258058550227
X	0.012818218117360627	1057	1.057	0.22974755063263763
X	0.0128515632688839	6322	6.322	0.12667779318046155
X	0.012264460440876207	859	0.859	0.24259681471524283
X	0.01443254739132867	6508	6.508	0.1304062729007754
X	0.012597543951845818	1406	1.406	0.20769857673578784
X	0.012293381889379583	1709	1.709	0.19303813071778114
X	0.012797080367741541	2326	2.326	0.17653618170053245
X	0.012822498236046559	2107	2.107	0.18257278375609245
X	0.013949377327649415	2866	2.866	0.1694700191954622
X	0.01279806456620493	1217	1.217	0.2190876546384816
X	0.012820666680585012	2063	2.063	0.18385288977330178
X	0.014846097343324828	8871	8.871	0.11872620192520414
X	0.012795676141413043	2201	2.201	0.17981023101374383
X	0.012835981284575554	3476	3.476	0.15456687829835639
X	0.0128151033166208	1360	1.36	0.21121644781696208
X	0.013295980263894603	3238	3.238	0.1601331225896539
X	0.012397308477995549	1461	1.461	0.20396660347908374
X	0.014932263677390905	1974	1.974	0.1963026521975729
X	0.012741149935941496	1193	1.193	0.2202196463791313
X	0.01278476343026108	1296	1.296	0.21446803114374402
X	0.01258687512122745	1064	1.064	0.22785510601814463
X	0.014083587318521142	1145	1.145	0.2308355010379108
X	0.012829925580361883	1420	1.42	0.2082788947052867
X	0.012968720386814498	2846	2.846	0.16578837584867998
X	0.012610863005543713	1351	1.351	0.21055383351768797
X	0.014580407093510305	5896	5.896	0.13522927666467655
X	0.015241167545158224	11872	11.872	0.1086837921201512
X	0.01281324208046699	3890	3.89	0.14878865229167668
X	0.012849572741645702	973	0.973	0.23636977892279767
X	0.014301396094362628	1187	1.187	0.22924994424670414
X	0.014501681153881922	1129	1.129	0.23419348180966712
X	0.01276489297383833	1274	1.274	0.2155836976647095
X	0.0127462099286349	2000	2.0	0.18540311381442762
X	0.012769029382112607	2474	2.474	0.17281684158462748
X	0.014595168804073045	4122	4.122	0.1524171061257197
X	0.012870673545514535	2565	2.565	0.17120034485398922
X	0.014617212164707561	8376	8.376	0.12039523367916048
X	0.012832972357703799	2721	2.721	0.1676999394007536
X	0.013311433347372241	17356	17.356	0.09153595708696091
X	0.012786255574113881	2165	2.165	0.18075701468983224
X	0.012858443200485003	2959	2.959	0.1631853404531718
X	0.014502705367606946	2600	2.6	0.17734756313182384
X	0.012734137194838714	305	0.305	0.34691511983282775
X	0.015898301709683083	7513	7.513	0.12838445699173942
X	0.012739165796533839	1252	1.252	0.21669333503030497
X	0.012861939781058101	6443	6.443	0.12591364034331778
X	0.012532187976354385	766	0.766	0.25386298185811096
X	0.012757486941434876	2929	2.929	0.16331093060885185
X	0.012789408198302312	2269	2.269	0.17796666046845117
X	0.012805701908958406	1835	1.835	0.19109783778538356
X	0.012639222639869387	2455	2.455	0.17267241211671147
X	0.01328262218329193	3790	3.79	0.1518966525752064
X	0.012794306693737848	556	0.556	0.2844334408963854
X	0.012719206755190417	862	0.862	0.2452736648972784
X	0.01237395763444751	1764	1.764	0.19142706821341407
X	0.012921513085405002	3739	3.739	0.151188689357906
X	0.0145743043034887	1226	1.226	0.22822635155359286
X	0.014425601083862854	4410	4.41	0.148444841253912
X	0.012783922915124448	1806	1.806	0.19200629425348745
X	0.01283860492882151	3386	3.386	0.1559350080540102
X	0.012768656226697138	2827	2.827	0.165300104709859
X	0.0128110240193568	4979	4.979	0.13702944741409356
X	0.012735185137676797	1877	1.877	0.18931295309026158
X	0.014450235498711982	5049	5.049	0.14197869654288708
X	0.01282786686739891	1880	1.88	0.18967009639213986
X	0.012710534679589214	4275	4.275	0.14379459665652675
X	0.012775514770478974	1649	1.649	0.19787271720886163
X	0.01348301805313153	2763	2.763	0.16961675242033142
X	0.01280007594426176	3312	3.312	0.1569305172077538
X	0.012898868808790908	1629	1.629	0.19931661998577407
X	0.012707360391630612	5656	5.656	0.13097306234698747
X	0.014466525675242129	3599	3.599	0.15899890442203574
X	0.014340101655842558	9002	9.002	0.1167896635027756
X	0.012788767734984167	1464	1.464	0.20595035318762445
X	0.012317572256995797	619	0.619	0.2709847325834602
X	0.012675479937861459	1658	1.658	0.19699715853961672
X	0.014641865067625544	1781	1.781	0.201826176882828
X	0.014364383197333392	4177	4.177	0.15094109711173156
X	0.012794076428842638	1713	1.713	0.19547168616675237
X	0.012625280418280531	1742	1.742	0.19352227496304134
X	0.01284076484670737	1382	1.382	0.21022980295683982
X	0.012828035514626817	4906	4.906	0.13776668337916018
X	0.01279844751326844	769	0.769	0.25531538401109616
X	0.014600361074172038	5729	5.729	0.13659294163435196
X	0.012792428793521304	1882	1.882	0.18942812609159002
X	0.01460726481528248	9476	9.476	0.11551754489028884
X	0.015278613605350851	2730	2.73	0.17754444973099892
X	0.012807103599256207	3700	3.7	0.1512689292779739
X	0.0126531109592625	970	0.97	0.23540112141238287
X	0.014794836920264683	3465	3.465	0.16223167336900926
X	0.014431539246573621	2845	2.845	0.17182121853729485
X	0.012812798113165255	2173	2.173	0.18065975583948962
X	0.014458909642797581	4291	4.291	0.14991980536946756
X	0.014567949590781816	5074	5.074	0.14212898536992935
X	0.012848376893792033	2443	2.443	0.17390347764083247
X	0.012795679817155669	1826	1.826	0.19136133717571444
X	0.012623129682936999	2607	2.607	0.16917726678645978
X	0.014147490924679625	605	0.605	0.28596179628961566
X	0.012778405253705844	881	0.881	0.24387479439830062
X	0.012532007843758658	535	0.535	0.2861248811837909
X	0.012676464021847723	1200	1.2	0.21941802116904205
X	0.012798496782610028	13570	13.57	0.09806778629333683
X	0.012788365750285654	1119	1.119	0.22524836852108132
X	0.012988137887428293	2448	2.448	0.17441285547446594
X	0.012841084658840353	1027	1.027	0.23210104111460161
X	0.01282496169796075	3924	3.924	0.14840289029622142
X	0.012834627703676323	2005	2.005	0.18567622571317202
X	0.01379855991429113	2126	2.126	0.18653349800229013
X	0.013207046154905271	2057	2.057	0.18586194086507274
X	0.012632238810475573	1082	1.082	0.22685639281259423
X	0.014357283840563978	1274	1.274	0.22419931746423352
X	0.01278486238286139	3162	3.162	0.15931062105291643
X	0.012500955092020028	740	0.74	0.25658848259253125
X	0.012787366613765377	1121	1.121	0.2251084691996733
X	0.012734072319379997	1799	1.799	0.19200478391408446
X	0.013050563709715727	1745	1.745	0.19555906249811028
X	0.014594814118107268	3892	3.892	0.15536096573310304
X	0.012659884279704898	1735	1.735	0.19395906666055934
X	0.012787799186456852	1497	1.497	0.20442060492181321
X	0.014608704464320409	6403	6.403	0.13164652522515713
X	0.014452653612863027	4953	4.953	0.1428980893577383
X	0.012755488137257584	712	0.712	0.26166101990163426
X	0.013411191922812381	12188	12.188	0.10323928244203455
X	0.014601912289721571	42964	42.964	0.06978600584711282
X	0.012848826815525651	2451	2.451	0.17371609361322807
X	0.014588041036366451	5293	5.293	0.1402054925478084
X	0.01264588239449111	772	0.772	0.25396699447548954
X	0.013486927889813587	3178	3.178	0.16190231085164603
X	0.013685665841288469	5885	5.885	0.13248696604671015
X	0.013555685180020762	89713	89.713	0.05326255939123677
X	0.012846605256796375	2414	2.414	0.1745890645136311
X	0.014682182269227788	85173	85.173	0.05565397931532004
X	0.014377144386836122	24171	24.171	0.08409935635317123
X	0.014429505681560687	13225	13.225	0.10294816049538258
X	0.014386741557680391	11554	11.554	0.10758292791428026
X	0.012791791008189248	1184	1.184	0.22106834042366758
X	0.012845386391065056	3673	3.673	0.15178962266915533
X	0.014846971814475748	183845	183.845	0.043224371487133845
X	0.013855820876129838	199306	199.306	0.04111850307550467
X	0.014396197011609662	3626	3.626	0.1583458408316239
X	0.014682903526549535	30097	30.097	0.07872202292688674
X	0.015110986502475169	21074	21.074	0.08950527144122784
X	0.01523532973169505	16122	16.122	0.09813207461082495
X	0.014618939739162506	53393	53.393	0.0649347516993713
X	0.012890699608506053	31931	31.931	0.07390741106668688
X	0.015273985751775642	19388	19.388	0.09235770303986694
X	0.0134081764064364	14262	14.262	0.09796322961693553
X	0.012711001063610547	1303	1.303	0.21367076903858698
X	0.01506140265349624	103803	103.803	0.052547473275698614
X	0.013113718733326906	4574	4.574	0.14206155629708825
X	0.012784872096903908	2562	2.562	0.1708857127115385
X	0.012830824764107568	9377	9.377	0.11101889129810238
X	0.012901694204892155	40931	40.931	0.06805574558984652
X	0.015711672004848853	180050	180.05	0.04435505201204619
X	0.01499325278640914	49651	49.651	0.06708971615691427
X	0.012809037149375363	1351	1.351	0.21165102630645466
X	0.013317669159878632	18811	18.811	0.0891262114681072
X	0.014718490324494089	9172	9.172	0.11707560396508804
X	0.012889735708784399	20504	20.504	0.0856646818894157
X	0.013416509190338944	38545	38.545	0.07034347639836568
X	0.014535699478651104	13588	13.588	0.10227279677540846
X	0.012841002820757254	22653	22.653	0.0827607476671484
X	0.01241615857214286	602	0.602	0.2742396544912027
X	0.012797993656333767	3801	3.801	0.1498814917741287
X	0.014261110451592993	62179	62.179	0.06121199537549933
X	0.014868073966569131	31299	31.299	0.07802637254064058
X	0.012822000386315206	41668	41.668	0.06751252898040397
X	0.012687874555032732	3758	3.758	0.1500182287390044
X	0.014282158891786129	189614	189.614	0.04223195181367612
X	0.012879678144891701	8346	8.346	0.11556035748980999
X	0.012853963595933236	19035	19.035	0.08773267974015755
X	0.014836867775541296	85467	85.467	0.055784634096701285
X	0.013598425966845885	1814	1.814	0.1957118487788538
X	0.01286189799693587	46778	46.778	0.06502611441096867
X	0.015619277407251358	137842	137.842	0.048390464489128905
X	0.015220213161077653	284502	284.502	0.03768007891561324
X	0.014583105082032165	18778	18.778	0.09191792747253566
X	0.013068146869576499	13784	13.784	0.09823801550948491
X	0.012793900174511864	3849	3.849	0.1492399227473073
X	0.012808931832322368	4130	4.13	0.14583248879450975
X	0.014391613466153917	92568	92.568	0.05377117604635498
X	0.014580465909289344	5911	5.911	0.13511497363232838
X	0.01287921171488343	16875	16.875	0.091386455595806
X	0.013778594067500144	26553	26.553	0.08035824377329458
X	0.01307703158686344	12026	12.026	0.10283224668855237
X	0.012805550584325677	13248	13.248	0.09887412325914234
X	0.014535508984681518	3774	3.774	0.15675068571704545
X	0.013876923817475862	146785	146.785	0.04555511797054063
X	0.013418695748861901	8545	8.545	0.11623388296555773
X	0.014968068238960923	11054	11.054	0.11063233993642385
X	0.014620681250281738	15065	15.065	0.09900705846851904
X	0.014650523584510475	47903	47.903	0.06737469483062387
X	0.013128790409286144	9753	9.753	0.11041519158165662
X	0.015280560713702514	109293	109.293	0.05190178741870189
X	0.012782786163780924	4679	4.679	0.1397947385654883
X	0.014243878256342516	22964	22.964	0.08528256192847941
X	0.01460334510874622	5984	5.984	0.13463364007763484
X	0.015282029446454907	154606	154.606	0.04623649408872698
X	0.012873595900260737	26878	26.878	0.07824098470802682
X	0.012699465361190716	4588	4.588	0.14040654180755943
X	0.013333594574255625	87902	87.902	0.053331380996256436
X	0.014870050950757467	109770	109.77	0.05135817673522404
X	0.013254314445986312	3376	3.376	0.15775565780318934
X	0.013379021127303914	87785	87.785	0.05341558741117248
X	0.015266726093957904	130521	130.521	0.04890521514077146
X	0.015272468575842855	259301	259.301	0.03890765140979225
X	0.013054102737088557	17112	17.112	0.09137248120115006
X	0.01281717686964752	1827	1.827	0.1914335016178597
X	0.013149989964617284	6039	6.039	0.1296144627136967
X	0.012871130367158614	4621	4.621	0.14069983027286734
X	0.012843127256167458	12239	12.239	0.10161900783629743
X	0.013090010450403497	14637	14.637	0.09634502789510942
X	0.012604426658348997	1298	1.298	0.21334516306397955
X	0.014775354541210773	42620	42.62	0.07024921708397829
X	0.015115014138683266	75736	75.736	0.05843890430173693
X	0.012703434705402469	4810	4.81	0.13822672797620733
X	0.012866047615468336	5770	5.77	0.13064409368227314
X	0.012865344286261498	16810	16.81	0.0914712399873312
X	0.014600876899208493	9259	9.259	0.11639606144397241
X	0.014587991975964422	10085	10.085	0.1130940797505199
X	0.014595487094986863	8820	8.82	0.11828145804030687
X	0.012849911842137917	2969	2.969	0.16296586651407807
X	0.01453779232562176	30980	30.98	0.07770918408394151
X	0.013020406341672524	28231	28.231	0.07726202595462503
X	0.014424764990084287	14109	14.109	0.10074051598467096
X	0.01509494419205025	43670	43.67	0.07018044321239683
X	0.01284395254965546	7019	7.019	0.12231352525249074
X	0.015195674628291092	286019	286.019	0.03759311967949645
X	0.013237206933398216	8387	8.387	0.11642955521409493
X	0.014806994567593384	51008	51.008	0.06621294029759721
X	0.012757072213628805	1498	1.498	0.20421128283298298
X	0.012831383520190037	2373	2.373	0.1755194499516503
X	0.012828794512771554	2895	2.895	0.16425266583109532
X	0.012912999066566658	8401	8.401	0.11540697163758512
X	0.014788164784530839	2491	2.491	0.18107014578095013
X	0.013826431910850397	44559	44.559	0.06770045089615667
X	0.013094917055565458	2928	2.928	0.16475700672503266
X	0.014124077231048023	2386	2.386	0.1808963822296139
X	0.015076260680099995	152526	152.526	0.04623631910147845
X	0.0144861739791719	37062	37.062	0.07311530259172176
X	0.014245792227801599	24585	24.585	0.0833691674347527
X	0.01524595117493008	92321	92.321	0.0548636471760431
X	0.015317339047115937	226422	226.422	0.04074630525546704
X	0.0144439558990729	6095	6.095	0.13332271464005635
X	0.014595600160159605	8272	8.272	0.12083807845973815
X	0.014598366566635736	11062	11.062	0.10968743231804551
X	0.012642227406366086	1066	1.066	0.22804582784817834
X	0.01274176743857371	2535	2.535	0.17129732071164724
X	0.012856657391861639	12370	12.37	0.10129456033427647
X	0.012796306459723227	3027	3.027	0.16169282065923166
X	0.012822944039313958	2745	2.745	0.16716619838789265
X	0.012670419231803206	3004	3.004	0.16157110140069586
X	0.012829955871276721	5192	5.192	0.1351958911930317
X	0.015180936356221173	224130	224.13	0.040763004745770506
X	0.012701122411097148	12694	12.694	0.10001869933149493
X	0.015058268251376595	56682	56.682	0.06428504534562747
X	0.014437062913985142	4111	4.111	0.15200007098287305
X	0.014555954819566355	3360	3.36	0.16301734180836028
X	0.013264027121888448	25781	25.781	0.08012939815227813
X	0.015235646542795597	152326	152.326	0.046418982545564334
X	0.013407269521923676	245198	245.198	0.03795547861086482
X	0.014607673134234986	9507	9.507	0.11539292520298396
X	0.013627550443345921	26020	26.02	0.08060651940326083
X	0.012831543087574957	2133	2.133	0.18187067433177
X	0.014438791542532107	4597	4.597	0.1464486752725645
X	0.013814241823379246	431348	431.348	0.03175653343814068
X	0.014903385343176511	12437	12.437	0.10621595676642673
X	0.012857514458063352	7292	7.292	0.12081015841038587
X	0.014613257426138638	28591	28.591	0.0799538221468416
X	0.015190712652066759	473009	473.009	0.031786027015172295
X	0.012693108542498588	20736	20.736	0.08490781595882858
X	0.012801841657558178	5983	5.983	0.1288599077576437
X	0.01451104144425352	6435	6.435	0.13113441316038058
X	0.014616294549688583	21065	21.065	0.08853030103462704
X	0.012860028461766489	3029	3.029	0.1619251167328747
X	0.015129859160255029	104428	104.428	0.052521764527497225
X	0.014923068712432723	10831	10.831	0.11127472447634741
X	0.012625573032368245	780	0.78	0.25296018210707505
X	0.013384713346874638	27846	27.846	0.07833372011281416
X	0.01485725800441209	13358	13.358	0.1036093801109206
X	0.01278188835225356	1118	1.118	0.2252774589121631
X	0.014564637237292533	57524	57.524	0.06326306050723574
X	0.01464524876602831	64430	64.43	0.06102899345462091
X	0.015300843157902784	140976	140.976	0.04770055072188594
X	0.01519084218858758	133481	133.481	0.048460452697273046
X	0.014014760412633241	144851	144.851	0.04590796691606643
X	0.01380535969463156	15741	15.741	0.09572053909342387
X	0.0128657656307086	13710	13.71	0.09790376505935615
X	0.014913364759303346	49477	49.477	0.06704876155959187
X	0.012881894670988986	88610	88.61	0.05258142307316519
X	0.01426579591213208	5242	5.242	0.13961529448025406
X	0.014605812689940377	32638	32.638	0.07648930759484786
X	0.013615012857187792	84422	84.422	0.05443198092425289
X	0.015071108564980177	78868	78.868	0.057598979662184815
X	0.014495551087279047	61252	61.252	0.06185452635430813
X	0.014742029905371252	45644	45.644	0.06861059680968871
X	0.012863510430630958	7461	7.461	0.11990965524697196
X	0.01282251539074023	3498	3.498	0.15418820099303376
X	0.014961549937758618	11454	11.454	0.109313332902514
X	0.015213849927456365	79224	79.224	0.057693575397715795
X	0.014562335632634853	70457	70.457	0.05912470175657456
X	0.014492030032183162	188893	188.893	0.04249173396398632
X	0.015555281484657483	87242	87.242	0.056283887603335296
X	0.015070477754546808	37891	37.891	0.07354113321035462
X	0.01461335326235252	12672	12.672	0.10486604916987777
X	0.015081612355730072	154308	154.308	0.04606309451785807
X	0.014242930119800013	2022	2.022	0.19169291610454284
X	0.014299357419067845	6086	6.086	0.13294179076596865
X	0.015196444076522552	120466	120.466	0.05015248946515039
X	0.013230549404232938	11752	11.752	0.10402922554328874
X	0.012728177539994001	3948	3.948	0.14772807095940435
X	0.013287879293766194	14098	14.098	0.09804664168495168
X	0.015178950284660424	311828	311.828	0.03651255378836332
X	0.013417808472267043	11686	11.686	0.1047141171396534
X	0.014391462476116286	56889	56.889	0.06324490481620568
X	0.012821658305333779	1638	1.638	0.1985531434631172
X	0.012822666001983172	3105	3.105	0.16043738518188932
X	0.014575193221969082	2939	2.939	0.17053161758610955
X	0.014612821144327567	5856	5.856	0.13563684113763116
X	0.01511681903605127	15325	15.325	0.09954512057731671
X	0.014013002170969563	40908	40.908	0.06996931905202121
X	0.014106392118939602	2581	2.581	0.17614729666375775
X	0.012886575441678849	67268	67.268	0.05764695007330649
X	0.012675784916177345	3023	3.023	0.1612546467020346
X	0.01583610697231408	41768	41.768	0.07237716738399894
X	0.01332333735669555	6406	6.406	0.12764643698712802
X	0.01299176598104287	52646	52.646	0.0627240756417937
X	0.01438228851870045	5077	5.077	0.1414947342379995
X	0.013664048384986853	12677	12.677	0.10253078921258783
X	0.014278249066992939	15356	15.356	0.09760355060963209
X	0.015046135595685968	15979	15.979	0.09801483012161588
X	0.01449495515738654	50015	50.015	0.06617676517319036
X	0.014003379189021368	215427	215.427	0.04020785500537138
X	0.014042997714551685	3751	3.751	0.1552759709773618
X	0.013764452505029027	3183	3.183	0.16291988729749193
X	0.013059606461321724	40718	40.718	0.06845122384908625
X	0.014420472587688901	10571	10.571	0.11090585480582273
X	0.01526052388965479	363124	363.124	0.034767342541797655
X	0.012683176149184381	1093	1.093	0.22639628045795404
X	0.014529756406482752	75572	75.572	0.05771638731457655
X	0.01508586330175689	159315	159.315	0.04557966897182535
X	0.014401037381972364	2975	2.975	0.16916178890779499
X	0.012983588758305823	83806	83.806	0.05370811121353407
X	0.0145169882440105	11495	11.495	0.10809091250450627
X	0.01513280436340673	412262	412.262	0.03323395574750603
X	0.012815864078176524	7664	7.664	0.11869455563611946
X	0.012831754577101628	8180	8.18	0.11619236200625946
X	0.014547454347879216	6262	6.262	0.13244166947961736
X	0.01289833202790309	2157	2.157	0.181507455706531
X	0.01274355498153452	8235	8.235	0.11566687381253252
X	0.012887243255776374	19796	19.796	0.08666841622489771
X	0.013859784073894418	7860	7.86	0.12081234274949784
X	0.015216953585654287	50744	50.744	0.06693413765268631
X	0.014427371787696673	12102	12.102	0.10603356379369566
X	0.012805010248155715	2172	2.172	0.18065086128629607
X	0.01284692197211868	11469	11.469	0.1038543067841076
X	0.015310936397241539	145346	145.346	0.04722800016122359
X	0.01520873729863861	41661	41.661	0.07146956784287861
X	0.015864188853627375	193159	193.159	0.043467798078753325
X	0.014624251467984815	8307	8.307	0.120747035931436
X	0.01338989341228606	4827	4.827	0.14050764033492275
X	0.014536927987783095	3401	3.401	0.16228884486005465
X	0.012829134510783605	4474	4.474	0.14206937164267328
X	0.015854070985741903	106840	106.84	0.052942239254001225
X	0.013028844786543608	6401	6.401	0.12673192384995616
X	0.012866510834321747	12164	12.164	0.10188919386981099
X	0.01283658927576016	21100	21.1	0.08473361401202588
X	0.01351921202427728	14230	14.23	0.09830648313454442
X	0.012851478136629511	3351	3.351	0.15652830579744614
X	0.014565611140058197	6031	6.031	0.13416723231233213
X	0.015947591458445865	166925	166.925	0.0457149138343073
X	0.012881391298571852	17057	17.057	0.09106539475160105
X	0.013188688586369032	6163	6.163	0.12886544624552032
X	0.012886721183840003	9435	9.435	0.11095158462218872
X	0.014254978152426658	77342	77.342	0.05690936231133264
X	0.014460307435621602	55043	55.043	0.06404597036966807
X	0.015296833775901936	185088	185.088	0.04355865169867782
X	0.014734112888547465	57972	57.972	0.06334347765730743
X	0.014275929875217974	3798	3.798	0.15548308481407674
X	0.012886516322997523	27599	27.599	0.07757957460891601
X	0.013012662652770093	25037	25.037	0.08040097103036342
X	0.012832551625580236	5808	5.808	0.13024532311122322
X	0.01281883168531175	8189	8.189	0.11611077522680324
X	0.01283963544721264	3228	3.228	0.1584429995718292
X	0.014622356337965358	15329	15.329	0.09843914803088351
X	0.013120816024741981	12777	12.777	0.100889038646766
X	0.01262333030018112	1512	1.512	0.20286512128749226
X	0.014607603259007909	16861	16.861	0.09533050175415443
X	0.012878002678633949	5086	5.086	0.13629838767020494
X	0.015002728966168983	26914	26.914	0.08229952686492968
X	0.01428779316166952	5805	5.805	0.13501674802105987
X	0.013803992462732298	14687	14.687	0.09795438287850934
X	0.012807087580141669	2562	2.562	0.17098463462245928
X	0.012836843266389121	10190	10.19	0.10800106005634341
X	0.014579719241877847	3403	3.403	0.16241609730493847
X	0.013166394396462052	8996	8.996	0.11353745048470426
X	0.012944842606275903	15915	15.915	0.0933462144774395
X	0.014424689331041575	82860	82.86	0.05583679899436188
X	0.014100399309055081	3581	3.581	0.15790978062561078
X	0.012697562220244369	1292	1.292	0.21419990735428046
X	0.012830393793602608	4238	4.238	0.1446637363845637
X	0.014513186093984122	62954	62.954	0.061316846535864465
X	0.015914263231615777	433778	433.778	0.03322816347378714
X	0.014583757845003513	21003	21.003	0.0885515234774273
X	0.013572829290249498	4303	4.303	0.14665594755619107
X	0.014370080970090847	60139	60.139	0.062053698459728475
X	0.015064513321756194	81209	81.209	0.057031787173849886
X	0.012610600277296533	1776	1.776	0.1922048163299569
X	0.013605739471101028	8865	8.865	0.1153491690304118
X	0.012865930923693304	16306	16.306	0.09240552223085087
X	0.015278126525976099	96410	96.41	0.05411479361861469
X	0.013885228719184966	219547	219.547	0.039842065362198344
X	0.01517319612113558	143461	143.461	0.04729128039195064
X	0.012850202721125945	7874	7.874	0.117734801572815
X	0.013140861419408637	2824	2.824	0.16694998370037206
X	0.012644122514499038	1265	1.265	0.21541009714473744
X	0.012841199080902371	40840	40.84	0.06799964026744633
X	0.014378142823357445	7321	7.321	0.12523069634540174
X	0.013040737649103481	11029	11.029	0.10574390403867982
X	0.015198286150591853	104589	104.589	0.05257382019283249
X	0.015229751420531495	359108	359.108	0.034872994788325895
X	0.012887855207824562	6265	6.265	0.12718030367481187
X	0.012830979605967614	1890	1.89	0.18935030260215324
X	0.014697454794667041	14544	14.544	0.10035047266218544
X	0.013913829443718278	41970	41.97	0.06921006154141543
X	0.014758431179548807	17991	17.991	0.09361130442804479
X	0.014570050729938801	1105	1.105	0.23624697805751885
X	0.01275474256957868	2879	2.879	0.16423914578372983
X	0.014051026871956883	12350	12.35	0.10439516112431814
X	0.012813018856359762	1627	1.627	0.1989548989582538
X	0.014134378576310567	35777	35.777	0.0733766025613572
X	0.012797011363431303	2870	2.87	0.16459206207632213
X	0.01411534889527289	12112	12.112	0.10523460736703709
X	0.014588682187380072	10957	10.957	0.11001235642986222
X	0.013131880857793706	4547	4.547	0.142407870813764
X	0.014363670825636658	2684	2.684	0.17491523750047844
X	0.013578264982174286	114974	114.974	0.049062334933305016
X	0.014673672167877658	285240	285.24	0.037191422941610666
X	0.014643354907541026	62395	62.395	0.06168273184446993
X	0.014334599431722425	38936	38.936	0.07167121101788106
X	0.013237994928622175	12591	12.591	0.10168432159030637
X	0.012816212465782633	10095	10.095	0.10828071754926119
X	0.015149674299003197	11074	11.074	0.11101106240181045
X	0.01521685087011618	84440	84.44	0.056484009828919005
X	0.015730702641219556	14552	14.552	0.10263019290049953
X	0.01425043120247992	19017	19.017	0.09082986452811766
X	0.015255126253166282	13348	13.348	0.1045522065081072
X	0.015337993258796168	785932	785.932	0.026923273121581014
X	0.012828631141912714	8225	8.225	0.1159706622433112
X	0.013491354249089293	40629	40.629	0.06924787820584108
X	0.014745321393488968	174223	174.223	0.04390522951378291
X	0.01512699333943528	33079	33.079	0.07704304061854562
X	0.013962645982180926	15565	15.565	0.09644347877493924
X	0.0151368322908608	334186	334.186	0.03564639230136508
X	0.014518096901855439	57585	57.585	0.06317328196790416
X	0.013227144483173272	33546	33.546	0.07332889122726445
X	0.012795694309469642	3644	3.644	0.1519947180052117
X	0.01403292204551066	17520	17.52	0.09286912141563658
X	0.012866924552309764	5566	5.566	0.13222406889827149
X	0.013513760698861206	112559	112.559	0.049332372187721525
X	0.015963800989134917	688348	688.348	0.028517366472624196
X	0.015195206495434673	61806	61.806	0.06264580431924446
X	0.014133947098074008	102186	102.186	0.05171589210467711
X	0.0146305919916864	37694	37.694	0.07294517283710272
X	0.013000086784786083	20905	20.905	0.08535560138071852
X	0.014537842076324862	21179	21.179	0.08821277794219257
X	0.012814801506708462	6013	6.013	0.12868864455608273
X	0.015215016310396227	237661	237.661	0.0400041303788441
X	0.015031473426743373	217402	217.402	0.04104365289280289
X	0.014975492974607077	32706	32.706	0.07707577467539985
X	0.015243511153759295	686382	686.382	0.028108631952803227
X	0.01506808691855918	145796	145.796	0.046928589656161604
X	0.01283511061471301	20386	20.386	0.08570823110615175
X	0.014424651829095135	2005	2.005	0.19304726543694722
X	0.014890629078772285	30450	30.45	0.07878470554886849
X	0.015048280079484666	69408	69.408	0.060074815423524205
X	0.012652068956578243	3225	3.225	0.15771655313898597
X	0.015322003211664036	414033	414.033	0.03332423567414431
X	0.01298646760544672	11240	11.24	0.10493207035421795
X	0.014706549035026238	18181	18.181	0.09317462842789175
X	0.014148545709941793	3577	3.577	0.15814821241731092
X	0.012833332947796509	5067	5.067	0.13631056538352151
X	0.0128594320646686	1697	1.697	0.19641741090182874
X	0.014553754025998056	5756	5.756	0.13623376166211656
X	0.01489557062909373	13905	13.905	0.10232035791627149
X	0.013472027908216411	12568	12.568	0.10234240078639453
X	0.015984135052165446	2014992	2014.992	0.019943678812649857
X	0.012825751533591096	9698	9.698	0.10976575800691844
X	0.012866216527913768	58447	58.447	0.060380431250805375
X	0.014581025380317305	4772	4.772	0.14510951045750492
X	0.012821908052001226	10898	10.898	0.10556872705256241
X	0.012819366208420889	6044	6.044	0.128483502220163
X	0.013431891913250392	106204	106.204	0.05019557552973238
X	0.014861122721878227	44604	44.604	0.06932543517935917
X	0.01561560563554897	567835	567.835	0.030184145908770832
X	0.012848729296943763	4731	4.731	0.13951976746162517
X	0.015135994658260757	119675	119.675	0.050196007618662455
X	0.01365862198710312	21328	21.328	0.08619569674916981
X	0.012858851910371295	88081	88.081	0.05265504473227892
X	0.015173463259974396	143175	143.175	0.04732302615231714
X	0.012842892432896796	10062	10.062	0.10847413043514041
X	0.012833468867888663	6299	6.299	0.1267722361654722
X	0.0158256709202935	61199	61.199	0.06370975500201395
X	0.014753994952922563	11055	11.055	0.11009906425853623
X	0.014098318713865389	32160	32.16	0.07596562139850475
X	0.013263258651722856	120840	120.84	0.04787915834801146
X	0.013035833740027649	6424	6.424	0.12660312533569595
X	0.014744684552673928	51268	51.268	0.06600796898103359
X	0.01302866645635627	13455	13.455	0.0989324478083683
X	0.015147094694189966	85782	85.782	0.056101919967615696
X	0.014998336793233943	198519	198.519	0.04227468117277798
X	0.012819612776300759	16494	16.494	0.09194249941669753
X	0.01285930359229976	5543	5.543	0.13238055281068262
X	0.014559541211915232	111277	111.277	0.050766913530966074
X	0.013602175955807351	41748	41.748	0.06881095277411714
X	0.014437319836222783	321241	321.241	0.03555373268870405
X	0.0145634557617958	21881	21.881	0.08731035584808879
X	0.013712342611821082	29587	29.587	0.07738739742497
X	0.013759373380071755	21354	21.354	0.08637203244132041
X	0.014586890424533892	69951	69.951	0.05930021451293104
X	0.01492795175455837	123598	123.598	0.04943064299899334
X	0.013590777125964619	17496	17.496	0.09192532490083914
X	0.012506259884939986	4492	4.492	0.1406789874837435
X	0.012808250178352215	6723	6.723	0.12396780080850518
X	0.013332777250281253	36417	36.417	0.07153833352456242
X	0.014424114362265572	44317	44.317	0.0687869839734404
X	0.013172470567596588	49070	49.07	0.06450851725353633
X	0.014548332579206475	9536	9.536	0.11511952599772135
X	0.01289294517430559	17612	17.612	0.09012552751259506
X	0.014925741892467509	26433	26.433	0.08265386319789127
X	0.015116756131379456	26986	26.986	0.0824340613200567
X	0.015197064271985515	21198	21.198	0.08949970505962411
X	0.014183223468371474	46515	46.515	0.0673070629908016
X	0.015088736901331232	10099	10.099	0.11432069022735826
X	0.012882590869722118	19620	19.62	0.08691633473595846
X	0.01254484032049792	1579	1.579	0.19953894386648377
X	0.014581046708455399	4689	4.689	0.14596077348589023
X	0.013214547758565861	63200	63.2	0.059353328031569765
X	0.01376205598662574	12737	12.737	0.1026137101611449
X	0.013762348485960172	37724	37.724	0.07145373552624044
X	0.012820096154211476	1658	1.658	0.19774351608819446
X	0.012790775870281372	4970	4.97	0.13703983700550368
X	0.014512916480241384	17983	17.983	0.09310310905130997
X	0.013831873814791779	33973	33.973	0.07411663719901322
X	0.014583826943832423	15758	15.758	0.09745186061176678
X	0.01284998734163883	2841	2.841	0.16537777909736842
X	0.01451926734757992	4945	4.945	0.1431944334123959
X	0.013612905340592724	15668	15.668	0.0954213933584348
X	0.014848113509963918	17497	17.497	0.09467512685672871
X	0.015181368683028018	71886	71.886	0.059550974070291085
X	0.012856093962091388	3505	3.505	0.15421987207374646
X	0.014298630846990415	7601	7.601	0.12344530408107275
X	0.012759264924363599	14132	14.132	0.0966512203735273
X	0.014617036949296948	6656	6.656	0.12998163948192284
X	0.014443321591212015	112594	112.594	0.05043328739485155
X	0.014574528020700793	5666	5.666	0.13701642856290983
X	0.012842086579581788	38004	38.004	0.06965229456880832
X	0.015119121325246856	117769	117.769	0.050446594647912936
X	0.012885374286232558	16457	16.457	0.09216841583345517
X	0.014976370227986351	95032	95.032	0.05401473288784101
X	0.014402827662240884	86714	86.714	0.05496921243174208
X	0.012799668224445096	1334	1.334	0.21249446770548003
X	0.013362441634329009	26726	26.726	0.07936894710488429
X	0.012881244168736199	8915	8.915	0.11305214885371316
X	0.014009555976945686	99865	99.865	0.051960145058531935
X	0.01372419247580568	67687	67.687	0.05874810261525235
X	0.014580924099346637	16344	16.344	0.09626658488074802
X	0.01287546731383946	37378	37.378	0.07009961729958569
X	0.012856384414348956	12178	12.178	0.10182340752764078
X	0.014433327584986012	16443	16.443	0.09574772651094633
X	0.012849108576203649	2926	2.926	0.1637568814246758
X	0.012819823707237683	6556	6.556	0.12504923407627533
X	0.0149581715896734	114802	114.802	0.050696310289186745
X	0.01596250040311245	391940	391.94	0.034405439581300074
X	0.013876830423186869	19497	19.497	0.0892841078020728
X	0.013110269917235903	11211	11.211	0.10535514767630515
X	0.01413178959060045	23050	23.05	0.08495235271409968
X	0.015084253640774708	80721	80.721	0.05717143559748725
X	0.012871615813820163	14501	14.501	0.09610479102656941
X	0.012874269972209399	5822	5.822	0.13028171602347602
X	0.01577381708895764	31105	31.105	0.07974476788528707
X	0.015239032969665495	88696	88.696	0.055592703099992424
X	0.01279188973736049	3557	3.557	0.15320876578879825
X	0.014905066049811821	21708	21.708	0.08822087562428575
X	0.012873037730758272	5803	5.803	0.13041958798358916
time for making epsilon is 1.5838322639465332
epsilons are
[0.24178932168336298, 0.09286150180095717, 0.2067329816157069, 0.10535599505507345, 0.0800371225819065, 0.08466161526122937, 0.06805789395922898, 0.07638237316177562, 0.10033496955131345, 0.10533936352982139, 0.1210268252519482, 0.06061402139377452, 0.12737015425282497, 0.03897371227043496, 0.08773614528818055, 0.08749486421049131, 0.06295302682643276, 0.08407880257348038, 0.0533132323230542, 0.05582142236359578, 0.09440643930149782, 0.04934887893632655, 0.12202519732140267, 0.0754654456570562, 0.1723646576463634, 0.06820499882382916, 0.06805000328138294, 0.12257002084618725, 0.05194887275044113, 0.02704760794829302, 0.11589122795140078, 0.026580407928200364, 0.10019020046694249, 0.11847380170815215, 0.1226638079043084, 0.04211507752424497, 0.0600880578532192, 0.23586398228461744, 0.15170893438056, 0.16337521332096344, 0.17956950525496682, 0.19235496210051214, 0.27191975910072125, 0.15635897300169727, 0.2663705834409965, 0.24395341866772613, 0.18398489148580519, 0.15447457383982693, 0.16509193590136798, 0.11602704808741603, 0.10420921628104529, 0.22101454459949352, 0.14218855603779, 0.18442610112860822, 0.1329942619171745, 0.13575505215393316, 0.2089266858203336, 0.1606007505054939, 0.17987465336888925, 0.16317939249360697, 0.13533542398542966, 0.20583121041058142, 0.1412845889106581, 0.12642878378594197, 0.173331069204789, 0.19913024846563152, 0.20391573468589583, 0.1653235840644541, 0.16002446486393712, 0.15575560939436348, 0.1480094819215315, 0.15495169087919106, 0.20172227830934414, 0.168621499995842, 0.15273903194523125, 0.11357424873677205, 0.2544142710205487, 0.2668365729629578, 0.18977385072289205, 0.1267938086609313, 0.19237006506772517, 0.20514978319141133, 0.1630637761685321, 0.19617654856520983, 0.18437261831650964, 0.24479319189975043, 0.16486843121786482, 0.2466631920582631, 0.17104913036091368, 0.15742877619357662, 0.18151721980846441, 0.2116501566510599, 0.23361770298256454, 0.11875455489401346, 0.1651642207171944, 0.16567272234331207, 0.184896714651838, 0.1208556607384637, 0.13500203121885715, 0.1792241836781127, 0.11383934998617885, 0.15072245705134596, 0.1569685443615261, 0.20130417433291634, 0.14555875977200775, 0.18398184725278752, 0.15977223559137838, 0.2482933400063797, 0.1920479789389946, 0.25756694259631385, 0.18624427943189176, 0.18252462614756515, 0.19544209605246715, 0.18049955036670967, 0.15233512100728588, 0.1841682885725995, 0.19875258058550227, 0.22974755063263763, 0.12667779318046155, 0.24259681471524283, 0.1304062729007754, 0.20769857673578784, 0.19303813071778114, 0.17653618170053245, 0.18257278375609245, 0.1694700191954622, 0.2190876546384816, 0.18385288977330178, 0.11872620192520414, 0.17981023101374383, 0.15456687829835639, 0.21121644781696208, 0.1601331225896539, 0.20396660347908374, 0.1963026521975729, 0.2202196463791313, 0.21446803114374402, 0.22785510601814463, 0.2308355010379108, 0.2082788947052867, 0.16578837584867998, 0.21055383351768797, 0.13522927666467655, 0.1086837921201512, 0.14878865229167668, 0.23636977892279767, 0.22924994424670414, 0.23419348180966712, 0.2155836976647095, 0.18540311381442762, 0.17281684158462748, 0.1524171061257197, 0.17120034485398922, 0.12039523367916048, 0.1676999394007536, 0.09153595708696091, 0.18075701468983224, 0.1631853404531718, 0.17734756313182384, 0.34691511983282775, 0.12838445699173942, 0.21669333503030497, 0.12591364034331778, 0.25386298185811096, 0.16331093060885185, 0.17796666046845117, 0.19109783778538356, 0.17267241211671147, 0.1518966525752064, 0.2844334408963854, 0.2452736648972784, 0.19142706821341407, 0.151188689357906, 0.22822635155359286, 0.148444841253912, 0.19200629425348745, 0.1559350080540102, 0.165300104709859, 0.13702944741409356, 0.18931295309026158, 0.14197869654288708, 0.18967009639213986, 0.14379459665652675, 0.19787271720886163, 0.16961675242033142, 0.1569305172077538, 0.19931661998577407, 0.13097306234698747, 0.15899890442203574, 0.1167896635027756, 0.20595035318762445, 0.2709847325834602, 0.19699715853961672, 0.201826176882828, 0.15094109711173156, 0.19547168616675237, 0.19352227496304134, 0.21022980295683982, 0.13776668337916018, 0.25531538401109616, 0.13659294163435196, 0.18942812609159002, 0.11551754489028884, 0.17754444973099892, 0.1512689292779739, 0.23540112141238287, 0.16223167336900926, 0.17182121853729485, 0.18065975583948962, 0.14991980536946756, 0.14212898536992935, 0.17390347764083247, 0.19136133717571444, 0.16917726678645978, 0.28596179628961566, 0.24387479439830062, 0.2861248811837909, 0.21941802116904205, 0.09806778629333683, 0.22524836852108132, 0.17441285547446594, 0.23210104111460161, 0.14840289029622142, 0.18567622571317202, 0.18653349800229013, 0.18586194086507274, 0.22685639281259423, 0.22419931746423352, 0.15931062105291643, 0.25658848259253125, 0.2251084691996733, 0.19200478391408446, 0.19555906249811028, 0.15536096573310304, 0.19395906666055934, 0.20442060492181321, 0.13164652522515713, 0.1428980893577383, 0.26166101990163426, 0.10323928244203455, 0.06978600584711282, 0.17371609361322807, 0.1402054925478084, 0.25396699447548954, 0.16190231085164603, 0.13248696604671015, 0.05326255939123677, 0.1745890645136311, 0.05565397931532004, 0.08409935635317123, 0.10294816049538258, 0.10758292791428026, 0.22106834042366758, 0.15178962266915533, 0.043224371487133845, 0.04111850307550467, 0.1583458408316239, 0.07872202292688674, 0.08950527144122784, 0.09813207461082495, 0.0649347516993713, 0.07390741106668688, 0.09235770303986694, 0.09796322961693553, 0.21367076903858698, 0.052547473275698614, 0.14206155629708825, 0.1708857127115385, 0.11101889129810238, 0.06805574558984652, 0.04435505201204619, 0.06708971615691427, 0.21165102630645466, 0.0891262114681072, 0.11707560396508804, 0.0856646818894157, 0.07034347639836568, 0.10227279677540846, 0.0827607476671484, 0.2742396544912027, 0.1498814917741287, 0.06121199537549933, 0.07802637254064058, 0.06751252898040397, 0.1500182287390044, 0.04223195181367612, 0.11556035748980999, 0.08773267974015755, 0.055784634096701285, 0.1957118487788538, 0.06502611441096867, 0.048390464489128905, 0.03768007891561324, 0.09191792747253566, 0.09823801550948491, 0.1492399227473073, 0.14583248879450975, 0.05377117604635498, 0.13511497363232838, 0.091386455595806, 0.08035824377329458, 0.10283224668855237, 0.09887412325914234, 0.15675068571704545, 0.04555511797054063, 0.11623388296555773, 0.11063233993642385, 0.09900705846851904, 0.06737469483062387, 0.11041519158165662, 0.05190178741870189, 0.1397947385654883, 0.08528256192847941, 0.13463364007763484, 0.04623649408872698, 0.07824098470802682, 0.14040654180755943, 0.053331380996256436, 0.05135817673522404, 0.15775565780318934, 0.05341558741117248, 0.04890521514077146, 0.03890765140979225, 0.09137248120115006, 0.1914335016178597, 0.1296144627136967, 0.14069983027286734, 0.10161900783629743, 0.09634502789510942, 0.21334516306397955, 0.07024921708397829, 0.05843890430173693, 0.13822672797620733, 0.13064409368227314, 0.0914712399873312, 0.11639606144397241, 0.1130940797505199, 0.11828145804030687, 0.16296586651407807, 0.07770918408394151, 0.07726202595462503, 0.10074051598467096, 0.07018044321239683, 0.12231352525249074, 0.03759311967949645, 0.11642955521409493, 0.06621294029759721, 0.20421128283298298, 0.1755194499516503, 0.16425266583109532, 0.11540697163758512, 0.18107014578095013, 0.06770045089615667, 0.16475700672503266, 0.1808963822296139, 0.04623631910147845, 0.07311530259172176, 0.0833691674347527, 0.0548636471760431, 0.04074630525546704, 0.13332271464005635, 0.12083807845973815, 0.10968743231804551, 0.22804582784817834, 0.17129732071164724, 0.10129456033427647, 0.16169282065923166, 0.16716619838789265, 0.16157110140069586, 0.1351958911930317, 0.040763004745770506, 0.10001869933149493, 0.06428504534562747, 0.15200007098287305, 0.16301734180836028, 0.08012939815227813, 0.046418982545564334, 0.03795547861086482, 0.11539292520298396, 0.08060651940326083, 0.18187067433177, 0.1464486752725645, 0.03175653343814068, 0.10621595676642673, 0.12081015841038587, 0.0799538221468416, 0.031786027015172295, 0.08490781595882858, 0.1288599077576437, 0.13113441316038058, 0.08853030103462704, 0.1619251167328747, 0.052521764527497225, 0.11127472447634741, 0.25296018210707505, 0.07833372011281416, 0.1036093801109206, 0.2252774589121631, 0.06326306050723574, 0.06102899345462091, 0.04770055072188594, 0.048460452697273046, 0.04590796691606643, 0.09572053909342387, 0.09790376505935615, 0.06704876155959187, 0.05258142307316519, 0.13961529448025406, 0.07648930759484786, 0.05443198092425289, 0.057598979662184815, 0.06185452635430813, 0.06861059680968871, 0.11990965524697196, 0.15418820099303376, 0.109313332902514, 0.057693575397715795, 0.05912470175657456, 0.04249173396398632, 0.056283887603335296, 0.07354113321035462, 0.10486604916987777, 0.04606309451785807, 0.19169291610454284, 0.13294179076596865, 0.05015248946515039, 0.10402922554328874, 0.14772807095940435, 0.09804664168495168, 0.03651255378836332, 0.1047141171396534, 0.06324490481620568, 0.1985531434631172, 0.16043738518188932, 0.17053161758610955, 0.13563684113763116, 0.09954512057731671, 0.06996931905202121, 0.17614729666375775, 0.05764695007330649, 0.1612546467020346, 0.07237716738399894, 0.12764643698712802, 0.0627240756417937, 0.1414947342379995, 0.10253078921258783, 0.09760355060963209, 0.09801483012161588, 0.06617676517319036, 0.04020785500537138, 0.1552759709773618, 0.16291988729749193, 0.06845122384908625, 0.11090585480582273, 0.034767342541797655, 0.22639628045795404, 0.05771638731457655, 0.04557966897182535, 0.16916178890779499, 0.05370811121353407, 0.10809091250450627, 0.03323395574750603, 0.11869455563611946, 0.11619236200625946, 0.13244166947961736, 0.181507455706531, 0.11566687381253252, 0.08666841622489771, 0.12081234274949784, 0.06693413765268631, 0.10603356379369566, 0.18065086128629607, 0.1038543067841076, 0.04722800016122359, 0.07146956784287861, 0.043467798078753325, 0.120747035931436, 0.14050764033492275, 0.16228884486005465, 0.14206937164267328, 0.052942239254001225, 0.12673192384995616, 0.10188919386981099, 0.08473361401202588, 0.09830648313454442, 0.15652830579744614, 0.13416723231233213, 0.0457149138343073, 0.09106539475160105, 0.12886544624552032, 0.11095158462218872, 0.05690936231133264, 0.06404597036966807, 0.04355865169867782, 0.06334347765730743, 0.15548308481407674, 0.07757957460891601, 0.08040097103036342, 0.13024532311122322, 0.11611077522680324, 0.1584429995718292, 0.09843914803088351, 0.100889038646766, 0.20286512128749226, 0.09533050175415443, 0.13629838767020494, 0.08229952686492968, 0.13501674802105987, 0.09795438287850934, 0.17098463462245928, 0.10800106005634341, 0.16241609730493847, 0.11353745048470426, 0.0933462144774395, 0.05583679899436188, 0.15790978062561078, 0.21419990735428046, 0.1446637363845637, 0.061316846535864465, 0.03322816347378714, 0.0885515234774273, 0.14665594755619107, 0.062053698459728475, 0.057031787173849886, 0.1922048163299569, 0.1153491690304118, 0.09240552223085087, 0.05411479361861469, 0.039842065362198344, 0.04729128039195064, 0.117734801572815, 0.16694998370037206, 0.21541009714473744, 0.06799964026744633, 0.12523069634540174, 0.10574390403867982, 0.05257382019283249, 0.034872994788325895, 0.12718030367481187, 0.18935030260215324, 0.10035047266218544, 0.06921006154141543, 0.09361130442804479, 0.23624697805751885, 0.16423914578372983, 0.10439516112431814, 0.1989548989582538, 0.0733766025613572, 0.16459206207632213, 0.10523460736703709, 0.11001235642986222, 0.142407870813764, 0.17491523750047844, 0.049062334933305016, 0.037191422941610666, 0.06168273184446993, 0.07167121101788106, 0.10168432159030637, 0.10828071754926119, 0.11101106240181045, 0.056484009828919005, 0.10263019290049953, 0.09082986452811766, 0.1045522065081072, 0.026923273121581014, 0.1159706622433112, 0.06924787820584108, 0.04390522951378291, 0.07704304061854562, 0.09644347877493924, 0.03564639230136508, 0.06317328196790416, 0.07332889122726445, 0.1519947180052117, 0.09286912141563658, 0.13222406889827149, 0.049332372187721525, 0.028517366472624196, 0.06264580431924446, 0.05171589210467711, 0.07294517283710272, 0.08535560138071852, 0.08821277794219257, 0.12868864455608273, 0.0400041303788441, 0.04104365289280289, 0.07707577467539985, 0.028108631952803227, 0.046928589656161604, 0.08570823110615175, 0.19304726543694722, 0.07878470554886849, 0.060074815423524205, 0.15771655313898597, 0.03332423567414431, 0.10493207035421795, 0.09317462842789175, 0.15814821241731092, 0.13631056538352151, 0.19641741090182874, 0.13623376166211656, 0.10232035791627149, 0.10234240078639453, 0.019943678812649857, 0.10976575800691844, 0.060380431250805375, 0.14510951045750492, 0.10556872705256241, 0.128483502220163, 0.05019557552973238, 0.06932543517935917, 0.030184145908770832, 0.13951976746162517, 0.050196007618662455, 0.08619569674916981, 0.05265504473227892, 0.04732302615231714, 0.10847413043514041, 0.1267722361654722, 0.06370975500201395, 0.11009906425853623, 0.07596562139850475, 0.04787915834801146, 0.12660312533569595, 0.06600796898103359, 0.0989324478083683, 0.056101919967615696, 0.04227468117277798, 0.09194249941669753, 0.13238055281068262, 0.050766913530966074, 0.06881095277411714, 0.03555373268870405, 0.08731035584808879, 0.07738739742497, 0.08637203244132041, 0.05930021451293104, 0.04943064299899334, 0.09192532490083914, 0.1406789874837435, 0.12396780080850518, 0.07153833352456242, 0.0687869839734404, 0.06450851725353633, 0.11511952599772135, 0.09012552751259506, 0.08265386319789127, 0.0824340613200567, 0.08949970505962411, 0.0673070629908016, 0.11432069022735826, 0.08691633473595846, 0.19953894386648377, 0.14596077348589023, 0.059353328031569765, 0.1026137101611449, 0.07145373552624044, 0.19774351608819446, 0.13703983700550368, 0.09310310905130997, 0.07411663719901322, 0.09745186061176678, 0.16537777909736842, 0.1431944334123959, 0.0954213933584348, 0.09467512685672871, 0.059550974070291085, 0.15421987207374646, 0.12344530408107275, 0.0966512203735273, 0.12998163948192284, 0.05043328739485155, 0.13701642856290983, 0.06965229456880832, 0.050446594647912936, 0.09216841583345517, 0.05401473288784101, 0.05496921243174208, 0.21249446770548003, 0.07936894710488429, 0.11305214885371316, 0.051960145058531935, 0.05874810261525235, 0.09626658488074802, 0.07009961729958569, 0.10182340752764078, 0.09574772651094633, 0.1637568814246758, 0.12504923407627533, 0.050696310289186745, 0.034405439581300074, 0.0892841078020728, 0.10535514767630515, 0.08495235271409968, 0.05717143559748725, 0.09610479102656941, 0.13028171602347602, 0.07974476788528707, 0.055592703099992424, 0.15320876578879825, 0.08822087562428575, 0.13041958798358916]
0.09233174222749402
Making ranges
torch.Size([35281, 2])
We keep 5.87e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([2472, 2])
We keep 6.20e+04/8.17e+05 =  7% of the original kernel matrix.

torch.Size([10544, 2])
We keep 6.54e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([29412, 2])
We keep 5.18e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([31373, 2])
We keep 5.07e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([4103, 2])
We keep 1.53e+05/2.72e+06 =  5% of the original kernel matrix.

torch.Size([13023, 2])
We keep 1.01e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([19187, 2])
We keep 7.01e+06/1.39e+08 =  5% of the original kernel matrix.

torch.Size([26113, 2])
We keep 4.22e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([47732, 2])
We keep 1.48e+07/9.08e+08 =  1% of the original kernel matrix.

torch.Size([42252, 2])
We keep 9.08e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([40022, 2])
We keep 8.37e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([38416, 2])
We keep 6.96e+06/4.51e+08 =  1% of the original kernel matrix.

torch.Size([78527, 2])
We keep 2.60e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([51746, 2])
We keep 1.19e+07/8.68e+08 =  1% of the original kernel matrix.

torch.Size([56261, 2])
We keep 1.39e+07/9.62e+08 =  1% of the original kernel matrix.

torch.Size([44777, 2])
We keep 9.13e+06/6.31e+08 =  1% of the original kernel matrix.

torch.Size([23611, 2])
We keep 1.19e+07/2.01e+08 =  5% of the original kernel matrix.

torch.Size([29082, 2])
We keep 4.87e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([19122, 2])
We keep 5.08e+06/1.56e+08 =  3% of the original kernel matrix.

torch.Size([26108, 2])
We keep 4.35e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([14857, 2])
We keep 1.77e+06/5.72e+07 =  3% of the original kernel matrix.

torch.Size([22709, 2])
We keep 2.98e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([98391, 2])
We keep 7.61e+07/3.34e+09 =  2% of the original kernel matrix.

torch.Size([57906, 2])
We keep 1.47e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([13870, 2])
We keep 1.19e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([22031, 2])
We keep 2.57e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([430822, 2])
We keep 7.11e+08/6.32e+10 =  1% of the original kernel matrix.

torch.Size([126977, 2])
We keep 5.63e+07/5.11e+09 =  1% of the original kernel matrix.

torch.Size([35608, 2])
We keep 6.24e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([35855, 2])
We keep 6.07e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([35713, 2])
We keep 7.37e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([36180, 2])
We keep 6.25e+06/4.00e+08 =  1% of the original kernel matrix.

torch.Size([100286, 2])
We keep 4.27e+07/2.71e+09 =  1% of the original kernel matrix.

torch.Size([59169, 2])
We keep 1.40e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([39182, 2])
We keep 1.14e+07/6.02e+08 =  1% of the original kernel matrix.

torch.Size([37183, 2])
We keep 7.53e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([157914, 2])
We keep 1.63e+08/9.85e+09 =  1% of the original kernel matrix.

torch.Size([75667, 2])
We keep 2.49e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([140932, 2])
We keep 1.03e+08/6.96e+09 =  1% of the original kernel matrix.

torch.Size([71483, 2])
We keep 2.12e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([26213, 2])
We keep 6.65e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([30741, 2])
We keep 5.12e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([205467, 2])
We keep 1.61e+08/1.68e+10 =  0% of the original kernel matrix.

torch.Size([88040, 2])
We keep 3.06e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([15859, 2])
We keep 1.88e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([23682, 2])
We keep 3.17e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([52523, 2])
We keep 3.08e+07/9.33e+08 =  3% of the original kernel matrix.

torch.Size([42872, 2])
We keep 8.92e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([5620, 2])
We keep 4.59e+05/6.43e+06 =  7% of the original kernel matrix.

torch.Size([14250, 2])
We keep 1.27e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([79010, 2])
We keep 2.80e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([52504, 2])
We keep 1.24e+07/9.17e+08 =  1% of the original kernel matrix.

torch.Size([79668, 2])
We keep 4.10e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([53026, 2])
We keep 1.33e+07/9.82e+08 =  1% of the original kernel matrix.

torch.Size([15301, 2])
We keep 2.08e+06/6.28e+07 =  3% of the original kernel matrix.

torch.Size([23193, 2])
We keep 3.16e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([119310, 2])
We keep 2.89e+08/1.14e+10 =  2% of the original kernel matrix.

torch.Size([64742, 2])
We keep 2.68e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([1434761, 2])
We keep 3.67e+09/5.29e+11 =  0% of the original kernel matrix.

torch.Size([240228, 2])
We keep 1.47e+08/1.48e+10 =  0% of the original kernel matrix.

torch.Size([16440, 2])
We keep 2.02e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([23911, 2])
We keep 3.18e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([1484301, 2])
We keep 5.25e+09/7.16e+11 =  0% of the original kernel matrix.

torch.Size([248926, 2])
We keep 1.72e+08/1.72e+10 =  1% of the original kernel matrix.

torch.Size([23040, 2])
We keep 9.50e+06/2.24e+08 =  4% of the original kernel matrix.

torch.Size([28720, 2])
We keep 5.21e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([16719, 2])
We keep 1.97e+06/6.10e+07 =  3% of the original kernel matrix.

torch.Size([24417, 2])
We keep 3.03e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([14883, 2])
We keep 1.43e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([22766, 2])
We keep 2.74e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([315195, 2])
We keep 5.66e+08/4.16e+10 =  1% of the original kernel matrix.

torch.Size([109439, 2])
We keep 4.72e+07/4.15e+09 =  1% of the original kernel matrix.

torch.Size([116273, 2])
We keep 7.55e+07/4.94e+09 =  1% of the original kernel matrix.

torch.Size([64641, 2])
We keep 1.85e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([2804, 2])
We keep 6.77e+04/9.60e+05 =  7% of the original kernel matrix.

torch.Size([11185, 2])
We keep 6.86e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([8854, 2])
We keep 6.03e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([17502, 2])
We keep 1.73e+06/7.48e+07 =  2% of the original kernel matrix.

torch.Size([7149, 2])
We keep 4.13e+05/8.67e+06 =  4% of the original kernel matrix.

torch.Size([15979, 2])
We keep 1.47e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([5847, 2])
We keep 3.31e+05/6.21e+06 =  5% of the original kernel matrix.

torch.Size([14753, 2])
We keep 1.36e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([3456, 2])
We keep 2.25e+05/2.93e+06 =  7% of the original kernel matrix.

torch.Size([10942, 2])
We keep 9.99e+05/3.48e+07 =  2% of the original kernel matrix.

torch.Size([1754, 2])
We keep 3.71e+04/3.93e+05 =  9% of the original kernel matrix.

torch.Size([9170, 2])
We keep 5.18e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([8059, 2])
We keep 4.91e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([16667, 2])
We keep 1.56e+06/6.61e+07 =  2% of the original kernel matrix.

torch.Size([1730, 2])
We keep 3.98e+04/4.60e+05 =  8% of the original kernel matrix.

torch.Size([9037, 2])
We keep 5.33e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([2465, 2])
We keep 5.57e+04/7.76e+05 =  7% of the original kernel matrix.

torch.Size([10599, 2])
We keep 6.38e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([5431, 2])
We keep 2.74e+05/5.15e+06 =  5% of the original kernel matrix.

torch.Size([14457, 2])
We keep 1.27e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([8930, 2])
We keep 5.31e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([17844, 2])
We keep 1.71e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([7049, 2])
We keep 4.04e+05/7.99e+06 =  5% of the original kernel matrix.

torch.Size([15835, 2])
We keep 1.43e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([15100, 2])
We keep 2.58e+06/6.99e+07 =  3% of the original kernel matrix.

torch.Size([22741, 2])
We keep 3.21e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([22177, 2])
We keep 3.69e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([28475, 2])
We keep 4.58e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([3073, 2])
We keep 9.29e+04/1.32e+06 =  7% of the original kernel matrix.

torch.Size([11207, 2])
We keep 7.63e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([10432, 2])
We keep 9.39e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([19089, 2])
We keep 2.25e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([5292, 2])
We keep 3.11e+05/5.35e+06 =  5% of the original kernel matrix.

torch.Size([14066, 2])
We keep 1.30e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([12864, 2])
We keep 1.03e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([21227, 2])
We keep 2.31e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([10014, 2])
We keep 1.35e+06/2.88e+07 =  4% of the original kernel matrix.

torch.Size([18439, 2])
We keep 2.32e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([3837, 2])
We keep 1.27e+05/1.98e+06 =  6% of the original kernel matrix.

torch.Size([12571, 2])
We keep 8.83e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([8025, 2])
We keep 4.04e+05/9.58e+06 =  4% of the original kernel matrix.

torch.Size([16867, 2])
We keep 1.51e+06/6.29e+07 =  2% of the original kernel matrix.

torch.Size([5549, 2])
We keep 3.23e+05/6.15e+06 =  5% of the original kernel matrix.

torch.Size([14406, 2])
We keep 1.36e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([6784, 2])
We keep 4.49e+05/9.36e+06 =  4% of the original kernel matrix.

torch.Size([15313, 2])
We keep 1.54e+06/6.22e+07 =  2% of the original kernel matrix.

torch.Size([12266, 2])
We keep 1.12e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([21183, 2])
We keep 2.48e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([3990, 2])
We keep 1.25e+05/2.17e+06 =  5% of the original kernel matrix.

torch.Size([12727, 2])
We keep 9.02e+05/3.00e+07 =  3% of the original kernel matrix.

torch.Size([10276, 2])
We keep 1.07e+06/2.67e+07 =  3% of the original kernel matrix.

torch.Size([18858, 2])
We keep 2.30e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([14324, 2])
We keep 1.19e+06/4.04e+07 =  2% of the original kernel matrix.

torch.Size([22333, 2])
We keep 2.57e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([6330, 2])
We keep 2.80e+05/6.07e+06 =  4% of the original kernel matrix.

torch.Size([15251, 2])
We keep 1.29e+06/5.01e+07 =  2% of the original kernel matrix.

torch.Size([4455, 2])
We keep 1.45e+05/2.62e+06 =  5% of the original kernel matrix.

torch.Size([13265, 2])
We keep 9.61e+05/3.29e+07 =  2% of the original kernel matrix.

torch.Size([3994, 2])
We keep 1.29e+05/2.26e+06 =  5% of the original kernel matrix.

torch.Size([12727, 2])
We keep 9.17e+05/3.06e+07 =  2% of the original kernel matrix.

torch.Size([7034, 2])
We keep 3.91e+05/8.05e+06 =  4% of the original kernel matrix.

torch.Size([15728, 2])
We keep 1.41e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([7946, 2])
We keep 4.28e+05/9.85e+06 =  4% of the original kernel matrix.

torch.Size([16848, 2])
We keep 1.54e+06/6.38e+07 =  2% of the original kernel matrix.

torch.Size([8283, 2])
We keep 5.03e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([17019, 2])
We keep 1.62e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([8489, 2])
We keep 8.95e+05/1.50e+07 =  5% of the original kernel matrix.

torch.Size([17207, 2])
We keep 1.70e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([8170, 2])
We keep 6.05e+05/1.18e+07 =  5% of the original kernel matrix.

torch.Size([16959, 2])
We keep 1.64e+06/6.99e+07 =  2% of the original kernel matrix.

torch.Size([4255, 2])
We keep 1.90e+05/3.15e+06 =  6% of the original kernel matrix.

torch.Size([12991, 2])
We keep 1.07e+06/3.61e+07 =  2% of the original kernel matrix.

torch.Size([6312, 2])
We keep 3.20e+05/6.73e+06 =  4% of the original kernel matrix.

torch.Size([14876, 2])
We keep 1.35e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([8476, 2])
We keep 7.27e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([16999, 2])
We keep 1.95e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([17656, 2])
We keep 2.84e+06/9.70e+07 =  2% of the original kernel matrix.

torch.Size([25067, 2])
We keep 3.75e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([2211, 2])
We keep 5.46e+04/5.96e+05 =  9% of the original kernel matrix.

torch.Size([10105, 2])
We keep 5.95e+05/1.57e+07 =  3% of the original kernel matrix.

torch.Size([1881, 2])
We keep 4.01e+04/4.41e+05 =  9% of the original kernel matrix.

torch.Size([9444, 2])
We keep 5.36e+05/1.35e+07 =  3% of the original kernel matrix.

torch.Size([5301, 2])
We keep 2.39e+05/4.55e+06 =  5% of the original kernel matrix.

torch.Size([14267, 2])
We keep 1.20e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([13714, 2])
We keep 1.71e+06/5.44e+07 =  3% of the original kernel matrix.

torch.Size([21677, 2])
We keep 2.97e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([4844, 2])
We keep 1.93e+05/3.44e+06 =  5% of the original kernel matrix.

torch.Size([13695, 2])
We keep 1.07e+06/3.77e+07 =  2% of the original kernel matrix.

torch.Size([3568, 2])
We keep 1.37e+05/2.18e+06 =  6% of the original kernel matrix.

torch.Size([11910, 2])
We keep 9.17e+05/3.00e+07 =  3% of the original kernel matrix.

torch.Size([6949, 2])
We keep 4.92e+05/8.74e+06 =  5% of the original kernel matrix.

torch.Size([15765, 2])
We keep 1.49e+06/6.01e+07 =  2% of the original kernel matrix.

torch.Size([4408, 2])
We keep 1.78e+05/2.96e+06 =  6% of the original kernel matrix.

torch.Size([13077, 2])
We keep 1.01e+06/3.50e+07 =  2% of the original kernel matrix.

torch.Size([5340, 2])
We keep 2.33e+05/4.19e+06 =  5% of the original kernel matrix.

torch.Size([14228, 2])
We keep 1.15e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([2440, 2])
We keep 5.69e+04/7.45e+05 =  7% of the original kernel matrix.

torch.Size([10558, 2])
We keep 6.32e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([6970, 2])
We keep 4.37e+05/8.20e+06 =  5% of the original kernel matrix.

torch.Size([15606, 2])
We keep 1.45e+06/5.82e+07 =  2% of the original kernel matrix.

torch.Size([2482, 2])
We keep 5.19e+04/7.21e+05 =  7% of the original kernel matrix.

torch.Size([10708, 2])
We keep 6.23e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([6476, 2])
We keep 3.09e+05/6.45e+06 =  4% of the original kernel matrix.

torch.Size([15310, 2])
We keep 1.32e+06/5.17e+07 =  2% of the original kernel matrix.

torch.Size([8008, 2])
We keep 5.64e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([16816, 2])
We keep 1.70e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([5849, 2])
We keep 2.22e+05/4.58e+06 =  4% of the original kernel matrix.

torch.Size([14793, 2])
We keep 1.16e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([3825, 2])
We keep 1.13e+05/1.83e+06 =  6% of the original kernel matrix.

torch.Size([12611, 2])
We keep 8.61e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([2832, 2])
We keep 7.12e+04/1.00e+06 =  7% of the original kernel matrix.

torch.Size([11098, 2])
We keep 6.90e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([15381, 2])
We keep 2.09e+06/6.11e+07 =  3% of the original kernel matrix.

torch.Size([23143, 2])
We keep 3.06e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([7149, 2])
We keep 5.08e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([16027, 2])
We keep 1.61e+06/6.50e+07 =  2% of the original kernel matrix.

torch.Size([7133, 2])
We keep 3.53e+05/7.93e+06 =  4% of the original kernel matrix.

torch.Size([16131, 2])
We keep 1.41e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([5473, 2])
We keep 2.80e+05/5.32e+06 =  5% of the original kernel matrix.

torch.Size([14513, 2])
We keep 1.29e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([12991, 2])
We keep 2.06e+06/5.27e+07 =  3% of the original kernel matrix.

torch.Size([20891, 2])
We keep 2.87e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([11756, 2])
We keep 1.25e+06/3.50e+07 =  3% of the original kernel matrix.

torch.Size([20010, 2])
We keep 2.53e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([6071, 2])
We keep 3.00e+05/6.28e+06 =  4% of the original kernel matrix.

torch.Size([15075, 2])
We keep 1.34e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([17220, 2])
We keep 2.55e+06/9.62e+07 =  2% of the original kernel matrix.

torch.Size([24698, 2])
We keep 3.73e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([9028, 2])
We keep 6.18e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([17755, 2])
We keep 1.76e+06/7.63e+07 =  2% of the original kernel matrix.

torch.Size([7878, 2])
We keep 5.04e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([16656, 2])
We keep 1.59e+06/6.68e+07 =  2% of the original kernel matrix.

torch.Size([4130, 2])
We keep 1.43e+05/2.31e+06 =  6% of the original kernel matrix.

torch.Size([12689, 2])
We keep 9.27e+05/3.09e+07 =  2% of the original kernel matrix.

torch.Size([9066, 2])
We keep 8.35e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([17632, 2])
We keep 1.91e+06/8.47e+07 =  2% of the original kernel matrix.

torch.Size([5559, 2])
We keep 2.32e+05/4.23e+06 =  5% of the original kernel matrix.

torch.Size([14624, 2])
We keep 1.15e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([8185, 2])
We keep 5.07e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([17175, 2])
We keep 1.73e+06/7.18e+07 =  2% of the original kernel matrix.

torch.Size([2309, 2])
We keep 5.98e+04/6.97e+05 =  8% of the original kernel matrix.

torch.Size([10255, 2])
We keep 6.26e+05/1.70e+07 =  3% of the original kernel matrix.

torch.Size([4652, 2])
We keep 1.91e+05/3.17e+06 =  6% of the original kernel matrix.

torch.Size([13245, 2])
We keep 1.03e+06/3.62e+07 =  2% of the original kernel matrix.

torch.Size([2091, 2])
We keep 6.67e+04/7.22e+05 =  9% of the original kernel matrix.

torch.Size([9937, 2])
We keep 6.51e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([4878, 2])
We keep 2.39e+05/3.84e+06 =  6% of the original kernel matrix.

torch.Size([13475, 2])
We keep 1.10e+06/3.99e+07 =  2% of the original kernel matrix.

torch.Size([5154, 2])
We keep 4.43e+05/5.71e+06 =  7% of the original kernel matrix.

torch.Size([13750, 2])
We keep 1.32e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([4038, 2])
We keep 1.60e+05/2.75e+06 =  5% of the original kernel matrix.

torch.Size([12345, 2])
We keep 9.78e+05/3.37e+07 =  2% of the original kernel matrix.

torch.Size([5349, 2])
We keep 2.83e+05/4.77e+06 =  5% of the original kernel matrix.

torch.Size([14183, 2])
We keep 1.19e+06/4.44e+07 =  2% of the original kernel matrix.

torch.Size([8462, 2])
We keep 7.71e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([17276, 2])
We keep 1.90e+06/8.24e+07 =  2% of the original kernel matrix.

torch.Size([5010, 2])
We keep 2.64e+05/4.73e+06 =  5% of the original kernel matrix.

torch.Size([13785, 2])
We keep 1.23e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([4357, 2])
We keep 1.55e+05/2.67e+06 =  5% of the original kernel matrix.

torch.Size([13097, 2])
We keep 9.69e+05/3.32e+07 =  2% of the original kernel matrix.

torch.Size([2945, 2])
We keep 7.28e+04/1.12e+06 =  6% of the original kernel matrix.

torch.Size([11275, 2])
We keep 7.20e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([13515, 2])
We keep 1.23e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([21652, 2])
We keep 2.58e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([2424, 2])
We keep 5.29e+04/7.38e+05 =  7% of the original kernel matrix.

torch.Size([10357, 2])
We keep 6.30e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([13726, 2])
We keep 1.30e+06/4.24e+07 =  3% of the original kernel matrix.

torch.Size([21992, 2])
We keep 2.70e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([3636, 2])
We keep 1.28e+05/1.98e+06 =  6% of the original kernel matrix.

torch.Size([12193, 2])
We keep 8.69e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([4441, 2])
We keep 1.80e+05/2.92e+06 =  6% of the original kernel matrix.

torch.Size([12934, 2])
We keep 9.98e+05/3.48e+07 =  2% of the original kernel matrix.

torch.Size([5991, 2])
We keep 2.71e+05/5.41e+06 =  5% of the original kernel matrix.

torch.Size([14928, 2])
We keep 1.24e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([5105, 2])
We keep 2.74e+05/4.44e+06 =  6% of the original kernel matrix.

torch.Size([13638, 2])
We keep 1.17e+06/4.28e+07 =  2% of the original kernel matrix.

torch.Size([6832, 2])
We keep 3.80e+05/8.21e+06 =  4% of the original kernel matrix.

torch.Size([15771, 2])
We keep 1.49e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([3271, 2])
We keep 9.63e+04/1.48e+06 =  6% of the original kernel matrix.

torch.Size([11666, 2])
We keep 7.94e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([5576, 2])
We keep 2.02e+05/4.26e+06 =  4% of the original kernel matrix.

torch.Size([14647, 2])
We keep 1.13e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([16611, 2])
We keep 2.19e+06/7.87e+07 =  2% of the original kernel matrix.

torch.Size([24312, 2])
We keep 3.45e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([5836, 2])
We keep 2.42e+05/4.84e+06 =  4% of the original kernel matrix.

torch.Size([14669, 2])
We keep 1.18e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([8467, 2])
We keep 4.73e+05/1.21e+07 =  3% of the original kernel matrix.

torch.Size([17327, 2])
We keep 1.65e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([3441, 2])
We keep 1.41e+05/1.85e+06 =  7% of the original kernel matrix.

torch.Size([11920, 2])
We keep 8.55e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([7645, 2])
We keep 5.25e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([16598, 2])
We keep 1.59e+06/6.58e+07 =  2% of the original kernel matrix.

torch.Size([3671, 2])
We keep 1.47e+05/2.13e+06 =  6% of the original kernel matrix.

torch.Size([11901, 2])
We keep 8.94e+05/2.97e+07 =  3% of the original kernel matrix.

torch.Size([4401, 2])
We keep 2.23e+05/3.90e+06 =  5% of the original kernel matrix.

torch.Size([13159, 2])
We keep 1.15e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([3270, 2])
We keep 9.30e+04/1.42e+06 =  6% of the original kernel matrix.

torch.Size([11725, 2])
We keep 7.78e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([3339, 2])
We keep 1.16e+05/1.68e+06 =  6% of the original kernel matrix.

torch.Size([11710, 2])
We keep 8.30e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([2930, 2])
We keep 8.21e+04/1.13e+06 =  7% of the original kernel matrix.

torch.Size([11049, 2])
We keep 7.24e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([2858, 2])
We keep 8.99e+04/1.31e+06 =  6% of the original kernel matrix.

torch.Size([11165, 2])
We keep 7.85e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([3778, 2])
We keep 1.22e+05/2.02e+06 =  6% of the original kernel matrix.

torch.Size([12364, 2])
We keep 8.89e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([6775, 2])
We keep 3.87e+05/8.10e+06 =  4% of the original kernel matrix.

torch.Size([15461, 2])
We keep 1.44e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([3534, 2])
We keep 1.23e+05/1.83e+06 =  6% of the original kernel matrix.

torch.Size([11927, 2])
We keep 8.50e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([11350, 2])
We keep 1.20e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([19681, 2])
We keep 2.53e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([20059, 2])
We keep 3.32e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([27161, 2])
We keep 4.34e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([9334, 2])
We keep 6.78e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([18138, 2])
We keep 1.80e+06/7.91e+07 =  2% of the original kernel matrix.

torch.Size([2837, 2])
We keep 6.61e+04/9.47e+05 =  6% of the original kernel matrix.

torch.Size([11133, 2])
We keep 6.82e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([3015, 2])
We keep 1.00e+05/1.41e+06 =  7% of the original kernel matrix.

torch.Size([11418, 2])
We keep 8.08e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([2690, 2])
We keep 9.26e+04/1.27e+06 =  7% of the original kernel matrix.

torch.Size([10922, 2])
We keep 7.79e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([3258, 2])
We keep 1.03e+05/1.62e+06 =  6% of the original kernel matrix.

torch.Size([11599, 2])
We keep 8.19e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([4692, 2])
We keep 2.43e+05/4.00e+06 =  6% of the original kernel matrix.

torch.Size([13436, 2])
We keep 1.12e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([6148, 2])
We keep 3.30e+05/6.12e+06 =  5% of the original kernel matrix.

torch.Size([14886, 2])
We keep 1.30e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([9108, 2])
We keep 6.87e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([17894, 2])
We keep 1.93e+06/8.38e+07 =  2% of the original kernel matrix.

torch.Size([6253, 2])
We keep 3.18e+05/6.58e+06 =  4% of the original kernel matrix.

torch.Size([15138, 2])
We keep 1.34e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([12339, 2])
We keep 2.90e+06/7.02e+07 =  4% of the original kernel matrix.

torch.Size([20413, 2])
We keep 3.29e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([6315, 2])
We keep 3.91e+05/7.40e+06 =  5% of the original kernel matrix.

torch.Size([14903, 2])
We keep 1.40e+06/5.53e+07 =  2% of the original kernel matrix.

torch.Size([29010, 2])
We keep 7.09e+06/3.01e+08 =  2% of the original kernel matrix.

torch.Size([31766, 2])
We keep 5.60e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([5787, 2])
We keep 2.29e+05/4.69e+06 =  4% of the original kernel matrix.

torch.Size([14686, 2])
We keep 1.17e+06/4.40e+07 =  2% of the original kernel matrix.

torch.Size([7243, 2])
We keep 4.44e+05/8.76e+06 =  5% of the original kernel matrix.

torch.Size([16093, 2])
We keep 1.48e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([6013, 2])
We keep 3.53e+05/6.76e+06 =  5% of the original kernel matrix.

torch.Size([14888, 2])
We keep 1.40e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([996, 2])
We keep 1.10e+04/9.30e+04 = 11% of the original kernel matrix.

torch.Size([7616, 2])
We keep 3.25e+05/6.20e+06 =  5% of the original kernel matrix.

torch.Size([13707, 2])
We keep 1.81e+06/5.64e+07 =  3% of the original kernel matrix.

torch.Size([22204, 2])
We keep 3.09e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([3293, 2])
We keep 1.18e+05/1.57e+06 =  7% of the original kernel matrix.

torch.Size([11587, 2])
We keep 8.13e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([12777, 2])
We keep 2.01e+06/4.15e+07 =  4% of the original kernel matrix.

torch.Size([20889, 2])
We keep 2.63e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([2204, 2])
We keep 5.01e+04/5.87e+05 =  8% of the original kernel matrix.

torch.Size([9956, 2])
We keep 5.84e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([6761, 2])
We keep 4.03e+05/8.58e+06 =  4% of the original kernel matrix.

torch.Size([15420, 2])
We keep 1.45e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([5575, 2])
We keep 2.78e+05/5.15e+06 =  5% of the original kernel matrix.

torch.Size([14377, 2])
We keep 1.21e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([5006, 2])
We keep 1.90e+05/3.37e+06 =  5% of the original kernel matrix.

torch.Size([13976, 2])
We keep 1.05e+06/3.73e+07 =  2% of the original kernel matrix.

torch.Size([6349, 2])
We keep 3.00e+05/6.03e+06 =  4% of the original kernel matrix.

torch.Size([15226, 2])
We keep 1.29e+06/4.99e+07 =  2% of the original kernel matrix.

torch.Size([8795, 2])
We keep 5.79e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([17620, 2])
We keep 1.79e+06/7.71e+07 =  2% of the original kernel matrix.

torch.Size([1631, 2])
We keep 2.84e+04/3.09e+05 =  9% of the original kernel matrix.

torch.Size([8999, 2])
We keep 4.83e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([2379, 2])
We keep 5.66e+04/7.43e+05 =  7% of the original kernel matrix.

torch.Size([10231, 2])
We keep 6.41e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([4529, 2])
We keep 2.11e+05/3.11e+06 =  6% of the original kernel matrix.

torch.Size([13177, 2])
We keep 1.03e+06/3.59e+07 =  2% of the original kernel matrix.

torch.Size([8939, 2])
We keep 6.57e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([17770, 2])
We keep 1.72e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([3132, 2])
We keep 9.93e+04/1.50e+06 =  6% of the original kernel matrix.

torch.Size([11724, 2])
We keep 8.22e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([9527, 2])
We keep 7.74e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([18278, 2])
We keep 2.03e+06/8.97e+07 =  2% of the original kernel matrix.

torch.Size([4679, 2])
We keep 1.81e+05/3.26e+06 =  5% of the original kernel matrix.

torch.Size([13444, 2])
We keep 1.04e+06/3.67e+07 =  2% of the original kernel matrix.

torch.Size([8244, 2])
We keep 5.55e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([17079, 2])
We keep 1.63e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([7159, 2])
We keep 3.57e+05/7.99e+06 =  4% of the original kernel matrix.

torch.Size([15971, 2])
We keep 1.42e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([10750, 2])
We keep 1.09e+06/2.48e+07 =  4% of the original kernel matrix.

torch.Size([19246, 2])
We keep 2.17e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([4903, 2])
We keep 1.86e+05/3.52e+06 =  5% of the original kernel matrix.

torch.Size([13657, 2])
We keep 1.07e+06/3.82e+07 =  2% of the original kernel matrix.

torch.Size([10557, 2])
We keep 9.95e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([19227, 2])
We keep 2.25e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([4874, 2])
We keep 2.20e+05/3.53e+06 =  6% of the original kernel matrix.

torch.Size([13642, 2])
We keep 1.07e+06/3.82e+07 =  2% of the original kernel matrix.

torch.Size([9165, 2])
We keep 8.75e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([17787, 2])
We keep 1.93e+06/8.69e+07 =  2% of the original kernel matrix.

torch.Size([4437, 2])
We keep 1.53e+05/2.72e+06 =  5% of the original kernel matrix.

torch.Size([13170, 2])
We keep 9.57e+05/3.35e+07 =  2% of the original kernel matrix.

torch.Size([6789, 2])
We keep 3.64e+05/7.63e+06 =  4% of the original kernel matrix.

torch.Size([15778, 2])
We keep 1.43e+06/5.62e+07 =  2% of the original kernel matrix.

torch.Size([8084, 2])
We keep 4.39e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([16878, 2])
We keep 1.58e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([4442, 2])
We keep 1.63e+05/2.65e+06 =  6% of the original kernel matrix.

torch.Size([13175, 2])
We keep 9.80e+05/3.31e+07 =  2% of the original kernel matrix.

torch.Size([11665, 2])
We keep 1.31e+06/3.20e+07 =  4% of the original kernel matrix.

torch.Size([19944, 2])
We keep 2.38e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([8399, 2])
We keep 5.59e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([17398, 2])
We keep 1.75e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([15850, 2])
We keep 2.18e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([23443, 2])
We keep 3.47e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([3814, 2])
We keep 1.26e+05/2.14e+06 =  5% of the original kernel matrix.

torch.Size([12323, 2])
We keep 9.02e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([1717, 2])
We keep 3.35e+04/3.83e+05 =  8% of the original kernel matrix.

torch.Size([9043, 2])
We keep 5.03e+05/1.26e+07 =  3% of the original kernel matrix.

torch.Size([4566, 2])
We keep 1.45e+05/2.75e+06 =  5% of the original kernel matrix.

torch.Size([13288, 2])
We keep 9.67e+05/3.37e+07 =  2% of the original kernel matrix.

torch.Size([4347, 2])
We keep 1.90e+05/3.17e+06 =  6% of the original kernel matrix.

torch.Size([13223, 2])
We keep 1.08e+06/3.62e+07 =  2% of the original kernel matrix.

torch.Size([8085, 2])
We keep 9.32e+05/1.74e+07 =  5% of the original kernel matrix.

torch.Size([16753, 2])
We keep 1.95e+06/8.49e+07 =  2% of the original kernel matrix.

torch.Size([4199, 2])
We keep 1.79e+05/2.93e+06 =  6% of the original kernel matrix.

torch.Size([12798, 2])
We keep 1.00e+06/3.48e+07 =  2% of the original kernel matrix.

torch.Size([4342, 2])
We keep 1.97e+05/3.03e+06 =  6% of the original kernel matrix.

torch.Size([13114, 2])
We keep 1.01e+06/3.54e+07 =  2% of the original kernel matrix.

torch.Size([3754, 2])
We keep 1.20e+05/1.91e+06 =  6% of the original kernel matrix.

torch.Size([12478, 2])
We keep 8.59e+05/2.81e+07 =  3% of the original kernel matrix.

torch.Size([10920, 2])
We keep 1.28e+06/2.41e+07 =  5% of the original kernel matrix.

torch.Size([19559, 2])
We keep 2.16e+06/9.98e+07 =  2% of the original kernel matrix.

torch.Size([2249, 2])
We keep 4.49e+04/5.91e+05 =  7% of the original kernel matrix.

torch.Size([10271, 2])
We keep 5.86e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([12505, 2])
We keep 9.99e+05/3.28e+07 =  3% of the original kernel matrix.

torch.Size([21343, 2])
We keep 2.41e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([5228, 2])
We keep 1.87e+05/3.54e+06 =  5% of the original kernel matrix.

torch.Size([14166, 2])
We keep 1.06e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([17846, 2])
We keep 2.16e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([25523, 2])
We keep 3.59e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([5876, 2])
We keep 3.72e+05/7.45e+06 =  4% of the original kernel matrix.

torch.Size([14712, 2])
We keep 1.47e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([8953, 2])
We keep 5.48e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([17569, 2])
We keep 1.71e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([2801, 2])
We keep 6.54e+04/9.41e+05 =  6% of the original kernel matrix.

torch.Size([11042, 2])
We keep 6.86e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([7378, 2])
We keep 6.51e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([16284, 2])
We keep 1.73e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([6598, 2])
We keep 3.87e+05/8.09e+06 =  4% of the original kernel matrix.

torch.Size([15583, 2])
We keep 1.49e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([5551, 2])
We keep 2.48e+05/4.72e+06 =  5% of the original kernel matrix.

torch.Size([14408, 2])
We keep 1.19e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([9682, 2])
We keep 7.04e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([18517, 2])
We keep 1.98e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([9971, 2])
We keep 1.17e+06/2.57e+07 =  4% of the original kernel matrix.

torch.Size([18579, 2])
We keep 2.28e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([5668, 2])
We keep 3.54e+05/5.97e+06 =  5% of the original kernel matrix.

torch.Size([14303, 2])
We keep 1.29e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([5230, 2])
We keep 1.82e+05/3.33e+06 =  5% of the original kernel matrix.

torch.Size([14257, 2])
We keep 1.05e+06/3.71e+07 =  2% of the original kernel matrix.

torch.Size([6829, 2])
We keep 3.45e+05/6.80e+06 =  5% of the original kernel matrix.

torch.Size([15734, 2])
We keep 1.35e+06/5.30e+07 =  2% of the original kernel matrix.

torch.Size([1542, 2])
We keep 3.78e+04/3.66e+05 = 10% of the original kernel matrix.

torch.Size([8747, 2])
We keep 5.31e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([2286, 2])
We keep 7.34e+04/7.76e+05 =  9% of the original kernel matrix.

torch.Size([10027, 2])
We keep 6.49e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([1580, 2])
We keep 2.65e+04/2.86e+05 =  9% of the original kernel matrix.

torch.Size([8886, 2])
We keep 4.56e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([3160, 2])
We keep 9.75e+04/1.44e+06 =  6% of the original kernel matrix.

torch.Size([11465, 2])
We keep 7.88e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([22347, 2])
We keep 5.58e+06/1.84e+08 =  3% of the original kernel matrix.

torch.Size([27576, 2])
We keep 4.71e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([3205, 2])
We keep 8.19e+04/1.25e+06 =  6% of the original kernel matrix.

torch.Size([11747, 2])
We keep 7.53e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([4913, 2])
We keep 4.30e+05/5.99e+06 =  7% of the original kernel matrix.

torch.Size([13098, 2])
We keep 1.30e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([2658, 2])
We keep 7.75e+04/1.05e+06 =  7% of the original kernel matrix.

torch.Size([10717, 2])
We keep 7.10e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([9138, 2])
We keep 6.31e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([17880, 2])
We keep 1.83e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([4143, 2])
We keep 2.97e+05/4.02e+06 =  7% of the original kernel matrix.

torch.Size([12192, 2])
We keep 1.13e+06/4.08e+07 =  2% of the original kernel matrix.

torch.Size([4719, 2])
We keep 2.55e+05/4.52e+06 =  5% of the original kernel matrix.

torch.Size([13065, 2])
We keep 1.19e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([5085, 2])
We keep 2.70e+05/4.23e+06 =  6% of the original kernel matrix.

torch.Size([13981, 2])
We keep 1.16e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([3081, 2])
We keep 8.24e+04/1.17e+06 =  7% of the original kernel matrix.

torch.Size([11459, 2])
We keep 7.38e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([3282, 2])
We keep 1.12e+05/1.62e+06 =  6% of the original kernel matrix.

torch.Size([11734, 2])
We keep 8.56e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([7796, 2])
We keep 4.42e+05/1.00e+07 =  4% of the original kernel matrix.

torch.Size([16529, 2])
We keep 1.56e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([2158, 2])
We keep 4.56e+04/5.48e+05 =  8% of the original kernel matrix.

torch.Size([9965, 2])
We keep 5.67e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([2939, 2])
We keep 1.10e+05/1.26e+06 =  8% of the original kernel matrix.

torch.Size([11052, 2])
We keep 7.63e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([4549, 2])
We keep 1.93e+05/3.24e+06 =  5% of the original kernel matrix.

torch.Size([13226, 2])
We keep 1.04e+06/3.66e+07 =  2% of the original kernel matrix.

torch.Size([4552, 2])
We keep 1.85e+05/3.05e+06 =  6% of the original kernel matrix.

torch.Size([13387, 2])
We keep 1.02e+06/3.55e+07 =  2% of the original kernel matrix.

torch.Size([8650, 2])
We keep 7.45e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([17495, 2])
We keep 1.86e+06/7.91e+07 =  2% of the original kernel matrix.

torch.Size([4480, 2])
We keep 1.82e+05/3.01e+06 =  6% of the original kernel matrix.

torch.Size([13009, 2])
We keep 1.01e+06/3.53e+07 =  2% of the original kernel matrix.

torch.Size([3632, 2])
We keep 1.52e+05/2.24e+06 =  6% of the original kernel matrix.

torch.Size([12043, 2])
We keep 9.11e+05/3.04e+07 =  2% of the original kernel matrix.

torch.Size([13425, 2])
We keep 1.24e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([22449, 2])
We keep 2.62e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([10772, 2])
We keep 1.02e+06/2.45e+07 =  4% of the original kernel matrix.

torch.Size([19507, 2])
We keep 2.20e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([1928, 2])
We keep 4.46e+04/5.07e+05 =  8% of the original kernel matrix.

torch.Size([9505, 2])
We keep 5.62e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([21670, 2])
We keep 4.86e+06/1.49e+08 =  3% of the original kernel matrix.

torch.Size([27803, 2])
We keep 4.28e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([51080, 2])
We keep 6.49e+07/1.85e+09 =  3% of the original kernel matrix.

torch.Size([41173, 2])
We keep 1.22e+07/8.74e+08 =  1% of the original kernel matrix.

torch.Size([5637, 2])
We keep 3.86e+05/6.01e+06 =  6% of the original kernel matrix.

torch.Size([14610, 2])
We keep 1.20e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([10569, 2])
We keep 1.30e+06/2.80e+07 =  4% of the original kernel matrix.

torch.Size([19279, 2])
We keep 2.35e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([1962, 2])
We keep 6.73e+04/5.96e+05 = 11% of the original kernel matrix.

torch.Size([9339, 2])
We keep 5.95e+05/1.57e+07 =  3% of the original kernel matrix.

torch.Size([7915, 2])
We keep 4.22e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([16971, 2])
We keep 1.57e+06/6.46e+07 =  2% of the original kernel matrix.

torch.Size([12637, 2])
We keep 1.13e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([20923, 2])
We keep 2.48e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([119360, 2])
We keep 2.06e+08/8.05e+09 =  2% of the original kernel matrix.

torch.Size([64137, 2])
We keep 2.26e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([6232, 2])
We keep 2.91e+05/5.83e+06 =  4% of the original kernel matrix.

torch.Size([15173, 2])
We keep 1.28e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([133127, 2])
We keep 9.96e+07/7.25e+09 =  1% of the original kernel matrix.

torch.Size([68721, 2])
We keep 2.19e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([36889, 2])
We keep 3.34e+07/5.84e+08 =  5% of the original kernel matrix.

torch.Size([35827, 2])
We keep 7.01e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([21101, 2])
We keep 5.37e+06/1.75e+08 =  3% of the original kernel matrix.

torch.Size([27180, 2])
We keep 4.67e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([21120, 2])
We keep 3.66e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([27770, 2])
We keep 4.10e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([3293, 2])
We keep 9.07e+04/1.40e+06 =  6% of the original kernel matrix.

torch.Size([11764, 2])
We keep 7.82e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([8376, 2])
We keep 6.65e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([17207, 2])
We keep 1.71e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([286186, 2])
We keep 4.16e+08/3.38e+10 =  1% of the original kernel matrix.

torch.Size([104008, 2])
We keep 4.29e+07/3.74e+09 =  1% of the original kernel matrix.

torch.Size([361589, 2])
We keep 4.86e+08/3.97e+10 =  1% of the original kernel matrix.

torch.Size([116867, 2])
We keep 4.41e+07/4.05e+09 =  1% of the original kernel matrix.

torch.Size([8021, 2])
We keep 7.71e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([16937, 2])
We keep 1.78e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([49591, 2])
We keep 1.80e+07/9.06e+08 =  1% of the original kernel matrix.

torch.Size([42219, 2])
We keep 9.04e+06/6.12e+08 =  1% of the original kernel matrix.

torch.Size([31929, 2])
We keep 1.83e+07/4.44e+08 =  4% of the original kernel matrix.

torch.Size([34535, 2])
We keep 6.78e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([25228, 2])
We keep 1.35e+07/2.60e+08 =  5% of the original kernel matrix.

torch.Size([30351, 2])
We keep 5.46e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([69796, 2])
We keep 7.58e+07/2.85e+09 =  2% of the original kernel matrix.

torch.Size([48194, 2])
We keep 1.47e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([36066, 2])
We keep 6.52e+07/1.02e+09 =  6% of the original kernel matrix.

torch.Size([32771, 2])
We keep 9.40e+06/6.49e+08 =  1% of the original kernel matrix.

torch.Size([30804, 2])
We keep 1.00e+07/3.76e+08 =  2% of the original kernel matrix.

torch.Size([32016, 2])
We keep 5.91e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([26122, 2])
We keep 5.95e+06/2.03e+08 =  2% of the original kernel matrix.

torch.Size([30576, 2])
We keep 4.79e+06/2.90e+08 =  1% of the original kernel matrix.

torch.Size([3679, 2])
We keep 1.09e+05/1.70e+06 =  6% of the original kernel matrix.

torch.Size([12293, 2])
We keep 8.19e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([116240, 2])
We keep 2.52e+08/1.08e+10 =  2% of the original kernel matrix.

torch.Size([62379, 2])
We keep 2.59e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([9811, 2])
We keep 1.78e+06/2.09e+07 =  8% of the original kernel matrix.

torch.Size([18586, 2])
We keep 1.94e+06/9.30e+07 =  2% of the original kernel matrix.

torch.Size([6470, 2])
We keep 3.01e+05/6.56e+06 =  4% of the original kernel matrix.

torch.Size([15341, 2])
We keep 1.32e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([19204, 2])
We keep 2.51e+06/8.79e+07 =  2% of the original kernel matrix.

torch.Size([26204, 2])
We keep 3.43e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([64843, 2])
We keep 5.15e+07/1.68e+09 =  3% of the original kernel matrix.

torch.Size([45717, 2])
We keep 1.15e+07/8.32e+08 =  1% of the original kernel matrix.

torch.Size([273485, 2])
We keep 4.36e+08/3.24e+10 =  1% of the original kernel matrix.

torch.Size([103366, 2])
We keep 4.19e+07/3.66e+09 =  1% of the original kernel matrix.

torch.Size([83062, 2])
We keep 3.89e+07/2.47e+09 =  1% of the original kernel matrix.

torch.Size([53621, 2])
We keep 1.36e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([3803, 2])
We keep 1.14e+05/1.83e+06 =  6% of the original kernel matrix.

torch.Size([12512, 2])
We keep 8.58e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([31062, 2])
We keep 8.78e+06/3.54e+08 =  2% of the original kernel matrix.

torch.Size([33346, 2])
We keep 6.12e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([15892, 2])
We keep 2.70e+06/8.41e+07 =  3% of the original kernel matrix.

torch.Size([23487, 2])
We keep 3.55e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([32965, 2])
We keep 1.12e+07/4.20e+08 =  2% of the original kernel matrix.

torch.Size([34195, 2])
We keep 6.46e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([69934, 2])
We keep 1.99e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([49550, 2])
We keep 1.08e+07/7.84e+08 =  1% of the original kernel matrix.

torch.Size([21360, 2])
We keep 8.52e+06/1.85e+08 =  4% of the original kernel matrix.

torch.Size([27677, 2])
We keep 4.83e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([42442, 2])
We keep 1.09e+07/5.13e+08 =  2% of the original kernel matrix.

torch.Size([39126, 2])
We keep 6.76e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([1906, 2])
We keep 3.09e+04/3.62e+05 =  8% of the original kernel matrix.

torch.Size([9546, 2])
We keep 4.89e+05/1.22e+07 =  3% of the original kernel matrix.

torch.Size([9291, 2])
We keep 5.66e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([18083, 2])
We keep 1.76e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([100810, 2])
We keep 6.28e+07/3.87e+09 =  1% of the original kernel matrix.

torch.Size([58630, 2])
We keep 1.66e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([44595, 2])
We keep 2.63e+07/9.80e+08 =  2% of the original kernel matrix.

torch.Size([39105, 2])
We keep 9.43e+06/6.36e+08 =  1% of the original kernel matrix.

torch.Size([70134, 2])
We keep 4.01e+07/1.74e+09 =  2% of the original kernel matrix.

torch.Size([48047, 2])
We keep 1.16e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([8608, 2])
We keep 6.16e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([17376, 2])
We keep 1.74e+06/7.64e+07 =  2% of the original kernel matrix.

torch.Size([290947, 2])
We keep 3.08e+08/3.60e+10 =  0% of the original kernel matrix.

torch.Size([104706, 2])
We keep 4.34e+07/3.86e+09 =  1% of the original kernel matrix.

torch.Size([17847, 2])
We keep 1.67e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([25294, 2])
We keep 3.13e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([35643, 2])
We keep 6.71e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([35803, 2])
We keep 6.04e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([132784, 2])
We keep 1.03e+08/7.30e+09 =  1% of the original kernel matrix.

torch.Size([68867, 2])
We keep 2.19e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([4349, 2])
We keep 1.85e+05/3.29e+06 =  5% of the original kernel matrix.

torch.Size([13062, 2])
We keep 1.07e+06/3.69e+07 =  2% of the original kernel matrix.

torch.Size([89234, 2])
We keep 4.34e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([55076, 2])
We keep 1.28e+07/9.51e+08 =  1% of the original kernel matrix.

torch.Size([196922, 2])
We keep 3.96e+08/1.90e+10 =  2% of the original kernel matrix.

torch.Size([86127, 2])
We keep 3.34e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([453795, 2])
We keep 1.17e+09/8.09e+10 =  1% of the original kernel matrix.

torch.Size([130345, 2])
We keep 6.37e+07/5.79e+09 =  1% of the original kernel matrix.

torch.Size([29930, 2])
We keep 7.04e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([31720, 2])
We keep 5.84e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([25193, 2])
We keep 4.27e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([30248, 2])
We keep 4.72e+06/2.80e+08 =  1% of the original kernel matrix.

torch.Size([8478, 2])
We keep 8.72e+05/1.48e+07 =  5% of the original kernel matrix.

torch.Size([17071, 2])
We keep 1.79e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([10085, 2])
We keep 6.09e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([18710, 2])
We keep 1.85e+06/8.40e+07 =  2% of the original kernel matrix.

torch.Size([129837, 2])
We keep 1.61e+08/8.57e+09 =  1% of the original kernel matrix.

torch.Size([67516, 2])
We keep 2.36e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([11623, 2])
We keep 1.20e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([20184, 2])
We keep 2.51e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([29420, 2])
We keep 9.08e+06/2.85e+08 =  3% of the original kernel matrix.

torch.Size([32364, 2])
We keep 5.54e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([43244, 2])
We keep 2.08e+07/7.05e+08 =  2% of the original kernel matrix.

torch.Size([39245, 2])
We keep 7.26e+06/5.40e+08 =  1% of the original kernel matrix.

torch.Size([23475, 2])
We keep 4.89e+06/1.45e+08 =  3% of the original kernel matrix.

torch.Size([29060, 2])
We keep 4.03e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([21413, 2])
We keep 7.27e+06/1.76e+08 =  4% of the original kernel matrix.

torch.Size([26988, 2])
We keep 4.61e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([8571, 2])
We keep 5.74e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([17736, 2])
We keep 1.79e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([154879, 2])
We keep 3.67e+08/2.15e+10 =  1% of the original kernel matrix.

torch.Size([71681, 2])
We keep 3.46e+07/2.99e+09 =  1% of the original kernel matrix.

torch.Size([15463, 2])
We keep 4.09e+06/7.30e+07 =  5% of the original kernel matrix.

torch.Size([23378, 2])
We keep 3.25e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([15514, 2])
We keep 9.73e+06/1.22e+08 =  7% of the original kernel matrix.

torch.Size([23023, 2])
We keep 4.14e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([25769, 2])
We keep 4.98e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([31334, 2])
We keep 5.21e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([63168, 2])
We keep 6.37e+07/2.29e+09 =  2% of the original kernel matrix.

torch.Size([46656, 2])
We keep 1.30e+07/9.74e+08 =  1% of the original kernel matrix.

torch.Size([19222, 2])
We keep 2.44e+06/9.51e+07 =  2% of the original kernel matrix.

torch.Size([26284, 2])
We keep 3.57e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([134121, 2])
We keep 3.14e+08/1.19e+10 =  2% of the original kernel matrix.

torch.Size([67577, 2])
We keep 2.74e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([10116, 2])
We keep 1.25e+06/2.19e+07 =  5% of the original kernel matrix.

torch.Size([19023, 2])
We keep 2.05e+06/9.52e+07 =  2% of the original kernel matrix.

torch.Size([28465, 2])
We keep 1.73e+07/5.27e+08 =  3% of the original kernel matrix.

torch.Size([30436, 2])
We keep 7.28e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([12219, 2])
We keep 1.17e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([20857, 2])
We keep 2.51e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([186350, 2])
We keep 5.77e+08/2.39e+10 =  2% of the original kernel matrix.

torch.Size([80696, 2])
We keep 3.67e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([42171, 2])
We keep 1.08e+07/7.22e+08 =  1% of the original kernel matrix.

torch.Size([38350, 2])
We keep 7.90e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([10771, 2])
We keep 7.39e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([19379, 2])
We keep 2.00e+06/9.33e+07 =  2% of the original kernel matrix.

torch.Size([160743, 2])
We keep 7.81e+07/7.73e+09 =  1% of the original kernel matrix.

torch.Size([76369, 2])
We keep 2.18e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([182616, 2])
We keep 1.21e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([81963, 2])
We keep 2.68e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([7994, 2])
We keep 5.43e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([16924, 2])
We keep 1.63e+06/6.87e+07 =  2% of the original kernel matrix.

torch.Size([149017, 2])
We keep 2.18e+08/7.71e+09 =  2% of the original kernel matrix.

torch.Size([73341, 2])
We keep 2.20e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([205147, 2])
We keep 1.61e+08/1.70e+10 =  0% of the original kernel matrix.

torch.Size([88069, 2])
We keep 3.16e+07/2.65e+09 =  1% of the original kernel matrix.

torch.Size([448972, 2])
We keep 6.30e+08/6.72e+10 =  0% of the original kernel matrix.

torch.Size([132280, 2])
We keep 5.70e+07/5.27e+09 =  1% of the original kernel matrix.

torch.Size([26310, 2])
We keep 1.79e+07/2.93e+08 =  6% of the original kernel matrix.

torch.Size([30654, 2])
We keep 5.61e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([4395, 2])
We keep 1.92e+05/3.34e+06 =  5% of the original kernel matrix.

torch.Size([12972, 2])
We keep 1.04e+06/3.72e+07 =  2% of the original kernel matrix.

torch.Size([13810, 2])
We keep 1.09e+06/3.65e+07 =  2% of the original kernel matrix.

torch.Size([22029, 2])
We keep 2.47e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([9242, 2])
We keep 8.30e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([18296, 2])
We keep 2.01e+06/9.40e+07 =  2% of the original kernel matrix.

torch.Size([23892, 2])
We keep 3.50e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([29339, 2])
We keep 4.20e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([26786, 2])
We keep 8.58e+06/2.14e+08 =  4% of the original kernel matrix.

torch.Size([31608, 2])
We keep 5.07e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([3596, 2])
We keep 1.03e+05/1.68e+06 =  6% of the original kernel matrix.

torch.Size([12149, 2])
We keep 8.23e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([71698, 2])
We keep 3.70e+07/1.82e+09 =  2% of the original kernel matrix.

torch.Size([50080, 2])
We keep 1.20e+07/8.67e+08 =  1% of the original kernel matrix.

torch.Size([122309, 2])
We keep 7.35e+07/5.74e+09 =  1% of the original kernel matrix.

torch.Size([65884, 2])
We keep 1.97e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([5261, 2])
We keep 4.02e+06/2.31e+07 = 17% of the original kernel matrix.

torch.Size([13095, 2])
We keep 1.83e+06/9.78e+07 =  1% of the original kernel matrix.

torch.Size([12337, 2])
We keep 1.21e+06/3.33e+07 =  3% of the original kernel matrix.

torch.Size([20743, 2])
We keep 2.38e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([30299, 2])
We keep 7.04e+06/2.83e+08 =  2% of the original kernel matrix.

torch.Size([32812, 2])
We keep 5.34e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([17421, 2])
We keep 2.31e+06/8.57e+07 =  2% of the original kernel matrix.

torch.Size([25075, 2])
We keep 3.51e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([16461, 2])
We keep 4.26e+06/1.02e+08 =  4% of the original kernel matrix.

torch.Size([24023, 2])
We keep 3.82e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([16608, 2])
We keep 2.07e+06/7.78e+07 =  2% of the original kernel matrix.

torch.Size([24420, 2])
We keep 3.41e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([7719, 2])
We keep 3.88e+05/8.81e+06 =  4% of the original kernel matrix.

torch.Size([16655, 2])
We keep 1.46e+06/6.04e+07 =  2% of the original kernel matrix.

torch.Size([42792, 2])
We keep 4.88e+07/9.60e+08 =  5% of the original kernel matrix.

torch.Size([38223, 2])
We keep 8.90e+06/6.30e+08 =  1% of the original kernel matrix.

torch.Size([52598, 2])
We keep 1.40e+07/7.97e+08 =  1% of the original kernel matrix.

torch.Size([43227, 2])
We keep 8.27e+06/5.74e+08 =  1% of the original kernel matrix.

torch.Size([24721, 2])
We keep 5.16e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([30155, 2])
We keep 4.80e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([63097, 2])
We keep 4.70e+07/1.91e+09 =  2% of the original kernel matrix.

torch.Size([46582, 2])
We keep 1.26e+07/8.88e+08 =  1% of the original kernel matrix.

torch.Size([14265, 2])
We keep 1.69e+06/4.93e+07 =  3% of the original kernel matrix.

torch.Size([22247, 2])
We keep 2.80e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([462715, 2])
We keep 8.80e+08/8.18e+10 =  1% of the original kernel matrix.

torch.Size([132636, 2])
We keep 6.21e+07/5.82e+09 =  1% of the original kernel matrix.

torch.Size([16416, 2])
We keep 2.18e+06/7.03e+07 =  3% of the original kernel matrix.

torch.Size([24071, 2])
We keep 3.24e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([84236, 2])
We keep 3.82e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([54204, 2])
We keep 1.39e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([3914, 2])
We keep 1.32e+05/2.24e+06 =  5% of the original kernel matrix.

torch.Size([12488, 2])
We keep 9.17e+05/3.05e+07 =  3% of the original kernel matrix.

torch.Size([6415, 2])
We keep 2.79e+05/5.63e+06 =  4% of the original kernel matrix.

torch.Size([15369, 2])
We keep 1.26e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([7142, 2])
We keep 3.58e+05/8.38e+06 =  4% of the original kernel matrix.

torch.Size([16116, 2])
We keep 1.43e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([18095, 2])
We keep 2.13e+06/7.06e+07 =  3% of the original kernel matrix.

torch.Size([25490, 2])
We keep 3.05e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([5629, 2])
We keep 3.58e+05/6.21e+06 =  5% of the original kernel matrix.

torch.Size([14383, 2])
We keep 1.37e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([72040, 2])
We keep 7.32e+07/1.99e+09 =  3% of the original kernel matrix.

torch.Size([49968, 2])
We keep 1.25e+07/9.06e+08 =  1% of the original kernel matrix.

torch.Size([7142, 2])
We keep 3.86e+05/8.57e+06 =  4% of the original kernel matrix.

torch.Size([15947, 2])
We keep 1.47e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([5505, 2])
We keep 2.79e+05/5.69e+06 =  4% of the original kernel matrix.

torch.Size([14285, 2])
We keep 1.29e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([234786, 2])
We keep 2.81e+08/2.33e+10 =  1% of the original kernel matrix.

torch.Size([95838, 2])
We keep 3.67e+07/3.10e+09 =  1% of the original kernel matrix.

torch.Size([60157, 2])
We keep 2.99e+07/1.37e+09 =  2% of the original kernel matrix.

torch.Size([46093, 2])
We keep 1.08e+07/7.54e+08 =  1% of the original kernel matrix.

torch.Size([40974, 2])
We keep 1.09e+07/6.04e+08 =  1% of the original kernel matrix.

torch.Size([38624, 2])
We keep 7.66e+06/5.00e+08 =  1% of the original kernel matrix.

torch.Size([147604, 2])
We keep 1.08e+08/8.52e+09 =  1% of the original kernel matrix.

torch.Size([73805, 2])
We keep 2.34e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([379502, 2])
We keep 4.54e+08/5.13e+10 =  0% of the original kernel matrix.

torch.Size([121007, 2])
We keep 5.15e+07/4.60e+09 =  1% of the original kernel matrix.

torch.Size([12531, 2])
We keep 1.19e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([21053, 2])
We keep 2.57e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([15820, 2])
We keep 1.90e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([23698, 2])
We keep 3.24e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([16303, 2])
We keep 4.96e+06/1.22e+08 =  4% of the original kernel matrix.

torch.Size([24086, 2])
We keep 4.05e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([3037, 2])
We keep 7.66e+04/1.14e+06 =  6% of the original kernel matrix.

torch.Size([11371, 2])
We keep 7.32e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([6456, 2])
We keep 2.97e+05/6.43e+06 =  4% of the original kernel matrix.

torch.Size([15167, 2])
We keep 1.31e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([23719, 2])
We keep 3.87e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([29300, 2])
We keep 4.29e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([7286, 2])
We keep 4.68e+05/9.16e+06 =  5% of the original kernel matrix.

torch.Size([16175, 2])
We keep 1.45e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([7175, 2])
We keep 3.30e+05/7.54e+06 =  4% of the original kernel matrix.

torch.Size([16076, 2])
We keep 1.39e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([7681, 2])
We keep 4.04e+05/9.02e+06 =  4% of the original kernel matrix.

torch.Size([16502, 2])
We keep 1.49e+06/6.11e+07 =  2% of the original kernel matrix.

torch.Size([11375, 2])
We keep 1.16e+06/2.70e+07 =  4% of the original kernel matrix.

torch.Size([19791, 2])
We keep 2.18e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([345509, 2])
We keep 6.43e+08/5.02e+10 =  1% of the original kernel matrix.

torch.Size([113966, 2])
We keep 5.00e+07/4.56e+09 =  1% of the original kernel matrix.

torch.Size([17256, 2])
We keep 5.16e+06/1.61e+08 =  3% of the original kernel matrix.

torch.Size([23783, 2])
We keep 4.44e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([89351, 2])
We keep 7.84e+07/3.21e+09 =  2% of the original kernel matrix.

torch.Size([55962, 2])
We keep 1.55e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([7433, 2])
We keep 1.19e+06/1.69e+07 =  7% of the original kernel matrix.

torch.Size([16206, 2])
We keep 1.90e+06/8.36e+07 =  2% of the original kernel matrix.

torch.Size([6859, 2])
We keep 6.12e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([15550, 2])
We keep 1.68e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([44383, 2])
We keep 1.27e+07/6.65e+08 =  1% of the original kernel matrix.

torch.Size([39457, 2])
We keep 7.84e+06/5.24e+08 =  1% of the original kernel matrix.

torch.Size([210357, 2])
We keep 6.22e+08/2.32e+10 =  2% of the original kernel matrix.

torch.Size([88938, 2])
We keep 3.30e+07/3.10e+09 =  1% of the original kernel matrix.

torch.Size([472912, 2])
We keep 5.06e+08/6.01e+10 =  0% of the original kernel matrix.

torch.Size([133666, 2])
We keep 5.36e+07/4.99e+09 =  1% of the original kernel matrix.

torch.Size([18022, 2])
We keep 2.92e+06/9.04e+07 =  3% of the original kernel matrix.

torch.Size([25517, 2])
We keep 3.46e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([46063, 2])
We keep 1.33e+07/6.77e+08 =  1% of the original kernel matrix.

torch.Size([40788, 2])
We keep 7.86e+06/5.29e+08 =  1% of the original kernel matrix.

torch.Size([5607, 2])
We keep 2.35e+05/4.55e+06 =  5% of the original kernel matrix.

torch.Size([14537, 2])
We keep 1.16e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([9055, 2])
We keep 1.55e+06/2.11e+07 =  7% of the original kernel matrix.

torch.Size([17822, 2])
We keep 2.02e+06/9.35e+07 =  2% of the original kernel matrix.

torch.Size([581779, 2])
We keep 3.18e+09/1.86e+11 =  1% of the original kernel matrix.

torch.Size([146785, 2])
We keep 9.24e+07/8.77e+09 =  1% of the original kernel matrix.

torch.Size([21927, 2])
We keep 4.69e+06/1.55e+08 =  3% of the original kernel matrix.

torch.Size([28596, 2])
We keep 4.42e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([15307, 2])
We keep 1.52e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([23051, 2])
We keep 2.85e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([42649, 2])
We keep 1.95e+07/8.17e+08 =  2% of the original kernel matrix.

torch.Size([38559, 2])
We keep 8.66e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([732498, 2])
We keep 2.17e+09/2.24e+11 =  0% of the original kernel matrix.

torch.Size([168933, 2])
We keep 1.01e+08/9.62e+09 =  1% of the original kernel matrix.

torch.Size([35114, 2])
We keep 8.38e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([34777, 2])
We keep 6.55e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([12572, 2])
We keep 1.22e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([20698, 2])
We keep 2.44e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([11856, 2])
We keep 1.87e+06/4.14e+07 =  4% of the original kernel matrix.

torch.Size([20279, 2])
We keep 2.71e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([33966, 2])
We keep 7.88e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([35715, 2])
We keep 6.83e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([7593, 2])
We keep 6.94e+05/9.17e+06 =  7% of the original kernel matrix.

torch.Size([16673, 2])
We keep 1.43e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([162363, 2])
We keep 1.56e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([77084, 2])
We keep 2.60e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([18287, 2])
We keep 4.11e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([25475, 2])
We keep 3.99e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([2349, 2])
We keep 4.63e+04/6.08e+05 =  7% of the original kernel matrix.

torch.Size([10353, 2])
We keep 5.86e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([51191, 2])
We keep 1.69e+07/7.75e+08 =  2% of the original kernel matrix.

torch.Size([42863, 2])
We keep 7.94e+06/5.66e+08 =  1% of the original kernel matrix.

torch.Size([22886, 2])
We keep 3.82e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([29203, 2])
We keep 4.75e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([3053, 2])
We keep 9.08e+04/1.25e+06 =  7% of the original kernel matrix.

torch.Size([11220, 2])
We keep 7.62e+05/2.27e+07 =  3% of the original kernel matrix.

torch.Size([95831, 2])
We keep 6.15e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([57307, 2])
We keep 1.56e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([105609, 2])
We keep 7.68e+07/4.15e+09 =  1% of the original kernel matrix.

torch.Size([60929, 2])
We keep 1.69e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([214148, 2])
We keep 3.57e+08/1.99e+10 =  1% of the original kernel matrix.

torch.Size([90067, 2])
We keep 3.39e+07/2.87e+09 =  1% of the original kernel matrix.

torch.Size([197365, 2])
We keep 2.86e+08/1.78e+10 =  1% of the original kernel matrix.

torch.Size([85021, 2])
We keep 3.23e+07/2.71e+09 =  1% of the original kernel matrix.

torch.Size([222778, 2])
We keep 3.00e+08/2.10e+10 =  1% of the original kernel matrix.

torch.Size([89899, 2])
We keep 3.45e+07/2.95e+09 =  1% of the original kernel matrix.

torch.Size([24838, 2])
We keep 6.89e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([29845, 2])
We keep 5.42e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([23996, 2])
We keep 5.36e+06/1.88e+08 =  2% of the original kernel matrix.

torch.Size([29206, 2])
We keep 4.61e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([85214, 2])
We keep 3.77e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([53976, 2])
We keep 1.36e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([169211, 2])
We keep 1.10e+08/7.85e+09 =  1% of the original kernel matrix.

torch.Size([78613, 2])
We keep 2.17e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([10861, 2])
We keep 1.20e+06/2.75e+07 =  4% of the original kernel matrix.

torch.Size([19559, 2])
We keep 2.30e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([48041, 2])
We keep 2.49e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([41245, 2])
We keep 8.89e+06/6.64e+08 =  1% of the original kernel matrix.

torch.Size([122538, 2])
We keep 1.19e+08/7.13e+09 =  1% of the original kernel matrix.

torch.Size([65700, 2])
We keep 2.10e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([116175, 2])
We keep 1.46e+08/6.22e+09 =  2% of the original kernel matrix.

torch.Size([64153, 2])
We keep 1.99e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([98456, 2])
We keep 7.67e+07/3.75e+09 =  2% of the original kernel matrix.

torch.Size([58296, 2])
We keep 1.64e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([54052, 2])
We keep 1.07e+08/2.08e+09 =  5% of the original kernel matrix.

torch.Size([41917, 2])
We keep 1.28e+07/9.28e+08 =  1% of the original kernel matrix.

torch.Size([14947, 2])
We keep 1.78e+06/5.57e+07 =  3% of the original kernel matrix.

torch.Size([22842, 2])
We keep 2.93e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([8782, 2])
We keep 5.05e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([17581, 2])
We keep 1.66e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([18010, 2])
We keep 5.75e+06/1.31e+08 =  4% of the original kernel matrix.

torch.Size([25087, 2])
We keep 4.18e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([129884, 2])
We keep 7.21e+07/6.28e+09 =  1% of the original kernel matrix.

torch.Size([68444, 2])
We keep 2.02e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([119151, 2])
We keep 5.87e+07/4.96e+09 =  1% of the original kernel matrix.

torch.Size([65126, 2])
We keep 1.84e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([258517, 2])
We keep 5.12e+08/3.57e+10 =  1% of the original kernel matrix.

torch.Size([97261, 2])
We keep 4.37e+07/3.84e+09 =  1% of the original kernel matrix.

torch.Size([127976, 2])
We keep 1.32e+08/7.61e+09 =  1% of the original kernel matrix.

torch.Size([67834, 2])
We keep 2.25e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([59257, 2])
We keep 2.87e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([45793, 2])
We keep 1.09e+07/7.71e+08 =  1% of the original kernel matrix.

torch.Size([19538, 2])
We keep 3.67e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([26060, 2])
We keep 4.45e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([246644, 2])
We keep 2.91e+08/2.38e+10 =  1% of the original kernel matrix.

torch.Size([98437, 2])
We keep 3.71e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([4552, 2])
We keep 3.13e+05/4.09e+06 =  7% of the original kernel matrix.

torch.Size([13186, 2])
We keep 1.17e+06/4.11e+07 =  2% of the original kernel matrix.

torch.Size([11837, 2])
We keep 1.51e+06/3.70e+07 =  4% of the original kernel matrix.

torch.Size([20343, 2])
We keep 2.59e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([192108, 2])
We keep 1.79e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([84291, 2])
We keep 2.94e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([22241, 2])
We keep 3.81e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([28356, 2])
We keep 4.17e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([9201, 2])
We keep 7.22e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([17864, 2])
We keep 1.83e+06/8.03e+07 =  2% of the original kernel matrix.

torch.Size([25733, 2])
We keep 4.02e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([30441, 2])
We keep 4.73e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([458391, 2])
We keep 1.93e+09/9.72e+10 =  1% of the original kernel matrix.

torch.Size([130777, 2])
We keep 6.91e+07/6.34e+09 =  1% of the original kernel matrix.

torch.Size([21827, 2])
We keep 3.25e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([27919, 2])
We keep 4.14e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([97117, 2])
We keep 6.48e+07/3.24e+09 =  2% of the original kernel matrix.

torch.Size([57998, 2])
We keep 1.54e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([4205, 2])
We keep 1.61e+05/2.68e+06 =  5% of the original kernel matrix.

torch.Size([12801, 2])
We keep 9.76e+05/3.33e+07 =  2% of the original kernel matrix.

torch.Size([7561, 2])
We keep 4.47e+05/9.64e+06 =  4% of the original kernel matrix.

torch.Size([16366, 2])
We keep 1.54e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([6402, 2])
We keep 4.34e+05/8.64e+06 =  5% of the original kernel matrix.

torch.Size([15139, 2])
We keep 1.53e+06/5.98e+07 =  2% of the original kernel matrix.

torch.Size([10609, 2])
We keep 1.77e+06/3.43e+07 =  5% of the original kernel matrix.

torch.Size([19131, 2])
We keep 2.52e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([25740, 2])
We keep 5.16e+06/2.35e+08 =  2% of the original kernel matrix.

torch.Size([31036, 2])
We keep 5.20e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([70646, 2])
We keep 4.53e+07/1.67e+09 =  2% of the original kernel matrix.

torch.Size([48854, 2])
We keep 1.16e+07/8.32e+08 =  1% of the original kernel matrix.

torch.Size([6141, 2])
We keep 3.23e+05/6.66e+06 =  4% of the original kernel matrix.

torch.Size([15246, 2])
We keep 1.37e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([122525, 2])
We keep 9.39e+07/4.52e+09 =  2% of the original kernel matrix.

torch.Size([65927, 2])
We keep 1.67e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([7236, 2])
We keep 4.58e+05/9.14e+06 =  5% of the original kernel matrix.

torch.Size([16034, 2])
We keep 1.49e+06/6.15e+07 =  2% of the original kernel matrix.

torch.Size([66980, 2])
We keep 3.18e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([48848, 2])
We keep 1.21e+07/8.49e+08 =  1% of the original kernel matrix.

torch.Size([12269, 2])
We keep 1.76e+06/4.10e+07 =  4% of the original kernel matrix.

torch.Size([20466, 2])
We keep 2.66e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([101403, 2])
We keep 4.02e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([58954, 2])
We keep 1.41e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([9878, 2])
We keep 1.40e+06/2.58e+07 =  5% of the original kernel matrix.

torch.Size([18374, 2])
We keep 2.26e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([23326, 2])
We keep 3.78e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([29029, 2])
We keep 4.38e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([27100, 2])
We keep 5.18e+06/2.36e+08 =  2% of the original kernel matrix.

torch.Size([31737, 2])
We keep 5.23e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([24424, 2])
We keep 9.38e+06/2.55e+08 =  3% of the original kernel matrix.

torch.Size([29544, 2])
We keep 5.45e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([82868, 2])
We keep 4.22e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([54136, 2])
We keep 1.38e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([331474, 2])
We keep 8.85e+08/4.64e+10 =  1% of the original kernel matrix.

torch.Size([110559, 2])
We keep 4.68e+07/4.38e+09 =  1% of the original kernel matrix.

torch.Size([8810, 2])
We keep 5.72e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([17689, 2])
We keep 1.78e+06/7.63e+07 =  2% of the original kernel matrix.

torch.Size([6339, 2])
We keep 8.08e+05/1.01e+07 =  7% of the original kernel matrix.

torch.Size([15120, 2])
We keep 1.57e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([74056, 2])
We keep 4.40e+07/1.66e+09 =  2% of the original kernel matrix.

torch.Size([50760, 2])
We keep 1.14e+07/8.28e+08 =  1% of the original kernel matrix.

torch.Size([18942, 2])
We keep 2.88e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([26035, 2])
We keep 3.93e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([618517, 2])
We keep 9.96e+08/1.32e+11 =  0% of the original kernel matrix.

torch.Size([155414, 2])
We keep 7.80e+07/7.38e+09 =  1% of the original kernel matrix.

torch.Size([2942, 2])
We keep 9.19e+04/1.19e+06 =  7% of the original kernel matrix.

torch.Size([11248, 2])
We keep 7.34e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([122499, 2])
We keep 7.65e+07/5.71e+09 =  1% of the original kernel matrix.

torch.Size([65798, 2])
We keep 1.95e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([257747, 2])
We keep 2.12e+08/2.54e+10 =  0% of the original kernel matrix.

torch.Size([100607, 2])
We keep 3.71e+07/3.24e+09 =  1% of the original kernel matrix.

torch.Size([6916, 2])
We keep 4.15e+05/8.85e+06 =  4% of the original kernel matrix.

torch.Size([15883, 2])
We keep 1.54e+06/6.05e+07 =  2% of the original kernel matrix.

torch.Size([119753, 2])
We keep 1.61e+08/7.02e+09 =  2% of the original kernel matrix.

torch.Size([63888, 2])
We keep 2.02e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([16617, 2])
We keep 4.41e+06/1.32e+08 =  3% of the original kernel matrix.

torch.Size([23911, 2])
We keep 4.19e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([695214, 2])
We keep 1.68e+09/1.70e+11 =  0% of the original kernel matrix.

torch.Size([167213, 2])
We keep 8.87e+07/8.38e+09 =  1% of the original kernel matrix.

torch.Size([16420, 2])
We keep 1.76e+06/5.87e+07 =  2% of the original kernel matrix.

torch.Size([24008, 2])
We keep 2.94e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([16583, 2])
We keep 2.01e+06/6.69e+07 =  3% of the original kernel matrix.

torch.Size([24026, 2])
We keep 3.12e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([9903, 2])
We keep 2.42e+06/3.92e+07 =  6% of the original kernel matrix.

torch.Size([18362, 2])
We keep 2.69e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([5659, 2])
We keep 2.42e+05/4.65e+06 =  5% of the original kernel matrix.

torch.Size([14417, 2])
We keep 1.17e+06/4.39e+07 =  2% of the original kernel matrix.

torch.Size([15537, 2])
We keep 2.25e+06/6.78e+07 =  3% of the original kernel matrix.

torch.Size([23128, 2])
We keep 3.15e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([35632, 2])
We keep 2.72e+07/3.92e+08 =  6% of the original kernel matrix.

torch.Size([35698, 2])
We keep 5.92e+06/4.03e+08 =  1% of the original kernel matrix.

torch.Size([15391, 2])
We keep 6.44e+06/6.18e+07 = 10% of the original kernel matrix.

torch.Size([23431, 2])
We keep 3.05e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([82177, 2])
We keep 3.89e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([53459, 2])
We keep 1.40e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([20765, 2])
We keep 3.26e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([27479, 2])
We keep 4.36e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([5641, 2])
We keep 2.95e+05/4.72e+06 =  6% of the original kernel matrix.

torch.Size([14559, 2])
We keep 1.18e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([21747, 2])
We keep 3.16e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([27885, 2])
We keep 4.06e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([178237, 2])
We keep 3.29e+08/2.11e+10 =  1% of the original kernel matrix.

torch.Size([80622, 2])
We keep 3.47e+07/2.96e+09 =  1% of the original kernel matrix.

torch.Size([58656, 2])
We keep 4.72e+07/1.74e+09 =  2% of the original kernel matrix.

torch.Size([45388, 2])
We keep 1.19e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([302751, 2])
We keep 3.77e+08/3.73e+10 =  1% of the original kernel matrix.

torch.Size([109166, 2])
We keep 4.44e+07/3.93e+09 =  1% of the original kernel matrix.

torch.Size([15711, 2])
We keep 2.81e+06/6.90e+07 =  4% of the original kernel matrix.

torch.Size([23717, 2])
We keep 3.15e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([9097, 2])
We keep 1.00e+06/2.33e+07 =  4% of the original kernel matrix.

torch.Size([17939, 2])
We keep 2.11e+06/9.82e+07 =  2% of the original kernel matrix.

torch.Size([8108, 2])
We keep 4.85e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([17266, 2])
We keep 1.65e+06/6.92e+07 =  2% of the original kernel matrix.

torch.Size([8918, 2])
We keep 1.87e+06/2.00e+07 =  9% of the original kernel matrix.

torch.Size([17694, 2])
We keep 1.96e+06/9.10e+07 =  2% of the original kernel matrix.

torch.Size([159197, 2])
We keep 3.24e+08/1.14e+10 =  2% of the original kernel matrix.

torch.Size([76520, 2])
We keep 2.54e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([13921, 2])
We keep 1.31e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([22042, 2])
We keep 2.61e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([23353, 2])
We keep 3.55e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([29176, 2])
We keep 4.23e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([37965, 2])
We keep 1.02e+07/4.45e+08 =  2% of the original kernel matrix.

torch.Size([36569, 2])
We keep 6.64e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([20446, 2])
We keep 7.28e+06/2.02e+08 =  3% of the original kernel matrix.

torch.Size([26331, 2])
We keep 4.84e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([8243, 2])
We keep 5.67e+05/1.12e+07 =  5% of the original kernel matrix.

torch.Size([17303, 2])
We keep 1.61e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([11733, 2])
We keep 1.32e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([20076, 2])
We keep 2.57e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([235970, 2])
We keep 3.86e+08/2.79e+10 =  1% of the original kernel matrix.

torch.Size([93076, 2])
We keep 3.94e+07/3.39e+09 =  1% of the original kernel matrix.

torch.Size([31244, 2])
We keep 7.81e+06/2.91e+08 =  2% of the original kernel matrix.

torch.Size([34218, 2])
We keep 5.66e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([12439, 2])
We keep 1.31e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([20848, 2])
We keep 2.50e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([19014, 2])
We keep 2.62e+06/8.90e+07 =  2% of the original kernel matrix.

torch.Size([26179, 2])
We keep 3.48e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([95312, 2])
We keep 4.85e+08/5.98e+09 =  8% of the original kernel matrix.

torch.Size([56806, 2])
We keep 2.02e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([94094, 2])
We keep 4.54e+07/3.03e+09 =  1% of the original kernel matrix.

torch.Size([56968, 2])
We keep 1.49e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([209102, 2])
We keep 5.67e+08/3.43e+10 =  1% of the original kernel matrix.

torch.Size([84729, 2])
We keep 4.32e+07/3.76e+09 =  1% of the original kernel matrix.

torch.Size([98230, 2])
We keep 5.17e+07/3.36e+09 =  1% of the original kernel matrix.

torch.Size([58515, 2])
We keep 1.55e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([8536, 2])
We keep 9.30e+05/1.44e+07 =  6% of the original kernel matrix.

torch.Size([17608, 2])
We keep 1.68e+06/7.72e+07 =  2% of the original kernel matrix.

torch.Size([51399, 2])
We keep 1.23e+07/7.62e+08 =  1% of the original kernel matrix.

torch.Size([42733, 2])
We keep 8.01e+06/5.61e+08 =  1% of the original kernel matrix.

torch.Size([31719, 2])
We keep 2.90e+07/6.27e+08 =  4% of the original kernel matrix.

torch.Size([31884, 2])
We keep 7.73e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([13018, 2])
We keep 1.30e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([21424, 2])
We keep 2.37e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([14990, 2])
We keep 1.76e+06/6.71e+07 =  2% of the original kernel matrix.

torch.Size([22886, 2])
We keep 3.07e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([8128, 2])
We keep 4.32e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([17229, 2])
We keep 1.56e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([25722, 2])
We keep 1.08e+07/2.35e+08 =  4% of the original kernel matrix.

torch.Size([30594, 2])
We keep 5.29e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([21862, 2])
We keep 5.14e+06/1.63e+08 =  3% of the original kernel matrix.

torch.Size([27796, 2])
We keep 4.48e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([3697, 2])
We keep 1.84e+05/2.29e+06 =  8% of the original kernel matrix.

torch.Size([11936, 2])
We keep 9.17e+05/3.07e+07 =  2% of the original kernel matrix.

torch.Size([28157, 2])
We keep 5.83e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([32251, 2])
We keep 5.66e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([10470, 2])
We keep 1.10e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([19008, 2])
We keep 2.16e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([33489, 2])
We keep 2.21e+07/7.24e+08 =  3% of the original kernel matrix.

torch.Size([33734, 2])
We keep 8.26e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([10537, 2])
We keep 1.57e+06/3.37e+07 =  4% of the original kernel matrix.

torch.Size([18989, 2])
We keep 2.50e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([23827, 2])
We keep 7.28e+06/2.16e+08 =  3% of the original kernel matrix.

torch.Size([28984, 2])
We keep 4.94e+06/2.99e+08 =  1% of the original kernel matrix.

torch.Size([5656, 2])
We keep 8.71e+05/6.56e+06 = 13% of the original kernel matrix.

torch.Size([14494, 2])
We keep 1.24e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([19070, 2])
We keep 3.51e+06/1.04e+08 =  3% of the original kernel matrix.

torch.Size([25998, 2])
We keep 3.70e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([7563, 2])
We keep 5.58e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([16489, 2])
We keep 1.69e+06/6.92e+07 =  2% of the original kernel matrix.

torch.Size([17984, 2])
We keep 2.01e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([25372, 2])
We keep 3.40e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([30262, 2])
We keep 4.67e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([33680, 2])
We keep 5.22e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([135481, 2])
We keep 1.32e+08/6.87e+09 =  1% of the original kernel matrix.

torch.Size([69546, 2])
We keep 2.11e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([8142, 2])
We keep 5.54e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([17157, 2])
We keep 1.71e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([3562, 2])
We keep 1.05e+05/1.67e+06 =  6% of the original kernel matrix.

torch.Size([12168, 2])
We keep 8.32e+05/2.63e+07 =  3% of the original kernel matrix.

torch.Size([9338, 2])
We keep 8.72e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([17872, 2])
We keep 1.92e+06/8.62e+07 =  2% of the original kernel matrix.

torch.Size([97832, 2])
We keep 9.04e+07/3.96e+09 =  2% of the original kernel matrix.

torch.Size([58665, 2])
We keep 1.63e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([707890, 2])
We keep 2.24e+09/1.88e+11 =  1% of the original kernel matrix.

torch.Size([172014, 2])
We keep 9.31e+07/8.82e+09 =  1% of the original kernel matrix.

torch.Size([30960, 2])
We keep 1.10e+07/4.41e+08 =  2% of the original kernel matrix.

torch.Size([33222, 2])
We keep 6.89e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([9249, 2])
We keep 7.61e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([17705, 2])
We keep 1.96e+06/8.75e+07 =  2% of the original kernel matrix.

torch.Size([89173, 2])
We keep 8.48e+07/3.62e+09 =  2% of the original kernel matrix.

torch.Size([55111, 2])
We keep 1.61e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([132195, 2])
We keep 7.55e+07/6.59e+09 =  1% of the original kernel matrix.

torch.Size([69294, 2])
We keep 2.08e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([4827, 2])
We keep 1.74e+05/3.15e+06 =  5% of the original kernel matrix.

torch.Size([13555, 2])
We keep 1.03e+06/3.61e+07 =  2% of the original kernel matrix.

torch.Size([15973, 2])
We keep 2.88e+06/7.86e+07 =  3% of the original kernel matrix.

torch.Size([23647, 2])
We keep 3.38e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([31733, 2])
We keep 4.42e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([32177, 2])
We keep 4.88e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([98396, 2])
We keep 2.97e+08/9.29e+09 =  3% of the original kernel matrix.

torch.Size([57830, 2])
We keep 2.42e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([237306, 2])
We keep 1.14e+09/4.82e+10 =  2% of the original kernel matrix.

torch.Size([88486, 2])
We keep 4.99e+07/4.46e+09 =  1% of the original kernel matrix.

torch.Size([224433, 2])
We keep 2.08e+08/2.06e+10 =  1% of the original kernel matrix.

torch.Size([91913, 2])
We keep 3.44e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([17231, 2])
We keep 1.76e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([24826, 2])
We keep 2.97e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([7253, 2])
We keep 3.50e+05/7.97e+06 =  4% of the original kernel matrix.

torch.Size([16287, 2])
We keep 1.43e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([3503, 2])
We keep 9.32e+04/1.60e+06 =  5% of the original kernel matrix.

torch.Size([11951, 2])
We keep 7.99e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([69670, 2])
We keep 3.76e+07/1.67e+09 =  2% of the original kernel matrix.

torch.Size([48376, 2])
We keep 1.14e+07/8.31e+08 =  1% of the original kernel matrix.

torch.Size([15082, 2])
We keep 1.48e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([23162, 2])
We keep 2.92e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([16939, 2])
We keep 1.22e+07/1.22e+08 = 10% of the original kernel matrix.

torch.Size([23993, 2])
We keep 3.62e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([160707, 2])
We keep 1.74e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([76379, 2])
We keep 2.62e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([627263, 2])
We keep 1.05e+09/1.29e+11 =  0% of the original kernel matrix.

torch.Size([155855, 2])
We keep 7.76e+07/7.30e+09 =  1% of the original kernel matrix.

torch.Size([12859, 2])
We keep 1.54e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([20928, 2])
We keep 2.58e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([4968, 2])
We keep 2.02e+05/3.57e+06 =  5% of the original kernel matrix.

torch.Size([13838, 2])
We keep 1.08e+06/3.84e+07 =  2% of the original kernel matrix.

torch.Size([24964, 2])
We keep 1.46e+07/2.12e+08 =  6% of the original kernel matrix.

torch.Size([30612, 2])
We keep 4.65e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([73191, 2])
We keep 1.73e+08/1.76e+09 =  9% of the original kernel matrix.

torch.Size([50812, 2])
We keep 1.07e+07/8.54e+08 =  1% of the original kernel matrix.

torch.Size([29432, 2])
We keep 7.40e+06/3.24e+08 =  2% of the original kernel matrix.

torch.Size([33271, 2])
We keep 5.95e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([2752, 2])
We keep 1.13e+05/1.22e+06 =  9% of the original kernel matrix.

torch.Size([10972, 2])
We keep 7.21e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([6649, 2])
We keep 4.94e+05/8.29e+06 =  5% of the original kernel matrix.

torch.Size([15202, 2])
We keep 1.45e+06/5.85e+07 =  2% of the original kernel matrix.

torch.Size([22504, 2])
We keep 4.25e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([28552, 2])
We keep 4.26e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([4356, 2])
We keep 1.66e+05/2.65e+06 =  6% of the original kernel matrix.

torch.Size([13135, 2])
We keep 9.62e+05/3.31e+07 =  2% of the original kernel matrix.

torch.Size([49307, 2])
We keep 5.53e+07/1.28e+09 =  4% of the original kernel matrix.

torch.Size([40695, 2])
We keep 1.05e+07/7.28e+08 =  1% of the original kernel matrix.

torch.Size([7245, 2])
We keep 4.13e+05/8.24e+06 =  5% of the original kernel matrix.

torch.Size([16176, 2])
We keep 1.43e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([22692, 2])
We keep 3.11e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([28947, 2])
We keep 4.31e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([18911, 2])
We keep 3.11e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([26117, 2])
We keep 4.08e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([10944, 2])
We keep 7.60e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([19500, 2])
We keep 2.03e+06/9.25e+07 =  2% of the original kernel matrix.

torch.Size([6351, 2])
We keep 3.81e+05/7.20e+06 =  5% of the original kernel matrix.

torch.Size([15336, 2])
We keep 1.40e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([203837, 2])
We keep 1.44e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([87070, 2])
We keep 2.77e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([508278, 2])
We keep 8.47e+08/8.14e+10 =  1% of the original kernel matrix.

torch.Size([138265, 2])
We keep 6.31e+07/5.80e+09 =  1% of the original kernel matrix.

torch.Size([97608, 2])
We keep 1.30e+08/3.89e+09 =  3% of the original kernel matrix.

torch.Size([58838, 2])
We keep 1.66e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([53695, 2])
We keep 5.69e+07/1.52e+09 =  3% of the original kernel matrix.

torch.Size([42443, 2])
We keep 1.13e+07/7.92e+08 =  1% of the original kernel matrix.

torch.Size([24571, 2])
We keep 3.22e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([29966, 2])
We keep 4.32e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([20095, 2])
We keep 3.34e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([26724, 2])
We keep 3.70e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([18841, 2])
We keep 3.30e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([26074, 2])
We keep 4.10e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([136690, 2])
We keep 8.70e+07/7.13e+09 =  1% of the original kernel matrix.

torch.Size([70522, 2])
We keep 2.17e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([21975, 2])
We keep 6.12e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([28074, 2])
We keep 5.01e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([30702, 2])
We keep 4.89e+07/3.62e+08 = 13% of the original kernel matrix.

torch.Size([33873, 2])
We keep 5.65e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([21837, 2])
We keep 4.22e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([28232, 2])
We keep 4.75e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([1444519, 2])
We keep 4.38e+09/6.18e+11 =  0% of the original kernel matrix.

torch.Size([245373, 2])
We keep 1.60e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([17276, 2])
We keep 1.96e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([24832, 2])
We keep 3.11e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([71870, 2])
We keep 2.63e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([49487, 2])
We keep 1.14e+07/8.26e+08 =  1% of the original kernel matrix.

torch.Size([258377, 2])
We keep 4.74e+08/3.04e+10 =  1% of the original kernel matrix.

torch.Size([98152, 2])
We keep 4.09e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([53755, 2])
We keep 1.80e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([43810, 2])
We keep 9.60e+06/6.73e+08 =  1% of the original kernel matrix.

torch.Size([26785, 2])
We keep 6.67e+06/2.42e+08 =  2% of the original kernel matrix.

torch.Size([31429, 2])
We keep 5.31e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([547752, 2])
We keep 1.40e+09/1.12e+11 =  1% of the original kernel matrix.

torch.Size([144716, 2])
We keep 7.38e+07/6.80e+09 =  1% of the original kernel matrix.

torch.Size([97579, 2])
We keep 4.27e+07/3.32e+09 =  1% of the original kernel matrix.

torch.Size([58424, 2])
We keep 1.54e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([61836, 2])
We keep 2.70e+07/1.13e+09 =  2% of the original kernel matrix.

torch.Size([46582, 2])
We keep 9.74e+06/6.82e+08 =  1% of the original kernel matrix.

torch.Size([8598, 2])
We keep 6.70e+05/1.33e+07 =  5% of the original kernel matrix.

torch.Size([17581, 2])
We keep 1.68e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([29076, 2])
We keep 9.76e+06/3.07e+08 =  3% of the original kernel matrix.

torch.Size([31507, 2])
We keep 5.50e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([12015, 2])
We keep 1.47e+06/3.10e+07 =  4% of the original kernel matrix.

torch.Size([20678, 2])
We keep 2.34e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([201970, 2])
We keep 3.29e+08/1.27e+10 =  2% of the original kernel matrix.

torch.Size([86578, 2])
We keep 2.75e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([1215417, 2])
We keep 3.97e+09/4.74e+11 =  0% of the original kernel matrix.

torch.Size([221152, 2])
We keep 1.42e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([95640, 2])
We keep 1.48e+08/3.82e+09 =  3% of the original kernel matrix.

torch.Size([58485, 2])
We keep 1.66e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([166960, 2])
We keep 1.24e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([77234, 2])
We keep 2.54e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([58635, 2])
We keep 7.93e+07/1.42e+09 =  5% of the original kernel matrix.

torch.Size([45162, 2])
We keep 1.10e+07/7.67e+08 =  1% of the original kernel matrix.

torch.Size([29894, 2])
We keep 1.93e+07/4.37e+08 =  4% of the original kernel matrix.

torch.Size([31693, 2])
We keep 6.61e+06/4.25e+08 =  1% of the original kernel matrix.

torch.Size([33346, 2])
We keep 9.50e+06/4.49e+08 =  2% of the original kernel matrix.

torch.Size([35026, 2])
We keep 6.88e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([11098, 2])
We keep 1.69e+06/3.62e+07 =  4% of the original kernel matrix.

torch.Size([19299, 2])
We keep 2.48e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([238468, 2])
We keep 1.45e+09/5.65e+10 =  2% of the original kernel matrix.

torch.Size([91024, 2])
We keep 5.44e+07/4.83e+09 =  1% of the original kernel matrix.

torch.Size([365882, 2])
We keep 3.91e+08/4.73e+10 =  0% of the original kernel matrix.

torch.Size([118134, 2])
We keep 4.91e+07/4.42e+09 =  1% of the original kernel matrix.

torch.Size([33397, 2])
We keep 3.16e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([31662, 2])
We keep 9.81e+06/6.65e+08 =  1% of the original kernel matrix.

torch.Size([938285, 2])
We keep 7.14e+09/4.71e+11 =  1% of the original kernel matrix.

torch.Size([184555, 2])
We keep 1.43e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([178501, 2])
We keep 5.36e+08/2.13e+10 =  2% of the original kernel matrix.

torch.Size([79713, 2])
We keep 3.48e+07/2.96e+09 =  1% of the original kernel matrix.

torch.Size([38006, 2])
We keep 7.31e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([36907, 2])
We keep 6.36e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([4505, 2])
We keep 2.68e+05/4.02e+06 =  6% of the original kernel matrix.

torch.Size([13208, 2])
We keep 1.16e+06/4.08e+07 =  2% of the original kernel matrix.

torch.Size([49042, 2])
We keep 2.62e+07/9.27e+08 =  2% of the original kernel matrix.

torch.Size([41991, 2])
We keep 8.89e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([115500, 2])
We keep 6.02e+07/4.82e+09 =  1% of the original kernel matrix.

torch.Size([63835, 2])
We keep 1.79e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([7094, 2])
We keep 5.77e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([15716, 2])
We keep 1.56e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([699436, 2])
We keep 1.31e+09/1.71e+11 =  0% of the original kernel matrix.

torch.Size([167791, 2])
We keep 8.86e+07/8.42e+09 =  1% of the original kernel matrix.

torch.Size([21056, 2])
We keep 3.34e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([27567, 2])
We keep 3.75e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([26937, 2])
We keep 9.29e+06/3.31e+08 =  2% of the original kernel matrix.

torch.Size([30665, 2])
We keep 5.87e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([8164, 2])
We keep 5.48e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([17237, 2])
We keep 1.70e+06/7.27e+07 =  2% of the original kernel matrix.

torch.Size([11508, 2])
We keep 8.92e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([19874, 2])
We keep 2.18e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([4423, 2])
We keep 1.56e+05/2.88e+06 =  5% of the original kernel matrix.

torch.Size([13103, 2])
We keep 9.94e+05/3.45e+07 =  2% of the original kernel matrix.

torch.Size([10120, 2])
We keep 1.87e+06/3.31e+07 =  5% of the original kernel matrix.

torch.Size([18534, 2])
We keep 2.49e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([23248, 2])
We keep 5.87e+06/1.93e+08 =  3% of the original kernel matrix.

torch.Size([29041, 2])
We keep 4.90e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([22620, 2])
We keep 4.27e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([28458, 2])
We keep 4.41e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([2927427, 2])
We keep 3.62e+10/4.06e+12 =  0% of the original kernel matrix.

torch.Size([337747, 2])
We keep 3.88e+08/4.10e+10 =  0% of the original kernel matrix.

torch.Size([17033, 2])
We keep 3.49e+06/9.41e+07 =  3% of the original kernel matrix.

torch.Size([24148, 2])
We keep 3.58e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([113196, 2])
We keep 4.24e+07/3.42e+09 =  1% of the original kernel matrix.

torch.Size([62802, 2])
We keep 1.52e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([9866, 2])
We keep 8.87e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([18575, 2])
We keep 2.15e+06/9.70e+07 =  2% of the original kernel matrix.

torch.Size([21686, 2])
We keep 2.84e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([27997, 2])
We keep 3.84e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([12524, 2])
We keep 1.32e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([20665, 2])
We keep 2.50e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([144996, 2])
We keep 2.33e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([71114, 2])
We keep 2.61e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([48488, 2])
We keep 8.02e+07/1.99e+09 =  4% of the original kernel matrix.

torch.Size([39747, 2])
We keep 1.27e+07/9.07e+08 =  1% of the original kernel matrix.

torch.Size([976262, 2])
We keep 2.26e+09/3.22e+11 =  0% of the original kernel matrix.

torch.Size([199332, 2])
We keep 1.18e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([11250, 2])
We keep 8.02e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([19890, 2])
We keep 2.07e+06/9.62e+07 =  2% of the original kernel matrix.

torch.Size([193651, 2])
We keep 1.74e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([84994, 2])
We keep 2.94e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([33211, 2])
We keep 1.98e+07/4.55e+08 =  4% of the original kernel matrix.

torch.Size([34376, 2])
We keep 6.84e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([157859, 2])
We keep 1.04e+08/7.76e+09 =  1% of the original kernel matrix.

torch.Size([75429, 2])
We keep 2.19e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([186290, 2])
We keep 3.51e+08/2.05e+10 =  1% of the original kernel matrix.

torch.Size([82875, 2])
We keep 3.33e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([20591, 2])
We keep 2.73e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([27169, 2])
We keep 3.57e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([12629, 2])
We keep 1.53e+06/3.97e+07 =  3% of the original kernel matrix.

torch.Size([20960, 2])
We keep 2.57e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([98755, 2])
We keep 4.97e+07/3.75e+09 =  1% of the original kernel matrix.

torch.Size([59692, 2])
We keep 1.63e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([19118, 2])
We keep 6.75e+06/1.22e+08 =  5% of the original kernel matrix.

torch.Size([26283, 2])
We keep 3.78e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([47900, 2])
We keep 3.20e+07/1.03e+09 =  3% of the original kernel matrix.

torch.Size([40548, 2])
We keep 9.21e+06/6.54e+08 =  1% of the original kernel matrix.

torch.Size([206363, 2])
We keep 2.53e+08/1.46e+10 =  1% of the original kernel matrix.

torch.Size([87035, 2])
We keep 2.92e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([11960, 2])
We keep 1.54e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([20499, 2])
We keep 2.60e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([78329, 2])
We keep 8.98e+07/2.63e+09 =  3% of the original kernel matrix.

torch.Size([51513, 2])
We keep 1.42e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([23442, 2])
We keep 6.05e+06/1.81e+08 =  3% of the original kernel matrix.

torch.Size([28674, 2])
We keep 4.48e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([132154, 2])
We keep 1.07e+08/7.36e+09 =  1% of the original kernel matrix.

torch.Size([68512, 2])
We keep 2.18e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([307080, 2])
We keep 4.97e+08/3.94e+10 =  1% of the original kernel matrix.

torch.Size([107436, 2])
We keep 4.57e+07/4.04e+09 =  1% of the original kernel matrix.

torch.Size([31359, 2])
We keep 6.16e+06/2.72e+08 =  2% of the original kernel matrix.

torch.Size([32391, 2])
We keep 5.05e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([12232, 2])
We keep 1.13e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([20667, 2])
We keep 2.35e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([159516, 2])
We keep 2.15e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([74981, 2])
We keep 2.74e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([71565, 2])
We keep 3.41e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([49039, 2])
We keep 1.18e+07/8.49e+08 =  1% of the original kernel matrix.

torch.Size([337491, 2])
We keep 2.50e+09/1.03e+11 =  2% of the original kernel matrix.

torch.Size([106618, 2])
We keep 7.12e+07/6.53e+09 =  1% of the original kernel matrix.

torch.Size([33264, 2])
We keep 1.62e+07/4.79e+08 =  3% of the original kernel matrix.

torch.Size([34199, 2])
We keep 6.97e+06/4.45e+08 =  1% of the original kernel matrix.

torch.Size([45371, 2])
We keep 2.64e+07/8.75e+08 =  3% of the original kernel matrix.

torch.Size([39300, 2])
We keep 8.69e+06/6.02e+08 =  1% of the original kernel matrix.

torch.Size([34149, 2])
We keep 1.63e+07/4.56e+08 =  3% of the original kernel matrix.

torch.Size([35045, 2])
We keep 6.81e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([91179, 2])
We keep 1.23e+08/4.89e+09 =  2% of the original kernel matrix.

torch.Size([54369, 2])
We keep 1.86e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([166559, 2])
We keep 2.96e+08/1.53e+10 =  1% of the original kernel matrix.

torch.Size([77260, 2])
We keep 3.04e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([31108, 2])
We keep 6.52e+06/3.06e+08 =  2% of the original kernel matrix.

torch.Size([32261, 2])
We keep 5.41e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([10072, 2])
We keep 7.70e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([18466, 2])
We keep 1.99e+06/9.13e+07 =  2% of the original kernel matrix.

torch.Size([14058, 2])
We keep 1.47e+06/4.52e+07 =  3% of the original kernel matrix.

torch.Size([22023, 2])
We keep 2.71e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([55581, 2])
We keep 3.99e+07/1.33e+09 =  3% of the original kernel matrix.

torch.Size([43022, 2])
We keep 1.05e+07/7.41e+08 =  1% of the original kernel matrix.

torch.Size([68713, 2])
We keep 7.33e+07/1.96e+09 =  3% of the original kernel matrix.

torch.Size([48505, 2])
We keep 1.24e+07/9.01e+08 =  1% of the original kernel matrix.

torch.Size([94680, 2])
We keep 3.89e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([57276, 2])
We keep 1.32e+07/9.98e+08 =  1% of the original kernel matrix.

torch.Size([17439, 2])
We keep 2.44e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([24842, 2])
We keep 3.66e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([33433, 2])
We keep 5.34e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([34758, 2])
We keep 5.69e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([42546, 2])
We keep 2.55e+07/6.99e+08 =  3% of the original kernel matrix.

torch.Size([39131, 2])
We keep 7.96e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([44721, 2])
We keep 1.20e+07/7.28e+08 =  1% of the original kernel matrix.

torch.Size([41004, 2])
We keep 8.15e+06/5.49e+08 =  1% of the original kernel matrix.

torch.Size([33292, 2])
We keep 1.27e+07/4.49e+08 =  2% of the original kernel matrix.

torch.Size([35059, 2])
We keep 6.84e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([84479, 2])
We keep 3.11e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([54047, 2])
We keep 1.28e+07/9.46e+08 =  1% of the original kernel matrix.

torch.Size([15459, 2])
We keep 4.39e+06/1.02e+08 =  4% of the original kernel matrix.

torch.Size([23400, 2])
We keep 3.80e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([36599, 2])
We keep 8.00e+06/3.85e+08 =  2% of the original kernel matrix.

torch.Size([36609, 2])
We keep 6.21e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([4071, 2])
We keep 1.45e+05/2.49e+06 =  5% of the original kernel matrix.

torch.Size([12577, 2])
We keep 9.46e+05/3.21e+07 =  2% of the original kernel matrix.

torch.Size([9920, 2])
We keep 8.88e+05/2.20e+07 =  4% of the original kernel matrix.

torch.Size([18812, 2])
We keep 2.14e+06/9.54e+07 =  2% of the original kernel matrix.

torch.Size([119714, 2])
We keep 4.66e+07/3.99e+09 =  1% of the original kernel matrix.

torch.Size([64993, 2])
We keep 1.64e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([22207, 2])
We keep 4.97e+06/1.62e+08 =  3% of the original kernel matrix.

torch.Size([28016, 2])
We keep 4.53e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([68849, 2])
We keep 1.95e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([48940, 2])
We keep 1.06e+07/7.67e+08 =  1% of the original kernel matrix.

torch.Size([4460, 2])
We keep 1.66e+05/2.75e+06 =  6% of the original kernel matrix.

torch.Size([13324, 2])
We keep 9.88e+05/3.37e+07 =  2% of the original kernel matrix.

torch.Size([10814, 2])
We keep 1.61e+06/2.47e+07 =  6% of the original kernel matrix.

torch.Size([19440, 2])
We keep 2.18e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([30324, 2])
We keep 8.42e+06/3.23e+08 =  2% of the original kernel matrix.

torch.Size([32758, 2])
We keep 5.68e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([58343, 2])
We keep 2.11e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([44975, 2])
We keep 9.81e+06/6.91e+08 =  1% of the original kernel matrix.

torch.Size([27219, 2])
We keep 5.18e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([31967, 2])
We keep 5.32e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([7078, 2])
We keep 3.74e+05/8.07e+06 =  4% of the original kernel matrix.

torch.Size([15970, 2])
We keep 1.43e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([8854, 2])
We keep 1.25e+06/2.45e+07 =  5% of the original kernel matrix.

torch.Size([17423, 2])
We keep 2.23e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([28296, 2])
We keep 6.53e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([32421, 2])
We keep 5.32e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([29256, 2])
We keep 8.03e+06/3.06e+08 =  2% of the original kernel matrix.

torch.Size([32950, 2])
We keep 5.74e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([112103, 2])
We keep 1.16e+08/5.17e+09 =  2% of the original kernel matrix.

torch.Size([62635, 2])
We keep 1.90e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([8337, 2])
We keep 5.34e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([17284, 2])
We keep 1.68e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([13592, 2])
We keep 2.43e+06/5.78e+07 =  4% of the original kernel matrix.

torch.Size([21552, 2])
We keep 3.07e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([19886, 2])
We keep 6.77e+06/2.00e+08 =  3% of the original kernel matrix.

torch.Size([26116, 2])
We keep 4.86e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([12729, 2])
We keep 1.49e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([21106, 2])
We keep 2.73e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([169755, 2])
We keep 2.46e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([78906, 2])
We keep 2.73e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([11133, 2])
We keep 3.06e+06/3.21e+07 =  9% of the original kernel matrix.

torch.Size([19908, 2])
We keep 2.29e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([65247, 2])
We keep 3.01e+07/1.44e+09 =  2% of the original kernel matrix.

torch.Size([47283, 2])
We keep 1.07e+07/7.73e+08 =  1% of the original kernel matrix.

torch.Size([190413, 2])
We keep 1.74e+08/1.39e+10 =  1% of the original kernel matrix.

torch.Size([83898, 2])
We keep 2.87e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([31254, 2])
We keep 5.88e+06/2.71e+08 =  2% of the original kernel matrix.

torch.Size([31998, 2])
We keep 5.02e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([150242, 2])
We keep 1.65e+08/9.03e+09 =  1% of the original kernel matrix.

torch.Size([74348, 2])
We keep 2.41e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([150217, 2])
We keep 9.75e+07/7.52e+09 =  1% of the original kernel matrix.

torch.Size([73946, 2])
We keep 2.19e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([3791, 2])
We keep 1.06e+05/1.78e+06 =  5% of the original kernel matrix.

torch.Size([12591, 2])
We keep 8.36e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([49131, 2])
We keep 1.25e+07/7.14e+08 =  1% of the original kernel matrix.

torch.Size([42079, 2])
We keep 8.07e+06/5.43e+08 =  1% of the original kernel matrix.

torch.Size([18447, 2])
We keep 1.85e+06/7.95e+07 =  2% of the original kernel matrix.

torch.Size([25697, 2])
We keep 3.31e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([168717, 2])
We keep 2.19e+08/9.97e+09 =  2% of the original kernel matrix.

torch.Size([78263, 2])
We keep 2.49e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([109438, 2])
We keep 8.67e+07/4.58e+09 =  1% of the original kernel matrix.

torch.Size([62206, 2])
We keep 1.71e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([23520, 2])
We keep 1.01e+07/2.67e+08 =  3% of the original kernel matrix.

torch.Size([29138, 2])
We keep 5.64e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([72341, 2])
We keep 2.07e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([50152, 2])
We keep 1.05e+07/7.60e+08 =  1% of the original kernel matrix.

torch.Size([22939, 2])
We keep 5.31e+06/1.48e+08 =  3% of the original kernel matrix.

torch.Size([28705, 2])
We keep 4.16e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([18391, 2])
We keep 1.18e+07/2.70e+08 =  4% of the original kernel matrix.

torch.Size([24889, 2])
We keep 5.56e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([7349, 2])
We keep 3.62e+05/8.56e+06 =  4% of the original kernel matrix.

torch.Size([16340, 2])
We keep 1.43e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([13816, 2])
We keep 1.50e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([21895, 2])
We keep 2.65e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([185030, 2])
We keep 3.93e+08/1.32e+10 =  2% of the original kernel matrix.

torch.Size([82766, 2])
We keep 2.77e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([636467, 2])
We keep 2.15e+09/1.54e+11 =  1% of the original kernel matrix.

torch.Size([160803, 2])
We keep 8.28e+07/7.97e+09 =  1% of the original kernel matrix.

torch.Size([29563, 2])
We keep 9.22e+06/3.80e+08 =  2% of the original kernel matrix.

torch.Size([32091, 2])
We keep 6.32e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([21472, 2])
We keep 3.08e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([27830, 2])
We keep 3.95e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([38498, 2])
We keep 1.06e+07/5.31e+08 =  1% of the original kernel matrix.

torch.Size([37680, 2])
We keep 7.31e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([119341, 2])
We keep 1.47e+08/6.52e+09 =  2% of the original kernel matrix.

torch.Size([65286, 2])
We keep 2.03e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([27912, 2])
We keep 4.42e+06/2.10e+08 =  2% of the original kernel matrix.

torch.Size([32454, 2])
We keep 4.87e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([13162, 2])
We keep 1.11e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([21616, 2])
We keep 2.40e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([47874, 2])
We keep 1.81e+07/9.68e+08 =  1% of the original kernel matrix.

torch.Size([41512, 2])
We keep 9.32e+06/6.33e+08 =  1% of the original kernel matrix.

torch.Size([143748, 2])
We keep 1.11e+08/7.87e+09 =  1% of the original kernel matrix.

torch.Size([72526, 2])
We keep 2.25e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([7935, 2])
We keep 7.03e+05/1.27e+07 =  5% of the original kernel matrix.

torch.Size([16867, 2])
We keep 1.66e+06/7.23e+07 =  2% of the original kernel matrix.

torch.Size([35279, 2])
We keep 1.66e+07/4.71e+08 =  3% of the original kernel matrix.

torch.Size([36008, 2])
We keep 6.86e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([12886, 2])
We keep 3.74e+06/3.37e+07 = 11% of the original kernel matrix.

torch.Size([21478, 2])
We keep 2.17e+06/1.18e+08 =  1% of the original kernel matrix.

time for making ranges is 3.523577928543091
Sorting X and nu_X
time for sorting X is 0.08216428756713867
Sorting Z and nu_Z
time for sorting Z is 0.0002639293670654297
Starting Optim
sum tnu_Z before tensor(28345920., device='cuda:0')
c= tensor(1121.9515, device='cuda:0')
c= tensor(90272.7188, device='cuda:0')
c= tensor(92592.5781, device='cuda:0')
c= tensor(184926.3438, device='cuda:0')
c= tensor(435767., device='cuda:0')
c= tensor(577360.8750, device='cuda:0')
c= tensor(1055246.7500, device='cuda:0')
c= tensor(1289050.8750, device='cuda:0')
c= tensor(1642314.1250, device='cuda:0')
c= tensor(2803800.7500, device='cuda:0')
c= tensor(2828171.2500, device='cuda:0')
c= tensor(5480234., device='cuda:0')
c= tensor(5496058., device='cuda:0')
c= tensor(25213042., device='cuda:0')
c= tensor(25367636., device='cuda:0')
c= tensor(25480724., device='cuda:0')
c= tensor(26363124., device='cuda:0')
c= tensor(26753600., device='cuda:0')
c= tensor(31584014., device='cuda:0')
c= tensor(34044888., device='cuda:0')
c= tensor(34145384., device='cuda:0')
c= tensor(39055976., device='cuda:0')
c= tensor(39079420., device='cuda:0')
c= tensor(39691336., device='cuda:0')
c= tensor(39725200., device='cuda:0')
c= tensor(40374660., device='cuda:0')
c= tensor(41840704., device='cuda:0')
c= tensor(41866940., device='cuda:0')
c= tensor(49000864., device='cuda:0')
c= tensor(1.7342e+08, device='cuda:0')
c= tensor(1.7345e+08, device='cuda:0')
c= tensor(3.6916e+08, device='cuda:0')
c= tensor(3.6932e+08, device='cuda:0')
c= tensor(3.6935e+08, device='cuda:0')
c= tensor(3.6937e+08, device='cuda:0')
c= tensor(3.8389e+08, device='cuda:0')
c= tensor(3.8573e+08, device='cuda:0')
c= tensor(3.8573e+08, device='cuda:0')
c= tensor(3.8574e+08, device='cuda:0')
c= tensor(3.8575e+08, device='cuda:0')
c= tensor(3.8575e+08, device='cuda:0')
c= tensor(3.8575e+08, device='cuda:0')
c= tensor(3.8575e+08, device='cuda:0')
c= tensor(3.8576e+08, device='cuda:0')
c= tensor(3.8576e+08, device='cuda:0')
c= tensor(3.8576e+08, device='cuda:0')
c= tensor(3.8577e+08, device='cuda:0')
c= tensor(3.8577e+08, device='cuda:0')
c= tensor(3.8578e+08, device='cuda:0')
c= tensor(3.8582e+08, device='cuda:0')
c= tensor(3.8587e+08, device='cuda:0')
c= tensor(3.8587e+08, device='cuda:0')
c= tensor(3.8589e+08, device='cuda:0')
c= tensor(3.8589e+08, device='cuda:0')
c= tensor(3.8590e+08, device='cuda:0')
c= tensor(3.8592e+08, device='cuda:0')
c= tensor(3.8592e+08, device='cuda:0')
c= tensor(3.8593e+08, device='cuda:0')
c= tensor(3.8593e+08, device='cuda:0')
c= tensor(3.8594e+08, device='cuda:0')
c= tensor(3.8595e+08, device='cuda:0')
c= tensor(3.8596e+08, device='cuda:0')
c= tensor(3.8597e+08, device='cuda:0')
c= tensor(3.8598e+08, device='cuda:0')
c= tensor(3.8599e+08, device='cuda:0')
c= tensor(3.8599e+08, device='cuda:0')
c= tensor(3.8599e+08, device='cuda:0')
c= tensor(3.8600e+08, device='cuda:0')
c= tensor(3.8600e+08, device='cuda:0')
c= tensor(3.8601e+08, device='cuda:0')
c= tensor(3.8603e+08, device='cuda:0')
c= tensor(3.8604e+08, device='cuda:0')
c= tensor(3.8604e+08, device='cuda:0')
c= tensor(3.8604e+08, device='cuda:0')
c= tensor(3.8605e+08, device='cuda:0')
c= tensor(3.8609e+08, device='cuda:0')
c= tensor(3.8609e+08, device='cuda:0')
c= tensor(3.8609e+08, device='cuda:0')
c= tensor(3.8609e+08, device='cuda:0')
c= tensor(3.8612e+08, device='cuda:0')
c= tensor(3.8612e+08, device='cuda:0')
c= tensor(3.8612e+08, device='cuda:0')
c= tensor(3.8613e+08, device='cuda:0')
c= tensor(3.8613e+08, device='cuda:0')
c= tensor(3.8614e+08, device='cuda:0')
c= tensor(3.8614e+08, device='cuda:0')
c= tensor(3.8614e+08, device='cuda:0')
c= tensor(3.8614e+08, device='cuda:0')
c= tensor(3.8615e+08, device='cuda:0')
c= tensor(3.8615e+08, device='cuda:0')
c= tensor(3.8616e+08, device='cuda:0')
c= tensor(3.8616e+08, device='cuda:0')
c= tensor(3.8616e+08, device='cuda:0')
c= tensor(3.8619e+08, device='cuda:0')
c= tensor(3.8619e+08, device='cuda:0')
c= tensor(3.8620e+08, device='cuda:0')
c= tensor(3.8620e+08, device='cuda:0')
c= tensor(3.8623e+08, device='cuda:0')
c= tensor(3.8625e+08, device='cuda:0')
c= tensor(3.8625e+08, device='cuda:0')
c= tensor(3.8629e+08, device='cuda:0')
c= tensor(3.8630e+08, device='cuda:0')
c= tensor(3.8631e+08, device='cuda:0')
c= tensor(3.8631e+08, device='cuda:0')
c= tensor(3.8632e+08, device='cuda:0')
c= tensor(3.8632e+08, device='cuda:0')
c= tensor(3.8633e+08, device='cuda:0')
c= tensor(3.8633e+08, device='cuda:0')
c= tensor(3.8633e+08, device='cuda:0')
c= tensor(3.8633e+08, device='cuda:0')
c= tensor(3.8634e+08, device='cuda:0')
c= tensor(3.8634e+08, device='cuda:0')
c= tensor(3.8634e+08, device='cuda:0')
c= tensor(3.8635e+08, device='cuda:0')
c= tensor(3.8636e+08, device='cuda:0')
c= tensor(3.8636e+08, device='cuda:0')
c= tensor(3.8636e+08, device='cuda:0')
c= tensor(3.8636e+08, device='cuda:0')
c= tensor(3.8638e+08, device='cuda:0')
c= tensor(3.8638e+08, device='cuda:0')
c= tensor(3.8640e+08, device='cuda:0')
c= tensor(3.8640e+08, device='cuda:0')
c= tensor(3.8640e+08, device='cuda:0')
c= tensor(3.8641e+08, device='cuda:0')
c= tensor(3.8641e+08, device='cuda:0')
c= tensor(3.8642e+08, device='cuda:0')
c= tensor(3.8642e+08, device='cuda:0')
c= tensor(3.8642e+08, device='cuda:0')
c= tensor(3.8645e+08, device='cuda:0')
c= tensor(3.8645e+08, device='cuda:0')
c= tensor(3.8646e+08, device='cuda:0')
c= tensor(3.8646e+08, device='cuda:0')
c= tensor(3.8647e+08, device='cuda:0')
c= tensor(3.8647e+08, device='cuda:0')
c= tensor(3.8647e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8649e+08, device='cuda:0')
c= tensor(3.8649e+08, device='cuda:0')
c= tensor(3.8651e+08, device='cuda:0')
c= tensor(3.8656e+08, device='cuda:0')
c= tensor(3.8657e+08, device='cuda:0')
c= tensor(3.8657e+08, device='cuda:0')
c= tensor(3.8657e+08, device='cuda:0')
c= tensor(3.8657e+08, device='cuda:0')
c= tensor(3.8657e+08, device='cuda:0')
c= tensor(3.8658e+08, device='cuda:0')
c= tensor(3.8658e+08, device='cuda:0')
c= tensor(3.8659e+08, device='cuda:0')
c= tensor(3.8659e+08, device='cuda:0')
c= tensor(3.8664e+08, device='cuda:0')
c= tensor(3.8664e+08, device='cuda:0')
c= tensor(3.8676e+08, device='cuda:0')
c= tensor(3.8677e+08, device='cuda:0')
c= tensor(3.8677e+08, device='cuda:0')
c= tensor(3.8678e+08, device='cuda:0')
c= tensor(3.8678e+08, device='cuda:0')
c= tensor(3.8680e+08, device='cuda:0')
c= tensor(3.8681e+08, device='cuda:0')
c= tensor(3.8683e+08, device='cuda:0')
c= tensor(3.8683e+08, device='cuda:0')
c= tensor(3.8684e+08, device='cuda:0')
c= tensor(3.8684e+08, device='cuda:0')
c= tensor(3.8685e+08, device='cuda:0')
c= tensor(3.8685e+08, device='cuda:0')
c= tensor(3.8686e+08, device='cuda:0')
c= tensor(3.8686e+08, device='cuda:0')
c= tensor(3.8686e+08, device='cuda:0')
c= tensor(3.8686e+08, device='cuda:0')
c= tensor(3.8687e+08, device='cuda:0')
c= tensor(3.8687e+08, device='cuda:0')
c= tensor(3.8688e+08, device='cuda:0')
c= tensor(3.8689e+08, device='cuda:0')
c= tensor(3.8689e+08, device='cuda:0')
c= tensor(3.8690e+08, device='cuda:0')
c= tensor(3.8691e+08, device='cuda:0')
c= tensor(3.8691e+08, device='cuda:0')
c= tensor(3.8693e+08, device='cuda:0')
c= tensor(3.8693e+08, device='cuda:0')
c= tensor(3.8694e+08, device='cuda:0')
c= tensor(3.8694e+08, device='cuda:0')
c= tensor(3.8695e+08, device='cuda:0')
c= tensor(3.8695e+08, device='cuda:0')
c= tensor(3.8696e+08, device='cuda:0')
c= tensor(3.8697e+08, device='cuda:0')
c= tensor(3.8698e+08, device='cuda:0')
c= tensor(3.8701e+08, device='cuda:0')
c= tensor(3.8702e+08, device='cuda:0')
c= tensor(3.8702e+08, device='cuda:0')
c= tensor(3.8702e+08, device='cuda:0')
c= tensor(3.8702e+08, device='cuda:0')
c= tensor(3.8703e+08, device='cuda:0')
c= tensor(3.8704e+08, device='cuda:0')
c= tensor(3.8704e+08, device='cuda:0')
c= tensor(3.8704e+08, device='cuda:0')
c= tensor(3.8705e+08, device='cuda:0')
c= tensor(3.8706e+08, device='cuda:0')
c= tensor(3.8707e+08, device='cuda:0')
c= tensor(3.8707e+08, device='cuda:0')
c= tensor(3.8710e+08, device='cuda:0')
c= tensor(3.8711e+08, device='cuda:0')
c= tensor(3.8711e+08, device='cuda:0')
c= tensor(3.8711e+08, device='cuda:0')
c= tensor(3.8712e+08, device='cuda:0')
c= tensor(3.8713e+08, device='cuda:0')
c= tensor(3.8713e+08, device='cuda:0')
c= tensor(3.8714e+08, device='cuda:0')
c= tensor(3.8715e+08, device='cuda:0')
c= tensor(3.8716e+08, device='cuda:0')
c= tensor(3.8716e+08, device='cuda:0')
c= tensor(3.8717e+08, device='cuda:0')
c= tensor(3.8717e+08, device='cuda:0')
c= tensor(3.8717e+08, device='cuda:0')
c= tensor(3.8717e+08, device='cuda:0')
c= tensor(3.8717e+08, device='cuda:0')
c= tensor(3.8726e+08, device='cuda:0')
c= tensor(3.8726e+08, device='cuda:0')
c= tensor(3.8726e+08, device='cuda:0')
c= tensor(3.8726e+08, device='cuda:0')
c= tensor(3.8727e+08, device='cuda:0')
c= tensor(3.8728e+08, device='cuda:0')
c= tensor(3.8728e+08, device='cuda:0')
c= tensor(3.8728e+08, device='cuda:0')
c= tensor(3.8729e+08, device='cuda:0')
c= tensor(3.8729e+08, device='cuda:0')
c= tensor(3.8729e+08, device='cuda:0')
c= tensor(3.8729e+08, device='cuda:0')
c= tensor(3.8730e+08, device='cuda:0')
c= tensor(3.8730e+08, device='cuda:0')
c= tensor(3.8730e+08, device='cuda:0')
c= tensor(3.8731e+08, device='cuda:0')
c= tensor(3.8731e+08, device='cuda:0')
c= tensor(3.8731e+08, device='cuda:0')
c= tensor(3.8735e+08, device='cuda:0')
c= tensor(3.8736e+08, device='cuda:0')
c= tensor(3.8736e+08, device='cuda:0')
c= tensor(3.8746e+08, device='cuda:0')
c= tensor(3.8912e+08, device='cuda:0')
c= tensor(3.8913e+08, device='cuda:0')
c= tensor(3.8914e+08, device='cuda:0')
c= tensor(3.8915e+08, device='cuda:0')
c= tensor(3.8915e+08, device='cuda:0')
c= tensor(3.8917e+08, device='cuda:0')
c= tensor(3.9494e+08, device='cuda:0')
c= tensor(3.9494e+08, device='cuda:0')
c= tensor(3.9716e+08, device='cuda:0')
c= tensor(3.9817e+08, device='cuda:0')
c= tensor(3.9825e+08, device='cuda:0')
c= tensor(3.9845e+08, device='cuda:0')
c= tensor(3.9845e+08, device='cuda:0')
c= tensor(3.9846e+08, device='cuda:0')
c= tensor(4.0903e+08, device='cuda:0')
c= tensor(4.2679e+08, device='cuda:0')
c= tensor(4.2680e+08, device='cuda:0')
c= tensor(4.2707e+08, device='cuda:0')
c= tensor(4.2761e+08, device='cuda:0')
c= tensor(4.2787e+08, device='cuda:0')
c= tensor(4.2946e+08, device='cuda:0')
c= tensor(4.3079e+08, device='cuda:0')
c= tensor(4.3103e+08, device='cuda:0')
c= tensor(4.3119e+08, device='cuda:0')
c= tensor(4.3120e+08, device='cuda:0')
c= tensor(4.3941e+08, device='cuda:0')
c= tensor(4.3947e+08, device='cuda:0')
c= tensor(4.3947e+08, device='cuda:0')
c= tensor(4.3951e+08, device='cuda:0')
c= tensor(4.4042e+08, device='cuda:0')
c= tensor(4.5586e+08, device='cuda:0')
c= tensor(4.5665e+08, device='cuda:0')
c= tensor(4.5665e+08, device='cuda:0')
c= tensor(4.5681e+08, device='cuda:0')
c= tensor(4.5684e+08, device='cuda:0')
c= tensor(4.5717e+08, device='cuda:0')
c= tensor(4.5774e+08, device='cuda:0')
c= tensor(4.5859e+08, device='cuda:0')
c= tensor(4.5886e+08, device='cuda:0')
c= tensor(4.5886e+08, device='cuda:0')
c= tensor(4.5887e+08, device='cuda:0')
c= tensor(4.6020e+08, device='cuda:0')
c= tensor(4.6079e+08, device='cuda:0')
c= tensor(4.6156e+08, device='cuda:0')
c= tensor(4.6157e+08, device='cuda:0')
c= tensor(4.8115e+08, device='cuda:0')
c= tensor(4.8117e+08, device='cuda:0')
c= tensor(4.8135e+08, device='cuda:0')
c= tensor(4.8398e+08, device='cuda:0')
c= tensor(4.8398e+08, device='cuda:0')
c= tensor(4.8475e+08, device='cuda:0')
c= tensor(4.9482e+08, device='cuda:0')
c= tensor(5.3497e+08, device='cuda:0')
c= tensor(5.3509e+08, device='cuda:0')
c= tensor(5.3516e+08, device='cuda:0')
c= tensor(5.3517e+08, device='cuda:0')
c= tensor(5.3518e+08, device='cuda:0')
c= tensor(5.3911e+08, device='cuda:0')
c= tensor(5.3913e+08, device='cuda:0')
c= tensor(5.3945e+08, device='cuda:0')
c= tensor(5.4147e+08, device='cuda:0')
c= tensor(5.4165e+08, device='cuda:0')
c= tensor(5.4184e+08, device='cuda:0')
c= tensor(5.4185e+08, device='cuda:0')
c= tensor(5.5437e+08, device='cuda:0')
c= tensor(5.5445e+08, device='cuda:0')
c= tensor(5.5475e+08, device='cuda:0')
c= tensor(5.5483e+08, device='cuda:0')
c= tensor(5.5708e+08, device='cuda:0')
c= tensor(5.5711e+08, device='cuda:0')
c= tensor(5.6413e+08, device='cuda:0')
c= tensor(5.6417e+08, device='cuda:0')
c= tensor(5.6552e+08, device='cuda:0')
c= tensor(5.6556e+08, device='cuda:0')
c= tensor(5.8135e+08, device='cuda:0')
c= tensor(5.8197e+08, device='cuda:0')
c= tensor(5.8198e+08, device='cuda:0')
c= tensor(5.8467e+08, device='cuda:0')
c= tensor(5.8748e+08, device='cuda:0')
c= tensor(5.8749e+08, device='cuda:0')
c= tensor(5.9189e+08, device='cuda:0')
c= tensor(5.9932e+08, device='cuda:0')
c= tensor(6.1985e+08, device='cuda:0')
c= tensor(6.2024e+08, device='cuda:0')
c= tensor(6.2024e+08, device='cuda:0')
c= tensor(6.2026e+08, device='cuda:0')
c= tensor(6.2031e+08, device='cuda:0')
c= tensor(6.2041e+08, device='cuda:0')
c= tensor(6.2054e+08, device='cuda:0')
c= tensor(6.2054e+08, device='cuda:0')
c= tensor(6.2134e+08, device='cuda:0')
c= tensor(6.2280e+08, device='cuda:0')
c= tensor(6.2292e+08, device='cuda:0')
c= tensor(6.2295e+08, device='cuda:0')
c= tensor(6.2327e+08, device='cuda:0')
c= tensor(6.2330e+08, device='cuda:0')
c= tensor(6.2337e+08, device='cuda:0')
c= tensor(6.2340e+08, device='cuda:0')
c= tensor(6.2340e+08, device='cuda:0')
c= tensor(6.2762e+08, device='cuda:0')
c= tensor(6.2807e+08, device='cuda:0')
c= tensor(6.2818e+08, device='cuda:0')
c= tensor(6.2934e+08, device='cuda:0')
c= tensor(6.2937e+08, device='cuda:0')
c= tensor(6.6268e+08, device='cuda:0')
c= tensor(6.6271e+08, device='cuda:0')
c= tensor(6.6410e+08, device='cuda:0')
c= tensor(6.6410e+08, device='cuda:0')
c= tensor(6.6410e+08, device='cuda:0')
c= tensor(6.6411e+08, device='cuda:0')
c= tensor(6.6415e+08, device='cuda:0')
c= tensor(6.6416e+08, device='cuda:0')
c= tensor(6.6575e+08, device='cuda:0')
c= tensor(6.6575e+08, device='cuda:0')
c= tensor(6.6576e+08, device='cuda:0')
c= tensor(6.7265e+08, device='cuda:0')
c= tensor(6.7331e+08, device='cuda:0')
c= tensor(6.7356e+08, device='cuda:0')
c= tensor(6.7616e+08, device='cuda:0')
c= tensor(6.8795e+08, device='cuda:0')
c= tensor(6.8797e+08, device='cuda:0')
c= tensor(6.8799e+08, device='cuda:0')
c= tensor(6.8807e+08, device='cuda:0')
c= tensor(6.8807e+08, device='cuda:0')
c= tensor(6.8808e+08, device='cuda:0')
c= tensor(6.8816e+08, device='cuda:0')
c= tensor(6.8817e+08, device='cuda:0')
c= tensor(6.8818e+08, device='cuda:0')
c= tensor(6.8818e+08, device='cuda:0')
c= tensor(6.8820e+08, device='cuda:0')
c= tensor(7.0886e+08, device='cuda:0')
c= tensor(7.0894e+08, device='cuda:0')
c= tensor(7.1081e+08, device='cuda:0')
c= tensor(7.1086e+08, device='cuda:0')
c= tensor(7.1086e+08, device='cuda:0')
c= tensor(7.1108e+08, device='cuda:0')
c= tensor(7.3567e+08, device='cuda:0')
c= tensor(7.5361e+08, device='cuda:0')
c= tensor(7.5369e+08, device='cuda:0')
c= tensor(7.5399e+08, device='cuda:0')
c= tensor(7.5399e+08, device='cuda:0')
c= tensor(7.5403e+08, device='cuda:0')
c= tensor(8.5829e+08, device='cuda:0')
c= tensor(8.5838e+08, device='cuda:0')
c= tensor(8.5840e+08, device='cuda:0')
c= tensor(8.5912e+08, device='cuda:0')
c= tensor(9.3123e+08, device='cuda:0')
c= tensor(9.3139e+08, device='cuda:0')
c= tensor(9.3141e+08, device='cuda:0')
c= tensor(9.3144e+08, device='cuda:0')
c= tensor(9.3158e+08, device='cuda:0')
c= tensor(9.3160e+08, device='cuda:0')
c= tensor(9.3551e+08, device='cuda:0')
c= tensor(9.3558e+08, device='cuda:0')
c= tensor(9.3558e+08, device='cuda:0')
c= tensor(9.3588e+08, device='cuda:0')
c= tensor(9.3594e+08, device='cuda:0')
c= tensor(9.3595e+08, device='cuda:0')
c= tensor(9.3710e+08, device='cuda:0')
c= tensor(9.3901e+08, device='cuda:0')
c= tensor(9.4824e+08, device='cuda:0')
c= tensor(9.5540e+08, device='cuda:0')
c= tensor(9.6319e+08, device='cuda:0')
c= tensor(9.6330e+08, device='cuda:0')
c= tensor(9.6339e+08, device='cuda:0')
c= tensor(9.6404e+08, device='cuda:0')
c= tensor(9.6666e+08, device='cuda:0')
c= tensor(9.6667e+08, device='cuda:0')
c= tensor(9.6941e+08, device='cuda:0')
c= tensor(9.7307e+08, device='cuda:0')
c= tensor(9.7849e+08, device='cuda:0')
c= tensor(9.7995e+08, device='cuda:0')
c= tensor(9.8322e+08, device='cuda:0')
c= tensor(9.8326e+08, device='cuda:0')
c= tensor(9.8327e+08, device='cuda:0')
c= tensor(9.8337e+08, device='cuda:0')
c= tensor(9.8490e+08, device='cuda:0')
c= tensor(9.8664e+08, device='cuda:0')
c= tensor(1.0059e+09, device='cuda:0')
c= tensor(1.0095e+09, device='cuda:0')
c= tensor(1.0104e+09, device='cuda:0')
c= tensor(1.0106e+09, device='cuda:0')
c= tensor(1.0178e+09, device='cuda:0')
c= tensor(1.0178e+09, device='cuda:0')
c= tensor(1.0178e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0229e+09, device='cuda:0')
c= tensor(1.0229e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0837e+09, device='cuda:0')
c= tensor(1.0838e+09, device='cuda:0')
c= tensor(1.0851e+09, device='cuda:0')
c= tensor(1.0851e+09, device='cuda:0')
c= tensor(1.0851e+09, device='cuda:0')
c= tensor(1.0851e+09, device='cuda:0')
c= tensor(1.0852e+09, device='cuda:0')
c= tensor(1.0853e+09, device='cuda:0')
c= tensor(1.0862e+09, device='cuda:0')
c= tensor(1.0862e+09, device='cuda:0')
c= tensor(1.0886e+09, device='cuda:0')
c= tensor(1.0886e+09, device='cuda:0')
c= tensor(1.0892e+09, device='cuda:0')
c= tensor(1.0893e+09, device='cuda:0')
c= tensor(1.0900e+09, device='cuda:0')
c= tensor(1.0900e+09, device='cuda:0')
c= tensor(1.0901e+09, device='cuda:0')
c= tensor(1.0901e+09, device='cuda:0')
c= tensor(1.0903e+09, device='cuda:0')
c= tensor(1.0915e+09, device='cuda:0')
c= tensor(1.1276e+09, device='cuda:0')
c= tensor(1.1276e+09, device='cuda:0')
c= tensor(1.1276e+09, device='cuda:0')
c= tensor(1.1289e+09, device='cuda:0')
c= tensor(1.1289e+09, device='cuda:0')
c= tensor(1.1650e+09, device='cuda:0')
c= tensor(1.1650e+09, device='cuda:0')
c= tensor(1.1668e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1778e+09, device='cuda:0')
c= tensor(1.1780e+09, device='cuda:0')
c= tensor(1.2294e+09, device='cuda:0')
c= tensor(1.2295e+09, device='cuda:0')
c= tensor(1.2295e+09, device='cuda:0')
c= tensor(1.2295e+09, device='cuda:0')
c= tensor(1.2296e+09, device='cuda:0')
c= tensor(1.2296e+09, device='cuda:0')
c= tensor(1.2304e+09, device='cuda:0')
c= tensor(1.2306e+09, device='cuda:0')
c= tensor(1.2314e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2398e+09, device='cuda:0')
c= tensor(1.2406e+09, device='cuda:0')
c= tensor(1.2520e+09, device='cuda:0')
c= tensor(1.2521e+09, device='cuda:0')
c= tensor(1.2521e+09, device='cuda:0')
c= tensor(1.2521e+09, device='cuda:0')
c= tensor(1.2522e+09, device='cuda:0')
c= tensor(1.2642e+09, device='cuda:0')
c= tensor(1.2643e+09, device='cuda:0')
c= tensor(1.2643e+09, device='cuda:0')
c= tensor(1.2652e+09, device='cuda:0')
c= tensor(1.2656e+09, device='cuda:0')
c= tensor(1.2656e+09, device='cuda:0')
c= tensor(1.2656e+09, device='cuda:0')
c= tensor(1.2823e+09, device='cuda:0')
c= tensor(1.2825e+09, device='cuda:0')
c= tensor(1.2825e+09, device='cuda:0')
c= tensor(1.2826e+09, device='cuda:0')
c= tensor(1.2955e+09, device='cuda:0')
c= tensor(1.2965e+09, device='cuda:0')
c= tensor(1.3115e+09, device='cuda:0')
c= tensor(1.3126e+09, device='cuda:0')
c= tensor(1.3126e+09, device='cuda:0')
c= tensor(1.3130e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3137e+09, device='cuda:0')
c= tensor(1.3141e+09, device='cuda:0')
c= tensor(1.3142e+09, device='cuda:0')
c= tensor(1.3142e+09, device='cuda:0')
c= tensor(1.3143e+09, device='cuda:0')
c= tensor(1.3143e+09, device='cuda:0')
c= tensor(1.3152e+09, device='cuda:0')
c= tensor(1.3152e+09, device='cuda:0')
c= tensor(1.3153e+09, device='cuda:0')
c= tensor(1.3154e+09, device='cuda:0')
c= tensor(1.3154e+09, device='cuda:0')
c= tensor(1.3154e+09, device='cuda:0')
c= tensor(1.3155e+09, device='cuda:0')
c= tensor(1.3156e+09, device='cuda:0')
c= tensor(1.3204e+09, device='cuda:0')
c= tensor(1.3204e+09, device='cuda:0')
c= tensor(1.3204e+09, device='cuda:0')
c= tensor(1.3204e+09, device='cuda:0')
c= tensor(1.3231e+09, device='cuda:0')
c= tensor(1.3919e+09, device='cuda:0')
c= tensor(1.3921e+09, device='cuda:0')
c= tensor(1.3921e+09, device='cuda:0')
c= tensor(1.3937e+09, device='cuda:0')
c= tensor(1.3951e+09, device='cuda:0')
c= tensor(1.3951e+09, device='cuda:0')
c= tensor(1.3952e+09, device='cuda:0')
c= tensor(1.3953e+09, device='cuda:0')
c= tensor(1.4058e+09, device='cuda:0')
c= tensor(1.4437e+09, device='cuda:0')
c= tensor(1.4490e+09, device='cuda:0')
c= tensor(1.4490e+09, device='cuda:0')
c= tensor(1.4490e+09, device='cuda:0')
c= tensor(1.4490e+09, device='cuda:0')
c= tensor(1.4500e+09, device='cuda:0')
c= tensor(1.4500e+09, device='cuda:0')
c= tensor(1.4503e+09, device='cuda:0')
c= tensor(1.4541e+09, device='cuda:0')
c= tensor(1.4828e+09, device='cuda:0')
c= tensor(1.4828e+09, device='cuda:0')
c= tensor(1.4828e+09, device='cuda:0')
c= tensor(1.4831e+09, device='cuda:0')
c= tensor(1.4877e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4891e+09, device='cuda:0')
c= tensor(1.4891e+09, device='cuda:0')
c= tensor(1.4892e+09, device='cuda:0')
c= tensor(1.4892e+09, device='cuda:0')
c= tensor(1.4892e+09, device='cuda:0')
c= tensor(1.4892e+09, device='cuda:0')
c= tensor(1.4934e+09, device='cuda:0')
c= tensor(1.5203e+09, device='cuda:0')
c= tensor(1.5234e+09, device='cuda:0')
c= tensor(1.5247e+09, device='cuda:0')
c= tensor(1.5248e+09, device='cuda:0')
c= tensor(1.5248e+09, device='cuda:0')
c= tensor(1.5249e+09, device='cuda:0')
c= tensor(1.5266e+09, device='cuda:0')
c= tensor(1.5267e+09, device='cuda:0')
c= tensor(1.5278e+09, device='cuda:0')
c= tensor(1.5279e+09, device='cuda:0')
c= tensor(1.6823e+09, device='cuda:0')
c= tensor(1.6823e+09, device='cuda:0')
c= tensor(1.6828e+09, device='cuda:0')
c= tensor(1.6982e+09, device='cuda:0')
c= tensor(1.6987e+09, device='cuda:0')
c= tensor(1.6988e+09, device='cuda:0')
c= tensor(1.7389e+09, device='cuda:0')
c= tensor(1.7405e+09, device='cuda:0')
c= tensor(1.7410e+09, device='cuda:0')
c= tensor(1.7410e+09, device='cuda:0')
c= tensor(1.7411e+09, device='cuda:0')
c= tensor(1.7412e+09, device='cuda:0')
c= tensor(1.7490e+09, device='cuda:0')
c= tensor(1.8705e+09, device='cuda:0')
c= tensor(1.8749e+09, device='cuda:0')
c= tensor(1.8778e+09, device='cuda:0')
c= tensor(1.8793e+09, device='cuda:0')
c= tensor(1.8796e+09, device='cuda:0')
c= tensor(1.8798e+09, device='cuda:0')
c= tensor(1.8799e+09, device='cuda:0')
c= tensor(1.9171e+09, device='cuda:0')
c= tensor(1.9293e+09, device='cuda:0')
c= tensor(1.9298e+09, device='cuda:0')
c= tensor(2.1751e+09, device='cuda:0')
c= tensor(2.1941e+09, device='cuda:0')
c= tensor(2.1942e+09, device='cuda:0')
c= tensor(2.1942e+09, device='cuda:0')
c= tensor(2.1948e+09, device='cuda:0')
c= tensor(2.1962e+09, device='cuda:0')
c= tensor(2.1962e+09, device='cuda:0')
c= tensor(2.2358e+09, device='cuda:0')
c= tensor(2.2360e+09, device='cuda:0')
c= tensor(2.2361e+09, device='cuda:0')
c= tensor(2.2362e+09, device='cuda:0')
c= tensor(2.2362e+09, device='cuda:0')
c= tensor(2.2362e+09, device='cuda:0')
c= tensor(2.2362e+09, device='cuda:0')
c= tensor(2.2363e+09, device='cuda:0')
c= tensor(2.2364e+09, device='cuda:0')
c= tensor(3.7208e+09, device='cuda:0')
c= tensor(3.7212e+09, device='cuda:0')
c= tensor(3.7221e+09, device='cuda:0')
c= tensor(3.7221e+09, device='cuda:0')
c= tensor(3.7221e+09, device='cuda:0')
c= tensor(3.7221e+09, device='cuda:0')
c= tensor(3.7290e+09, device='cuda:0')
c= tensor(3.7307e+09, device='cuda:0')
c= tensor(3.8189e+09, device='cuda:0')
c= tensor(3.8189e+09, device='cuda:0')
c= tensor(3.8232e+09, device='cuda:0')
c= tensor(3.8237e+09, device='cuda:0')
c= tensor(3.8265e+09, device='cuda:0')
c= tensor(3.8401e+09, device='cuda:0')
c= tensor(3.8401e+09, device='cuda:0')
c= tensor(3.8402e+09, device='cuda:0')
c= tensor(3.8414e+09, device='cuda:0')
c= tensor(3.8416e+09, device='cuda:0')
c= tensor(3.8424e+09, device='cuda:0')
c= tensor(3.8485e+09, device='cuda:0')
c= tensor(3.8485e+09, device='cuda:0')
c= tensor(3.8506e+09, device='cuda:0')
c= tensor(3.8508e+09, device='cuda:0')
c= tensor(3.8533e+09, device='cuda:0')
c= tensor(3.8668e+09, device='cuda:0')
c= tensor(3.8669e+09, device='cuda:0')
c= tensor(3.8669e+09, device='cuda:0')
c= tensor(3.8728e+09, device='cuda:0')
c= tensor(3.8734e+09, device='cuda:0')
c= tensor(3.9371e+09, device='cuda:0')
c= tensor(3.9374e+09, device='cuda:0')
c= tensor(3.9380e+09, device='cuda:0')
c= tensor(3.9384e+09, device='cuda:0')
c= tensor(3.9440e+09, device='cuda:0')
c= tensor(3.9513e+09, device='cuda:0')
c= tensor(3.9513e+09, device='cuda:0')
c= tensor(3.9514e+09, device='cuda:0')
c= tensor(3.9514e+09, device='cuda:0')
c= tensor(3.9524e+09, device='cuda:0')
c= tensor(3.9539e+09, device='cuda:0')
c= tensor(3.9547e+09, device='cuda:0')
c= tensor(3.9547e+09, device='cuda:0')
c= tensor(3.9549e+09, device='cuda:0')
c= tensor(3.9555e+09, device='cuda:0')
c= tensor(3.9559e+09, device='cuda:0')
c= tensor(3.9562e+09, device='cuda:0')
c= tensor(3.9568e+09, device='cuda:0')
c= tensor(3.9572e+09, device='cuda:0')
c= tensor(3.9574e+09, device='cuda:0')
c= tensor(3.9574e+09, device='cuda:0')
c= tensor(3.9574e+09, device='cuda:0')
c= tensor(3.9585e+09, device='cuda:0')
c= tensor(3.9587e+09, device='cuda:0')
c= tensor(3.9591e+09, device='cuda:0')
c= tensor(3.9591e+09, device='cuda:0')
c= tensor(3.9591e+09, device='cuda:0')
c= tensor(3.9593e+09, device='cuda:0')
c= tensor(3.9597e+09, device='cuda:0')
c= tensor(3.9598e+09, device='cuda:0')
c= tensor(3.9598e+09, device='cuda:0')
c= tensor(3.9598e+09, device='cuda:0')
c= tensor(3.9599e+09, device='cuda:0')
c= tensor(3.9601e+09, device='cuda:0')
c= tensor(3.9629e+09, device='cuda:0')
c= tensor(3.9629e+09, device='cuda:0')
c= tensor(3.9630e+09, device='cuda:0')
c= tensor(3.9631e+09, device='cuda:0')
c= tensor(3.9632e+09, device='cuda:0')
c= tensor(3.9707e+09, device='cuda:0')
c= tensor(3.9708e+09, device='cuda:0')
c= tensor(3.9720e+09, device='cuda:0')
c= tensor(3.9762e+09, device='cuda:0')
c= tensor(3.9763e+09, device='cuda:0')
c= tensor(3.9798e+09, device='cuda:0')
c= tensor(3.9821e+09, device='cuda:0')
c= tensor(3.9821e+09, device='cuda:0')
c= tensor(3.9823e+09, device='cuda:0')
c= tensor(3.9824e+09, device='cuda:0')
c= tensor(3.9867e+09, device='cuda:0')
c= tensor(3.9895e+09, device='cuda:0')
c= tensor(3.9896e+09, device='cuda:0')
c= tensor(3.9901e+09, device='cuda:0')
c= tensor(3.9903e+09, device='cuda:0')
c= tensor(3.9912e+09, device='cuda:0')
c= tensor(3.9912e+09, device='cuda:0')
c= tensor(3.9912e+09, device='cuda:0')
c= tensor(4.0024e+09, device='cuda:0')
c= tensor(4.0796e+09, device='cuda:0')
c= tensor(4.0797e+09, device='cuda:0')
c= tensor(4.0798e+09, device='cuda:0')
c= tensor(4.0800e+09, device='cuda:0')
c= tensor(4.0837e+09, device='cuda:0')
c= tensor(4.0838e+09, device='cuda:0')
c= tensor(4.0838e+09, device='cuda:0')
c= tensor(4.0841e+09, device='cuda:0')
c= tensor(4.0865e+09, device='cuda:0')
c= tensor(4.0865e+09, device='cuda:0')
c= tensor(4.0868e+09, device='cuda:0')
c= tensor(4.0869e+09, device='cuda:0')
memory (bytes)
4491116544
time for making loss 2 is 13.4549241065979
p0 True
it  0 : 1858975232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 61% |
shape of L is 
torch.Size([])
memory (bytes)
4491313152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4491862016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  43152085000.0
relative error loss 10.558696
shape of L is 
torch.Size([])
memory (bytes)
4682174464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  3% | 11% |
memory (bytes)
4682174464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  43151870000.0
relative error loss 10.558643
shape of L is 
torch.Size([])
memory (bytes)
4685434880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4685434880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  43151410000.0
relative error loss 10.558531
shape of L is 
torch.Size([])
memory (bytes)
4687618048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4687626240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% | 11% |
error is  43148380000.0
relative error loss 10.557789
shape of L is 
torch.Size([])
memory (bytes)
4689625088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4689625088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  43115065000.0
relative error loss 10.549638
shape of L is 
torch.Size([])
memory (bytes)
4691910656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4691918848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  42948620000.0
relative error loss 10.508911
shape of L is 
torch.Size([])
memory (bytes)
4693925888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4694081536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  41154392000.0
relative error loss 10.069889
shape of L is 
torch.Size([])
memory (bytes)
4695937024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4695937024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  33117725000.0
relative error loss 8.103432
shape of L is 
torch.Size([])
memory (bytes)
4698267648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4698267648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  10546749000.0
relative error loss 2.5806382
shape of L is 
torch.Size([])
memory (bytes)
4700499968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4700499968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  6418252300.0
relative error loss 1.5704542
time to take a step is 218.72055578231812
it  1 : 2258704896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4702535680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4702535680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  6418252300.0
relative error loss 1.5704542
shape of L is 
torch.Size([])
memory (bytes)
4704735232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
4704735232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  7808855000.0
relative error loss 1.9107147
shape of L is 
torch.Size([])
memory (bytes)
4706914304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4706922496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  5175807500.0
relative error loss 1.2664458
shape of L is 
torch.Size([])
memory (bytes)
4709052416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4709052416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  4390019000.0
relative error loss 1.0741746
shape of L is 
torch.Size([])
memory (bytes)
4711010304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4711010304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4125558500.0
relative error loss 1.009465
shape of L is 
torch.Size([])
memory (bytes)
4713119744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4713119744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3912661200.0
relative error loss 0.95737207
shape of L is 
torch.Size([])
memory (bytes)
4715454464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4715462656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3823663900.0
relative error loss 0.9355957
shape of L is 
torch.Size([])
memory (bytes)
4717572096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4717572096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  4195629300.0
relative error loss 1.0266103
shape of L is 
torch.Size([])
memory (bytes)
4719697920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4719697920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 11% |
error is  3735706600.0
relative error loss 0.91407377
shape of L is 
torch.Size([])
memory (bytes)
4721541120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4721799168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3490878000.0
relative error loss 0.85416776
time to take a step is 211.44649577140808
it  2 : 2372913664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4723785728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4723785728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3490878000.0
relative error loss 0.85416776
shape of L is 
torch.Size([])
memory (bytes)
4725981184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4725981184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3330913300.0
relative error loss 0.8150267
shape of L is 
torch.Size([])
memory (bytes)
4728143872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4728152064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3072196400.0
relative error loss 0.75172234
shape of L is 
torch.Size([])
memory (bytes)
4730011648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
4730273792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2902477300.0
relative error loss 0.7101945
shape of L is 
torch.Size([])
memory (bytes)
4732313600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4732313600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2648836600.0
relative error loss 0.6481323
shape of L is 
torch.Size([])
memory (bytes)
4734513152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4734513152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2460650000.0
relative error loss 0.6020857
shape of L is 
torch.Size([])
memory (bytes)
4736512000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4736512000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2625029000.0
relative error loss 0.6423069
shape of L is 
torch.Size([])
memory (bytes)
4738633728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4738633728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2237888500.0
relative error loss 0.54757917
shape of L is 
torch.Size([])
memory (bytes)
4740812800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4740812800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2054873900.0
relative error loss 0.50279814
shape of L is 
torch.Size([])
memory (bytes)
4743020544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4743032832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1854059300.0
relative error loss 0.45366168
time to take a step is 211.87172889709473
it  3 : 2372913664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4744933376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4744933376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1854059300.0
relative error loss 0.45366168
shape of L is 
torch.Size([])
memory (bytes)
4747313152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4747313152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1647917000.0
relative error loss 0.40322164
shape of L is 
torch.Size([])
memory (bytes)
4749447168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4749455360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1431920900.0
relative error loss 0.3503705
shape of L is 
torch.Size([])
memory (bytes)
4751585280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
4751593472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1262563100.0
relative error loss 0.30893105
shape of L is 
torch.Size([])
memory (bytes)
4753731584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4753739776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1111362000.0
relative error loss 0.27193433
shape of L is 
torch.Size([])
memory (bytes)
4755636224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4755873792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1025617400.0
relative error loss 0.25095385
shape of L is 
torch.Size([])
memory (bytes)
4757893120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4757893120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  944217600.0
relative error loss 0.2310365
shape of L is 
torch.Size([])
memory (bytes)
4760150016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4760150016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  858978560.0
relative error loss 0.21017973
shape of L is 
torch.Size([])
memory (bytes)
4762275840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4762275840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  798244600.0
relative error loss 0.195319
shape of L is 
torch.Size([])
memory (bytes)
4764352512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4764352512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 11% |
error is  772432640.0
relative error loss 0.18900318
time to take a step is 234.33048558235168
c= tensor(1121.9515, device='cuda:0')
c= tensor(90272.7188, device='cuda:0')
c= tensor(92592.5781, device='cuda:0')
c= tensor(184926.3438, device='cuda:0')
c= tensor(435767., device='cuda:0')
c= tensor(577360.8750, device='cuda:0')
c= tensor(1055246.7500, device='cuda:0')
c= tensor(1289050.8750, device='cuda:0')
c= tensor(1642314.1250, device='cuda:0')
c= tensor(2803800.7500, device='cuda:0')
c= tensor(2828171.2500, device='cuda:0')
c= tensor(5480234., device='cuda:0')
c= tensor(5496058., device='cuda:0')
c= tensor(25213042., device='cuda:0')
c= tensor(25367636., device='cuda:0')
c= tensor(25480724., device='cuda:0')
c= tensor(26363124., device='cuda:0')
c= tensor(26753600., device='cuda:0')
c= tensor(31584014., device='cuda:0')
c= tensor(34044888., device='cuda:0')
c= tensor(34145384., device='cuda:0')
c= tensor(39055976., device='cuda:0')
c= tensor(39079420., device='cuda:0')
c= tensor(39691336., device='cuda:0')
c= tensor(39725200., device='cuda:0')
c= tensor(40374660., device='cuda:0')
c= tensor(41840704., device='cuda:0')
c= tensor(41866940., device='cuda:0')
c= tensor(49000864., device='cuda:0')
c= tensor(1.7342e+08, device='cuda:0')
c= tensor(1.7345e+08, device='cuda:0')
c= tensor(3.6916e+08, device='cuda:0')
c= tensor(3.6932e+08, device='cuda:0')
c= tensor(3.6935e+08, device='cuda:0')
c= tensor(3.6937e+08, device='cuda:0')
c= tensor(3.8389e+08, device='cuda:0')
c= tensor(3.8573e+08, device='cuda:0')
c= tensor(3.8573e+08, device='cuda:0')
c= tensor(3.8574e+08, device='cuda:0')
c= tensor(3.8575e+08, device='cuda:0')
c= tensor(3.8575e+08, device='cuda:0')
c= tensor(3.8575e+08, device='cuda:0')
c= tensor(3.8575e+08, device='cuda:0')
c= tensor(3.8576e+08, device='cuda:0')
c= tensor(3.8576e+08, device='cuda:0')
c= tensor(3.8576e+08, device='cuda:0')
c= tensor(3.8577e+08, device='cuda:0')
c= tensor(3.8577e+08, device='cuda:0')
c= tensor(3.8578e+08, device='cuda:0')
c= tensor(3.8582e+08, device='cuda:0')
c= tensor(3.8587e+08, device='cuda:0')
c= tensor(3.8587e+08, device='cuda:0')
c= tensor(3.8589e+08, device='cuda:0')
c= tensor(3.8589e+08, device='cuda:0')
c= tensor(3.8590e+08, device='cuda:0')
c= tensor(3.8592e+08, device='cuda:0')
c= tensor(3.8592e+08, device='cuda:0')
c= tensor(3.8593e+08, device='cuda:0')
c= tensor(3.8593e+08, device='cuda:0')
c= tensor(3.8594e+08, device='cuda:0')
c= tensor(3.8595e+08, device='cuda:0')
c= tensor(3.8596e+08, device='cuda:0')
c= tensor(3.8597e+08, device='cuda:0')
c= tensor(3.8598e+08, device='cuda:0')
c= tensor(3.8599e+08, device='cuda:0')
c= tensor(3.8599e+08, device='cuda:0')
c= tensor(3.8599e+08, device='cuda:0')
c= tensor(3.8600e+08, device='cuda:0')
c= tensor(3.8600e+08, device='cuda:0')
c= tensor(3.8601e+08, device='cuda:0')
c= tensor(3.8603e+08, device='cuda:0')
c= tensor(3.8604e+08, device='cuda:0')
c= tensor(3.8604e+08, device='cuda:0')
c= tensor(3.8604e+08, device='cuda:0')
c= tensor(3.8605e+08, device='cuda:0')
c= tensor(3.8609e+08, device='cuda:0')
c= tensor(3.8609e+08, device='cuda:0')
c= tensor(3.8609e+08, device='cuda:0')
c= tensor(3.8609e+08, device='cuda:0')
c= tensor(3.8612e+08, device='cuda:0')
c= tensor(3.8612e+08, device='cuda:0')
c= tensor(3.8612e+08, device='cuda:0')
c= tensor(3.8613e+08, device='cuda:0')
c= tensor(3.8613e+08, device='cuda:0')
c= tensor(3.8614e+08, device='cuda:0')
c= tensor(3.8614e+08, device='cuda:0')
c= tensor(3.8614e+08, device='cuda:0')
c= tensor(3.8614e+08, device='cuda:0')
c= tensor(3.8615e+08, device='cuda:0')
c= tensor(3.8615e+08, device='cuda:0')
c= tensor(3.8616e+08, device='cuda:0')
c= tensor(3.8616e+08, device='cuda:0')
c= tensor(3.8616e+08, device='cuda:0')
c= tensor(3.8619e+08, device='cuda:0')
c= tensor(3.8619e+08, device='cuda:0')
c= tensor(3.8620e+08, device='cuda:0')
c= tensor(3.8620e+08, device='cuda:0')
c= tensor(3.8623e+08, device='cuda:0')
c= tensor(3.8625e+08, device='cuda:0')
c= tensor(3.8625e+08, device='cuda:0')
c= tensor(3.8629e+08, device='cuda:0')
c= tensor(3.8630e+08, device='cuda:0')
c= tensor(3.8631e+08, device='cuda:0')
c= tensor(3.8631e+08, device='cuda:0')
c= tensor(3.8632e+08, device='cuda:0')
c= tensor(3.8632e+08, device='cuda:0')
c= tensor(3.8633e+08, device='cuda:0')
c= tensor(3.8633e+08, device='cuda:0')
c= tensor(3.8633e+08, device='cuda:0')
c= tensor(3.8633e+08, device='cuda:0')
c= tensor(3.8634e+08, device='cuda:0')
c= tensor(3.8634e+08, device='cuda:0')
c= tensor(3.8634e+08, device='cuda:0')
c= tensor(3.8635e+08, device='cuda:0')
c= tensor(3.8636e+08, device='cuda:0')
c= tensor(3.8636e+08, device='cuda:0')
c= tensor(3.8636e+08, device='cuda:0')
c= tensor(3.8636e+08, device='cuda:0')
c= tensor(3.8638e+08, device='cuda:0')
c= tensor(3.8638e+08, device='cuda:0')
c= tensor(3.8640e+08, device='cuda:0')
c= tensor(3.8640e+08, device='cuda:0')
c= tensor(3.8640e+08, device='cuda:0')
c= tensor(3.8641e+08, device='cuda:0')
c= tensor(3.8641e+08, device='cuda:0')
c= tensor(3.8642e+08, device='cuda:0')
c= tensor(3.8642e+08, device='cuda:0')
c= tensor(3.8642e+08, device='cuda:0')
c= tensor(3.8645e+08, device='cuda:0')
c= tensor(3.8645e+08, device='cuda:0')
c= tensor(3.8646e+08, device='cuda:0')
c= tensor(3.8646e+08, device='cuda:0')
c= tensor(3.8647e+08, device='cuda:0')
c= tensor(3.8647e+08, device='cuda:0')
c= tensor(3.8647e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8649e+08, device='cuda:0')
c= tensor(3.8649e+08, device='cuda:0')
c= tensor(3.8651e+08, device='cuda:0')
c= tensor(3.8656e+08, device='cuda:0')
c= tensor(3.8657e+08, device='cuda:0')
c= tensor(3.8657e+08, device='cuda:0')
c= tensor(3.8657e+08, device='cuda:0')
c= tensor(3.8657e+08, device='cuda:0')
c= tensor(3.8657e+08, device='cuda:0')
c= tensor(3.8658e+08, device='cuda:0')
c= tensor(3.8658e+08, device='cuda:0')
c= tensor(3.8659e+08, device='cuda:0')
c= tensor(3.8659e+08, device='cuda:0')
c= tensor(3.8664e+08, device='cuda:0')
c= tensor(3.8664e+08, device='cuda:0')
c= tensor(3.8676e+08, device='cuda:0')
c= tensor(3.8677e+08, device='cuda:0')
c= tensor(3.8677e+08, device='cuda:0')
c= tensor(3.8678e+08, device='cuda:0')
c= tensor(3.8678e+08, device='cuda:0')
c= tensor(3.8680e+08, device='cuda:0')
c= tensor(3.8681e+08, device='cuda:0')
c= tensor(3.8683e+08, device='cuda:0')
c= tensor(3.8683e+08, device='cuda:0')
c= tensor(3.8684e+08, device='cuda:0')
c= tensor(3.8684e+08, device='cuda:0')
c= tensor(3.8685e+08, device='cuda:0')
c= tensor(3.8685e+08, device='cuda:0')
c= tensor(3.8686e+08, device='cuda:0')
c= tensor(3.8686e+08, device='cuda:0')
c= tensor(3.8686e+08, device='cuda:0')
c= tensor(3.8686e+08, device='cuda:0')
c= tensor(3.8687e+08, device='cuda:0')
c= tensor(3.8687e+08, device='cuda:0')
c= tensor(3.8688e+08, device='cuda:0')
c= tensor(3.8689e+08, device='cuda:0')
c= tensor(3.8689e+08, device='cuda:0')
c= tensor(3.8690e+08, device='cuda:0')
c= tensor(3.8691e+08, device='cuda:0')
c= tensor(3.8691e+08, device='cuda:0')
c= tensor(3.8693e+08, device='cuda:0')
c= tensor(3.8693e+08, device='cuda:0')
c= tensor(3.8694e+08, device='cuda:0')
c= tensor(3.8694e+08, device='cuda:0')
c= tensor(3.8695e+08, device='cuda:0')
c= tensor(3.8695e+08, device='cuda:0')
c= tensor(3.8696e+08, device='cuda:0')
c= tensor(3.8697e+08, device='cuda:0')
c= tensor(3.8698e+08, device='cuda:0')
c= tensor(3.8701e+08, device='cuda:0')
c= tensor(3.8702e+08, device='cuda:0')
c= tensor(3.8702e+08, device='cuda:0')
c= tensor(3.8702e+08, device='cuda:0')
c= tensor(3.8702e+08, device='cuda:0')
c= tensor(3.8703e+08, device='cuda:0')
c= tensor(3.8704e+08, device='cuda:0')
c= tensor(3.8704e+08, device='cuda:0')
c= tensor(3.8704e+08, device='cuda:0')
c= tensor(3.8705e+08, device='cuda:0')
c= tensor(3.8706e+08, device='cuda:0')
c= tensor(3.8707e+08, device='cuda:0')
c= tensor(3.8707e+08, device='cuda:0')
c= tensor(3.8710e+08, device='cuda:0')
c= tensor(3.8711e+08, device='cuda:0')
c= tensor(3.8711e+08, device='cuda:0')
c= tensor(3.8711e+08, device='cuda:0')
c= tensor(3.8712e+08, device='cuda:0')
c= tensor(3.8713e+08, device='cuda:0')
c= tensor(3.8713e+08, device='cuda:0')
c= tensor(3.8714e+08, device='cuda:0')
c= tensor(3.8715e+08, device='cuda:0')
c= tensor(3.8716e+08, device='cuda:0')
c= tensor(3.8716e+08, device='cuda:0')
c= tensor(3.8717e+08, device='cuda:0')
c= tensor(3.8717e+08, device='cuda:0')
c= tensor(3.8717e+08, device='cuda:0')
c= tensor(3.8717e+08, device='cuda:0')
c= tensor(3.8717e+08, device='cuda:0')
c= tensor(3.8726e+08, device='cuda:0')
c= tensor(3.8726e+08, device='cuda:0')
c= tensor(3.8726e+08, device='cuda:0')
c= tensor(3.8726e+08, device='cuda:0')
c= tensor(3.8727e+08, device='cuda:0')
c= tensor(3.8728e+08, device='cuda:0')
c= tensor(3.8728e+08, device='cuda:0')
c= tensor(3.8728e+08, device='cuda:0')
c= tensor(3.8729e+08, device='cuda:0')
c= tensor(3.8729e+08, device='cuda:0')
c= tensor(3.8729e+08, device='cuda:0')
c= tensor(3.8729e+08, device='cuda:0')
c= tensor(3.8730e+08, device='cuda:0')
c= tensor(3.8730e+08, device='cuda:0')
c= tensor(3.8730e+08, device='cuda:0')
c= tensor(3.8731e+08, device='cuda:0')
c= tensor(3.8731e+08, device='cuda:0')
c= tensor(3.8731e+08, device='cuda:0')
c= tensor(3.8735e+08, device='cuda:0')
c= tensor(3.8736e+08, device='cuda:0')
c= tensor(3.8736e+08, device='cuda:0')
c= tensor(3.8746e+08, device='cuda:0')
c= tensor(3.8912e+08, device='cuda:0')
c= tensor(3.8913e+08, device='cuda:0')
c= tensor(3.8914e+08, device='cuda:0')
c= tensor(3.8915e+08, device='cuda:0')
c= tensor(3.8915e+08, device='cuda:0')
c= tensor(3.8917e+08, device='cuda:0')
c= tensor(3.9494e+08, device='cuda:0')
c= tensor(3.9494e+08, device='cuda:0')
c= tensor(3.9716e+08, device='cuda:0')
c= tensor(3.9817e+08, device='cuda:0')
c= tensor(3.9825e+08, device='cuda:0')
c= tensor(3.9845e+08, device='cuda:0')
c= tensor(3.9845e+08, device='cuda:0')
c= tensor(3.9846e+08, device='cuda:0')
c= tensor(4.0903e+08, device='cuda:0')
c= tensor(4.2679e+08, device='cuda:0')
c= tensor(4.2680e+08, device='cuda:0')
c= tensor(4.2707e+08, device='cuda:0')
c= tensor(4.2761e+08, device='cuda:0')
c= tensor(4.2787e+08, device='cuda:0')
c= tensor(4.2946e+08, device='cuda:0')
c= tensor(4.3079e+08, device='cuda:0')
c= tensor(4.3103e+08, device='cuda:0')
c= tensor(4.3119e+08, device='cuda:0')
c= tensor(4.3120e+08, device='cuda:0')
c= tensor(4.3941e+08, device='cuda:0')
c= tensor(4.3947e+08, device='cuda:0')
c= tensor(4.3947e+08, device='cuda:0')
c= tensor(4.3951e+08, device='cuda:0')
c= tensor(4.4042e+08, device='cuda:0')
c= tensor(4.5586e+08, device='cuda:0')
c= tensor(4.5665e+08, device='cuda:0')
c= tensor(4.5665e+08, device='cuda:0')
c= tensor(4.5681e+08, device='cuda:0')
c= tensor(4.5684e+08, device='cuda:0')
c= tensor(4.5717e+08, device='cuda:0')
c= tensor(4.5774e+08, device='cuda:0')
c= tensor(4.5859e+08, device='cuda:0')
c= tensor(4.5886e+08, device='cuda:0')
c= tensor(4.5886e+08, device='cuda:0')
c= tensor(4.5887e+08, device='cuda:0')
c= tensor(4.6020e+08, device='cuda:0')
c= tensor(4.6079e+08, device='cuda:0')
c= tensor(4.6156e+08, device='cuda:0')
c= tensor(4.6157e+08, device='cuda:0')
c= tensor(4.8115e+08, device='cuda:0')
c= tensor(4.8117e+08, device='cuda:0')
c= tensor(4.8135e+08, device='cuda:0')
c= tensor(4.8398e+08, device='cuda:0')
c= tensor(4.8398e+08, device='cuda:0')
c= tensor(4.8475e+08, device='cuda:0')
c= tensor(4.9482e+08, device='cuda:0')
c= tensor(5.3497e+08, device='cuda:0')
c= tensor(5.3509e+08, device='cuda:0')
c= tensor(5.3516e+08, device='cuda:0')
c= tensor(5.3517e+08, device='cuda:0')
c= tensor(5.3518e+08, device='cuda:0')
c= tensor(5.3911e+08, device='cuda:0')
c= tensor(5.3913e+08, device='cuda:0')
c= tensor(5.3945e+08, device='cuda:0')
c= tensor(5.4147e+08, device='cuda:0')
c= tensor(5.4165e+08, device='cuda:0')
c= tensor(5.4184e+08, device='cuda:0')
c= tensor(5.4185e+08, device='cuda:0')
c= tensor(5.5437e+08, device='cuda:0')
c= tensor(5.5445e+08, device='cuda:0')
c= tensor(5.5475e+08, device='cuda:0')
c= tensor(5.5483e+08, device='cuda:0')
c= tensor(5.5708e+08, device='cuda:0')
c= tensor(5.5711e+08, device='cuda:0')
c= tensor(5.6413e+08, device='cuda:0')
c= tensor(5.6417e+08, device='cuda:0')
c= tensor(5.6552e+08, device='cuda:0')
c= tensor(5.6556e+08, device='cuda:0')
c= tensor(5.8135e+08, device='cuda:0')
c= tensor(5.8197e+08, device='cuda:0')
c= tensor(5.8198e+08, device='cuda:0')
c= tensor(5.8467e+08, device='cuda:0')
c= tensor(5.8748e+08, device='cuda:0')
c= tensor(5.8749e+08, device='cuda:0')
c= tensor(5.9189e+08, device='cuda:0')
c= tensor(5.9932e+08, device='cuda:0')
c= tensor(6.1985e+08, device='cuda:0')
c= tensor(6.2024e+08, device='cuda:0')
c= tensor(6.2024e+08, device='cuda:0')
c= tensor(6.2026e+08, device='cuda:0')
c= tensor(6.2031e+08, device='cuda:0')
c= tensor(6.2041e+08, device='cuda:0')
c= tensor(6.2054e+08, device='cuda:0')
c= tensor(6.2054e+08, device='cuda:0')
c= tensor(6.2134e+08, device='cuda:0')
c= tensor(6.2280e+08, device='cuda:0')
c= tensor(6.2292e+08, device='cuda:0')
c= tensor(6.2295e+08, device='cuda:0')
c= tensor(6.2327e+08, device='cuda:0')
c= tensor(6.2330e+08, device='cuda:0')
c= tensor(6.2337e+08, device='cuda:0')
c= tensor(6.2340e+08, device='cuda:0')
c= tensor(6.2340e+08, device='cuda:0')
c= tensor(6.2762e+08, device='cuda:0')
c= tensor(6.2807e+08, device='cuda:0')
c= tensor(6.2818e+08, device='cuda:0')
c= tensor(6.2934e+08, device='cuda:0')
c= tensor(6.2937e+08, device='cuda:0')
c= tensor(6.6268e+08, device='cuda:0')
c= tensor(6.6271e+08, device='cuda:0')
c= tensor(6.6410e+08, device='cuda:0')
c= tensor(6.6410e+08, device='cuda:0')
c= tensor(6.6410e+08, device='cuda:0')
c= tensor(6.6411e+08, device='cuda:0')
c= tensor(6.6415e+08, device='cuda:0')
c= tensor(6.6416e+08, device='cuda:0')
c= tensor(6.6575e+08, device='cuda:0')
c= tensor(6.6575e+08, device='cuda:0')
c= tensor(6.6576e+08, device='cuda:0')
c= tensor(6.7265e+08, device='cuda:0')
c= tensor(6.7331e+08, device='cuda:0')
c= tensor(6.7356e+08, device='cuda:0')
c= tensor(6.7616e+08, device='cuda:0')
c= tensor(6.8795e+08, device='cuda:0')
c= tensor(6.8797e+08, device='cuda:0')
c= tensor(6.8799e+08, device='cuda:0')
c= tensor(6.8807e+08, device='cuda:0')
c= tensor(6.8807e+08, device='cuda:0')
c= tensor(6.8808e+08, device='cuda:0')
c= tensor(6.8816e+08, device='cuda:0')
c= tensor(6.8817e+08, device='cuda:0')
c= tensor(6.8818e+08, device='cuda:0')
c= tensor(6.8818e+08, device='cuda:0')
c= tensor(6.8820e+08, device='cuda:0')
c= tensor(7.0886e+08, device='cuda:0')
c= tensor(7.0894e+08, device='cuda:0')
c= tensor(7.1081e+08, device='cuda:0')
c= tensor(7.1086e+08, device='cuda:0')
c= tensor(7.1086e+08, device='cuda:0')
c= tensor(7.1108e+08, device='cuda:0')
c= tensor(7.3567e+08, device='cuda:0')
c= tensor(7.5361e+08, device='cuda:0')
c= tensor(7.5369e+08, device='cuda:0')
c= tensor(7.5399e+08, device='cuda:0')
c= tensor(7.5399e+08, device='cuda:0')
c= tensor(7.5403e+08, device='cuda:0')
c= tensor(8.5829e+08, device='cuda:0')
c= tensor(8.5838e+08, device='cuda:0')
c= tensor(8.5840e+08, device='cuda:0')
c= tensor(8.5912e+08, device='cuda:0')
c= tensor(9.3123e+08, device='cuda:0')
c= tensor(9.3139e+08, device='cuda:0')
c= tensor(9.3141e+08, device='cuda:0')
c= tensor(9.3144e+08, device='cuda:0')
c= tensor(9.3158e+08, device='cuda:0')
c= tensor(9.3160e+08, device='cuda:0')
c= tensor(9.3551e+08, device='cuda:0')
c= tensor(9.3558e+08, device='cuda:0')
c= tensor(9.3558e+08, device='cuda:0')
c= tensor(9.3588e+08, device='cuda:0')
c= tensor(9.3594e+08, device='cuda:0')
c= tensor(9.3595e+08, device='cuda:0')
c= tensor(9.3710e+08, device='cuda:0')
c= tensor(9.3901e+08, device='cuda:0')
c= tensor(9.4824e+08, device='cuda:0')
c= tensor(9.5540e+08, device='cuda:0')
c= tensor(9.6319e+08, device='cuda:0')
c= tensor(9.6330e+08, device='cuda:0')
c= tensor(9.6339e+08, device='cuda:0')
c= tensor(9.6404e+08, device='cuda:0')
c= tensor(9.6666e+08, device='cuda:0')
c= tensor(9.6667e+08, device='cuda:0')
c= tensor(9.6941e+08, device='cuda:0')
c= tensor(9.7307e+08, device='cuda:0')
c= tensor(9.7849e+08, device='cuda:0')
c= tensor(9.7995e+08, device='cuda:0')
c= tensor(9.8322e+08, device='cuda:0')
c= tensor(9.8326e+08, device='cuda:0')
c= tensor(9.8327e+08, device='cuda:0')
c= tensor(9.8337e+08, device='cuda:0')
c= tensor(9.8490e+08, device='cuda:0')
c= tensor(9.8664e+08, device='cuda:0')
c= tensor(1.0059e+09, device='cuda:0')
c= tensor(1.0095e+09, device='cuda:0')
c= tensor(1.0104e+09, device='cuda:0')
c= tensor(1.0106e+09, device='cuda:0')
c= tensor(1.0178e+09, device='cuda:0')
c= tensor(1.0178e+09, device='cuda:0')
c= tensor(1.0178e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0229e+09, device='cuda:0')
c= tensor(1.0229e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0837e+09, device='cuda:0')
c= tensor(1.0838e+09, device='cuda:0')
c= tensor(1.0851e+09, device='cuda:0')
c= tensor(1.0851e+09, device='cuda:0')
c= tensor(1.0851e+09, device='cuda:0')
c= tensor(1.0851e+09, device='cuda:0')
c= tensor(1.0852e+09, device='cuda:0')
c= tensor(1.0853e+09, device='cuda:0')
c= tensor(1.0862e+09, device='cuda:0')
c= tensor(1.0862e+09, device='cuda:0')
c= tensor(1.0886e+09, device='cuda:0')
c= tensor(1.0886e+09, device='cuda:0')
c= tensor(1.0892e+09, device='cuda:0')
c= tensor(1.0893e+09, device='cuda:0')
c= tensor(1.0900e+09, device='cuda:0')
c= tensor(1.0900e+09, device='cuda:0')
c= tensor(1.0901e+09, device='cuda:0')
c= tensor(1.0901e+09, device='cuda:0')
c= tensor(1.0903e+09, device='cuda:0')
c= tensor(1.0915e+09, device='cuda:0')
c= tensor(1.1276e+09, device='cuda:0')
c= tensor(1.1276e+09, device='cuda:0')
c= tensor(1.1276e+09, device='cuda:0')
c= tensor(1.1289e+09, device='cuda:0')
c= tensor(1.1289e+09, device='cuda:0')
c= tensor(1.1650e+09, device='cuda:0')
c= tensor(1.1650e+09, device='cuda:0')
c= tensor(1.1668e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1778e+09, device='cuda:0')
c= tensor(1.1780e+09, device='cuda:0')
c= tensor(1.2294e+09, device='cuda:0')
c= tensor(1.2295e+09, device='cuda:0')
c= tensor(1.2295e+09, device='cuda:0')
c= tensor(1.2295e+09, device='cuda:0')
c= tensor(1.2296e+09, device='cuda:0')
c= tensor(1.2296e+09, device='cuda:0')
c= tensor(1.2304e+09, device='cuda:0')
c= tensor(1.2306e+09, device='cuda:0')
c= tensor(1.2314e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2398e+09, device='cuda:0')
c= tensor(1.2406e+09, device='cuda:0')
c= tensor(1.2520e+09, device='cuda:0')
c= tensor(1.2521e+09, device='cuda:0')
c= tensor(1.2521e+09, device='cuda:0')
c= tensor(1.2521e+09, device='cuda:0')
c= tensor(1.2522e+09, device='cuda:0')
c= tensor(1.2642e+09, device='cuda:0')
c= tensor(1.2643e+09, device='cuda:0')
c= tensor(1.2643e+09, device='cuda:0')
c= tensor(1.2652e+09, device='cuda:0')
c= tensor(1.2656e+09, device='cuda:0')
c= tensor(1.2656e+09, device='cuda:0')
c= tensor(1.2656e+09, device='cuda:0')
c= tensor(1.2823e+09, device='cuda:0')
c= tensor(1.2825e+09, device='cuda:0')
c= tensor(1.2825e+09, device='cuda:0')
c= tensor(1.2826e+09, device='cuda:0')
c= tensor(1.2955e+09, device='cuda:0')
c= tensor(1.2965e+09, device='cuda:0')
c= tensor(1.3115e+09, device='cuda:0')
c= tensor(1.3126e+09, device='cuda:0')
c= tensor(1.3126e+09, device='cuda:0')
c= tensor(1.3130e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3137e+09, device='cuda:0')
c= tensor(1.3141e+09, device='cuda:0')
c= tensor(1.3142e+09, device='cuda:0')
c= tensor(1.3142e+09, device='cuda:0')
c= tensor(1.3143e+09, device='cuda:0')
c= tensor(1.3143e+09, device='cuda:0')
c= tensor(1.3152e+09, device='cuda:0')
c= tensor(1.3152e+09, device='cuda:0')
c= tensor(1.3153e+09, device='cuda:0')
c= tensor(1.3154e+09, device='cuda:0')
c= tensor(1.3154e+09, device='cuda:0')
c= tensor(1.3154e+09, device='cuda:0')
c= tensor(1.3155e+09, device='cuda:0')
c= tensor(1.3156e+09, device='cuda:0')
c= tensor(1.3204e+09, device='cuda:0')
c= tensor(1.3204e+09, device='cuda:0')
c= tensor(1.3204e+09, device='cuda:0')
c= tensor(1.3204e+09, device='cuda:0')
c= tensor(1.3231e+09, device='cuda:0')
c= tensor(1.3919e+09, device='cuda:0')
c= tensor(1.3921e+09, device='cuda:0')
c= tensor(1.3921e+09, device='cuda:0')
c= tensor(1.3937e+09, device='cuda:0')
c= tensor(1.3951e+09, device='cuda:0')
c= tensor(1.3951e+09, device='cuda:0')
c= tensor(1.3952e+09, device='cuda:0')
c= tensor(1.3953e+09, device='cuda:0')
c= tensor(1.4058e+09, device='cuda:0')
c= tensor(1.4437e+09, device='cuda:0')
c= tensor(1.4490e+09, device='cuda:0')
c= tensor(1.4490e+09, device='cuda:0')
c= tensor(1.4490e+09, device='cuda:0')
c= tensor(1.4490e+09, device='cuda:0')
c= tensor(1.4500e+09, device='cuda:0')
c= tensor(1.4500e+09, device='cuda:0')
c= tensor(1.4503e+09, device='cuda:0')
c= tensor(1.4541e+09, device='cuda:0')
c= tensor(1.4828e+09, device='cuda:0')
c= tensor(1.4828e+09, device='cuda:0')
c= tensor(1.4828e+09, device='cuda:0')
c= tensor(1.4831e+09, device='cuda:0')
c= tensor(1.4877e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4891e+09, device='cuda:0')
c= tensor(1.4891e+09, device='cuda:0')
c= tensor(1.4892e+09, device='cuda:0')
c= tensor(1.4892e+09, device='cuda:0')
c= tensor(1.4892e+09, device='cuda:0')
c= tensor(1.4892e+09, device='cuda:0')
c= tensor(1.4934e+09, device='cuda:0')
c= tensor(1.5203e+09, device='cuda:0')
c= tensor(1.5234e+09, device='cuda:0')
c= tensor(1.5247e+09, device='cuda:0')
c= tensor(1.5248e+09, device='cuda:0')
c= tensor(1.5248e+09, device='cuda:0')
c= tensor(1.5249e+09, device='cuda:0')
c= tensor(1.5266e+09, device='cuda:0')
c= tensor(1.5267e+09, device='cuda:0')
c= tensor(1.5278e+09, device='cuda:0')
c= tensor(1.5279e+09, device='cuda:0')
c= tensor(1.6823e+09, device='cuda:0')
c= tensor(1.6823e+09, device='cuda:0')
c= tensor(1.6828e+09, device='cuda:0')
c= tensor(1.6982e+09, device='cuda:0')
c= tensor(1.6987e+09, device='cuda:0')
c= tensor(1.6988e+09, device='cuda:0')
c= tensor(1.7389e+09, device='cuda:0')
c= tensor(1.7405e+09, device='cuda:0')
c= tensor(1.7410e+09, device='cuda:0')
c= tensor(1.7410e+09, device='cuda:0')
c= tensor(1.7411e+09, device='cuda:0')
c= tensor(1.7412e+09, device='cuda:0')
c= tensor(1.7490e+09, device='cuda:0')
c= tensor(1.8705e+09, device='cuda:0')
c= tensor(1.8749e+09, device='cuda:0')
c= tensor(1.8778e+09, device='cuda:0')
c= tensor(1.8793e+09, device='cuda:0')
c= tensor(1.8796e+09, device='cuda:0')
c= tensor(1.8798e+09, device='cuda:0')
c= tensor(1.8799e+09, device='cuda:0')
c= tensor(1.9171e+09, device='cuda:0')
c= tensor(1.9293e+09, device='cuda:0')
c= tensor(1.9298e+09, device='cuda:0')
c= tensor(2.1751e+09, device='cuda:0')
c= tensor(2.1941e+09, device='cuda:0')
c= tensor(2.1942e+09, device='cuda:0')
c= tensor(2.1942e+09, device='cuda:0')
c= tensor(2.1948e+09, device='cuda:0')
c= tensor(2.1962e+09, device='cuda:0')
c= tensor(2.1962e+09, device='cuda:0')
c= tensor(2.2358e+09, device='cuda:0')
c= tensor(2.2360e+09, device='cuda:0')
c= tensor(2.2361e+09, device='cuda:0')
c= tensor(2.2362e+09, device='cuda:0')
c= tensor(2.2362e+09, device='cuda:0')
c= tensor(2.2362e+09, device='cuda:0')
c= tensor(2.2362e+09, device='cuda:0')
c= tensor(2.2363e+09, device='cuda:0')
c= tensor(2.2364e+09, device='cuda:0')
c= tensor(3.7208e+09, device='cuda:0')
c= tensor(3.7212e+09, device='cuda:0')
c= tensor(3.7221e+09, device='cuda:0')
c= tensor(3.7221e+09, device='cuda:0')
c= tensor(3.7221e+09, device='cuda:0')
c= tensor(3.7221e+09, device='cuda:0')
c= tensor(3.7290e+09, device='cuda:0')
c= tensor(3.7307e+09, device='cuda:0')
c= tensor(3.8189e+09, device='cuda:0')
c= tensor(3.8189e+09, device='cuda:0')
c= tensor(3.8232e+09, device='cuda:0')
c= tensor(3.8237e+09, device='cuda:0')
c= tensor(3.8265e+09, device='cuda:0')
c= tensor(3.8401e+09, device='cuda:0')
c= tensor(3.8401e+09, device='cuda:0')
c= tensor(3.8402e+09, device='cuda:0')
c= tensor(3.8414e+09, device='cuda:0')
c= tensor(3.8416e+09, device='cuda:0')
c= tensor(3.8424e+09, device='cuda:0')
c= tensor(3.8485e+09, device='cuda:0')
c= tensor(3.8485e+09, device='cuda:0')
c= tensor(3.8506e+09, device='cuda:0')
c= tensor(3.8508e+09, device='cuda:0')
c= tensor(3.8533e+09, device='cuda:0')
c= tensor(3.8668e+09, device='cuda:0')
c= tensor(3.8669e+09, device='cuda:0')
c= tensor(3.8669e+09, device='cuda:0')
c= tensor(3.8728e+09, device='cuda:0')
c= tensor(3.8734e+09, device='cuda:0')
c= tensor(3.9371e+09, device='cuda:0')
c= tensor(3.9374e+09, device='cuda:0')
c= tensor(3.9380e+09, device='cuda:0')
c= tensor(3.9384e+09, device='cuda:0')
c= tensor(3.9440e+09, device='cuda:0')
c= tensor(3.9513e+09, device='cuda:0')
c= tensor(3.9513e+09, device='cuda:0')
c= tensor(3.9514e+09, device='cuda:0')
c= tensor(3.9514e+09, device='cuda:0')
c= tensor(3.9524e+09, device='cuda:0')
c= tensor(3.9539e+09, device='cuda:0')
c= tensor(3.9547e+09, device='cuda:0')
c= tensor(3.9547e+09, device='cuda:0')
c= tensor(3.9549e+09, device='cuda:0')
c= tensor(3.9555e+09, device='cuda:0')
c= tensor(3.9559e+09, device='cuda:0')
c= tensor(3.9562e+09, device='cuda:0')
c= tensor(3.9568e+09, device='cuda:0')
c= tensor(3.9572e+09, device='cuda:0')
c= tensor(3.9574e+09, device='cuda:0')
c= tensor(3.9574e+09, device='cuda:0')
c= tensor(3.9574e+09, device='cuda:0')
c= tensor(3.9585e+09, device='cuda:0')
c= tensor(3.9587e+09, device='cuda:0')
c= tensor(3.9591e+09, device='cuda:0')
c= tensor(3.9591e+09, device='cuda:0')
c= tensor(3.9591e+09, device='cuda:0')
c= tensor(3.9593e+09, device='cuda:0')
c= tensor(3.9597e+09, device='cuda:0')
c= tensor(3.9598e+09, device='cuda:0')
c= tensor(3.9598e+09, device='cuda:0')
c= tensor(3.9598e+09, device='cuda:0')
c= tensor(3.9599e+09, device='cuda:0')
c= tensor(3.9601e+09, device='cuda:0')
c= tensor(3.9629e+09, device='cuda:0')
c= tensor(3.9629e+09, device='cuda:0')
c= tensor(3.9630e+09, device='cuda:0')
c= tensor(3.9631e+09, device='cuda:0')
c= tensor(3.9632e+09, device='cuda:0')
c= tensor(3.9707e+09, device='cuda:0')
c= tensor(3.9708e+09, device='cuda:0')
c= tensor(3.9720e+09, device='cuda:0')
c= tensor(3.9762e+09, device='cuda:0')
c= tensor(3.9763e+09, device='cuda:0')
c= tensor(3.9798e+09, device='cuda:0')
c= tensor(3.9821e+09, device='cuda:0')
c= tensor(3.9821e+09, device='cuda:0')
c= tensor(3.9823e+09, device='cuda:0')
c= tensor(3.9824e+09, device='cuda:0')
c= tensor(3.9867e+09, device='cuda:0')
c= tensor(3.9895e+09, device='cuda:0')
c= tensor(3.9896e+09, device='cuda:0')
c= tensor(3.9901e+09, device='cuda:0')
c= tensor(3.9903e+09, device='cuda:0')
c= tensor(3.9912e+09, device='cuda:0')
c= tensor(3.9912e+09, device='cuda:0')
c= tensor(3.9912e+09, device='cuda:0')
c= tensor(4.0024e+09, device='cuda:0')
c= tensor(4.0796e+09, device='cuda:0')
c= tensor(4.0797e+09, device='cuda:0')
c= tensor(4.0798e+09, device='cuda:0')
c= tensor(4.0800e+09, device='cuda:0')
c= tensor(4.0837e+09, device='cuda:0')
c= tensor(4.0838e+09, device='cuda:0')
c= tensor(4.0838e+09, device='cuda:0')
c= tensor(4.0841e+09, device='cuda:0')
c= tensor(4.0865e+09, device='cuda:0')
c= tensor(4.0865e+09, device='cuda:0')
c= tensor(4.0868e+09, device='cuda:0')
c= tensor(4.0869e+09, device='cuda:0')
time to make c is 14.232990980148315
time for making loss is 14.233051538467407
p0 True
it  0 : 1859219456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4766625792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
4766851072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  772432640.0
relative error loss 0.18900318
shape of L is 
torch.Size([])
memory (bytes)
4793692160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
4793765888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  767730940.0
relative error loss 0.18785274
shape of L is 
torch.Size([])
memory (bytes)
4797079552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4797337600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  740802300.0
relative error loss 0.18126369
shape of L is 
torch.Size([])
memory (bytes)
4800548864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4800548864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  718298400.0
relative error loss 0.1757573
shape of L is 
torch.Size([])
memory (bytes)
4803776512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4803776512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 11% |
error is  707401200.0
relative error loss 0.17309092
shape of L is 
torch.Size([])
memory (bytes)
4806877184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4806877184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  699249660.0
relative error loss 0.17109635
shape of L is 
torch.Size([])
memory (bytes)
4810219520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4810219520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  693850900.0
relative error loss 0.16977535
shape of L is 
torch.Size([])
memory (bytes)
4813303808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
4813303808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  689634050.0
relative error loss 0.16874355
shape of L is 
torch.Size([])
memory (bytes)
4816646144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4816646144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  685761300.0
relative error loss 0.16779594
shape of L is 
torch.Size([])
memory (bytes)
4819824640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
4819824640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  682754300.0
relative error loss 0.16706018
time to take a step is 290.42609167099
it  1 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4823068672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4823068672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  682754300.0
relative error loss 0.16706018
shape of L is 
torch.Size([])
memory (bytes)
4826075136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
4826292224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  680296200.0
relative error loss 0.16645871
shape of L is 
torch.Size([])
memory (bytes)
4829515776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4829515776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  678033900.0
relative error loss 0.16590516
shape of L is 
torch.Size([])
memory (bytes)
4832542720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
4832755712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  676497400.0
relative error loss 0.1655292
shape of L is 
torch.Size([])
memory (bytes)
4835794944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4835794944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  675054850.0
relative error loss 0.16517623
shape of L is 
torch.Size([])
memory (bytes)
4839182336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4839182336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  673624060.0
relative error loss 0.16482614
shape of L is 
torch.Size([])
memory (bytes)
4842303488
| ID | GPU | MEM |
------------------
|  0 | 23% |  0% |
|  1 | 14% | 11% |
memory (bytes)
4842401792
| ID | GPU | MEM |
------------------
|  0 | 23% |  0% |
|  1 | 99% | 11% |
error is  672540400.0
relative error loss 0.16456099
shape of L is 
torch.Size([])
memory (bytes)
4845625344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4845625344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  671749900.0
relative error loss 0.16436756
shape of L is 
torch.Size([])
memory (bytes)
4848779264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
4848779264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  670998500.0
relative error loss 0.1641837
shape of L is 
torch.Size([])
memory (bytes)
4852068352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4852068352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  669962500.0
relative error loss 0.16393021
time to take a step is 280.2562336921692
it  2 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4855050240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4855291904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  669962500.0
relative error loss 0.16393021
shape of L is 
torch.Size([])
memory (bytes)
4858503168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
4858503168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  669722100.0
relative error loss 0.1638714
shape of L is 
torch.Size([])
memory (bytes)
4861587456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4861587456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 11% |
error is  668790800.0
relative error loss 0.16364351
shape of L is 
torch.Size([])
memory (bytes)
4864901120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4864929792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  668356100.0
relative error loss 0.16353714
shape of L is 
torch.Size([])
memory (bytes)
4868145152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4868145152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  667722240.0
relative error loss 0.16338205
shape of L is 
torch.Size([])
memory (bytes)
4871360512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
4871360512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  666823400.0
relative error loss 0.16316213
shape of L is 
torch.Size([])
memory (bytes)
4874399744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
4874399744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  666495500.0
relative error loss 0.16308188
shape of L is 
torch.Size([])
memory (bytes)
4877795328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
4877795328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  665455900.0
relative error loss 0.1628275
shape of L is 
torch.Size([])
memory (bytes)
4881014784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4881014784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  665081340.0
relative error loss 0.16273586
shape of L is 
torch.Size([])
memory (bytes)
4884221952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4884221952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  664447500.0
relative error loss 0.16258076
time to take a step is 279.97176361083984
it  3 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4887408640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4887408640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  664447500.0
relative error loss 0.16258076
shape of L is 
torch.Size([])
memory (bytes)
4890513408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
4890513408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  664497400.0
relative error loss 0.16259298
shape of L is 
torch.Size([])
memory (bytes)
4893876224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4893876224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  664164860.0
relative error loss 0.1625116
shape of L is 
torch.Size([])
memory (bytes)
4896878592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4896878592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  663798300.0
relative error loss 0.16242191
shape of L is 
torch.Size([])
memory (bytes)
4900270080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4900298752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 11% |
error is  663310100.0
relative error loss 0.16230245
shape of L is 
torch.Size([])
memory (bytes)
4903292928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4903481344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  663028740.0
relative error loss 0.16223362
shape of L is 
torch.Size([])
memory (bytes)
4906704896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
4906725376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  662732300.0
relative error loss 0.16216108
shape of L is 
torch.Size([])
memory (bytes)
4909793280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
4909944832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  662474500.0
relative error loss 0.162098
shape of L is 
torch.Size([])
memory (bytes)
4913156096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4913156096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  662259460.0
relative error loss 0.16204539
shape of L is 
torch.Size([])
memory (bytes)
4916232192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4916232192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  661943040.0
relative error loss 0.16196796
time to take a step is 279.46227049827576
it  4 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4919562240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4919582720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  661943040.0
relative error loss 0.16196796
shape of L is 
torch.Size([])
memory (bytes)
4922798080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4922802176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  661655040.0
relative error loss 0.1618975
shape of L is 
torch.Size([])
memory (bytes)
4925943808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4925943808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  661446660.0
relative error loss 0.1618465
shape of L is 
torch.Size([])
memory (bytes)
4929232896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4929232896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  661240060.0
relative error loss 0.16179596
shape of L is 
torch.Size([])
memory (bytes)
4932276224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4932456448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  661071900.0
relative error loss 0.1617548
shape of L is 
torch.Size([])
memory (bytes)
4935667712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4935667712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  660796900.0
relative error loss 0.16168752
shape of L is 
torch.Size([])
memory (bytes)
4938788864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
4938788864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  660685060.0
relative error loss 0.16166015
shape of L is 
torch.Size([])
memory (bytes)
4941869056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4942098432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  660577300.0
relative error loss 0.16163377
shape of L is 
torch.Size([])
memory (bytes)
4945285120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4945313792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  660364300.0
relative error loss 0.16158167
shape of L is 
torch.Size([])
memory (bytes)
4948443136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
4948533248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  660239900.0
relative error loss 0.16155122
time to take a step is 280.58671736717224
it  5 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4951748608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
4951748608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  660239900.0
relative error loss 0.16155122
shape of L is 
torch.Size([])
memory (bytes)
4954755072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
4954935296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  660070900.0
relative error loss 0.16150987
shape of L is 
torch.Size([])
memory (bytes)
4958171136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4958175232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  659961600.0
relative error loss 0.16148312
shape of L is 
torch.Size([])
memory (bytes)
4961378304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4961378304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  659770400.0
relative error loss 0.16143633
shape of L is 
torch.Size([])
memory (bytes)
4964610048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4964610048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  659587100.0
relative error loss 0.1613915
shape of L is 
torch.Size([])
memory (bytes)
4967800832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4967800832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  659377900.0
relative error loss 0.16134031
shape of L is 
torch.Size([])
memory (bytes)
4970967040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
4971057152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  659270140.0
relative error loss 0.16131394
shape of L is 
torch.Size([])
memory (bytes)
4974227456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
4974256128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  659095800.0
relative error loss 0.16127129
shape of L is 
torch.Size([])
memory (bytes)
4977328128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4977328128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  658974700.0
relative error loss 0.16124165
shape of L is 
torch.Size([])
memory (bytes)
4980670464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4980678656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  658799100.0
relative error loss 0.16119869
time to take a step is 280.5974774360657
it  6 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4983824384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4983824384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  658799100.0
relative error loss 0.16119869
shape of L is 
torch.Size([])
memory (bytes)
4987109376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4987109376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  658620900.0
relative error loss 0.16115509
shape of L is 
torch.Size([])
memory (bytes)
4990238720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
4990238720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  658525440.0
relative error loss 0.16113172
shape of L is 
torch.Size([])
memory (bytes)
4993359872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
4993552384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  658496260.0
relative error loss 0.16112459
shape of L is 
torch.Size([])
memory (bytes)
4996722688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
4996751360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  658236700.0
relative error loss 0.16106106
shape of L is 
torch.Size([])
memory (bytes)
4999880704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4999966720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  658175740.0
relative error loss 0.16104616
shape of L is 
torch.Size([])
memory (bytes)
5003182080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
5003182080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  658065900.0
relative error loss 0.16101928
shape of L is 
torch.Size([])
memory (bytes)
5006352384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5006352384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  658008300.0
relative error loss 0.16100518
shape of L is 
torch.Size([])
memory (bytes)
5009620992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5009625088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  657875460.0
relative error loss 0.16097268
shape of L is 
torch.Size([])
memory (bytes)
5012709376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5012709376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 11% |
error is  657790700.0
relative error loss 0.16095194
time to take a step is 280.5806350708008
it  7 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5016027136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5016055808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  657790700.0
relative error loss 0.16095194
shape of L is 
torch.Size([])
memory (bytes)
5019234304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5019234304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  657723650.0
relative error loss 0.16093554
shape of L is 
torch.Size([])
memory (bytes)
5022490624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
5022490624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  657627400.0
relative error loss 0.16091198
shape of L is 
torch.Size([])
memory (bytes)
5025677312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5025705984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  657499900.0
relative error loss 0.16088079
shape of L is 
torch.Size([])
memory (bytes)
5028843520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5028843520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  657430300.0
relative error loss 0.16086376
shape of L is 
torch.Size([])
memory (bytes)
5032108032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5032136704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  657343000.0
relative error loss 0.16084239
shape of L is 
torch.Size([])
memory (bytes)
5035323392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5035323392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 11% |
error is  657287940.0
relative error loss 0.16082892
shape of L is 
torch.Size([])
memory (bytes)
5038534656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5038563328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  657153000.0
relative error loss 0.16079591
shape of L is 
torch.Size([])
memory (bytes)
5041745920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5041745920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  657088260.0
relative error loss 0.16078007
shape of L is 
torch.Size([])
memory (bytes)
5044985856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5044985856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  657026560.0
relative error loss 0.16076496
time to take a step is 282.14833545684814
it  8 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5048172544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5048172544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  657026560.0
relative error loss 0.16076496
shape of L is 
torch.Size([])
memory (bytes)
5051342848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5051342848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656903940.0
relative error loss 0.16073497
shape of L is 
torch.Size([])
memory (bytes)
5054611456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5054640128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656926200.0
relative error loss 0.16074042
shape of L is 
torch.Size([])
memory (bytes)
5057601536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5057785856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656838140.0
relative error loss 0.16071887
shape of L is 
torch.Size([])
memory (bytes)
5061074944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5061074944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  656759040.0
relative error loss 0.16069952
shape of L is 
torch.Size([])
memory (bytes)
5064151040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5064151040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656706800.0
relative error loss 0.16068673
shape of L is 
torch.Size([])
memory (bytes)
5067509760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5067509760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  656607500.0
relative error loss 0.16066243
shape of L is 
torch.Size([])
memory (bytes)
5070553088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5070553088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  656554000.0
relative error loss 0.16064933
shape of L is 
torch.Size([])
memory (bytes)
5073907712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5073907712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656502500.0
relative error loss 0.16063675
shape of L is 
torch.Size([])
memory (bytes)
5077147648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5077147648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656458750.0
relative error loss 0.16062604
time to take a step is 280.7132182121277
it  9 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5080293376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5080293376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656458750.0
relative error loss 0.16062604
shape of L is 
torch.Size([])
memory (bytes)
5083574272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5083574272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  656399600.0
relative error loss 0.16061157
shape of L is 
torch.Size([])
memory (bytes)
5086584832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5086584832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  656321540.0
relative error loss 0.16059247
shape of L is 
torch.Size([])
memory (bytes)
5090013184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
5090013184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656267260.0
relative error loss 0.16057917
shape of L is 
torch.Size([])
memory (bytes)
5093187584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5093187584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656221200.0
relative error loss 0.16056791
shape of L is 
torch.Size([])
memory (bytes)
5096435712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5096435712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656182000.0
relative error loss 0.16055833
shape of L is 
torch.Size([])
memory (bytes)
5099659264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5099659264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656125700.0
relative error loss 0.16054454
shape of L is 
torch.Size([])
memory (bytes)
5102694400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5102874624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  656140300.0
relative error loss 0.1605481
shape of L is 
torch.Size([])
memory (bytes)
5106069504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5106098176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656081660.0
relative error loss 0.16053377
shape of L is 
torch.Size([])
memory (bytes)
5109227520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5109227520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  656003300.0
relative error loss 0.1605146
time to take a step is 281.0239517688751
it  10 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5112500224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5112528896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  656003300.0
relative error loss 0.1605146
shape of L is 
torch.Size([])
memory (bytes)
5115682816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5115682816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  655922940.0
relative error loss 0.16049492
shape of L is 
torch.Size([])
memory (bytes)
5118959616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5118963712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655853600.0
relative error loss 0.16047795
shape of L is 
torch.Size([])
memory (bytes)
5122093056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5122093056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  655816200.0
relative error loss 0.1604688
shape of L is 
torch.Size([])
memory (bytes)
5125386240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5125386240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  655761660.0
relative error loss 0.16045547
shape of L is 
torch.Size([])
memory (bytes)
5128564736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5128564736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655727900.0
relative error loss 0.1604472
shape of L is 
torch.Size([])
memory (bytes)
5131804672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5131804672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655688200.0
relative error loss 0.1604375
shape of L is 
torch.Size([])
memory (bytes)
5134991360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5135020032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655703300.0
relative error loss 0.16044119
shape of L is 
torch.Size([])
memory (bytes)
5138071552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5138239488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655651300.0
relative error loss 0.16042846
shape of L is 
torch.Size([])
memory (bytes)
5141422080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5141422080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655597800.0
relative error loss 0.16041538
time to take a step is 280.4673433303833
it  11 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5144580096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5144580096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655597800.0
relative error loss 0.16041538
shape of L is 
torch.Size([])
memory (bytes)
5147881472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5147881472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  655537400.0
relative error loss 0.1604006
shape of L is 
torch.Size([])
memory (bytes)
5151014912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5151014912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655474700.0
relative error loss 0.16038525
shape of L is 
torch.Size([])
memory (bytes)
5154283520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5154312192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655468000.0
relative error loss 0.16038363
shape of L is 
torch.Size([])
memory (bytes)
5157445632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5157445632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655382800.0
relative error loss 0.16036277
shape of L is 
torch.Size([])
memory (bytes)
5160738816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5160738816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  655360500.0
relative error loss 0.16035731
shape of L is 
torch.Size([])
memory (bytes)
5163827200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5163958272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655319300.0
relative error loss 0.16034722
shape of L is 
torch.Size([])
memory (bytes)
5167071232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5167071232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655290400.0
relative error loss 0.16034015
shape of L is 
torch.Size([])
memory (bytes)
5170393088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5170393088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655245060.0
relative error loss 0.16032906
shape of L is 
torch.Size([])
memory (bytes)
5173436416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5173608448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 11% |
error is  655205900.0
relative error loss 0.16031948
time to take a step is 284.23991084098816
it  12 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5176819712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5176819712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655205900.0
relative error loss 0.16031948
shape of L is 
torch.Size([])
memory (bytes)
5180035072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% | 11% |
memory (bytes)
5180035072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655178500.0
relative error loss 0.16031277
shape of L is 
torch.Size([])
memory (bytes)
5183217664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5183242240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  655128300.0
relative error loss 0.1603005
shape of L is 
torch.Size([])
memory (bytes)
5186355200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5186355200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  655075100.0
relative error loss 0.16028747
shape of L is 
torch.Size([])
memory (bytes)
5189685248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5189685248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 11% |
error is  655043840.0
relative error loss 0.16027983
shape of L is 
torch.Size([])
memory (bytes)
5192876032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5192876032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  655001860.0
relative error loss 0.16026956
shape of L is 
torch.Size([])
memory (bytes)
5196115968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5196115968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  654949100.0
relative error loss 0.16025665
shape of L is 
torch.Size([])
memory (bytes)
5199298560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5199298560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  654907900.0
relative error loss 0.16024657
shape of L is 
torch.Size([])
memory (bytes)
5202546688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5202546688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654885100.0
relative error loss 0.160241
shape of L is 
torch.Size([])
memory (bytes)
5205602304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5205602304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654863100.0
relative error loss 0.1602356
time to take a step is 319.96403431892395
it  13 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5208977408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5208977408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654863100.0
relative error loss 0.1602356
shape of L is 
torch.Size([])
memory (bytes)
5212143616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5212143616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  654798100.0
relative error loss 0.1602197
shape of L is 
torch.Size([])
memory (bytes)
5215412224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5215412224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654830100.0
relative error loss 0.16022752
shape of L is 
torch.Size([])
memory (bytes)
5218627584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5218627584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654771200.0
relative error loss 0.16021311
shape of L is 
torch.Size([])
memory (bytes)
5221847040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5221847040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  654741500.0
relative error loss 0.16020584
shape of L is 
torch.Size([])
memory (bytes)
5225037824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5225062400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  654688260.0
relative error loss 0.16019282
shape of L is 
torch.Size([])
memory (bytes)
5228236800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5228236800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654670100.0
relative error loss 0.16018838
shape of L is 
torch.Size([])
memory (bytes)
5231501312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5231501312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  654636800.0
relative error loss 0.16018023
shape of L is 
torch.Size([])
memory (bytes)
5234642944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5234642944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654615550.0
relative error loss 0.16017503
shape of L is 
torch.Size([])
memory (bytes)
5237936128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5237936128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654597900.0
relative error loss 0.1601707
time to take a step is 291.3839707374573
it  14 : 2375351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5241118720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5241147392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  654597900.0
relative error loss 0.1601707
shape of L is 
torch.Size([])
memory (bytes)
5244297216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5244375040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654554600.0
relative error loss 0.16016012
shape of L is 
torch.Size([])
memory (bytes)
5247598592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5247598592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654579200.0
relative error loss 0.16016613
shape of L is 
torch.Size([])
memory (bytes)
5250818048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5250818048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  654536960.0
relative error loss 0.1601558
shape of L is 
torch.Size([])
memory (bytes)
5254037504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5254037504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  654506000.0
relative error loss 0.16014822
shape of L is 
torch.Size([])
memory (bytes)
5257236480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
5257240576
| ID | GPU | MEM |
------------------
|  0 |  5% |  0% |
|  1 | 99% | 11% |
error is  654476540.0
relative error loss 0.16014102
shape of L is 
torch.Size([])
memory (bytes)
5260451840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5260451840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654452200.0
relative error loss 0.16013506
shape of L is 
torch.Size([])
memory (bytes)
5263667200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
5263667200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  654416640.0
relative error loss 0.16012636
shape of L is 
torch.Size([])
memory (bytes)
5266882560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5266882560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 11% |
error is  654402300.0
relative error loss 0.16012286
shape of L is 
torch.Size([])
memory (bytes)
5270093824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5270093824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  654384100.0
relative error loss 0.1601184
time to take a step is 292.96656250953674
sum tnnu_Z after tensor(11964926., device='cuda:0')
shape of features
(5084,)
shape of features
(5084,)
number of orig particles 20336
number of new particles after remove low mass 18482
tnuZ shape should be parts x labs
torch.Size([20336, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  772357950.0
relative error without small mass is  0.1889849
nnu_Z shape should be number of particles by maxV
(20336, 702)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
shape of features
(20336,)
Tue Jan 31 10:34:49 EST 2023
