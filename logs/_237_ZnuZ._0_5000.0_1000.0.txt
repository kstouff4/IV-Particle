Thu Feb 2 04:28:07 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 33872342
numbers of Z: 30871
shape of features
(30871,)
shape of features
(30871,)
ZX	Vol	Parts	Cubes	Eps
Z	0.025009060920458112	30871	30.871	0.09322138741145582
X	0.021251020529320465	664	0.664	0.3174952535796215
X	0.022168099796273884	19664	19.664	0.1040763854301152
X	0.022200833219680936	2026	2.026	0.22211434664479512
X	0.021792356491852426	4247	4.247	0.17248075501244534
X	0.022264785449361824	36479	36.479	0.08482525944902433
X	0.021964423493595512	10493	10.493	0.12792046966776352
X	0.02201894193646006	50911	50.911	0.075624341528638
X	0.022405921276831595	35708	35.708	0.08561156311099373
X	0.022240756092820238	5068	5.068	0.16372097175839248
X	0.021830346006241223	10955	10.955	0.1258392377951739
X	0.02260487601090987	6428	6.428	0.1520694891639688
X	0.021774474222128207	110929	110.929	0.05811668844285765
X	0.022112000311899586	9106	9.106	0.13441058210826495
X	0.0227848173681791	195789	195.789	0.04882239587701151
X	0.022032127105892127	18475	18.475	0.10604511330089976
X	0.02249774673278893	28707	28.707	0.09219705978265928
X	0.022114568559646035	51240	51.24	0.07557122288416955
X	0.022055603605007187	34725	34.725	0.08595917350872083
X	0.0231198472009678	197002	197.002	0.04895962785986521
X	0.02409566389965665	108517	108.517	0.060554585995826145
X	0.022129072287159816	4871	4.871	0.1656208563932727
X	0.023345601703614324	248677	248.677	0.04544887638327956
X	0.022033943470198783	11616	11.616	0.12378799893406323
X	0.023799060955597785	27144	27.144	0.09571103035913374
X	0.023201766257348152	6652	6.652	0.1516548378024694
X	0.023100911340642093	74684	74.684	0.06762913733584336
X	0.02364520958569059	60816	60.816	0.07298637125200846
X	0.021935372797861542	9633	9.633	0.1315611980904284
X	0.023224559590163677	49034	49.034	0.0779501080435688
X	0.024395389396034244	1227748	1227.748	0.027085249453800003
X	0.021919504388903575	5824	5.824	0.15554991853682099
X	0.024794422997544308	479348	479.348	0.03725927174795823
X	0.023265299479405508	22155	22.155	0.10164334860869231
X	0.021846801531850087	3852	3.852	0.17833388124488894
X	0.02178720909177708	3249	3.249	0.1885749335427451
X	0.023644923601983864	109372	109.372	0.06001741053787286
X	0.02378262535165007	73144	73.144	0.06876387822824606
X	0.02139880682006307	1312	1.312	0.25360155238796395
X	0.02196850600695042	5673	5.673	0.15703480270561956
X	0.021691986105588545	1929	1.929	0.22403834886584598
X	0.02192029236706455	3291	3.291	0.1881508364408173
X	0.02106732688425125	937	0.937	0.2822430658663995
X	0.020844779547088202	604	0.604	0.3255768042233376
X	0.02158611636736562	2101	2.101	0.2173949679222388
X	0.02173011181771695	957	0.957	0.28317180412994414
X	0.021741894675338106	1996	1.996	0.2216727369870328
X	0.021864918980063888	6879	6.879	0.14703052940929054
X	0.02187355001653538	2900	2.9	0.1961133199861863
X	0.021675409650378796	2364	2.364	0.20930195293327392
X	0.022202621084809755	10994	10.994	0.1264007075124867
X	0.022770511956612265	12921	12.921	0.12078849114168691
X	0.022147724345335042	3355	3.355	0.1875910848768945
X	0.022041842819804945	5246	5.246	0.16136395814937046
X	0.021797723103568496	1331	1.331	0.25394774377934665
X	0.022139442693281105	8186	8.186	0.13932583291299458
X	0.02191767577189193	5473	5.473	0.15880213976592547
X	0.02176793587302287	987	0.987	0.28043574277031186
X	0.021919757896458433	2335	2.335	0.2109516642812325
X	0.022069834023581876	2408	2.408	0.20927347932030205
X	0.022064008727466083	3568	3.568	0.18354949401582513
X	0.022835933126131674	12783	12.783	0.1213375787610273
X	0.021893138772182283	1340	1.34	0.2537470939527128
X	0.022302956787248588	20788	20.788	0.10237248485908156
X	0.022024290890652054	4797	4.797	0.1662049951510869
X	0.021918277862008912	2094	2.094	0.21874757656288066
X	0.021871185762833863	2307	2.307	0.2116451130427222
X	0.022087973793907416	1981	1.981	0.22340375453137543
X	0.021778972676943474	2274	2.274	0.21236469092122048
X	0.021943927120809323	2651	2.651	0.2022871092223191
X	0.021601290596742494	2089	2.089	0.2178614650539724
X	0.0212005584113347	1788	1.788	0.22803066926623067
X	0.021812371844379536	3593	3.593	0.18242396537917685
X	0.021509919960085583	1972	1.972	0.22177399911546444
X	0.021615283488742493	1267	1.267	0.25743135361927033
X	0.02190884539528043	2946	2.946	0.1951921057896056
X	0.022158044877714125	14823	14.823	0.11434006716988758
X	0.021981105347270838	1914	1.914	0.22561563782153998
X	0.022794186331157273	364	0.364	0.3971070251408431
X	0.021805730271064073	2151	2.151	0.21642632005933626
X	0.021837683894571133	5373	5.373	0.15958665734021843
X	0.021891297258622183	2910	2.91	0.1959413833036804
X	0.021444561129130298	950	0.95	0.2826167204679721
X	0.02202071856083281	5116	5.116	0.1626673219785204
X	0.021614103262200295	1881	1.881	0.22565734572332535
X	0.021903471458150827	2870	2.87	0.19688396705879918
X	0.021503144981504338	700	0.7	0.31318526034037675
X	0.02206224484603587	7887	7.887	0.14090052773766237
X	0.021606334120035646	933	0.933	0.2850360655446031
X	0.022163019106466383	3071	3.071	0.19324860525334467
X	0.02191988778983127	2872	2.872	0.19688741802160162
X	0.021935345636007565	3753	3.753	0.18013107431547074
X	0.021772744125463626	758	0.758	0.30625368357767263
X	0.021610446106418123	1021	1.021	0.27661729494767645
X	0.021994983913167076	6624	6.624	0.14918818884994145
X	0.02233534573963939	4541	4.541	0.17006452322442528
X	0.021918239177130868	3163	3.163	0.19064942068370772
X	0.02210849334422407	4888	4.888	0.1653773327797656
X	0.022566691219154954	13508	13.508	0.11865670336235985
X	0.021907468092950664	9099	9.099	0.13402922538126838
X	0.022018218336579617	2468	2.468	0.20740171322859294
X	0.022554902751772583	20118	20.118	0.10388481045191286
X	0.022049196547441776	7778	7.778	0.14152775249619257
X	0.022182293116063456	5092	5.092	0.1633199913522434
X	0.021709872868051868	1342	1.342	0.25291131216174795
X	0.02193804192388178	2733	2.733	0.20022551073411443
X	0.021738856226396354	1858	1.858	0.2270197599960032
X	0.022177344096423517	6823	6.823	0.14813057928465903
X	0.02134396132807961	336	0.336	0.3990052248334526
X	0.021990958281220392	3906	3.906	0.17789782507651009
X	0.021775667910391282	2044	2.044	0.22003758229710652
X	0.02164240863857071	1047	1.047	0.27444351334087486
X	0.021780003725334243	1584	1.584	0.23957100651218374
X	0.021598485153588896	1793	1.793	0.22923503889839003
X	0.022009263072162843	2926	2.926	0.19593426551405568
X	0.02206812258672855	3841	3.841	0.17910471825758942
X	0.022152161442532473	1313	1.313	0.25647820206832334
X	0.021918418443247573	1324	1.324	0.2548631674420325
X	0.02178367403056941	1336	1.336	0.2535760449521378
X	0.022249447758549784	8638	8.638	0.13707829020589737
X	0.021906270748618305	996	0.996	0.2801795096338303
X	0.022170810339398023	10653	10.653	0.12767421998739714
X	0.021792679945447497	2774	2.774	0.1987931404907364
X	0.021531875385542684	1517	1.517	0.24212079559164143
X	0.0217549165382183	2151	2.151	0.21625807708593414
X	0.02166798767987559	1834	1.834	0.22775770756386318
X	0.0218452534293887	3400	3.4	0.1859057075739149
X	0.02181406151559847	1351	1.351	0.2527514921386706
X	0.02191239922350202	2424	2.424	0.2083143104343451
X	0.022350332864344354	11724	11.724	0.12399458762180623
X	0.022595574039372655	3816	3.816	0.18091378964662191
X	0.021947496541635844	3782	3.782	0.17970265278711703
X	0.02173032194952828	1064	1.064	0.2733431390996333
X	0.021866197637313824	6731	6.731	0.14810324000202213
X	0.021586656498382078	1609	1.609	0.23761641845678075
X	0.021507691123007683	9397	9.397	0.1317856697334285
X	0.022048642158280365	2011	2.011	0.22215525604982564
X	0.021164588113836553	1093	1.093	0.2685323132329482
X	0.02225362720866796	1127	1.127	0.2702872943064903
X	0.021441664866093606	1730	1.73	0.23142172876278566
X	0.02223364319162202	1972	1.972	0.22423388446250783
X	0.02192077226384533	2854	2.854	0.19730312343485357
X	0.02174264481915471	1106	1.106	0.2698893504869606
X	0.02457645436386328	7085	7.085	0.15137696405841553
X	0.023551819493101	34470	34.47	0.088076705239081
X	0.022244531311812626	5779	5.779	0.15671967477413123
X	0.021647967964780978	1542	1.542	0.24123718624761964
X	0.02187701896061776	1759	1.759	0.23169007736812008
X	0.022939509035069923	2763	2.763	0.20248883322417024
X	0.02219483825487001	1349	1.349	0.25433922577006723
X	0.02192486006248578	2356	2.356	0.21033934378105446
X	0.02203745545226191	1590	1.59	0.24020835422640752
X	0.022212146021471823	7917	7.917	0.14104032226991142
X	0.02185059949478437	2841	2.841	0.19739252737306548
X	0.02198037692556734	15341	15.341	0.11273544631816956
X	0.022175641937158125	3442	3.442	0.18607517875004
X	0.02283233252303859	18063	18.063	0.10812350931214329
X	0.02177321088595524	1590	1.59	0.23924440212747608
X	0.021851932579353067	2259	2.259	0.21307109624971537
X	0.02205628557291788	7536	7.536	0.14304208399745166
X	0.021763014573707152	492	0.492	0.3536589474340252
X	0.024160062939756772	8049	8.049	0.14425095974045093
X	0.021922716432191375	1440	1.44	0.24784336185663539
X	0.022152661809987914	5632	5.632	0.15785357562586852
X	0.021337466293143627	878	0.878	0.28965616566195385
X	0.021921556736039877	2385	2.385	0.20947281523350203
X	0.021936735951117618	1594	1.594	0.23964105906172908
X	0.0219030100045893	2126	2.126	0.2175939655721209
X	0.02151585363623428	1690	1.69	0.23350201118585281
X	0.02180536908958869	3013	3.013	0.19342893330472943
X	0.02076941642517904	278	0.278	0.42117233129286846
X	0.02180988186032418	1295	1.295	0.2563270945748527
X	0.02176357082472995	1558	1.558	0.2408357196411329
X	0.022059509774894594	4765	4.765	0.16666496572097855
X	0.02168970812493797	1699	1.699	0.23371510208249016
X	0.02358392297440293	5278	5.278	0.1647083262524006
X	0.0219323064884212	6236	6.236	0.15207555142892065
X	0.021531522699591025	3033	3.033	0.19219147119097496
X	0.022155578164598903	6370	6.37	0.15151229796577462
X	0.02196371301714991	7538	7.538	0.14282904831598556
X	0.02124962346046313	1263	1.263	0.2562413985958191
X	0.02259192583176791	3202	3.202	0.19179795634384078
X	0.022111778782567218	4439	4.439	0.17078360576190987
X	0.021745089838408845	3619	3.619	0.18179874229043239
X	0.02146525065587672	1545	1.545	0.24040075474999678
X	0.02191097873991795	5553	5.553	0.15801974913573114
X	0.022017371623108663	8671	8.671	0.1364265077482202
X	0.021888760662718942	2693	2.693	0.20106120130620783
X	0.02162066060262138	4832	4.832	0.16478360935739153
X	0.02218236088661757	4832	4.832	0.1661984415201699
X	0.02228818670285809	29105	29.105	0.09148903451117918
X	0.022114073928465327	1338	1.338	0.25472459548457893
X	0.0205484074844267	375	0.375	0.37982391031538976
X	0.022004296525187202	2739	2.739	0.20028041632833937
X	0.021763849672974424	1639	1.639	0.23680212099800765
X	0.021787656773410432	1801	1.801	0.22956143846090338
X	0.021920108579603026	2734	2.734	0.2001465293532583
X	0.021511894590512903	1278	1.278	0.2562807202006985
X	0.021792539080708908	1354	1.354	0.252481593803014
X	0.02194055412744293	2670	2.67	0.20179579404846099
X	0.022093091416070958	1169	1.169	0.26636714722649846
X	0.022008957171397545	6582	6.582	0.14953649459318843
X	0.02103045072586243	1512	1.512	0.2404910562577275
X	0.022176924221963778	12589	12.589	0.12077304783579686
X	0.021969046025641526	5814	5.814	0.15575621696820824
X	0.021918580223735527	2174	2.174	0.21603177128372922
X	0.02175119794436711	470	0.47	0.35902810011313846
X	0.023059856510105926	5121	5.121	0.16513302688387954
X	0.021910190131225377	3576	3.576	0.18298530407526728
X	0.02213731888800782	5146	5.146	0.16263664737301795
X	0.023308336090469166	10049	10.049	0.13237200340996136
X	0.022066340452684984	7379	7.379	0.14407134193293958
X	0.02178222115523068	1242	1.242	0.2598125880881467
X	0.021722796928622973	794	0.794	0.30132262808815863
X	0.02213629668995772	1527	1.527	0.24383084476664002
X	0.021251695667151183	841	0.841	0.29344893030319547
X	0.021070568145625922	606	0.606	0.32638826871277665
X	0.021799297855801574	759	0.759	0.3062435213294478
X	0.021565567479042595	565	0.565	0.33669563099456834
X	0.02172246380650995	3776	3.776	0.17918116895785113
X	0.021787192739418336	1464	1.464	0.2459726006328152
X	0.02137501132188541	4538	4.538	0.1676282794086131
X	0.021786576112352338	2541	2.541	0.20467341672928474
X	0.021762139272721588	2610	2.61	0.20277777777165976
X	0.021491291243445246	1206	1.206	0.26119913474015516
X	0.023549439702418268	6682	6.682	0.15218015979591185
X	0.0219453386990679	3235	3.235	0.18930234811598365
X	0.021211065835245214	483	0.483	0.3528080239102928
X	0.02072613202343734	831	0.831	0.29217232654455805
X	0.021861599511963575	4694	4.694	0.16699864586248978
X	0.020758180241637173	657	0.657	0.31613649672619865
X	0.021527361927830203	1141	1.141	0.2662165530978304
X	0.021887676679474624	1394	1.394	0.2504065186387752
X	0.02293801979227003	4867	4.867	0.16766079673815368
X	0.022533979939207267	6111	6.111	0.15449290513841657
X	0.021902109200049837	1656	1.656	0.23648768820100913
X	0.021709155080938212	1027	1.027	0.2764972561046669
X	0.02191030732915716	4419	4.419	0.17051980313611034
X	0.022123372219512112	6603	6.603	0.14963620191066104
X	0.021201537737933075	1318	1.318	0.2524356282475574
X	0.022097784826862633	7398	7.398	0.1440162410076723
X	0.022328893246163474	54823	54.823	0.0741255849267472
X	0.022012309040293238	2558	2.558	0.20492189048599802
X	0.022056933851154318	8178	8.178	0.13919789833488294
X	0.021026228020949184	468	0.468	0.35549910000636165
X	0.0225002424120256	3791	3.791	0.1810552563087514
X	0.022053649663542087	43842	43.842	0.07952982701812462
X	0.02201829189399053	181675	181.675	0.04948757261120027
X	0.0220192686778677	1646	1.646	0.2373874124521937
X	0.023907704158142133	32131	32.131	0.09061592825547736
X	0.02191375757853119	29776	29.776	0.09028525570497492
X	0.022115536648845486	8025	8.025	0.14020090394033236
X	0.024064038724362035	83195	83.195	0.06613384358788979
X	0.02173660682503214	1360	1.36	0.251893878723565
X	0.021822251895603138	7215	7.215	0.14461757550745144
X	0.022132991953187922	125446	125.446	0.056086856883111845
X	0.022510532661179274	132329	132.329	0.055408566402769845
X	0.02195615818913474	3615	3.615	0.18245230309260718
X	0.022645571032071078	23050	23.05	0.09941168816961314
X	0.023073679799403078	18789	18.789	0.10708724153300252
X	0.022460041720916642	41023	41.023	0.08180773725491819
X	0.022028797328818038	27634	27.634	0.09272190477059371
X	0.02430745092011755	41288	41.288	0.08381180434703256
X	0.022856057119462765	45151	45.151	0.07969749046160048
X	0.023156082824172756	17414	17.414	0.10996526320257793
X	0.021786436089576682	1654	1.654	0.23616573998187668
X	0.022560250002932054	214801	214.801	0.047181223962062944
X	0.02193185776198318	2823	2.823	0.19805608079607215
X	0.021580474211178195	3180	3.18	0.18932644836256002
X	0.021981560055443338	24274	24.274	0.09674734476951681
X	0.02205397359534973	25950	25.95	0.09472177535386428
X	0.02448076230923708	307883	307.883	0.04300111754799084
X	0.02222853953216363	63564	63.564	0.07045306695109273
X	0.02208667379403045	1363	1.363	0.2530529980844065
X	0.022352575888899957	10359	10.359	0.1292220366617982
X	0.022307352536620684	21047	21.047	0.10195752313515985
X	0.02194646103557392	11825	11.825	0.12289129374240979
X	0.023060135299156972	93985	93.985	0.06260385858817397
X	0.02212887949871674	3794	3.794	0.1800061593380905
X	0.021943561120155542	16633	16.633	0.10967614415170594
X	0.022117443350768845	564	0.564	0.3397441127997298
X	0.022040421413145623	2798	2.798	0.19897143308181686
X	0.02227710081889776	37800	37.8	0.08384084051104508
X	0.022253284345553244	43486	43.486	0.07998616694235516
X	0.021944465456165817	19532	19.532	0.10395836253705262
X	0.022172436082510894	10367	10.367	0.12884080738961973
X	0.02252655128434143	260096	260.096	0.04424392814456558
X	0.022049290037116174	8676	8.676	0.13646617606231656
X	0.02223792860566527	21569	21.569	0.10102327428012009
X	0.023314421353876808	81593	81.593	0.06586538678840038
X	0.02178956411332784	3530	3.53	0.18343883308983305
X	0.022085073546233198	53536	53.536	0.07444193330695996
X	0.02405271823794889	241591	241.591	0.04634765911882928
X	0.02313711194456539	202114	202.114	0.048555405293948625
X	0.021878201869757422	8926	8.926	0.13482949697450555
X	0.02184854935710312	4273	4.273	0.1722780330963839
X	0.021376635147216397	5189	5.189	0.16030678023197084
X	0.021666377307300116	2286	2.286	0.21162648824551858
X	0.022062156789711643	24900	24.9	0.09604679406934995
X	0.02219682999323974	7918	7.918	0.14100196102898685
X	0.022022623538032134	22324	22.324	0.09954795596486783
X	0.022661936555796063	58893	58.893	0.0727351681529209
X	0.023291419378906168	39996	39.996	0.08350768900686374
X	0.021941559721572	8276	8.276	0.13840411566975944
X	0.022231190948724153	6086	6.086	0.1540081108664618
X	0.02210584754507506	88319	88.319	0.0630208624829261
X	0.02219481138544602	40858	40.858	0.08159398009158354
X	0.021985047601111782	10791	10.791	0.12677158081165674
X	0.023200493278499157	42429	42.429	0.08177329057786516
X	0.024761231802769894	400786	400.786	0.03953240485835786
X	0.023144818343149336	5061	5.061	0.16598639002272805
X	0.023036225021706207	95784	95.784	0.06218793188649423
X	0.021846459359692205	3621	3.621	0.1820472734389364
X	0.02181695783910012	17757	17.757	0.1071046186498278
X	0.021970101554932682	10474	10.474	0.1280088015472043
X	0.024748739744269777	284595	284.595	0.044303898248435385
X	0.022019662699324077	48548	48.548	0.07683276094424213
X	0.022013500688820745	3588	3.588	0.1830679109692672
X	0.023210095549194575	92941	92.941	0.06297331113857955
X	0.022375533821828185	100811	100.811	0.06054642348039866
X	0.021917579642144608	6273	6.273	0.15174198767135416
X	0.02231867727620501	46114	46.114	0.07851365929866032
X	0.023284206850778408	160830	160.83	0.052508723702090206
X	0.024044817173797226	306831	306.831	0.04279313277718611
X	0.02230985133991475	22961	22.961	0.0990456240366135
X	0.021693730052521176	1352	1.352	0.2522236735172564
X	0.02216610336621675	8289	8.289	0.13880200455284158
X	0.021818306059216617	3512	3.512	0.1838324495274925
X	0.021901226817283768	18630	18.63	0.1055403498208503
X	0.023116395767521954	6519	6.519	0.15249176111131277
X	0.02188753949907282	3890	3.89	0.17786170335383883
X	0.022075726972539445	35969	35.969	0.08498237877686989
X	0.023368489357709993	119654	119.654	0.05801867490554893
X	0.021728695924260088	9734	9.734	0.13069152936529815
X	0.022039973523476863	5780	5.78	0.15622879271121767
X	0.021953318152232542	8109	8.109	0.13937267116177657
X	0.022021561268493806	12717	12.717	0.12008473709029806
X	0.021953044667691518	8406	8.406	0.1377109449830529
X	0.022381496434594514	20685	20.685	0.10266234930240688
X	0.02201857027908502	3015	3.015	0.19401438572990443
X	0.02218260625118395	53258	53.258	0.07468084770937891
X	0.022176939780006236	24701	24.701	0.09647075864721509
X	0.02430484508626082	19648	19.648	0.10734739320732821
X	0.024692994106040513	28432	28.432	0.09540887627494653
X	0.021809154353847567	9660	9.66	0.13118592239833715
X	0.02287125685019789	401790	401.79	0.038467766322411265
X	0.022180730241896793	10485	10.485	0.12837165346503066
X	0.02315385779681729	51541	51.541	0.07658731431400619
X	0.021674013090164163	1266	1.266	0.257732117496531
X	0.022167110036268135	5725	5.725	0.15702827578150008
X	0.021894892660526747	4062	4.062	0.17533460277309287
X	0.022095714623738442	8744	8.744	0.13620696032639
X	0.022871143759550495	3622	3.622	0.18483312769214644
X	0.022519952990348133	63204	63.204	0.07089369922842753
X	0.021949420818453594	3440	3.44	0.18547620763921313
X	0.022022780965580854	5974	5.974	0.1544788331574767
X	0.02331417792052552	251218	251.218	0.04527479065696028
X	0.022157864591130505	28390	28.39	0.09207057113039077
X	0.023251704250252858	34535	34.535	0.08764593464638237
X	0.023881989793018817	110880	110.88	0.059943085922022875
X	0.024052883365670654	170422	170.422	0.05206515462991167
X	0.02198880596881365	4798	4.798	0.16610414413349045
X	0.022061310397143298	4981	4.981	0.16422461577422504
X	0.02198997712133817	24091	24.091	0.0970040760205449
X	0.02132120923477853	469	0.469	0.356899820720872
X	0.022036920690136606	3808	3.808	0.17953590820851753
X	0.0220462998621243	11589	11.589	0.12390721122338458
X	0.021805562123011542	1431	1.431	0.24791866102574572
X	0.022208331121271484	5399	5.399	0.16022644570441918
X	0.021836392649173645	2044	2.044	0.22024192847488375
X	0.023497107143670638	2723	2.723	0.20511098424283372
X	0.023226376415155504	245135	245.135	0.045588857884693086
X	0.02198216475092251	13068	13.068	0.1189288202772196
X	0.022170642386376494	52948	52.948	0.07481285677782114
X	0.021780680051459028	7077	7.077	0.14545904992507278
X	0.02202117534334679	8499	8.499	0.13734859523111456
X	0.02251855692818741	7942	7.942	0.14153707765214782
X	0.024070709504312452	595675	595.675	0.03431572451886834
X	0.023246152656174086	418916	418.916	0.03814240259229633
X	0.022515276884414053	14296	14.296	0.11634674854057286
X	0.02220208871962159	39875	39.875	0.0822680201753916
X	0.021889174895805035	2291	2.291	0.21219481485559044
X	0.02212165084091291	27267	27.267	0.0932667245692251
X	0.022444871787209584	35756	35.756	0.08562279492489797
X	0.023190396705173748	24744	24.744	0.09786170026227518
X	0.021904563017691708	8423	8.423	0.1375168548389043
X	0.022445342860644025	54577	54.577	0.07436561856960541
X	0.02387218244419005	372512	372.512	0.04001756132254701
X	0.022178766693674707	16076	16.076	0.11132340077951693
X	0.02236545499264765	3239	3.239	0.19042425414708092
X	0.02456411833054293	21325	21.325	0.10482641022391068
X	0.023080891302701564	26773	26.773	0.09517404168845515
X	0.02186493672512782	2171	2.171	0.21595476965539517
X	0.024647427091264086	89366	89.366	0.06509282054040483
X	0.021941585232988125	16397	16.397	0.11019651706928162
X	0.0215745674578284	681	0.681	0.31642077883694336
X	0.022591935344025368	51358	51.358	0.07605278306124484
X	0.02338436793629304	30955	30.955	0.0910748103980224
X	0.021796270931085774	917	0.917	0.2875219160914224
X	0.023264872194770605	109450	109.45	0.05967992954054966
X	0.022471421159399405	101078	101.078	0.0605793533385487
X	0.024445751265053505	141808	141.808	0.05565457690418974
X	0.022795062339750267	168136	168.136	0.0513720256833986
X	0.0223972133871483	102173	102.173	0.060295645301879905
X	0.022313995172931287	11452	11.452	0.12490083213570363
X	0.023023779872489162	25147	25.147	0.09710244100331639
X	0.02217935396346826	46198	46.198	0.07830242985320551
X	0.022543423651436578	119744	119.744	0.05731329410066308
X	0.02259554790728036	4467	4.467	0.1716599472628012
X	0.02206851123567335	65429	65.429	0.06960935097536385
X	0.022618514386182186	290553	290.553	0.0426985280862736
X	0.02219219599296398	82423	82.423	0.06457294895979356
X	0.022569621235521677	24589	24.589	0.09718393235453719
X	0.02270190960184707	39974	39.974	0.08281231535673184
X	0.02218003201111289	5610	5.61	0.15812471986324333
X	0.022178427502424488	1468	1.468	0.2472112506028048
X	0.02226891023006779	6061	6.061	0.15430673907219958
X	0.023257905213109966	88178	88.178	0.06413131089683935
X	0.022078046430325425	50883	50.883	0.07570582755184184
X	0.023772422391153594	356493	356.493	0.040551551712475785
X	0.024651799055997858	162992	162.992	0.05327957804648039
X	0.02311587146890757	47760	47.76	0.07851427973401591
X	0.02216060730794888	12749	12.749	0.12023618250356972
X	0.023178782577446447	73608	73.608	0.06803335181997104
X	0.021832735922838624	1655	1.655	0.23628530951573787
X	0.021777673577551837	3065	3.065	0.1922473380350486
X	0.022751988725889957	181953	181.953	0.05000576179546437
X	0.022133227130045147	4490	4.49	0.17018951471251778
X	0.021673494847776902	3048	3.048	0.1922964771446865
X	0.0221958180830035	12319	12.319	0.12168355816282685
X	0.02220689678292378	100172	100.172	0.060522079689551464
X	0.023116699324759	16859	16.859	0.11109589308059578
X	0.024718988018130005	88781	88.781	0.06529855209630002
X	0.021922135079634978	8577	8.577	0.13672538232291673
X	0.021925799890766106	9746	9.746	0.1310316911859233
X	0.02306874934884438	3685	3.685	0.18430149270488919
X	0.02195101710454991	4856	4.856	0.16534535161983635
X	0.023878696658378832	43202	43.202	0.08206706880752838
X	0.022203805875175557	25268	25.268	0.09578235654225638
X	0.022418523649597403	7368	7.368	0.14490580948596696
X	0.021954766417873837	77452	77.452	0.06568991925468622
X	0.022113597839147917	1662	1.662	0.23696059367433991
X	0.024486047440155038	44422	44.422	0.08199238629938495
X	0.021852152410657716	5662	5.662	0.15685852202539707
X	0.022529892116495264	46178	46.178	0.07872415315450867
X	0.022919223350515213	4943	4.943	0.16675150494188812
X	0.022204905531640565	46561	46.561	0.07812839036784736
X	0.021990290875238	6924	6.924	0.14699118969200017
X	0.02367881766905532	40916	40.916	0.0833340101427524
X	0.023175522383009958	52562	52.562	0.07611189764689127
X	0.02230832931348959	145215	145.215	0.05355728124198149
X	0.023018629162916422	6462	6.462	0.1527229019112067
X	0.02154398240832589	5741	5.741	0.15539825807629476
X	0.023260757749728803	114749	114.749	0.058743296782622675
X	0.02206842086338538	6428	6.428	0.15085688292503727
X	0.023192432682716764	455079	455.079	0.03707545616909235
X	0.021945897757274808	1089	1.089	0.2721292077268063
X	0.022523531371040087	47067	47.067	0.07821799062905402
X	0.023832682682396366	112403	112.403	0.05963002746328357
X	0.021722086410917624	2480	2.48	0.20613415249913333
X	0.021891778051877336	60553	60.553	0.07123855866333373
X	0.022273403599566702	23712	23.712	0.09793534707933779
X	0.024793379161149123	422556	422.556	0.038858301750750494
X	0.02199382097618434	4296	4.296	0.1723503372926482
X	0.022047002658055908	17515	17.515	0.10797249513691624
X	0.023198970139700518	6661	6.661	0.15158041464959776
X	0.02201126599015864	3597	3.597	0.1829089098739022
X	0.021929695396877922	13003	13.003	0.11903180307604189
X	0.02240013601827033	19121	19.121	0.10541765503756369
X	0.02206669817917137	7573	7.573	0.1428312157047153
X	0.023240821382103375	29555	29.555	0.09230109142271445
X	0.02193629924297733	15496	15.496	0.11228314918915806
X	0.021685097407572688	2281	2.281	0.21184198181671585
X	0.02196767583248922	6739	6.739	0.14827327662906123
X	0.024859217177229175	266523	266.523	0.045350726827176654
X	0.0231809398992477	35939	35.939	0.08640157994386828
X	0.02390035102640839	190328	190.328	0.05007648824871356
X	0.023179986561539422	28700	28.7	0.09312731770109363
X	0.02188304229458093	8021	8.021	0.1397310945900709
X	0.02285784409534603	5970	5.97	0.15644210286899132
X	0.021632081884176913	4365	4.365	0.17049226877786064
X	0.024068689168127395	337621	337.621	0.04146429912071498
X	0.024387813224076647	12002	12.002	0.12666007648707978
X	0.022045796349130554	20279	20.279	0.10282366635907257
X	0.02228861623515339	39728	39.728	0.08247623007572558
X	0.02414241016949855	46806	46.806	0.08019729103730963
X	0.021777872895194592	2356	2.356	0.20986824181921432
X	0.021832432800932288	3512	3.512	0.18387211642071258
X	0.02447813609501304	164549	164.549	0.05298598748585908
X	0.021969211537972846	12509	12.509	0.12065091670820192
X	0.02178143219653327	27128	27.128	0.09294432661043348
X	0.022243747584493365	30154	30.154	0.09035544779268671
X	0.024851501374934682	106088	106.088	0.06164465926534276
X	0.02388983260387551	143437	143.437	0.055019584259894644
X	0.023551365213361986	143623	143.623	0.054734862451129386
X	0.02218885404480855	87661	87.661	0.06325713005252266
X	0.022008797635584606	7092	7.092	0.1458621229121246
X	0.02226027245481283	39601	39.601	0.08252928975304807
X	0.02274198783569849	15001	15.001	0.1148777896089627
X	0.02230292560630937	6288	6.288	0.15250465526897392
X	0.02314287842690209	11773	11.773	0.12526894214765055
X	0.021925306218797555	4248	4.248	0.17281723533966079
X	0.021899620466098228	14881	14.881	0.11374565708441085
X	0.02194580641519449	7860	7.86	0.1408130804544792
X	0.021289493512270056	524	0.524	0.3437779473658895
X	0.022571206402308495	14355	14.355	0.11628325814561448
X	0.021750381746625632	5911	5.911	0.15438388828971392
X	0.02284957337897035	63254	63.254	0.07121913597279508
X	0.022058206439133136	6953	6.953	0.146937503169448
X	0.02303916873578602	19844	19.844	0.10510237479207553
X	0.021202467823783714	12032	12.032	0.12078588799954341
X	0.02255902170368387	40366	40.366	0.08236983235300571
X	0.02189808591863599	2233	2.233	0.2140453565467476
X	0.022301718330933137	7496	7.496	0.14382561823195744
X	0.02217635630063909	17035	17.035	0.10918995430762882
X	0.02319987125677104	126387	126.387	0.056832189760710104
X	0.022254853326529038	6654	6.654	0.14954801284270908
X	0.021388004728737843	961	0.961	0.2812865466492709
X	0.021989209085067005	3020	3.02	0.19382102618598235
X	0.0225157336309813	253382	253.382	0.044624166168254764
X	0.023740568503558438	299234	299.234	0.042969476121052445
X	0.02203636404060363	10085	10.085	0.1297641357419286
X	0.023162471440745146	11338	11.338	0.12688671327227236
X	0.022742569973603556	26457	26.457	0.09508229317978995
X	0.024143362994004512	115529	115.529	0.05934292060354453
X	0.021888890237488492	1772	1.772	0.23116389773300666
X	0.022880479774283622	7239	7.239	0.1467558819545423
X	0.022024851860441237	9979	9.979	0.13019930254499598
X	0.022211278190885097	27211	27.211	0.09345653672710609
X	0.021861341395820924	8047	8.047	0.13953428619409183
X	0.022831647942161047	116268	116.268	0.058124471765782185
X	0.022053793417019102	8535	8.535	0.13722289967580284
X	0.0238074640873377	9101	9.101	0.13778692492787498
X	0.0218276043059098	1038	1.038	0.27601725950109524
X	0.022014170680463158	20142	20.142	0.10300696213882787
X	0.02209714601127064	11210	11.21	0.1253844052521015
X	0.022075378050336627	28052	28.052	0.09232403419754046
X	0.02302117733930505	70544	70.544	0.06884765455001633
X	0.023268550119022905	324345	324.345	0.04155156367315114
X	0.021945421570572783	7253	7.253	0.1446356717094673
X	0.021980762283732837	3400	3.4	0.18628931430502937
X	0.022648741024450532	13625	13.625	0.11845930627433805
X	0.023247757988260717	210423	210.423	0.047983945589967374
X	0.021961882911107355	23759	23.759	0.09741230160726352
X	0.021475736349707064	1510	1.51	0.24228342560303362
X	0.021993200870228474	3563	3.563	0.18343866150009916
X	0.022622502933862153	18643	18.643	0.10666164676718899
X	0.021687804275077494	1468	1.468	0.24537473479602057
X	0.022474568050658453	39940	39.94	0.08255836769063966
X	0.02202154726365835	2978	2.978	0.1948233673491438
X	0.022289201747385433	15762	15.762	0.1122434647251633
X	0.021841778512956413	9726	9.726	0.13095374148814892
X	0.02203458206260401	3573	3.573	0.18338223738365006
X	0.021533002084608312	1596	1.596	0.23806226710790201
X	0.022620268094532242	115186	115.186	0.05812540834625819
X	0.0240953970728622	258714	258.714	0.04532850395440446
X	0.022091045748086093	133005	133.005	0.05496878054982755
X	0.02219904375131323	28895	28.895	0.09158770770302828
X	0.022813854499212787	23148	23.148	0.09951649338063626
X	0.021905352456068308	11353	11.353	0.12449345958151735
X	0.023503366480010866	8745	8.745	0.139034768964097
X	0.023320418176543203	94643	94.643	0.06269255147827092
X	0.024841259587254517	36326	36.326	0.08810191427556359
X	0.024280808211724327	61223	61.223	0.07347103828374414
X	0.02265309707996429	11727	11.727	0.12454134558542532
X	0.024607756292505188	449445	449.445	0.037972147095486984
X	0.021972655814369586	14915	14.915	0.11378537148325926
X	0.024441997114286153	44554	44.554	0.08186218443816586
X	0.024856508880573527	175516	175.516	0.052124408485299555
X	0.023709810432298938	45038	45.038	0.08074512819169113
X	0.022947586168308366	44230	44.23	0.08035384999972978
X	0.02320654822451694	78307	78.307	0.06667095159287134
X	0.02202367584684378	31173	31.173	0.08906431572354062
X	0.022048875796277647	16189	16.189	0.11084654030781436
X	0.022187664503120968	7161	7.161	0.1457849320886083
X	0.02308672308782469	31941	31.941	0.0897438289736436
X	0.02198282759475611	7664	7.664	0.14208318567055214
X	0.022582536244213268	80689	80.689	0.06541130207271205
X	0.02432023033602317	864473	864.473	0.030413897882191163
X	0.0232786543500587	92144	92.144	0.06321647421771763
X	0.02249953840269567	115103	115.103	0.058035757195063556
X	0.022510851795442786	18894	18.894	0.10601223873299963
X	0.021680185463796263	18514	18.514	0.1054033088437628
X	0.024338479592899746	27537	27.537	0.09596781496101407
X	0.021941409931598033	19060	19.06	0.10480464832970977
X	0.02284872930810856	23388	23.388	0.09922542988626334
X	0.02313944347932368	219875	219.875	0.04721271972919027
X	0.02151870794810495	11976	11.976	0.12157234709557664
X	0.022846844733761154	133383	133.383	0.05553608354995087
X	0.02421769599378361	135762	135.762	0.05629262903310154
X	0.022174077202872747	19752	19.752	0.10393093302360437
X	0.021714481483882764	2447	2.447	0.20703248644918734
X	0.02352400571031611	40910	40.91	0.08315606541636483
X	0.023215669611701618	72207	72.207	0.06850684561627332
X	0.021734775972861952	3256	3.256	0.18828841112027644
X	0.023957083010418045	593519	593.519	0.03430307902198962
X	0.02261065041104987	25247	25.247	0.09639054261915968
X	0.02283649055387342	36756	36.756	0.08532973041888954
X	0.02182496444187525	5803	5.803	0.15551310455705902
X	0.021932289920382972	2779	2.779	0.19909719606166038
X	0.021651051098261757	2456	2.456	0.20657774903307127
X	0.021912349670574118	1664	1.664	0.23614488475681303
X	0.022130109037539782	10838	10.838	0.1268658700913936
X	0.021993023319390015	14175	14.175	0.11576766906846171
X	0.024005576419791525	2712754	2712.754	0.02068397045311989
X	0.021943824896070715	7492	7.492	0.14307755832401847
X	0.022300250992179062	20942	20.942	0.10211680037987454
X	0.021844226139423686	4288	4.288	0.1720655640128585
X	0.021990954366436066	8611	8.611	0.13668792877040747
X	0.02297830973019707	9289	9.289	0.13524349085108928
X	0.022263604933512458	171249	171.249	0.050658906869783256
X	0.022471337586106303	10797	10.797	0.12767580583033897
X	0.024027298686457057	833058	833.058	0.030667372564659214
X	0.02233044341997358	5287	5.287	0.16164515011157052
X	0.023579473727680517	119059	119.059	0.05828953969540185
X	0.022205354652664894	37417	37.417	0.08403552481541536
X	0.0233723869675893	123888	123.888	0.05735323801576444
X	0.0239576097441346	273371	273.371	0.04441847884242874
X	0.022038867646771675	11617	11.617	0.123793667398205
X	0.021900512261762102	2754	2.754	0.19960133689351764
X	0.023975727302709072	99837	99.837	0.062157483397303044
X	0.02289835228446591	13739	13.739	0.1185631428580842
X	0.021969208252501392	30482	30.482	0.0896582619398427
X	0.022503228252751452	28892	28.892	0.09200732543733468
X	0.021944092468864473	13824	13.824	0.11665265646115539
X	0.024597354485772258	45463	45.463	0.08148483113280747
X	0.021630618249487938	17889	17.889	0.10653548554156389
X	0.0240589048917486	203042	203.042	0.049116791685337474
X	0.0230676683641784	290259	290.259	0.04299381679003757
X	0.022032437914962608	11334	11.334	0.12480340918795474
X	0.021856963466014998	6818	6.818	0.14744983010481288
X	0.02293971349446591	66556	66.556	0.07011327585023196
X	0.02209778341651831	19649	19.649	0.10399267587429947
X	0.023945934121064338	43272	43.272	0.08209970657964359
X	0.022787493993910248	9012	9.012	0.13623597202250262
X	0.02211447988214217	46605	46.605	0.07799762984076505
X	0.02318324089744031	45285	45.285	0.07999691858305358
X	0.021928850865082374	90293	90.293	0.06239080613561896
X	0.02199385483504614	36682	36.682	0.0843236601497003
X	0.022287742670683598	10217	10.217	0.12969231142661886
X	0.022203304662134098	7087	7.087	0.14632495361785394
X	0.021951109580203694	12695	12.695	0.12002579489424571
X	0.022023777118521846	31923	31.923	0.08836142310915547
X	0.02255707820543213	81680	81.68	0.06512119659309663
X	0.022123963239171752	143543	143.543	0.05361590445008121
X	0.02195065668452182	11560	11.56	0.1238311466464657
X	0.022164482365027043	22275	22.275	0.09983434205975214
X	0.02297085024838548	38926	38.926	0.08387755531558767
X	0.023007032121808973	166784	166.784	0.05166963628977735
X	0.0239716305557437	37719	37.719	0.08597637600410488
X	0.022655227479702025	64859	64.859	0.0704259519844878
X	0.023131418618356898	7152	7.152	0.14788527076624294
X	0.022142251442166628	21663	21.663	0.1007320624755341
X	0.021835699693154763	2161	2.161	0.21619091948923497
X	0.02345321948776987	8175	8.175	0.1420926352866288
X	0.02268439678863674	56553	56.553	0.07374917997229083
X	0.021971889793328377	21771	21.771	0.10030663886896297
X	0.022630296922217776	39173	39.173	0.08328519492875318
X	0.021860816455965822	1991	1.991	0.2222619085528058
X	0.022009401484131032	7337	7.337	0.14422146557247367
X	0.022265205671639036	55438	55.438	0.07378018146057183
X	0.022019336774233348	22842	22.842	0.0987847796713694
X	0.0223007076276525	15883	15.883	0.11197696767839917
X	0.021773817058552625	2742	2.742	0.19950588290470161
X	0.0217097926488126	4711	4.711	0.1664105504259652
X	0.022723657306563966	19117	19.117	0.10593012798083831
X	0.022035429273043323	20124	20.124	0.1030708210489674
X	0.022842620849076668	67840	67.84	0.06956968399260464
X	0.021771483333259744	2558	2.558	0.20417183235669903
X	0.021946853666133432	13852	13.852	0.11657889334504815
X	0.021937173202440683	3635	3.635	0.1820645601811496
X	0.022056204619113458	13822	13.822	0.11685661390780153
X	0.023740553842095998	232700	232.7	0.04672666882922915
X	0.024181517079232016	17129	17.129	0.11218045008587002
X	0.022156206756337973	30840	30.84	0.08956266612292987
X	0.024732231556195538	115737	115.737	0.05978565995976177
X	0.02205266507914828	24843	24.843	0.09610640839142313
X	0.024486690503322226	81047	81.047	0.067101317809991
X	0.024433615218311597	145122	145.122	0.0552185279933661
X	0.023903285208913966	4076	4.076	0.18033269483422878
X	0.021996131062898126	25287	25.287	0.09545887900908519
X	0.022132662286870122	14812	14.812	0.11432467742373763
X	0.0232204741198556	86192	86.192	0.06458545273455486
X	0.022429662996720126	128333	128.333	0.05591068338068577
X	0.022773880810090154	19873	19.873	0.10464645889650545
X	0.021993371662797894	45225	45.225	0.07863895721622698
X	0.02188516888322725	3119	3.119	0.19144535892728073
X	0.021860496916946057	7438	7.438	0.14324120042075922
X	0.021944467100649075	8985	8.985	0.13466941880063663
X	0.02243132406254354	12587	12.587	0.12123952489175097
X	0.023952445542404256	187385	187.385	0.050373834709197846
X	0.024359161711158366	447043	447.043	0.03791150394809176
X	0.022013136250616655	9207	9.207	0.1337174007967305
X	0.021953211353832783	17694	17.694	0.10745435258655613
X	0.022840164955993437	11275	11.275	0.12653020991239555
X	0.023212014111646646	110584	110.584	0.0594301523577338
X	0.023006476468150832	9646	9.646	0.13360856024324266
X	0.022014987752823817	6609	6.609	0.1493462190229424
X	0.0241059548142934	36718	36.718	0.0869124061738447
X	0.023219431408526447	95249	95.249	0.062468879860684424
X	0.022491631277082952	3455	3.455	0.18672005551257856
X	0.02336472639929402	44273	44.273	0.0808116391944075
X	0.021884978301824957	2293	2.293	0.21211954581343792
time for making epsilon is 1.798994541168213
epsilons are
[0.3174952535796215, 0.1040763854301152, 0.22211434664479512, 0.17248075501244534, 0.08482525944902433, 0.12792046966776352, 0.075624341528638, 0.08561156311099373, 0.16372097175839248, 0.1258392377951739, 0.1520694891639688, 0.05811668844285765, 0.13441058210826495, 0.04882239587701151, 0.10604511330089976, 0.09219705978265928, 0.07557122288416955, 0.08595917350872083, 0.04895962785986521, 0.060554585995826145, 0.1656208563932727, 0.04544887638327956, 0.12378799893406323, 0.09571103035913374, 0.1516548378024694, 0.06762913733584336, 0.07298637125200846, 0.1315611980904284, 0.0779501080435688, 0.027085249453800003, 0.15554991853682099, 0.03725927174795823, 0.10164334860869231, 0.17833388124488894, 0.1885749335427451, 0.06001741053787286, 0.06876387822824606, 0.25360155238796395, 0.15703480270561956, 0.22403834886584598, 0.1881508364408173, 0.2822430658663995, 0.3255768042233376, 0.2173949679222388, 0.28317180412994414, 0.2216727369870328, 0.14703052940929054, 0.1961133199861863, 0.20930195293327392, 0.1264007075124867, 0.12078849114168691, 0.1875910848768945, 0.16136395814937046, 0.25394774377934665, 0.13932583291299458, 0.15880213976592547, 0.28043574277031186, 0.2109516642812325, 0.20927347932030205, 0.18354949401582513, 0.1213375787610273, 0.2537470939527128, 0.10237248485908156, 0.1662049951510869, 0.21874757656288066, 0.2116451130427222, 0.22340375453137543, 0.21236469092122048, 0.2022871092223191, 0.2178614650539724, 0.22803066926623067, 0.18242396537917685, 0.22177399911546444, 0.25743135361927033, 0.1951921057896056, 0.11434006716988758, 0.22561563782153998, 0.3971070251408431, 0.21642632005933626, 0.15958665734021843, 0.1959413833036804, 0.2826167204679721, 0.1626673219785204, 0.22565734572332535, 0.19688396705879918, 0.31318526034037675, 0.14090052773766237, 0.2850360655446031, 0.19324860525334467, 0.19688741802160162, 0.18013107431547074, 0.30625368357767263, 0.27661729494767645, 0.14918818884994145, 0.17006452322442528, 0.19064942068370772, 0.1653773327797656, 0.11865670336235985, 0.13402922538126838, 0.20740171322859294, 0.10388481045191286, 0.14152775249619257, 0.1633199913522434, 0.25291131216174795, 0.20022551073411443, 0.2270197599960032, 0.14813057928465903, 0.3990052248334526, 0.17789782507651009, 0.22003758229710652, 0.27444351334087486, 0.23957100651218374, 0.22923503889839003, 0.19593426551405568, 0.17910471825758942, 0.25647820206832334, 0.2548631674420325, 0.2535760449521378, 0.13707829020589737, 0.2801795096338303, 0.12767421998739714, 0.1987931404907364, 0.24212079559164143, 0.21625807708593414, 0.22775770756386318, 0.1859057075739149, 0.2527514921386706, 0.2083143104343451, 0.12399458762180623, 0.18091378964662191, 0.17970265278711703, 0.2733431390996333, 0.14810324000202213, 0.23761641845678075, 0.1317856697334285, 0.22215525604982564, 0.2685323132329482, 0.2702872943064903, 0.23142172876278566, 0.22423388446250783, 0.19730312343485357, 0.2698893504869606, 0.15137696405841553, 0.088076705239081, 0.15671967477413123, 0.24123718624761964, 0.23169007736812008, 0.20248883322417024, 0.25433922577006723, 0.21033934378105446, 0.24020835422640752, 0.14104032226991142, 0.19739252737306548, 0.11273544631816956, 0.18607517875004, 0.10812350931214329, 0.23924440212747608, 0.21307109624971537, 0.14304208399745166, 0.3536589474340252, 0.14425095974045093, 0.24784336185663539, 0.15785357562586852, 0.28965616566195385, 0.20947281523350203, 0.23964105906172908, 0.2175939655721209, 0.23350201118585281, 0.19342893330472943, 0.42117233129286846, 0.2563270945748527, 0.2408357196411329, 0.16666496572097855, 0.23371510208249016, 0.1647083262524006, 0.15207555142892065, 0.19219147119097496, 0.15151229796577462, 0.14282904831598556, 0.2562413985958191, 0.19179795634384078, 0.17078360576190987, 0.18179874229043239, 0.24040075474999678, 0.15801974913573114, 0.1364265077482202, 0.20106120130620783, 0.16478360935739153, 0.1661984415201699, 0.09148903451117918, 0.25472459548457893, 0.37982391031538976, 0.20028041632833937, 0.23680212099800765, 0.22956143846090338, 0.2001465293532583, 0.2562807202006985, 0.252481593803014, 0.20179579404846099, 0.26636714722649846, 0.14953649459318843, 0.2404910562577275, 0.12077304783579686, 0.15575621696820824, 0.21603177128372922, 0.35902810011313846, 0.16513302688387954, 0.18298530407526728, 0.16263664737301795, 0.13237200340996136, 0.14407134193293958, 0.2598125880881467, 0.30132262808815863, 0.24383084476664002, 0.29344893030319547, 0.32638826871277665, 0.3062435213294478, 0.33669563099456834, 0.17918116895785113, 0.2459726006328152, 0.1676282794086131, 0.20467341672928474, 0.20277777777165976, 0.26119913474015516, 0.15218015979591185, 0.18930234811598365, 0.3528080239102928, 0.29217232654455805, 0.16699864586248978, 0.31613649672619865, 0.2662165530978304, 0.2504065186387752, 0.16766079673815368, 0.15449290513841657, 0.23648768820100913, 0.2764972561046669, 0.17051980313611034, 0.14963620191066104, 0.2524356282475574, 0.1440162410076723, 0.0741255849267472, 0.20492189048599802, 0.13919789833488294, 0.35549910000636165, 0.1810552563087514, 0.07952982701812462, 0.04948757261120027, 0.2373874124521937, 0.09061592825547736, 0.09028525570497492, 0.14020090394033236, 0.06613384358788979, 0.251893878723565, 0.14461757550745144, 0.056086856883111845, 0.055408566402769845, 0.18245230309260718, 0.09941168816961314, 0.10708724153300252, 0.08180773725491819, 0.09272190477059371, 0.08381180434703256, 0.07969749046160048, 0.10996526320257793, 0.23616573998187668, 0.047181223962062944, 0.19805608079607215, 0.18932644836256002, 0.09674734476951681, 0.09472177535386428, 0.04300111754799084, 0.07045306695109273, 0.2530529980844065, 0.1292220366617982, 0.10195752313515985, 0.12289129374240979, 0.06260385858817397, 0.1800061593380905, 0.10967614415170594, 0.3397441127997298, 0.19897143308181686, 0.08384084051104508, 0.07998616694235516, 0.10395836253705262, 0.12884080738961973, 0.04424392814456558, 0.13646617606231656, 0.10102327428012009, 0.06586538678840038, 0.18343883308983305, 0.07444193330695996, 0.04634765911882928, 0.048555405293948625, 0.13482949697450555, 0.1722780330963839, 0.16030678023197084, 0.21162648824551858, 0.09604679406934995, 0.14100196102898685, 0.09954795596486783, 0.0727351681529209, 0.08350768900686374, 0.13840411566975944, 0.1540081108664618, 0.0630208624829261, 0.08159398009158354, 0.12677158081165674, 0.08177329057786516, 0.03953240485835786, 0.16598639002272805, 0.06218793188649423, 0.1820472734389364, 0.1071046186498278, 0.1280088015472043, 0.044303898248435385, 0.07683276094424213, 0.1830679109692672, 0.06297331113857955, 0.06054642348039866, 0.15174198767135416, 0.07851365929866032, 0.052508723702090206, 0.04279313277718611, 0.0990456240366135, 0.2522236735172564, 0.13880200455284158, 0.1838324495274925, 0.1055403498208503, 0.15249176111131277, 0.17786170335383883, 0.08498237877686989, 0.05801867490554893, 0.13069152936529815, 0.15622879271121767, 0.13937267116177657, 0.12008473709029806, 0.1377109449830529, 0.10266234930240688, 0.19401438572990443, 0.07468084770937891, 0.09647075864721509, 0.10734739320732821, 0.09540887627494653, 0.13118592239833715, 0.038467766322411265, 0.12837165346503066, 0.07658731431400619, 0.257732117496531, 0.15702827578150008, 0.17533460277309287, 0.13620696032639, 0.18483312769214644, 0.07089369922842753, 0.18547620763921313, 0.1544788331574767, 0.04527479065696028, 0.09207057113039077, 0.08764593464638237, 0.059943085922022875, 0.05206515462991167, 0.16610414413349045, 0.16422461577422504, 0.0970040760205449, 0.356899820720872, 0.17953590820851753, 0.12390721122338458, 0.24791866102574572, 0.16022644570441918, 0.22024192847488375, 0.20511098424283372, 0.045588857884693086, 0.1189288202772196, 0.07481285677782114, 0.14545904992507278, 0.13734859523111456, 0.14153707765214782, 0.03431572451886834, 0.03814240259229633, 0.11634674854057286, 0.0822680201753916, 0.21219481485559044, 0.0932667245692251, 0.08562279492489797, 0.09786170026227518, 0.1375168548389043, 0.07436561856960541, 0.04001756132254701, 0.11132340077951693, 0.19042425414708092, 0.10482641022391068, 0.09517404168845515, 0.21595476965539517, 0.06509282054040483, 0.11019651706928162, 0.31642077883694336, 0.07605278306124484, 0.0910748103980224, 0.2875219160914224, 0.05967992954054966, 0.0605793533385487, 0.05565457690418974, 0.0513720256833986, 0.060295645301879905, 0.12490083213570363, 0.09710244100331639, 0.07830242985320551, 0.05731329410066308, 0.1716599472628012, 0.06960935097536385, 0.0426985280862736, 0.06457294895979356, 0.09718393235453719, 0.08281231535673184, 0.15812471986324333, 0.2472112506028048, 0.15430673907219958, 0.06413131089683935, 0.07570582755184184, 0.040551551712475785, 0.05327957804648039, 0.07851427973401591, 0.12023618250356972, 0.06803335181997104, 0.23628530951573787, 0.1922473380350486, 0.05000576179546437, 0.17018951471251778, 0.1922964771446865, 0.12168355816282685, 0.060522079689551464, 0.11109589308059578, 0.06529855209630002, 0.13672538232291673, 0.1310316911859233, 0.18430149270488919, 0.16534535161983635, 0.08206706880752838, 0.09578235654225638, 0.14490580948596696, 0.06568991925468622, 0.23696059367433991, 0.08199238629938495, 0.15685852202539707, 0.07872415315450867, 0.16675150494188812, 0.07812839036784736, 0.14699118969200017, 0.0833340101427524, 0.07611189764689127, 0.05355728124198149, 0.1527229019112067, 0.15539825807629476, 0.058743296782622675, 0.15085688292503727, 0.03707545616909235, 0.2721292077268063, 0.07821799062905402, 0.05963002746328357, 0.20613415249913333, 0.07123855866333373, 0.09793534707933779, 0.038858301750750494, 0.1723503372926482, 0.10797249513691624, 0.15158041464959776, 0.1829089098739022, 0.11903180307604189, 0.10541765503756369, 0.1428312157047153, 0.09230109142271445, 0.11228314918915806, 0.21184198181671585, 0.14827327662906123, 0.045350726827176654, 0.08640157994386828, 0.05007648824871356, 0.09312731770109363, 0.1397310945900709, 0.15644210286899132, 0.17049226877786064, 0.04146429912071498, 0.12666007648707978, 0.10282366635907257, 0.08247623007572558, 0.08019729103730963, 0.20986824181921432, 0.18387211642071258, 0.05298598748585908, 0.12065091670820192, 0.09294432661043348, 0.09035544779268671, 0.06164465926534276, 0.055019584259894644, 0.054734862451129386, 0.06325713005252266, 0.1458621229121246, 0.08252928975304807, 0.1148777896089627, 0.15250465526897392, 0.12526894214765055, 0.17281723533966079, 0.11374565708441085, 0.1408130804544792, 0.3437779473658895, 0.11628325814561448, 0.15438388828971392, 0.07121913597279508, 0.146937503169448, 0.10510237479207553, 0.12078588799954341, 0.08236983235300571, 0.2140453565467476, 0.14382561823195744, 0.10918995430762882, 0.056832189760710104, 0.14954801284270908, 0.2812865466492709, 0.19382102618598235, 0.044624166168254764, 0.042969476121052445, 0.1297641357419286, 0.12688671327227236, 0.09508229317978995, 0.05934292060354453, 0.23116389773300666, 0.1467558819545423, 0.13019930254499598, 0.09345653672710609, 0.13953428619409183, 0.058124471765782185, 0.13722289967580284, 0.13778692492787498, 0.27601725950109524, 0.10300696213882787, 0.1253844052521015, 0.09232403419754046, 0.06884765455001633, 0.04155156367315114, 0.1446356717094673, 0.18628931430502937, 0.11845930627433805, 0.047983945589967374, 0.09741230160726352, 0.24228342560303362, 0.18343866150009916, 0.10666164676718899, 0.24537473479602057, 0.08255836769063966, 0.1948233673491438, 0.1122434647251633, 0.13095374148814892, 0.18338223738365006, 0.23806226710790201, 0.05812540834625819, 0.04532850395440446, 0.05496878054982755, 0.09158770770302828, 0.09951649338063626, 0.12449345958151735, 0.139034768964097, 0.06269255147827092, 0.08810191427556359, 0.07347103828374414, 0.12454134558542532, 0.037972147095486984, 0.11378537148325926, 0.08186218443816586, 0.052124408485299555, 0.08074512819169113, 0.08035384999972978, 0.06667095159287134, 0.08906431572354062, 0.11084654030781436, 0.1457849320886083, 0.0897438289736436, 0.14208318567055214, 0.06541130207271205, 0.030413897882191163, 0.06321647421771763, 0.058035757195063556, 0.10601223873299963, 0.1054033088437628, 0.09596781496101407, 0.10480464832970977, 0.09922542988626334, 0.04721271972919027, 0.12157234709557664, 0.05553608354995087, 0.05629262903310154, 0.10393093302360437, 0.20703248644918734, 0.08315606541636483, 0.06850684561627332, 0.18828841112027644, 0.03430307902198962, 0.09639054261915968, 0.08532973041888954, 0.15551310455705902, 0.19909719606166038, 0.20657774903307127, 0.23614488475681303, 0.1268658700913936, 0.11576766906846171, 0.02068397045311989, 0.14307755832401847, 0.10211680037987454, 0.1720655640128585, 0.13668792877040747, 0.13524349085108928, 0.050658906869783256, 0.12767580583033897, 0.030667372564659214, 0.16164515011157052, 0.05828953969540185, 0.08403552481541536, 0.05735323801576444, 0.04441847884242874, 0.123793667398205, 0.19960133689351764, 0.062157483397303044, 0.1185631428580842, 0.0896582619398427, 0.09200732543733468, 0.11665265646115539, 0.08148483113280747, 0.10653548554156389, 0.049116791685337474, 0.04299381679003757, 0.12480340918795474, 0.14744983010481288, 0.07011327585023196, 0.10399267587429947, 0.08209970657964359, 0.13623597202250262, 0.07799762984076505, 0.07999691858305358, 0.06239080613561896, 0.0843236601497003, 0.12969231142661886, 0.14632495361785394, 0.12002579489424571, 0.08836142310915547, 0.06512119659309663, 0.05361590445008121, 0.1238311466464657, 0.09983434205975214, 0.08387755531558767, 0.05166963628977735, 0.08597637600410488, 0.0704259519844878, 0.14788527076624294, 0.1007320624755341, 0.21619091948923497, 0.1420926352866288, 0.07374917997229083, 0.10030663886896297, 0.08328519492875318, 0.2222619085528058, 0.14422146557247367, 0.07378018146057183, 0.0987847796713694, 0.11197696767839917, 0.19950588290470161, 0.1664105504259652, 0.10593012798083831, 0.1030708210489674, 0.06956968399260464, 0.20417183235669903, 0.11657889334504815, 0.1820645601811496, 0.11685661390780153, 0.04672666882922915, 0.11218045008587002, 0.08956266612292987, 0.05978565995976177, 0.09610640839142313, 0.067101317809991, 0.0552185279933661, 0.18033269483422878, 0.09545887900908519, 0.11432467742373763, 0.06458545273455486, 0.05591068338068577, 0.10464645889650545, 0.07863895721622698, 0.19144535892728073, 0.14324120042075922, 0.13466941880063663, 0.12123952489175097, 0.050373834709197846, 0.03791150394809176, 0.1337174007967305, 0.10745435258655613, 0.12653020991239555, 0.0594301523577338, 0.13360856024324266, 0.1493462190229424, 0.0869124061738447, 0.062468879860684424, 0.18672005551257856, 0.0808116391944075, 0.21211954581343792]
0.09322138741145582
Making ranges
torch.Size([53596, 2])
We keep 9.02e+06/9.53e+08 =  0% of the original kernel matrix.

torch.Size([2016, 2])
We keep 3.13e+04/4.41e+05 =  7% of the original kernel matrix.

torch.Size([13052, 2])
We keep 6.46e+05/2.05e+07 =  3% of the original kernel matrix.

torch.Size([34286, 2])
We keep 6.78e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([43620, 2])
We keep 7.00e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([5110, 2])
We keep 1.78e+05/4.10e+06 =  4% of the original kernel matrix.

torch.Size([18426, 2])
We keep 1.34e+06/6.25e+07 =  2% of the original kernel matrix.

torch.Size([7120, 2])
We keep 2.59e+06/1.80e+07 = 14% of the original kernel matrix.

torch.Size([19946, 2])
We keep 2.19e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([62132, 2])
We keep 1.42e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([59040, 2])
We keep 1.14e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([21660, 2])
We keep 2.51e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([34159, 2])
We keep 4.34e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([84917, 2])
We keep 3.03e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([67038, 2])
We keep 1.50e+07/1.57e+09 =  0% of the original kernel matrix.

torch.Size([61303, 2])
We keep 1.44e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([58599, 2])
We keep 1.13e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([11031, 2])
We keep 9.64e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([24878, 2])
We keep 2.54e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([13269, 2])
We keep 6.60e+06/1.20e+08 =  5% of the original kernel matrix.

torch.Size([26212, 2])
We keep 4.41e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([13171, 2])
We keep 1.25e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([26730, 2])
We keep 3.04e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([154137, 2])
We keep 1.75e+08/1.23e+10 =  1% of the original kernel matrix.

torch.Size([87882, 2])
We keep 2.88e+07/3.42e+09 =  0% of the original kernel matrix.

torch.Size([19408, 2])
We keep 1.79e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([32183, 2])
We keep 3.89e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([320684, 2])
We keep 3.30e+08/3.83e+10 =  0% of the original kernel matrix.

torch.Size([134818, 2])
We keep 4.75e+07/6.04e+09 =  0% of the original kernel matrix.

torch.Size([32205, 2])
We keep 5.45e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([41794, 2])
We keep 6.65e+06/5.70e+08 =  1% of the original kernel matrix.

torch.Size([47407, 2])
We keep 1.36e+07/8.24e+08 =  1% of the original kernel matrix.

torch.Size([52617, 2])
We keep 9.71e+06/8.86e+08 =  1% of the original kernel matrix.

torch.Size([86452, 2])
We keep 4.20e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([68133, 2])
We keep 1.52e+07/1.58e+09 =  0% of the original kernel matrix.

torch.Size([54489, 2])
We keep 1.80e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([53908, 2])
We keep 1.09e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([303648, 2])
We keep 5.31e+08/3.88e+10 =  1% of the original kernel matrix.

torch.Size([130744, 2])
We keep 4.75e+07/6.08e+09 =  0% of the original kernel matrix.

torch.Size([173709, 2])
We keep 1.26e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([96832, 2])
We keep 2.86e+07/3.35e+09 =  0% of the original kernel matrix.

torch.Size([10451, 2])
We keep 8.77e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([24120, 2])
We keep 2.47e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([396883, 2])
We keep 4.58e+08/6.18e+10 =  0% of the original kernel matrix.

torch.Size([151750, 2])
We keep 5.79e+07/7.68e+09 =  0% of the original kernel matrix.

torch.Size([22811, 2])
We keep 3.01e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([35011, 2])
We keep 4.65e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([43171, 2])
We keep 2.50e+07/7.37e+08 =  3% of the original kernel matrix.

torch.Size([48933, 2])
We keep 9.32e+06/8.38e+08 =  1% of the original kernel matrix.

torch.Size([10741, 2])
We keep 2.69e+06/4.42e+07 =  6% of the original kernel matrix.

torch.Size([24044, 2])
We keep 2.99e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([126536, 2])
We keep 5.04e+07/5.58e+09 =  0% of the original kernel matrix.

torch.Size([81904, 2])
We keep 2.07e+07/2.31e+09 =  0% of the original kernel matrix.

torch.Size([85989, 2])
We keep 5.90e+07/3.70e+09 =  1% of the original kernel matrix.

torch.Size([66811, 2])
We keep 1.78e+07/1.88e+09 =  0% of the original kernel matrix.

torch.Size([20084, 2])
We keep 2.17e+06/9.28e+07 =  2% of the original kernel matrix.

torch.Size([32906, 2])
We keep 3.95e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([66164, 2])
We keep 6.43e+07/2.40e+09 =  2% of the original kernel matrix.

torch.Size([59004, 2])
We keep 1.44e+07/1.51e+09 =  0% of the original kernel matrix.

torch.Size([2194036, 2])
We keep 7.24e+09/1.51e+12 =  0% of the original kernel matrix.

torch.Size([366159, 2])
We keep 2.49e+08/3.79e+10 =  0% of the original kernel matrix.

torch.Size([12825, 2])
We keep 1.12e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([26603, 2])
We keep 2.79e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([685109, 2])
We keep 2.19e+09/2.30e+11 =  0% of the original kernel matrix.

torch.Size([199181, 2])
We keep 1.06e+08/1.48e+10 =  0% of the original kernel matrix.

torch.Size([30630, 2])
We keep 4.48e+07/4.91e+08 =  9% of the original kernel matrix.

torch.Size([40201, 2])
We keep 7.60e+06/6.84e+08 =  1% of the original kernel matrix.

torch.Size([9451, 2])
We keep 4.73e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([23411, 2])
We keep 2.07e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([7215, 2])
We keep 4.20e+05/1.06e+07 =  3% of the original kernel matrix.

torch.Size([20633, 2])
We keep 1.84e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([150097, 2])
We keep 1.96e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([89327, 2])
We keep 2.89e+07/3.38e+09 =  0% of the original kernel matrix.

torch.Size([121258, 2])
We keep 7.76e+07/5.35e+09 =  1% of the original kernel matrix.

torch.Size([80419, 2])
We keep 2.05e+07/2.26e+09 =  0% of the original kernel matrix.

torch.Size([3674, 2])
We keep 8.35e+04/1.72e+06 =  4% of the original kernel matrix.

torch.Size([16246, 2])
We keep 9.96e+05/4.05e+07 =  2% of the original kernel matrix.

torch.Size([13074, 2])
We keep 9.32e+05/3.22e+07 =  2% of the original kernel matrix.

torch.Size([26745, 2])
We keep 2.72e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([4296, 2])
We keep 2.10e+05/3.72e+06 =  5% of the original kernel matrix.

torch.Size([16542, 2])
We keep 1.30e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([8126, 2])
We keep 3.64e+05/1.08e+07 =  3% of the original kernel matrix.

torch.Size([21905, 2])
We keep 1.85e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([1917, 2])
We keep 7.06e+04/8.78e+05 =  8% of the original kernel matrix.

torch.Size([11515, 2])
We keep 8.14e+05/2.89e+07 =  2% of the original kernel matrix.

torch.Size([1477, 2])
We keep 6.20e+04/3.65e+05 = 16% of the original kernel matrix.

torch.Size([10954, 2])
We keep 6.28e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([5240, 2])
We keep 2.13e+05/4.41e+06 =  4% of the original kernel matrix.

torch.Size([18029, 2])
We keep 1.38e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([2585, 2])
We keep 5.28e+04/9.16e+05 =  5% of the original kernel matrix.

torch.Size([14153, 2])
We keep 8.26e+05/2.95e+07 =  2% of the original kernel matrix.

torch.Size([4770, 2])
We keep 1.80e+05/3.98e+06 =  4% of the original kernel matrix.

torch.Size([17704, 2])
We keep 1.31e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([14158, 2])
We keep 1.84e+06/4.73e+07 =  3% of the original kernel matrix.

torch.Size([27676, 2])
We keep 3.18e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([7043, 2])
We keep 3.12e+05/8.41e+06 =  3% of the original kernel matrix.

torch.Size([20507, 2])
We keep 1.71e+06/8.95e+07 =  1% of the original kernel matrix.

torch.Size([5620, 2])
We keep 2.56e+05/5.59e+06 =  4% of the original kernel matrix.

torch.Size([18633, 2])
We keep 1.49e+06/7.30e+07 =  2% of the original kernel matrix.

torch.Size([19730, 2])
We keep 3.67e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([32331, 2])
We keep 4.49e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([23860, 2])
We keep 3.57e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([35941, 2])
We keep 5.12e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([7871, 2])
We keep 4.05e+05/1.13e+07 =  3% of the original kernel matrix.

torch.Size([21624, 2])
We keep 1.89e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([10971, 2])
We keep 8.98e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([24526, 2])
We keep 2.61e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([3281, 2])
We keep 1.01e+05/1.77e+06 =  5% of the original kernel matrix.

torch.Size([15156, 2])
We keep 1.03e+06/4.11e+07 =  2% of the original kernel matrix.

torch.Size([17997, 2])
We keep 1.49e+06/6.70e+07 =  2% of the original kernel matrix.

torch.Size([31041, 2])
We keep 3.60e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([10515, 2])
We keep 1.13e+06/3.00e+07 =  3% of the original kernel matrix.

torch.Size([23975, 2])
We keep 2.69e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([2867, 2])
We keep 5.35e+04/9.74e+05 =  5% of the original kernel matrix.

torch.Size([14839, 2])
We keep 8.29e+05/3.05e+07 =  2% of the original kernel matrix.

torch.Size([5688, 2])
We keep 2.25e+05/5.45e+06 =  4% of the original kernel matrix.

torch.Size([18877, 2])
We keep 1.48e+06/7.21e+07 =  2% of the original kernel matrix.

torch.Size([6023, 2])
We keep 2.44e+05/5.80e+06 =  4% of the original kernel matrix.

torch.Size([19482, 2])
We keep 1.53e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([7771, 2])
We keep 5.24e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([21217, 2])
We keep 1.99e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([24974, 2])
We keep 3.09e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([36891, 2])
We keep 5.05e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([3395, 2])
We keep 9.89e+04/1.80e+06 =  5% of the original kernel matrix.

torch.Size([15734, 2])
We keep 1.01e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([36480, 2])
We keep 7.02e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([45295, 2])
We keep 7.33e+06/6.42e+08 =  1% of the original kernel matrix.

torch.Size([11374, 2])
We keep 7.52e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([25158, 2])
We keep 2.46e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([5219, 2])
We keep 1.76e+05/4.38e+06 =  4% of the original kernel matrix.

torch.Size([18335, 2])
We keep 1.36e+06/6.46e+07 =  2% of the original kernel matrix.

torch.Size([5492, 2])
We keep 2.46e+05/5.32e+06 =  4% of the original kernel matrix.

torch.Size([18541, 2])
We keep 1.47e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([4803, 2])
We keep 2.03e+05/3.92e+06 =  5% of the original kernel matrix.

torch.Size([17713, 2])
We keep 1.32e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([5511, 2])
We keep 2.29e+05/5.17e+06 =  4% of the original kernel matrix.

torch.Size([18411, 2])
We keep 1.46e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([6661, 2])
We keep 2.87e+05/7.03e+06 =  4% of the original kernel matrix.

torch.Size([20356, 2])
We keep 1.60e+06/8.18e+07 =  1% of the original kernel matrix.

torch.Size([5600, 2])
We keep 1.94e+05/4.36e+06 =  4% of the original kernel matrix.

torch.Size([18677, 2])
We keep 1.37e+06/6.45e+07 =  2% of the original kernel matrix.

torch.Size([4081, 2])
We keep 1.85e+05/3.20e+06 =  5% of the original kernel matrix.

torch.Size([15817, 2])
We keep 1.21e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([8011, 2])
We keep 5.64e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([21429, 2])
We keep 1.99e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([5155, 2])
We keep 1.85e+05/3.89e+06 =  4% of the original kernel matrix.

torch.Size([18125, 2])
We keep 1.32e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([3038, 2])
We keep 9.47e+04/1.61e+06 =  5% of the original kernel matrix.

torch.Size([14772, 2])
We keep 9.87e+05/3.91e+07 =  2% of the original kernel matrix.

torch.Size([6824, 2])
We keep 3.34e+05/8.68e+06 =  3% of the original kernel matrix.

torch.Size([20321, 2])
We keep 1.73e+06/9.09e+07 =  1% of the original kernel matrix.

torch.Size([26755, 2])
We keep 4.95e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([38295, 2])
We keep 5.56e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([4964, 2])
We keep 1.79e+05/3.66e+06 =  4% of the original kernel matrix.

torch.Size([17978, 2])
We keep 1.30e+06/5.91e+07 =  2% of the original kernel matrix.

torch.Size([1022, 2])
We keep 1.25e+04/1.32e+05 =  9% of the original kernel matrix.

torch.Size([10270, 2])
We keep 4.67e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([5601, 2])
We keep 2.01e+05/4.63e+06 =  4% of the original kernel matrix.

torch.Size([18865, 2])
We keep 1.39e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([12059, 2])
We keep 8.68e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([25432, 2])
We keep 2.65e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([7010, 2])
We keep 3.24e+05/8.47e+06 =  3% of the original kernel matrix.

torch.Size([20508, 2])
We keep 1.71e+06/8.98e+07 =  1% of the original kernel matrix.

torch.Size([2357, 2])
We keep 6.16e+04/9.02e+05 =  6% of the original kernel matrix.

torch.Size([13175, 2])
We keep 8.16e+05/2.93e+07 =  2% of the original kernel matrix.

torch.Size([11051, 2])
We keep 8.29e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([24700, 2])
We keep 2.56e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([4982, 2])
We keep 1.63e+05/3.54e+06 =  4% of the original kernel matrix.

torch.Size([17948, 2])
We keep 1.28e+06/5.81e+07 =  2% of the original kernel matrix.

torch.Size([6726, 2])
We keep 3.20e+05/8.24e+06 =  3% of the original kernel matrix.

torch.Size([20206, 2])
We keep 1.69e+06/8.86e+07 =  1% of the original kernel matrix.

torch.Size([2209, 2])
We keep 3.13e+04/4.90e+05 =  6% of the original kernel matrix.

torch.Size([13594, 2])
We keep 6.64e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([15115, 2])
We keep 1.98e+06/6.22e+07 =  3% of the original kernel matrix.

torch.Size([28529, 2])
We keep 3.49e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([2635, 2])
We keep 5.29e+04/8.70e+05 =  6% of the original kernel matrix.

torch.Size([14374, 2])
We keep 8.13e+05/2.88e+07 =  2% of the original kernel matrix.

torch.Size([7801, 2])
We keep 3.36e+05/9.43e+06 =  3% of the original kernel matrix.

torch.Size([21720, 2])
We keep 1.78e+06/9.48e+07 =  1% of the original kernel matrix.

torch.Size([6979, 2])
We keep 2.94e+05/8.25e+06 =  3% of the original kernel matrix.

torch.Size([20565, 2])
We keep 1.68e+06/8.87e+07 =  1% of the original kernel matrix.

torch.Size([8799, 2])
We keep 4.78e+05/1.41e+07 =  3% of the original kernel matrix.

torch.Size([22481, 2])
We keep 2.04e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([2226, 2])
We keep 3.72e+04/5.75e+05 =  6% of the original kernel matrix.

torch.Size([13557, 2])
We keep 7.08e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([2656, 2])
We keep 6.04e+04/1.04e+06 =  5% of the original kernel matrix.

torch.Size([14129, 2])
We keep 8.50e+05/3.15e+07 =  2% of the original kernel matrix.

torch.Size([14630, 2])
We keep 1.19e+06/4.39e+07 =  2% of the original kernel matrix.

torch.Size([28142, 2])
We keep 3.06e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([10534, 2])
We keep 6.97e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([24328, 2])
We keep 2.37e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([7915, 2])
We keep 3.57e+05/1.00e+07 =  3% of the original kernel matrix.

torch.Size([21567, 2])
We keep 1.82e+06/9.76e+07 =  1% of the original kernel matrix.

torch.Size([11355, 2])
We keep 7.04e+05/2.39e+07 =  2% of the original kernel matrix.

torch.Size([25245, 2])
We keep 2.44e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([20700, 2])
We keep 4.80e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([33028, 2])
We keep 5.33e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([18270, 2])
We keep 2.28e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([31200, 2])
We keep 3.91e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([6374, 2])
We keep 2.27e+05/6.09e+06 =  3% of the original kernel matrix.

torch.Size([19895, 2])
We keep 1.52e+06/7.62e+07 =  1% of the original kernel matrix.

torch.Size([35260, 2])
We keep 6.25e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([44292, 2])
We keep 7.14e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([16875, 2])
We keep 1.73e+06/6.05e+07 =  2% of the original kernel matrix.

torch.Size([30206, 2])
We keep 3.42e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([11698, 2])
We keep 8.27e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([25627, 2])
We keep 2.52e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([3645, 2])
We keep 9.22e+04/1.80e+06 =  5% of the original kernel matrix.

torch.Size([16235, 2])
We keep 1.02e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([6431, 2])
We keep 3.51e+05/7.47e+06 =  4% of the original kernel matrix.

torch.Size([19757, 2])
We keep 1.66e+06/8.44e+07 =  1% of the original kernel matrix.

torch.Size([4597, 2])
We keep 1.66e+05/3.45e+06 =  4% of the original kernel matrix.

torch.Size([17452, 2])
We keep 1.26e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([15558, 2])
We keep 1.15e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([28971, 2])
We keep 3.14e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([1083, 2])
We keep 1.07e+04/1.13e+05 =  9% of the original kernel matrix.

torch.Size([10741, 2])
We keep 4.35e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([8911, 2])
We keep 5.59e+05/1.53e+07 =  3% of the original kernel matrix.

torch.Size([22814, 2])
We keep 2.10e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([5289, 2])
We keep 1.86e+05/4.18e+06 =  4% of the original kernel matrix.

torch.Size([18487, 2])
We keep 1.35e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([2693, 2])
We keep 6.50e+04/1.10e+06 =  5% of the original kernel matrix.

torch.Size([14261, 2])
We keep 8.76e+05/3.23e+07 =  2% of the original kernel matrix.

torch.Size([3879, 2])
We keep 1.56e+05/2.51e+06 =  6% of the original kernel matrix.

torch.Size([16185, 2])
We keep 1.15e+06/4.89e+07 =  2% of the original kernel matrix.

torch.Size([4048, 2])
We keep 1.72e+05/3.21e+06 =  5% of the original kernel matrix.

torch.Size([16302, 2])
We keep 1.25e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([7047, 2])
We keep 3.29e+05/8.56e+06 =  3% of the original kernel matrix.

torch.Size([20611, 2])
We keep 1.73e+06/9.03e+07 =  1% of the original kernel matrix.

torch.Size([8882, 2])
We keep 5.13e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([22633, 2])
We keep 2.09e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([3013, 2])
We keep 1.03e+05/1.72e+06 =  5% of the original kernel matrix.

torch.Size([14602, 2])
We keep 1.02e+06/4.05e+07 =  2% of the original kernel matrix.

torch.Size([3547, 2])
We keep 9.14e+04/1.75e+06 =  5% of the original kernel matrix.

torch.Size([15964, 2])
We keep 1.01e+06/4.09e+07 =  2% of the original kernel matrix.

torch.Size([3551, 2])
We keep 8.48e+04/1.78e+06 =  4% of the original kernel matrix.

torch.Size([15970, 2])
We keep 1.02e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([17069, 2])
We keep 1.94e+06/7.46e+07 =  2% of the original kernel matrix.

torch.Size([30160, 2])
We keep 3.76e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([2731, 2])
We keep 5.88e+04/9.92e+05 =  5% of the original kernel matrix.

torch.Size([14427, 2])
We keep 8.58e+05/3.07e+07 =  2% of the original kernel matrix.

torch.Size([22401, 2])
We keep 2.18e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([34724, 2])
We keep 4.36e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([6889, 2])
We keep 3.17e+05/7.70e+06 =  4% of the original kernel matrix.

torch.Size([20318, 2])
We keep 1.65e+06/8.56e+07 =  1% of the original kernel matrix.

torch.Size([4031, 2])
We keep 1.13e+05/2.30e+06 =  4% of the original kernel matrix.

torch.Size([16466, 2])
We keep 1.11e+06/4.68e+07 =  2% of the original kernel matrix.

torch.Size([5522, 2])
We keep 2.01e+05/4.63e+06 =  4% of the original kernel matrix.

torch.Size([18712, 2])
We keep 1.37e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([5000, 2])
We keep 1.52e+05/3.36e+06 =  4% of the original kernel matrix.

torch.Size([18198, 2])
We keep 1.25e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([8626, 2])
We keep 3.76e+05/1.16e+07 =  3% of the original kernel matrix.

torch.Size([22521, 2])
We keep 1.90e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([3639, 2])
We keep 8.89e+04/1.83e+06 =  4% of the original kernel matrix.

torch.Size([16075, 2])
We keep 1.02e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([5703, 2])
We keep 2.90e+05/5.88e+06 =  4% of the original kernel matrix.

torch.Size([18870, 2])
We keep 1.51e+06/7.48e+07 =  2% of the original kernel matrix.

torch.Size([23339, 2])
We keep 2.67e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([35516, 2])
We keep 4.71e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([8887, 2])
We keep 4.63e+05/1.46e+07 =  3% of the original kernel matrix.

torch.Size([22690, 2])
We keep 2.08e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([8201, 2])
We keep 7.73e+05/1.43e+07 =  5% of the original kernel matrix.

torch.Size([21617, 2])
We keep 2.08e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([2673, 2])
We keep 7.08e+04/1.13e+06 =  6% of the original kernel matrix.

torch.Size([14140, 2])
We keep 8.78e+05/3.28e+07 =  2% of the original kernel matrix.

torch.Size([13748, 2])
We keep 1.41e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([27097, 2])
We keep 3.13e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([3505, 2])
We keep 1.66e+05/2.59e+06 =  6% of the original kernel matrix.

torch.Size([15108, 2])
We keep 1.16e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([16342, 2])
We keep 1.17e+07/8.83e+07 = 13% of the original kernel matrix.

torch.Size([29381, 2])
We keep 3.94e+06/2.90e+08 =  1% of the original kernel matrix.

torch.Size([4979, 2])
We keep 1.79e+05/4.04e+06 =  4% of the original kernel matrix.

torch.Size([18181, 2])
We keep 1.30e+06/6.21e+07 =  2% of the original kernel matrix.

torch.Size([2912, 2])
We keep 6.80e+04/1.19e+06 =  5% of the original kernel matrix.

torch.Size([14602, 2])
We keep 8.91e+05/3.37e+07 =  2% of the original kernel matrix.

torch.Size([3028, 2])
We keep 7.44e+04/1.27e+06 =  5% of the original kernel matrix.

torch.Size([15121, 2])
We keep 9.25e+05/3.48e+07 =  2% of the original kernel matrix.

torch.Size([4437, 2])
We keep 1.40e+05/2.99e+06 =  4% of the original kernel matrix.

torch.Size([17154, 2])
We keep 1.21e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([4927, 2])
We keep 1.73e+05/3.89e+06 =  4% of the original kernel matrix.

torch.Size([17893, 2])
We keep 1.33e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([6752, 2])
We keep 3.08e+05/8.15e+06 =  3% of the original kernel matrix.

torch.Size([20091, 2])
We keep 1.69e+06/8.81e+07 =  1% of the original kernel matrix.

torch.Size([2919, 2])
We keep 1.07e+05/1.22e+06 =  8% of the original kernel matrix.

torch.Size([14583, 2])
We keep 9.05e+05/3.41e+07 =  2% of the original kernel matrix.

torch.Size([13824, 2])
We keep 1.47e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([27463, 2])
We keep 3.32e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([54598, 2])
We keep 1.72e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([55211, 2])
We keep 1.11e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([12631, 2])
We keep 1.01e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([26395, 2])
We keep 2.79e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([4174, 2])
We keep 1.18e+05/2.38e+06 =  4% of the original kernel matrix.

torch.Size([16996, 2])
We keep 1.12e+06/4.76e+07 =  2% of the original kernel matrix.

torch.Size([4744, 2])
We keep 1.39e+05/3.09e+06 =  4% of the original kernel matrix.

torch.Size([17972, 2])
We keep 1.21e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([6176, 2])
We keep 4.71e+05/7.63e+06 =  6% of the original kernel matrix.

torch.Size([19449, 2])
We keep 1.70e+06/8.53e+07 =  1% of the original kernel matrix.

torch.Size([3457, 2])
We keep 9.65e+04/1.82e+06 =  5% of the original kernel matrix.

torch.Size([15795, 2])
We keep 1.03e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([5620, 2])
We keep 2.53e+05/5.55e+06 =  4% of the original kernel matrix.

torch.Size([18611, 2])
We keep 1.48e+06/7.27e+07 =  2% of the original kernel matrix.

torch.Size([4038, 2])
We keep 1.23e+05/2.53e+06 =  4% of the original kernel matrix.

torch.Size([16265, 2])
We keep 1.14e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([16501, 2])
We keep 1.62e+06/6.27e+07 =  2% of the original kernel matrix.

torch.Size([29815, 2])
We keep 3.50e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([7052, 2])
We keep 3.14e+05/8.07e+06 =  3% of the original kernel matrix.

torch.Size([20508, 2])
We keep 1.68e+06/8.77e+07 =  1% of the original kernel matrix.

torch.Size([22488, 2])
We keep 6.13e+06/2.35e+08 =  2% of the original kernel matrix.

torch.Size([34103, 2])
We keep 5.84e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([8236, 2])
We keep 4.24e+05/1.18e+07 =  3% of the original kernel matrix.

torch.Size([22110, 2])
We keep 1.94e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([31556, 2])
We keep 6.04e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([41615, 2])
We keep 6.63e+06/5.58e+08 =  1% of the original kernel matrix.

torch.Size([4356, 2])
We keep 1.14e+05/2.53e+06 =  4% of the original kernel matrix.

torch.Size([17198, 2])
We keep 1.13e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([5697, 2])
We keep 2.39e+05/5.10e+06 =  4% of the original kernel matrix.

torch.Size([18931, 2])
We keep 1.44e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([16244, 2])
We keep 1.50e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([29666, 2])
We keep 3.36e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([1326, 2])
We keep 2.20e+04/2.42e+05 =  9% of the original kernel matrix.

torch.Size([11022, 2])
We keep 5.45e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([16358, 2])
We keep 1.77e+06/6.48e+07 =  2% of the original kernel matrix.

torch.Size([30020, 2])
We keep 3.64e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([3607, 2])
We keep 1.16e+05/2.07e+06 =  5% of the original kernel matrix.

torch.Size([15727, 2])
We keep 1.09e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([12244, 2])
We keep 9.40e+05/3.17e+07 =  2% of the original kernel matrix.

torch.Size([26050, 2])
We keep 2.74e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([2273, 2])
We keep 5.43e+04/7.71e+05 =  7% of the original kernel matrix.

torch.Size([13283, 2])
We keep 7.81e+05/2.71e+07 =  2% of the original kernel matrix.

torch.Size([5586, 2])
We keep 2.75e+05/5.69e+06 =  4% of the original kernel matrix.

torch.Size([18544, 2])
We keep 1.52e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([4424, 2])
We keep 1.16e+05/2.54e+06 =  4% of the original kernel matrix.

torch.Size([17481, 2])
We keep 1.14e+06/4.92e+07 =  2% of the original kernel matrix.

torch.Size([5579, 2])
We keep 1.86e+05/4.52e+06 =  4% of the original kernel matrix.

torch.Size([18984, 2])
We keep 1.38e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([4300, 2])
We keep 1.31e+05/2.86e+06 =  4% of the original kernel matrix.

torch.Size([16870, 2])
We keep 1.19e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([7412, 2])
We keep 3.41e+05/9.08e+06 =  3% of the original kernel matrix.

torch.Size([20918, 2])
We keep 1.75e+06/9.30e+07 =  1% of the original kernel matrix.

torch.Size([933, 2])
We keep 9.26e+03/7.73e+04 = 11% of the original kernel matrix.

torch.Size([9910, 2])
We keep 3.84e+05/8.58e+06 =  4% of the original kernel matrix.

torch.Size([2988, 2])
We keep 1.00e+05/1.68e+06 =  5% of the original kernel matrix.

torch.Size([14494, 2])
We keep 1.00e+06/4.00e+07 =  2% of the original kernel matrix.

torch.Size([3955, 2])
We keep 1.12e+05/2.43e+06 =  4% of the original kernel matrix.

torch.Size([16470, 2])
We keep 1.13e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([11491, 2])
We keep 6.79e+05/2.27e+07 =  2% of the original kernel matrix.

torch.Size([25423, 2])
We keep 2.41e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([4374, 2])
We keep 1.64e+05/2.89e+06 =  5% of the original kernel matrix.

torch.Size([17200, 2])
We keep 1.20e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([11065, 2])
We keep 9.94e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([24845, 2])
We keep 2.68e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([10770, 2])
We keep 5.94e+06/3.89e+07 = 15% of the original kernel matrix.

torch.Size([24268, 2])
We keep 2.94e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([7422, 2])
We keep 4.10e+05/9.20e+06 =  4% of the original kernel matrix.

torch.Size([20915, 2])
We keep 1.77e+06/9.36e+07 =  1% of the original kernel matrix.

torch.Size([14286, 2])
We keep 1.04e+06/4.06e+07 =  2% of the original kernel matrix.

torch.Size([27952, 2])
We keep 2.97e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([13978, 2])
We keep 2.03e+06/5.68e+07 =  3% of the original kernel matrix.

torch.Size([27231, 2])
We keep 3.43e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([3094, 2])
We keep 9.02e+04/1.60e+06 =  5% of the original kernel matrix.

torch.Size([14616, 2])
We keep 9.88e+05/3.90e+07 =  2% of the original kernel matrix.

torch.Size([7450, 2])
We keep 3.67e+05/1.03e+07 =  3% of the original kernel matrix.

torch.Size([21145, 2])
We keep 1.84e+06/9.88e+07 =  1% of the original kernel matrix.

torch.Size([9984, 2])
We keep 6.69e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([23798, 2])
We keep 2.32e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([7350, 2])
We keep 6.23e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([20544, 2])
We keep 2.02e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([4082, 2])
We keep 1.17e+05/2.39e+06 =  4% of the original kernel matrix.

torch.Size([16702, 2])
We keep 1.13e+06/4.77e+07 =  2% of the original kernel matrix.

torch.Size([12500, 2])
We keep 8.68e+05/3.08e+07 =  2% of the original kernel matrix.

torch.Size([26183, 2])
We keep 2.71e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([18214, 2])
We keep 1.97e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([31287, 2])
We keep 3.71e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([6630, 2])
We keep 2.87e+05/7.25e+06 =  3% of the original kernel matrix.

torch.Size([20075, 2])
We keep 1.62e+06/8.31e+07 =  1% of the original kernel matrix.

torch.Size([9186, 2])
We keep 9.68e+05/2.33e+07 =  4% of the original kernel matrix.

torch.Size([22311, 2])
We keep 2.46e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([11295, 2])
We keep 7.09e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([25188, 2])
We keep 2.46e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([45997, 2])
We keep 1.76e+07/8.47e+08 =  2% of the original kernel matrix.

torch.Size([50448, 2])
We keep 9.75e+06/8.99e+08 =  1% of the original kernel matrix.

torch.Size([3454, 2])
We keep 9.86e+04/1.79e+06 =  5% of the original kernel matrix.

torch.Size([15625, 2])
We keep 1.02e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([1165, 2])
We keep 1.28e+04/1.41e+05 =  9% of the original kernel matrix.

torch.Size([10700, 2])
We keep 4.58e+05/1.16e+07 =  3% of the original kernel matrix.

torch.Size([6913, 2])
We keep 3.00e+05/7.50e+06 =  3% of the original kernel matrix.

torch.Size([20506, 2])
We keep 1.65e+06/8.46e+07 =  1% of the original kernel matrix.

torch.Size([3617, 2])
We keep 3.17e+05/2.69e+06 = 11% of the original kernel matrix.

torch.Size([15657, 2])
We keep 1.16e+06/5.06e+07 =  2% of the original kernel matrix.

torch.Size([4127, 2])
We keep 1.64e+05/3.24e+06 =  5% of the original kernel matrix.

torch.Size([16384, 2])
We keep 1.25e+06/5.56e+07 =  2% of the original kernel matrix.

torch.Size([6558, 2])
We keep 3.10e+05/7.47e+06 =  4% of the original kernel matrix.

torch.Size([20126, 2])
We keep 1.64e+06/8.44e+07 =  1% of the original kernel matrix.

torch.Size([3331, 2])
We keep 8.69e+04/1.63e+06 =  5% of the original kernel matrix.

torch.Size([15466, 2])
We keep 9.85e+05/3.95e+07 =  2% of the original kernel matrix.

torch.Size([3077, 2])
We keep 1.16e+05/1.83e+06 =  6% of the original kernel matrix.

torch.Size([14537, 2])
We keep 1.04e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([6671, 2])
We keep 2.76e+05/7.13e+06 =  3% of the original kernel matrix.

torch.Size([20335, 2])
We keep 1.62e+06/8.24e+07 =  1% of the original kernel matrix.

torch.Size([3247, 2])
We keep 7.49e+04/1.37e+06 =  5% of the original kernel matrix.

torch.Size([15589, 2])
We keep 9.42e+05/3.61e+07 =  2% of the original kernel matrix.

torch.Size([14699, 2])
We keep 1.63e+06/4.33e+07 =  3% of the original kernel matrix.

torch.Size([28271, 2])
We keep 2.99e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([4040, 2])
We keep 1.12e+05/2.29e+06 =  4% of the original kernel matrix.

torch.Size([16356, 2])
We keep 1.10e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([24931, 2])
We keep 2.97e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([36777, 2])
We keep 4.95e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([13279, 2])
We keep 9.50e+05/3.38e+07 =  2% of the original kernel matrix.

torch.Size([27001, 2])
We keep 2.79e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([5588, 2])
We keep 1.97e+05/4.73e+06 =  4% of the original kernel matrix.

torch.Size([18697, 2])
We keep 1.39e+06/6.71e+07 =  2% of the original kernel matrix.

torch.Size([1455, 2])
We keep 1.85e+04/2.21e+05 =  8% of the original kernel matrix.

torch.Size([11777, 2])
We keep 5.31e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([10775, 2])
We keep 9.15e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([24570, 2])
We keep 2.60e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([8400, 2])
We keep 4.63e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([22069, 2])
We keep 1.98e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([11343, 2])
We keep 9.16e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([25231, 2])
We keep 2.56e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([19934, 2])
We keep 2.33e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([32847, 2])
We keep 4.24e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([16150, 2])
We keep 1.31e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([29436, 2])
We keep 3.31e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([3137, 2])
We keep 8.88e+04/1.54e+06 =  5% of the original kernel matrix.

torch.Size([15048, 2])
We keep 9.78e+05/3.83e+07 =  2% of the original kernel matrix.

torch.Size([2330, 2])
We keep 4.04e+04/6.30e+05 =  6% of the original kernel matrix.

torch.Size([13736, 2])
We keep 7.32e+05/2.45e+07 =  2% of the original kernel matrix.

torch.Size([3780, 2])
We keep 1.51e+05/2.33e+06 =  6% of the original kernel matrix.

torch.Size([16093, 2])
We keep 1.13e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([2295, 2])
We keep 4.58e+04/7.07e+05 =  6% of the original kernel matrix.

torch.Size([13448, 2])
We keep 7.57e+05/2.60e+07 =  2% of the original kernel matrix.

torch.Size([1642, 2])
We keep 3.04e+04/3.67e+05 =  8% of the original kernel matrix.

torch.Size([11634, 2])
We keep 6.21e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([2164, 2])
We keep 3.88e+04/5.76e+05 =  6% of the original kernel matrix.

torch.Size([13290, 2])
We keep 7.08e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([1558, 2])
We keep 2.62e+04/3.19e+05 =  8% of the original kernel matrix.

torch.Size([11821, 2])
We keep 5.90e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([8257, 2])
We keep 6.33e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([21717, 2])
We keep 2.07e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([3648, 2])
We keep 1.15e+05/2.14e+06 =  5% of the original kernel matrix.

torch.Size([15940, 2])
We keep 1.09e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([7010, 2])
We keep 1.12e+06/2.06e+07 =  5% of the original kernel matrix.

torch.Size([18656, 2])
We keep 2.37e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([6157, 2])
We keep 2.58e+05/6.46e+06 =  3% of the original kernel matrix.

torch.Size([19520, 2])
We keep 1.54e+06/7.84e+07 =  1% of the original kernel matrix.

torch.Size([6110, 2])
We keep 2.85e+05/6.81e+06 =  4% of the original kernel matrix.

torch.Size([19183, 2])
We keep 1.60e+06/8.06e+07 =  1% of the original kernel matrix.

torch.Size([2926, 2])
We keep 9.10e+04/1.45e+06 =  6% of the original kernel matrix.

torch.Size([14290, 2])
We keep 9.52e+05/3.72e+07 =  2% of the original kernel matrix.

torch.Size([12928, 2])
We keep 1.31e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([26477, 2])
We keep 3.15e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([7661, 2])
We keep 4.10e+05/1.05e+07 =  3% of the original kernel matrix.

torch.Size([21221, 2])
We keep 1.85e+06/9.99e+07 =  1% of the original kernel matrix.

torch.Size([1516, 2])
We keep 1.81e+04/2.33e+05 =  7% of the original kernel matrix.

torch.Size([11852, 2])
We keep 5.39e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([2119, 2])
We keep 4.97e+04/6.91e+05 =  7% of the original kernel matrix.

torch.Size([12767, 2])
We keep 7.49e+05/2.57e+07 =  2% of the original kernel matrix.

torch.Size([10136, 2])
We keep 8.41e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([23737, 2])
We keep 2.41e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([1916, 2])
We keep 3.20e+04/4.32e+05 =  7% of the original kernel matrix.

torch.Size([12438, 2])
We keep 6.39e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([2771, 2])
We keep 9.39e+04/1.30e+06 =  7% of the original kernel matrix.

torch.Size([14258, 2])
We keep 9.19e+05/3.52e+07 =  2% of the original kernel matrix.

torch.Size([3365, 2])
We keep 1.07e+05/1.94e+06 =  5% of the original kernel matrix.

torch.Size([15288, 2])
We keep 1.05e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([10797, 2])
We keep 7.94e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([24642, 2])
We keep 2.47e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([13147, 2])
We keep 1.04e+06/3.73e+07 =  2% of the original kernel matrix.

torch.Size([26871, 2])
We keep 2.91e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([4308, 2])
We keep 1.26e+05/2.74e+06 =  4% of the original kernel matrix.

torch.Size([17093, 2])
We keep 1.17e+06/5.11e+07 =  2% of the original kernel matrix.

torch.Size([2632, 2])
We keep 7.05e+04/1.05e+06 =  6% of the original kernel matrix.

torch.Size([13920, 2])
We keep 8.71e+05/3.17e+07 =  2% of the original kernel matrix.

torch.Size([9811, 2])
We keep 2.27e+06/1.95e+07 = 11% of the original kernel matrix.

torch.Size([23625, 2])
We keep 2.23e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([14472, 2])
We keep 1.15e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([28085, 2])
We keep 3.05e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([2468, 2])
We keep 3.71e+05/1.74e+06 = 21% of the original kernel matrix.

torch.Size([12805, 2])
We keep 9.95e+05/4.07e+07 =  2% of the original kernel matrix.

torch.Size([15098, 2])
We keep 1.81e+06/5.47e+07 =  3% of the original kernel matrix.

torch.Size([28583, 2])
We keep 3.33e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([53029, 2])
We keep 7.53e+07/3.01e+09 =  2% of the original kernel matrix.

torch.Size([49579, 2])
We keep 1.63e+07/1.69e+09 =  0% of the original kernel matrix.

torch.Size([6147, 2])
We keep 4.10e+05/6.54e+06 =  6% of the original kernel matrix.

torch.Size([19662, 2])
We keep 1.50e+06/7.90e+07 =  1% of the original kernel matrix.

torch.Size([17809, 2])
We keep 1.60e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([31038, 2])
We keep 3.56e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([1390, 2])
We keep 1.95e+04/2.19e+05 =  8% of the original kernel matrix.

torch.Size([11133, 2])
We keep 5.27e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([8796, 2])
We keep 4.71e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([22689, 2])
We keep 2.07e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([57312, 2])
We keep 1.99e+08/1.92e+09 = 10% of the original kernel matrix.

torch.Size([54458, 2])
We keep 1.28e+07/1.35e+09 =  0% of the original kernel matrix.

torch.Size([188863, 2])
We keep 7.08e+08/3.30e+10 =  2% of the original kernel matrix.

torch.Size([95981, 2])
We keep 4.44e+07/5.61e+09 =  0% of the original kernel matrix.

torch.Size([4318, 2])
We keep 1.24e+05/2.71e+06 =  4% of the original kernel matrix.

torch.Size([17138, 2])
We keep 1.17e+06/5.08e+07 =  2% of the original kernel matrix.

torch.Size([43554, 2])
We keep 1.95e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([48656, 2])
We keep 1.06e+07/9.92e+08 =  1% of the original kernel matrix.

torch.Size([41285, 2])
We keep 2.98e+07/8.87e+08 =  3% of the original kernel matrix.

torch.Size([46717, 2])
We keep 1.00e+07/9.19e+08 =  1% of the original kernel matrix.

torch.Size([13724, 2])
We keep 2.17e+06/6.44e+07 =  3% of the original kernel matrix.

torch.Size([26843, 2])
We keep 3.54e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([99601, 2])
We keep 3.45e+08/6.92e+09 =  4% of the original kernel matrix.

torch.Size([72321, 2])
We keep 2.23e+07/2.57e+09 =  0% of the original kernel matrix.

torch.Size([3675, 2])
We keep 1.12e+05/1.85e+06 =  6% of the original kernel matrix.

torch.Size([16075, 2])
We keep 1.03e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([15138, 2])
We keep 1.98e+06/5.21e+07 =  3% of the original kernel matrix.

torch.Size([28479, 2])
We keep 3.24e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([188024, 2])
We keep 1.70e+08/1.57e+10 =  1% of the original kernel matrix.

torch.Size([99796, 2])
We keep 3.24e+07/3.87e+09 =  0% of the original kernel matrix.

torch.Size([224015, 2])
We keep 1.59e+08/1.75e+10 =  0% of the original kernel matrix.

torch.Size([110943, 2])
We keep 3.37e+07/4.09e+09 =  0% of the original kernel matrix.

torch.Size([8153, 2])
We keep 5.84e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([21777, 2])
We keep 2.00e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([39275, 2])
We keep 8.83e+06/5.31e+08 =  1% of the original kernel matrix.

torch.Size([46817, 2])
We keep 7.96e+06/7.12e+08 =  1% of the original kernel matrix.

torch.Size([25699, 2])
We keep 2.45e+07/3.53e+08 =  6% of the original kernel matrix.

torch.Size([36682, 2])
We keep 6.93e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([61553, 2])
We keep 1.08e+08/1.68e+09 =  6% of the original kernel matrix.

torch.Size([57181, 2])
We keep 1.26e+07/1.27e+09 =  0% of the original kernel matrix.

torch.Size([38349, 2])
We keep 2.48e+07/7.64e+08 =  3% of the original kernel matrix.

torch.Size([44202, 2])
We keep 8.83e+06/8.53e+08 =  1% of the original kernel matrix.

torch.Size([51027, 2])
We keep 5.71e+07/1.70e+09 =  3% of the original kernel matrix.

torch.Size([51315, 2])
We keep 1.30e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([68077, 2])
We keep 3.41e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([60638, 2])
We keep 1.39e+07/1.39e+09 =  0% of the original kernel matrix.

torch.Size([29176, 2])
We keep 8.52e+06/3.03e+08 =  2% of the original kernel matrix.

torch.Size([39849, 2])
We keep 6.46e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([4071, 2])
We keep 1.72e+05/2.74e+06 =  6% of the original kernel matrix.

torch.Size([16565, 2])
We keep 1.17e+06/5.11e+07 =  2% of the original kernel matrix.

torch.Size([212802, 2])
We keep 6.99e+08/4.61e+10 =  1% of the original kernel matrix.

torch.Size([102293, 2])
We keep 5.19e+07/6.63e+09 =  0% of the original kernel matrix.

torch.Size([6404, 2])
We keep 4.31e+05/7.97e+06 =  5% of the original kernel matrix.

torch.Size([19732, 2])
We keep 1.71e+06/8.71e+07 =  1% of the original kernel matrix.

torch.Size([7815, 2])
We keep 3.54e+05/1.01e+07 =  3% of the original kernel matrix.

torch.Size([21364, 2])
We keep 1.81e+06/9.82e+07 =  1% of the original kernel matrix.

torch.Size([39905, 2])
We keep 1.24e+07/5.89e+08 =  2% of the original kernel matrix.

torch.Size([47129, 2])
We keep 8.15e+06/7.49e+08 =  1% of the original kernel matrix.

torch.Size([34121, 2])
We keep 1.62e+07/6.73e+08 =  2% of the original kernel matrix.

torch.Size([42108, 2])
We keep 8.87e+06/8.01e+08 =  1% of the original kernel matrix.

torch.Size([458130, 2])
We keep 8.35e+08/9.48e+10 =  0% of the original kernel matrix.

torch.Size([162565, 2])
We keep 7.17e+07/9.50e+09 =  0% of the original kernel matrix.

torch.Size([110059, 2])
We keep 4.62e+07/4.04e+09 =  1% of the original kernel matrix.

torch.Size([76095, 2])
We keep 1.80e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([3508, 2])
We keep 9.62e+04/1.86e+06 =  5% of the original kernel matrix.

torch.Size([15871, 2])
We keep 1.03e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([19474, 2])
We keep 4.16e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([32314, 2])
We keep 4.28e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([32887, 2])
We keep 1.11e+07/4.43e+08 =  2% of the original kernel matrix.

torch.Size([42347, 2])
We keep 7.48e+06/6.50e+08 =  1% of the original kernel matrix.

torch.Size([19108, 2])
We keep 3.75e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([31844, 2])
We keep 4.70e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([138105, 2])
We keep 2.30e+08/8.83e+09 =  2% of the original kernel matrix.

torch.Size([85856, 2])
We keep 2.51e+07/2.90e+09 =  0% of the original kernel matrix.

torch.Size([9088, 2])
We keep 4.55e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([22888, 2])
We keep 2.04e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([30107, 2])
We keep 5.50e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([40533, 2])
We keep 6.17e+06/5.13e+08 =  1% of the original kernel matrix.

torch.Size([1578, 2])
We keep 2.63e+04/3.18e+05 =  8% of the original kernel matrix.

torch.Size([11764, 2])
We keep 6.03e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([6753, 2])
We keep 2.93e+05/7.83e+06 =  3% of the original kernel matrix.

torch.Size([20220, 2])
We keep 1.68e+06/8.64e+07 =  1% of the original kernel matrix.

torch.Size([44491, 2])
We keep 3.00e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([46396, 2])
We keep 1.20e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([57808, 2])
We keep 5.52e+07/1.89e+09 =  2% of the original kernel matrix.

torch.Size([54296, 2])
We keep 1.31e+07/1.34e+09 =  0% of the original kernel matrix.

torch.Size([28727, 2])
We keep 8.25e+06/3.81e+08 =  2% of the original kernel matrix.

torch.Size([38539, 2])
We keep 6.96e+06/6.03e+08 =  1% of the original kernel matrix.

torch.Size([17908, 2])
We keep 4.76e+06/1.07e+08 =  4% of the original kernel matrix.

torch.Size([30799, 2])
We keep 4.36e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([308430, 2])
We keep 4.28e+08/6.76e+10 =  0% of the original kernel matrix.

torch.Size([130157, 2])
We keep 6.06e+07/8.03e+09 =  0% of the original kernel matrix.

torch.Size([19075, 2])
We keep 1.60e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([31942, 2])
We keep 3.71e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([37417, 2])
We keep 6.36e+06/4.65e+08 =  1% of the original kernel matrix.

torch.Size([45348, 2])
We keep 7.47e+06/6.66e+08 =  1% of the original kernel matrix.

torch.Size([117812, 2])
We keep 9.02e+07/6.66e+09 =  1% of the original kernel matrix.

torch.Size([78161, 2])
We keep 2.26e+07/2.52e+09 =  0% of the original kernel matrix.

torch.Size([8390, 2])
We keep 4.70e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([22171, 2])
We keep 1.95e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([86867, 2])
We keep 6.59e+07/2.87e+09 =  2% of the original kernel matrix.

torch.Size([67535, 2])
We keep 1.58e+07/1.65e+09 =  0% of the original kernel matrix.

torch.Size([316508, 2])
We keep 7.30e+08/5.84e+10 =  1% of the original kernel matrix.

torch.Size([133276, 2])
We keep 5.81e+07/7.46e+09 =  0% of the original kernel matrix.

torch.Size([280888, 2])
We keep 4.85e+08/4.09e+10 =  1% of the original kernel matrix.

torch.Size([124379, 2])
We keep 4.92e+07/6.24e+09 =  0% of the original kernel matrix.

torch.Size([16717, 2])
We keep 2.17e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([29695, 2])
We keep 3.85e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([9709, 2])
We keep 6.48e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([23462, 2])
We keep 2.25e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([10569, 2])
We keep 1.05e+06/2.69e+07 =  3% of the original kernel matrix.

torch.Size([23467, 2])
We keep 2.57e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([5967, 2])
We keep 2.07e+05/5.23e+06 =  3% of the original kernel matrix.

torch.Size([19233, 2])
We keep 1.45e+06/7.06e+07 =  2% of the original kernel matrix.

torch.Size([31960, 2])
We keep 1.68e+07/6.20e+08 =  2% of the original kernel matrix.

torch.Size([40734, 2])
We keep 8.49e+06/7.69e+08 =  1% of the original kernel matrix.

torch.Size([16356, 2])
We keep 1.56e+06/6.27e+07 =  2% of the original kernel matrix.

torch.Size([29920, 2])
We keep 3.51e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([34212, 2])
We keep 1.13e+07/4.98e+08 =  2% of the original kernel matrix.

torch.Size([42585, 2])
We keep 7.69e+06/6.89e+08 =  1% of the original kernel matrix.

torch.Size([79041, 2])
We keep 7.56e+07/3.47e+09 =  2% of the original kernel matrix.

torch.Size([65121, 2])
We keep 1.61e+07/1.82e+09 =  0% of the original kernel matrix.

torch.Size([55779, 2])
We keep 5.36e+07/1.60e+09 =  3% of the original kernel matrix.

torch.Size([54961, 2])
We keep 1.24e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([12193, 2])
We keep 3.08e+06/6.85e+07 =  4% of the original kernel matrix.

torch.Size([24813, 2])
We keep 3.66e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([13870, 2])
We keep 9.87e+05/3.70e+07 =  2% of the original kernel matrix.

torch.Size([27670, 2])
We keep 2.90e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([50813, 2])
We keep 2.07e+08/7.80e+09 =  2% of the original kernel matrix.

torch.Size([45906, 2])
We keep 2.41e+07/2.73e+09 =  0% of the original kernel matrix.

torch.Size([46670, 2])
We keep 1.83e+08/1.67e+09 = 10% of the original kernel matrix.

torch.Size([48927, 2])
We keep 1.24e+07/1.26e+09 =  0% of the original kernel matrix.

torch.Size([17034, 2])
We keep 8.37e+06/1.16e+08 =  7% of the original kernel matrix.

torch.Size([29986, 2])
We keep 4.42e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([66747, 2])
We keep 2.55e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([60615, 2])
We keep 1.31e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([467268, 2])
We keep 3.78e+09/1.61e+11 =  2% of the original kernel matrix.

torch.Size([158348, 2])
We keep 9.21e+07/1.24e+10 =  0% of the original kernel matrix.

torch.Size([11369, 2])
We keep 9.06e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([25244, 2])
We keep 2.56e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([100704, 2])
We keep 2.16e+08/9.17e+09 =  2% of the original kernel matrix.

torch.Size([68439, 2])
We keep 2.56e+07/2.96e+09 =  0% of the original kernel matrix.

torch.Size([7439, 2])
We keep 5.67e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([20760, 2])
We keep 1.99e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([19182, 2])
We keep 9.12e+06/3.15e+08 =  2% of the original kernel matrix.

torch.Size([29220, 2])
We keep 6.57e+06/5.48e+08 =  1% of the original kernel matrix.

torch.Size([19542, 2])
We keep 3.54e+06/1.10e+08 =  3% of the original kernel matrix.

torch.Size([32525, 2])
We keep 4.28e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([278135, 2])
We keep 1.77e+09/8.10e+10 =  2% of the original kernel matrix.

torch.Size([116308, 2])
We keep 6.79e+07/8.79e+09 =  0% of the original kernel matrix.

torch.Size([72043, 2])
We keep 2.35e+07/2.36e+09 =  0% of the original kernel matrix.

torch.Size([62046, 2])
We keep 1.45e+07/1.50e+09 =  0% of the original kernel matrix.

torch.Size([8490, 2])
We keep 4.98e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([22175, 2])
We keep 1.97e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([150623, 2])
We keep 7.16e+07/8.64e+09 =  0% of the original kernel matrix.

torch.Size([89821, 2])
We keep 2.50e+07/2.87e+09 =  0% of the original kernel matrix.

torch.Size([173660, 2])
We keep 9.68e+07/1.02e+10 =  0% of the original kernel matrix.

torch.Size([96388, 2])
We keep 2.65e+07/3.11e+09 =  0% of the original kernel matrix.

torch.Size([11391, 2])
We keep 1.77e+06/3.94e+07 =  4% of the original kernel matrix.

torch.Size([24724, 2])
We keep 2.92e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([77644, 2])
We keep 4.02e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([64996, 2])
We keep 1.39e+07/1.42e+09 =  0% of the original kernel matrix.

torch.Size([256299, 2])
We keep 1.83e+08/2.59e+10 =  0% of the original kernel matrix.

torch.Size([119630, 2])
We keep 4.00e+07/4.96e+09 =  0% of the original kernel matrix.

torch.Size([481987, 2])
We keep 5.69e+08/9.41e+10 =  0% of the original kernel matrix.

torch.Size([168967, 2])
We keep 7.07e+07/9.47e+09 =  0% of the original kernel matrix.

torch.Size([32714, 2])
We keep 2.50e+07/5.27e+08 =  4% of the original kernel matrix.

torch.Size([41823, 2])
We keep 7.96e+06/7.09e+08 =  1% of the original kernel matrix.

torch.Size([3343, 2])
We keep 9.95e+04/1.83e+06 =  5% of the original kernel matrix.

torch.Size([15223, 2])
We keep 1.01e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([18173, 2])
We keep 1.59e+06/6.87e+07 =  2% of the original kernel matrix.

torch.Size([31333, 2])
We keep 3.63e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([6226, 2])
We keep 8.24e+05/1.23e+07 =  6% of the original kernel matrix.

torch.Size([19172, 2])
We keep 1.94e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([32205, 2])
We keep 6.46e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([41600, 2])
We keep 6.69e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([13894, 2])
We keep 1.98e+06/4.25e+07 =  4% of the original kernel matrix.

torch.Size([27564, 2])
We keep 3.06e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([9091, 2])
We keep 5.52e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([22959, 2])
We keep 2.06e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([59845, 2])
We keep 2.02e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([57593, 2])
We keep 1.13e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([188820, 2])
We keep 1.38e+08/1.43e+10 =  0% of the original kernel matrix.

torch.Size([100885, 2])
We keep 3.13e+07/3.69e+09 =  0% of the original kernel matrix.

torch.Size([13133, 2])
We keep 6.47e+06/9.48e+07 =  6% of the original kernel matrix.

torch.Size([25950, 2])
We keep 3.99e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([12729, 2])
We keep 1.06e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([26312, 2])
We keep 2.82e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([15869, 2])
We keep 1.65e+06/6.58e+07 =  2% of the original kernel matrix.

torch.Size([29062, 2])
We keep 3.58e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([24990, 2])
We keep 2.86e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([36745, 2])
We keep 4.97e+06/3.93e+08 =  1% of the original kernel matrix.

torch.Size([17846, 2])
We keep 1.82e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([30940, 2])
We keep 3.66e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([36266, 2])
We keep 6.95e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([45135, 2])
We keep 7.26e+06/6.39e+08 =  1% of the original kernel matrix.

torch.Size([7894, 2])
We keep 3.10e+05/9.09e+06 =  3% of the original kernel matrix.

torch.Size([21817, 2])
We keep 1.74e+06/9.31e+07 =  1% of the original kernel matrix.

torch.Size([65388, 2])
We keep 1.11e+08/2.84e+09 =  3% of the original kernel matrix.

torch.Size([57001, 2])
We keep 1.58e+07/1.64e+09 =  0% of the original kernel matrix.

torch.Size([43397, 2])
We keep 7.46e+06/6.10e+08 =  1% of the original kernel matrix.

torch.Size([49859, 2])
We keep 8.41e+06/7.63e+08 =  1% of the original kernel matrix.

torch.Size([33350, 2])
We keep 6.69e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([43361, 2])
We keep 7.13e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([41175, 2])
We keep 1.96e+07/8.08e+08 =  2% of the original kernel matrix.

torch.Size([47525, 2])
We keep 9.54e+06/8.78e+08 =  1% of the original kernel matrix.

torch.Size([14967, 2])
We keep 3.23e+06/9.33e+07 =  3% of the original kernel matrix.

torch.Size([27630, 2])
We keep 4.12e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([524802, 2])
We keep 6.98e+09/1.61e+11 =  4% of the original kernel matrix.

torch.Size([169886, 2])
We keep 8.86e+07/1.24e+10 =  0% of the original kernel matrix.

torch.Size([21243, 2])
We keep 2.52e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([33903, 2])
We keep 4.28e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([83451, 2])
We keep 2.78e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([66802, 2])
We keep 1.53e+07/1.59e+09 =  0% of the original kernel matrix.

torch.Size([3284, 2])
We keep 9.22e+04/1.60e+06 =  5% of the original kernel matrix.

torch.Size([15433, 2])
We keep 9.93e+05/3.91e+07 =  2% of the original kernel matrix.

torch.Size([12597, 2])
We keep 1.09e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([26353, 2])
We keep 2.73e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([9426, 2])
We keep 5.30e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([23033, 2])
We keep 2.15e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([18377, 2])
We keep 2.20e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([31570, 2])
We keep 3.69e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([8537, 2])
We keep 5.06e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([22323, 2])
We keep 2.04e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([78217, 2])
We keep 9.70e+07/3.99e+09 =  2% of the original kernel matrix.

torch.Size([61419, 2])
We keep 1.83e+07/1.95e+09 =  0% of the original kernel matrix.

torch.Size([8330, 2])
We keep 3.96e+05/1.18e+07 =  3% of the original kernel matrix.

torch.Size([22110, 2])
We keep 1.93e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([12993, 2])
We keep 1.17e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([26648, 2])
We keep 2.83e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([355209, 2])
We keep 7.64e+08/6.31e+10 =  1% of the original kernel matrix.

torch.Size([141347, 2])
We keep 5.94e+07/7.76e+09 =  0% of the original kernel matrix.

torch.Size([44883, 2])
We keep 1.69e+07/8.06e+08 =  2% of the original kernel matrix.

torch.Size([50140, 2])
We keep 9.60e+06/8.76e+08 =  1% of the original kernel matrix.

torch.Size([52805, 2])
We keep 2.20e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([53777, 2])
We keep 1.11e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([170521, 2])
We keep 1.54e+08/1.23e+10 =  1% of the original kernel matrix.

torch.Size([96645, 2])
We keep 2.94e+07/3.42e+09 =  0% of the original kernel matrix.

torch.Size([199703, 2])
We keep 3.60e+08/2.90e+10 =  1% of the original kernel matrix.

torch.Size([99323, 2])
We keep 4.26e+07/5.26e+09 =  0% of the original kernel matrix.

torch.Size([11437, 2])
We keep 6.68e+05/2.30e+07 =  2% of the original kernel matrix.

torch.Size([25146, 2])
We keep 2.42e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([11299, 2])
We keep 7.64e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([24887, 2])
We keep 2.51e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([33601, 2])
We keep 3.05e+07/5.80e+08 =  5% of the original kernel matrix.

torch.Size([42045, 2])
We keep 8.26e+06/7.44e+08 =  1% of the original kernel matrix.

torch.Size([1193, 2])
We keep 2.14e+04/2.20e+05 =  9% of the original kernel matrix.

torch.Size([10341, 2])
We keep 5.17e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([8750, 2])
We keep 5.76e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([22517, 2])
We keep 2.05e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([20642, 2])
We keep 3.25e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([33197, 2])
We keep 4.68e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([3704, 2])
We keep 1.08e+05/2.05e+06 =  5% of the original kernel matrix.

torch.Size([16087, 2])
We keep 1.07e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([12339, 2])
We keep 9.46e+05/2.91e+07 =  3% of the original kernel matrix.

torch.Size([26224, 2])
We keep 2.62e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([4671, 2])
We keep 2.10e+05/4.18e+06 =  5% of the original kernel matrix.

torch.Size([17254, 2])
We keep 1.35e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([6289, 2])
We keep 2.95e+05/7.41e+06 =  3% of the original kernel matrix.

torch.Size([19759, 2])
We keep 1.67e+06/8.41e+07 =  1% of the original kernel matrix.

torch.Size([340600, 2])
We keep 9.49e+08/6.01e+10 =  1% of the original kernel matrix.

torch.Size([137520, 2])
We keep 5.76e+07/7.57e+09 =  0% of the original kernel matrix.

torch.Size([18896, 2])
We keep 6.22e+06/1.71e+08 =  3% of the original kernel matrix.

torch.Size([30989, 2])
We keep 5.19e+06/4.03e+08 =  1% of the original kernel matrix.

torch.Size([83024, 2])
We keep 4.85e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([66209, 2])
We keep 1.56e+07/1.63e+09 =  0% of the original kernel matrix.

torch.Size([12289, 2])
We keep 2.98e+06/5.01e+07 =  5% of the original kernel matrix.

torch.Size([25679, 2])
We keep 3.19e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([14495, 2])
We keep 5.25e+06/7.22e+07 =  7% of the original kernel matrix.

torch.Size([27624, 2])
We keep 3.56e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([15745, 2])
We keep 2.08e+06/6.31e+07 =  3% of the original kernel matrix.

torch.Size([29011, 2])
We keep 3.55e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([884762, 2])
We keep 4.80e+09/3.55e+11 =  1% of the original kernel matrix.

torch.Size([227004, 2])
We keep 1.27e+08/1.84e+10 =  0% of the original kernel matrix.

torch.Size([707626, 2])
We keep 9.38e+08/1.75e+11 =  0% of the original kernel matrix.

torch.Size([201780, 2])
We keep 9.29e+07/1.29e+10 =  0% of the original kernel matrix.

torch.Size([27016, 2])
We keep 4.25e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([38189, 2])
We keep 5.49e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([66856, 2])
We keep 3.50e+07/1.59e+09 =  2% of the original kernel matrix.

torch.Size([60604, 2])
We keep 1.23e+07/1.23e+09 =  0% of the original kernel matrix.

torch.Size([5491, 2])
We keep 2.42e+05/5.25e+06 =  4% of the original kernel matrix.

torch.Size([18622, 2])
We keep 1.46e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([25454, 2])
We keep 9.81e+07/7.43e+08 = 13% of the original kernel matrix.

torch.Size([34338, 2])
We keep 8.47e+06/8.42e+08 =  1% of the original kernel matrix.

torch.Size([44040, 2])
We keep 4.03e+07/1.28e+09 =  3% of the original kernel matrix.

torch.Size([47556, 2])
We keep 1.13e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([38932, 2])
We keep 1.61e+07/6.12e+08 =  2% of the original kernel matrix.

torch.Size([46861, 2])
We keep 8.29e+06/7.64e+08 =  1% of the original kernel matrix.

torch.Size([17224, 2])
We keep 2.86e+06/7.09e+07 =  4% of the original kernel matrix.

torch.Size([30309, 2])
We keep 3.61e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([80906, 2])
We keep 4.26e+07/2.98e+09 =  1% of the original kernel matrix.

torch.Size([65061, 2])
We keep 1.61e+07/1.68e+09 =  0% of the original kernel matrix.

torch.Size([529840, 2])
We keep 1.72e+09/1.39e+11 =  1% of the original kernel matrix.

torch.Size([171102, 2])
We keep 8.50e+07/1.15e+10 =  0% of the original kernel matrix.

torch.Size([25543, 2])
We keep 6.07e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([36616, 2])
We keep 6.05e+06/4.96e+08 =  1% of the original kernel matrix.

torch.Size([7325, 2])
We keep 4.43e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([20816, 2])
We keep 1.86e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([32425, 2])
We keep 1.32e+07/4.55e+08 =  2% of the original kernel matrix.

torch.Size([42573, 2])
We keep 7.69e+06/6.58e+08 =  1% of the original kernel matrix.

torch.Size([44233, 2])
We keep 1.04e+07/7.17e+08 =  1% of the original kernel matrix.

torch.Size([49887, 2])
We keep 9.05e+06/8.27e+08 =  1% of the original kernel matrix.

torch.Size([5354, 2])
We keep 2.34e+05/4.71e+06 =  4% of the original kernel matrix.

torch.Size([18607, 2])
We keep 1.42e+06/6.70e+07 =  2% of the original kernel matrix.

torch.Size([138850, 2])
We keep 1.13e+08/7.99e+09 =  1% of the original kernel matrix.

torch.Size([86267, 2])
We keep 2.44e+07/2.76e+09 =  0% of the original kernel matrix.

torch.Size([27734, 2])
We keep 5.75e+06/2.69e+08 =  2% of the original kernel matrix.

torch.Size([38684, 2])
We keep 6.01e+06/5.06e+08 =  1% of the original kernel matrix.

torch.Size([2087, 2])
We keep 3.09e+04/4.64e+05 =  6% of the original kernel matrix.

torch.Size([13184, 2])
We keep 6.63e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([85448, 2])
We keep 3.18e+07/2.64e+09 =  1% of the original kernel matrix.

torch.Size([68155, 2])
We keep 1.51e+07/1.59e+09 =  0% of the original kernel matrix.

torch.Size([49902, 2])
We keep 1.42e+07/9.58e+08 =  1% of the original kernel matrix.

torch.Size([52873, 2])
We keep 1.01e+07/9.56e+08 =  1% of the original kernel matrix.

torch.Size([2420, 2])
We keep 5.38e+04/8.41e+05 =  6% of the original kernel matrix.

torch.Size([13845, 2])
We keep 7.96e+05/2.83e+07 =  2% of the original kernel matrix.

torch.Size([145970, 2])
We keep 1.70e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([86993, 2])
We keep 2.91e+07/3.38e+09 =  0% of the original kernel matrix.

torch.Size([162683, 2])
We keep 1.23e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([93186, 2])
We keep 2.69e+07/3.12e+09 =  0% of the original kernel matrix.

torch.Size([213806, 2])
We keep 3.13e+08/2.01e+10 =  1% of the original kernel matrix.

torch.Size([108670, 2])
We keep 3.65e+07/4.38e+09 =  0% of the original kernel matrix.

torch.Size([247945, 2])
We keep 5.76e+08/2.83e+10 =  2% of the original kernel matrix.

torch.Size([115611, 2])
We keep 4.23e+07/5.19e+09 =  0% of the original kernel matrix.

torch.Size([142813, 2])
We keep 2.21e+08/1.04e+10 =  2% of the original kernel matrix.

torch.Size([85605, 2])
We keep 2.71e+07/3.15e+09 =  0% of the original kernel matrix.

torch.Size([19105, 2])
We keep 3.76e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([31806, 2])
We keep 4.68e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([36463, 2])
We keep 1.64e+07/6.32e+08 =  2% of the original kernel matrix.

torch.Size([43909, 2])
We keep 8.48e+06/7.76e+08 =  1% of the original kernel matrix.

torch.Size([80050, 2])
We keep 2.21e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([66107, 2])
We keep 1.38e+07/1.43e+09 =  0% of the original kernel matrix.

torch.Size([205620, 2])
We keep 1.24e+08/1.43e+10 =  0% of the original kernel matrix.

torch.Size([106209, 2])
We keep 3.07e+07/3.70e+09 =  0% of the original kernel matrix.

torch.Size([9845, 2])
We keep 6.84e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([23508, 2])
We keep 2.34e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([90969, 2])
We keep 1.39e+08/4.28e+09 =  3% of the original kernel matrix.

torch.Size([69570, 2])
We keep 1.79e+07/2.02e+09 =  0% of the original kernel matrix.

torch.Size([395698, 2])
We keep 2.78e+09/8.44e+10 =  3% of the original kernel matrix.

torch.Size([148802, 2])
We keep 6.58e+07/8.97e+09 =  0% of the original kernel matrix.

torch.Size([132870, 2])
We keep 1.29e+08/6.79e+09 =  1% of the original kernel matrix.

torch.Size([83692, 2])
We keep 2.26e+07/2.54e+09 =  0% of the original kernel matrix.

torch.Size([31414, 2])
We keep 1.94e+07/6.05e+08 =  3% of the original kernel matrix.

torch.Size([40971, 2])
We keep 8.27e+06/7.59e+08 =  1% of the original kernel matrix.

torch.Size([37195, 2])
We keep 8.37e+07/1.60e+09 =  5% of the original kernel matrix.

torch.Size([41676, 2])
We keep 1.24e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([12507, 2])
We keep 8.93e+05/3.15e+07 =  2% of the original kernel matrix.

torch.Size([26273, 2])
We keep 2.72e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([3538, 2])
We keep 1.16e+05/2.16e+06 =  5% of the original kernel matrix.

torch.Size([15537, 2])
We keep 1.10e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([9447, 2])
We keep 5.40e+06/3.67e+07 = 14% of the original kernel matrix.

torch.Size([22268, 2])
We keep 2.95e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([153019, 2])
We keep 6.87e+07/7.78e+09 =  0% of the original kernel matrix.

torch.Size([89530, 2])
We keep 2.39e+07/2.72e+09 =  0% of the original kernel matrix.

torch.Size([85624, 2])
We keep 2.65e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([67739, 2])
We keep 1.50e+07/1.57e+09 =  0% of the original kernel matrix.

torch.Size([496026, 2])
We keep 1.53e+09/1.27e+11 =  1% of the original kernel matrix.

torch.Size([164635, 2])
We keep 8.20e+07/1.10e+10 =  0% of the original kernel matrix.

torch.Size([211714, 2])
We keep 4.60e+08/2.66e+10 =  1% of the original kernel matrix.

torch.Size([107645, 2])
We keep 4.15e+07/5.03e+09 =  0% of the original kernel matrix.

torch.Size([73682, 2])
We keep 3.27e+07/2.28e+09 =  1% of the original kernel matrix.

torch.Size([62765, 2])
We keep 1.43e+07/1.47e+09 =  0% of the original kernel matrix.

torch.Size([18971, 2])
We keep 2.98e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([31817, 2])
We keep 5.01e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([124277, 2])
We keep 7.92e+07/5.42e+09 =  1% of the original kernel matrix.

torch.Size([80797, 2])
We keep 2.06e+07/2.27e+09 =  0% of the original kernel matrix.

torch.Size([3876, 2])
We keep 1.39e+05/2.74e+06 =  5% of the original kernel matrix.

torch.Size([15995, 2])
We keep 1.18e+06/5.11e+07 =  2% of the original kernel matrix.

torch.Size([7254, 2])
We keep 3.62e+05/9.39e+06 =  3% of the original kernel matrix.

torch.Size([20548, 2])
We keep 1.77e+06/9.46e+07 =  1% of the original kernel matrix.

torch.Size([291899, 2])
We keep 2.95e+08/3.31e+10 =  0% of the original kernel matrix.

torch.Size([127851, 2])
We keep 4.47e+07/5.62e+09 =  0% of the original kernel matrix.

torch.Size([10671, 2])
We keep 6.66e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([24677, 2])
We keep 2.31e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([7318, 2])
We keep 4.14e+05/9.29e+06 =  4% of the original kernel matrix.

torch.Size([20835, 2])
We keep 1.78e+06/9.41e+07 =  1% of the original kernel matrix.

torch.Size([20241, 2])
We keep 3.71e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([32944, 2])
We keep 4.82e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([118417, 2])
We keep 2.76e+08/1.00e+10 =  2% of the original kernel matrix.

torch.Size([77566, 2])
We keep 2.66e+07/3.09e+09 =  0% of the original kernel matrix.

torch.Size([27651, 2])
We keep 6.20e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([38458, 2])
We keep 6.24e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([136460, 2])
We keep 1.12e+08/7.88e+09 =  1% of the original kernel matrix.

torch.Size([85530, 2])
We keep 2.44e+07/2.74e+09 =  0% of the original kernel matrix.

torch.Size([9475, 2])
We keep 7.40e+06/7.36e+07 = 10% of the original kernel matrix.

torch.Size([21785, 2])
We keep 3.63e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([15345, 2])
We keep 5.79e+06/9.50e+07 =  6% of the original kernel matrix.

torch.Size([28434, 2])
We keep 3.94e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([7909, 2])
We keep 5.48e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([21327, 2])
We keep 2.07e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([11158, 2])
We keep 7.50e+05/2.36e+07 =  3% of the original kernel matrix.

torch.Size([25073, 2])
We keep 2.45e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([66642, 2])
We keep 3.12e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([60838, 2])
We keep 1.33e+07/1.33e+09 =  0% of the original kernel matrix.

torch.Size([35924, 2])
We keep 1.51e+07/6.38e+08 =  2% of the original kernel matrix.

torch.Size([43843, 2])
We keep 8.73e+06/7.80e+08 =  1% of the original kernel matrix.

torch.Size([12909, 2])
We keep 3.68e+06/5.43e+07 =  6% of the original kernel matrix.

torch.Size([26432, 2])
We keep 3.24e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([116986, 2])
We keep 9.85e+07/6.00e+09 =  1% of the original kernel matrix.

torch.Size([76469, 2])
We keep 2.13e+07/2.39e+09 =  0% of the original kernel matrix.

torch.Size([4340, 2])
We keep 1.39e+05/2.76e+06 =  5% of the original kernel matrix.

torch.Size([17220, 2])
We keep 1.19e+06/5.13e+07 =  2% of the original kernel matrix.

torch.Size([69282, 2])
We keep 5.46e+07/1.97e+09 =  2% of the original kernel matrix.

torch.Size([62214, 2])
We keep 1.36e+07/1.37e+09 =  0% of the original kernel matrix.

torch.Size([8586, 2])
We keep 3.37e+06/3.21e+07 = 10% of the original kernel matrix.

torch.Size([21404, 2])
We keep 2.74e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([77253, 2])
We keep 2.44e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([64827, 2])
We keep 1.39e+07/1.43e+09 =  0% of the original kernel matrix.

torch.Size([10506, 2])
We keep 8.74e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([24252, 2])
We keep 2.54e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([68394, 2])
We keep 1.02e+08/2.17e+09 =  4% of the original kernel matrix.

torch.Size([60212, 2])
We keep 1.43e+07/1.44e+09 =  0% of the original kernel matrix.

torch.Size([14627, 2])
We keep 1.70e+06/4.79e+07 =  3% of the original kernel matrix.

torch.Size([28081, 2])
We keep 3.16e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([64363, 2])
We keep 2.23e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([60060, 2])
We keep 1.26e+07/1.26e+09 =  0% of the original kernel matrix.

torch.Size([83539, 2])
We keep 3.46e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([67102, 2])
We keep 1.56e+07/1.62e+09 =  0% of the original kernel matrix.

torch.Size([182133, 2])
We keep 3.04e+08/2.11e+10 =  1% of the original kernel matrix.

torch.Size([96465, 2])
We keep 3.66e+07/4.48e+09 =  0% of the original kernel matrix.

torch.Size([13396, 2])
We keep 1.47e+06/4.18e+07 =  3% of the original kernel matrix.

torch.Size([27050, 2])
We keep 3.05e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([9806, 2])
We keep 1.51e+06/3.30e+07 =  4% of the original kernel matrix.

torch.Size([22769, 2])
We keep 2.72e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([169020, 2])
We keep 2.87e+08/1.32e+10 =  2% of the original kernel matrix.

torch.Size([94489, 2])
We keep 3.02e+07/3.54e+09 =  0% of the original kernel matrix.

torch.Size([13626, 2])
We keep 1.18e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([27061, 2])
We keep 3.01e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([771292, 2])
We keep 1.20e+09/2.07e+11 =  0% of the original kernel matrix.

torch.Size([211340, 2])
We keep 1.00e+08/1.40e+10 =  0% of the original kernel matrix.

torch.Size([2546, 2])
We keep 8.22e+04/1.19e+06 =  6% of the original kernel matrix.

torch.Size([13734, 2])
We keep 9.04e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([71065, 2])
We keep 3.02e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([61171, 2])
We keep 1.42e+07/1.45e+09 =  0% of the original kernel matrix.

torch.Size([181892, 2])
We keep 9.94e+07/1.26e+10 =  0% of the original kernel matrix.

torch.Size([99450, 2])
We keep 2.95e+07/3.47e+09 =  0% of the original kernel matrix.

torch.Size([6121, 2])
We keep 2.42e+05/6.15e+06 =  3% of the original kernel matrix.

torch.Size([19352, 2])
We keep 1.54e+06/7.66e+07 =  2% of the original kernel matrix.

torch.Size([69466, 2])
We keep 6.94e+07/3.67e+09 =  1% of the original kernel matrix.

torch.Size([57177, 2])
We keep 1.73e+07/1.87e+09 =  0% of the original kernel matrix.

torch.Size([26555, 2])
We keep 1.57e+07/5.62e+08 =  2% of the original kernel matrix.

torch.Size([35876, 2])
We keep 8.15e+06/7.32e+08 =  1% of the original kernel matrix.

torch.Size([595477, 2])
We keep 2.35e+09/1.79e+11 =  1% of the original kernel matrix.

torch.Size([182504, 2])
We keep 9.54e+07/1.30e+10 =  0% of the original kernel matrix.

torch.Size([10097, 2])
We keep 6.06e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([23734, 2])
We keep 2.26e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([29772, 2])
We keep 6.16e+06/3.07e+08 =  2% of the original kernel matrix.

torch.Size([40136, 2])
We keep 6.38e+06/5.41e+08 =  1% of the original kernel matrix.

torch.Size([9807, 2])
We keep 7.58e+06/4.44e+07 = 17% of the original kernel matrix.

torch.Size([22983, 2])
We keep 3.20e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([8535, 2])
We keep 4.49e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([22153, 2])
We keep 1.99e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([21649, 2])
We keep 4.54e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([33840, 2])
We keep 5.12e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([32787, 2])
We keep 6.14e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([42473, 2])
We keep 6.64e+06/5.90e+08 =  1% of the original kernel matrix.

torch.Size([14484, 2])
We keep 2.25e+06/5.74e+07 =  3% of the original kernel matrix.

torch.Size([27904, 2])
We keep 3.35e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([45644, 2])
We keep 1.55e+07/8.73e+08 =  1% of the original kernel matrix.

torch.Size([51069, 2])
We keep 9.96e+06/9.12e+08 =  1% of the original kernel matrix.

torch.Size([28024, 2])
We keep 4.03e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([39147, 2])
We keep 5.81e+06/4.78e+08 =  1% of the original kernel matrix.

torch.Size([5533, 2])
We keep 2.78e+05/5.20e+06 =  5% of the original kernel matrix.

torch.Size([18549, 2])
We keep 1.45e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([13071, 2])
We keep 1.94e+06/4.54e+07 =  4% of the original kernel matrix.

torch.Size([26623, 2])
We keep 3.10e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([293162, 2])
We keep 1.13e+09/7.10e+10 =  1% of the original kernel matrix.

torch.Size([125778, 2])
We keep 6.33e+07/8.23e+09 =  0% of the original kernel matrix.

torch.Size([43749, 2])
We keep 4.66e+07/1.29e+09 =  3% of the original kernel matrix.

torch.Size([47885, 2])
We keep 1.16e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([306242, 2])
We keep 3.24e+08/3.62e+10 =  0% of the original kernel matrix.

torch.Size([132358, 2])
We keep 4.67e+07/5.88e+09 =  0% of the original kernel matrix.

torch.Size([43983, 2])
We keep 1.96e+07/8.24e+08 =  2% of the original kernel matrix.

torch.Size([46793, 2])
We keep 8.69e+06/8.86e+08 =  0% of the original kernel matrix.

torch.Size([14133, 2])
We keep 2.15e+06/6.43e+07 =  3% of the original kernel matrix.

torch.Size([27493, 2])
We keep 3.58e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([13873, 2])
We keep 1.08e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([27714, 2])
We keep 2.87e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([7683, 2])
We keep 1.07e+06/1.91e+07 =  5% of the original kernel matrix.

torch.Size([20643, 2])
We keep 2.28e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([524051, 2])
We keep 1.15e+09/1.14e+11 =  1% of the original kernel matrix.

torch.Size([174470, 2])
We keep 7.78e+07/1.04e+10 =  0% of the original kernel matrix.

torch.Size([22266, 2])
We keep 2.95e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([34670, 2])
We keep 4.89e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([35573, 2])
We keep 7.61e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([44397, 2])
We keep 7.20e+06/6.26e+08 =  1% of the original kernel matrix.

torch.Size([57994, 2])
We keep 3.32e+07/1.58e+09 =  2% of the original kernel matrix.

torch.Size([55242, 2])
We keep 1.23e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([58087, 2])
We keep 4.06e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([54432, 2])
We keep 1.42e+07/1.44e+09 =  0% of the original kernel matrix.

torch.Size([5986, 2])
We keep 2.41e+05/5.55e+06 =  4% of the original kernel matrix.

torch.Size([19304, 2])
We keep 1.47e+06/7.27e+07 =  2% of the original kernel matrix.

torch.Size([7806, 2])
We keep 5.27e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([21152, 2])
We keep 1.97e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([184905, 2])
We keep 4.05e+08/2.71e+10 =  1% of the original kernel matrix.

torch.Size([97847, 2])
We keep 4.17e+07/5.08e+09 =  0% of the original kernel matrix.

torch.Size([23585, 2])
We keep 3.66e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([35747, 2])
We keep 4.92e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([28575, 2])
We keep 4.46e+07/7.36e+08 =  6% of the original kernel matrix.

torch.Size([36620, 2])
We keep 8.55e+06/8.37e+08 =  1% of the original kernel matrix.

torch.Size([51480, 2])
We keep 1.56e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([54480, 2])
We keep 1.00e+07/9.31e+08 =  1% of the original kernel matrix.

torch.Size([143743, 2])
We keep 3.14e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([87860, 2])
We keep 2.87e+07/3.28e+09 =  0% of the original kernel matrix.

torch.Size([199670, 2])
We keep 3.79e+08/2.06e+10 =  1% of the original kernel matrix.

torch.Size([104258, 2])
We keep 3.70e+07/4.43e+09 =  0% of the original kernel matrix.

torch.Size([198902, 2])
We keep 2.65e+08/2.06e+10 =  1% of the original kernel matrix.

torch.Size([103023, 2])
We keep 3.68e+07/4.43e+09 =  0% of the original kernel matrix.

torch.Size([146479, 2])
We keep 1.65e+08/7.68e+09 =  2% of the original kernel matrix.

torch.Size([87972, 2])
We keep 2.38e+07/2.71e+09 =  0% of the original kernel matrix.

torch.Size([13923, 2])
We keep 2.39e+06/5.03e+07 =  4% of the original kernel matrix.

torch.Size([27495, 2])
We keep 3.18e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([65953, 2])
We keep 1.75e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([59970, 2])
We keep 1.21e+07/1.22e+09 =  0% of the original kernel matrix.

torch.Size([21964, 2])
We keep 1.03e+07/2.25e+08 =  4% of the original kernel matrix.

torch.Size([33702, 2])
We keep 5.69e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([14136, 2])
We keep 1.04e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([27732, 2])
We keep 2.94e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([19465, 2])
We keep 3.20e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([32028, 2])
We keep 4.75e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([10160, 2])
We keep 6.10e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([24048, 2])
We keep 2.23e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([25328, 2])
We keep 7.91e+06/2.21e+08 =  3% of the original kernel matrix.

torch.Size([36937, 2])
We keep 5.62e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([15529, 2])
We keep 1.71e+06/6.18e+07 =  2% of the original kernel matrix.

torch.Size([28835, 2])
We keep 3.51e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([1478, 2])
We keep 2.15e+04/2.75e+05 =  7% of the original kernel matrix.

torch.Size([11566, 2])
We keep 5.66e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([26159, 2])
We keep 3.78e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([37628, 2])
We keep 5.52e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([9359, 2])
We keep 1.92e+06/3.49e+07 =  5% of the original kernel matrix.

torch.Size([21962, 2])
We keep 2.81e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([71237, 2])
We keep 9.77e+07/4.00e+09 =  2% of the original kernel matrix.

torch.Size([59400, 2])
We keep 1.81e+07/1.95e+09 =  0% of the original kernel matrix.

torch.Size([13181, 2])
We keep 1.54e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([26459, 2])
We keep 3.22e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([34330, 2])
We keep 8.43e+06/3.94e+08 =  2% of the original kernel matrix.

torch.Size([43808, 2])
We keep 7.06e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([11989, 2])
We keep 1.33e+07/1.45e+08 =  9% of the original kernel matrix.

torch.Size([23079, 2])
We keep 4.69e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([58896, 2])
We keep 4.36e+07/1.63e+09 =  2% of the original kernel matrix.

torch.Size([56015, 2])
We keep 1.26e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([5467, 2])
We keep 2.64e+05/4.99e+06 =  5% of the original kernel matrix.

torch.Size([18621, 2])
We keep 1.45e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([15833, 2])
We keep 1.46e+06/5.62e+07 =  2% of the original kernel matrix.

torch.Size([29124, 2])
We keep 3.36e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([31989, 2])
We keep 4.55e+06/2.90e+08 =  1% of the original kernel matrix.

torch.Size([42192, 2])
We keep 6.23e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([190586, 2])
We keep 1.87e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([101102, 2])
We keep 3.26e+07/3.90e+09 =  0% of the original kernel matrix.

torch.Size([13849, 2])
We keep 1.29e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([27423, 2])
We keep 3.11e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([2710, 2])
We keep 5.71e+04/9.24e+05 =  6% of the original kernel matrix.

torch.Size([14287, 2])
We keep 8.29e+05/2.97e+07 =  2% of the original kernel matrix.

torch.Size([6007, 2])
We keep 5.21e+05/9.12e+06 =  5% of the original kernel matrix.

torch.Size([18582, 2])
We keep 1.79e+06/9.32e+07 =  1% of the original kernel matrix.

torch.Size([364589, 2])
We keep 7.91e+08/6.42e+10 =  1% of the original kernel matrix.

torch.Size([142659, 2])
We keep 5.96e+07/7.82e+09 =  0% of the original kernel matrix.

torch.Size([456895, 2])
We keep 1.22e+09/8.95e+10 =  1% of the original kernel matrix.

torch.Size([163357, 2])
We keep 6.93e+07/9.24e+09 =  0% of the original kernel matrix.

torch.Size([16453, 2])
We keep 3.28e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([29227, 2])
We keep 4.21e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([21097, 2])
We keep 3.25e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([33765, 2])
We keep 4.60e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([36118, 2])
We keep 1.43e+07/7.00e+08 =  2% of the original kernel matrix.

torch.Size([43990, 2])
We keep 9.15e+06/8.17e+08 =  1% of the original kernel matrix.

torch.Size([187778, 2])
We keep 1.11e+08/1.33e+10 =  0% of the original kernel matrix.

torch.Size([101268, 2])
We keep 3.03e+07/3.57e+09 =  0% of the original kernel matrix.

torch.Size([4649, 2])
We keep 1.39e+05/3.14e+06 =  4% of the original kernel matrix.

torch.Size([17508, 2])
We keep 1.24e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([14020, 2])
We keep 1.62e+06/5.24e+07 =  3% of the original kernel matrix.

torch.Size([27702, 2])
We keep 3.31e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([21284, 2])
We keep 1.91e+06/9.96e+07 =  1% of the original kernel matrix.

torch.Size([33647, 2])
We keep 4.14e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([27890, 2])
We keep 2.86e+07/7.40e+08 =  3% of the original kernel matrix.

torch.Size([36259, 2])
We keep 8.74e+06/8.40e+08 =  1% of the original kernel matrix.

torch.Size([14528, 2])
We keep 2.95e+06/6.48e+07 =  4% of the original kernel matrix.

torch.Size([28027, 2])
We keep 3.54e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([191126, 2])
We keep 1.32e+08/1.35e+10 =  0% of the original kernel matrix.

torch.Size([101859, 2])
We keep 3.03e+07/3.59e+09 =  0% of the original kernel matrix.

torch.Size([18137, 2])
We keep 1.71e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([31074, 2])
We keep 3.72e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([16454, 2])
We keep 6.28e+06/8.28e+07 =  7% of the original kernel matrix.

torch.Size([29914, 2])
We keep 3.68e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([2929, 2])
We keep 6.24e+04/1.08e+06 =  5% of the original kernel matrix.

torch.Size([14800, 2])
We keep 8.70e+05/3.20e+07 =  2% of the original kernel matrix.

torch.Size([34018, 2])
We keep 1.08e+07/4.06e+08 =  2% of the original kernel matrix.

torch.Size([43076, 2])
We keep 7.02e+06/6.22e+08 =  1% of the original kernel matrix.

torch.Size([22405, 2])
We keep 2.70e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([34707, 2])
We keep 4.51e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([36349, 2])
We keep 2.30e+07/7.87e+08 =  2% of the original kernel matrix.

torch.Size([43614, 2])
We keep 9.29e+06/8.66e+08 =  1% of the original kernel matrix.

torch.Size([105355, 2])
We keep 2.38e+08/4.98e+09 =  4% of the original kernel matrix.

torch.Size([73963, 2])
We keep 1.97e+07/2.18e+09 =  0% of the original kernel matrix.

torch.Size([530149, 2])
We keep 6.84e+08/1.05e+11 =  0% of the original kernel matrix.

torch.Size([174679, 2])
We keep 7.37e+07/1.00e+10 =  0% of the original kernel matrix.

torch.Size([14216, 2])
We keep 2.15e+06/5.26e+07 =  4% of the original kernel matrix.

torch.Size([27598, 2])
We keep 3.33e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([8448, 2])
We keep 4.15e+05/1.16e+07 =  3% of the original kernel matrix.

torch.Size([22398, 2])
We keep 1.92e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([26175, 2])
We keep 3.35e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([37785, 2])
We keep 5.25e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([255009, 2])
We keep 3.05e+09/4.43e+10 =  6% of the original kernel matrix.

torch.Size([118898, 2])
We keep 5.20e+07/6.50e+09 =  0% of the original kernel matrix.

torch.Size([41945, 2])
We keep 9.06e+06/5.64e+08 =  1% of the original kernel matrix.

torch.Size([48044, 2])
We keep 8.08e+06/7.33e+08 =  1% of the original kernel matrix.

torch.Size([3285, 2])
We keep 2.85e+05/2.28e+06 = 12% of the original kernel matrix.

torch.Size([14864, 2])
We keep 1.10e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([7850, 2])
We keep 5.90e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([21336, 2])
We keep 1.99e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([32580, 2])
We keep 1.40e+07/3.48e+08 =  4% of the original kernel matrix.

torch.Size([42261, 2])
We keep 6.65e+06/5.76e+08 =  1% of the original kernel matrix.

torch.Size([3816, 2])
We keep 1.11e+05/2.16e+06 =  5% of the original kernel matrix.

torch.Size([16348, 2])
We keep 1.08e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([49185, 2])
We keep 4.03e+07/1.60e+09 =  2% of the original kernel matrix.

torch.Size([49693, 2])
We keep 1.25e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([6955, 2])
We keep 3.57e+05/8.87e+06 =  4% of the original kernel matrix.

torch.Size([20345, 2])
We keep 1.74e+06/9.19e+07 =  1% of the original kernel matrix.

torch.Size([29540, 2])
We keep 3.98e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([40322, 2])
We keep 5.90e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([18584, 2])
We keep 2.20e+06/9.46e+07 =  2% of the original kernel matrix.

torch.Size([31489, 2])
We keep 4.06e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([8852, 2])
We keep 4.11e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([22740, 2])
We keep 1.97e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([4191, 2])
We keep 1.24e+05/2.55e+06 =  4% of the original kernel matrix.

torch.Size([16736, 2])
We keep 1.14e+06/4.93e+07 =  2% of the original kernel matrix.

torch.Size([193162, 2])
We keep 1.25e+08/1.33e+10 =  0% of the original kernel matrix.

torch.Size([102149, 2])
We keep 2.99e+07/3.56e+09 =  0% of the original kernel matrix.

torch.Size([393844, 2])
We keep 5.54e+08/6.69e+10 =  0% of the original kernel matrix.

torch.Size([150754, 2])
We keep 6.14e+07/7.99e+09 =  0% of the original kernel matrix.

torch.Size([208809, 2])
We keep 2.67e+08/1.77e+10 =  1% of the original kernel matrix.

torch.Size([106296, 2])
We keep 3.41e+07/4.11e+09 =  0% of the original kernel matrix.

torch.Size([40878, 2])
We keep 2.59e+07/8.35e+08 =  3% of the original kernel matrix.

torch.Size([46269, 2])
We keep 9.54e+06/8.92e+08 =  1% of the original kernel matrix.

torch.Size([39536, 2])
We keep 7.42e+06/5.36e+08 =  1% of the original kernel matrix.

torch.Size([46476, 2])
We keep 7.93e+06/7.15e+08 =  1% of the original kernel matrix.

torch.Size([21596, 2])
We keep 3.10e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([33776, 2])
We keep 4.61e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([17211, 2])
We keep 2.17e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([30547, 2])
We keep 3.85e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([160326, 2])
We keep 7.96e+07/8.96e+09 =  0% of the original kernel matrix.

torch.Size([91446, 2])
We keep 2.55e+07/2.92e+09 =  0% of the original kernel matrix.

torch.Size([53825, 2])
We keep 4.84e+07/1.32e+09 =  3% of the original kernel matrix.

torch.Size([55427, 2])
We keep 1.19e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([70173, 2])
We keep 1.40e+08/3.75e+09 =  3% of the original kernel matrix.

torch.Size([60174, 2])
We keep 1.81e+07/1.89e+09 =  0% of the original kernel matrix.

torch.Size([22404, 2])
We keep 3.31e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([34814, 2])
We keep 4.72e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([561046, 2])
We keep 2.42e+09/2.02e+11 =  1% of the original kernel matrix.

torch.Size([174621, 2])
We keep 1.02e+08/1.39e+10 =  0% of the original kernel matrix.

torch.Size([26577, 2])
We keep 7.36e+06/2.22e+08 =  3% of the original kernel matrix.

torch.Size([37866, 2])
We keep 5.54e+06/4.60e+08 =  1% of the original kernel matrix.

torch.Size([66772, 2])
We keep 2.91e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([61034, 2])
We keep 1.37e+07/1.38e+09 =  0% of the original kernel matrix.

torch.Size([262876, 2])
We keep 3.65e+08/3.08e+10 =  1% of the original kernel matrix.

torch.Size([121681, 2])
We keep 4.32e+07/5.42e+09 =  0% of the original kernel matrix.

torch.Size([73011, 2])
We keep 2.45e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([63825, 2])
We keep 1.37e+07/1.39e+09 =  0% of the original kernel matrix.

torch.Size([54488, 2])
We keep 1.41e+08/1.96e+09 =  7% of the original kernel matrix.

torch.Size([53115, 2])
We keep 1.38e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([115544, 2])
We keep 1.33e+08/6.13e+09 =  2% of the original kernel matrix.

torch.Size([77302, 2])
We keep 2.20e+07/2.42e+09 =  0% of the original kernel matrix.

torch.Size([51926, 2])
We keep 1.14e+07/9.72e+08 =  1% of the original kernel matrix.

torch.Size([53374, 2])
We keep 1.01e+07/9.62e+08 =  1% of the original kernel matrix.

torch.Size([28513, 2])
We keep 6.24e+06/2.62e+08 =  2% of the original kernel matrix.

torch.Size([39514, 2])
We keep 5.95e+06/5.00e+08 =  1% of the original kernel matrix.

torch.Size([12710, 2])
We keep 1.93e+06/5.13e+07 =  3% of the original kernel matrix.

torch.Size([25935, 2])
We keep 3.21e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([47621, 2])
We keep 2.36e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([51014, 2])
We keep 1.05e+07/9.86e+08 =  1% of the original kernel matrix.

torch.Size([14614, 2])
We keep 2.29e+06/5.87e+07 =  3% of the original kernel matrix.

torch.Size([28195, 2])
We keep 3.38e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([139919, 2])
We keep 1.04e+08/6.51e+09 =  1% of the original kernel matrix.

torch.Size([85366, 2])
We keep 2.22e+07/2.49e+09 =  0% of the original kernel matrix.

torch.Size([1414874, 2])
We keep 4.89e+09/7.47e+11 =  0% of the original kernel matrix.

torch.Size([295452, 2])
We keep 1.82e+08/2.67e+10 =  0% of the original kernel matrix.

torch.Size([133342, 2])
We keep 1.96e+08/8.49e+09 =  2% of the original kernel matrix.

torch.Size([84511, 2])
We keep 2.50e+07/2.84e+09 =  0% of the original kernel matrix.

torch.Size([134255, 2])
We keep 2.24e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([79440, 2])
We keep 3.04e+07/3.55e+09 =  0% of the original kernel matrix.

torch.Size([28827, 2])
We keep 2.23e+07/3.57e+08 =  6% of the original kernel matrix.

torch.Size([39496, 2])
We keep 6.70e+06/5.83e+08 =  1% of the original kernel matrix.

torch.Size([22317, 2])
We keep 1.38e+07/3.43e+08 =  4% of the original kernel matrix.

torch.Size([33474, 2])
We keep 6.77e+06/5.72e+08 =  1% of the original kernel matrix.

torch.Size([43106, 2])
We keep 1.08e+07/7.58e+08 =  1% of the original kernel matrix.

torch.Size([49642, 2])
We keep 9.34e+06/8.50e+08 =  1% of the original kernel matrix.

torch.Size([24765, 2])
We keep 1.71e+07/3.63e+08 =  4% of the original kernel matrix.

torch.Size([35346, 2])
We keep 6.89e+06/5.88e+08 =  1% of the original kernel matrix.

torch.Size([36895, 2])
We keep 1.47e+07/5.47e+08 =  2% of the original kernel matrix.

torch.Size([45188, 2])
We keep 8.15e+06/7.22e+08 =  1% of the original kernel matrix.

torch.Size([356235, 2])
We keep 2.90e+08/4.83e+10 =  0% of the original kernel matrix.

torch.Size([143326, 2])
We keep 5.25e+07/6.79e+09 =  0% of the original kernel matrix.

torch.Size([13038, 2])
We keep 8.20e+06/1.43e+08 =  5% of the original kernel matrix.

torch.Size([25215, 2])
We keep 4.80e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([116024, 2])
We keep 5.46e+08/1.78e+10 =  3% of the original kernel matrix.

torch.Size([72207, 2])
We keep 3.37e+07/4.12e+09 =  0% of the original kernel matrix.

torch.Size([151239, 2])
We keep 3.97e+08/1.84e+10 =  2% of the original kernel matrix.

torch.Size([88576, 2])
We keep 3.56e+07/4.19e+09 =  0% of the original kernel matrix.

torch.Size([34538, 2])
We keep 5.79e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([43465, 2])
We keep 7.00e+06/6.10e+08 =  1% of the original kernel matrix.

torch.Size([6063, 2])
We keep 2.80e+05/5.99e+06 =  4% of the original kernel matrix.

torch.Size([19327, 2])
We keep 1.53e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([62788, 2])
We keep 3.72e+07/1.67e+09 =  2% of the original kernel matrix.

torch.Size([58516, 2])
We keep 1.27e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([122381, 2])
We keep 5.05e+07/5.21e+09 =  0% of the original kernel matrix.

torch.Size([79976, 2])
We keep 2.02e+07/2.23e+09 =  0% of the original kernel matrix.

torch.Size([6801, 2])
We keep 4.07e+05/1.06e+07 =  3% of the original kernel matrix.

torch.Size([19896, 2])
We keep 1.85e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([1004463, 2])
We keep 1.88e+09/3.52e+11 =  0% of the original kernel matrix.

torch.Size([243863, 2])
We keep 1.28e+08/1.83e+10 =  0% of the original kernel matrix.

torch.Size([33143, 2])
We keep 1.76e+07/6.37e+08 =  2% of the original kernel matrix.

torch.Size([42299, 2])
We keep 8.35e+06/7.79e+08 =  1% of the original kernel matrix.

torch.Size([54397, 2])
We keep 3.71e+07/1.35e+09 =  2% of the original kernel matrix.

torch.Size([53965, 2])
We keep 1.16e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([13161, 2])
We keep 9.28e+05/3.37e+07 =  2% of the original kernel matrix.

torch.Size([26929, 2])
We keep 2.72e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([6770, 2])
We keep 2.92e+05/7.72e+06 =  3% of the original kernel matrix.

torch.Size([20250, 2])
We keep 1.67e+06/8.58e+07 =  1% of the original kernel matrix.

torch.Size([5924, 2])
We keep 2.63e+05/6.03e+06 =  4% of the original kernel matrix.

torch.Size([18969, 2])
We keep 1.54e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([3817, 2])
We keep 1.47e+05/2.77e+06 =  5% of the original kernel matrix.

torch.Size([15972, 2])
We keep 1.19e+06/5.14e+07 =  2% of the original kernel matrix.

torch.Size([20458, 2])
We keep 4.43e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([33245, 2])
We keep 4.47e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([25661, 2])
We keep 5.81e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([37103, 2])
We keep 5.35e+06/4.38e+08 =  1% of the original kernel matrix.

torch.Size([4046154, 2])
We keep 5.72e+10/7.36e+12 =  0% of the original kernel matrix.

torch.Size([461449, 2])
We keep 5.31e+08/8.37e+10 =  0% of the original kernel matrix.

torch.Size([14700, 2])
We keep 1.89e+06/5.61e+07 =  3% of the original kernel matrix.

torch.Size([28294, 2])
We keep 3.37e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([35666, 2])
We keep 6.86e+06/4.39e+08 =  1% of the original kernel matrix.

torch.Size([44750, 2])
We keep 7.36e+06/6.47e+08 =  1% of the original kernel matrix.

torch.Size([9716, 2])
We keep 6.75e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([23487, 2])
We keep 2.25e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([17559, 2])
We keep 2.03e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([30411, 2])
We keep 3.73e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([18119, 2])
We keep 2.32e+06/8.63e+07 =  2% of the original kernel matrix.

torch.Size([31185, 2])
We keep 3.99e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([203706, 2])
We keep 4.49e+08/2.93e+10 =  1% of the original kernel matrix.

torch.Size([102045, 2])
We keep 4.21e+07/5.29e+09 =  0% of the original kernel matrix.

torch.Size([19825, 2])
We keep 5.15e+06/1.17e+08 =  4% of the original kernel matrix.

torch.Size([32814, 2])
We keep 4.46e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([1406532, 2])
We keep 3.36e+09/6.94e+11 =  0% of the original kernel matrix.

torch.Size([293569, 2])
We keep 1.75e+08/2.57e+10 =  0% of the original kernel matrix.

torch.Size([11628, 2])
We keep 1.06e+06/2.80e+07 =  3% of the original kernel matrix.

torch.Size([25525, 2])
We keep 2.62e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([189136, 2])
We keep 1.68e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([101601, 2])
We keep 3.10e+07/3.68e+09 =  0% of the original kernel matrix.

torch.Size([49225, 2])
We keep 6.37e+07/1.40e+09 =  4% of the original kernel matrix.

torch.Size([51085, 2])
We keep 1.18e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([189345, 2])
We keep 1.71e+08/1.53e+10 =  1% of the original kernel matrix.

torch.Size([101192, 2])
We keep 3.22e+07/3.82e+09 =  0% of the original kernel matrix.

torch.Size([340340, 2])
We keep 7.00e+08/7.47e+10 =  0% of the original kernel matrix.

torch.Size([137415, 2])
We keep 6.42e+07/8.44e+09 =  0% of the original kernel matrix.

torch.Size([22784, 2])
We keep 2.93e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([34875, 2])
We keep 4.66e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([6760, 2])
We keep 3.34e+05/7.58e+06 =  4% of the original kernel matrix.

torch.Size([20232, 2])
We keep 1.67e+06/8.50e+07 =  1% of the original kernel matrix.

torch.Size([164614, 2])
We keep 8.13e+07/9.97e+09 =  0% of the original kernel matrix.

torch.Size([93620, 2])
We keep 2.67e+07/3.08e+09 =  0% of the original kernel matrix.

torch.Size([24825, 2])
We keep 5.12e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([36859, 2])
We keep 5.35e+06/4.24e+08 =  1% of the original kernel matrix.

torch.Size([42690, 2])
We keep 2.67e+07/9.29e+08 =  2% of the original kernel matrix.

torch.Size([46677, 2])
We keep 1.01e+07/9.41e+08 =  1% of the original kernel matrix.

torch.Size([46502, 2])
We keep 1.29e+07/8.35e+08 =  1% of the original kernel matrix.

torch.Size([51463, 2])
We keep 9.79e+06/8.92e+08 =  1% of the original kernel matrix.

torch.Size([18261, 2])
We keep 1.37e+07/1.91e+08 =  7% of the original kernel matrix.

torch.Size([30735, 2])
We keep 5.26e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([65675, 2])
We keep 4.14e+07/2.07e+09 =  2% of the original kernel matrix.

torch.Size([60212, 2])
We keep 1.41e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([25969, 2])
We keep 1.03e+07/3.20e+08 =  3% of the original kernel matrix.

torch.Size([36142, 2])
We keep 6.50e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([280127, 2])
We keep 5.72e+08/4.12e+10 =  1% of the original kernel matrix.

torch.Size([124513, 2])
We keep 5.00e+07/6.27e+09 =  0% of the original kernel matrix.

torch.Size([373634, 2])
We keep 9.99e+08/8.43e+10 =  1% of the original kernel matrix.

torch.Size([141592, 2])
We keep 6.83e+07/8.96e+09 =  0% of the original kernel matrix.

torch.Size([22635, 2])
We keep 3.23e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([34736, 2])
We keep 4.55e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([13386, 2])
We keep 1.88e+06/4.65e+07 =  4% of the original kernel matrix.

torch.Size([26994, 2])
We keep 3.14e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([84431, 2])
We keep 1.13e+08/4.43e+09 =  2% of the original kernel matrix.

torch.Size([65535, 2])
We keep 1.87e+07/2.05e+09 =  0% of the original kernel matrix.

torch.Size([30781, 2])
We keep 8.76e+06/3.86e+08 =  2% of the original kernel matrix.

torch.Size([40494, 2])
We keep 6.99e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([63519, 2])
We keep 3.97e+07/1.87e+09 =  2% of the original kernel matrix.

torch.Size([58557, 2])
We keep 1.33e+07/1.34e+09 =  0% of the original kernel matrix.

torch.Size([15624, 2])
We keep 2.89e+06/8.12e+07 =  3% of the original kernel matrix.

torch.Size([28743, 2])
We keep 3.89e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([55746, 2])
We keep 4.52e+07/2.17e+09 =  2% of the original kernel matrix.

torch.Size([52645, 2])
We keep 1.40e+07/1.44e+09 =  0% of the original kernel matrix.

torch.Size([64230, 2])
We keep 3.68e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([58283, 2])
We keep 1.38e+07/1.40e+09 =  0% of the original kernel matrix.

torch.Size([82362, 2])
We keep 3.37e+08/8.15e+09 =  4% of the original kernel matrix.

torch.Size([61942, 2])
We keep 2.40e+07/2.79e+09 =  0% of the original kernel matrix.

torch.Size([55542, 2])
We keep 4.28e+07/1.35e+09 =  3% of the original kernel matrix.

torch.Size([54538, 2])
We keep 1.15e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([20980, 2])
We keep 2.32e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([33450, 2])
We keep 4.26e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([15403, 2])
We keep 1.55e+06/5.02e+07 =  3% of the original kernel matrix.

torch.Size([28925, 2])
We keep 3.22e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([24339, 2])
We keep 3.20e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([36158, 2])
We keep 4.95e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([39129, 2])
We keep 2.48e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([44409, 2])
We keep 1.05e+07/9.85e+08 =  1% of the original kernel matrix.

torch.Size([122818, 2])
We keep 1.18e+08/6.67e+09 =  1% of the original kernel matrix.

torch.Size([79013, 2])
We keep 2.25e+07/2.52e+09 =  0% of the original kernel matrix.

torch.Size([236304, 2])
We keep 3.43e+08/2.06e+10 =  1% of the original kernel matrix.

torch.Size([113803, 2])
We keep 3.63e+07/4.43e+09 =  0% of the original kernel matrix.

torch.Size([23240, 2])
We keep 2.85e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([35524, 2])
We keep 4.65e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([36964, 2])
We keep 7.02e+06/4.96e+08 =  1% of the original kernel matrix.

torch.Size([44652, 2])
We keep 7.67e+06/6.88e+08 =  1% of the original kernel matrix.

torch.Size([61927, 2])
We keep 2.39e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([58077, 2])
We keep 1.21e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([249990, 2])
We keep 7.11e+08/2.78e+10 =  2% of the original kernel matrix.

torch.Size([117758, 2])
We keep 4.08e+07/5.15e+09 =  0% of the original kernel matrix.

torch.Size([56422, 2])
We keep 3.13e+07/1.42e+09 =  2% of the original kernel matrix.

torch.Size([55936, 2])
We keep 1.16e+07/1.16e+09 =  0% of the original kernel matrix.

torch.Size([110605, 2])
We keep 4.79e+07/4.21e+09 =  1% of the original kernel matrix.

torch.Size([76580, 2])
We keep 1.84e+07/2.00e+09 =  0% of the original kernel matrix.

torch.Size([13417, 2])
We keep 4.88e+06/5.12e+07 =  9% of the original kernel matrix.

torch.Size([27234, 2])
We keep 3.29e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([37469, 2])
We keep 8.00e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([45710, 2])
We keep 7.48e+06/6.69e+08 =  1% of the original kernel matrix.

torch.Size([4929, 2])
We keep 2.23e+05/4.67e+06 =  4% of the original kernel matrix.

torch.Size([17714, 2])
We keep 1.41e+06/6.67e+07 =  2% of the original kernel matrix.

torch.Size([16262, 2])
We keep 1.69e+06/6.68e+07 =  2% of the original kernel matrix.

torch.Size([29730, 2])
We keep 3.66e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([97992, 2])
We keep 3.12e+07/3.20e+09 =  0% of the original kernel matrix.

torch.Size([72569, 2])
We keep 1.64e+07/1.75e+09 =  0% of the original kernel matrix.

torch.Size([32242, 2])
We keep 1.15e+07/4.74e+08 =  2% of the original kernel matrix.

torch.Size([40963, 2])
We keep 7.60e+06/6.72e+08 =  1% of the original kernel matrix.

torch.Size([65463, 2])
We keep 1.60e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([60684, 2])
We keep 1.22e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([5047, 2])
We keep 1.59e+05/3.96e+06 =  4% of the original kernel matrix.

torch.Size([18184, 2])
We keep 1.31e+06/6.15e+07 =  2% of the original kernel matrix.

torch.Size([15511, 2])
We keep 2.19e+06/5.38e+07 =  4% of the original kernel matrix.

torch.Size([28955, 2])
We keep 3.34e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([86802, 2])
We keep 5.58e+07/3.07e+09 =  1% of the original kernel matrix.

torch.Size([67200, 2])
We keep 1.60e+07/1.71e+09 =  0% of the original kernel matrix.

torch.Size([37140, 2])
We keep 8.66e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([45068, 2])
We keep 7.96e+06/7.05e+08 =  1% of the original kernel matrix.

torch.Size([29870, 2])
We keep 4.12e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([40615, 2])
We keep 5.89e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([6306, 2])
We keep 3.13e+05/7.52e+06 =  4% of the original kernel matrix.

torch.Size([19576, 2])
We keep 1.66e+06/8.46e+07 =  1% of the original kernel matrix.

torch.Size([9713, 2])
We keep 1.19e+06/2.22e+07 =  5% of the original kernel matrix.

torch.Size([23270, 2])
We keep 2.42e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([33840, 2])
We keep 6.85e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([43334, 2])
We keep 6.86e+06/5.90e+08 =  1% of the original kernel matrix.

torch.Size([35375, 2])
We keep 7.41e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([44448, 2])
We keep 7.13e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([103303, 2])
We keep 1.05e+08/4.60e+09 =  2% of the original kernel matrix.

torch.Size([73715, 2])
We keep 1.93e+07/2.09e+09 =  0% of the original kernel matrix.

torch.Size([5104, 2])
We keep 4.03e+05/6.54e+06 =  6% of the original kernel matrix.

torch.Size([17686, 2])
We keep 1.55e+06/7.90e+07 =  1% of the original kernel matrix.

torch.Size([21080, 2])
We keep 2.25e+07/1.92e+08 = 11% of the original kernel matrix.

torch.Size([33100, 2])
We keep 5.05e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([7144, 2])
We keep 7.07e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([20024, 2])
We keep 2.04e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([23302, 2])
We keep 6.88e+06/1.91e+08 =  3% of the original kernel matrix.

torch.Size([35381, 2])
We keep 5.35e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([343212, 2])
We keep 5.08e+08/5.41e+10 =  0% of the original kernel matrix.

torch.Size([141891, 2])
We keep 5.59e+07/7.18e+09 =  0% of the original kernel matrix.

torch.Size([25888, 2])
We keep 1.89e+07/2.93e+08 =  6% of the original kernel matrix.

torch.Size([37284, 2])
We keep 6.38e+06/5.29e+08 =  1% of the original kernel matrix.

torch.Size([46083, 2])
We keep 2.27e+07/9.51e+08 =  2% of the original kernel matrix.

torch.Size([49714, 2])
We keep 1.02e+07/9.52e+08 =  1% of the original kernel matrix.

torch.Size([180216, 2])
We keep 1.33e+08/1.34e+10 =  0% of the original kernel matrix.

torch.Size([98422, 2])
We keep 3.05e+07/3.57e+09 =  0% of the original kernel matrix.

torch.Size([41911, 2])
We keep 1.22e+07/6.17e+08 =  1% of the original kernel matrix.

torch.Size([48801, 2])
We keep 8.47e+06/7.67e+08 =  1% of the original kernel matrix.

torch.Size([123727, 2])
We keep 1.17e+08/6.57e+09 =  1% of the original kernel matrix.

torch.Size([81252, 2])
We keep 2.26e+07/2.50e+09 =  0% of the original kernel matrix.

torch.Size([225015, 2])
We keep 2.02e+08/2.11e+10 =  0% of the original kernel matrix.

torch.Size([111525, 2])
We keep 3.71e+07/4.48e+09 =  0% of the original kernel matrix.

torch.Size([8854, 2])
We keep 6.69e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([22707, 2])
We keep 2.19e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([44058, 2])
We keep 9.54e+06/6.39e+08 =  1% of the original kernel matrix.

torch.Size([49359, 2])
We keep 8.46e+06/7.81e+08 =  1% of the original kernel matrix.

torch.Size([28536, 2])
We keep 5.71e+06/2.19e+08 =  2% of the original kernel matrix.

torch.Size([39527, 2])
We keep 5.58e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([139301, 2])
We keep 1.26e+08/7.43e+09 =  1% of the original kernel matrix.

torch.Size([84864, 2])
We keep 2.35e+07/2.66e+09 =  0% of the original kernel matrix.

torch.Size([182572, 2])
We keep 1.93e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([98549, 2])
We keep 3.28e+07/3.96e+09 =  0% of the original kernel matrix.

torch.Size([28555, 2])
We keep 1.23e+07/3.95e+08 =  3% of the original kernel matrix.

torch.Size([38755, 2])
We keep 7.11e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([77186, 2])
We keep 4.01e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([64588, 2])
We keep 1.37e+07/1.40e+09 =  0% of the original kernel matrix.

torch.Size([7676, 2])
We keep 3.25e+05/9.73e+06 =  3% of the original kernel matrix.

torch.Size([21423, 2])
We keep 1.79e+06/9.63e+07 =  1% of the original kernel matrix.

torch.Size([10467, 2])
We keep 2.49e+06/5.53e+07 =  4% of the original kernel matrix.

torch.Size([23466, 2])
We keep 3.32e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([15148, 2])
We keep 1.23e+07/8.07e+07 = 15% of the original kernel matrix.

torch.Size([28383, 2])
We keep 3.95e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([23800, 2])
We keep 3.62e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([36031, 2])
We keep 4.95e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([280438, 2])
We keep 5.44e+08/3.51e+10 =  1% of the original kernel matrix.

torch.Size([124952, 2])
We keep 4.59e+07/5.78e+09 =  0% of the original kernel matrix.

torch.Size([650705, 2])
We keep 3.66e+09/2.00e+11 =  1% of the original kernel matrix.

torch.Size([193756, 2])
We keep 1.00e+08/1.38e+10 =  0% of the original kernel matrix.

torch.Size([14182, 2])
We keep 3.70e+06/8.48e+07 =  4% of the original kernel matrix.

torch.Size([27179, 2])
We keep 3.93e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([32279, 2])
We keep 5.14e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([42184, 2])
We keep 6.41e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([21504, 2])
We keep 2.98e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([34105, 2])
We keep 4.57e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([163982, 2])
We keep 1.50e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([93904, 2])
We keep 2.88e+07/3.41e+09 =  0% of the original kernel matrix.

torch.Size([19488, 2])
We keep 2.21e+06/9.30e+07 =  2% of the original kernel matrix.

torch.Size([32248, 2])
We keep 4.11e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([13801, 2])
We keep 1.69e+06/4.37e+07 =  3% of the original kernel matrix.

torch.Size([27178, 2])
We keep 3.06e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([53962, 2])
We keep 3.21e+07/1.35e+09 =  2% of the original kernel matrix.

torch.Size([54586, 2])
We keep 1.17e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([163196, 2])
We keep 9.91e+07/9.07e+09 =  1% of the original kernel matrix.

torch.Size([93344, 2])
We keep 2.56e+07/2.94e+09 =  0% of the original kernel matrix.

torch.Size([7186, 2])
We keep 5.05e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([20482, 2])
We keep 1.95e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([67467, 2])
We keep 4.13e+07/1.96e+09 =  2% of the original kernel matrix.

torch.Size([60273, 2])
We keep 1.35e+07/1.37e+09 =  0% of the original kernel matrix.

torch.Size([5985, 2])
We keep 2.58e+05/5.26e+06 =  4% of the original kernel matrix.

torch.Size([19616, 2])
We keep 1.47e+06/7.08e+07 =  2% of the original kernel matrix.

time for making ranges is 4.773179531097412
Sorting X and nu_X
time for sorting X is 0.0833890438079834
Sorting Z and nu_Z
time for sorting Z is 0.000293731689453125
Starting Optim
sum tnu_Z before tensor(32159244., device='cuda:0')
c= tensor(793.6008, device='cuda:0')
c= tensor(104476.5312, device='cuda:0')
c= tensor(107404.7734, device='cuda:0')
c= tensor(126855.6875, device='cuda:0')
c= tensor(377988.6875, device='cuda:0')
c= tensor(416353.6875, device='cuda:0')
c= tensor(1047826.8125, device='cuda:0')
c= tensor(1291909.3750, device='cuda:0')
c= tensor(1306536.5000, device='cuda:0')
c= tensor(2054945.6250, device='cuda:0')
c= tensor(2073634.6250, device='cuda:0')
c= tensor(7229439.5000, device='cuda:0')
c= tensor(7253882.5000, device='cuda:0')
c= tensor(16191748., device='cuda:0')
c= tensor(16308056., device='cuda:0')
c= tensor(16520947., device='cuda:0')
c= tensor(17537694., device='cuda:0')
c= tensor(18062872., device='cuda:0')
c= tensor(31859886., device='cuda:0')
c= tensor(34955560., device='cuda:0')
c= tensor(34967012., device='cuda:0')
c= tensor(50862108., device='cuda:0')
c= tensor(50899736., device='cuda:0')
c= tensor(51369960., device='cuda:0')
c= tensor(51469848., device='cuda:0')
c= tensor(52571132., device='cuda:0')
c= tensor(54013228., device='cuda:0')
c= tensor(54051724., device='cuda:0')
c= tensor(56031244., device='cuda:0')
c= tensor(3.0639e+08, device='cuda:0')
c= tensor(3.0641e+08, device='cuda:0')
c= tensor(3.9380e+08, device='cuda:0')
c= tensor(3.9551e+08, device='cuda:0')
c= tensor(3.9551e+08, device='cuda:0')
c= tensor(3.9552e+08, device='cuda:0')
c= tensor(4.0015e+08, device='cuda:0')
c= tensor(4.0238e+08, device='cuda:0')
c= tensor(4.0238e+08, device='cuda:0')
c= tensor(4.0239e+08, device='cuda:0')
c= tensor(4.0240e+08, device='cuda:0')
c= tensor(4.0240e+08, device='cuda:0')
c= tensor(4.0240e+08, device='cuda:0')
c= tensor(4.0240e+08, device='cuda:0')
c= tensor(4.0241e+08, device='cuda:0')
c= tensor(4.0241e+08, device='cuda:0')
c= tensor(4.0241e+08, device='cuda:0')
c= tensor(4.0244e+08, device='cuda:0')
c= tensor(4.0244e+08, device='cuda:0')
c= tensor(4.0244e+08, device='cuda:0')
c= tensor(4.0249e+08, device='cuda:0')
c= tensor(4.0255e+08, device='cuda:0')
c= tensor(4.0256e+08, device='cuda:0')
c= tensor(4.0257e+08, device='cuda:0')
c= tensor(4.0257e+08, device='cuda:0')
c= tensor(4.0259e+08, device='cuda:0')
c= tensor(4.0261e+08, device='cuda:0')
c= tensor(4.0261e+08, device='cuda:0')
c= tensor(4.0261e+08, device='cuda:0')
c= tensor(4.0261e+08, device='cuda:0')
c= tensor(4.0262e+08, device='cuda:0')
c= tensor(4.0267e+08, device='cuda:0')
c= tensor(4.0267e+08, device='cuda:0')
c= tensor(4.0279e+08, device='cuda:0')
c= tensor(4.0280e+08, device='cuda:0')
c= tensor(4.0280e+08, device='cuda:0')
c= tensor(4.0281e+08, device='cuda:0')
c= tensor(4.0281e+08, device='cuda:0')
c= tensor(4.0281e+08, device='cuda:0')
c= tensor(4.0282e+08, device='cuda:0')
c= tensor(4.0282e+08, device='cuda:0')
c= tensor(4.0282e+08, device='cuda:0')
c= tensor(4.0283e+08, device='cuda:0')
c= tensor(4.0283e+08, device='cuda:0')
c= tensor(4.0284e+08, device='cuda:0')
c= tensor(4.0284e+08, device='cuda:0')
c= tensor(4.0291e+08, device='cuda:0')
c= tensor(4.0292e+08, device='cuda:0')
c= tensor(4.0292e+08, device='cuda:0')
c= tensor(4.0292e+08, device='cuda:0')
c= tensor(4.0293e+08, device='cuda:0')
c= tensor(4.0294e+08, device='cuda:0')
c= tensor(4.0294e+08, device='cuda:0')
c= tensor(4.0295e+08, device='cuda:0')
c= tensor(4.0295e+08, device='cuda:0')
c= tensor(4.0296e+08, device='cuda:0')
c= tensor(4.0296e+08, device='cuda:0')
c= tensor(4.0298e+08, device='cuda:0')
c= tensor(4.0298e+08, device='cuda:0')
c= tensor(4.0299e+08, device='cuda:0')
c= tensor(4.0299e+08, device='cuda:0')
c= tensor(4.0300e+08, device='cuda:0')
c= tensor(4.0300e+08, device='cuda:0')
c= tensor(4.0300e+08, device='cuda:0')
c= tensor(4.0302e+08, device='cuda:0')
c= tensor(4.0303e+08, device='cuda:0')
c= tensor(4.0303e+08, device='cuda:0')
c= tensor(4.0304e+08, device='cuda:0')
c= tensor(4.0312e+08, device='cuda:0')
c= tensor(4.0315e+08, device='cuda:0')
c= tensor(4.0316e+08, device='cuda:0')
c= tensor(4.0326e+08, device='cuda:0')
c= tensor(4.0328e+08, device='cuda:0')
c= tensor(4.0330e+08, device='cuda:0')
c= tensor(4.0330e+08, device='cuda:0')
c= tensor(4.0330e+08, device='cuda:0')
c= tensor(4.0331e+08, device='cuda:0')
c= tensor(4.0332e+08, device='cuda:0')
c= tensor(4.0332e+08, device='cuda:0')
c= tensor(4.0333e+08, device='cuda:0')
c= tensor(4.0333e+08, device='cuda:0')
c= tensor(4.0333e+08, device='cuda:0')
c= tensor(4.0334e+08, device='cuda:0')
c= tensor(4.0334e+08, device='cuda:0')
c= tensor(4.0334e+08, device='cuda:0')
c= tensor(4.0335e+08, device='cuda:0')
c= tensor(4.0335e+08, device='cuda:0')
c= tensor(4.0335e+08, device='cuda:0')
c= tensor(4.0336e+08, device='cuda:0')
c= tensor(4.0338e+08, device='cuda:0')
c= tensor(4.0339e+08, device='cuda:0')
c= tensor(4.0341e+08, device='cuda:0')
c= tensor(4.0342e+08, device='cuda:0')
c= tensor(4.0342e+08, device='cuda:0')
c= tensor(4.0342e+08, device='cuda:0')
c= tensor(4.0343e+08, device='cuda:0')
c= tensor(4.0343e+08, device='cuda:0')
c= tensor(4.0343e+08, device='cuda:0')
c= tensor(4.0344e+08, device='cuda:0')
c= tensor(4.0347e+08, device='cuda:0')
c= tensor(4.0348e+08, device='cuda:0')
c= tensor(4.0349e+08, device='cuda:0')
c= tensor(4.0349e+08, device='cuda:0')
c= tensor(4.0351e+08, device='cuda:0')
c= tensor(4.0352e+08, device='cuda:0')
c= tensor(4.0392e+08, device='cuda:0')
c= tensor(4.0393e+08, device='cuda:0')
c= tensor(4.0393e+08, device='cuda:0')
c= tensor(4.0393e+08, device='cuda:0')
c= tensor(4.0393e+08, device='cuda:0')
c= tensor(4.0394e+08, device='cuda:0')
c= tensor(4.0394e+08, device='cuda:0')
c= tensor(4.0394e+08, device='cuda:0')
c= tensor(4.0397e+08, device='cuda:0')
c= tensor(4.0430e+08, device='cuda:0')
c= tensor(4.0431e+08, device='cuda:0')
c= tensor(4.0431e+08, device='cuda:0')
c= tensor(4.0432e+08, device='cuda:0')
c= tensor(4.0432e+08, device='cuda:0')
c= tensor(4.0432e+08, device='cuda:0')
c= tensor(4.0433e+08, device='cuda:0')
c= tensor(4.0433e+08, device='cuda:0')
c= tensor(4.0435e+08, device='cuda:0')
c= tensor(4.0436e+08, device='cuda:0')
c= tensor(4.0446e+08, device='cuda:0')
c= tensor(4.0447e+08, device='cuda:0')
c= tensor(4.0457e+08, device='cuda:0')
c= tensor(4.0457e+08, device='cuda:0')
c= tensor(4.0457e+08, device='cuda:0')
c= tensor(4.0459e+08, device='cuda:0')
c= tensor(4.0459e+08, device='cuda:0')
c= tensor(4.0462e+08, device='cuda:0')
c= tensor(4.0462e+08, device='cuda:0')
c= tensor(4.0464e+08, device='cuda:0')
c= tensor(4.0464e+08, device='cuda:0')
c= tensor(4.0464e+08, device='cuda:0')
c= tensor(4.0464e+08, device='cuda:0')
c= tensor(4.0465e+08, device='cuda:0')
c= tensor(4.0465e+08, device='cuda:0')
c= tensor(4.0466e+08, device='cuda:0')
c= tensor(4.0466e+08, device='cuda:0')
c= tensor(4.0466e+08, device='cuda:0')
c= tensor(4.0466e+08, device='cuda:0')
c= tensor(4.0467e+08, device='cuda:0')
c= tensor(4.0467e+08, device='cuda:0')
c= tensor(4.0469e+08, device='cuda:0')
c= tensor(4.0491e+08, device='cuda:0')
c= tensor(4.0492e+08, device='cuda:0')
c= tensor(4.0493e+08, device='cuda:0')
c= tensor(4.0496e+08, device='cuda:0')
c= tensor(4.0497e+08, device='cuda:0')
c= tensor(4.0497e+08, device='cuda:0')
c= tensor(4.0498e+08, device='cuda:0')
c= tensor(4.0499e+08, device='cuda:0')
c= tensor(4.0499e+08, device='cuda:0')
c= tensor(4.0500e+08, device='cuda:0')
c= tensor(4.0503e+08, device='cuda:0')
c= tensor(4.0503e+08, device='cuda:0')
c= tensor(4.0505e+08, device='cuda:0')
c= tensor(4.0506e+08, device='cuda:0')
c= tensor(4.0546e+08, device='cuda:0')
c= tensor(4.0546e+08, device='cuda:0')
c= tensor(4.0546e+08, device='cuda:0')
c= tensor(4.0546e+08, device='cuda:0')
c= tensor(4.0547e+08, device='cuda:0')
c= tensor(4.0547e+08, device='cuda:0')
c= tensor(4.0548e+08, device='cuda:0')
c= tensor(4.0548e+08, device='cuda:0')
c= tensor(4.0548e+08, device='cuda:0')
c= tensor(4.0549e+08, device='cuda:0')
c= tensor(4.0549e+08, device='cuda:0')
c= tensor(4.0561e+08, device='cuda:0')
c= tensor(4.0562e+08, device='cuda:0')
c= tensor(4.0571e+08, device='cuda:0')
c= tensor(4.0573e+08, device='cuda:0')
c= tensor(4.0573e+08, device='cuda:0')
c= tensor(4.0573e+08, device='cuda:0')
c= tensor(4.0574e+08, device='cuda:0')
c= tensor(4.0575e+08, device='cuda:0')
c= tensor(4.0576e+08, device='cuda:0')
c= tensor(4.0580e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0583e+08, device='cuda:0')
c= tensor(4.0583e+08, device='cuda:0')
c= tensor(4.0583e+08, device='cuda:0')
c= tensor(4.0584e+08, device='cuda:0')
c= tensor(4.0586e+08, device='cuda:0')
c= tensor(4.0586e+08, device='cuda:0')
c= tensor(4.0587e+08, device='cuda:0')
c= tensor(4.0587e+08, device='cuda:0')
c= tensor(4.0589e+08, device='cuda:0')
c= tensor(4.0589e+08, device='cuda:0')
c= tensor(4.0589e+08, device='cuda:0')
c= tensor(4.0590e+08, device='cuda:0')
c= tensor(4.0591e+08, device='cuda:0')
c= tensor(4.0591e+08, device='cuda:0')
c= tensor(4.0591e+08, device='cuda:0')
c= tensor(4.0591e+08, device='cuda:0')
c= tensor(4.0592e+08, device='cuda:0')
c= tensor(4.0594e+08, device='cuda:0')
c= tensor(4.0594e+08, device='cuda:0')
c= tensor(4.0594e+08, device='cuda:0')
c= tensor(4.0654e+08, device='cuda:0')
c= tensor(4.0656e+08, device='cuda:0')
c= tensor(4.0657e+08, device='cuda:0')
c= tensor(4.0660e+08, device='cuda:0')
c= tensor(4.0841e+08, device='cuda:0')
c= tensor(4.0842e+08, device='cuda:0')
c= tensor(4.0844e+08, device='cuda:0')
c= tensor(4.0844e+08, device='cuda:0')
c= tensor(4.0845e+08, device='cuda:0')
c= tensor(4.1606e+08, device='cuda:0')
c= tensor(4.4261e+08, device='cuda:0')
c= tensor(4.4261e+08, device='cuda:0')
c= tensor(4.4297e+08, device='cuda:0')
c= tensor(4.4354e+08, device='cuda:0')
c= tensor(4.4357e+08, device='cuda:0')
c= tensor(4.5423e+08, device='cuda:0')
c= tensor(4.5423e+08, device='cuda:0')
c= tensor(4.5428e+08, device='cuda:0')
c= tensor(4.5792e+08, device='cuda:0')
c= tensor(4.6184e+08, device='cuda:0')
c= tensor(4.6184e+08, device='cuda:0')
c= tensor(4.6197e+08, device='cuda:0')
c= tensor(4.6257e+08, device='cuda:0')
c= tensor(4.6532e+08, device='cuda:0')
c= tensor(4.6582e+08, device='cuda:0')
c= tensor(4.6716e+08, device='cuda:0')
c= tensor(4.6801e+08, device='cuda:0')
c= tensor(4.6822e+08, device='cuda:0')
c= tensor(4.6822e+08, device='cuda:0')
c= tensor(4.9245e+08, device='cuda:0')
c= tensor(4.9245e+08, device='cuda:0')
c= tensor(4.9246e+08, device='cuda:0')
c= tensor(4.9267e+08, device='cuda:0')
c= tensor(4.9296e+08, device='cuda:0')
c= tensor(5.2068e+08, device='cuda:0')
c= tensor(5.2152e+08, device='cuda:0')
c= tensor(5.2153e+08, device='cuda:0')
c= tensor(5.2159e+08, device='cuda:0')
c= tensor(5.2183e+08, device='cuda:0')
c= tensor(5.2193e+08, device='cuda:0')
c= tensor(5.3441e+08, device='cuda:0')
c= tensor(5.3442e+08, device='cuda:0')
c= tensor(5.3452e+08, device='cuda:0')
c= tensor(5.3452e+08, device='cuda:0')
c= tensor(5.3453e+08, device='cuda:0')
c= tensor(5.3516e+08, device='cuda:0')
c= tensor(5.3663e+08, device='cuda:0')
c= tensor(5.3679e+08, device='cuda:0')
c= tensor(5.3687e+08, device='cuda:0')
c= tensor(5.6531e+08, device='cuda:0')
c= tensor(5.6534e+08, device='cuda:0')
c= tensor(5.6549e+08, device='cuda:0')
c= tensor(5.6766e+08, device='cuda:0')
c= tensor(5.6767e+08, device='cuda:0')
c= tensor(5.6902e+08, device='cuda:0')
c= tensor(5.9077e+08, device='cuda:0')
c= tensor(6.1361e+08, device='cuda:0')
c= tensor(6.1365e+08, device='cuda:0')
c= tensor(6.1366e+08, device='cuda:0')
c= tensor(6.1367e+08, device='cuda:0')
c= tensor(6.1367e+08, device='cuda:0')
c= tensor(6.1403e+08, device='cuda:0')
c= tensor(6.1406e+08, device='cuda:0')
c= tensor(6.1443e+08, device='cuda:0')
c= tensor(6.2077e+08, device='cuda:0')
c= tensor(6.2199e+08, device='cuda:0')
c= tensor(6.2204e+08, device='cuda:0')
c= tensor(6.2206e+08, device='cuda:0')
c= tensor(6.2936e+08, device='cuda:0')
c= tensor(6.3933e+08, device='cuda:0')
c= tensor(6.3954e+08, device='cuda:0')
c= tensor(6.4005e+08, device='cuda:0')
c= tensor(7.6657e+08, device='cuda:0')
c= tensor(7.6658e+08, device='cuda:0')
c= tensor(7.7135e+08, device='cuda:0')
c= tensor(7.7137e+08, device='cuda:0')
c= tensor(7.7202e+08, device='cuda:0')
c= tensor(7.7222e+08, device='cuda:0')
c= tensor(8.3244e+08, device='cuda:0')
c= tensor(8.3365e+08, device='cuda:0')
c= tensor(8.3366e+08, device='cuda:0')
c= tensor(8.3613e+08, device='cuda:0')
c= tensor(8.3886e+08, device='cuda:0')
c= tensor(8.3889e+08, device='cuda:0')
c= tensor(8.3952e+08, device='cuda:0')
c= tensor(8.4691e+08, device='cuda:0')
c= tensor(8.6354e+08, device='cuda:0')
c= tensor(8.6432e+08, device='cuda:0')
c= tensor(8.6432e+08, device='cuda:0')
c= tensor(8.6434e+08, device='cuda:0')
c= tensor(8.6436e+08, device='cuda:0')
c= tensor(8.6455e+08, device='cuda:0')
c= tensor(8.6457e+08, device='cuda:0')
c= tensor(8.6458e+08, device='cuda:0')
c= tensor(8.6495e+08, device='cuda:0')
c= tensor(8.6812e+08, device='cuda:0')
c= tensor(8.6825e+08, device='cuda:0')
c= tensor(8.6826e+08, device='cuda:0')
c= tensor(8.6829e+08, device='cuda:0')
c= tensor(8.6834e+08, device='cuda:0')
c= tensor(8.6836e+08, device='cuda:0')
c= tensor(8.6848e+08, device='cuda:0')
c= tensor(8.6848e+08, device='cuda:0')
c= tensor(8.7209e+08, device='cuda:0')
c= tensor(8.7223e+08, device='cuda:0')
c= tensor(8.7235e+08, device='cuda:0')
c= tensor(8.7289e+08, device='cuda:0')
c= tensor(8.7294e+08, device='cuda:0')
c= tensor(1.3112e+09, device='cuda:0')
c= tensor(1.3112e+09, device='cuda:0')
c= tensor(1.3119e+09, device='cuda:0')
c= tensor(1.3119e+09, device='cuda:0')
c= tensor(1.3119e+09, device='cuda:0')
c= tensor(1.3119e+09, device='cuda:0')
c= tensor(1.3120e+09, device='cuda:0')
c= tensor(1.3120e+09, device='cuda:0')
c= tensor(1.3140e+09, device='cuda:0')
c= tensor(1.3141e+09, device='cuda:0')
c= tensor(1.3141e+09, device='cuda:0')
c= tensor(1.3355e+09, device='cuda:0')
c= tensor(1.3359e+09, device='cuda:0')
c= tensor(1.3365e+09, device='cuda:0')
c= tensor(1.3404e+09, device='cuda:0')
c= tensor(1.3491e+09, device='cuda:0')
c= tensor(1.3491e+09, device='cuda:0')
c= tensor(1.3491e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3802e+09, device='cuda:0')
c= tensor(1.3804e+09, device='cuda:0')
c= tensor(1.3815e+09, device='cuda:0')
c= tensor(1.3816e+09, device='cuda:0')
c= tensor(1.3816e+09, device='cuda:0')
c= tensor(1.3817e+09, device='cuda:0')
c= tensor(1.5849e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6189e+09, device='cuda:0')
c= tensor(1.6189e+09, device='cuda:0')
c= tensor(1.6204e+09, device='cuda:0')
c= tensor(1.6212e+09, device='cuda:0')
c= tensor(1.6216e+09, device='cuda:0')
c= tensor(1.6217e+09, device='cuda:0')
c= tensor(1.6231e+09, device='cuda:0')
c= tensor(1.6906e+09, device='cuda:0')
c= tensor(1.6907e+09, device='cuda:0')
c= tensor(1.6907e+09, device='cuda:0')
c= tensor(1.6910e+09, device='cuda:0')
c= tensor(1.6912e+09, device='cuda:0')
c= tensor(1.6912e+09, device='cuda:0')
c= tensor(1.6939e+09, device='cuda:0')
c= tensor(1.6941e+09, device='cuda:0')
c= tensor(1.6941e+09, device='cuda:0')
c= tensor(1.6946e+09, device='cuda:0')
c= tensor(1.6949e+09, device='cuda:0')
c= tensor(1.6949e+09, device='cuda:0')
c= tensor(1.6984e+09, device='cuda:0')
c= tensor(1.7013e+09, device='cuda:0')
c= tensor(1.7083e+09, device='cuda:0')
c= tensor(1.7212e+09, device='cuda:0')
c= tensor(1.7271e+09, device='cuda:0')
c= tensor(1.7271e+09, device='cuda:0')
c= tensor(1.7276e+09, device='cuda:0')
c= tensor(1.7280e+09, device='cuda:0')
c= tensor(1.7311e+09, device='cuda:0')
c= tensor(1.7311e+09, device='cuda:0')
c= tensor(1.7402e+09, device='cuda:0')
c= tensor(1.9204e+09, device='cuda:0')
c= tensor(1.9246e+09, device='cuda:0')
c= tensor(1.9250e+09, device='cuda:0')
c= tensor(1.9278e+09, device='cuda:0')
c= tensor(1.9278e+09, device='cuda:0')
c= tensor(1.9278e+09, device='cuda:0')
c= tensor(1.9279e+09, device='cuda:0')
c= tensor(1.9292e+09, device='cuda:0')
c= tensor(1.9300e+09, device='cuda:0')
c= tensor(1.9835e+09, device='cuda:0')
c= tensor(1.9966e+09, device='cuda:0')
c= tensor(1.9974e+09, device='cuda:0')
c= tensor(1.9976e+09, device='cuda:0')
c= tensor(1.9990e+09, device='cuda:0')
c= tensor(1.9990e+09, device='cuda:0')
c= tensor(1.9991e+09, device='cuda:0')
c= tensor(2.0080e+09, device='cuda:0')
c= tensor(2.0080e+09, device='cuda:0')
c= tensor(2.0080e+09, device='cuda:0')
c= tensor(2.0081e+09, device='cuda:0')
c= tensor(2.0156e+09, device='cuda:0')
c= tensor(2.0157e+09, device='cuda:0')
c= tensor(2.0179e+09, device='cuda:0')
c= tensor(2.0180e+09, device='cuda:0')
c= tensor(2.0181e+09, device='cuda:0')
c= tensor(2.0181e+09, device='cuda:0')
c= tensor(2.0181e+09, device='cuda:0')
c= tensor(2.0187e+09, device='cuda:0')
c= tensor(2.0190e+09, device='cuda:0')
c= tensor(2.0191e+09, device='cuda:0')
c= tensor(2.0209e+09, device='cuda:0')
c= tensor(2.0209e+09, device='cuda:0')
c= tensor(2.0226e+09, device='cuda:0')
c= tensor(2.0227e+09, device='cuda:0')
c= tensor(2.0231e+09, device='cuda:0')
c= tensor(2.0231e+09, device='cuda:0')
c= tensor(2.0247e+09, device='cuda:0')
c= tensor(2.0248e+09, device='cuda:0')
c= tensor(2.0252e+09, device='cuda:0')
c= tensor(2.0261e+09, device='cuda:0')
c= tensor(2.0351e+09, device='cuda:0')
c= tensor(2.0352e+09, device='cuda:0')
c= tensor(2.0352e+09, device='cuda:0')
c= tensor(2.0454e+09, device='cuda:0')
c= tensor(2.0454e+09, device='cuda:0')
c= tensor(2.0905e+09, device='cuda:0')
c= tensor(2.0905e+09, device='cuda:0')
c= tensor(2.0911e+09, device='cuda:0')
c= tensor(2.0930e+09, device='cuda:0')
c= tensor(2.0930e+09, device='cuda:0')
c= tensor(2.0946e+09, device='cuda:0')
c= tensor(2.0952e+09, device='cuda:0')
c= tensor(2.1629e+09, device='cuda:0')
c= tensor(2.1629e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1631e+09, device='cuda:0')
c= tensor(2.1631e+09, device='cuda:0')
c= tensor(2.1632e+09, device='cuda:0')
c= tensor(2.1635e+09, device='cuda:0')
c= tensor(2.1636e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.2031e+09, device='cuda:0')
c= tensor(2.2043e+09, device='cuda:0')
c= tensor(2.2129e+09, device='cuda:0')
c= tensor(2.2132e+09, device='cuda:0')
c= tensor(2.2133e+09, device='cuda:0')
c= tensor(2.2133e+09, device='cuda:0')
c= tensor(2.2133e+09, device='cuda:0')
c= tensor(2.2454e+09, device='cuda:0')
c= tensor(2.2454e+09, device='cuda:0')
c= tensor(2.2455e+09, device='cuda:0')
c= tensor(2.2463e+09, device='cuda:0')
c= tensor(2.2482e+09, device='cuda:0')
c= tensor(2.2482e+09, device='cuda:0')
c= tensor(2.2482e+09, device='cuda:0')
c= tensor(2.2664e+09, device='cuda:0')
c= tensor(2.2664e+09, device='cuda:0')
c= tensor(2.2674e+09, device='cuda:0')
c= tensor(2.2676e+09, device='cuda:0')
c= tensor(2.2757e+09, device='cuda:0')
c= tensor(2.2862e+09, device='cuda:0')
c= tensor(2.2928e+09, device='cuda:0')
c= tensor(2.2972e+09, device='cuda:0')
c= tensor(2.2972e+09, device='cuda:0')
c= tensor(2.2977e+09, device='cuda:0')
c= tensor(2.2979e+09, device='cuda:0')
c= tensor(2.2979e+09, device='cuda:0')
c= tensor(2.2980e+09, device='cuda:0')
c= tensor(2.2980e+09, device='cuda:0')
c= tensor(2.2984e+09, device='cuda:0')
c= tensor(2.2984e+09, device='cuda:0')
c= tensor(2.2984e+09, device='cuda:0')
c= tensor(2.2985e+09, device='cuda:0')
c= tensor(2.2985e+09, device='cuda:0')
c= tensor(2.3012e+09, device='cuda:0')
c= tensor(2.3012e+09, device='cuda:0')
c= tensor(2.3014e+09, device='cuda:0')
c= tensor(2.3018e+09, device='cuda:0')
c= tensor(2.3027e+09, device='cuda:0')
c= tensor(2.3027e+09, device='cuda:0')
c= tensor(2.3028e+09, device='cuda:0')
c= tensor(2.3028e+09, device='cuda:0')
c= tensor(2.3087e+09, device='cuda:0')
c= tensor(2.3087e+09, device='cuda:0')
c= tensor(2.3087e+09, device='cuda:0')
c= tensor(2.3087e+09, device='cuda:0')
c= tensor(2.3355e+09, device='cuda:0')
c= tensor(2.3700e+09, device='cuda:0')
c= tensor(2.3700e+09, device='cuda:0')
c= tensor(2.3700e+09, device='cuda:0')
c= tensor(2.3703e+09, device='cuda:0')
c= tensor(2.3725e+09, device='cuda:0')
c= tensor(2.3725e+09, device='cuda:0')
c= tensor(2.3726e+09, device='cuda:0')
c= tensor(2.3726e+09, device='cuda:0')
c= tensor(2.3743e+09, device='cuda:0')
c= tensor(2.3745e+09, device='cuda:0')
c= tensor(2.3777e+09, device='cuda:0')
c= tensor(2.3777e+09, device='cuda:0')
c= tensor(2.3780e+09, device='cuda:0')
c= tensor(2.3780e+09, device='cuda:0')
c= tensor(2.3782e+09, device='cuda:0')
c= tensor(2.3782e+09, device='cuda:0')
c= tensor(2.3790e+09, device='cuda:0')
c= tensor(2.3852e+09, device='cuda:0')
c= tensor(2.4027e+09, device='cuda:0')
c= tensor(2.4027e+09, device='cuda:0')
c= tensor(2.4027e+09, device='cuda:0')
c= tensor(2.4027e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.4869e+09, device='cuda:0')
c= tensor(2.4869e+09, device='cuda:0')
c= tensor(2.4869e+09, device='cuda:0')
c= tensor(2.4873e+09, device='cuda:0')
c= tensor(2.4873e+09, device='cuda:0')
c= tensor(2.4882e+09, device='cuda:0')
c= tensor(2.4882e+09, device='cuda:0')
c= tensor(2.4883e+09, device='cuda:0')
c= tensor(2.4883e+09, device='cuda:0')
c= tensor(2.4883e+09, device='cuda:0')
c= tensor(2.4883e+09, device='cuda:0')
c= tensor(2.4914e+09, device='cuda:0')
c= tensor(2.5089e+09, device='cuda:0')
c= tensor(2.5165e+09, device='cuda:0')
c= tensor(2.5170e+09, device='cuda:0')
c= tensor(2.5172e+09, device='cuda:0')
c= tensor(2.5173e+09, device='cuda:0')
c= tensor(2.5173e+09, device='cuda:0')
c= tensor(2.5187e+09, device='cuda:0')
c= tensor(2.5195e+09, device='cuda:0')
c= tensor(2.5231e+09, device='cuda:0')
c= tensor(2.5232e+09, device='cuda:0')
c= tensor(2.6060e+09, device='cuda:0')
c= tensor(2.6061e+09, device='cuda:0')
c= tensor(2.6067e+09, device='cuda:0')
c= tensor(2.6208e+09, device='cuda:0')
c= tensor(2.6213e+09, device='cuda:0')
c= tensor(2.6236e+09, device='cuda:0')
c= tensor(2.6261e+09, device='cuda:0')
c= tensor(2.6263e+09, device='cuda:0')
c= tensor(2.6264e+09, device='cuda:0')
c= tensor(2.6265e+09, device='cuda:0')
c= tensor(2.6269e+09, device='cuda:0')
c= tensor(2.6269e+09, device='cuda:0')
c= tensor(2.6289e+09, device='cuda:0')
c= tensor(2.7869e+09, device='cuda:0')
c= tensor(2.7927e+09, device='cuda:0')
c= tensor(2.7985e+09, device='cuda:0')
c= tensor(2.7989e+09, device='cuda:0')
c= tensor(2.7991e+09, device='cuda:0')
c= tensor(2.7993e+09, device='cuda:0')
c= tensor(2.7997e+09, device='cuda:0')
c= tensor(2.7999e+09, device='cuda:0')
c= tensor(2.8083e+09, device='cuda:0')
c= tensor(2.8083e+09, device='cuda:0')
c= tensor(2.8265e+09, device='cuda:0')
c= tensor(2.8384e+09, device='cuda:0')
c= tensor(2.8386e+09, device='cuda:0')
c= tensor(2.8386e+09, device='cuda:0')
c= tensor(2.8395e+09, device='cuda:0')
c= tensor(2.8405e+09, device='cuda:0')
c= tensor(2.8405e+09, device='cuda:0')
c= tensor(2.8924e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8938e+09, device='cuda:0')
c= tensor(2.8939e+09, device='cuda:0')
c= tensor(2.8939e+09, device='cuda:0')
c= tensor(2.8939e+09, device='cuda:0')
c= tensor(2.8939e+09, device='cuda:0')
c= tensor(2.8940e+09, device='cuda:0')
c= tensor(2.8940e+09, device='cuda:0')
c= tensor(5.3734e+09, device='cuda:0')
c= tensor(5.3734e+09, device='cuda:0')
c= tensor(5.3735e+09, device='cuda:0')
c= tensor(5.3736e+09, device='cuda:0')
c= tensor(5.3736e+09, device='cuda:0')
c= tensor(5.3736e+09, device='cuda:0')
c= tensor(5.3878e+09, device='cuda:0')
c= tensor(5.3879e+09, device='cuda:0')
c= tensor(5.5156e+09, device='cuda:0')
c= tensor(5.5156e+09, device='cuda:0')
c= tensor(5.5202e+09, device='cuda:0')
c= tensor(5.5219e+09, device='cuda:0')
c= tensor(5.5274e+09, device='cuda:0')
c= tensor(5.5524e+09, device='cuda:0')
c= tensor(5.5524e+09, device='cuda:0')
c= tensor(5.5524e+09, device='cuda:0')
c= tensor(5.5541e+09, device='cuda:0')
c= tensor(5.5542e+09, device='cuda:0')
c= tensor(5.5547e+09, device='cuda:0')
c= tensor(5.5549e+09, device='cuda:0')
c= tensor(5.5554e+09, device='cuda:0')
c= tensor(5.5562e+09, device='cuda:0')
c= tensor(5.5564e+09, device='cuda:0')
c= tensor(5.5760e+09, device='cuda:0')
c= tensor(5.6027e+09, device='cuda:0')
c= tensor(5.6028e+09, device='cuda:0')
c= tensor(5.6028e+09, device='cuda:0')
c= tensor(5.6064e+09, device='cuda:0')
c= tensor(5.6066e+09, device='cuda:0')
c= tensor(5.6073e+09, device='cuda:0')
c= tensor(5.6074e+09, device='cuda:0')
c= tensor(5.6088e+09, device='cuda:0')
c= tensor(5.6095e+09, device='cuda:0')
c= tensor(5.6184e+09, device='cuda:0')
c= tensor(5.6195e+09, device='cuda:0')
c= tensor(5.6195e+09, device='cuda:0')
c= tensor(5.6195e+09, device='cuda:0')
c= tensor(5.6196e+09, device='cuda:0')
c= tensor(5.6202e+09, device='cuda:0')
c= tensor(5.6231e+09, device='cuda:0')
c= tensor(5.6334e+09, device='cuda:0')
c= tensor(5.6334e+09, device='cuda:0')
c= tensor(5.6336e+09, device='cuda:0')
c= tensor(5.6341e+09, device='cuda:0')
c= tensor(5.6506e+09, device='cuda:0')
c= tensor(5.6512e+09, device='cuda:0')
c= tensor(5.6522e+09, device='cuda:0')
c= tensor(5.6526e+09, device='cuda:0')
c= tensor(5.6528e+09, device='cuda:0')
c= tensor(5.6528e+09, device='cuda:0')
c= tensor(5.6529e+09, device='cuda:0')
c= tensor(5.6535e+09, device='cuda:0')
c= tensor(5.6538e+09, device='cuda:0')
c= tensor(5.6541e+09, device='cuda:0')
c= tensor(5.6541e+09, device='cuda:0')
c= tensor(5.6542e+09, device='cuda:0')
c= tensor(5.6552e+09, device='cuda:0')
c= tensor(5.6554e+09, device='cuda:0')
c= tensor(5.6555e+09, device='cuda:0')
c= tensor(5.6555e+09, device='cuda:0')
c= tensor(5.6555e+09, device='cuda:0')
c= tensor(5.6556e+09, device='cuda:0')
c= tensor(5.6557e+09, device='cuda:0')
c= tensor(5.6583e+09, device='cuda:0')
c= tensor(5.6583e+09, device='cuda:0')
c= tensor(5.6589e+09, device='cuda:0')
c= tensor(5.6589e+09, device='cuda:0')
c= tensor(5.6590e+09, device='cuda:0')
c= tensor(5.6748e+09, device='cuda:0')
c= tensor(5.6753e+09, device='cuda:0')
c= tensor(5.6757e+09, device='cuda:0')
c= tensor(5.6785e+09, device='cuda:0')
c= tensor(5.6787e+09, device='cuda:0')
c= tensor(5.6808e+09, device='cuda:0')
c= tensor(5.6854e+09, device='cuda:0')
c= tensor(5.6854e+09, device='cuda:0')
c= tensor(5.6856e+09, device='cuda:0')
c= tensor(5.6857e+09, device='cuda:0')
c= tensor(5.6878e+09, device='cuda:0')
c= tensor(5.6932e+09, device='cuda:0')
c= tensor(5.6934e+09, device='cuda:0')
c= tensor(5.6943e+09, device='cuda:0')
c= tensor(5.6943e+09, device='cuda:0')
c= tensor(5.6945e+09, device='cuda:0')
c= tensor(5.6947e+09, device='cuda:0')
c= tensor(5.6947e+09, device='cuda:0')
c= tensor(5.7095e+09, device='cuda:0')
c= tensor(5.8398e+09, device='cuda:0')
c= tensor(5.8399e+09, device='cuda:0')
c= tensor(5.8400e+09, device='cuda:0')
c= tensor(5.8401e+09, device='cuda:0')
c= tensor(5.8439e+09, device='cuda:0')
c= tensor(5.8440e+09, device='cuda:0')
c= tensor(5.8440e+09, device='cuda:0')
c= tensor(5.8445e+09, device='cuda:0')
c= tensor(5.8464e+09, device='cuda:0')
c= tensor(5.8464e+09, device='cuda:0')
c= tensor(5.8472e+09, device='cuda:0')
c= tensor(5.8472e+09, device='cuda:0')
memory (bytes)
4736204800
time for making loss 2 is 13.373883962631226
p0 True
it  0 : 2138701312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 92% |
shape of L is 
torch.Size([])
memory (bytes)
4736466944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4737040384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  38786560000.0
relative error loss 6.633329
shape of L is 
torch.Size([])
memory (bytes)
4919545856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4919738368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  38786425000.0
relative error loss 6.633306
shape of L is 
torch.Size([])
memory (bytes)
4925833216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4925906944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  38785210000.0
relative error loss 6.633098
shape of L is 
torch.Size([])
memory (bytes)
4927922176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4927922176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  38773047000.0
relative error loss 6.631018
shape of L is 
torch.Size([])
memory (bytes)
4930056192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4930105344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  38712240000.0
relative error loss 6.6206183
shape of L is 
torch.Size([])
memory (bytes)
4932182016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4932231168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  38379057000.0
relative error loss 6.5636373
shape of L is 
torch.Size([])
memory (bytes)
4934348800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4934348800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  34887336000.0
relative error loss 5.966479
shape of L is 
torch.Size([])
memory (bytes)
4936486912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4936486912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  21577392000.0
relative error loss 3.6901944
shape of L is 
torch.Size([])
memory (bytes)
4938575872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4938625024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  10029111000.0
relative error loss 1.715192
shape of L is 
torch.Size([])
memory (bytes)
4940771328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
4940771328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  7185186000.0
relative error loss 1.2288201
time to take a step is 209.6563515663147
it  1 : 2745507328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4942712832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
4942712832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  7185186000.0
relative error loss 1.2288201
shape of L is 
torch.Size([])
memory (bytes)
4945035264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4945035264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  5436189700.0
relative error loss 0.9297044
shape of L is 
torch.Size([])
memory (bytes)
4947173376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4947173376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  538030740000.0
relative error loss 92.01473
shape of L is 
torch.Size([])
memory (bytes)
4949098496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
4949340160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  9041503000.0
relative error loss 1.5462899
shape of L is 
torch.Size([])
memory (bytes)
4951318528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
4951318528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4896454000.0
relative error loss 0.8373981
shape of L is 
torch.Size([])
memory (bytes)
4953616384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
4953624576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4524540000.0
relative error loss 0.77379286
shape of L is 
torch.Size([])
memory (bytes)
4955701248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
4955701248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4202779600.0
relative error loss 0.71876496
shape of L is 
torch.Size([])
memory (bytes)
4957847552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
4957859840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4020572700.0
relative error loss 0.6876037
shape of L is 
torch.Size([])
memory (bytes)
4959731712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
4959731712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3851292200.0
relative error loss 0.65865314
shape of L is 
torch.Size([])
memory (bytes)
4961685504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
4961685504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  3546118100.0
relative error loss 0.6064618
time to take a step is 120.20124983787537
it  2 : 2918880768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4964073472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
4964110336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  3546118100.0
relative error loss 0.6064618
shape of L is 
torch.Size([])
memory (bytes)
4966236160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
4966236160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3263580000.0
relative error loss 0.55814177
shape of L is 
torch.Size([])
memory (bytes)
4968292352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
4968292352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3437856000.0
relative error loss 0.5879467
shape of L is 
torch.Size([])
memory (bytes)
4970287104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4970553344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  2972446200.0
relative error loss 0.5083517
shape of L is 
torch.Size([])
memory (bytes)
4972515328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 12% |
memory (bytes)
4972662784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2721431800.0
relative error loss 0.4654229
shape of L is 
torch.Size([])
memory (bytes)
4974792704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4974792704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2465024500.0
relative error loss 0.42157176
shape of L is 
torch.Size([])
memory (bytes)
4976721920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 12% |
memory (bytes)
4976721920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2288000500.0
relative error loss 0.3912969
shape of L is 
torch.Size([])
memory (bytes)
4978958336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
4978958336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2146534400.0
relative error loss 0.36710316
shape of L is 
torch.Size([])
memory (bytes)
4981051392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4981051392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1997376000.0
relative error loss 0.3415939
shape of L is 
torch.Size([])
memory (bytes)
4983341056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
4983341056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1834421500.0
relative error loss 0.3137252
time to take a step is 189.7116823196411
it  3 : 2918880768
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  8% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4985380864
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4985380864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1834421500.0
relative error loss 0.3137252
shape of L is 
torch.Size([])
memory (bytes)
4987420672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4987609088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1728060700.0
relative error loss 0.29553524
shape of L is 
torch.Size([])
memory (bytes)
4989751296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4989751296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1630660100.0
relative error loss 0.27887765
shape of L is 
torch.Size([])
memory (bytes)
4991877120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4991930368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1494651900.0
relative error loss 0.25561735
shape of L is 
torch.Size([])
memory (bytes)
4994019328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
4994068480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1396079100.0
relative error loss 0.23875931
shape of L is 
torch.Size([])
memory (bytes)
4996046848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
4996046848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1303039000.0
relative error loss 0.22284746
shape of L is 
torch.Size([])
memory (bytes)
4998197248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4998197248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1229794800.0
relative error loss 0.21032114
shape of L is 
torch.Size([])
memory (bytes)
5000445952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5000499200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1151520300.0
relative error loss 0.19693452
shape of L is 
torch.Size([])
memory (bytes)
5002612736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
5002612736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1076042800.0
relative error loss 0.18402627
shape of L is 
torch.Size([])
memory (bytes)
5004566528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5004566528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1011469300.0
relative error loss 0.17298283
time to take a step is 206.75923037528992
c= tensor(793.6008, device='cuda:0')
c= tensor(104476.5312, device='cuda:0')
c= tensor(107404.7734, device='cuda:0')
c= tensor(126855.6875, device='cuda:0')
c= tensor(377988.6875, device='cuda:0')
c= tensor(416353.6875, device='cuda:0')
c= tensor(1047826.8125, device='cuda:0')
c= tensor(1291909.3750, device='cuda:0')
c= tensor(1306536.5000, device='cuda:0')
c= tensor(2054945.6250, device='cuda:0')
c= tensor(2073634.6250, device='cuda:0')
c= tensor(7229439.5000, device='cuda:0')
c= tensor(7253882.5000, device='cuda:0')
c= tensor(16191748., device='cuda:0')
c= tensor(16308056., device='cuda:0')
c= tensor(16520947., device='cuda:0')
c= tensor(17537694., device='cuda:0')
c= tensor(18062872., device='cuda:0')
c= tensor(31859886., device='cuda:0')
c= tensor(34955560., device='cuda:0')
c= tensor(34967012., device='cuda:0')
c= tensor(50862108., device='cuda:0')
c= tensor(50899736., device='cuda:0')
c= tensor(51369960., device='cuda:0')
c= tensor(51469848., device='cuda:0')
c= tensor(52571132., device='cuda:0')
c= tensor(54013228., device='cuda:0')
c= tensor(54051724., device='cuda:0')
c= tensor(56031244., device='cuda:0')
c= tensor(3.0639e+08, device='cuda:0')
c= tensor(3.0641e+08, device='cuda:0')
c= tensor(3.9380e+08, device='cuda:0')
c= tensor(3.9551e+08, device='cuda:0')
c= tensor(3.9551e+08, device='cuda:0')
c= tensor(3.9552e+08, device='cuda:0')
c= tensor(4.0015e+08, device='cuda:0')
c= tensor(4.0238e+08, device='cuda:0')
c= tensor(4.0238e+08, device='cuda:0')
c= tensor(4.0239e+08, device='cuda:0')
c= tensor(4.0240e+08, device='cuda:0')
c= tensor(4.0240e+08, device='cuda:0')
c= tensor(4.0240e+08, device='cuda:0')
c= tensor(4.0240e+08, device='cuda:0')
c= tensor(4.0241e+08, device='cuda:0')
c= tensor(4.0241e+08, device='cuda:0')
c= tensor(4.0241e+08, device='cuda:0')
c= tensor(4.0244e+08, device='cuda:0')
c= tensor(4.0244e+08, device='cuda:0')
c= tensor(4.0244e+08, device='cuda:0')
c= tensor(4.0249e+08, device='cuda:0')
c= tensor(4.0255e+08, device='cuda:0')
c= tensor(4.0256e+08, device='cuda:0')
c= tensor(4.0257e+08, device='cuda:0')
c= tensor(4.0257e+08, device='cuda:0')
c= tensor(4.0259e+08, device='cuda:0')
c= tensor(4.0261e+08, device='cuda:0')
c= tensor(4.0261e+08, device='cuda:0')
c= tensor(4.0261e+08, device='cuda:0')
c= tensor(4.0261e+08, device='cuda:0')
c= tensor(4.0262e+08, device='cuda:0')
c= tensor(4.0267e+08, device='cuda:0')
c= tensor(4.0267e+08, device='cuda:0')
c= tensor(4.0279e+08, device='cuda:0')
c= tensor(4.0280e+08, device='cuda:0')
c= tensor(4.0280e+08, device='cuda:0')
c= tensor(4.0281e+08, device='cuda:0')
c= tensor(4.0281e+08, device='cuda:0')
c= tensor(4.0281e+08, device='cuda:0')
c= tensor(4.0282e+08, device='cuda:0')
c= tensor(4.0282e+08, device='cuda:0')
c= tensor(4.0282e+08, device='cuda:0')
c= tensor(4.0283e+08, device='cuda:0')
c= tensor(4.0283e+08, device='cuda:0')
c= tensor(4.0284e+08, device='cuda:0')
c= tensor(4.0284e+08, device='cuda:0')
c= tensor(4.0291e+08, device='cuda:0')
c= tensor(4.0292e+08, device='cuda:0')
c= tensor(4.0292e+08, device='cuda:0')
c= tensor(4.0292e+08, device='cuda:0')
c= tensor(4.0293e+08, device='cuda:0')
c= tensor(4.0294e+08, device='cuda:0')
c= tensor(4.0294e+08, device='cuda:0')
c= tensor(4.0295e+08, device='cuda:0')
c= tensor(4.0295e+08, device='cuda:0')
c= tensor(4.0296e+08, device='cuda:0')
c= tensor(4.0296e+08, device='cuda:0')
c= tensor(4.0298e+08, device='cuda:0')
c= tensor(4.0298e+08, device='cuda:0')
c= tensor(4.0299e+08, device='cuda:0')
c= tensor(4.0299e+08, device='cuda:0')
c= tensor(4.0300e+08, device='cuda:0')
c= tensor(4.0300e+08, device='cuda:0')
c= tensor(4.0300e+08, device='cuda:0')
c= tensor(4.0302e+08, device='cuda:0')
c= tensor(4.0303e+08, device='cuda:0')
c= tensor(4.0303e+08, device='cuda:0')
c= tensor(4.0304e+08, device='cuda:0')
c= tensor(4.0312e+08, device='cuda:0')
c= tensor(4.0315e+08, device='cuda:0')
c= tensor(4.0316e+08, device='cuda:0')
c= tensor(4.0326e+08, device='cuda:0')
c= tensor(4.0328e+08, device='cuda:0')
c= tensor(4.0330e+08, device='cuda:0')
c= tensor(4.0330e+08, device='cuda:0')
c= tensor(4.0330e+08, device='cuda:0')
c= tensor(4.0331e+08, device='cuda:0')
c= tensor(4.0332e+08, device='cuda:0')
c= tensor(4.0332e+08, device='cuda:0')
c= tensor(4.0333e+08, device='cuda:0')
c= tensor(4.0333e+08, device='cuda:0')
c= tensor(4.0333e+08, device='cuda:0')
c= tensor(4.0334e+08, device='cuda:0')
c= tensor(4.0334e+08, device='cuda:0')
c= tensor(4.0334e+08, device='cuda:0')
c= tensor(4.0335e+08, device='cuda:0')
c= tensor(4.0335e+08, device='cuda:0')
c= tensor(4.0335e+08, device='cuda:0')
c= tensor(4.0336e+08, device='cuda:0')
c= tensor(4.0338e+08, device='cuda:0')
c= tensor(4.0339e+08, device='cuda:0')
c= tensor(4.0341e+08, device='cuda:0')
c= tensor(4.0342e+08, device='cuda:0')
c= tensor(4.0342e+08, device='cuda:0')
c= tensor(4.0342e+08, device='cuda:0')
c= tensor(4.0343e+08, device='cuda:0')
c= tensor(4.0343e+08, device='cuda:0')
c= tensor(4.0343e+08, device='cuda:0')
c= tensor(4.0344e+08, device='cuda:0')
c= tensor(4.0347e+08, device='cuda:0')
c= tensor(4.0348e+08, device='cuda:0')
c= tensor(4.0349e+08, device='cuda:0')
c= tensor(4.0349e+08, device='cuda:0')
c= tensor(4.0351e+08, device='cuda:0')
c= tensor(4.0352e+08, device='cuda:0')
c= tensor(4.0392e+08, device='cuda:0')
c= tensor(4.0393e+08, device='cuda:0')
c= tensor(4.0393e+08, device='cuda:0')
c= tensor(4.0393e+08, device='cuda:0')
c= tensor(4.0393e+08, device='cuda:0')
c= tensor(4.0394e+08, device='cuda:0')
c= tensor(4.0394e+08, device='cuda:0')
c= tensor(4.0394e+08, device='cuda:0')
c= tensor(4.0397e+08, device='cuda:0')
c= tensor(4.0430e+08, device='cuda:0')
c= tensor(4.0431e+08, device='cuda:0')
c= tensor(4.0431e+08, device='cuda:0')
c= tensor(4.0432e+08, device='cuda:0')
c= tensor(4.0432e+08, device='cuda:0')
c= tensor(4.0432e+08, device='cuda:0')
c= tensor(4.0433e+08, device='cuda:0')
c= tensor(4.0433e+08, device='cuda:0')
c= tensor(4.0435e+08, device='cuda:0')
c= tensor(4.0436e+08, device='cuda:0')
c= tensor(4.0446e+08, device='cuda:0')
c= tensor(4.0447e+08, device='cuda:0')
c= tensor(4.0457e+08, device='cuda:0')
c= tensor(4.0457e+08, device='cuda:0')
c= tensor(4.0457e+08, device='cuda:0')
c= tensor(4.0459e+08, device='cuda:0')
c= tensor(4.0459e+08, device='cuda:0')
c= tensor(4.0462e+08, device='cuda:0')
c= tensor(4.0462e+08, device='cuda:0')
c= tensor(4.0464e+08, device='cuda:0')
c= tensor(4.0464e+08, device='cuda:0')
c= tensor(4.0464e+08, device='cuda:0')
c= tensor(4.0464e+08, device='cuda:0')
c= tensor(4.0465e+08, device='cuda:0')
c= tensor(4.0465e+08, device='cuda:0')
c= tensor(4.0466e+08, device='cuda:0')
c= tensor(4.0466e+08, device='cuda:0')
c= tensor(4.0466e+08, device='cuda:0')
c= tensor(4.0466e+08, device='cuda:0')
c= tensor(4.0467e+08, device='cuda:0')
c= tensor(4.0467e+08, device='cuda:0')
c= tensor(4.0469e+08, device='cuda:0')
c= tensor(4.0491e+08, device='cuda:0')
c= tensor(4.0492e+08, device='cuda:0')
c= tensor(4.0493e+08, device='cuda:0')
c= tensor(4.0496e+08, device='cuda:0')
c= tensor(4.0497e+08, device='cuda:0')
c= tensor(4.0497e+08, device='cuda:0')
c= tensor(4.0498e+08, device='cuda:0')
c= tensor(4.0499e+08, device='cuda:0')
c= tensor(4.0499e+08, device='cuda:0')
c= tensor(4.0500e+08, device='cuda:0')
c= tensor(4.0503e+08, device='cuda:0')
c= tensor(4.0503e+08, device='cuda:0')
c= tensor(4.0505e+08, device='cuda:0')
c= tensor(4.0506e+08, device='cuda:0')
c= tensor(4.0546e+08, device='cuda:0')
c= tensor(4.0546e+08, device='cuda:0')
c= tensor(4.0546e+08, device='cuda:0')
c= tensor(4.0546e+08, device='cuda:0')
c= tensor(4.0547e+08, device='cuda:0')
c= tensor(4.0547e+08, device='cuda:0')
c= tensor(4.0548e+08, device='cuda:0')
c= tensor(4.0548e+08, device='cuda:0')
c= tensor(4.0548e+08, device='cuda:0')
c= tensor(4.0549e+08, device='cuda:0')
c= tensor(4.0549e+08, device='cuda:0')
c= tensor(4.0561e+08, device='cuda:0')
c= tensor(4.0562e+08, device='cuda:0')
c= tensor(4.0571e+08, device='cuda:0')
c= tensor(4.0573e+08, device='cuda:0')
c= tensor(4.0573e+08, device='cuda:0')
c= tensor(4.0573e+08, device='cuda:0')
c= tensor(4.0574e+08, device='cuda:0')
c= tensor(4.0575e+08, device='cuda:0')
c= tensor(4.0576e+08, device='cuda:0')
c= tensor(4.0580e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0582e+08, device='cuda:0')
c= tensor(4.0583e+08, device='cuda:0')
c= tensor(4.0583e+08, device='cuda:0')
c= tensor(4.0583e+08, device='cuda:0')
c= tensor(4.0584e+08, device='cuda:0')
c= tensor(4.0586e+08, device='cuda:0')
c= tensor(4.0586e+08, device='cuda:0')
c= tensor(4.0587e+08, device='cuda:0')
c= tensor(4.0587e+08, device='cuda:0')
c= tensor(4.0589e+08, device='cuda:0')
c= tensor(4.0589e+08, device='cuda:0')
c= tensor(4.0589e+08, device='cuda:0')
c= tensor(4.0590e+08, device='cuda:0')
c= tensor(4.0591e+08, device='cuda:0')
c= tensor(4.0591e+08, device='cuda:0')
c= tensor(4.0591e+08, device='cuda:0')
c= tensor(4.0591e+08, device='cuda:0')
c= tensor(4.0592e+08, device='cuda:0')
c= tensor(4.0594e+08, device='cuda:0')
c= tensor(4.0594e+08, device='cuda:0')
c= tensor(4.0594e+08, device='cuda:0')
c= tensor(4.0654e+08, device='cuda:0')
c= tensor(4.0656e+08, device='cuda:0')
c= tensor(4.0657e+08, device='cuda:0')
c= tensor(4.0660e+08, device='cuda:0')
c= tensor(4.0841e+08, device='cuda:0')
c= tensor(4.0842e+08, device='cuda:0')
c= tensor(4.0844e+08, device='cuda:0')
c= tensor(4.0844e+08, device='cuda:0')
c= tensor(4.0845e+08, device='cuda:0')
c= tensor(4.1606e+08, device='cuda:0')
c= tensor(4.4261e+08, device='cuda:0')
c= tensor(4.4261e+08, device='cuda:0')
c= tensor(4.4297e+08, device='cuda:0')
c= tensor(4.4354e+08, device='cuda:0')
c= tensor(4.4357e+08, device='cuda:0')
c= tensor(4.5423e+08, device='cuda:0')
c= tensor(4.5423e+08, device='cuda:0')
c= tensor(4.5428e+08, device='cuda:0')
c= tensor(4.5792e+08, device='cuda:0')
c= tensor(4.6184e+08, device='cuda:0')
c= tensor(4.6184e+08, device='cuda:0')
c= tensor(4.6197e+08, device='cuda:0')
c= tensor(4.6257e+08, device='cuda:0')
c= tensor(4.6532e+08, device='cuda:0')
c= tensor(4.6582e+08, device='cuda:0')
c= tensor(4.6716e+08, device='cuda:0')
c= tensor(4.6801e+08, device='cuda:0')
c= tensor(4.6822e+08, device='cuda:0')
c= tensor(4.6822e+08, device='cuda:0')
c= tensor(4.9245e+08, device='cuda:0')
c= tensor(4.9245e+08, device='cuda:0')
c= tensor(4.9246e+08, device='cuda:0')
c= tensor(4.9267e+08, device='cuda:0')
c= tensor(4.9296e+08, device='cuda:0')
c= tensor(5.2068e+08, device='cuda:0')
c= tensor(5.2152e+08, device='cuda:0')
c= tensor(5.2153e+08, device='cuda:0')
c= tensor(5.2159e+08, device='cuda:0')
c= tensor(5.2183e+08, device='cuda:0')
c= tensor(5.2193e+08, device='cuda:0')
c= tensor(5.3441e+08, device='cuda:0')
c= tensor(5.3442e+08, device='cuda:0')
c= tensor(5.3452e+08, device='cuda:0')
c= tensor(5.3452e+08, device='cuda:0')
c= tensor(5.3453e+08, device='cuda:0')
c= tensor(5.3516e+08, device='cuda:0')
c= tensor(5.3663e+08, device='cuda:0')
c= tensor(5.3679e+08, device='cuda:0')
c= tensor(5.3687e+08, device='cuda:0')
c= tensor(5.6531e+08, device='cuda:0')
c= tensor(5.6534e+08, device='cuda:0')
c= tensor(5.6549e+08, device='cuda:0')
c= tensor(5.6766e+08, device='cuda:0')
c= tensor(5.6767e+08, device='cuda:0')
c= tensor(5.6902e+08, device='cuda:0')
c= tensor(5.9077e+08, device='cuda:0')
c= tensor(6.1361e+08, device='cuda:0')
c= tensor(6.1365e+08, device='cuda:0')
c= tensor(6.1366e+08, device='cuda:0')
c= tensor(6.1367e+08, device='cuda:0')
c= tensor(6.1367e+08, device='cuda:0')
c= tensor(6.1403e+08, device='cuda:0')
c= tensor(6.1406e+08, device='cuda:0')
c= tensor(6.1443e+08, device='cuda:0')
c= tensor(6.2077e+08, device='cuda:0')
c= tensor(6.2199e+08, device='cuda:0')
c= tensor(6.2204e+08, device='cuda:0')
c= tensor(6.2206e+08, device='cuda:0')
c= tensor(6.2936e+08, device='cuda:0')
c= tensor(6.3933e+08, device='cuda:0')
c= tensor(6.3954e+08, device='cuda:0')
c= tensor(6.4005e+08, device='cuda:0')
c= tensor(7.6657e+08, device='cuda:0')
c= tensor(7.6658e+08, device='cuda:0')
c= tensor(7.7135e+08, device='cuda:0')
c= tensor(7.7137e+08, device='cuda:0')
c= tensor(7.7202e+08, device='cuda:0')
c= tensor(7.7222e+08, device='cuda:0')
c= tensor(8.3244e+08, device='cuda:0')
c= tensor(8.3365e+08, device='cuda:0')
c= tensor(8.3366e+08, device='cuda:0')
c= tensor(8.3613e+08, device='cuda:0')
c= tensor(8.3886e+08, device='cuda:0')
c= tensor(8.3889e+08, device='cuda:0')
c= tensor(8.3952e+08, device='cuda:0')
c= tensor(8.4691e+08, device='cuda:0')
c= tensor(8.6354e+08, device='cuda:0')
c= tensor(8.6432e+08, device='cuda:0')
c= tensor(8.6432e+08, device='cuda:0')
c= tensor(8.6434e+08, device='cuda:0')
c= tensor(8.6436e+08, device='cuda:0')
c= tensor(8.6455e+08, device='cuda:0')
c= tensor(8.6457e+08, device='cuda:0')
c= tensor(8.6458e+08, device='cuda:0')
c= tensor(8.6495e+08, device='cuda:0')
c= tensor(8.6812e+08, device='cuda:0')
c= tensor(8.6825e+08, device='cuda:0')
c= tensor(8.6826e+08, device='cuda:0')
c= tensor(8.6829e+08, device='cuda:0')
c= tensor(8.6834e+08, device='cuda:0')
c= tensor(8.6836e+08, device='cuda:0')
c= tensor(8.6848e+08, device='cuda:0')
c= tensor(8.6848e+08, device='cuda:0')
c= tensor(8.7209e+08, device='cuda:0')
c= tensor(8.7223e+08, device='cuda:0')
c= tensor(8.7235e+08, device='cuda:0')
c= tensor(8.7289e+08, device='cuda:0')
c= tensor(8.7294e+08, device='cuda:0')
c= tensor(1.3112e+09, device='cuda:0')
c= tensor(1.3112e+09, device='cuda:0')
c= tensor(1.3119e+09, device='cuda:0')
c= tensor(1.3119e+09, device='cuda:0')
c= tensor(1.3119e+09, device='cuda:0')
c= tensor(1.3119e+09, device='cuda:0')
c= tensor(1.3120e+09, device='cuda:0')
c= tensor(1.3120e+09, device='cuda:0')
c= tensor(1.3140e+09, device='cuda:0')
c= tensor(1.3141e+09, device='cuda:0')
c= tensor(1.3141e+09, device='cuda:0')
c= tensor(1.3355e+09, device='cuda:0')
c= tensor(1.3359e+09, device='cuda:0')
c= tensor(1.3365e+09, device='cuda:0')
c= tensor(1.3404e+09, device='cuda:0')
c= tensor(1.3491e+09, device='cuda:0')
c= tensor(1.3491e+09, device='cuda:0')
c= tensor(1.3491e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3802e+09, device='cuda:0')
c= tensor(1.3804e+09, device='cuda:0')
c= tensor(1.3815e+09, device='cuda:0')
c= tensor(1.3816e+09, device='cuda:0')
c= tensor(1.3816e+09, device='cuda:0')
c= tensor(1.3817e+09, device='cuda:0')
c= tensor(1.5849e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6189e+09, device='cuda:0')
c= tensor(1.6189e+09, device='cuda:0')
c= tensor(1.6204e+09, device='cuda:0')
c= tensor(1.6212e+09, device='cuda:0')
c= tensor(1.6216e+09, device='cuda:0')
c= tensor(1.6217e+09, device='cuda:0')
c= tensor(1.6231e+09, device='cuda:0')
c= tensor(1.6906e+09, device='cuda:0')
c= tensor(1.6907e+09, device='cuda:0')
c= tensor(1.6907e+09, device='cuda:0')
c= tensor(1.6910e+09, device='cuda:0')
c= tensor(1.6912e+09, device='cuda:0')
c= tensor(1.6912e+09, device='cuda:0')
c= tensor(1.6939e+09, device='cuda:0')
c= tensor(1.6941e+09, device='cuda:0')
c= tensor(1.6941e+09, device='cuda:0')
c= tensor(1.6946e+09, device='cuda:0')
c= tensor(1.6949e+09, device='cuda:0')
c= tensor(1.6949e+09, device='cuda:0')
c= tensor(1.6984e+09, device='cuda:0')
c= tensor(1.7013e+09, device='cuda:0')
c= tensor(1.7083e+09, device='cuda:0')
c= tensor(1.7212e+09, device='cuda:0')
c= tensor(1.7271e+09, device='cuda:0')
c= tensor(1.7271e+09, device='cuda:0')
c= tensor(1.7276e+09, device='cuda:0')
c= tensor(1.7280e+09, device='cuda:0')
c= tensor(1.7311e+09, device='cuda:0')
c= tensor(1.7311e+09, device='cuda:0')
c= tensor(1.7402e+09, device='cuda:0')
c= tensor(1.9204e+09, device='cuda:0')
c= tensor(1.9246e+09, device='cuda:0')
c= tensor(1.9250e+09, device='cuda:0')
c= tensor(1.9278e+09, device='cuda:0')
c= tensor(1.9278e+09, device='cuda:0')
c= tensor(1.9278e+09, device='cuda:0')
c= tensor(1.9279e+09, device='cuda:0')
c= tensor(1.9292e+09, device='cuda:0')
c= tensor(1.9300e+09, device='cuda:0')
c= tensor(1.9835e+09, device='cuda:0')
c= tensor(1.9966e+09, device='cuda:0')
c= tensor(1.9974e+09, device='cuda:0')
c= tensor(1.9976e+09, device='cuda:0')
c= tensor(1.9990e+09, device='cuda:0')
c= tensor(1.9990e+09, device='cuda:0')
c= tensor(1.9991e+09, device='cuda:0')
c= tensor(2.0080e+09, device='cuda:0')
c= tensor(2.0080e+09, device='cuda:0')
c= tensor(2.0080e+09, device='cuda:0')
c= tensor(2.0081e+09, device='cuda:0')
c= tensor(2.0156e+09, device='cuda:0')
c= tensor(2.0157e+09, device='cuda:0')
c= tensor(2.0179e+09, device='cuda:0')
c= tensor(2.0180e+09, device='cuda:0')
c= tensor(2.0181e+09, device='cuda:0')
c= tensor(2.0181e+09, device='cuda:0')
c= tensor(2.0181e+09, device='cuda:0')
c= tensor(2.0187e+09, device='cuda:0')
c= tensor(2.0190e+09, device='cuda:0')
c= tensor(2.0191e+09, device='cuda:0')
c= tensor(2.0209e+09, device='cuda:0')
c= tensor(2.0209e+09, device='cuda:0')
c= tensor(2.0226e+09, device='cuda:0')
c= tensor(2.0227e+09, device='cuda:0')
c= tensor(2.0231e+09, device='cuda:0')
c= tensor(2.0231e+09, device='cuda:0')
c= tensor(2.0247e+09, device='cuda:0')
c= tensor(2.0248e+09, device='cuda:0')
c= tensor(2.0252e+09, device='cuda:0')
c= tensor(2.0261e+09, device='cuda:0')
c= tensor(2.0351e+09, device='cuda:0')
c= tensor(2.0352e+09, device='cuda:0')
c= tensor(2.0352e+09, device='cuda:0')
c= tensor(2.0454e+09, device='cuda:0')
c= tensor(2.0454e+09, device='cuda:0')
c= tensor(2.0905e+09, device='cuda:0')
c= tensor(2.0905e+09, device='cuda:0')
c= tensor(2.0911e+09, device='cuda:0')
c= tensor(2.0930e+09, device='cuda:0')
c= tensor(2.0930e+09, device='cuda:0')
c= tensor(2.0946e+09, device='cuda:0')
c= tensor(2.0952e+09, device='cuda:0')
c= tensor(2.1629e+09, device='cuda:0')
c= tensor(2.1629e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1631e+09, device='cuda:0')
c= tensor(2.1631e+09, device='cuda:0')
c= tensor(2.1632e+09, device='cuda:0')
c= tensor(2.1635e+09, device='cuda:0')
c= tensor(2.1636e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.2031e+09, device='cuda:0')
c= tensor(2.2043e+09, device='cuda:0')
c= tensor(2.2129e+09, device='cuda:0')
c= tensor(2.2132e+09, device='cuda:0')
c= tensor(2.2133e+09, device='cuda:0')
c= tensor(2.2133e+09, device='cuda:0')
c= tensor(2.2133e+09, device='cuda:0')
c= tensor(2.2454e+09, device='cuda:0')
c= tensor(2.2454e+09, device='cuda:0')
c= tensor(2.2455e+09, device='cuda:0')
c= tensor(2.2463e+09, device='cuda:0')
c= tensor(2.2482e+09, device='cuda:0')
c= tensor(2.2482e+09, device='cuda:0')
c= tensor(2.2482e+09, device='cuda:0')
c= tensor(2.2664e+09, device='cuda:0')
c= tensor(2.2664e+09, device='cuda:0')
c= tensor(2.2674e+09, device='cuda:0')
c= tensor(2.2676e+09, device='cuda:0')
c= tensor(2.2757e+09, device='cuda:0')
c= tensor(2.2862e+09, device='cuda:0')
c= tensor(2.2928e+09, device='cuda:0')
c= tensor(2.2972e+09, device='cuda:0')
c= tensor(2.2972e+09, device='cuda:0')
c= tensor(2.2977e+09, device='cuda:0')
c= tensor(2.2979e+09, device='cuda:0')
c= tensor(2.2979e+09, device='cuda:0')
c= tensor(2.2980e+09, device='cuda:0')
c= tensor(2.2980e+09, device='cuda:0')
c= tensor(2.2984e+09, device='cuda:0')
c= tensor(2.2984e+09, device='cuda:0')
c= tensor(2.2984e+09, device='cuda:0')
c= tensor(2.2985e+09, device='cuda:0')
c= tensor(2.2985e+09, device='cuda:0')
c= tensor(2.3012e+09, device='cuda:0')
c= tensor(2.3012e+09, device='cuda:0')
c= tensor(2.3014e+09, device='cuda:0')
c= tensor(2.3018e+09, device='cuda:0')
c= tensor(2.3027e+09, device='cuda:0')
c= tensor(2.3027e+09, device='cuda:0')
c= tensor(2.3028e+09, device='cuda:0')
c= tensor(2.3028e+09, device='cuda:0')
c= tensor(2.3087e+09, device='cuda:0')
c= tensor(2.3087e+09, device='cuda:0')
c= tensor(2.3087e+09, device='cuda:0')
c= tensor(2.3087e+09, device='cuda:0')
c= tensor(2.3355e+09, device='cuda:0')
c= tensor(2.3700e+09, device='cuda:0')
c= tensor(2.3700e+09, device='cuda:0')
c= tensor(2.3700e+09, device='cuda:0')
c= tensor(2.3703e+09, device='cuda:0')
c= tensor(2.3725e+09, device='cuda:0')
c= tensor(2.3725e+09, device='cuda:0')
c= tensor(2.3726e+09, device='cuda:0')
c= tensor(2.3726e+09, device='cuda:0')
c= tensor(2.3743e+09, device='cuda:0')
c= tensor(2.3745e+09, device='cuda:0')
c= tensor(2.3777e+09, device='cuda:0')
c= tensor(2.3777e+09, device='cuda:0')
c= tensor(2.3780e+09, device='cuda:0')
c= tensor(2.3780e+09, device='cuda:0')
c= tensor(2.3782e+09, device='cuda:0')
c= tensor(2.3782e+09, device='cuda:0')
c= tensor(2.3790e+09, device='cuda:0')
c= tensor(2.3852e+09, device='cuda:0')
c= tensor(2.4027e+09, device='cuda:0')
c= tensor(2.4027e+09, device='cuda:0')
c= tensor(2.4027e+09, device='cuda:0')
c= tensor(2.4027e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.4869e+09, device='cuda:0')
c= tensor(2.4869e+09, device='cuda:0')
c= tensor(2.4869e+09, device='cuda:0')
c= tensor(2.4873e+09, device='cuda:0')
c= tensor(2.4873e+09, device='cuda:0')
c= tensor(2.4882e+09, device='cuda:0')
c= tensor(2.4882e+09, device='cuda:0')
c= tensor(2.4883e+09, device='cuda:0')
c= tensor(2.4883e+09, device='cuda:0')
c= tensor(2.4883e+09, device='cuda:0')
c= tensor(2.4883e+09, device='cuda:0')
c= tensor(2.4914e+09, device='cuda:0')
c= tensor(2.5089e+09, device='cuda:0')
c= tensor(2.5165e+09, device='cuda:0')
c= tensor(2.5170e+09, device='cuda:0')
c= tensor(2.5172e+09, device='cuda:0')
c= tensor(2.5173e+09, device='cuda:0')
c= tensor(2.5173e+09, device='cuda:0')
c= tensor(2.5187e+09, device='cuda:0')
c= tensor(2.5195e+09, device='cuda:0')
c= tensor(2.5231e+09, device='cuda:0')
c= tensor(2.5232e+09, device='cuda:0')
c= tensor(2.6060e+09, device='cuda:0')
c= tensor(2.6061e+09, device='cuda:0')
c= tensor(2.6067e+09, device='cuda:0')
c= tensor(2.6208e+09, device='cuda:0')
c= tensor(2.6213e+09, device='cuda:0')
c= tensor(2.6236e+09, device='cuda:0')
c= tensor(2.6261e+09, device='cuda:0')
c= tensor(2.6263e+09, device='cuda:0')
c= tensor(2.6264e+09, device='cuda:0')
c= tensor(2.6265e+09, device='cuda:0')
c= tensor(2.6269e+09, device='cuda:0')
c= tensor(2.6269e+09, device='cuda:0')
c= tensor(2.6289e+09, device='cuda:0')
c= tensor(2.7869e+09, device='cuda:0')
c= tensor(2.7927e+09, device='cuda:0')
c= tensor(2.7985e+09, device='cuda:0')
c= tensor(2.7989e+09, device='cuda:0')
c= tensor(2.7991e+09, device='cuda:0')
c= tensor(2.7993e+09, device='cuda:0')
c= tensor(2.7997e+09, device='cuda:0')
c= tensor(2.7999e+09, device='cuda:0')
c= tensor(2.8083e+09, device='cuda:0')
c= tensor(2.8083e+09, device='cuda:0')
c= tensor(2.8265e+09, device='cuda:0')
c= tensor(2.8384e+09, device='cuda:0')
c= tensor(2.8386e+09, device='cuda:0')
c= tensor(2.8386e+09, device='cuda:0')
c= tensor(2.8395e+09, device='cuda:0')
c= tensor(2.8405e+09, device='cuda:0')
c= tensor(2.8405e+09, device='cuda:0')
c= tensor(2.8924e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8938e+09, device='cuda:0')
c= tensor(2.8939e+09, device='cuda:0')
c= tensor(2.8939e+09, device='cuda:0')
c= tensor(2.8939e+09, device='cuda:0')
c= tensor(2.8939e+09, device='cuda:0')
c= tensor(2.8940e+09, device='cuda:0')
c= tensor(2.8940e+09, device='cuda:0')
c= tensor(5.3734e+09, device='cuda:0')
c= tensor(5.3734e+09, device='cuda:0')
c= tensor(5.3735e+09, device='cuda:0')
c= tensor(5.3736e+09, device='cuda:0')
c= tensor(5.3736e+09, device='cuda:0')
c= tensor(5.3736e+09, device='cuda:0')
c= tensor(5.3878e+09, device='cuda:0')
c= tensor(5.3879e+09, device='cuda:0')
c= tensor(5.5156e+09, device='cuda:0')
c= tensor(5.5156e+09, device='cuda:0')
c= tensor(5.5202e+09, device='cuda:0')
c= tensor(5.5219e+09, device='cuda:0')
c= tensor(5.5274e+09, device='cuda:0')
c= tensor(5.5524e+09, device='cuda:0')
c= tensor(5.5524e+09, device='cuda:0')
c= tensor(5.5524e+09, device='cuda:0')
c= tensor(5.5541e+09, device='cuda:0')
c= tensor(5.5542e+09, device='cuda:0')
c= tensor(5.5547e+09, device='cuda:0')
c= tensor(5.5549e+09, device='cuda:0')
c= tensor(5.5554e+09, device='cuda:0')
c= tensor(5.5562e+09, device='cuda:0')
c= tensor(5.5564e+09, device='cuda:0')
c= tensor(5.5760e+09, device='cuda:0')
c= tensor(5.6027e+09, device='cuda:0')
c= tensor(5.6028e+09, device='cuda:0')
c= tensor(5.6028e+09, device='cuda:0')
c= tensor(5.6064e+09, device='cuda:0')
c= tensor(5.6066e+09, device='cuda:0')
c= tensor(5.6073e+09, device='cuda:0')
c= tensor(5.6074e+09, device='cuda:0')
c= tensor(5.6088e+09, device='cuda:0')
c= tensor(5.6095e+09, device='cuda:0')
c= tensor(5.6184e+09, device='cuda:0')
c= tensor(5.6195e+09, device='cuda:0')
c= tensor(5.6195e+09, device='cuda:0')
c= tensor(5.6195e+09, device='cuda:0')
c= tensor(5.6196e+09, device='cuda:0')
c= tensor(5.6202e+09, device='cuda:0')
c= tensor(5.6231e+09, device='cuda:0')
c= tensor(5.6334e+09, device='cuda:0')
c= tensor(5.6334e+09, device='cuda:0')
c= tensor(5.6336e+09, device='cuda:0')
c= tensor(5.6341e+09, device='cuda:0')
c= tensor(5.6506e+09, device='cuda:0')
c= tensor(5.6512e+09, device='cuda:0')
c= tensor(5.6522e+09, device='cuda:0')
c= tensor(5.6526e+09, device='cuda:0')
c= tensor(5.6528e+09, device='cuda:0')
c= tensor(5.6528e+09, device='cuda:0')
c= tensor(5.6529e+09, device='cuda:0')
c= tensor(5.6535e+09, device='cuda:0')
c= tensor(5.6538e+09, device='cuda:0')
c= tensor(5.6541e+09, device='cuda:0')
c= tensor(5.6541e+09, device='cuda:0')
c= tensor(5.6542e+09, device='cuda:0')
c= tensor(5.6552e+09, device='cuda:0')
c= tensor(5.6554e+09, device='cuda:0')
c= tensor(5.6555e+09, device='cuda:0')
c= tensor(5.6555e+09, device='cuda:0')
c= tensor(5.6555e+09, device='cuda:0')
c= tensor(5.6556e+09, device='cuda:0')
c= tensor(5.6557e+09, device='cuda:0')
c= tensor(5.6583e+09, device='cuda:0')
c= tensor(5.6583e+09, device='cuda:0')
c= tensor(5.6589e+09, device='cuda:0')
c= tensor(5.6589e+09, device='cuda:0')
c= tensor(5.6590e+09, device='cuda:0')
c= tensor(5.6748e+09, device='cuda:0')
c= tensor(5.6753e+09, device='cuda:0')
c= tensor(5.6757e+09, device='cuda:0')
c= tensor(5.6785e+09, device='cuda:0')
c= tensor(5.6787e+09, device='cuda:0')
c= tensor(5.6808e+09, device='cuda:0')
c= tensor(5.6854e+09, device='cuda:0')
c= tensor(5.6854e+09, device='cuda:0')
c= tensor(5.6856e+09, device='cuda:0')
c= tensor(5.6857e+09, device='cuda:0')
c= tensor(5.6878e+09, device='cuda:0')
c= tensor(5.6932e+09, device='cuda:0')
c= tensor(5.6934e+09, device='cuda:0')
c= tensor(5.6943e+09, device='cuda:0')
c= tensor(5.6943e+09, device='cuda:0')
c= tensor(5.6945e+09, device='cuda:0')
c= tensor(5.6947e+09, device='cuda:0')
c= tensor(5.6947e+09, device='cuda:0')
c= tensor(5.7095e+09, device='cuda:0')
c= tensor(5.8398e+09, device='cuda:0')
c= tensor(5.8399e+09, device='cuda:0')
c= tensor(5.8400e+09, device='cuda:0')
c= tensor(5.8401e+09, device='cuda:0')
c= tensor(5.8439e+09, device='cuda:0')
c= tensor(5.8440e+09, device='cuda:0')
c= tensor(5.8440e+09, device='cuda:0')
c= tensor(5.8445e+09, device='cuda:0')
c= tensor(5.8464e+09, device='cuda:0')
c= tensor(5.8464e+09, device='cuda:0')
c= tensor(5.8472e+09, device='cuda:0')
c= tensor(5.8472e+09, device='cuda:0')
time to make c is 9.849448680877686
time for making loss is 9.849467754364014
p0 True
it  0 : 2139072000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5006950400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5007163392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 11% |
error is  1011469300.0
relative error loss 0.17298283
shape of L is 
torch.Size([])
memory (bytes)
5033762816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 11% |
memory (bytes)
5033762816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  996237300.0
relative error loss 0.17037784
shape of L is 
torch.Size([])
memory (bytes)
5037006848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5037256704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  958433300.0
relative error loss 0.16391253
shape of L is 
torch.Size([])
memory (bytes)
5040369664
| ID | GPU | MEM |
------------------
|  0 | 18% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5040451584
| ID | GPU  | MEM |
-------------------
|  0 |  13% |  0% |
|  1 | 100% | 11% |
error is  946049540.0
relative error loss 0.16179465
shape of L is 
torch.Size([])
memory (bytes)
5043613696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5043691520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  934716400.0
relative error loss 0.15985644
shape of L is 
torch.Size([])
memory (bytes)
5046902784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5046902784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  923009000.0
relative error loss 0.15785423
shape of L is 
torch.Size([])
memory (bytes)
5050097664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5050142720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  916628000.0
relative error loss 0.15676294
shape of L is 
torch.Size([])
memory (bytes)
5053243392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 11% |
memory (bytes)
5053243392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  909893600.0
relative error loss 0.15561122
shape of L is 
torch.Size([])
memory (bytes)
5056471040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5056557056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  902715900.0
relative error loss 0.15438367
shape of L is 
torch.Size([])
memory (bytes)
5059592192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5059592192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  898844160.0
relative error loss 0.15372153
time to take a step is 283.91202783584595
it  1 : 2922583040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5062979584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5062979584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  898844160.0
relative error loss 0.15372153
shape of L is 
torch.Size([])
memory (bytes)
5066125312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5066211328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  893025300.0
relative error loss 0.15272637
shape of L is 
torch.Size([])
memory (bytes)
5069205504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5069205504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  889731100.0
relative error loss 0.152163
shape of L is 
torch.Size([])
memory (bytes)
5072629760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
5072629760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  885613600.0
relative error loss 0.15145881
shape of L is 
torch.Size([])
memory (bytes)
5075783680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5075849216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  883745800.0
relative error loss 0.15113938
shape of L is 
torch.Size([])
memory (bytes)
5078851584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5079060480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  881051650.0
relative error loss 0.15067862
shape of L is 
torch.Size([])
memory (bytes)
5082251264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5082279936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  878959100.0
relative error loss 0.15032075
shape of L is 
torch.Size([])
memory (bytes)
5085339648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5085339648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  876156400.0
relative error loss 0.14984144
shape of L is 
torch.Size([])
memory (bytes)
5088710656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5088710656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  874760700.0
relative error loss 0.14960274
shape of L is 
torch.Size([])
memory (bytes)
5091684352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
5091684352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  873440800.0
relative error loss 0.149377
time to take a step is 284.1937575340271
it  2 : 2922583040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5095129088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5095129088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  873440800.0
relative error loss 0.149377
shape of L is 
torch.Size([])
memory (bytes)
5098106880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5098332160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  870853100.0
relative error loss 0.14893445
shape of L is 
torch.Size([])
memory (bytes)
5101559808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5101559808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  871022100.0
relative error loss 0.14896336
shape of L is 
torch.Size([])
memory (bytes)
5104668672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 11% |
memory (bytes)
5104766976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  869392400.0
relative error loss 0.14868464
shape of L is 
torch.Size([])
memory (bytes)
5107888128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5107990528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  867459600.0
relative error loss 0.1483541
shape of L is 
torch.Size([])
memory (bytes)
5111123968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5111209984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  865774100.0
relative error loss 0.14806584
shape of L is 
torch.Size([])
memory (bytes)
5114408960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5114408960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  863900160.0
relative error loss 0.14774536
shape of L is 
torch.Size([])
memory (bytes)
5117546496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5117628416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  862404600.0
relative error loss 0.14748958
shape of L is 
torch.Size([])
memory (bytes)
5120794624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5120794624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  861526500.0
relative error loss 0.14733942
shape of L is 
torch.Size([])
memory (bytes)
5123977216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5124063232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  860288000.0
relative error loss 0.1471276
time to take a step is 284.2986114025116
it  3 : 2923607040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5127094272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5127266304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  860288000.0
relative error loss 0.1471276
shape of L is 
torch.Size([])
memory (bytes)
5130477568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5130477568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  859546600.0
relative error loss 0.1470008
shape of L is 
torch.Size([])
memory (bytes)
5133660160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5133697024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  858497540.0
relative error loss 0.1468214
shape of L is 
torch.Size([])
memory (bytes)
5136916480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5136916480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  857896960.0
relative error loss 0.14671868
shape of L is 
torch.Size([])
memory (bytes)
5140008960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 12% |
memory (bytes)
5140008960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  857410050.0
relative error loss 0.14663541
shape of L is 
torch.Size([])
memory (bytes)
5143334912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5143334912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  856554500.0
relative error loss 0.14648908
shape of L is 
torch.Size([])
memory (bytes)
5146415104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5146533888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  855711740.0
relative error loss 0.14634496
shape of L is 
torch.Size([])
memory (bytes)
5149761536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5149765632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  855151600.0
relative error loss 0.14624918
shape of L is 
torch.Size([])
memory (bytes)
5152772096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5152772096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  854495740.0
relative error loss 0.146137
shape of L is 
torch.Size([])
memory (bytes)
5156102144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5156188160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  853413400.0
relative error loss 0.1459519
time to take a step is 284.2181017398834
it  4 : 2923607040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5159313408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5159313408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  853413400.0
relative error loss 0.1459519
shape of L is 
torch.Size([])
memory (bytes)
5162618880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5162622976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  854151200.0
relative error loss 0.14607807
shape of L is 
torch.Size([])
memory (bytes)
5165715456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 12% |
memory (bytes)
5165715456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  852820500.0
relative error loss 0.1458505
shape of L is 
torch.Size([])
memory (bytes)
5169037312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5169049600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  852067840.0
relative error loss 0.14572178
shape of L is 
torch.Size([])
memory (bytes)
5172264960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5172264960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  851299840.0
relative error loss 0.14559042
shape of L is 
torch.Size([])
memory (bytes)
5175484416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5175484416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  850798600.0
relative error loss 0.14550471
shape of L is 
torch.Size([])
memory (bytes)
5178679296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5178695680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  850204700.0
relative error loss 0.14540313
shape of L is 
torch.Size([])
memory (bytes)
5181722624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5181722624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  849776100.0
relative error loss 0.14532985
shape of L is 
torch.Size([])
memory (bytes)
5185069056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5185134592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  849356800.0
relative error loss 0.14525813
shape of L is 
torch.Size([])
memory (bytes)
5188177920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5188177920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  849822200.0
relative error loss 0.14533773
shape of L is 
torch.Size([])
memory (bytes)
5191520256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5191557120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  848979460.0
relative error loss 0.14519359
time to take a step is 298.43322348594666
it  5 : 2923607552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5194629120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5194629120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  848979460.0
relative error loss 0.14519359
shape of L is 
torch.Size([])
memory (bytes)
5197975552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5197975552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  848435200.0
relative error loss 0.14510052
shape of L is 
torch.Size([])
memory (bytes)
5201080320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5201080320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  848091650.0
relative error loss 0.14504176
shape of L is 
torch.Size([])
memory (bytes)
5204402176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5204402176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  847740400.0
relative error loss 0.1449817
shape of L is 
torch.Size([])
memory (bytes)
5207478272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5207478272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  847254500.0
relative error loss 0.1448986
shape of L is 
torch.Size([])
memory (bytes)
5210796032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 12% |
memory (bytes)
5210832896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  846752800.0
relative error loss 0.14481279
shape of L is 
torch.Size([])
memory (bytes)
5214056448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5214056448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  846351360.0
relative error loss 0.14474414
shape of L is 
torch.Size([])
memory (bytes)
5217214464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5217214464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  846009860.0
relative error loss 0.14468573
shape of L is 
torch.Size([])
memory (bytes)
5220405248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5220491264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  845666800.0
relative error loss 0.14462706
shape of L is 
torch.Size([])
memory (bytes)
5223624704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5223624704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  845398500.0
relative error loss 0.14458118
time to take a step is 270.50510716438293
it  6 : 2923607040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5226860544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 12% |
memory (bytes)
5226860544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  845398500.0
relative error loss 0.14458118
shape of L is 
torch.Size([])
memory (bytes)
5230112768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5230112768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  845205000.0
relative error loss 0.14454809
shape of L is 
torch.Size([])
memory (bytes)
5233242112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5233324032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  845093400.0
relative error loss 0.144529
shape of L is 
torch.Size([])
memory (bytes)
5236465664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5236465664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  845010400.0
relative error loss 0.14451481
shape of L is 
torch.Size([])
memory (bytes)
5239762944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5239762944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  844914700.0
relative error loss 0.14449844
shape of L is 
torch.Size([])
memory (bytes)
5242834944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5242982400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  844798460.0
relative error loss 0.14447856
shape of L is 
torch.Size([])
memory (bytes)
5246111744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5246197760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  844750850.0
relative error loss 0.14447041
shape of L is 
torch.Size([])
memory (bytes)
5249404928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5249404928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  844657660.0
relative error loss 0.14445448
shape of L is 
torch.Size([])
memory (bytes)
5252616192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5252616192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  844518900.0
relative error loss 0.14443074
shape of L is 
torch.Size([])
memory (bytes)
5255827456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5255827456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  844395500.0
relative error loss 0.14440964
time to take a step is 317.0306475162506
it  7 : 2922583040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5258956800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5259038720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  844395500.0
relative error loss 0.14440964
shape of L is 
torch.Size([])
memory (bytes)
5262254080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5262254080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  844100600.0
relative error loss 0.14435922
shape of L is 
torch.Size([])
memory (bytes)
5265379328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 12% |
memory (bytes)
5265473536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  843732500.0
relative error loss 0.14429626
shape of L is 
torch.Size([])
memory (bytes)
5268606976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5268688896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  843302900.0
relative error loss 0.14422278
shape of L is 
torch.Size([])
memory (bytes)
5271805952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5271900160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  842682900.0
relative error loss 0.14411674
shape of L is 
torch.Size([])
memory (bytes)
5275029504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5275111424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  842300400.0
relative error loss 0.14405134
shape of L is 
torch.Size([])
memory (bytes)
5278314496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5278314496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  841863200.0
relative error loss 0.14397655
shape of L is 
torch.Size([])
memory (bytes)
5281456128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5281456128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  841701400.0
relative error loss 0.14394888
shape of L is 
torch.Size([])
memory (bytes)
5284741120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5284741120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  841252350.0
relative error loss 0.1438721
shape of L is 
torch.Size([])
memory (bytes)
5287878656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5287878656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  841155600.0
relative error loss 0.14385554
time to take a step is 303.4831178188324
it  8 : 2922583040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5291167744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5291167744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  841155600.0
relative error loss 0.14385554
shape of L is 
torch.Size([])
memory (bytes)
5294370816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5294370816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  840746500.0
relative error loss 0.14378558
shape of L is 
torch.Size([])
memory (bytes)
5297606656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5297606656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  840519200.0
relative error loss 0.1437467
shape of L is 
torch.Size([])
memory (bytes)
5300793344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5300793344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  840157700.0
relative error loss 0.1436849
shape of L is 
torch.Size([])
memory (bytes)
5304029184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5304029184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  839827460.0
relative error loss 0.1436284
shape of L is 
torch.Size([])
memory (bytes)
5307170816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5307170816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  839735800.0
relative error loss 0.14361274
shape of L is 
torch.Size([])
memory (bytes)
5310476288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5310476288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  839385600.0
relative error loss 0.14355284
shape of L is 
torch.Size([])
memory (bytes)
5313662976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5313662976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  839244800.0
relative error loss 0.14352876
shape of L is 
torch.Size([])
memory (bytes)
5316845568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5316845568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  838986240.0
relative error loss 0.14348455
shape of L is 
torch.Size([])
memory (bytes)
5320134656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5320134656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  838660600.0
relative error loss 0.14342885
time to take a step is 282.10540986061096
it  9 : 2922583040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5323341824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5323341824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  838660600.0
relative error loss 0.14342885
shape of L is 
torch.Size([])
memory (bytes)
5326557184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5326557184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  838911500.0
relative error loss 0.14347176
shape of L is 
torch.Size([])
memory (bytes)
5329747968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 12% |
memory (bytes)
5329747968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  838499300.0
relative error loss 0.14340127
shape of L is 
torch.Size([])
memory (bytes)
5332992000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5332992000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  838196200.0
relative error loss 0.14334944
shape of L is 
torch.Size([])
memory (bytes)
5336051712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5336051712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  837965800.0
relative error loss 0.14331003
shape of L is 
torch.Size([])
memory (bytes)
5339430912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5339430912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  837756900.0
relative error loss 0.1432743
shape of L is 
torch.Size([])
memory (bytes)
5342642176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5342642176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  837579800.0
relative error loss 0.14324401
shape of L is 
torch.Size([])
memory (bytes)
5345828864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5345873920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  837445100.0
relative error loss 0.14322098
shape of L is 
torch.Size([])
memory (bytes)
5348999168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5349085184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  837309950.0
relative error loss 0.14319786
shape of L is 
torch.Size([])
memory (bytes)
5352161280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5352161280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  837172200.0
relative error loss 0.1431743
time to take a step is 269.4248688220978
it  10 : 2922583040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5355495424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 12% |
memory (bytes)
5355507712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  837172200.0
relative error loss 0.1431743
shape of L is 
torch.Size([])
memory (bytes)
5358624768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5358624768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836953600.0
relative error loss 0.14313692
shape of L is 
torch.Size([])
memory (bytes)
5361934336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 12% |
memory (bytes)
5361934336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836870140.0
relative error loss 0.14312264
shape of L is 
torch.Size([])
memory (bytes)
5364961280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5364961280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836791800.0
relative error loss 0.14310925
shape of L is 
torch.Size([])
memory (bytes)
5368377344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5368377344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836657660.0
relative error loss 0.14308631
shape of L is 
torch.Size([])
memory (bytes)
5371523072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5371609088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836555800.0
relative error loss 0.14306888
shape of L is 
torch.Size([])
memory (bytes)
5374824448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5374824448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836471800.0
relative error loss 0.14305452
shape of L is 
torch.Size([])
memory (bytes)
5378043904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5378043904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836404740.0
relative error loss 0.14304306
shape of L is 
torch.Size([])
memory (bytes)
5381255168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5381255168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836295200.0
relative error loss 0.14302431
shape of L is 
torch.Size([])
memory (bytes)
5384470528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5384470528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836224000.0
relative error loss 0.14301214
time to take a step is 275.42163252830505
it  11 : 2922583040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5387689984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5387689984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836224000.0
relative error loss 0.14301214
shape of L is 
torch.Size([])
memory (bytes)
5390770176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5390893056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836081150.0
relative error loss 0.14298771
shape of L is 
torch.Size([])
memory (bytes)
5394116608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 12% |
memory (bytes)
5394116608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  836017660.0
relative error loss 0.14297685
shape of L is 
torch.Size([])
memory (bytes)
5397331968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5397331968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  835928060.0
relative error loss 0.14296153
shape of L is 
torch.Size([])
memory (bytes)
5400559616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5400559616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  835792400.0
relative error loss 0.14293833
shape of L is 
torch.Size([])
memory (bytes)
5403693056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5403693056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  835622900.0
relative error loss 0.14290935
shape of L is 
torch.Size([])
memory (bytes)
5406978048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5406978048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  835460600.0
relative error loss 0.14288159
shape of L is 
torch.Size([])
memory (bytes)
5410013184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5410189312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  835308000.0
relative error loss 0.1428555
shape of L is 
torch.Size([])
memory (bytes)
5413408768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 12% |
memory (bytes)
5413408768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  835194400.0
relative error loss 0.14283605
shape of L is 
torch.Size([])
memory (bytes)
5416501248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5416628224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  835012600.0
relative error loss 0.14280497
time to take a step is 286.23708319664
it  12 : 2922583040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5419761664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 12% |
memory (bytes)
5419847680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  835012600.0
relative error loss 0.14280497
shape of L is 
torch.Size([])
memory (bytes)
5423058944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 12% |
memory (bytes)
5423058944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834930200.0
relative error loss 0.14279087
shape of L is 
torch.Size([])
memory (bytes)
5426274304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5426274304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834847200.0
relative error loss 0.14277668
shape of L is 
torch.Size([])
memory (bytes)
5429481472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5429481472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834720260.0
relative error loss 0.14275497
shape of L is 
torch.Size([])
memory (bytes)
5432676352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5432676352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834616800.0
relative error loss 0.14273728
shape of L is 
torch.Size([])
memory (bytes)
5435908096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5435908096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834555400.0
relative error loss 0.14272678
shape of L is 
torch.Size([])
memory (bytes)
5439131648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5439131648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834477060.0
relative error loss 0.14271338
shape of L is 
torch.Size([])
memory (bytes)
5442174976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5442334720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834394100.0
relative error loss 0.1426992
shape of L is 
torch.Size([])
memory (bytes)
5445562368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 12% |
memory (bytes)
5445562368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834288640.0
relative error loss 0.14268115
shape of L is 
torch.Size([])
memory (bytes)
5448634368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5448761344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834197000.0
relative error loss 0.14266548
time to take a step is 278.195109128952
it  13 : 2922583040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5451976704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5451976704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834197000.0
relative error loss 0.14266548
shape of L is 
torch.Size([])
memory (bytes)
5455167488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5455167488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834208800.0
relative error loss 0.14266749
shape of L is 
torch.Size([])
memory (bytes)
5458415616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5458415616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834120200.0
relative error loss 0.14265235
shape of L is 
torch.Size([])
memory (bytes)
5461618688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 12% |
memory (bytes)
5461622784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  834025500.0
relative error loss 0.14263615
shape of L is 
torch.Size([])
memory (bytes)
5464834048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5464834048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833916400.0
relative error loss 0.1426175
shape of L is 
torch.Size([])
memory (bytes)
5468041216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5468041216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833817100.0
relative error loss 0.1426005
shape of L is 
torch.Size([])
memory (bytes)
5471215616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5471256576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833720800.0
relative error loss 0.14258404
shape of L is 
torch.Size([])
memory (bytes)
5474467840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5474467840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833627140.0
relative error loss 0.14256802
shape of L is 
torch.Size([])
memory (bytes)
5477490688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5477675008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833545200.0
relative error loss 0.14255401
shape of L is 
torch.Size([])
memory (bytes)
5480886272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5480886272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833430500.0
relative error loss 0.1425344
time to take a step is 305.3714578151703
it  14 : 2922583040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5483991040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5483991040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833430500.0
relative error loss 0.1425344
shape of L is 
torch.Size([])
memory (bytes)
5487316992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5487316992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  833509400.0
relative error loss 0.14254789
shape of L is 
torch.Size([])
memory (bytes)
5490356224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 12% |
memory (bytes)
5490524160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833386000.0
relative error loss 0.14252678
shape of L is 
torch.Size([])
memory (bytes)
5493755904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5493755904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833323000.0
relative error loss 0.14251602
shape of L is 
torch.Size([])
memory (bytes)
5496963072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5496963072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833239040.0
relative error loss 0.14250165
shape of L is 
torch.Size([])
memory (bytes)
5500092416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5500182528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833176600.0
relative error loss 0.14249097
shape of L is 
torch.Size([])
memory (bytes)
5503393792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5503393792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833089000.0
relative error loss 0.14247599
shape of L is 
torch.Size([])
memory (bytes)
5506543616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5506543616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833038850.0
relative error loss 0.14246741
shape of L is 
torch.Size([])
memory (bytes)
5509750784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5509824512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  832972800.0
relative error loss 0.14245611
shape of L is 
torch.Size([])
memory (bytes)
5512896512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5512896512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  833019400.0
relative error loss 0.14246409
shape of L is 
torch.Size([])
memory (bytes)
5516259328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 12% |
memory (bytes)
5516259328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  832891900.0
relative error loss 0.14244229
time to take a step is 300.34061789512634
sum tnnu_Z after tensor(11933023., device='cuda:0')
shape of features
(7718,)
shape of features
(7718,)
number of orig particles 30871
number of new particles after remove low mass 30750
tnuZ shape should be parts x labs
torch.Size([30871, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1011430460.0
relative error without small mass is  0.17297618
nnu_Z shape should be number of particles by maxV
(30871, 702)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
shape of features
(30871,)
Thu Feb 2 05:54:34 EST 2023
