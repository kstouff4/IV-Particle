Thu Feb 2 22:37:11 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 20035200
numbers of Z: 13350
shape of features
(13350,)
shape of features
(13350,)
ZX	Vol	Parts	Cubes	Eps
Z	0.013793366813602867	13350	13.35	0.1010949992446738
X	0.011043002948859322	687	0.687	0.25237334483968216
X	0.011312040221234542	8289	8.289	0.11092078479451437
X	0.011183752819380988	778	0.778	0.24314722698687535
X	0.010930876339708052	1280	1.28	0.20440036665883568
X	0.012985024117245602	17292	17.292	0.09089343392529084
X	0.011385020592049721	10283	10.283	0.10345178650577416
X	0.01138015302798965	27177	27.177	0.07481388295579658
X	0.012166140482657945	32213	32.213	0.07228372940675433
X	0.011341012256264799	2619	2.619	0.16299418364193521
X	0.011207672468815983	20176	20.176	0.08220409525616622
X	0.011570188008361841	3929	3.929	0.14333517986294994
X	0.011108241919771507	128961	128.961	0.04416338082870883
X	0.011220738735627863	5673	5.673	0.12552673195358438
X	0.011253792670775361	55375	55.375	0.0587933748483098
X	0.011030869504715173	12762	12.762	0.09525702245544256
X	0.011299393323589894	24636	24.636	0.0771190451403063
X	0.011449830206194082	21450	21.45	0.0811192481237978
X	0.011896467110510017	21452	21.452	0.08215804063203902
X	0.012054884403214458	369925	369.925	0.03194109417334091
X	0.011919955991097882	49174	49.174	0.062351423314248304
X	0.011915298418259668	3985	3.985	0.14406512685865502
X	0.012883863116726355	302104	302.104	0.034937879462827136
X	0.011099037811716962	14063	14.063	0.09241357788592372
X	0.012955268805660194	5528	5.528	0.1328289678727166
X	0.0109717470234929	12810	12.81	0.09496761949747173
X	0.01275930527186772	38090	38.09	0.06944996297224997
X	0.012259849109688994	3883	3.883	0.14670308438382726
X	0.011216910507668612	4254	4.254	0.1381524495792773
X	0.011388280625827765	16629	16.629	0.0881449310411799
X	0.012939071105277833	1117186	1117.186	0.02262520580007638
X	0.011309301635698368	1682	1.682	0.18874259943003485
X	0.012367595486170667	88787	88.787	0.05183764900167178
X	0.012573009622789668	5071	5.071	0.13534697252211433
X	0.01133449677714974	4304	4.304	0.13809458848930403
X	0.011423291986393774	2145	2.145	0.17463099442898508
X	0.01144179817464389	14752	14.752	0.09187861918828198
X	0.011453311393511365	30086	30.086	0.0724751082083385
X	0.010850919099702658	612	0.612	0.2607584361673329
X	0.011321487441449136	2261	2.261	0.17108068307570218
X	0.010548685782239586	1159	1.159	0.20878773039136866
X	0.011066068695998004	3166	3.166	0.15176122278206375
X	0.010438079061578112	333	0.333	0.3153010735882999
X	0.01087355250554752	715	0.715	0.24775464451366097
X	0.011018769759019262	2745	2.745	0.15892671983821255
X	0.011148500735805431	729	0.729	0.24821592769821632
X	0.011356410613004131	1251	1.251	0.20860654284016247
X	0.011337860892244025	2569	2.569	0.16402964311093662
X	0.011222025793141683	2154	2.154	0.17335718831523242
X	0.010642366360320904	804	0.804	0.2365524146364691
X	0.011260263907903495	3181	3.181	0.15240350907698524
X	0.011574446798745601	2598	2.598	0.1645458873771702
X	0.010930215437258183	1017	1.017	0.22068319166760286
X	0.011213973116024852	2037	2.037	0.176572402905443
X	0.010612503526847079	914	0.914	0.2264421226550231
X	0.012219504439650858	3859	3.859	0.14684514924163172
X	0.01090955467468839	1088	1.088	0.2156383711123845
X	0.011234735547600844	2084	2.084	0.17534297862452722
X	0.011140605071275322	1304	1.304	0.20442957661830743
X	0.010835470618355294	1360	1.36	0.19972677530809602
X	0.011159259326704732	1236	1.236	0.20822792264165102
X	0.011226866075113449	2751	2.751	0.15980461501652596
X	0.01116295378269975	1125	1.125	0.21488636926544386
X	0.013497350171097044	2446	2.446	0.17671117752839607
X	0.0110353963881187	4077	4.077	0.1393635098196466
X	0.0113258009641539	2090	2.09	0.17564705788900387
X	0.011016587880058335	960	0.96	0.22555820166904664
X	0.01120145885765568	1902	1.902	0.18058764111359496
X	0.010413800521798322	3259	3.259	0.14729057739515258
X	0.011301138024404809	2229	2.229	0.1717924355652469
X	0.010834945454338923	835	0.835	0.2349885769612531
X	0.01130130293038411	2866	2.866	0.1579855780033702
X	0.011016404378570604	1992	1.992	0.17684122379449072
X	0.011069623117600026	3329	3.329	0.14925870865875143
X	0.01024472187658828	834	0.834	0.2307338825621299
X	0.01208070236364696	1478	1.478	0.20143700246434182
X	0.010942360514977996	2161	2.161	0.17171915818958558
X	0.012036695518241271	1379	1.379	0.20589588620330046
X	0.010936678685410001	278	0.278	0.3401054194755613
X	0.011075554261574402	2717	2.717	0.15974426553682816
X	0.010735011890803201	6337	6.337	0.11920823949707399
X	0.011013271744700131	3263	3.263	0.15000293632099504
X	0.0113393338405633	1275	1.275	0.20718538604695833
X	0.011206172719550406	1600	1.6	0.19132825464362785
X	0.010610881653088001	1104	1.104	0.21261511341978984
X	0.010800828736660769	2631	2.631	0.1601199369978736
X	0.010812007801889687	788	0.788	0.23940141387873187
X	0.010945958086699683	1774	1.774	0.18341438268353008
X	0.011367423068989394	603	0.603	0.26614291167079585
X	0.010775356959507933	1611	1.611	0.18841351072857837
X	0.011121180518711691	1205	1.205	0.20975932708525272
X	0.011193435648567174	2142	2.142	0.1735326957690354
X	0.011083090656225602	825	0.825	0.2377217830359667
X	0.01180347683133503	841	0.841	0.24121518942189418
X	0.01099946290742678	1566	1.566	0.19151078683124015
X	0.010469600728916258	1135	1.135	0.20972236798280544
X	0.010759195041504003	2047	2.047	0.17386839532246526
X	0.010278960568890931	1043	1.043	0.2143984446976549
X	0.010896968668430575	8541	8.541	0.10845901603744285
X	0.011678982720500207	1806	1.806	0.18630696451515544
X	0.010599281114965484	2054	2.054	0.17280593617498488
X	0.011377561409277752	3495	3.495	0.1482065289475117
X	0.011616900366860347	1668	1.668	0.19096982030276025
X	0.011365752589640286	3864	3.864	0.1432804796740955
X	0.00952897822757023	664	0.664	0.24301108534194527
X	0.011779178621219334	1764	1.764	0.18830946185018446
X	0.010982432934828298	718	0.718	0.2482321555561001
X	0.011945114617727475	2880	2.88	0.16066919257309364
X	0.011071929072412992	437	0.437	0.29370676693181214
X	0.01087108902459179	1446	1.446	0.1959002182161367
X	0.010501476019939355	690	0.69	0.24781851567842642
X	0.010984490146527453	780	0.78	0.24148774733639086
X	0.011321371615218444	691	0.691	0.253984374822272
X	0.010786353351160776	544	0.544	0.27066072940178476
X	0.01141509617745258	2301	2.301	0.17055101908453393
X	0.011167950013892099	1943	1.943	0.17912940648743422
X	0.010375930545221613	693	0.693	0.2464703076185887
X	0.0125741188571614	1536	1.536	0.20154039953407524
X	0.01106044623282905	1242	1.242	0.20727663570158242
X	0.013561326869034933	2742	2.742	0.17037723966212562
X	0.011365303215767675	942	0.942	0.2293559659278884
X	0.011276727225428955	6289	6.289	0.1214881242973489
X	0.011316292635794338	1710	1.71	0.1877454207694119
X	0.01081602434218445	734	0.734	0.24516428235737464
X	0.01122699996967443	1369	1.369	0.20166015242398
X	0.010609077211174035	835	0.835	0.2333442184560905
X	0.01048634637596676	1619	1.619	0.18640566927752056
X	0.010927608936420993	767	0.767	0.24242454745184405
X	0.010971674524114327	1219	1.219	0.2080126451737979
X	0.013065447615055872	8576	8.576	0.11506586507755796
X	0.010584073498603346	1249	1.249	0.20387476317202807
X	0.011340502909977603	2082	2.082	0.17594780897324708
X	0.010870378562617683	465	0.465	0.2859327526995404
X	0.011217014811442902	2673	2.673	0.16129693374422302
X	0.0102952213028116	441	0.441	0.28580228654307865
X	0.01085795309439012	1486	1.486	0.1940482441002962
X	0.011151250864490352	732	0.732	0.24789675109608053
X	0.011032175512366469	755	0.755	0.24447703525100112
X	0.010257471566897283	482	0.482	0.27711780338663183
X	0.010579903100016599	713	0.713	0.24573332291006683
X	0.011036622065084052	1372	1.372	0.20036753400200139
X	0.011165969797554213	1675	1.675	0.18820329059659816
X	0.010197868254175825	998	0.998	0.21699994240623408
X	0.011335691290944932	1959	1.959	0.17953034192984998
X	0.013186337208722027	5492	5.492	0.13390533582357367
X	0.011560547148616693	2898	2.898	0.15859640403862257
X	0.010914792047386226	1201	1.201	0.20868480096269024
X	0.010997209138580698	966	0.966	0.2249581780413309
X	0.012416936824175948	813	0.813	0.24810830891532895
X	0.011000098690908816	1015	1.015	0.22129767259707267
X	0.010710763336675333	749	0.749	0.24272408273150442
X	0.010802392873916001	821	0.821	0.23607985894841257
X	0.011628496255674015	1159	1.159	0.21568173765932924
X	0.011318524906123617	2018	2.018	0.1776735949239257
X	0.011971022363075942	2860	2.86	0.16115918767693715
X	0.010724812065524482	1510	1.51	0.1922225117285313
X	0.01084548654137949	4874	4.874	0.1305532952887821
X	0.01116621563636563	1758	1.758	0.18519491019605686
X	0.010979716458113898	1010	1.01	0.22152525767596715
X	0.013100871646155145	3937	3.937	0.1492948839864967
X	0.010829311157764185	318	0.318	0.3241339866600286
X	0.012725586008663829	1999	1.999	0.1853339572180439
X	0.010979016218144259	1168	1.168	0.21104425187271658
X	0.013245641905187814	1258	1.258	0.219178250592063
X	0.010162161445120003	496	0.496	0.2736327895650454
X	0.010726149069446519	1021	1.021	0.21901441269996352
X	0.010442637421673038	651	0.651	0.25219883158466966
X	0.01245024514902584	2235	2.235	0.17726927090318575
X	0.010944207974257021	1765	1.765	0.18371581450198998
X	0.010998653003997218	2515	2.515	0.16353115486917358
X	0.010440266257175455	608	0.608	0.25798987678648533
X	0.01054941721599107	978	0.978	0.22095128817077292
X	0.01056339986257878	1031	1.031	0.21719426396093303
X	0.01253653754930434	5297	5.297	0.13326493064821643
X	0.011440236007182001	1383	1.383	0.20224180588610705
X	0.012483109131963707	2415	2.415	0.17290274070973558
X	0.01136057345694114	992	0.992	0.22540464458224657
X	0.011076325446368395	2592	2.592	0.16227572951518782
X	0.0110352061950073	1949	1.949	0.17823358301820252
X	0.010983372184531109	3020	3.02	0.15378344508612685
X	0.010452282162428111	1743	1.743	0.1816789366513315
X	0.012097130554834778	1191	1.191	0.21656596356153032
X	0.010534528136561104	1031	1.031	0.21699620604033767
X	0.010996022089557342	947	0.947	0.22644455114464174
X	0.010519588112994027	1711	1.711	0.18319610765934322
X	0.012308041492197396	3497	3.497	0.1521123373667336
X	0.011331606780707439	1988	1.988	0.17863163440120705
X	0.0109631426010483	934	0.934	0.22726334554483324
X	0.011326521257561693	1876	1.876	0.18209075786473883
X	0.01224963789919503	1926	1.926	0.18527734673151353
X	0.012828540074255233	2971	2.971	0.162838912923356
X	0.010808216097836635	880	0.88	0.2307227657257264
X	0.010147682617861174	532	0.532	0.2671889015019178
X	0.011022803850167203	1997	1.997	0.17672771873394166
X	0.011064775845005101	1072	1.072	0.21772883307196872
X	0.010979606824438102	833	0.833	0.23621851176591221
X	0.013293794921169656	1663	1.663	0.19994884902147597
X	0.010868298751726048	1242	1.242	0.20606931338104184
X	0.010676826981589891	372	0.372	0.30617267014351474
X	0.011070483799499781	1378	1.378	0.20028071249146978
X	0.011220155011004299	547	0.547	0.27373931544753244
X	0.012727704627296698	1968	1.968	0.18631235897741313
X	0.011110635651447695	977	0.977	0.22487859236620053
X	0.01350933882663267	3585	3.585	0.15561387287192255
X	0.01124024271916376	1654	1.654	0.18941457536877374
X	0.010979192854126562	1771	1.771	0.18370343799766709
X	0.010898383398728935	608	0.608	0.2617095079574255
X	0.011105752751733792	880	0.88	0.23282079942770226
X	0.01095652296261509	1029	1.029	0.21999819932972078
X	0.011474928922157143	1913	1.913	0.18169585141722042
X	0.010821602963765414	2615	2.615	0.1605486411912855
X	0.010989681646101002	1595	1.595	0.19028657158762313
X	0.01090102664540521	1338	1.338	0.20121966853178083
X	0.010956279591938296	1560	1.56	0.1915047289238312
X	0.011749421721367332	2164	2.164	0.17575987913148633
X	0.010745457548177979	399	0.399	0.2997441199566978
X	0.011085766044176977	951	0.951	0.22674013314850228
X	0.01072025131009632	672	0.672	0.2517359443592088
X	0.010103726412548211	354	0.354	0.3056042844884769
X	0.011130565314522903	5534	5.534	0.12622906225190222
X	0.011039588481184241	917	0.917	0.2291893940205433
X	0.010840923548448601	1878	1.878	0.17938669612250904
X	0.011285133523758878	846	0.846	0.23716218686166687
X	0.010868796179631681	2820	2.82	0.15678726298546034
X	0.011244877917291483	946	0.946	0.22822043883749377
X	0.011213370300902173	1382	1.382	0.20094446143317837
X	0.011037746675160488	2175	2.175	0.1718463796193004
X	0.0110063873320928	696	0.696	0.25100277189593057
X	0.011063975993255431	987	0.987	0.22380237486126972
X	0.011728451112141889	2062	2.062	0.17850511574386257
X	0.010813820958234842	501	0.501	0.2784285824588389
X	0.010973915493167223	688	0.688	0.25172386485490694
X	0.010937567590677218	1194	1.194	0.20923715567147524
X	0.01221689000835585	1450	1.45	0.2034842909559475
X	0.012929111531328322	2156	2.156	0.18167979943795334
X	0.010404959794210198	685	0.685	0.2476566220296186
X	0.01098572926137346	807	0.807	0.23877294666825902
X	0.011279422064913001	1521	1.521	0.19500805226050447
X	0.012787635865870258	3461	3.461	0.1545952765767805
X	0.011108472928396193	854	0.854	0.23517915742894874
X	0.011116783055848749	3242	3.242	0.15079561285748494
X	0.011210343628483584	7660	7.66	0.11353507517092376
X	0.012469868217448556	6396	6.396	0.12492551148441562
X	0.011280816336351028	3004	3.004	0.15543430180617168
X	0.009839387400606484	301	0.301	0.3197426142771403
X	0.01095338363123965	2124	2.124	0.17276851303105586
X	0.0111272240965976	9579	9.579	0.1051208556179499
X	0.010734650841662205	18906	18.906	0.08280618117783761
X	0.0109873681347705	1827	1.827	0.18185212328224304
X	0.011081673692698461	11318	11.318	0.09929907953628594
X	0.010728856879159612	16055	16.055	0.08742743037635713
X	0.010986817863807544	4929	4.929	0.1306284285261882
X	0.012932259412631585	286460	286.46	0.03560711659878799
X	0.0108504348886976	1019	1.019	0.2200008305571123
X	0.011130958111680168	3056	3.056	0.15386023786051284
X	0.011703739919374668	81921	81.921	0.05227689754248154
X	0.011370995280810998	376327	376.327	0.0311466278378439
X	0.01116444736472636	2877	2.877	0.157144477354561
X	0.01241100143039596	8488	8.488	0.11350120830048939
X	0.011212391235601579	3844	3.844	0.14288005917751398
X	0.012595306819328154	23060	23.06	0.08174285764302242
X	0.011016341157472248	14770	14.77	0.09068851569674093
X	0.012913096420800647	42259	42.259	0.06735508564048985
X	0.01221028273853846	18908	18.908	0.08643574421065708
X	0.011264713168919083	8834	8.834	0.10843951861104083
X	0.0108738119063022	3803	3.803	0.14193358212639434
X	0.011768742743891952	24477	24.477	0.07834127353156585
X	0.010715770645647847	2607	2.607	0.1601870706506402
X	0.010277871544798516	1118	1.118	0.20948541620444797
X	0.011268238986359777	37843	37.843	0.06677647614084858
X	0.010684112564613381	5291	5.291	0.12639606817461177
X	0.01182047260242421	70335	70.335	0.05518500882120035
X	0.011262771981103916	77733	77.733	0.05252264889434579
X	0.010777133890288716	958	0.958	0.22406770009074975
X	0.01119960660503013	3643	3.643	0.1454056221340839
X	0.012568079794184313	3825	3.825	0.1486662875948655
X	0.011108095102711654	4031	4.031	0.14019814381461884
X	0.012634365203723843	71793	71.793	0.05603900835804301
X	0.010921685161785329	4455	4.455	0.13483852140658323
X	0.011160431649717192	30276	30.276	0.0717013773749112
X	0.010743748841626844	506	0.506	0.2769077495664963
X	0.011114078494444382	2734	2.734	0.15959709541878286
X	0.011332275949845797	1944	1.944	0.17997283598835367
X	0.012174841383971765	101990	101.99	0.04923817036794013
X	0.01132003488135255	15010	15.01	0.09102395274204714
X	0.011291570572248	2151	2.151	0.17379528136355604
X	0.01139594623198156	161526	161.526	0.04132085675524027
X	0.011540933825086043	12390	12.39	0.09766146051448774
X	0.011259784421835284	14283	14.283	0.09237835260582422
X	0.012848707444837707	46241	46.241	0.06525454128092052
X	0.011104317283576715	2671	2.671	0.16079504372550718
X	0.011106245655280487	18978	18.978	0.08364479946937775
X	0.01185823624659797	13710	13.71	0.0952783471384163
X	0.011139620560035467	63140	63.14	0.05608610407071011
X	0.012594301376217792	4614	4.614	0.13975440228890443
X	0.010981240984541521	4370	4.37	0.13595337557108997
X	0.01081895091236583	10654	10.654	0.10051344369527047
X	0.010972484735874479	3178	3.178	0.1511414893673842
X	0.011387840131216189	5640	5.64	0.1263923394769466
X	0.011903867653263081	6561	6.561	0.12196616000687811
X	0.011294676718710959	11819	11.819	0.09849881916551465
X	0.013031204784746743	65217	65.217	0.0584621232474768
X	0.011457121821250091	11796	11.796	0.09903307305343903
X	0.011343150543656004	2170	2.17	0.17355001440007653
X	0.01252013673931373	3725	3.725	0.14979394721191594
X	0.01145480927587488	21926	21.926	0.0805396029481938
X	0.011449961918306733	12962	12.962	0.09594978554176348
X	0.010643007869336385	2746	2.746	0.1570801413688441
X	0.013647253183064222	3135	3.135	0.16328109385697356
X	0.011617250009564557	30253	30.253	0.07268502811759767
X	0.010963879090711006	2852	2.852	0.1566528277539747
X	0.012703116144647494	310376	310.376	0.034462018588313915
X	0.01137212322685065	2006	2.006	0.1783077457303292
X	0.012232240104989094	4293	4.293	0.14176913936503036
X	0.011372515742868405	12660	12.66	0.09648821681389841
X	0.011254375459812224	3578	3.578	0.14651891292057023
X	0.011381160440526499	26805	26.805	0.07516060109388074
X	0.01120200430529928	2697	2.697	0.1607452923751986
X	0.011340590899061769	63684	63.684	0.056260266290246136
X	0.012892195261508338	85695	85.695	0.05318515923242694
X	0.011454879832303469	2553	2.553	0.16493515803334843
X	0.011073096875959162	23608	23.608	0.0776968301565478
X	0.011579422816256802	63390	63.39	0.056739920134212966
X	0.013686982877237403	150580	150.58	0.04496212310614555
X	0.011589839402448126	60824	60.824	0.057544101527592345
X	0.011222406868176932	1344	1.344	0.2028751779436784
X	0.011049721021495465	7134	7.134	0.11570161013676482
X	0.011391969681945452	7515	7.515	0.11487440594660214
X	0.011053235142203426	90051	90.051	0.049697394799562554
X	0.011013716825391172	4987	4.987	0.1302261434062345
X	0.011008213535947681	1501	1.501	0.19428773457897075
X	0.011932091644233212	79504	79.504	0.05314256759684941
X	0.01127545305719869	99624	99.624	0.04837155052647222
X	0.010767730608646291	21447	21.447	0.07947902728412881
X	0.010840468234128608	4647	4.647	0.1326249163210752
X	0.011030803271820324	5478	5.478	0.12627819642981036
X	0.012598846787589246	3800	3.8	0.1491130771848884
X	0.011943755414066068	2918	2.918	0.15996263073611872
X	0.011548331433400302	4969	4.969	0.1324595923799108
X	0.011058509050513171	3668	3.668	0.14446270755005325
X	0.010883422627722295	19844	19.844	0.08185504003579244
X	0.011438913023040403	20229	20.229	0.08269326020876372
X	0.012025605142740212	14551	14.551	0.09384360864268869
X	0.012310595491541903	5632	5.632	0.12977931325681621
X	0.011025786882486016	2162	2.162	0.17212791231927344
X	0.011410270889931774	396108	396.108	0.030654491315329182
X	0.011215971795844137	3050	3.05	0.15435203383980525
X	0.011128998604455901	52980	52.98	0.05944491512811626
X	0.011302276214859002	1311	1.311	0.20504746683100825
X	0.011833522207835281	1700	1.7	0.19093624158063952
X	0.011089417809250247	1303	1.303	0.2041682048397571
X	0.013076109670347925	10951	10.951	0.1060901162121797
X	0.013398454895154116	1780	1.78	0.19597991821194893
X	0.010914117165944363	1540	1.54	0.19208316229584999
X	0.011017252519481177	1603	1.603	0.19012822502817883
X	0.012562316241424528	1507	1.507	0.20276149743351282
X	0.013186174231527768	229584	229.584	0.038582680553093614
X	0.011327210522258739	45175	45.175	0.06305820493984633
X	0.011124015341674318	6175	6.175	0.12167691939013402
X	0.01212843956933687	28242	28.242	0.07544604760729923
X	0.011091134835553535	3867	3.867	0.1420803236283265
X	0.01068453375974108	1609	1.609	0.1879604648909827
X	0.010952275936926818	3708	3.708	0.14347895905899077
X	0.011115163610230794	4090	4.09	0.13955032445595283
X	0.010956553540849383	611	0.611	0.261744512262761
X	0.012143222193154703	3444	3.444	0.15220313560925533
X	0.012245925674535925	3812	3.812	0.14755238795259565
X	0.011012194046183931	2295	2.295	0.16866710413558925
X	0.011673310008715672	2069	2.069	0.17802370811718382
X	0.010941636610714841	2029	2.029	0.1753611614968511
X	0.011941507859980369	2265	2.265	0.17404584273071114
X	0.011055317748011856	174568	174.568	0.039859838161133844
X	0.010877432938140615	5252	5.252	0.12746782213172386
X	0.012163193875043908	11954	11.954	0.10057996015620953
X	0.011209851211336089	2926	2.926	0.15647387656979375
X	0.010997237089404957	28174	28.174	0.073082213484343
X	0.01086721940581514	1867	1.867	0.17988351798796928
X	0.012182080794512943	496837	496.837	0.029051541513811084
X	0.011463809646581837	311020	311.02	0.03327976235113942
X	0.011468498824972497	17130	17.13	0.0874815368154166
X	0.011364675916105347	8502	8.502	0.11015697085108364
X	0.010368343628914761	1501	1.501	0.19044791831801408
X	0.010986368362214421	10317	10.317	0.10211751596915239
X	0.011264198287996905	4360	4.36	0.13721595077844786
X	0.011405345484831362	38511	38.511	0.06665630207759017
X	0.011061633119093174	2267	2.267	0.16961174020387185
X	0.011970128329758814	58246	58.246	0.05901252990455967
X	0.011244723892121047	62430	62.43	0.056474425334085136
X	0.010825129571665548	15057	15.057	0.08958422707451268
X	0.012535920846133825	5432	5.432	0.13214949058326597
X	0.013631857137548047	3203	3.203	0.16205634106889366
X	0.01265274468157384	5311	5.311	0.13355786901430203
X	0.011308120627940768	1923	1.923	0.1804971640844995
X	0.01197152635950615	63619	63.619	0.057304346289356316
X	0.010929064486924024	21944	21.944	0.07926638724616875
X	0.010857931572627037	1827	1.827	0.18113519835426967
X	0.011407041115997047	62853	62.853	0.05661727964065441
X	0.01357247032597469	5775	5.775	0.13295427436467178
X	0.011049496478302126	737	0.737	0.2465802754124996
X	0.011271484129008877	8507	8.507	0.10983351697461624
X	0.011444837678158112	69226	69.226	0.0548841319544133
X	0.012818049388153376	35927	35.927	0.07092516076426937
X	0.012716825262817446	102625	102.625	0.049854975967453995
X	0.011113933413154411	51556	51.556	0.05996017041255728
X	0.01126980598600317	3715	3.715	0.1447613219798059
X	0.011266091584800398	28985	28.985	0.07297934999184637
X	0.011432327018957498	79074	79.074	0.052484811708768425
X	0.011363460016892648	64418	64.418	0.0560834149739381
X	0.011018830396603601	3098	3.098	0.15264574223921168
X	0.012313427092469461	70574	70.574	0.055878499653245296
X	0.011452113481525698	129432	129.432	0.0445602884661883
X	0.011438420438042	34042	34.042	0.06952115778478227
X	0.01290952654292372	24737	24.737	0.08051085311662921
X	0.011080921344215039	1399	1.399	0.19933614382726317
X	0.01116995885813418	5836	5.836	0.12415915934969692
X	0.010990339446819626	2144	2.144	0.17242309710652995
X	0.010724647752083016	1548	1.548	0.19063560929816484
X	0.01139224859750808	104015	104.015	0.04784513966461493
X	0.011399375525333518	58938	58.938	0.0578311580178973
X	0.011099837413150652	75017	75.017	0.05289147052040268
X	0.011866311734177206	15390	15.39	0.09169782707282052
X	0.011303591952220207	29102	29.102	0.07296219290445304
X	0.01120632902259936	9641	9.641	0.1051430156024426
X	0.011371560793585901	23295	23.295	0.07873825533654029
X	0.010802449145517604	631	0.631	0.2577295944712026
X	0.010920950689306104	5608	5.608	0.12487748600766573
X	0.011995912189275993	83277	83.277	0.0524206902804534
X	0.011127844925777508	4001	4.001	0.1406309264485852
X	0.011514719243428163	2420	2.42	0.16819483781612346
X	0.01102853580546475	5711	5.711	0.12452844521023855
X	0.012729747845106623	127160	127.16	0.046432609753111156
X	0.011343063569345183	20658	20.658	0.08188682470514519
X	0.012336196639671573	19948	19.948	0.08519758960622381
X	0.010713828967689566	16146	16.146	0.08722210969233846
X	0.011047886029075502	18352	18.352	0.08443683155294712
X	0.011040594225880848	1076	1.076	0.21730016649865744
X	0.011503029212171448	907	0.907	0.23320345401601666
X	0.01127731577290575	21172	21.172	0.08106149677929643
X	0.011019454653977905	10111	10.111	0.10290946740358518
X	0.010925764704133251	1894	1.894	0.17934555729243462
X	0.011426020695031913	134049	134.049	0.04400923397253241
X	0.011307083525955273	1466	1.466	0.19757813272489877
X	0.011130246897056642	10028	10.028	0.10353729734143337
X	0.01130885927557939	3720	3.72	0.14486338197057544
X	0.012359738070037502	44386	44.386	0.06530118857479715
X	0.012148248188752266	3665	3.665	0.14910077207704708
X	0.01130794965639012	9738	9.738	0.10510854688127258
X	0.010842412480762247	5138	5.138	0.12826572150438953
X	0.013452208903603774	10340	10.34	0.10916691094471649
X	0.011354538587848462	15099	15.099	0.09093696071143711
X	0.011369953016557622	313882	313.882	0.03308751005561054
X	0.012015783147272847	3323	3.323	0.15348781848653076
X	0.010977481904234001	2923	2.923	0.15543827626485585
X	0.012424422094971365	66444	66.444	0.05718386109707374
X	0.011319083656263202	4951	4.951	0.13173649806151702
X	0.012739849117224741	97887	97.887	0.05067725568959397
X	0.010868927454195041	859	0.859	0.23302249181924972
X	0.011053297333915298	45736	45.736	0.06228898075529799
X	0.011434441969302813	90644	90.644	0.0501524288971356
X	0.010907335556827522	1540	1.54	0.1920433697104049
X	0.011271623645100001	216671	216.671	0.03733033585611538
X	0.01142782332008259	2929	2.929	0.1574277838746581
X	0.013710926746233242	32663	32.663	0.07487501341541963
X	0.01136383940844829	4669	4.669	0.13451387112355565
X	0.012624924780763705	115176	115.176	0.04785811649852952
X	0.011033353039462913	1350	1.35	0.2014301991344874
X	0.011652625752724024	1627	1.627	0.19275785366320347
X	0.011814774761209176	2445	2.445	0.16906311282859388
X	0.011363672296314955	14868	14.868	0.09142999925560469
X	0.011124005331600444	21124	21.124	0.0807535598741431
X	0.011980293267265352	16440	16.44	0.08998881757890458
X	0.011362511741447017	7727	7.727	0.11371588725858676
X	0.011283255602243065	1885	1.885	0.18156871665958524
X	0.011218922644595043	6508	6.508	0.11990423882011132
X	0.012683611477172339	71852	71.852	0.05609636105576192
X	0.012680771982959188	10268	10.268	0.10728852577765353
X	0.01219848920215298	114459	114.459	0.04741168237587686
X	0.011388833620390459	13525	13.525	0.09443087013848235
X	0.011356440884198403	8120	8.12	0.11183096481475133
X	0.011046028570236	1234	1.234	0.20763329731994107
X	0.011311327495645316	3376	3.376	0.14963632935842702
X	0.012564761377261733	341583	341.583	0.033257224253434185
X	0.012289180616507677	2952	2.952	0.16086770202207404
X	0.011339840972364424	5359	5.359	0.1283830637996216
X	0.011043314889315136	43460	43.46	0.06333881169568299
X	0.011421055563292232	28100	28.1	0.07407414400198586
X	0.01133719952983445	1671	1.671	0.18931126030685208
X	0.01129732662495751	1321	1.321	0.20449889037522243
X	0.010964815491530699	13314	13.314	0.09373409525317443
X	0.011342875275197731	8286	8.286	0.11103487531871911
X	0.010443959338605951	3899	3.899	0.13887923397610114
X	0.011386717278324297	13754	13.754	0.09389803542860291
X	0.012772834217071385	11736	11.736	0.1028621801370481
X	0.013396103812424351	12337	12.337	0.10278340003897889
X	0.012974399457001821	117484	117.484	0.04797734158160607
X	0.011433697808330232	34427	34.427	0.06925149872003374
X	0.01150709699030366	2591	2.591	0.16437387251206648
X	0.011458296589558034	43069	43.069	0.06431591073207989
X	0.01134112780523384	2964	2.964	0.15640813891719507
X	0.01111375337010728	3629	3.629	0.14521938579659283
X	0.01092824033882795	11335	11.335	0.09878922332793982
X	0.012876244296677301	3629	3.629	0.1525225578350978
X	0.011449969126328305	60610	60.61	0.057379048722987935
X	0.012168228312358198	4675	4.675	0.13755675163486164
X	0.010875760600843918	1113	1.113	0.21379037955387017
X	0.011059131409198555	7551	7.551	0.11356352105462388
X	0.011425792736502812	6335	6.335	0.12172503483304202
X	0.01110529372177824	11872	11.872	0.0977992191327359
X	0.0113541546621195	39342	39.342	0.0660844769899378
X	0.01092940728560078	2496	2.496	0.16360026762000818
X	0.010857557848142668	1667	1.667	0.1867521100075809
X	0.011858198159124813	4701	4.701	0.1361265435898301
X	0.011350813930218969	1565	1.565	0.193569778992843
X	0.010661691396581748	4092	4.092	0.1376037155769772
X	0.01140002864952856	10234	10.234	0.10366214156206781
X	0.011437113477909077	71645	71.645	0.05424714016517395
X	0.013700948977620435	3975	3.975	0.1510559507437721
X	0.01092865439439388	1403	1.403	0.1982301336156442
X	0.010825783270183777	1496	1.496	0.19342350327663269
X	0.011427422896365622	35692	35.692	0.06841097894516744
X	0.013759931826141469	303019	303.019	0.03567649167498546
X	0.010469448359812776	13270	13.27	0.09240262140900436
X	0.01253506818927742	2432	2.432	0.17273792965497756
X	0.011915676365678628	5348	5.348	0.13060980072386896
X	0.01371312493402345	105551	105.551	0.0506475015992783
X	0.010756071720256681	1255	1.255	0.20464602866065232
X	0.0110398270837512	4630	4.63	0.1335960937971171
X	0.011417153860512503	12716	12.716	0.09647225827373596
X	0.01170520876463128	2824	2.824	0.16063425267755335
X	0.011357381290480341	22091	22.091	0.08011016203700777
X	0.011462069172581675	34246	34.246	0.06943062226134
X	0.011966602183391241	7305	7.305	0.11788273710282818
X	0.01076947832631781	3454	3.454	0.1460912184022001
X	0.010586993012244004	1950	1.95	0.17575701646937603
X	0.010920970007804934	17833	17.833	0.08492044636656639
X	0.011161650753152388	21746	21.746	0.08006628569486857
X	0.011219538944971217	35960	35.96	0.0678242670356343
X	0.011986819375500508	20227	20.227	0.08399578545078766
X	0.013033047620614954	146151	146.151	0.04467673189567811
X	0.01104459076905051	2459	2.459	0.16499230886781813
X	0.010973418541114813	1611	1.611	0.18956091712939166
X	0.011329575591519769	4218	4.218	0.13900668569148325
X	0.012571036713410472	48478	48.478	0.0637688815820217
X	0.010658538483149608	20805	20.805	0.08001596473812828
X	0.010987960359482725	2123	2.123	0.1729772665464092
X	0.01178599382055882	1976	1.976	0.18135370100220466
X	0.011168570330140534	21137	21.137	0.08084467360367335
X	0.010967493881157253	1260	1.26	0.20570536494369052
X	0.010910551887179964	2851	2.851	0.1564167173272439
X	0.011017002017465923	2309	2.309	0.16835001729805102
X	0.012991431304123975	8821	8.821	0.11377488181313558
X	0.01141823502637002	3774	3.774	0.14463243370652834
X	0.011356051844144371	3149	3.149	0.15335039605845252
X	0.01059497488898262	1894	1.894	0.17751701466907038
X	0.011461359243062704	64253	64.253	0.05629211571994747
X	0.011088733069321113	70737	70.737	0.05391940577095405
X	0.011458385755325144	119510	119.51	0.04576917288875912
X	0.01303538323197302	136308	136.308	0.04572995977436196
X	0.011400602627036358	22237	22.237	0.08003572268716626
X	0.010989254502403477	3557	3.557	0.1456447346884351
X	0.010753598786849311	3002	3.002	0.15300808524340992
X	0.01203044202686749	70926	70.926	0.05535523586923235
X	0.012872409704045833	3338	3.338	0.15681628960897911
X	0.012508401699259004	14787	14.787	0.09457446989229411
X	0.01162479749678873	4732	4.732	0.13493156372230608
X	0.013174703395857601	130207	130.207	0.04659816748054704
X	0.011455173875640863	43713	43.713	0.06399268867349903
X	0.013286276002806081	5728	5.728	0.13237320586876516
X	0.01258038720081307	51021	51.021	0.06270685415856857
X	0.012075357802101	21587	21.587	0.08239532520456248
X	0.011385123848080944	15605	15.605	0.090023935648921
X	0.013038353934884457	77950	77.95	0.05509796373497408
X	0.012686739162748723	49948	49.948	0.06333016739538949
X	0.01130829996470125	36902	36.902	0.06741902566837275
X	0.011056250150623123	7353	7.353	0.11456389310755336
X	0.012891910577589039	75571	75.571	0.05546095860387603
X	0.011429393290650718	5917	5.917	0.12453952433705405
X	0.011685366069467157	29980	29.98	0.07304719849741025
X	0.012893918787370251	336246	336.246	0.0337216850531518
X	0.01096913230382833	11878	11.878	0.09738146817933367
X	0.011654883572785995	2339	2.339	0.1708018508712304
X	0.011618347997876479	5524	5.524	0.12812392568486006
X	0.011097335218152479	6115	6.115	0.12197591403445081
X	0.012566753940860508	6169	6.169	0.12676599816848022
X	0.01109689607317564	3053	3.053	0.15375346250528993
X	0.011083706711924969	7641	7.641	0.11319961293785988
X	0.013042847629304988	158049	158.049	0.04353717272253202
X	0.010930421499263498	1207	1.207	0.20843783158840695
X	0.01135737764886394	325089	325.089	0.032690778583051645
X	0.01168320187239651	8881	8.881	0.10957210464731734
X	0.011370185069961243	17961	17.961	0.085864241979023
X	0.011282110334382036	4971	4.971	0.13141618630806376
X	0.012981168847225095	34117	34.117	0.07246266814373846
X	0.011677912836872764	110416	110.416	0.047290917483413186
X	0.011092436732406404	1360	1.36	0.20129331432719638
X	0.01144601670445999	356791	356.791	0.03177460508120792
X	0.011419846983330836	23377	23.377	0.0787572426283741
X	0.012737473390431492	9821	9.821	0.10905423265139691
X	0.01133302555108896	3294	3.294	0.15096424210355291
X	0.011072247762240731	4765	4.765	0.1324517284704813
X	0.010936081871645001	2118	2.118	0.17284040118640354
X	0.010987923567709102	921	0.921	0.22849954162327038
X	0.011318486570094264	4231	4.231	0.13881885081150744
X	0.01174170657594806	31919	31.919	0.07165183472748886
X	0.013113312190978398	605690	605.69	0.027871092910663503
X	0.011073052653163688	42804	42.804	0.06371783386423321
X	0.01111301844559442	14550	14.55	0.0914091521266091
X	0.011911611319294232	3510	3.51	0.1502753676341883
X	0.011265733823450374	22748	22.748	0.07911741493427482
X	0.010758065672447754	1475	1.475	0.19393106848629538
X	0.01110771747666	29892	29.892	0.07189355355799186
X	0.011326077327436875	4830	4.83	0.1328548654447905
X	0.01369798440327274	481115	481.115	0.03053532804264685
X	0.012814228862747773	2733	2.733	0.16737256875123027
X	0.011522329032845913	29646	29.646	0.07297790462032629
X	0.012139018462144829	6180	6.18	0.12523672409547926
X	0.011869563872856013	25878	25.878	0.07712017702000076
X	0.012916851136080277	242334	242.334	0.037634033559957696
X	0.01103877964717176	15163	15.163	0.0899591430572875
X	0.011037567900177288	2672	2.672	0.16045218840877967
X	0.01310776537689078	30276	30.276	0.0756501954107701
X	0.012142120923053943	4008	4.008	0.1446956961554488
X	0.01108519591581084	2046	2.046	0.175635613166116
X	0.011357582145947747	50293	50.293	0.06089656295741989
X	0.011454461020855506	317234	317.234	0.03305204310498882
X	0.013466388732829352	9789	9.789	0.11121693941012752
X	0.011295951185882691	25700	25.7	0.07603202566184844
X	0.011178591778008758	48097	48.097	0.061483186746923255
X	0.012257476916335264	9369	9.369	0.10937110880843273
X	0.011053487766549672	18447	18.447	0.08430587929950792
X	0.011759039416185753	2449	2.449	0.1687048962047941
X	0.010897752014561902	99359	99.359	0.04786777449242551
X	0.011874487794194309	34605	34.605	0.07000977425482474
X	0.011198261823657478	54591	54.591	0.05897616417847618
X	0.01117705039766584	6587	6.587	0.11927420080228009
X	0.01107816179834422	4915	4.915	0.13111369616145518
X	0.010848519660585566	8419	8.419	0.10881864745927552
X	0.011148162421155162	258417	258.417	0.03507201641155117
X	0.011259049217407628	38809	38.809	0.06619976103689132
X	0.01104263650769068	10709	10.709	0.10102789077379754
X	0.011682620789764684	1382	1.382	0.2037092480429832
X	0.01308458936963957	7849	7.849	0.11857193927314655
X	0.011727100808371565	29757	29.757	0.07331629406881954
X	0.011308634484964402	40458	40.458	0.06538356336684421
X	0.01161506972611757	148351	148.351	0.04278029559350475
X	0.0109995412459424	3395	3.395	0.14797156417075935
X	0.011334635446304144	15070	15.07	0.09094205592821146
X	0.012416595380816187	19181	19.181	0.08650550807468005
X	0.012365914763532004	25604	25.604	0.07845840530741063
X	0.011335453631678281	29511	29.511	0.07269172800416118
X	0.011452940477423634	53297	53.297	0.059896955270056555
X	0.010870046140107004	5484	5.484	0.12561590950378201
X	0.011441075363400829	21294	21.294	0.08129612878242092
X	0.012100211897910662	915	0.915	0.23647793656845376
X	0.01363078614947313	2904	2.904	0.16743314338930437
X	0.01178022973805933	76391	76.391	0.053625397156240914
X	0.011333583787288472	30265	30.265	0.07207901787590924
X	0.011438983944886828	34033	34.033	0.06952842723753316
X	0.011213448833727362	1324	1.324	0.20383733854335706
X	0.012280282639891707	2439	2.439	0.17139527850759806
X	0.011308277728697762	45487	45.487	0.06287862802905973
X	0.010968724733467905	28870	28.87	0.07242743715396796
X	0.011348664107112053	16007	16.007	0.08916852835479407
X	0.01113587616565	1280	1.28	0.20567024925139
X	0.011357937130721953	2043	2.043	0.1771510365898992
X	0.011078865692145253	11005	11.005	0.1002232350240099
X	0.010998944036652693	9742	9.742	0.10412802288430553
X	0.011435922111751127	38384	38.384	0.06678931449982034
X	0.011263205394711505	6266	6.266	0.1215879504412464
X	0.011011443120884251	3081	3.081	0.1528917953686175
X	0.01073007095023328	1909	1.909	0.17780019041949813
X	0.011068244140913414	2236	2.236	0.1704259186429083
X	0.012062986936979016	76566	76.566	0.054009849381672956
X	0.011404598319040688	2974	2.974	0.15652354538986496
X	0.011270883550117006	20932	20.932	0.08135465584392236
X	0.01195031648196133	69700	69.7	0.055554026665720214
X	0.01134687892588355	14248	14.248	0.09269169803215867
X	0.012294440250418082	47837	47.837	0.0635791828093769
X	0.012108731904511965	91079	91.079	0.05103797913538966
X	0.0113092044140738	3013	3.013	0.15540952776692524
X	0.011287146676204842	54487	54.487	0.05916938981897088
X	0.011397327368761192	16414	16.414	0.08855155621050939
X	0.011379045812669082	177540	177.54	0.04001933583753632
X	0.011403636036086191	119999	119.999	0.045634003733782645
X	0.010567755409682597	6935	6.935	0.11507441149305395
X	0.011445132490483605	31991	31.991	0.07099008064901162
X	0.011039445425067825	4527	4.527	0.13460016350451512
X	0.010904502611494704	2182	2.182	0.1709688687847982
X	0.011367270705543569	3155	3.155	0.15330357396530608
X	0.011597569449272082	6784	6.784	0.11957132658082247
X	0.012009609547698894	65706	65.706	0.05675114387061684
X	0.01164175637094258	30177	30.177	0.0727971115821582
X	0.010872680200820425	1869	1.869	0.17984944596805785
X	0.011420249369016635	14907	14.907	0.09150154880271431
X	0.011413044417933277	6556	6.556	0.12029683120245342
X	0.012214258215552002	162293	162.293	0.042220409010286473
X	0.011335322389008641	10107	10.107	0.1038972063749671
X	0.011665776299550535	6591	6.591	0.1209634349631017
X	0.012119497803006472	37755	37.755	0.06847048170405605
X	0.011431299253158849	58961	58.961	0.057877564963900514
X	0.010753093439559568	3080	3.08	0.15170302274934938
X	0.011355334435915604	45016	45.016	0.06318456634324206
X	0.011287056320742592	6010	6.01	0.12337734114485456
time for making epsilon is 1.061983585357666
epsilons are
[0.25237334483968216, 0.11092078479451437, 0.24314722698687535, 0.20440036665883568, 0.09089343392529084, 0.10345178650577416, 0.07481388295579658, 0.07228372940675433, 0.16299418364193521, 0.08220409525616622, 0.14333517986294994, 0.04416338082870883, 0.12552673195358438, 0.0587933748483098, 0.09525702245544256, 0.0771190451403063, 0.0811192481237978, 0.08215804063203902, 0.03194109417334091, 0.062351423314248304, 0.14406512685865502, 0.034937879462827136, 0.09241357788592372, 0.1328289678727166, 0.09496761949747173, 0.06944996297224997, 0.14670308438382726, 0.1381524495792773, 0.0881449310411799, 0.02262520580007638, 0.18874259943003485, 0.05183764900167178, 0.13534697252211433, 0.13809458848930403, 0.17463099442898508, 0.09187861918828198, 0.0724751082083385, 0.2607584361673329, 0.17108068307570218, 0.20878773039136866, 0.15176122278206375, 0.3153010735882999, 0.24775464451366097, 0.15892671983821255, 0.24821592769821632, 0.20860654284016247, 0.16402964311093662, 0.17335718831523242, 0.2365524146364691, 0.15240350907698524, 0.1645458873771702, 0.22068319166760286, 0.176572402905443, 0.2264421226550231, 0.14684514924163172, 0.2156383711123845, 0.17534297862452722, 0.20442957661830743, 0.19972677530809602, 0.20822792264165102, 0.15980461501652596, 0.21488636926544386, 0.17671117752839607, 0.1393635098196466, 0.17564705788900387, 0.22555820166904664, 0.18058764111359496, 0.14729057739515258, 0.1717924355652469, 0.2349885769612531, 0.1579855780033702, 0.17684122379449072, 0.14925870865875143, 0.2307338825621299, 0.20143700246434182, 0.17171915818958558, 0.20589588620330046, 0.3401054194755613, 0.15974426553682816, 0.11920823949707399, 0.15000293632099504, 0.20718538604695833, 0.19132825464362785, 0.21261511341978984, 0.1601199369978736, 0.23940141387873187, 0.18341438268353008, 0.26614291167079585, 0.18841351072857837, 0.20975932708525272, 0.1735326957690354, 0.2377217830359667, 0.24121518942189418, 0.19151078683124015, 0.20972236798280544, 0.17386839532246526, 0.2143984446976549, 0.10845901603744285, 0.18630696451515544, 0.17280593617498488, 0.1482065289475117, 0.19096982030276025, 0.1432804796740955, 0.24301108534194527, 0.18830946185018446, 0.2482321555561001, 0.16066919257309364, 0.29370676693181214, 0.1959002182161367, 0.24781851567842642, 0.24148774733639086, 0.253984374822272, 0.27066072940178476, 0.17055101908453393, 0.17912940648743422, 0.2464703076185887, 0.20154039953407524, 0.20727663570158242, 0.17037723966212562, 0.2293559659278884, 0.1214881242973489, 0.1877454207694119, 0.24516428235737464, 0.20166015242398, 0.2333442184560905, 0.18640566927752056, 0.24242454745184405, 0.2080126451737979, 0.11506586507755796, 0.20387476317202807, 0.17594780897324708, 0.2859327526995404, 0.16129693374422302, 0.28580228654307865, 0.1940482441002962, 0.24789675109608053, 0.24447703525100112, 0.27711780338663183, 0.24573332291006683, 0.20036753400200139, 0.18820329059659816, 0.21699994240623408, 0.17953034192984998, 0.13390533582357367, 0.15859640403862257, 0.20868480096269024, 0.2249581780413309, 0.24810830891532895, 0.22129767259707267, 0.24272408273150442, 0.23607985894841257, 0.21568173765932924, 0.1776735949239257, 0.16115918767693715, 0.1922225117285313, 0.1305532952887821, 0.18519491019605686, 0.22152525767596715, 0.1492948839864967, 0.3241339866600286, 0.1853339572180439, 0.21104425187271658, 0.219178250592063, 0.2736327895650454, 0.21901441269996352, 0.25219883158466966, 0.17726927090318575, 0.18371581450198998, 0.16353115486917358, 0.25798987678648533, 0.22095128817077292, 0.21719426396093303, 0.13326493064821643, 0.20224180588610705, 0.17290274070973558, 0.22540464458224657, 0.16227572951518782, 0.17823358301820252, 0.15378344508612685, 0.1816789366513315, 0.21656596356153032, 0.21699620604033767, 0.22644455114464174, 0.18319610765934322, 0.1521123373667336, 0.17863163440120705, 0.22726334554483324, 0.18209075786473883, 0.18527734673151353, 0.162838912923356, 0.2307227657257264, 0.2671889015019178, 0.17672771873394166, 0.21772883307196872, 0.23621851176591221, 0.19994884902147597, 0.20606931338104184, 0.30617267014351474, 0.20028071249146978, 0.27373931544753244, 0.18631235897741313, 0.22487859236620053, 0.15561387287192255, 0.18941457536877374, 0.18370343799766709, 0.2617095079574255, 0.23282079942770226, 0.21999819932972078, 0.18169585141722042, 0.1605486411912855, 0.19028657158762313, 0.20121966853178083, 0.1915047289238312, 0.17575987913148633, 0.2997441199566978, 0.22674013314850228, 0.2517359443592088, 0.3056042844884769, 0.12622906225190222, 0.2291893940205433, 0.17938669612250904, 0.23716218686166687, 0.15678726298546034, 0.22822043883749377, 0.20094446143317837, 0.1718463796193004, 0.25100277189593057, 0.22380237486126972, 0.17850511574386257, 0.2784285824588389, 0.25172386485490694, 0.20923715567147524, 0.2034842909559475, 0.18167979943795334, 0.2476566220296186, 0.23877294666825902, 0.19500805226050447, 0.1545952765767805, 0.23517915742894874, 0.15079561285748494, 0.11353507517092376, 0.12492551148441562, 0.15543430180617168, 0.3197426142771403, 0.17276851303105586, 0.1051208556179499, 0.08280618117783761, 0.18185212328224304, 0.09929907953628594, 0.08742743037635713, 0.1306284285261882, 0.03560711659878799, 0.2200008305571123, 0.15386023786051284, 0.05227689754248154, 0.0311466278378439, 0.157144477354561, 0.11350120830048939, 0.14288005917751398, 0.08174285764302242, 0.09068851569674093, 0.06735508564048985, 0.08643574421065708, 0.10843951861104083, 0.14193358212639434, 0.07834127353156585, 0.1601870706506402, 0.20948541620444797, 0.06677647614084858, 0.12639606817461177, 0.05518500882120035, 0.05252264889434579, 0.22406770009074975, 0.1454056221340839, 0.1486662875948655, 0.14019814381461884, 0.05603900835804301, 0.13483852140658323, 0.0717013773749112, 0.2769077495664963, 0.15959709541878286, 0.17997283598835367, 0.04923817036794013, 0.09102395274204714, 0.17379528136355604, 0.04132085675524027, 0.09766146051448774, 0.09237835260582422, 0.06525454128092052, 0.16079504372550718, 0.08364479946937775, 0.0952783471384163, 0.05608610407071011, 0.13975440228890443, 0.13595337557108997, 0.10051344369527047, 0.1511414893673842, 0.1263923394769466, 0.12196616000687811, 0.09849881916551465, 0.0584621232474768, 0.09903307305343903, 0.17355001440007653, 0.14979394721191594, 0.0805396029481938, 0.09594978554176348, 0.1570801413688441, 0.16328109385697356, 0.07268502811759767, 0.1566528277539747, 0.034462018588313915, 0.1783077457303292, 0.14176913936503036, 0.09648821681389841, 0.14651891292057023, 0.07516060109388074, 0.1607452923751986, 0.056260266290246136, 0.05318515923242694, 0.16493515803334843, 0.0776968301565478, 0.056739920134212966, 0.04496212310614555, 0.057544101527592345, 0.2028751779436784, 0.11570161013676482, 0.11487440594660214, 0.049697394799562554, 0.1302261434062345, 0.19428773457897075, 0.05314256759684941, 0.04837155052647222, 0.07947902728412881, 0.1326249163210752, 0.12627819642981036, 0.1491130771848884, 0.15996263073611872, 0.1324595923799108, 0.14446270755005325, 0.08185504003579244, 0.08269326020876372, 0.09384360864268869, 0.12977931325681621, 0.17212791231927344, 0.030654491315329182, 0.15435203383980525, 0.05944491512811626, 0.20504746683100825, 0.19093624158063952, 0.2041682048397571, 0.1060901162121797, 0.19597991821194893, 0.19208316229584999, 0.19012822502817883, 0.20276149743351282, 0.038582680553093614, 0.06305820493984633, 0.12167691939013402, 0.07544604760729923, 0.1420803236283265, 0.1879604648909827, 0.14347895905899077, 0.13955032445595283, 0.261744512262761, 0.15220313560925533, 0.14755238795259565, 0.16866710413558925, 0.17802370811718382, 0.1753611614968511, 0.17404584273071114, 0.039859838161133844, 0.12746782213172386, 0.10057996015620953, 0.15647387656979375, 0.073082213484343, 0.17988351798796928, 0.029051541513811084, 0.03327976235113942, 0.0874815368154166, 0.11015697085108364, 0.19044791831801408, 0.10211751596915239, 0.13721595077844786, 0.06665630207759017, 0.16961174020387185, 0.05901252990455967, 0.056474425334085136, 0.08958422707451268, 0.13214949058326597, 0.16205634106889366, 0.13355786901430203, 0.1804971640844995, 0.057304346289356316, 0.07926638724616875, 0.18113519835426967, 0.05661727964065441, 0.13295427436467178, 0.2465802754124996, 0.10983351697461624, 0.0548841319544133, 0.07092516076426937, 0.049854975967453995, 0.05996017041255728, 0.1447613219798059, 0.07297934999184637, 0.052484811708768425, 0.0560834149739381, 0.15264574223921168, 0.055878499653245296, 0.0445602884661883, 0.06952115778478227, 0.08051085311662921, 0.19933614382726317, 0.12415915934969692, 0.17242309710652995, 0.19063560929816484, 0.04784513966461493, 0.0578311580178973, 0.05289147052040268, 0.09169782707282052, 0.07296219290445304, 0.1051430156024426, 0.07873825533654029, 0.2577295944712026, 0.12487748600766573, 0.0524206902804534, 0.1406309264485852, 0.16819483781612346, 0.12452844521023855, 0.046432609753111156, 0.08188682470514519, 0.08519758960622381, 0.08722210969233846, 0.08443683155294712, 0.21730016649865744, 0.23320345401601666, 0.08106149677929643, 0.10290946740358518, 0.17934555729243462, 0.04400923397253241, 0.19757813272489877, 0.10353729734143337, 0.14486338197057544, 0.06530118857479715, 0.14910077207704708, 0.10510854688127258, 0.12826572150438953, 0.10916691094471649, 0.09093696071143711, 0.03308751005561054, 0.15348781848653076, 0.15543827626485585, 0.05718386109707374, 0.13173649806151702, 0.05067725568959397, 0.23302249181924972, 0.06228898075529799, 0.0501524288971356, 0.1920433697104049, 0.03733033585611538, 0.1574277838746581, 0.07487501341541963, 0.13451387112355565, 0.04785811649852952, 0.2014301991344874, 0.19275785366320347, 0.16906311282859388, 0.09142999925560469, 0.0807535598741431, 0.08998881757890458, 0.11371588725858676, 0.18156871665958524, 0.11990423882011132, 0.05609636105576192, 0.10728852577765353, 0.04741168237587686, 0.09443087013848235, 0.11183096481475133, 0.20763329731994107, 0.14963632935842702, 0.033257224253434185, 0.16086770202207404, 0.1283830637996216, 0.06333881169568299, 0.07407414400198586, 0.18931126030685208, 0.20449889037522243, 0.09373409525317443, 0.11103487531871911, 0.13887923397610114, 0.09389803542860291, 0.1028621801370481, 0.10278340003897889, 0.04797734158160607, 0.06925149872003374, 0.16437387251206648, 0.06431591073207989, 0.15640813891719507, 0.14521938579659283, 0.09878922332793982, 0.1525225578350978, 0.057379048722987935, 0.13755675163486164, 0.21379037955387017, 0.11356352105462388, 0.12172503483304202, 0.0977992191327359, 0.0660844769899378, 0.16360026762000818, 0.1867521100075809, 0.1361265435898301, 0.193569778992843, 0.1376037155769772, 0.10366214156206781, 0.05424714016517395, 0.1510559507437721, 0.1982301336156442, 0.19342350327663269, 0.06841097894516744, 0.03567649167498546, 0.09240262140900436, 0.17273792965497756, 0.13060980072386896, 0.0506475015992783, 0.20464602866065232, 0.1335960937971171, 0.09647225827373596, 0.16063425267755335, 0.08011016203700777, 0.06943062226134, 0.11788273710282818, 0.1460912184022001, 0.17575701646937603, 0.08492044636656639, 0.08006628569486857, 0.0678242670356343, 0.08399578545078766, 0.04467673189567811, 0.16499230886781813, 0.18956091712939166, 0.13900668569148325, 0.0637688815820217, 0.08001596473812828, 0.1729772665464092, 0.18135370100220466, 0.08084467360367335, 0.20570536494369052, 0.1564167173272439, 0.16835001729805102, 0.11377488181313558, 0.14463243370652834, 0.15335039605845252, 0.17751701466907038, 0.05629211571994747, 0.05391940577095405, 0.04576917288875912, 0.04572995977436196, 0.08003572268716626, 0.1456447346884351, 0.15300808524340992, 0.05535523586923235, 0.15681628960897911, 0.09457446989229411, 0.13493156372230608, 0.04659816748054704, 0.06399268867349903, 0.13237320586876516, 0.06270685415856857, 0.08239532520456248, 0.090023935648921, 0.05509796373497408, 0.06333016739538949, 0.06741902566837275, 0.11456389310755336, 0.05546095860387603, 0.12453952433705405, 0.07304719849741025, 0.0337216850531518, 0.09738146817933367, 0.1708018508712304, 0.12812392568486006, 0.12197591403445081, 0.12676599816848022, 0.15375346250528993, 0.11319961293785988, 0.04353717272253202, 0.20843783158840695, 0.032690778583051645, 0.10957210464731734, 0.085864241979023, 0.13141618630806376, 0.07246266814373846, 0.047290917483413186, 0.20129331432719638, 0.03177460508120792, 0.0787572426283741, 0.10905423265139691, 0.15096424210355291, 0.1324517284704813, 0.17284040118640354, 0.22849954162327038, 0.13881885081150744, 0.07165183472748886, 0.027871092910663503, 0.06371783386423321, 0.0914091521266091, 0.1502753676341883, 0.07911741493427482, 0.19393106848629538, 0.07189355355799186, 0.1328548654447905, 0.03053532804264685, 0.16737256875123027, 0.07297790462032629, 0.12523672409547926, 0.07712017702000076, 0.037634033559957696, 0.0899591430572875, 0.16045218840877967, 0.0756501954107701, 0.1446956961554488, 0.175635613166116, 0.06089656295741989, 0.03305204310498882, 0.11121693941012752, 0.07603202566184844, 0.061483186746923255, 0.10937110880843273, 0.08430587929950792, 0.1687048962047941, 0.04786777449242551, 0.07000977425482474, 0.05897616417847618, 0.11927420080228009, 0.13111369616145518, 0.10881864745927552, 0.03507201641155117, 0.06619976103689132, 0.10102789077379754, 0.2037092480429832, 0.11857193927314655, 0.07331629406881954, 0.06538356336684421, 0.04278029559350475, 0.14797156417075935, 0.09094205592821146, 0.08650550807468005, 0.07845840530741063, 0.07269172800416118, 0.059896955270056555, 0.12561590950378201, 0.08129612878242092, 0.23647793656845376, 0.16743314338930437, 0.053625397156240914, 0.07207901787590924, 0.06952842723753316, 0.20383733854335706, 0.17139527850759806, 0.06287862802905973, 0.07242743715396796, 0.08916852835479407, 0.20567024925139, 0.1771510365898992, 0.1002232350240099, 0.10412802288430553, 0.06678931449982034, 0.1215879504412464, 0.1528917953686175, 0.17780019041949813, 0.1704259186429083, 0.054009849381672956, 0.15652354538986496, 0.08135465584392236, 0.055554026665720214, 0.09269169803215867, 0.0635791828093769, 0.05103797913538966, 0.15540952776692524, 0.05916938981897088, 0.08855155621050939, 0.04001933583753632, 0.045634003733782645, 0.11507441149305395, 0.07099008064901162, 0.13460016350451512, 0.1709688687847982, 0.15330357396530608, 0.11957132658082247, 0.05675114387061684, 0.0727971115821582, 0.17984944596805785, 0.09150154880271431, 0.12029683120245342, 0.042220409010286473, 0.1038972063749671, 0.1209634349631017, 0.06847048170405605, 0.057877564963900514, 0.15170302274934938, 0.06318456634324206, 0.12337734114485456]
0.1010949992446738
Making ranges
torch.Size([19264, 2])
We keep 4.03e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([1452, 2])
We keep 4.97e+04/4.72e+05 = 10% of the original kernel matrix.

torch.Size([5856, 2])
We keep 5.04e+05/9.17e+06 =  5% of the original kernel matrix.

torch.Size([12077, 2])
We keep 3.31e+06/6.87e+07 =  4% of the original kernel matrix.

torch.Size([14730, 2])
We keep 3.13e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([1739, 2])
We keep 6.18e+04/6.05e+05 = 10% of the original kernel matrix.

torch.Size([6385, 2])
We keep 5.42e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([2560, 2])
We keep 1.65e+05/1.64e+06 = 10% of the original kernel matrix.

torch.Size([7164, 2])
We keep 7.88e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([19740, 2])
We keep 1.71e+07/2.99e+08 =  5% of the original kernel matrix.

torch.Size([18765, 2])
We keep 5.77e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([14761, 2])
We keep 3.56e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([16511, 2])
We keep 3.71e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([36003, 2])
We keep 2.01e+07/7.39e+08 =  2% of the original kernel matrix.

torch.Size([25048, 2])
We keep 8.26e+06/3.63e+08 =  2% of the original kernel matrix.

torch.Size([41398, 2])
We keep 2.54e+07/1.04e+09 =  2% of the original kernel matrix.

torch.Size([26664, 2])
We keep 9.53e+06/4.30e+08 =  2% of the original kernel matrix.

torch.Size([4780, 2])
We keep 5.70e+05/6.86e+06 =  8% of the original kernel matrix.

torch.Size([9468, 2])
We keep 1.21e+06/3.50e+07 =  3% of the original kernel matrix.

torch.Size([17843, 2])
We keep 2.52e+07/4.07e+08 =  6% of the original kernel matrix.

torch.Size([17241, 2])
We keep 6.15e+06/2.69e+08 =  2% of the original kernel matrix.

torch.Size([6372, 2])
We keep 8.68e+05/1.54e+07 =  5% of the original kernel matrix.

torch.Size([10463, 2])
We keep 1.75e+06/5.25e+07 =  3% of the original kernel matrix.

torch.Size([171071, 2])
We keep 2.27e+08/1.66e+10 =  1% of the original kernel matrix.

torch.Size([56317, 2])
We keep 3.13e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([8835, 2])
We keep 1.64e+06/3.22e+07 =  5% of the original kernel matrix.

torch.Size([12431, 2])
We keep 2.35e+06/7.57e+07 =  3% of the original kernel matrix.

torch.Size([75360, 2])
We keep 7.59e+07/3.07e+09 =  2% of the original kernel matrix.

torch.Size([36437, 2])
We keep 1.48e+07/7.39e+08 =  2% of the original kernel matrix.

torch.Size([17580, 2])
We keep 5.03e+06/1.63e+08 =  3% of the original kernel matrix.

torch.Size([17750, 2])
We keep 4.44e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([32162, 2])
We keep 1.88e+07/6.07e+08 =  3% of the original kernel matrix.

torch.Size([23772, 2])
We keep 7.62e+06/3.29e+08 =  2% of the original kernel matrix.

torch.Size([28671, 2])
We keep 1.16e+07/4.60e+08 =  2% of the original kernel matrix.

torch.Size([22739, 2])
We keep 6.62e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([26192, 2])
We keep 1.48e+07/4.60e+08 =  3% of the original kernel matrix.

torch.Size([21499, 2])
We keep 6.77e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([428165, 2])
We keep 3.12e+09/1.37e+11 =  2% of the original kernel matrix.

torch.Size([90373, 2])
We keep 8.39e+07/4.94e+09 =  1% of the original kernel matrix.

torch.Size([62913, 2])
We keep 5.87e+07/2.42e+09 =  2% of the original kernel matrix.

torch.Size([32864, 2])
We keep 1.36e+07/6.56e+08 =  2% of the original kernel matrix.

torch.Size([6225, 2])
We keep 1.04e+06/1.59e+07 =  6% of the original kernel matrix.

torch.Size([10483, 2])
We keep 1.81e+06/5.32e+07 =  3% of the original kernel matrix.

torch.Size([380018, 2])
We keep 1.21e+09/9.13e+10 =  1% of the original kernel matrix.

torch.Size([85686, 2])
We keep 6.80e+07/4.03e+09 =  1% of the original kernel matrix.

torch.Size([18619, 2])
We keep 6.84e+06/1.98e+08 =  3% of the original kernel matrix.

torch.Size([18206, 2])
We keep 4.80e+06/1.88e+08 =  2% of the original kernel matrix.

torch.Size([8159, 2])
We keep 1.39e+06/3.06e+07 =  4% of the original kernel matrix.

torch.Size([12141, 2])
We keep 2.34e+06/7.38e+07 =  3% of the original kernel matrix.

torch.Size([13861, 2])
We keep 8.92e+06/1.64e+08 =  5% of the original kernel matrix.

torch.Size([14732, 2])
We keep 4.34e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([47616, 2])
We keep 3.12e+07/1.45e+09 =  2% of the original kernel matrix.

torch.Size([28706, 2])
We keep 1.10e+07/5.09e+08 =  2% of the original kernel matrix.

torch.Size([6440, 2])
We keep 1.20e+06/1.51e+07 =  7% of the original kernel matrix.

torch.Size([10713, 2])
We keep 1.69e+06/5.18e+07 =  3% of the original kernel matrix.

torch.Size([7297, 2])
We keep 1.05e+06/1.81e+07 =  5% of the original kernel matrix.

torch.Size([11315, 2])
We keep 1.81e+06/5.68e+07 =  3% of the original kernel matrix.

torch.Size([20977, 2])
We keep 1.30e+07/2.77e+08 =  4% of the original kernel matrix.

torch.Size([19003, 2])
We keep 5.23e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([1581514, 2])
We keep 1.23e+10/1.25e+12 =  0% of the original kernel matrix.

torch.Size([181615, 2])
We keep 2.30e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([3206, 2])
We keep 2.37e+05/2.83e+06 =  8% of the original kernel matrix.

torch.Size([7846, 2])
We keep 9.48e+05/2.25e+07 =  4% of the original kernel matrix.

torch.Size([103185, 2])
We keep 2.30e+08/7.88e+09 =  2% of the original kernel matrix.

torch.Size([43277, 2])
We keep 2.29e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([7775, 2])
We keep 1.31e+06/2.57e+07 =  5% of the original kernel matrix.

torch.Size([11659, 2])
We keep 2.17e+06/6.77e+07 =  3% of the original kernel matrix.

torch.Size([7648, 2])
We keep 9.11e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([11596, 2])
We keep 1.87e+06/5.75e+07 =  3% of the original kernel matrix.

torch.Size([3929, 2])
We keep 4.44e+05/4.60e+06 =  9% of the original kernel matrix.

torch.Size([8610, 2])
We keep 1.13e+06/2.86e+07 =  3% of the original kernel matrix.

torch.Size([19440, 2])
We keep 8.86e+06/2.18e+08 =  4% of the original kernel matrix.

torch.Size([18770, 2])
We keep 4.91e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([41642, 2])
We keep 2.25e+07/9.05e+08 =  2% of the original kernel matrix.

torch.Size([26912, 2])
We keep 8.80e+06/4.02e+08 =  2% of the original kernel matrix.

torch.Size([1463, 2])
We keep 4.51e+04/3.75e+05 = 12% of the original kernel matrix.

torch.Size([5954, 2])
We keep 4.78e+05/8.17e+06 =  5% of the original kernel matrix.

torch.Size([4399, 2])
We keep 3.63e+05/5.11e+06 =  7% of the original kernel matrix.

torch.Size([9075, 2])
We keep 1.19e+06/3.02e+07 =  3% of the original kernel matrix.

torch.Size([2366, 2])
We keep 1.50e+05/1.34e+06 = 11% of the original kernel matrix.

torch.Size([6864, 2])
We keep 7.26e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([5235, 2])
We keep 7.76e+05/1.00e+07 =  7% of the original kernel matrix.

torch.Size([9628, 2])
We keep 1.48e+06/4.23e+07 =  3% of the original kernel matrix.

torch.Size([826, 2])
We keep 2.01e+04/1.11e+05 = 18% of the original kernel matrix.

torch.Size([4738, 2])
We keep 3.21e+05/4.45e+06 =  7% of the original kernel matrix.

torch.Size([1661, 2])
We keep 6.07e+04/5.11e+05 = 11% of the original kernel matrix.

torch.Size([6001, 2])
We keep 5.30e+05/9.55e+06 =  5% of the original kernel matrix.

torch.Size([4914, 2])
We keep 5.08e+05/7.54e+06 =  6% of the original kernel matrix.

torch.Size([9173, 2])
We keep 1.36e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([1761, 2])
We keep 5.94e+04/5.31e+05 = 11% of the original kernel matrix.

torch.Size([6377, 2])
We keep 5.31e+05/9.73e+06 =  5% of the original kernel matrix.

torch.Size([2680, 2])
We keep 1.28e+05/1.57e+06 =  8% of the original kernel matrix.

torch.Size([7404, 2])
We keep 7.63e+05/1.67e+07 =  4% of the original kernel matrix.

torch.Size([5009, 2])
We keep 4.32e+05/6.60e+06 =  6% of the original kernel matrix.

torch.Size([9607, 2])
We keep 1.27e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([4069, 2])
We keep 3.47e+05/4.64e+06 =  7% of the original kernel matrix.

torch.Size([8566, 2])
We keep 1.14e+06/2.88e+07 =  3% of the original kernel matrix.

torch.Size([1826, 2])
We keep 6.48e+04/6.46e+05 = 10% of the original kernel matrix.

torch.Size([6314, 2])
We keep 5.59e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([5435, 2])
We keep 8.46e+05/1.01e+07 =  8% of the original kernel matrix.

torch.Size([9728, 2])
We keep 1.51e+06/4.25e+07 =  3% of the original kernel matrix.

torch.Size([4949, 2])
We keep 4.67e+05/6.75e+06 =  6% of the original kernel matrix.

torch.Size([9481, 2])
We keep 1.31e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([2111, 2])
We keep 1.12e+05/1.03e+06 = 10% of the original kernel matrix.

torch.Size([6608, 2])
We keep 6.74e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([3623, 2])
We keep 4.00e+05/4.15e+06 =  9% of the original kernel matrix.

torch.Size([8230, 2])
We keep 1.10e+06/2.72e+07 =  4% of the original kernel matrix.

torch.Size([1948, 2])
We keep 8.63e+04/8.35e+05 = 10% of the original kernel matrix.

torch.Size([6458, 2])
We keep 6.21e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([6383, 2])
We keep 9.17e+05/1.49e+07 =  6% of the original kernel matrix.

torch.Size([10602, 2])
We keep 1.80e+06/5.15e+07 =  3% of the original kernel matrix.

torch.Size([2104, 2])
We keep 1.24e+05/1.18e+06 = 10% of the original kernel matrix.

torch.Size([6496, 2])
We keep 6.90e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([3407, 2])
We keep 6.07e+05/4.34e+06 = 13% of the original kernel matrix.

torch.Size([7830, 2])
We keep 1.14e+06/2.78e+07 =  4% of the original kernel matrix.

torch.Size([2705, 2])
We keep 1.43e+05/1.70e+06 =  8% of the original kernel matrix.

torch.Size([7342, 2])
We keep 7.94e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([2838, 2])
We keep 1.54e+05/1.85e+06 =  8% of the original kernel matrix.

torch.Size([7578, 2])
We keep 8.08e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([2571, 2])
We keep 1.57e+05/1.53e+06 = 10% of the original kernel matrix.

torch.Size([7180, 2])
We keep 7.74e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([5311, 2])
We keep 4.55e+05/7.57e+06 =  6% of the original kernel matrix.

torch.Size([9984, 2])
We keep 1.33e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([2247, 2])
We keep 1.17e+05/1.27e+06 =  9% of the original kernel matrix.

torch.Size([6896, 2])
We keep 7.13e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([4502, 2])
We keep 4.46e+05/5.98e+06 =  7% of the original kernel matrix.

torch.Size([9531, 2])
We keep 1.30e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([6945, 2])
We keep 1.00e+06/1.66e+07 =  6% of the original kernel matrix.

torch.Size([10962, 2])
We keep 1.83e+06/5.44e+07 =  3% of the original kernel matrix.

torch.Size([3883, 2])
We keep 3.58e+05/4.37e+06 =  8% of the original kernel matrix.

torch.Size([8471, 2])
We keep 1.12e+06/2.79e+07 =  3% of the original kernel matrix.

torch.Size([2162, 2])
We keep 8.53e+04/9.22e+05 =  9% of the original kernel matrix.

torch.Size([6791, 2])
We keep 6.28e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([3802, 2])
We keep 2.58e+05/3.62e+06 =  7% of the original kernel matrix.

torch.Size([8372, 2])
We keep 1.02e+06/2.54e+07 =  4% of the original kernel matrix.

torch.Size([6211, 2])
We keep 5.97e+05/1.06e+07 =  5% of the original kernel matrix.

torch.Size([10178, 2])
We keep 1.50e+06/4.35e+07 =  3% of the original kernel matrix.

torch.Size([4540, 2])
We keep 3.27e+05/4.97e+06 =  6% of the original kernel matrix.

torch.Size([9218, 2])
We keep 1.14e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([1854, 2])
We keep 7.41e+04/6.97e+05 = 10% of the original kernel matrix.

torch.Size([6347, 2])
We keep 5.87e+05/1.11e+07 =  5% of the original kernel matrix.

torch.Size([5107, 2])
We keep 5.78e+05/8.21e+06 =  7% of the original kernel matrix.

torch.Size([9366, 2])
We keep 1.41e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([4006, 2])
We keep 2.94e+05/3.97e+06 =  7% of the original kernel matrix.

torch.Size([8499, 2])
We keep 1.07e+06/2.66e+07 =  4% of the original kernel matrix.

torch.Size([6360, 2])
We keep 6.60e+05/1.11e+07 =  5% of the original kernel matrix.

torch.Size([10411, 2])
We keep 1.55e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([1698, 2])
We keep 9.32e+04/6.96e+05 = 13% of the original kernel matrix.

torch.Size([5982, 2])
We keep 5.83e+05/1.11e+07 =  5% of the original kernel matrix.

torch.Size([2516, 2])
We keep 1.96e+05/2.18e+06 =  8% of the original kernel matrix.

torch.Size([7001, 2])
We keep 9.01e+05/1.97e+07 =  4% of the original kernel matrix.

torch.Size([4341, 2])
We keep 3.44e+05/4.67e+06 =  7% of the original kernel matrix.

torch.Size([8968, 2])
We keep 1.13e+06/2.88e+07 =  3% of the original kernel matrix.

torch.Size([2688, 2])
We keep 1.67e+05/1.90e+06 =  8% of the original kernel matrix.

torch.Size([7458, 2])
We keep 8.34e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([688, 2])
We keep 1.20e+04/7.73e+04 = 15% of the original kernel matrix.

torch.Size([4549, 2])
We keep 2.77e+05/3.71e+06 =  7% of the original kernel matrix.

torch.Size([5065, 2])
We keep 5.19e+05/7.38e+06 =  7% of the original kernel matrix.

torch.Size([9426, 2])
We keep 1.35e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([10092, 2])
We keep 1.67e+06/4.02e+07 =  4% of the original kernel matrix.

torch.Size([13141, 2])
We keep 2.52e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([5449, 2])
We keep 1.01e+06/1.06e+07 =  9% of the original kernel matrix.

torch.Size([9669, 2])
We keep 1.56e+06/4.36e+07 =  3% of the original kernel matrix.

torch.Size([2373, 2])
We keep 2.21e+05/1.63e+06 = 13% of the original kernel matrix.

torch.Size([6821, 2])
We keep 8.03e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([3211, 2])
We keep 2.09e+05/2.56e+06 =  8% of the original kernel matrix.

torch.Size([8011, 2])
We keep 8.96e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([2387, 2])
We keep 1.29e+05/1.22e+06 = 10% of the original kernel matrix.

torch.Size([6972, 2])
We keep 7.07e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([4828, 2])
We keep 6.79e+05/6.92e+06 =  9% of the original kernel matrix.

torch.Size([9359, 2])
We keep 1.31e+06/3.51e+07 =  3% of the original kernel matrix.

torch.Size([1688, 2])
We keep 7.62e+04/6.21e+05 = 12% of the original kernel matrix.

torch.Size([6084, 2])
We keep 5.63e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([3850, 2])
We keep 2.28e+05/3.15e+06 =  7% of the original kernel matrix.

torch.Size([8535, 2])
We keep 9.77e+05/2.37e+07 =  4% of the original kernel matrix.

torch.Size([1407, 2])
We keep 4.13e+04/3.64e+05 = 11% of the original kernel matrix.

torch.Size([5854, 2])
We keep 4.62e+05/8.05e+06 =  5% of the original kernel matrix.

torch.Size([3235, 2])
We keep 2.04e+05/2.60e+06 =  7% of the original kernel matrix.

torch.Size([7842, 2])
We keep 9.14e+05/2.15e+07 =  4% of the original kernel matrix.

torch.Size([2552, 2])
We keep 1.29e+05/1.45e+06 =  8% of the original kernel matrix.

torch.Size([7231, 2])
We keep 7.50e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([4222, 2])
We keep 3.25e+05/4.59e+06 =  7% of the original kernel matrix.

torch.Size([8790, 2])
We keep 1.12e+06/2.86e+07 =  3% of the original kernel matrix.

torch.Size([1794, 2])
We keep 6.90e+04/6.81e+05 = 10% of the original kernel matrix.

torch.Size([6343, 2])
We keep 5.73e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([1725, 2])
We keep 8.26e+04/7.07e+05 = 11% of the original kernel matrix.

torch.Size([6269, 2])
We keep 5.97e+05/1.12e+07 =  5% of the original kernel matrix.

torch.Size([3114, 2])
We keep 2.24e+05/2.45e+06 =  9% of the original kernel matrix.

torch.Size([7781, 2])
We keep 9.13e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([2430, 2])
We keep 1.26e+05/1.29e+06 =  9% of the original kernel matrix.

torch.Size([6946, 2])
We keep 7.14e+05/1.52e+07 =  4% of the original kernel matrix.

torch.Size([4041, 2])
We keep 3.26e+05/4.19e+06 =  7% of the original kernel matrix.

torch.Size([8577, 2])
We keep 1.08e+06/2.73e+07 =  3% of the original kernel matrix.

torch.Size([2284, 2])
We keep 1.08e+05/1.09e+06 =  9% of the original kernel matrix.

torch.Size([6780, 2])
We keep 6.72e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([9488, 2])
We keep 7.28e+06/7.29e+07 =  9% of the original kernel matrix.

torch.Size([12704, 2])
We keep 3.33e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([3483, 2])
We keep 2.81e+05/3.26e+06 =  8% of the original kernel matrix.

torch.Size([8274, 2])
We keep 1.02e+06/2.41e+07 =  4% of the original kernel matrix.

torch.Size([4003, 2])
We keep 3.38e+05/4.22e+06 =  8% of the original kernel matrix.

torch.Size([8526, 2])
We keep 1.09e+06/2.74e+07 =  3% of the original kernel matrix.

torch.Size([6186, 2])
We keep 7.16e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([10460, 2])
We keep 1.62e+06/4.67e+07 =  3% of the original kernel matrix.

torch.Size([3347, 2])
We keep 2.18e+05/2.78e+06 =  7% of the original kernel matrix.

torch.Size([8113, 2])
We keep 9.46e+05/2.23e+07 =  4% of the original kernel matrix.

torch.Size([6471, 2])
We keep 7.95e+05/1.49e+07 =  5% of the original kernel matrix.

torch.Size([10497, 2])
We keep 1.74e+06/5.16e+07 =  3% of the original kernel matrix.

torch.Size([1454, 2])
We keep 5.53e+04/4.41e+05 = 12% of the original kernel matrix.

torch.Size([5545, 2])
We keep 4.90e+05/8.86e+06 =  5% of the original kernel matrix.

torch.Size([3139, 2])
We keep 2.95e+05/3.11e+06 =  9% of the original kernel matrix.

torch.Size([7685, 2])
We keep 1.01e+06/2.35e+07 =  4% of the original kernel matrix.

torch.Size([1666, 2])
We keep 5.15e+04/5.16e+05 =  9% of the original kernel matrix.

torch.Size([6147, 2])
We keep 5.15e+05/9.59e+06 =  5% of the original kernel matrix.

torch.Size([5157, 2])
We keep 5.32e+05/8.29e+06 =  6% of the original kernel matrix.

torch.Size([9558, 2])
We keep 1.43e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([992, 2])
We keep 2.60e+04/1.91e+05 = 13% of the original kernel matrix.

torch.Size([5145, 2])
We keep 3.82e+05/5.83e+06 =  6% of the original kernel matrix.

torch.Size([3013, 2])
We keep 1.77e+05/2.09e+06 =  8% of the original kernel matrix.

torch.Size([7691, 2])
We keep 8.46e+05/1.93e+07 =  4% of the original kernel matrix.

torch.Size([1740, 2])
We keep 5.06e+04/4.76e+05 = 10% of the original kernel matrix.

torch.Size([6310, 2])
We keep 4.95e+05/9.21e+06 =  5% of the original kernel matrix.

torch.Size([1702, 2])
We keep 7.17e+04/6.08e+05 = 11% of the original kernel matrix.

torch.Size([6039, 2])
We keep 5.59e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([1471, 2])
We keep 5.76e+04/4.77e+05 = 12% of the original kernel matrix.

torch.Size([5880, 2])
We keep 5.21e+05/9.22e+06 =  5% of the original kernel matrix.

torch.Size([1202, 2])
We keep 4.06e+04/2.96e+05 = 13% of the original kernel matrix.

torch.Size([5280, 2])
We keep 4.42e+05/7.26e+06 =  6% of the original kernel matrix.

torch.Size([4471, 2])
We keep 3.56e+05/5.29e+06 =  6% of the original kernel matrix.

torch.Size([9074, 2])
We keep 1.17e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([3447, 2])
We keep 3.43e+05/3.78e+06 =  9% of the original kernel matrix.

torch.Size([7990, 2])
We keep 1.06e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([1456, 2])
We keep 7.23e+04/4.80e+05 = 15% of the original kernel matrix.

torch.Size([5556, 2])
We keep 5.13e+05/9.25e+06 =  5% of the original kernel matrix.

torch.Size([2664, 2])
We keep 2.52e+05/2.36e+06 = 10% of the original kernel matrix.

torch.Size([7303, 2])
We keep 9.25e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([2658, 2])
We keep 1.49e+05/1.54e+06 =  9% of the original kernel matrix.

torch.Size([7365, 2])
We keep 7.71e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([4614, 2])
We keep 6.12e+05/7.52e+06 =  8% of the original kernel matrix.

torch.Size([9458, 2])
We keep 1.42e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([2053, 2])
We keep 8.85e+04/8.87e+05 =  9% of the original kernel matrix.

torch.Size([6609, 2])
We keep 6.38e+05/1.26e+07 =  5% of the original kernel matrix.

torch.Size([9697, 2])
We keep 1.82e+06/3.96e+07 =  4% of the original kernel matrix.

torch.Size([12997, 2])
We keep 2.53e+06/8.40e+07 =  3% of the original kernel matrix.

torch.Size([3060, 2])
We keep 2.48e+05/2.92e+06 =  8% of the original kernel matrix.

torch.Size([7537, 2])
We keep 9.66e+05/2.28e+07 =  4% of the original kernel matrix.

torch.Size([1636, 2])
We keep 6.43e+04/5.39e+05 = 11% of the original kernel matrix.

torch.Size([5991, 2])
We keep 5.43e+05/9.80e+06 =  5% of the original kernel matrix.

torch.Size([2758, 2])
We keep 1.70e+05/1.87e+06 =  9% of the original kernel matrix.

torch.Size([7399, 2])
We keep 8.19e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([1899, 2])
We keep 7.21e+04/6.97e+05 = 10% of the original kernel matrix.

torch.Size([6387, 2])
We keep 5.80e+05/1.11e+07 =  5% of the original kernel matrix.

torch.Size([3253, 2])
We keep 2.74e+05/2.62e+06 = 10% of the original kernel matrix.

torch.Size([7766, 2])
We keep 9.24e+05/2.16e+07 =  4% of the original kernel matrix.

torch.Size([1678, 2])
We keep 6.24e+04/5.88e+05 = 10% of the original kernel matrix.

torch.Size([6084, 2])
We keep 5.49e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([2670, 2])
We keep 1.32e+05/1.49e+06 =  8% of the original kernel matrix.

torch.Size([7380, 2])
We keep 7.54e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([11448, 2])
We keep 3.46e+06/7.35e+07 =  4% of the original kernel matrix.

torch.Size([14360, 2])
We keep 3.32e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([2618, 2])
We keep 1.52e+05/1.56e+06 =  9% of the original kernel matrix.

torch.Size([7171, 2])
We keep 7.62e+05/1.67e+07 =  4% of the original kernel matrix.

torch.Size([4134, 2])
We keep 3.07e+05/4.33e+06 =  7% of the original kernel matrix.

torch.Size([8770, 2])
We keep 1.10e+06/2.78e+07 =  3% of the original kernel matrix.

torch.Size([1301, 2])
We keep 2.59e+04/2.16e+05 = 11% of the original kernel matrix.

torch.Size([5783, 2])
We keep 3.88e+05/6.21e+06 =  6% of the original kernel matrix.

torch.Size([4935, 2])
We keep 5.27e+05/7.14e+06 =  7% of the original kernel matrix.

torch.Size([9397, 2])
We keep 1.34e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([1073, 2])
We keep 3.69e+04/1.94e+05 = 18% of the original kernel matrix.

torch.Size([5146, 2])
We keep 3.80e+05/5.89e+06 =  6% of the original kernel matrix.

torch.Size([3135, 2])
We keep 1.82e+05/2.21e+06 =  8% of the original kernel matrix.

torch.Size([7720, 2])
We keep 8.58e+05/1.98e+07 =  4% of the original kernel matrix.

torch.Size([1727, 2])
We keep 5.67e+04/5.36e+05 = 10% of the original kernel matrix.

torch.Size([6347, 2])
We keep 5.30e+05/9.77e+06 =  5% of the original kernel matrix.

torch.Size([1726, 2])
We keep 6.24e+04/5.70e+05 = 10% of the original kernel matrix.

torch.Size([6213, 2])
We keep 5.42e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([1153, 2])
We keep 2.92e+04/2.32e+05 = 12% of the original kernel matrix.

torch.Size([5320, 2])
We keep 3.99e+05/6.43e+06 =  6% of the original kernel matrix.

torch.Size([1644, 2])
We keep 5.82e+04/5.08e+05 = 11% of the original kernel matrix.

torch.Size([5949, 2])
We keep 5.19e+05/9.52e+06 =  5% of the original kernel matrix.

torch.Size([2628, 2])
We keep 1.48e+05/1.88e+06 =  7% of the original kernel matrix.

torch.Size([7280, 2])
We keep 7.87e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([3060, 2])
We keep 2.55e+05/2.81e+06 =  9% of the original kernel matrix.

torch.Size([7542, 2])
We keep 9.61e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([1949, 2])
We keep 1.26e+05/9.96e+05 = 12% of the original kernel matrix.

torch.Size([6307, 2])
We keep 6.52e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([3574, 2])
We keep 3.12e+05/3.84e+06 =  8% of the original kernel matrix.

torch.Size([8011, 2])
We keep 1.07e+06/2.62e+07 =  4% of the original kernel matrix.

torch.Size([8021, 2])
We keep 1.72e+06/3.02e+07 =  5% of the original kernel matrix.

torch.Size([12040, 2])
We keep 2.37e+06/7.33e+07 =  3% of the original kernel matrix.

torch.Size([5060, 2])
We keep 5.66e+05/8.40e+06 =  6% of the original kernel matrix.

torch.Size([9478, 2])
We keep 1.43e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([2605, 2])
We keep 1.22e+05/1.44e+06 =  8% of the original kernel matrix.

torch.Size([7228, 2])
We keep 7.30e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([2166, 2])
We keep 9.93e+04/9.33e+05 = 10% of the original kernel matrix.

torch.Size([6682, 2])
We keep 6.43e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([1811, 2])
We keep 7.37e+04/6.61e+05 = 11% of the original kernel matrix.

torch.Size([6569, 2])
We keep 5.95e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([2327, 2])
We keep 9.66e+04/1.03e+06 =  9% of the original kernel matrix.

torch.Size([7024, 2])
We keep 6.64e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([1679, 2])
We keep 6.74e+04/5.61e+05 = 12% of the original kernel matrix.

torch.Size([6056, 2])
We keep 5.39e+05/1.00e+07 =  5% of the original kernel matrix.

torch.Size([1731, 2])
We keep 7.88e+04/6.74e+05 = 11% of the original kernel matrix.

torch.Size([6002, 2])
We keep 5.81e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([2209, 2])
We keep 1.18e+05/1.34e+06 =  8% of the original kernel matrix.

torch.Size([6833, 2])
We keep 7.45e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([3892, 2])
We keep 2.93e+05/4.07e+06 =  7% of the original kernel matrix.

torch.Size([8518, 2])
We keep 1.08e+06/2.69e+07 =  4% of the original kernel matrix.

torch.Size([4712, 2])
We keep 8.24e+05/8.18e+06 = 10% of the original kernel matrix.

torch.Size([9233, 2])
We keep 1.43e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([2779, 2])
We keep 2.50e+05/2.28e+06 = 10% of the original kernel matrix.

torch.Size([7248, 2])
We keep 8.96e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([7539, 2])
We keep 1.68e+06/2.38e+07 =  7% of the original kernel matrix.

torch.Size([11413, 2])
We keep 2.12e+06/6.51e+07 =  3% of the original kernel matrix.

torch.Size([3758, 2])
We keep 2.39e+05/3.09e+06 =  7% of the original kernel matrix.

torch.Size([8420, 2])
We keep 9.73e+05/2.35e+07 =  4% of the original kernel matrix.

torch.Size([2126, 2])
We keep 1.02e+05/1.02e+06 = 10% of the original kernel matrix.

torch.Size([6705, 2])
We keep 6.65e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([6170, 2])
We keep 1.24e+06/1.55e+07 =  8% of the original kernel matrix.

torch.Size([10509, 2])
We keep 1.85e+06/5.26e+07 =  3% of the original kernel matrix.

torch.Size([861, 2])
We keep 1.75e+04/1.01e+05 = 17% of the original kernel matrix.

torch.Size([4832, 2])
We keep 3.15e+05/4.25e+06 =  7% of the original kernel matrix.

torch.Size([3710, 2])
We keep 3.38e+05/4.00e+06 =  8% of the original kernel matrix.

torch.Size([8533, 2])
We keep 1.12e+06/2.67e+07 =  4% of the original kernel matrix.

torch.Size([2372, 2])
We keep 1.56e+05/1.36e+06 = 11% of the original kernel matrix.

torch.Size([6843, 2])
We keep 7.41e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([2278, 2])
We keep 1.52e+05/1.58e+06 =  9% of the original kernel matrix.

torch.Size([6998, 2])
We keep 8.22e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([1081, 2])
We keep 3.34e+04/2.46e+05 = 13% of the original kernel matrix.

torch.Size([4989, 2])
We keep 4.10e+05/6.62e+06 =  6% of the original kernel matrix.

torch.Size([2177, 2])
We keep 1.05e+05/1.04e+06 = 10% of the original kernel matrix.

torch.Size([6716, 2])
We keep 6.68e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([1332, 2])
We keep 5.28e+04/4.24e+05 = 12% of the original kernel matrix.

torch.Size([5483, 2])
We keep 4.94e+05/8.69e+06 =  5% of the original kernel matrix.

torch.Size([3920, 2])
We keep 4.02e+05/5.00e+06 =  8% of the original kernel matrix.

torch.Size([8635, 2])
We keep 1.17e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([3743, 2])
We keep 2.25e+05/3.12e+06 =  7% of the original kernel matrix.

torch.Size([8423, 2])
We keep 9.75e+05/2.36e+07 =  4% of the original kernel matrix.

torch.Size([5000, 2])
We keep 4.44e+05/6.33e+06 =  7% of the original kernel matrix.

torch.Size([9431, 2])
We keep 1.27e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([1366, 2])
We keep 5.52e+04/3.70e+05 = 14% of the original kernel matrix.

torch.Size([5552, 2])
We keep 4.68e+05/8.12e+06 =  5% of the original kernel matrix.

torch.Size([1781, 2])
We keep 1.32e+05/9.56e+05 = 13% of the original kernel matrix.

torch.Size([5932, 2])
We keep 6.60e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([1980, 2])
We keep 1.09e+05/1.06e+06 = 10% of the original kernel matrix.

torch.Size([6344, 2])
We keep 6.76e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([7916, 2])
We keep 1.53e+06/2.81e+07 =  5% of the original kernel matrix.

torch.Size([11733, 2])
We keep 2.29e+06/7.07e+07 =  3% of the original kernel matrix.

torch.Size([2541, 2])
We keep 2.08e+05/1.91e+06 = 10% of the original kernel matrix.

torch.Size([7190, 2])
We keep 8.33e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([4316, 2])
We keep 4.32e+05/5.83e+06 =  7% of the original kernel matrix.

torch.Size([9093, 2])
We keep 1.22e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([2192, 2])
We keep 8.40e+04/9.84e+05 =  8% of the original kernel matrix.

torch.Size([6911, 2])
We keep 6.31e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([4865, 2])
We keep 4.87e+05/6.72e+06 =  7% of the original kernel matrix.

torch.Size([9248, 2])
We keep 1.29e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([3428, 2])
We keep 3.04e+05/3.80e+06 =  7% of the original kernel matrix.

torch.Size([7850, 2])
We keep 1.06e+06/2.60e+07 =  4% of the original kernel matrix.

torch.Size([5492, 2])
We keep 8.45e+05/9.12e+06 =  9% of the original kernel matrix.

torch.Size([9889, 2])
We keep 1.47e+06/4.03e+07 =  3% of the original kernel matrix.

torch.Size([3526, 2])
We keep 2.67e+05/3.04e+06 =  8% of the original kernel matrix.

torch.Size([8062, 2])
We keep 9.77e+05/2.33e+07 =  4% of the original kernel matrix.

torch.Size([2200, 2])
We keep 1.28e+05/1.42e+06 =  9% of the original kernel matrix.

torch.Size([6779, 2])
We keep 7.59e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([2072, 2])
We keep 1.10e+05/1.06e+06 = 10% of the original kernel matrix.

torch.Size([6562, 2])
We keep 6.73e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([1977, 2])
We keep 1.01e+05/8.97e+05 = 11% of the original kernel matrix.

torch.Size([6373, 2])
We keep 6.21e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([3485, 2])
We keep 2.77e+05/2.93e+06 =  9% of the original kernel matrix.

torch.Size([8090, 2])
We keep 9.50e+05/2.28e+07 =  4% of the original kernel matrix.

torch.Size([5673, 2])
We keep 7.79e+05/1.22e+07 =  6% of the original kernel matrix.

torch.Size([10043, 2])
We keep 1.65e+06/4.67e+07 =  3% of the original kernel matrix.

torch.Size([3813, 2])
We keep 2.91e+05/3.95e+06 =  7% of the original kernel matrix.

torch.Size([8434, 2])
We keep 1.07e+06/2.65e+07 =  4% of the original kernel matrix.

torch.Size([2091, 2])
We keep 8.60e+04/8.72e+05 =  9% of the original kernel matrix.

torch.Size([6663, 2])
We keep 6.33e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([3753, 2])
We keep 2.63e+05/3.52e+06 =  7% of the original kernel matrix.

torch.Size([8403, 2])
We keep 1.03e+06/2.50e+07 =  4% of the original kernel matrix.

torch.Size([3599, 2])
We keep 2.93e+05/3.71e+06 =  7% of the original kernel matrix.

torch.Size([8309, 2])
We keep 1.08e+06/2.57e+07 =  4% of the original kernel matrix.

torch.Size([5104, 2])
We keep 6.01e+05/8.83e+06 =  6% of the original kernel matrix.

torch.Size([9767, 2])
We keep 1.49e+06/3.97e+07 =  3% of the original kernel matrix.

torch.Size([1895, 2])
We keep 7.55e+04/7.74e+05 =  9% of the original kernel matrix.

torch.Size([6418, 2])
We keep 6.02e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([999, 2])
We keep 5.00e+04/2.83e+05 = 17% of the original kernel matrix.

torch.Size([4714, 2])
We keep 4.31e+05/7.10e+06 =  6% of the original kernel matrix.

torch.Size([4075, 2])
We keep 2.74e+05/3.99e+06 =  6% of the original kernel matrix.

torch.Size([8720, 2])
We keep 1.05e+06/2.67e+07 =  3% of the original kernel matrix.

torch.Size([2032, 2])
We keep 1.24e+05/1.15e+06 = 10% of the original kernel matrix.

torch.Size([6526, 2])
We keep 6.93e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([1739, 2])
We keep 8.10e+04/6.94e+05 = 11% of the original kernel matrix.

torch.Size([6130, 2])
We keep 5.89e+05/1.11e+07 =  5% of the original kernel matrix.

torch.Size([2876, 2])
We keep 2.42e+05/2.77e+06 =  8% of the original kernel matrix.

torch.Size([7585, 2])
We keep 9.98e+05/2.22e+07 =  4% of the original kernel matrix.

torch.Size([2747, 2])
We keep 1.38e+05/1.54e+06 =  8% of the original kernel matrix.

torch.Size([7384, 2])
We keep 7.68e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([1007, 2])
We keep 1.82e+04/1.38e+05 = 13% of the original kernel matrix.

torch.Size([5277, 2])
We keep 3.33e+05/4.97e+06 =  6% of the original kernel matrix.

torch.Size([2716, 2])
We keep 1.65e+05/1.90e+06 =  8% of the original kernel matrix.

torch.Size([7261, 2])
We keep 8.26e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([1266, 2])
We keep 3.85e+04/2.99e+05 = 12% of the original kernel matrix.

torch.Size([5588, 2])
We keep 4.38e+05/7.30e+06 =  5% of the original kernel matrix.

torch.Size([3544, 2])
We keep 3.17e+05/3.87e+06 =  8% of the original kernel matrix.

torch.Size([8292, 2])
We keep 1.11e+06/2.63e+07 =  4% of the original kernel matrix.

torch.Size([2129, 2])
We keep 1.01e+05/9.55e+05 = 10% of the original kernel matrix.

torch.Size([6636, 2])
We keep 6.56e+05/1.30e+07 =  5% of the original kernel matrix.

torch.Size([5883, 2])
We keep 8.02e+05/1.29e+07 =  6% of the original kernel matrix.

torch.Size([10298, 2])
We keep 1.73e+06/4.79e+07 =  3% of the original kernel matrix.

torch.Size([3300, 2])
We keep 2.29e+05/2.74e+06 =  8% of the original kernel matrix.

torch.Size([7917, 2])
We keep 9.48e+05/2.21e+07 =  4% of the original kernel matrix.

torch.Size([3643, 2])
We keep 2.54e+05/3.14e+06 =  8% of the original kernel matrix.

torch.Size([8293, 2])
We keep 9.87e+05/2.36e+07 =  4% of the original kernel matrix.

torch.Size([1383, 2])
We keep 4.28e+04/3.70e+05 = 11% of the original kernel matrix.

torch.Size([5750, 2])
We keep 4.72e+05/8.12e+06 =  5% of the original kernel matrix.

torch.Size([1861, 2])
We keep 7.59e+04/7.74e+05 =  9% of the original kernel matrix.

torch.Size([6524, 2])
We keep 6.05e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([2135, 2])
We keep 1.10e+05/1.06e+06 = 10% of the original kernel matrix.

torch.Size([6649, 2])
We keep 6.76e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([3837, 2])
We keep 2.56e+05/3.66e+06 =  7% of the original kernel matrix.

torch.Size([8453, 2])
We keep 1.02e+06/2.55e+07 =  4% of the original kernel matrix.

torch.Size([5007, 2])
We keep 4.61e+05/6.84e+06 =  6% of the original kernel matrix.

torch.Size([9394, 2])
We keep 1.30e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([2975, 2])
We keep 2.34e+05/2.54e+06 =  9% of the original kernel matrix.

torch.Size([7459, 2])
We keep 9.14e+05/2.13e+07 =  4% of the original kernel matrix.

torch.Size([2668, 2])
We keep 1.78e+05/1.79e+06 =  9% of the original kernel matrix.

torch.Size([7266, 2])
We keep 8.13e+05/1.79e+07 =  4% of the original kernel matrix.

torch.Size([3237, 2])
We keep 2.03e+05/2.43e+06 =  8% of the original kernel matrix.

torch.Size([7819, 2])
We keep 9.03e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([3988, 2])
We keep 3.96e+05/4.68e+06 =  8% of the original kernel matrix.

torch.Size([8515, 2])
We keep 1.17e+06/2.89e+07 =  4% of the original kernel matrix.

torch.Size([893, 2])
We keep 2.31e+04/1.59e+05 = 14% of the original kernel matrix.

torch.Size([4914, 2])
We keep 3.53e+05/5.33e+06 =  6% of the original kernel matrix.

torch.Size([2023, 2])
We keep 1.02e+05/9.04e+05 = 11% of the original kernel matrix.

torch.Size([6488, 2])
We keep 6.44e+05/1.27e+07 =  5% of the original kernel matrix.

torch.Size([1465, 2])
We keep 5.31e+04/4.52e+05 = 11% of the original kernel matrix.

torch.Size([5847, 2])
We keep 4.94e+05/8.97e+06 =  5% of the original kernel matrix.

torch.Size([970, 2])
We keep 1.62e+04/1.25e+05 = 12% of the original kernel matrix.

torch.Size([5167, 2])
We keep 3.20e+05/4.73e+06 =  6% of the original kernel matrix.

torch.Size([8219, 2])
We keep 3.10e+06/3.06e+07 = 10% of the original kernel matrix.

torch.Size([11914, 2])
We keep 2.36e+06/7.39e+07 =  3% of the original kernel matrix.

torch.Size([2002, 2])
We keep 8.35e+04/8.41e+05 =  9% of the original kernel matrix.

torch.Size([6587, 2])
We keep 6.17e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([3007, 2])
We keep 5.32e+05/3.53e+06 = 15% of the original kernel matrix.

torch.Size([7419, 2])
We keep 1.04e+06/2.51e+07 =  4% of the original kernel matrix.

torch.Size([1882, 2])
We keep 6.63e+04/7.16e+05 =  9% of the original kernel matrix.

torch.Size([6556, 2])
We keep 5.73e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([5009, 2])
We keep 7.69e+05/7.95e+06 =  9% of the original kernel matrix.

torch.Size([9334, 2])
We keep 1.40e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([1887, 2])
We keep 1.06e+05/8.95e+05 = 11% of the original kernel matrix.

torch.Size([6374, 2])
We keep 6.45e+05/1.26e+07 =  5% of the original kernel matrix.

torch.Size([2538, 2])
We keep 2.23e+05/1.91e+06 = 11% of the original kernel matrix.

torch.Size([7143, 2])
We keep 8.39e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([3958, 2])
We keep 4.46e+05/4.73e+06 =  9% of the original kernel matrix.

torch.Size([8523, 2])
We keep 1.16e+06/2.90e+07 =  3% of the original kernel matrix.

torch.Size([1392, 2])
We keep 5.64e+04/4.84e+05 = 11% of the original kernel matrix.

torch.Size([5589, 2])
We keep 5.22e+05/9.29e+06 =  5% of the original kernel matrix.

torch.Size([2197, 2])
We keep 9.43e+04/9.74e+05 =  9% of the original kernel matrix.

torch.Size([6822, 2])
We keep 6.53e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([3758, 2])
We keep 3.26e+05/4.25e+06 =  7% of the original kernel matrix.

torch.Size([8358, 2])
We keep 1.12e+06/2.75e+07 =  4% of the original kernel matrix.

torch.Size([1156, 2])
We keep 3.91e+04/2.51e+05 = 15% of the original kernel matrix.

torch.Size([5349, 2])
We keep 4.17e+05/6.69e+06 =  6% of the original kernel matrix.

torch.Size([1465, 2])
We keep 5.01e+04/4.73e+05 = 10% of the original kernel matrix.

torch.Size([5928, 2])
We keep 5.04e+05/9.18e+06 =  5% of the original kernel matrix.

torch.Size([2258, 2])
We keep 1.86e+05/1.43e+06 = 13% of the original kernel matrix.

torch.Size([6765, 2])
We keep 7.51e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([2705, 2])
We keep 1.77e+05/2.10e+06 =  8% of the original kernel matrix.

torch.Size([7405, 2])
We keep 8.64e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([3664, 2])
We keep 4.20e+05/4.65e+06 =  9% of the original kernel matrix.

torch.Size([8481, 2])
We keep 1.19e+06/2.88e+07 =  4% of the original kernel matrix.

torch.Size([1588, 2])
We keep 4.88e+04/4.69e+05 = 10% of the original kernel matrix.

torch.Size([6068, 2])
We keep 5.05e+05/9.14e+06 =  5% of the original kernel matrix.

torch.Size([1511, 2])
We keep 8.82e+04/6.51e+05 = 13% of the original kernel matrix.

torch.Size([5715, 2])
We keep 5.74e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([3201, 2])
We keep 1.83e+05/2.31e+06 =  7% of the original kernel matrix.

torch.Size([8021, 2])
We keep 8.80e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([5695, 2])
We keep 7.65e+05/1.20e+07 =  6% of the original kernel matrix.

torch.Size([10235, 2])
We keep 1.60e+06/4.62e+07 =  3% of the original kernel matrix.

torch.Size([1862, 2])
We keep 8.13e+04/7.29e+05 = 11% of the original kernel matrix.

torch.Size([6366, 2])
We keep 5.95e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([5648, 2])
We keep 5.56e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([10029, 2])
We keep 1.51e+06/4.33e+07 =  3% of the original kernel matrix.

torch.Size([10877, 2])
We keep 3.57e+06/5.87e+07 =  6% of the original kernel matrix.

torch.Size([13870, 2])
We keep 2.87e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([9072, 2])
We keep 2.33e+06/4.09e+07 =  5% of the original kernel matrix.

torch.Size([12898, 2])
We keep 2.37e+06/8.54e+07 =  2% of the original kernel matrix.

torch.Size([5265, 2])
We keep 6.95e+05/9.02e+06 =  7% of the original kernel matrix.

torch.Size([9683, 2])
We keep 1.47e+06/4.01e+07 =  3% of the original kernel matrix.

torch.Size([755, 2])
We keep 1.52e+04/9.06e+04 = 16% of the original kernel matrix.

torch.Size([4535, 2])
We keep 2.96e+05/4.02e+06 =  7% of the original kernel matrix.

torch.Size([4022, 2])
We keep 3.59e+05/4.51e+06 =  7% of the original kernel matrix.

torch.Size([8531, 2])
We keep 1.13e+06/2.84e+07 =  3% of the original kernel matrix.

torch.Size([12760, 2])
We keep 9.47e+06/9.18e+07 = 10% of the original kernel matrix.

torch.Size([14891, 2])
We keep 3.53e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([24909, 2])
We keep 2.62e+07/3.57e+08 =  7% of the original kernel matrix.

torch.Size([20816, 2])
We keep 6.08e+06/2.52e+08 =  2% of the original kernel matrix.

torch.Size([3672, 2])
We keep 2.80e+05/3.34e+06 =  8% of the original kernel matrix.

torch.Size([8266, 2])
We keep 1.01e+06/2.44e+07 =  4% of the original kernel matrix.

torch.Size([14821, 2])
We keep 5.79e+06/1.28e+08 =  4% of the original kernel matrix.

torch.Size([16154, 2])
We keep 3.97e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([19662, 2])
We keep 2.14e+07/2.58e+08 =  8% of the original kernel matrix.

torch.Size([18259, 2])
We keep 5.28e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([7715, 2])
We keep 1.78e+06/2.43e+07 =  7% of the original kernel matrix.

torch.Size([11651, 2])
We keep 2.09e+06/6.58e+07 =  3% of the original kernel matrix.

torch.Size([298093, 2])
We keep 1.95e+09/8.21e+10 =  2% of the original kernel matrix.

torch.Size([73418, 2])
We keep 6.64e+07/3.82e+09 =  1% of the original kernel matrix.

torch.Size([2222, 2])
We keep 1.08e+05/1.04e+06 = 10% of the original kernel matrix.

torch.Size([6840, 2])
We keep 6.64e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([5234, 2])
We keep 6.05e+05/9.34e+06 =  6% of the original kernel matrix.

torch.Size([9640, 2])
We keep 1.40e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([91543, 2])
We keep 2.12e+08/6.71e+09 =  3% of the original kernel matrix.

torch.Size([39255, 2])
We keep 2.15e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([529286, 2])
We keep 1.52e+09/1.42e+11 =  1% of the original kernel matrix.

torch.Size([102988, 2])
We keep 8.28e+07/5.02e+09 =  1% of the original kernel matrix.

torch.Size([5064, 2])
We keep 5.79e+05/8.28e+06 =  6% of the original kernel matrix.

torch.Size([9514, 2])
We keep 1.33e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([11296, 2])
We keep 4.57e+06/7.20e+07 =  6% of the original kernel matrix.

torch.Size([13968, 2])
We keep 3.24e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([6564, 2])
We keep 1.20e+06/1.48e+07 =  8% of the original kernel matrix.

torch.Size([10681, 2])
We keep 1.74e+06/5.13e+07 =  3% of the original kernel matrix.

torch.Size([26627, 2])
We keep 3.56e+07/5.32e+08 =  6% of the original kernel matrix.

torch.Size([21701, 2])
We keep 7.21e+06/3.08e+08 =  2% of the original kernel matrix.

torch.Size([16286, 2])
We keep 1.54e+07/2.18e+08 =  7% of the original kernel matrix.

torch.Size([16321, 2])
We keep 5.02e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([42719, 2])
We keep 5.18e+07/1.79e+09 =  2% of the original kernel matrix.

torch.Size([26469, 2])
We keep 1.21e+07/5.64e+08 =  2% of the original kernel matrix.

torch.Size([22645, 2])
We keep 2.07e+07/3.58e+08 =  5% of the original kernel matrix.

torch.Size([20269, 2])
We keep 5.83e+06/2.52e+08 =  2% of the original kernel matrix.

torch.Size([12618, 2])
We keep 3.93e+06/7.80e+07 =  5% of the original kernel matrix.

torch.Size([15102, 2])
We keep 3.28e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([5777, 2])
We keep 3.52e+06/1.45e+07 = 24% of the original kernel matrix.

torch.Size([9956, 2])
We keep 1.80e+06/5.08e+07 =  3% of the original kernel matrix.

torch.Size([21637, 2])
We keep 6.31e+07/5.99e+08 = 10% of the original kernel matrix.

torch.Size([19483, 2])
We keep 6.18e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([4872, 2])
We keep 7.25e+05/6.80e+06 = 10% of the original kernel matrix.

torch.Size([9458, 2])
We keep 1.29e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([2324, 2])
We keep 1.28e+05/1.25e+06 = 10% of the original kernel matrix.

torch.Size([6916, 2])
We keep 7.09e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([48640, 2])
We keep 3.89e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([28566, 2])
We keep 1.08e+07/5.05e+08 =  2% of the original kernel matrix.

torch.Size([6366, 2])
We keep 2.06e+06/2.80e+07 =  7% of the original kernel matrix.

torch.Size([9892, 2])
We keep 2.14e+06/7.06e+07 =  3% of the original kernel matrix.

torch.Size([83500, 2])
We keep 1.25e+08/4.95e+09 =  2% of the original kernel matrix.

torch.Size([38793, 2])
We keep 1.83e+07/9.39e+08 =  1% of the original kernel matrix.

torch.Size([104492, 2])
We keep 9.94e+07/6.04e+09 =  1% of the original kernel matrix.

torch.Size([43417, 2])
We keep 2.01e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([2153, 2])
We keep 8.62e+04/9.18e+05 =  9% of the original kernel matrix.

torch.Size([6732, 2])
We keep 6.30e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([6325, 2])
We keep 8.20e+05/1.33e+07 =  6% of the original kernel matrix.

torch.Size([10643, 2])
We keep 1.62e+06/4.86e+07 =  3% of the original kernel matrix.

torch.Size([6429, 2])
We keep 8.83e+05/1.46e+07 =  6% of the original kernel matrix.

torch.Size([10814, 2])
We keep 1.77e+06/5.11e+07 =  3% of the original kernel matrix.

torch.Size([6951, 2])
We keep 8.41e+05/1.62e+07 =  5% of the original kernel matrix.

torch.Size([11130, 2])
We keep 1.74e+06/5.38e+07 =  3% of the original kernel matrix.

torch.Size([83814, 2])
We keep 1.25e+08/5.15e+09 =  2% of the original kernel matrix.

torch.Size([38776, 2])
We keep 1.90e+07/9.58e+08 =  1% of the original kernel matrix.

torch.Size([7019, 2])
We keep 3.58e+06/1.98e+07 = 18% of the original kernel matrix.

torch.Size([10939, 2])
We keep 1.65e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([40708, 2])
We keep 2.05e+07/9.17e+08 =  2% of the original kernel matrix.

torch.Size([26378, 2])
We keep 8.93e+06/4.04e+08 =  2% of the original kernel matrix.

torch.Size([1055, 2])
We keep 3.63e+04/2.56e+05 = 14% of the original kernel matrix.

torch.Size([5145, 2])
We keep 4.24e+05/6.76e+06 =  6% of the original kernel matrix.

torch.Size([4868, 2])
We keep 5.94e+05/7.47e+06 =  7% of the original kernel matrix.

torch.Size([9328, 2])
We keep 1.35e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([3648, 2])
We keep 3.19e+05/3.78e+06 =  8% of the original kernel matrix.

torch.Size([8267, 2])
We keep 9.93e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([101727, 2])
We keep 3.45e+08/1.04e+10 =  3% of the original kernel matrix.

torch.Size([41865, 2])
We keep 2.63e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([17346, 2])
We keep 8.73e+06/2.25e+08 =  3% of the original kernel matrix.

torch.Size([17114, 2])
We keep 5.05e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([3990, 2])
We keep 3.35e+05/4.63e+06 =  7% of the original kernel matrix.

torch.Size([8575, 2])
We keep 1.11e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([191226, 2])
We keep 3.67e+08/2.61e+10 =  1% of the original kernel matrix.

torch.Size([59451, 2])
We keep 3.85e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([16969, 2])
We keep 6.39e+06/1.54e+08 =  4% of the original kernel matrix.

torch.Size([17718, 2])
We keep 4.33e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([19159, 2])
We keep 6.24e+06/2.04e+08 =  3% of the original kernel matrix.

torch.Size([18448, 2])
We keep 4.86e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([51430, 2])
We keep 6.39e+07/2.14e+09 =  2% of the original kernel matrix.

torch.Size([29930, 2])
We keep 1.31e+07/6.17e+08 =  2% of the original kernel matrix.

torch.Size([4920, 2])
We keep 7.02e+05/7.13e+06 =  9% of the original kernel matrix.

torch.Size([9436, 2])
We keep 1.36e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([23039, 2])
We keep 1.34e+07/3.60e+08 =  3% of the original kernel matrix.

torch.Size([20093, 2])
We keep 6.00e+06/2.53e+08 =  2% of the original kernel matrix.

torch.Size([17420, 2])
We keep 1.04e+07/1.88e+08 =  5% of the original kernel matrix.

torch.Size([17789, 2])
We keep 4.62e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([82510, 2])
We keep 8.98e+07/3.99e+09 =  2% of the original kernel matrix.

torch.Size([38050, 2])
We keep 1.68e+07/8.43e+08 =  1% of the original kernel matrix.

torch.Size([6828, 2])
We keep 1.31e+06/2.13e+07 =  6% of the original kernel matrix.

torch.Size([10989, 2])
We keep 2.05e+06/6.16e+07 =  3% of the original kernel matrix.

torch.Size([7231, 2])
We keep 1.12e+06/1.91e+07 =  5% of the original kernel matrix.

torch.Size([11025, 2])
We keep 1.91e+06/5.83e+07 =  3% of the original kernel matrix.

torch.Size([12454, 2])
We keep 5.37e+06/1.14e+08 =  4% of the original kernel matrix.

torch.Size([13810, 2])
We keep 3.76e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([5693, 2])
We keep 6.05e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([9852, 2])
We keep 1.49e+06/4.24e+07 =  3% of the original kernel matrix.

torch.Size([7820, 2])
We keep 1.66e+06/3.18e+07 =  5% of the original kernel matrix.

torch.Size([11716, 2])
We keep 2.22e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([9263, 2])
We keep 2.97e+06/4.30e+07 =  6% of the original kernel matrix.

torch.Size([12954, 2])
We keep 2.62e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([15667, 2])
We keep 5.09e+06/1.40e+08 =  3% of the original kernel matrix.

torch.Size([16640, 2])
We keep 4.17e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([61504, 2])
We keep 1.90e+08/4.25e+09 =  4% of the original kernel matrix.

torch.Size([33283, 2])
We keep 1.55e+07/8.71e+08 =  1% of the original kernel matrix.

torch.Size([16185, 2])
We keep 5.68e+06/1.39e+08 =  4% of the original kernel matrix.

torch.Size([17265, 2])
We keep 3.92e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([3556, 2])
We keep 4.49e+05/4.71e+06 =  9% of the original kernel matrix.

torch.Size([8085, 2])
We keep 1.17e+06/2.90e+07 =  4% of the original kernel matrix.

torch.Size([6090, 2])
We keep 8.33e+05/1.39e+07 =  6% of the original kernel matrix.

torch.Size([10452, 2])
We keep 1.66e+06/4.97e+07 =  3% of the original kernel matrix.

torch.Size([14664, 2])
We keep 3.51e+07/4.81e+08 =  7% of the original kernel matrix.

torch.Size([15726, 2])
We keep 5.81e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([16308, 2])
We keep 7.36e+06/1.68e+08 =  4% of the original kernel matrix.

torch.Size([17111, 2])
We keep 4.33e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([4550, 2])
We keep 8.86e+05/7.54e+06 = 11% of the original kernel matrix.

torch.Size([8851, 2])
We keep 1.35e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([5680, 2])
We keep 6.09e+05/9.83e+06 =  6% of the original kernel matrix.

torch.Size([10668, 2])
We keep 1.55e+06/4.19e+07 =  3% of the original kernel matrix.

torch.Size([34209, 2])
We keep 7.28e+07/9.15e+08 =  7% of the original kernel matrix.

torch.Size([24421, 2])
We keep 8.90e+06/4.04e+08 =  2% of the original kernel matrix.

torch.Size([5443, 2])
We keep 5.01e+05/8.13e+06 =  6% of the original kernel matrix.

torch.Size([9776, 2])
We keep 1.38e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([263104, 2])
We keep 3.72e+09/9.63e+10 =  3% of the original kernel matrix.

torch.Size([66300, 2])
We keep 7.22e+07/4.14e+09 =  1% of the original kernel matrix.

torch.Size([3394, 2])
We keep 3.07e+05/4.02e+06 =  7% of the original kernel matrix.

torch.Size([7995, 2])
We keep 1.05e+06/2.68e+07 =  3% of the original kernel matrix.

torch.Size([4892, 2])
We keep 3.60e+06/1.84e+07 = 19% of the original kernel matrix.

torch.Size([8991, 2])
We keep 1.95e+06/5.73e+07 =  3% of the original kernel matrix.

torch.Size([15923, 2])
We keep 6.44e+06/1.60e+08 =  4% of the original kernel matrix.

torch.Size([16835, 2])
We keep 4.13e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([5047, 2])
We keep 5.01e+06/1.28e+07 = 39% of the original kernel matrix.

torch.Size([9642, 2])
We keep 1.29e+06/4.78e+07 =  2% of the original kernel matrix.

torch.Size([32518, 2])
We keep 1.65e+07/7.19e+08 =  2% of the original kernel matrix.

torch.Size([23922, 2])
We keep 8.07e+06/3.58e+08 =  2% of the original kernel matrix.

torch.Size([5187, 2])
We keep 4.73e+05/7.27e+06 =  6% of the original kernel matrix.

torch.Size([9615, 2])
We keep 1.33e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([86079, 2])
We keep 6.67e+07/4.06e+09 =  1% of the original kernel matrix.

torch.Size([39327, 2])
We keep 1.68e+07/8.50e+08 =  1% of the original kernel matrix.

torch.Size([101613, 2])
We keep 1.29e+08/7.34e+09 =  1% of the original kernel matrix.

torch.Size([43155, 2])
We keep 2.21e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([4985, 2])
We keep 3.82e+05/6.52e+06 =  5% of the original kernel matrix.

torch.Size([9578, 2])
We keep 1.20e+06/3.41e+07 =  3% of the original kernel matrix.

torch.Size([31780, 2])
We keep 1.56e+07/5.57e+08 =  2% of the original kernel matrix.

torch.Size([23657, 2])
We keep 7.21e+06/3.15e+08 =  2% of the original kernel matrix.

torch.Size([77238, 2])
We keep 8.39e+07/4.02e+09 =  2% of the original kernel matrix.

torch.Size([37214, 2])
We keep 1.70e+07/8.46e+08 =  2% of the original kernel matrix.

torch.Size([158222, 2])
We keep 3.60e+08/2.27e+10 =  1% of the original kernel matrix.

torch.Size([55125, 2])
We keep 3.69e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([74945, 2])
We keep 8.30e+07/3.70e+09 =  2% of the original kernel matrix.

torch.Size([36341, 2])
We keep 1.62e+07/8.12e+08 =  1% of the original kernel matrix.

torch.Size([2370, 2])
We keep 3.02e+05/1.81e+06 = 16% of the original kernel matrix.

torch.Size([6837, 2])
We keep 8.07e+05/1.79e+07 =  4% of the original kernel matrix.

torch.Size([10638, 2])
We keep 2.12e+06/5.09e+07 =  4% of the original kernel matrix.

torch.Size([13529, 2])
We keep 2.78e+06/9.52e+07 =  2% of the original kernel matrix.

torch.Size([10145, 2])
We keep 2.31e+06/5.65e+07 =  4% of the original kernel matrix.

torch.Size([13508, 2])
We keep 2.78e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([110372, 2])
We keep 5.15e+08/8.11e+09 =  6% of the original kernel matrix.

torch.Size([44513, 2])
We keep 2.29e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([8715, 2])
We keep 1.18e+06/2.49e+07 =  4% of the original kernel matrix.

torch.Size([12341, 2])
We keep 2.10e+06/6.66e+07 =  3% of the original kernel matrix.

torch.Size([3060, 2])
We keep 2.07e+05/2.25e+06 =  9% of the original kernel matrix.

torch.Size([7723, 2])
We keep 8.81e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([97621, 2])
We keep 2.30e+08/6.32e+09 =  3% of the original kernel matrix.

torch.Size([42092, 2])
We keep 2.10e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([121133, 2])
We keep 1.98e+08/9.92e+09 =  1% of the original kernel matrix.

torch.Size([46781, 2])
We keep 2.52e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([17766, 2])
We keep 3.39e+07/4.60e+08 =  7% of the original kernel matrix.

torch.Size([16120, 2])
We keep 6.14e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([6478, 2])
We keep 2.36e+06/2.16e+07 = 10% of the original kernel matrix.

torch.Size([10593, 2])
We keep 2.04e+06/6.20e+07 =  3% of the original kernel matrix.

torch.Size([8221, 2])
We keep 2.31e+06/3.00e+07 =  7% of the original kernel matrix.

torch.Size([12002, 2])
We keep 2.27e+06/7.31e+07 =  3% of the original kernel matrix.

torch.Size([6289, 2])
We keep 8.71e+05/1.44e+07 =  6% of the original kernel matrix.

torch.Size([10561, 2])
We keep 1.78e+06/5.07e+07 =  3% of the original kernel matrix.

torch.Size([5001, 2])
We keep 6.66e+05/8.51e+06 =  7% of the original kernel matrix.

torch.Size([9519, 2])
We keep 1.46e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([7980, 2])
We keep 1.24e+06/2.47e+07 =  5% of the original kernel matrix.

torch.Size([11876, 2])
We keep 2.13e+06/6.63e+07 =  3% of the original kernel matrix.

torch.Size([6541, 2])
We keep 7.43e+05/1.35e+07 =  5% of the original kernel matrix.

torch.Size([10761, 2])
We keep 1.66e+06/4.90e+07 =  3% of the original kernel matrix.

torch.Size([22513, 2])
We keep 1.35e+07/3.94e+08 =  3% of the original kernel matrix.

torch.Size([19027, 2])
We keep 6.15e+06/2.65e+08 =  2% of the original kernel matrix.

torch.Size([27531, 2])
We keep 9.99e+06/4.09e+08 =  2% of the original kernel matrix.

torch.Size([22343, 2])
We keep 6.27e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([19106, 2])
We keep 7.64e+06/2.12e+08 =  3% of the original kernel matrix.

torch.Size([18610, 2])
We keep 4.85e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([8439, 2])
We keep 2.07e+06/3.17e+07 =  6% of the original kernel matrix.

torch.Size([12261, 2])
We keep 2.40e+06/7.52e+07 =  3% of the original kernel matrix.

torch.Size([3945, 2])
We keep 4.81e+05/4.67e+06 = 10% of the original kernel matrix.

torch.Size([8587, 2])
We keep 1.15e+06/2.89e+07 =  3% of the original kernel matrix.

torch.Size([564307, 2])
We keep 1.61e+09/1.57e+11 =  1% of the original kernel matrix.

torch.Size([105900, 2])
We keep 8.65e+07/5.29e+09 =  1% of the original kernel matrix.

torch.Size([5554, 2])
We keep 6.07e+05/9.30e+06 =  6% of the original kernel matrix.

torch.Size([9916, 2])
We keep 1.47e+06/4.07e+07 =  3% of the original kernel matrix.

torch.Size([71555, 2])
We keep 5.92e+07/2.81e+09 =  2% of the original kernel matrix.

torch.Size([35154, 2])
We keep 1.44e+07/7.07e+08 =  2% of the original kernel matrix.

torch.Size([2484, 2])
We keep 1.71e+05/1.72e+06 =  9% of the original kernel matrix.

torch.Size([7219, 2])
We keep 7.25e+05/1.75e+07 =  4% of the original kernel matrix.

torch.Size([3359, 2])
We keep 2.21e+05/2.89e+06 =  7% of the original kernel matrix.

torch.Size([8082, 2])
We keep 9.60e+05/2.27e+07 =  4% of the original kernel matrix.

torch.Size([2785, 2])
We keep 1.51e+05/1.70e+06 =  8% of the original kernel matrix.

torch.Size([7499, 2])
We keep 7.94e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([14151, 2])
We keep 7.48e+06/1.20e+08 =  6% of the original kernel matrix.

torch.Size([16025, 2])
We keep 3.55e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([3213, 2])
We keep 2.74e+05/3.17e+06 =  8% of the original kernel matrix.

torch.Size([8027, 2])
We keep 1.04e+06/2.38e+07 =  4% of the original kernel matrix.

torch.Size([3126, 2])
We keep 2.22e+05/2.37e+06 =  9% of the original kernel matrix.

torch.Size([7683, 2])
We keep 8.41e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([3303, 2])
We keep 2.16e+05/2.57e+06 =  8% of the original kernel matrix.

torch.Size([7917, 2])
We keep 9.18e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([2631, 2])
We keep 1.90e+05/2.27e+06 =  8% of the original kernel matrix.

torch.Size([7219, 2])
We keep 8.92e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([248450, 2])
We keep 9.41e+08/5.27e+10 =  1% of the original kernel matrix.

torch.Size([66973, 2])
We keep 5.41e+07/3.06e+09 =  1% of the original kernel matrix.

torch.Size([51331, 2])
We keep 9.52e+07/2.04e+09 =  4% of the original kernel matrix.

torch.Size([29557, 2])
We keep 1.28e+07/6.03e+08 =  2% of the original kernel matrix.

torch.Size([9610, 2])
We keep 2.06e+06/3.81e+07 =  5% of the original kernel matrix.

torch.Size([13142, 2])
We keep 2.47e+06/8.24e+07 =  2% of the original kernel matrix.

torch.Size([34113, 2])
We keep 2.95e+07/7.98e+08 =  3% of the original kernel matrix.

torch.Size([24543, 2])
We keep 8.51e+06/3.77e+08 =  2% of the original kernel matrix.

torch.Size([6118, 2])
We keep 5.07e+06/1.50e+07 = 33% of the original kernel matrix.

torch.Size([10421, 2])
We keep 1.39e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([3317, 2])
We keep 2.04e+05/2.59e+06 =  7% of the original kernel matrix.

torch.Size([7995, 2])
We keep 9.08e+05/2.15e+07 =  4% of the original kernel matrix.

torch.Size([6213, 2])
We keep 9.94e+05/1.37e+07 =  7% of the original kernel matrix.

torch.Size([10363, 2])
We keep 1.72e+06/4.95e+07 =  3% of the original kernel matrix.

torch.Size([7058, 2])
We keep 9.39e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([11137, 2])
We keep 1.79e+06/5.46e+07 =  3% of the original kernel matrix.

torch.Size([1368, 2])
We keep 4.62e+04/3.73e+05 = 12% of the original kernel matrix.

torch.Size([5727, 2])
We keep 4.82e+05/8.16e+06 =  5% of the original kernel matrix.

torch.Size([5674, 2])
We keep 7.70e+05/1.19e+07 =  6% of the original kernel matrix.

torch.Size([9854, 2])
We keep 1.63e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([5960, 2])
We keep 8.79e+05/1.45e+07 =  6% of the original kernel matrix.

torch.Size([10197, 2])
We keep 1.74e+06/5.09e+07 =  3% of the original kernel matrix.

torch.Size([3847, 2])
We keep 6.01e+05/5.27e+06 = 11% of the original kernel matrix.

torch.Size([8286, 2])
We keep 1.19e+06/3.06e+07 =  3% of the original kernel matrix.

torch.Size([3909, 2])
We keep 3.07e+05/4.28e+06 =  7% of the original kernel matrix.

torch.Size([8523, 2])
We keep 1.10e+06/2.76e+07 =  3% of the original kernel matrix.

torch.Size([3902, 2])
We keep 3.24e+05/4.12e+06 =  7% of the original kernel matrix.

torch.Size([8295, 2])
We keep 1.09e+06/2.71e+07 =  4% of the original kernel matrix.

torch.Size([4085, 2])
We keep 3.68e+05/5.13e+06 =  7% of the original kernel matrix.

torch.Size([8759, 2])
We keep 1.20e+06/3.02e+07 =  3% of the original kernel matrix.

torch.Size([197679, 2])
We keep 6.98e+08/3.05e+10 =  2% of the original kernel matrix.

torch.Size([58784, 2])
We keep 4.07e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([7486, 2])
We keep 2.24e+06/2.76e+07 =  8% of the original kernel matrix.

torch.Size([11311, 2])
We keep 2.25e+06/7.01e+07 =  3% of the original kernel matrix.

torch.Size([14793, 2])
We keep 6.81e+06/1.43e+08 =  4% of the original kernel matrix.

torch.Size([16028, 2])
We keep 4.14e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([4534, 2])
We keep 7.17e+05/8.56e+06 =  8% of the original kernel matrix.

torch.Size([9216, 2])
We keep 1.33e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([26891, 2])
We keep 5.04e+07/7.94e+08 =  6% of the original kernel matrix.

torch.Size([20382, 2])
We keep 8.46e+06/3.76e+08 =  2% of the original kernel matrix.

torch.Size([3718, 2])
We keep 2.67e+05/3.49e+06 =  7% of the original kernel matrix.

torch.Size([8432, 2])
We keep 1.00e+06/2.49e+07 =  4% of the original kernel matrix.

torch.Size([525216, 2])
We keep 6.59e+09/2.47e+11 =  2% of the original kernel matrix.

torch.Size([97776, 2])
We keep 1.02e+08/6.63e+09 =  1% of the original kernel matrix.

torch.Size([432073, 2])
We keep 1.09e+09/9.67e+10 =  1% of the original kernel matrix.

torch.Size([92527, 2])
We keep 6.90e+07/4.15e+09 =  1% of the original kernel matrix.

torch.Size([22943, 2])
We keep 1.09e+07/2.93e+08 =  3% of the original kernel matrix.

torch.Size([20391, 2])
We keep 5.29e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([12867, 2])
We keep 2.43e+06/7.23e+07 =  3% of the original kernel matrix.

torch.Size([15245, 2])
We keep 3.07e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([3273, 2])
We keep 1.78e+05/2.25e+06 =  7% of the original kernel matrix.

torch.Size([7898, 2])
We keep 8.48e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([11493, 2])
We keep 7.65e+06/1.06e+08 =  7% of the original kernel matrix.

torch.Size([13879, 2])
We keep 3.42e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([6292, 2])
We keep 1.51e+06/1.90e+07 =  7% of the original kernel matrix.

torch.Size([10648, 2])
We keep 1.76e+06/5.82e+07 =  3% of the original kernel matrix.

torch.Size([52377, 2])
We keep 4.75e+07/1.48e+09 =  3% of the original kernel matrix.

torch.Size([30254, 2])
We keep 1.10e+07/5.14e+08 =  2% of the original kernel matrix.

torch.Size([4214, 2])
We keep 3.62e+05/5.14e+06 =  7% of the original kernel matrix.

torch.Size([8697, 2])
We keep 1.15e+06/3.03e+07 =  3% of the original kernel matrix.

torch.Size([69303, 2])
We keep 1.14e+08/3.39e+09 =  3% of the original kernel matrix.

torch.Size([34657, 2])
We keep 1.61e+07/7.78e+08 =  2% of the original kernel matrix.

torch.Size([72080, 2])
We keep 9.32e+07/3.90e+09 =  2% of the original kernel matrix.

torch.Size([34740, 2])
We keep 1.67e+07/8.33e+08 =  2% of the original kernel matrix.

torch.Size([17413, 2])
We keep 1.01e+07/2.27e+08 =  4% of the original kernel matrix.

torch.Size([16972, 2])
We keep 4.99e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([7031, 2])
We keep 2.23e+06/2.95e+07 =  7% of the original kernel matrix.

torch.Size([10964, 2])
We keep 2.36e+06/7.25e+07 =  3% of the original kernel matrix.

torch.Size([5370, 2])
We keep 7.14e+05/1.03e+07 =  6% of the original kernel matrix.

torch.Size([10467, 2])
We keep 1.58e+06/4.28e+07 =  3% of the original kernel matrix.

torch.Size([7869, 2])
We keep 1.67e+06/2.82e+07 =  5% of the original kernel matrix.

torch.Size([11721, 2])
We keep 2.32e+06/7.09e+07 =  3% of the original kernel matrix.

torch.Size([3650, 2])
We keep 2.63e+05/3.70e+06 =  7% of the original kernel matrix.

torch.Size([8287, 2])
We keep 9.94e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([79376, 2])
We keep 8.56e+07/4.05e+09 =  2% of the original kernel matrix.

torch.Size([37523, 2])
We keep 1.71e+07/8.49e+08 =  2% of the original kernel matrix.

torch.Size([25915, 2])
We keep 2.11e+07/4.82e+08 =  4% of the original kernel matrix.

torch.Size([20760, 2])
We keep 6.81e+06/2.93e+08 =  2% of the original kernel matrix.

torch.Size([3180, 2])
We keep 3.69e+05/3.34e+06 = 11% of the original kernel matrix.

torch.Size([7590, 2])
We keep 1.02e+06/2.44e+07 =  4% of the original kernel matrix.

torch.Size([83641, 2])
We keep 8.94e+07/3.95e+09 =  2% of the original kernel matrix.

torch.Size([38700, 2])
We keep 1.65e+07/8.39e+08 =  1% of the original kernel matrix.

torch.Size([8746, 2])
We keep 1.66e+06/3.34e+07 =  4% of the original kernel matrix.

torch.Size([12920, 2])
We keep 2.45e+06/7.71e+07 =  3% of the original kernel matrix.

torch.Size([1685, 2])
We keep 6.49e+04/5.43e+05 = 11% of the original kernel matrix.

torch.Size([6176, 2])
We keep 5.41e+05/9.84e+06 =  5% of the original kernel matrix.

torch.Size([12502, 2])
We keep 4.49e+06/7.24e+07 =  6% of the original kernel matrix.

torch.Size([14917, 2])
We keep 2.99e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([91097, 2])
We keep 8.69e+07/4.79e+09 =  1% of the original kernel matrix.

torch.Size([40383, 2])
We keep 1.81e+07/9.24e+08 =  1% of the original kernel matrix.

torch.Size([40724, 2])
We keep 5.92e+07/1.29e+09 =  4% of the original kernel matrix.

torch.Size([26531, 2])
We keep 1.07e+07/4.80e+08 =  2% of the original kernel matrix.

torch.Size([101784, 2])
We keep 4.01e+08/1.05e+10 =  3% of the original kernel matrix.

torch.Size([41535, 2])
We keep 2.65e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([59608, 2])
We keep 8.75e+07/2.66e+09 =  3% of the original kernel matrix.

torch.Size([31064, 2])
We keep 1.43e+07/6.88e+08 =  2% of the original kernel matrix.

torch.Size([6243, 2])
We keep 1.18e+06/1.38e+07 =  8% of the original kernel matrix.

torch.Size([10470, 2])
We keep 1.72e+06/4.96e+07 =  3% of the original kernel matrix.

torch.Size([35251, 2])
We keep 2.55e+07/8.40e+08 =  3% of the original kernel matrix.

torch.Size([24116, 2])
We keep 8.56e+06/3.87e+08 =  2% of the original kernel matrix.

torch.Size([105584, 2])
We keep 1.04e+08/6.25e+09 =  1% of the original kernel matrix.

torch.Size([43764, 2])
We keep 2.03e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([86230, 2])
We keep 7.52e+07/4.15e+09 =  1% of the original kernel matrix.

torch.Size([39284, 2])
We keep 1.67e+07/8.60e+08 =  1% of the original kernel matrix.

torch.Size([5561, 2])
We keep 5.74e+05/9.60e+06 =  5% of the original kernel matrix.

torch.Size([9910, 2])
We keep 1.46e+06/4.14e+07 =  3% of the original kernel matrix.

torch.Size([59663, 2])
We keep 2.48e+08/4.98e+09 =  4% of the original kernel matrix.

torch.Size([32909, 2])
We keep 1.66e+07/9.42e+08 =  1% of the original kernel matrix.

torch.Size([158895, 2])
We keep 3.88e+08/1.68e+10 =  2% of the original kernel matrix.

torch.Size([54893, 2])
We keep 2.91e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([46314, 2])
We keep 3.52e+07/1.16e+09 =  3% of the original kernel matrix.

torch.Size([28488, 2])
We keep 9.71e+06/4.54e+08 =  2% of the original kernel matrix.

torch.Size([21616, 2])
We keep 2.92e+07/6.12e+08 =  4% of the original kernel matrix.

torch.Size([19310, 2])
We keep 7.06e+06/3.30e+08 =  2% of the original kernel matrix.

torch.Size([2642, 2])
We keep 2.46e+05/1.96e+06 = 12% of the original kernel matrix.

torch.Size([7254, 2])
We keep 8.35e+05/1.87e+07 =  4% of the original kernel matrix.

torch.Size([8428, 2])
We keep 2.98e+06/3.41e+07 =  8% of the original kernel matrix.

torch.Size([12095, 2])
We keep 2.42e+06/7.79e+07 =  3% of the original kernel matrix.

torch.Size([4193, 2])
We keep 3.31e+05/4.60e+06 =  7% of the original kernel matrix.

torch.Size([8711, 2])
We keep 1.13e+06/2.86e+07 =  3% of the original kernel matrix.

torch.Size([3192, 2])
We keep 1.91e+05/2.40e+06 =  7% of the original kernel matrix.

torch.Size([7818, 2])
We keep 8.76e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([133914, 2])
We keep 1.62e+08/1.08e+10 =  1% of the original kernel matrix.

torch.Size([50100, 2])
We keep 2.60e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([78783, 2])
We keep 6.37e+07/3.47e+09 =  1% of the original kernel matrix.

torch.Size([37472, 2])
We keep 1.57e+07/7.87e+08 =  1% of the original kernel matrix.

torch.Size([87020, 2])
We keep 1.52e+08/5.63e+09 =  2% of the original kernel matrix.

torch.Size([38917, 2])
We keep 1.93e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([19346, 2])
We keep 1.26e+07/2.37e+08 =  5% of the original kernel matrix.

torch.Size([18985, 2])
We keep 5.14e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([35835, 2])
We keep 3.12e+07/8.47e+08 =  3% of the original kernel matrix.

torch.Size([24574, 2])
We keep 8.68e+06/3.89e+08 =  2% of the original kernel matrix.

torch.Size([11740, 2])
We keep 3.33e+06/9.29e+07 =  3% of the original kernel matrix.

torch.Size([14216, 2])
We keep 3.54e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([31721, 2])
We keep 1.29e+07/5.43e+08 =  2% of the original kernel matrix.

torch.Size([23840, 2])
We keep 6.91e+06/3.11e+08 =  2% of the original kernel matrix.

torch.Size([1410, 2])
We keep 4.88e+04/3.98e+05 = 12% of the original kernel matrix.

torch.Size([5667, 2])
We keep 4.80e+05/8.42e+06 =  5% of the original kernel matrix.

torch.Size([8978, 2])
We keep 1.75e+06/3.14e+07 =  5% of the original kernel matrix.

torch.Size([12385, 2])
We keep 2.30e+06/7.49e+07 =  3% of the original kernel matrix.

torch.Size([98681, 2])
We keep 1.60e+08/6.94e+09 =  2% of the original kernel matrix.

torch.Size([41993, 2])
We keep 2.16e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([7082, 2])
We keep 8.72e+05/1.60e+07 =  5% of the original kernel matrix.

torch.Size([11033, 2])
We keep 1.77e+06/5.34e+07 =  3% of the original kernel matrix.

torch.Size([4401, 2])
We keep 4.36e+05/5.86e+06 =  7% of the original kernel matrix.

torch.Size([8900, 2])
We keep 1.25e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([8558, 2])
We keep 1.45e+06/3.26e+07 =  4% of the original kernel matrix.

torch.Size([12205, 2])
We keep 2.32e+06/7.62e+07 =  3% of the original kernel matrix.

torch.Size([104722, 2])
We keep 1.21e+09/1.62e+10 =  7% of the original kernel matrix.

torch.Size([42267, 2])
We keep 3.19e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([24935, 2])
We keep 1.79e+07/4.27e+08 =  4% of the original kernel matrix.

torch.Size([20593, 2])
We keep 6.59e+06/2.76e+08 =  2% of the original kernel matrix.

torch.Size([24735, 2])
We keep 1.13e+07/3.98e+08 =  2% of the original kernel matrix.

torch.Size([21232, 2])
We keep 6.32e+06/2.66e+08 =  2% of the original kernel matrix.

torch.Size([19457, 2])
We keep 9.44e+06/2.61e+08 =  3% of the original kernel matrix.

torch.Size([17751, 2])
We keep 5.32e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([19061, 2])
We keep 1.58e+07/3.37e+08 =  4% of the original kernel matrix.

torch.Size([17637, 2])
We keep 5.91e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([2128, 2])
We keep 1.14e+05/1.16e+06 =  9% of the original kernel matrix.

torch.Size([6708, 2])
We keep 6.90e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([1830, 2])
We keep 9.07e+04/8.23e+05 = 11% of the original kernel matrix.

torch.Size([6451, 2])
We keep 6.28e+05/1.21e+07 =  5% of the original kernel matrix.

torch.Size([27301, 2])
We keep 1.37e+07/4.48e+08 =  3% of the original kernel matrix.

torch.Size([21952, 2])
We keep 6.65e+06/2.83e+08 =  2% of the original kernel matrix.

torch.Size([13802, 2])
We keep 4.73e+06/1.02e+08 =  4% of the original kernel matrix.

torch.Size([15720, 2])
We keep 3.73e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([3573, 2])
We keep 2.88e+05/3.59e+06 =  8% of the original kernel matrix.

torch.Size([8177, 2])
We keep 1.02e+06/2.53e+07 =  4% of the original kernel matrix.

torch.Size([172411, 2])
We keep 2.34e+08/1.80e+10 =  1% of the original kernel matrix.

torch.Size([57611, 2])
We keep 3.25e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([3112, 2])
We keep 1.79e+05/2.15e+06 =  8% of the original kernel matrix.

torch.Size([7873, 2])
We keep 8.50e+05/1.96e+07 =  4% of the original kernel matrix.

torch.Size([14822, 2])
We keep 3.17e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([16471, 2])
We keep 3.60e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([5937, 2])
We keep 1.00e+06/1.38e+07 =  7% of the original kernel matrix.

torch.Size([10402, 2])
We keep 1.67e+06/4.97e+07 =  3% of the original kernel matrix.

torch.Size([57631, 2])
We keep 4.23e+07/1.97e+09 =  2% of the original kernel matrix.

torch.Size([30938, 2])
We keep 1.25e+07/5.93e+08 =  2% of the original kernel matrix.

torch.Size([6051, 2])
We keep 8.40e+05/1.34e+07 =  6% of the original kernel matrix.

torch.Size([10387, 2])
We keep 1.69e+06/4.89e+07 =  3% of the original kernel matrix.

torch.Size([13030, 2])
We keep 6.53e+06/9.48e+07 =  6% of the original kernel matrix.

torch.Size([15236, 2])
We keep 3.59e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([8346, 2])
We keep 1.84e+06/2.64e+07 =  6% of the original kernel matrix.

torch.Size([11985, 2])
We keep 2.11e+06/6.86e+07 =  3% of the original kernel matrix.

torch.Size([12477, 2])
We keep 5.67e+06/1.07e+08 =  5% of the original kernel matrix.

torch.Size([15162, 2])
We keep 3.83e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([21115, 2])
We keep 6.85e+06/2.28e+08 =  3% of the original kernel matrix.

torch.Size([19421, 2])
We keep 4.97e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([429276, 2])
We keep 1.11e+09/9.85e+10 =  1% of the original kernel matrix.

torch.Size([91321, 2])
We keep 6.97e+07/4.19e+09 =  1% of the original kernel matrix.

torch.Size([5828, 2])
We keep 7.17e+05/1.10e+07 =  6% of the original kernel matrix.

torch.Size([10254, 2])
We keep 1.58e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([4873, 2])
We keep 7.61e+05/8.54e+06 =  8% of the original kernel matrix.

torch.Size([9315, 2])
We keep 1.41e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([70206, 2])
We keep 1.75e+08/4.41e+09 =  3% of the original kernel matrix.

torch.Size([34835, 2])
We keep 1.80e+07/8.87e+08 =  2% of the original kernel matrix.

torch.Size([7661, 2])
We keep 1.66e+06/2.45e+07 =  6% of the original kernel matrix.

torch.Size([11563, 2])
We keep 2.13e+06/6.61e+07 =  3% of the original kernel matrix.

torch.Size([115698, 2])
We keep 2.06e+08/9.58e+09 =  2% of the original kernel matrix.

torch.Size([46075, 2])
We keep 2.41e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([1869, 2])
We keep 7.94e+04/7.38e+05 = 10% of the original kernel matrix.

torch.Size([6484, 2])
We keep 5.89e+05/1.15e+07 =  5% of the original kernel matrix.

torch.Size([54703, 2])
We keep 5.20e+07/2.09e+09 =  2% of the original kernel matrix.

torch.Size([29788, 2])
We keep 1.28e+07/6.11e+08 =  2% of the original kernel matrix.

torch.Size([118057, 2])
We keep 1.30e+08/8.22e+09 =  1% of the original kernel matrix.

torch.Size([46413, 2])
We keep 2.29e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([3009, 2])
We keep 2.01e+05/2.37e+06 =  8% of the original kernel matrix.

torch.Size([7677, 2])
We keep 8.84e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([292374, 2])
We keep 5.82e+08/4.69e+10 =  1% of the original kernel matrix.

torch.Size([73346, 2])
We keep 4.99e+07/2.89e+09 =  1% of the original kernel matrix.

torch.Size([4672, 2])
We keep 9.90e+05/8.58e+06 = 11% of the original kernel matrix.

torch.Size([9316, 2])
We keep 1.29e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([34127, 2])
We keep 6.90e+07/1.07e+09 =  6% of the original kernel matrix.

torch.Size([24237, 2])
We keep 9.13e+06/4.36e+08 =  2% of the original kernel matrix.

torch.Size([8061, 2])
We keep 1.11e+06/2.18e+07 =  5% of the original kernel matrix.

torch.Size([11832, 2])
We keep 2.00e+06/6.23e+07 =  3% of the original kernel matrix.

torch.Size([112076, 2])
We keep 4.38e+08/1.33e+10 =  3% of the original kernel matrix.

torch.Size([44064, 2])
We keep 2.96e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([2444, 2])
We keep 1.87e+05/1.82e+06 = 10% of the original kernel matrix.

torch.Size([6957, 2])
We keep 7.98e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([2962, 2])
We keep 2.31e+05/2.65e+06 =  8% of the original kernel matrix.

torch.Size([7630, 2])
We keep 9.51e+05/2.17e+07 =  4% of the original kernel matrix.

torch.Size([4069, 2])
We keep 6.19e+05/5.98e+06 = 10% of the original kernel matrix.

torch.Size([8589, 2])
We keep 1.29e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([19513, 2])
We keep 7.14e+06/2.21e+08 =  3% of the original kernel matrix.

torch.Size([18778, 2])
We keep 4.74e+06/1.98e+08 =  2% of the original kernel matrix.

torch.Size([26438, 2])
We keep 2.22e+07/4.46e+08 =  4% of the original kernel matrix.

torch.Size([21091, 2])
We keep 6.78e+06/2.82e+08 =  2% of the original kernel matrix.

torch.Size([21119, 2])
We keep 1.05e+07/2.70e+08 =  3% of the original kernel matrix.

torch.Size([19654, 2])
We keep 5.26e+06/2.19e+08 =  2% of the original kernel matrix.

torch.Size([11141, 2])
We keep 2.41e+06/5.97e+07 =  4% of the original kernel matrix.

torch.Size([14151, 2])
We keep 2.95e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([3699, 2])
We keep 2.72e+05/3.55e+06 =  7% of the original kernel matrix.

torch.Size([8299, 2])
We keep 1.03e+06/2.52e+07 =  4% of the original kernel matrix.

torch.Size([8383, 2])
We keep 4.68e+06/4.24e+07 = 11% of the original kernel matrix.

torch.Size([11964, 2])
We keep 2.67e+06/8.69e+07 =  3% of the original kernel matrix.

torch.Size([58008, 2])
We keep 2.71e+08/5.16e+09 =  5% of the original kernel matrix.

torch.Size([31313, 2])
We keep 1.95e+07/9.59e+08 =  2% of the original kernel matrix.

torch.Size([10361, 2])
We keep 9.02e+06/1.05e+08 =  8% of the original kernel matrix.

torch.Size([13333, 2])
We keep 3.88e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([140125, 2])
We keep 2.62e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([51580, 2])
We keep 2.86e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([17607, 2])
We keep 9.12e+06/1.83e+08 =  4% of the original kernel matrix.

torch.Size([17847, 2])
We keep 4.41e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([9301, 2])
We keep 4.73e+06/6.59e+07 =  7% of the original kernel matrix.

torch.Size([12705, 2])
We keep 3.17e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([2680, 2])
We keep 1.27e+05/1.52e+06 =  8% of the original kernel matrix.

torch.Size([7338, 2])
We keep 7.55e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([5011, 2])
We keep 1.43e+06/1.14e+07 = 12% of the original kernel matrix.

torch.Size([9274, 2])
We keep 1.62e+06/4.51e+07 =  3% of the original kernel matrix.

torch.Size([429952, 2])
We keep 1.77e+09/1.17e+11 =  1% of the original kernel matrix.

torch.Size([93199, 2])
We keep 7.28e+07/4.56e+09 =  1% of the original kernel matrix.

torch.Size([5140, 2])
We keep 5.87e+05/8.71e+06 =  6% of the original kernel matrix.

torch.Size([9626, 2])
We keep 1.45e+06/3.94e+07 =  3% of the original kernel matrix.

torch.Size([8436, 2])
We keep 1.49e+06/2.87e+07 =  5% of the original kernel matrix.

torch.Size([12129, 2])
We keep 2.17e+06/7.15e+07 =  3% of the original kernel matrix.

torch.Size([56893, 2])
We keep 6.02e+07/1.89e+09 =  3% of the original kernel matrix.

torch.Size([31169, 2])
We keep 1.21e+07/5.80e+08 =  2% of the original kernel matrix.

torch.Size([25433, 2])
We keep 4.88e+07/7.90e+08 =  6% of the original kernel matrix.

torch.Size([19353, 2])
We keep 8.28e+06/3.75e+08 =  2% of the original kernel matrix.

torch.Size([3251, 2])
We keep 2.00e+05/2.79e+06 =  7% of the original kernel matrix.

torch.Size([7970, 2])
We keep 9.01e+05/2.23e+07 =  4% of the original kernel matrix.

torch.Size([2670, 2])
We keep 1.65e+05/1.75e+06 =  9% of the original kernel matrix.

torch.Size([7306, 2])
We keep 8.08e+05/1.76e+07 =  4% of the original kernel matrix.

torch.Size([15480, 2])
We keep 3.08e+07/1.77e+08 = 17% of the original kernel matrix.

torch.Size([16417, 2])
We keep 4.62e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([12513, 2])
We keep 2.50e+06/6.87e+07 =  3% of the original kernel matrix.

torch.Size([15008, 2])
We keep 3.05e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([5550, 2])
We keep 1.38e+06/1.52e+07 =  9% of the original kernel matrix.

torch.Size([9681, 2])
We keep 1.69e+06/5.21e+07 =  3% of the original kernel matrix.

torch.Size([18729, 2])
We keep 7.76e+06/1.89e+08 =  4% of the original kernel matrix.

torch.Size([18508, 2])
We keep 4.61e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([15365, 2])
We keep 6.81e+06/1.38e+08 =  4% of the original kernel matrix.

torch.Size([17261, 2])
We keep 3.99e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([14989, 2])
We keep 6.56e+06/1.52e+08 =  4% of the original kernel matrix.

torch.Size([16650, 2])
We keep 4.39e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([107137, 2])
We keep 5.19e+08/1.38e+10 =  3% of the original kernel matrix.

torch.Size([43046, 2])
We keep 3.02e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([47065, 2])
We keep 2.90e+07/1.19e+09 =  2% of the original kernel matrix.

torch.Size([28494, 2])
We keep 9.87e+06/4.60e+08 =  2% of the original kernel matrix.

torch.Size([4825, 2])
We keep 4.75e+05/6.71e+06 =  7% of the original kernel matrix.

torch.Size([9418, 2])
We keep 1.23e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([55989, 2])
We keep 4.15e+07/1.85e+09 =  2% of the original kernel matrix.

torch.Size([31111, 2])
We keep 1.19e+07/5.75e+08 =  2% of the original kernel matrix.

torch.Size([5250, 2])
We keep 6.25e+05/8.79e+06 =  7% of the original kernel matrix.

torch.Size([9764, 2])
We keep 1.40e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([6434, 2])
We keep 7.37e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([10595, 2])
We keep 1.65e+06/4.84e+07 =  3% of the original kernel matrix.

torch.Size([14679, 2])
We keep 4.56e+06/1.28e+08 =  3% of the original kernel matrix.

torch.Size([16166, 2])
We keep 4.03e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([5789, 2])
We keep 7.97e+05/1.32e+07 =  6% of the original kernel matrix.

torch.Size([10203, 2])
We keep 1.68e+06/4.84e+07 =  3% of the original kernel matrix.

torch.Size([60855, 2])
We keep 1.03e+08/3.67e+09 =  2% of the original kernel matrix.

torch.Size([31696, 2])
We keep 1.64e+07/8.09e+08 =  2% of the original kernel matrix.

torch.Size([6416, 2])
We keep 1.63e+06/2.19e+07 =  7% of the original kernel matrix.

torch.Size([10368, 2])
We keep 2.09e+06/6.24e+07 =  3% of the original kernel matrix.

torch.Size([2243, 2])
We keep 1.15e+05/1.24e+06 =  9% of the original kernel matrix.

torch.Size([6731, 2])
We keep 7.14e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([10921, 2])
We keep 2.47e+06/5.70e+07 =  4% of the original kernel matrix.

torch.Size([13775, 2])
We keep 2.87e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([5030, 2])
We keep 4.84e+06/4.01e+07 = 12% of the original kernel matrix.

torch.Size([9228, 2])
We keep 2.16e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([14861, 2])
We keep 7.99e+06/1.41e+08 =  5% of the original kernel matrix.

torch.Size([16085, 2])
We keep 4.17e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([36930, 2])
We keep 1.01e+08/1.55e+09 =  6% of the original kernel matrix.

torch.Size([23386, 2])
We keep 1.14e+07/5.25e+08 =  2% of the original kernel matrix.

torch.Size([4125, 2])
We keep 7.03e+05/6.23e+06 = 11% of the original kernel matrix.

torch.Size([8717, 2])
We keep 1.17e+06/3.33e+07 =  3% of the original kernel matrix.

torch.Size([3179, 2])
We keep 3.02e+05/2.78e+06 = 10% of the original kernel matrix.

torch.Size([7845, 2])
We keep 8.89e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([7112, 2])
We keep 1.52e+06/2.21e+07 =  6% of the original kernel matrix.

torch.Size([11183, 2])
We keep 2.02e+06/6.28e+07 =  3% of the original kernel matrix.

torch.Size([2912, 2])
We keep 2.31e+05/2.45e+06 =  9% of the original kernel matrix.

torch.Size([7486, 2])
We keep 9.16e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([7021, 2])
We keep 9.72e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([10954, 2])
We keep 1.80e+06/5.46e+07 =  3% of the original kernel matrix.

torch.Size([14637, 2])
We keep 3.26e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([16513, 2])
We keep 3.66e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([93777, 2])
We keep 1.04e+08/5.13e+09 =  2% of the original kernel matrix.

torch.Size([41059, 2])
We keep 1.87e+07/9.56e+08 =  1% of the original kernel matrix.

torch.Size([5936, 2])
We keep 1.07e+06/1.58e+07 =  6% of the original kernel matrix.

torch.Size([10430, 2])
We keep 1.87e+06/5.31e+07 =  3% of the original kernel matrix.

torch.Size([3005, 2])
We keep 1.75e+05/1.97e+06 =  8% of the original kernel matrix.

torch.Size([7677, 2])
We keep 8.39e+05/1.87e+07 =  4% of the original kernel matrix.

torch.Size([3032, 2])
We keep 2.33e+05/2.24e+06 = 10% of the original kernel matrix.

torch.Size([7593, 2])
We keep 8.82e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([47040, 2])
We keep 4.34e+07/1.27e+09 =  3% of the original kernel matrix.

torch.Size([28409, 2])
We keep 1.02e+07/4.76e+08 =  2% of the original kernel matrix.

torch.Size([329043, 2])
We keep 2.01e+09/9.18e+10 =  2% of the original kernel matrix.

torch.Size([78577, 2])
We keep 7.03e+07/4.05e+09 =  1% of the original kernel matrix.

torch.Size([16318, 2])
We keep 1.24e+07/1.76e+08 =  7% of the original kernel matrix.

torch.Size([16371, 2])
We keep 4.54e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([4195, 2])
We keep 4.27e+05/5.91e+06 =  7% of the original kernel matrix.

torch.Size([8917, 2])
We keep 1.28e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([8134, 2])
We keep 1.41e+06/2.86e+07 =  4% of the original kernel matrix.

torch.Size([12129, 2])
We keep 2.12e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([116720, 2])
We keep 1.83e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([46311, 2])
We keep 2.69e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([2768, 2])
We keep 1.42e+05/1.58e+06 =  9% of the original kernel matrix.

torch.Size([7438, 2])
We keep 7.72e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([7307, 2])
We keep 1.20e+06/2.14e+07 =  5% of the original kernel matrix.

torch.Size([11409, 2])
We keep 1.94e+06/6.18e+07 =  3% of the original kernel matrix.

torch.Size([17443, 2])
We keep 4.35e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([17857, 2])
We keep 4.32e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([4207, 2])
We keep 8.49e+05/7.97e+06 = 10% of the original kernel matrix.

torch.Size([8868, 2])
We keep 1.28e+06/3.77e+07 =  3% of the original kernel matrix.

torch.Size([26388, 2])
We keep 2.97e+07/4.88e+08 =  6% of the original kernel matrix.

torch.Size([21287, 2])
We keep 6.97e+06/2.95e+08 =  2% of the original kernel matrix.

torch.Size([46587, 2])
We keep 2.84e+07/1.17e+09 =  2% of the original kernel matrix.

torch.Size([28431, 2])
We keep 9.87e+06/4.57e+08 =  2% of the original kernel matrix.

torch.Size([10895, 2])
We keep 1.97e+06/5.34e+07 =  3% of the original kernel matrix.

torch.Size([13927, 2])
We keep 2.84e+06/9.75e+07 =  2% of the original kernel matrix.

torch.Size([6206, 2])
We keep 1.26e+06/1.19e+07 = 10% of the original kernel matrix.

torch.Size([10474, 2])
We keep 1.59e+06/4.61e+07 =  3% of the original kernel matrix.

torch.Size([4014, 2])
We keep 3.20e+05/3.80e+06 =  8% of the original kernel matrix.

torch.Size([8587, 2])
We keep 1.05e+06/2.60e+07 =  4% of the original kernel matrix.

torch.Size([22085, 2])
We keep 1.08e+07/3.18e+08 =  3% of the original kernel matrix.

torch.Size([19323, 2])
We keep 5.80e+06/2.38e+08 =  2% of the original kernel matrix.

torch.Size([27421, 2])
We keep 2.35e+07/4.73e+08 =  4% of the original kernel matrix.

torch.Size([21907, 2])
We keep 6.89e+06/2.90e+08 =  2% of the original kernel matrix.

torch.Size([38288, 2])
We keep 4.46e+07/1.29e+09 =  3% of the original kernel matrix.

torch.Size([24905, 2])
We keep 1.03e+07/4.80e+08 =  2% of the original kernel matrix.

torch.Size([24920, 2])
We keep 1.25e+07/4.09e+08 =  3% of the original kernel matrix.

torch.Size([21176, 2])
We keep 6.49e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([167649, 2])
We keep 2.98e+08/2.14e+10 =  1% of the original kernel matrix.

torch.Size([56542, 2])
We keep 3.53e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([4662, 2])
We keep 4.96e+05/6.05e+06 =  8% of the original kernel matrix.

torch.Size([9150, 2])
We keep 1.22e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([3209, 2])
We keep 2.22e+05/2.60e+06 =  8% of the original kernel matrix.

torch.Size([7827, 2])
We keep 9.29e+05/2.15e+07 =  4% of the original kernel matrix.

torch.Size([7215, 2])
We keep 9.79e+05/1.78e+07 =  5% of the original kernel matrix.

torch.Size([11326, 2])
We keep 1.86e+06/5.63e+07 =  3% of the original kernel matrix.

torch.Size([58853, 2])
We keep 8.12e+07/2.35e+09 =  3% of the original kernel matrix.

torch.Size([32063, 2])
We keep 1.35e+07/6.47e+08 =  2% of the original kernel matrix.

torch.Size([29421, 2])
We keep 1.08e+07/4.33e+08 =  2% of the original kernel matrix.

torch.Size([22744, 2])
We keep 6.47e+06/2.78e+08 =  2% of the original kernel matrix.

torch.Size([4031, 2])
We keep 3.38e+05/4.51e+06 =  7% of the original kernel matrix.

torch.Size([8695, 2])
We keep 1.12e+06/2.83e+07 =  3% of the original kernel matrix.

torch.Size([3734, 2])
We keep 3.09e+05/3.90e+06 =  7% of the original kernel matrix.

torch.Size([8394, 2])
We keep 1.09e+06/2.64e+07 =  4% of the original kernel matrix.

torch.Size([28811, 2])
We keep 1.06e+07/4.47e+08 =  2% of the original kernel matrix.

torch.Size([22590, 2])
We keep 6.59e+06/2.82e+08 =  2% of the original kernel matrix.

torch.Size([2575, 2])
We keep 1.72e+05/1.59e+06 = 10% of the original kernel matrix.

torch.Size([7263, 2])
We keep 7.70e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([5097, 2])
We keep 6.48e+05/8.13e+06 =  7% of the original kernel matrix.

torch.Size([9627, 2])
We keep 1.33e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([4282, 2])
We keep 3.75e+05/5.33e+06 =  7% of the original kernel matrix.

torch.Size([8756, 2])
We keep 1.19e+06/3.08e+07 =  3% of the original kernel matrix.

torch.Size([11629, 2])
We keep 2.80e+06/7.78e+07 =  3% of the original kernel matrix.

torch.Size([14513, 2])
We keep 3.32e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([5833, 2])
We keep 9.08e+05/1.42e+07 =  6% of the original kernel matrix.

torch.Size([10188, 2])
We keep 1.73e+06/5.04e+07 =  3% of the original kernel matrix.

torch.Size([5799, 2])
We keep 5.89e+05/9.92e+06 =  5% of the original kernel matrix.

torch.Size([10084, 2])
We keep 1.50e+06/4.20e+07 =  3% of the original kernel matrix.

torch.Size([3626, 2])
We keep 3.31e+05/3.59e+06 =  9% of the original kernel matrix.

torch.Size([8067, 2])
We keep 1.03e+06/2.53e+07 =  4% of the original kernel matrix.

torch.Size([85227, 2])
We keep 7.50e+07/4.13e+09 =  1% of the original kernel matrix.

torch.Size([39049, 2])
We keep 1.70e+07/8.58e+08 =  1% of the original kernel matrix.

torch.Size([93324, 2])
We keep 1.27e+08/5.00e+09 =  2% of the original kernel matrix.

torch.Size([40885, 2])
We keep 1.81e+07/9.44e+08 =  1% of the original kernel matrix.

torch.Size([134901, 2])
We keep 4.20e+08/1.43e+10 =  2% of the original kernel matrix.

torch.Size([49759, 2])
We keep 2.99e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([144805, 2])
We keep 6.00e+08/1.86e+10 =  3% of the original kernel matrix.

torch.Size([51763, 2])
We keep 3.44e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([29973, 2])
We keep 1.39e+07/4.94e+08 =  2% of the original kernel matrix.

torch.Size([23044, 2])
We keep 6.93e+06/2.97e+08 =  2% of the original kernel matrix.

torch.Size([6396, 2])
We keep 7.86e+05/1.27e+07 =  6% of the original kernel matrix.

torch.Size([10518, 2])
We keep 1.65e+06/4.75e+07 =  3% of the original kernel matrix.

torch.Size([5404, 2])
We keep 7.33e+05/9.01e+06 =  8% of the original kernel matrix.

torch.Size([9735, 2])
We keep 1.44e+06/4.01e+07 =  3% of the original kernel matrix.

torch.Size([89024, 2])
We keep 8.86e+07/5.03e+09 =  1% of the original kernel matrix.

torch.Size([40313, 2])
We keep 1.86e+07/9.47e+08 =  1% of the original kernel matrix.

torch.Size([5563, 2])
We keep 7.51e+05/1.11e+07 =  6% of the original kernel matrix.

torch.Size([10333, 2])
We keep 1.62e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([17347, 2])
We keep 8.76e+06/2.19e+08 =  4% of the original kernel matrix.

torch.Size([17623, 2])
We keep 5.00e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([7823, 2])
We keep 1.44e+06/2.24e+07 =  6% of the original kernel matrix.

torch.Size([11592, 2])
We keep 2.06e+06/6.32e+07 =  3% of the original kernel matrix.

torch.Size([91475, 2])
We keep 8.46e+08/1.70e+10 =  4% of the original kernel matrix.

torch.Size([39884, 2])
We keep 3.30e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([57340, 2])
We keep 6.36e+07/1.91e+09 =  3% of the original kernel matrix.

torch.Size([31265, 2])
We keep 1.25e+07/5.84e+08 =  2% of the original kernel matrix.

torch.Size([8089, 2])
We keep 1.86e+06/3.28e+07 =  5% of the original kernel matrix.

torch.Size([12223, 2])
We keep 2.41e+06/7.65e+07 =  3% of the original kernel matrix.

torch.Size([62519, 2])
We keep 6.92e+07/2.60e+09 =  2% of the original kernel matrix.

torch.Size([33340, 2])
We keep 1.38e+07/6.81e+08 =  2% of the original kernel matrix.

torch.Size([27191, 2])
We keep 1.19e+07/4.66e+08 =  2% of the original kernel matrix.

torch.Size([22177, 2])
We keep 6.78e+06/2.88e+08 =  2% of the original kernel matrix.

torch.Size([19934, 2])
We keep 1.13e+07/2.44e+08 =  4% of the original kernel matrix.

torch.Size([18881, 2])
We keep 5.16e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([78565, 2])
We keep 2.36e+08/6.08e+09 =  3% of the original kernel matrix.

torch.Size([36754, 2])
We keep 2.10e+07/1.04e+09 =  2% of the original kernel matrix.

torch.Size([60328, 2])
We keep 5.05e+07/2.49e+09 =  2% of the original kernel matrix.

torch.Size([32178, 2])
We keep 1.40e+07/6.67e+08 =  2% of the original kernel matrix.

torch.Size([47666, 2])
We keep 5.62e+07/1.36e+09 =  4% of the original kernel matrix.

torch.Size([28166, 2])
We keep 1.06e+07/4.93e+08 =  2% of the original kernel matrix.

torch.Size([10401, 2])
We keep 2.53e+06/5.41e+07 =  4% of the original kernel matrix.

torch.Size([13334, 2])
We keep 2.84e+06/9.82e+07 =  2% of the original kernel matrix.

torch.Size([85468, 2])
We keep 1.91e+08/5.71e+09 =  3% of the original kernel matrix.

torch.Size([39021, 2])
We keep 2.02e+07/1.01e+09 =  2% of the original kernel matrix.

torch.Size([9341, 2])
We keep 1.78e+06/3.50e+07 =  5% of the original kernel matrix.

torch.Size([12917, 2])
We keep 2.37e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([39940, 2])
We keep 2.18e+07/8.99e+08 =  2% of the original kernel matrix.

torch.Size([26203, 2])
We keep 8.89e+06/4.00e+08 =  2% of the original kernel matrix.

torch.Size([369929, 2])
We keep 2.01e+09/1.13e+11 =  1% of the original kernel matrix.

torch.Size([83228, 2])
We keep 7.64e+07/4.49e+09 =  1% of the original kernel matrix.

torch.Size([16110, 2])
We keep 5.09e+06/1.41e+08 =  3% of the original kernel matrix.

torch.Size([16989, 2])
We keep 4.12e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([4112, 2])
We keep 5.34e+05/5.47e+06 =  9% of the original kernel matrix.

torch.Size([8812, 2])
We keep 1.12e+06/3.12e+07 =  3% of the original kernel matrix.

torch.Size([8561, 2])
We keep 1.71e+06/3.05e+07 =  5% of the original kernel matrix.

torch.Size([12291, 2])
We keep 2.30e+06/7.37e+07 =  3% of the original kernel matrix.

torch.Size([8946, 2])
We keep 2.68e+06/3.74e+07 =  7% of the original kernel matrix.

torch.Size([12481, 2])
We keep 2.32e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([9117, 2])
We keep 2.46e+06/3.81e+07 =  6% of the original kernel matrix.

torch.Size([13153, 2])
We keep 2.57e+06/8.24e+07 =  3% of the original kernel matrix.

torch.Size([5085, 2])
We keep 7.74e+05/9.32e+06 =  8% of the original kernel matrix.

torch.Size([9443, 2])
We keep 1.48e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([11555, 2])
We keep 2.99e+06/5.84e+07 =  5% of the original kernel matrix.

torch.Size([14267, 2])
We keep 2.83e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([183651, 2])
We keep 3.50e+08/2.50e+10 =  1% of the original kernel matrix.

torch.Size([59012, 2])
We keep 3.82e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([2412, 2])
We keep 1.48e+05/1.46e+06 = 10% of the original kernel matrix.

torch.Size([6973, 2])
We keep 7.22e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([326139, 2])
We keep 3.06e+09/1.06e+11 =  2% of the original kernel matrix.

torch.Size([76283, 2])
We keep 7.48e+07/4.34e+09 =  1% of the original kernel matrix.

torch.Size([12758, 2])
We keep 3.49e+06/7.89e+07 =  4% of the original kernel matrix.

torch.Size([15206, 2])
We keep 3.19e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([23815, 2])
We keep 8.89e+06/3.23e+08 =  2% of the original kernel matrix.

torch.Size([20567, 2])
We keep 5.84e+06/2.40e+08 =  2% of the original kernel matrix.

torch.Size([6588, 2])
We keep 2.42e+06/2.47e+07 =  9% of the original kernel matrix.

torch.Size([10606, 2])
We keep 2.17e+06/6.64e+07 =  3% of the original kernel matrix.

torch.Size([41233, 2])
We keep 2.96e+07/1.16e+09 =  2% of the original kernel matrix.

torch.Size([27098, 2])
We keep 1.01e+07/4.55e+08 =  2% of the original kernel matrix.

torch.Size([138422, 2])
We keep 2.00e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([50955, 2])
We keep 2.76e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([2659, 2])
We keep 1.59e+05/1.85e+06 =  8% of the original kernel matrix.

torch.Size([7256, 2])
We keep 8.09e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([483228, 2])
We keep 1.41e+09/1.27e+11 =  1% of the original kernel matrix.

torch.Size([99856, 2])
We keep 7.89e+07/4.76e+09 =  1% of the original kernel matrix.

torch.Size([28151, 2])
We keep 1.83e+07/5.46e+08 =  3% of the original kernel matrix.

torch.Size([22331, 2])
We keep 7.08e+06/3.12e+08 =  2% of the original kernel matrix.

torch.Size([12023, 2])
We keep 8.09e+06/9.65e+07 =  8% of the original kernel matrix.

torch.Size([14582, 2])
We keep 3.73e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([5564, 2])
We keep 6.96e+05/1.09e+07 =  6% of the original kernel matrix.

torch.Size([10070, 2])
We keep 1.47e+06/4.40e+07 =  3% of the original kernel matrix.

torch.Size([7849, 2])
We keep 1.21e+06/2.27e+07 =  5% of the original kernel matrix.

torch.Size([11573, 2])
We keep 2.04e+06/6.36e+07 =  3% of the original kernel matrix.

torch.Size([4011, 2])
We keep 4.00e+05/4.49e+06 =  8% of the original kernel matrix.

torch.Size([8551, 2])
We keep 1.12e+06/2.83e+07 =  3% of the original kernel matrix.

torch.Size([2008, 2])
We keep 8.65e+04/8.48e+05 = 10% of the original kernel matrix.

torch.Size([6521, 2])
We keep 6.23e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([7377, 2])
We keep 8.69e+05/1.79e+07 =  4% of the original kernel matrix.

torch.Size([11443, 2])
We keep 1.85e+06/5.65e+07 =  3% of the original kernel matrix.

torch.Size([36258, 2])
We keep 3.56e+07/1.02e+09 =  3% of the original kernel matrix.

torch.Size([24644, 2])
We keep 9.45e+06/4.26e+08 =  2% of the original kernel matrix.

torch.Size([305012, 2])
We keep 1.25e+10/3.67e+11 =  3% of the original kernel matrix.

torch.Size([60489, 2])
We keep 1.33e+08/8.09e+09 =  1% of the original kernel matrix.

torch.Size([35548, 2])
We keep 3.04e+08/1.83e+09 = 16% of the original kernel matrix.

torch.Size([23496, 2])
We keep 1.24e+07/5.71e+08 =  2% of the original kernel matrix.

torch.Size([19195, 2])
We keep 6.58e+06/2.12e+08 =  3% of the original kernel matrix.

torch.Size([18705, 2])
We keep 4.74e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([5314, 2])
We keep 1.40e+06/1.23e+07 = 11% of the original kernel matrix.

torch.Size([9688, 2])
We keep 1.70e+06/4.69e+07 =  3% of the original kernel matrix.

torch.Size([30304, 2])
We keep 1.34e+07/5.17e+08 =  2% of the original kernel matrix.

torch.Size([23133, 2])
We keep 7.10e+06/3.04e+08 =  2% of the original kernel matrix.

torch.Size([3006, 2])
We keep 1.79e+05/2.18e+06 =  8% of the original kernel matrix.

torch.Size([7587, 2])
We keep 8.25e+05/1.97e+07 =  4% of the original kernel matrix.

torch.Size([33397, 2])
We keep 3.72e+07/8.94e+08 =  4% of the original kernel matrix.

torch.Size([23604, 2])
We keep 8.73e+06/3.99e+08 =  2% of the original kernel matrix.

torch.Size([7680, 2])
We keep 1.40e+06/2.33e+07 =  6% of the original kernel matrix.

torch.Size([11576, 2])
We keep 2.03e+06/6.45e+07 =  3% of the original kernel matrix.

torch.Size([570690, 2])
We keep 2.83e+09/2.31e+11 =  1% of the original kernel matrix.

torch.Size([106986, 2])
We keep 1.05e+08/6.42e+09 =  1% of the original kernel matrix.

torch.Size([4364, 2])
We keep 5.07e+05/7.47e+06 =  6% of the original kernel matrix.

torch.Size([8963, 2])
We keep 1.37e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([40615, 2])
We keep 2.06e+07/8.79e+08 =  2% of the original kernel matrix.

torch.Size([26733, 2])
We keep 8.67e+06/3.96e+08 =  2% of the original kernel matrix.

torch.Size([9060, 2])
We keep 1.78e+06/3.82e+07 =  4% of the original kernel matrix.

torch.Size([12789, 2])
We keep 2.41e+06/8.25e+07 =  2% of the original kernel matrix.

torch.Size([31818, 2])
We keep 2.43e+07/6.70e+08 =  3% of the original kernel matrix.

torch.Size([23685, 2])
We keep 7.83e+06/3.45e+08 =  2% of the original kernel matrix.

torch.Size([282585, 2])
We keep 7.19e+08/5.87e+10 =  1% of the original kernel matrix.

torch.Size([72772, 2])
We keep 5.59e+07/3.24e+09 =  1% of the original kernel matrix.

torch.Size([20621, 2])
We keep 6.04e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([19334, 2])
We keep 5.04e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([5027, 2])
We keep 5.08e+05/7.14e+06 =  7% of the original kernel matrix.

torch.Size([9419, 2])
We keep 1.33e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([35973, 2])
We keep 2.07e+07/9.17e+08 =  2% of the original kernel matrix.

torch.Size([25451, 2])
We keep 9.12e+06/4.04e+08 =  2% of the original kernel matrix.

torch.Size([6284, 2])
We keep 9.84e+05/1.61e+07 =  6% of the original kernel matrix.

torch.Size([10504, 2])
We keep 1.81e+06/5.35e+07 =  3% of the original kernel matrix.

torch.Size([3669, 2])
We keep 4.34e+05/4.19e+06 = 10% of the original kernel matrix.

torch.Size([8208, 2])
We keep 1.04e+06/2.73e+07 =  3% of the original kernel matrix.

torch.Size([68075, 2])
We keep 5.61e+07/2.53e+09 =  2% of the original kernel matrix.

torch.Size([34198, 2])
We keep 1.38e+07/6.71e+08 =  2% of the original kernel matrix.

torch.Size([274519, 2])
We keep 3.57e+09/1.01e+11 =  3% of the original kernel matrix.

torch.Size([70367, 2])
We keep 7.24e+07/4.24e+09 =  1% of the original kernel matrix.

torch.Size([12029, 2])
We keep 4.23e+06/9.58e+07 =  4% of the original kernel matrix.

torch.Size([14940, 2])
We keep 3.68e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([33320, 2])
We keep 1.53e+07/6.60e+08 =  2% of the original kernel matrix.

torch.Size([23872, 2])
We keep 7.78e+06/3.43e+08 =  2% of the original kernel matrix.

torch.Size([53415, 2])
We keep 8.48e+07/2.31e+09 =  3% of the original kernel matrix.

torch.Size([29812, 2])
We keep 1.34e+07/6.42e+08 =  2% of the original kernel matrix.

torch.Size([10920, 2])
We keep 6.50e+06/8.78e+07 =  7% of the original kernel matrix.

torch.Size([13740, 2])
We keep 3.29e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([25210, 2])
We keep 9.43e+06/3.40e+08 =  2% of the original kernel matrix.

torch.Size([21419, 2])
We keep 5.91e+06/2.46e+08 =  2% of the original kernel matrix.

torch.Size([4528, 2])
We keep 3.92e+05/6.00e+06 =  6% of the original kernel matrix.

torch.Size([9175, 2])
We keep 1.24e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([107030, 2])
We keep 3.38e+08/9.87e+09 =  3% of the original kernel matrix.

torch.Size([42722, 2])
We keep 2.49e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([39104, 2])
We keep 3.94e+07/1.20e+09 =  3% of the original kernel matrix.

torch.Size([25035, 2])
We keep 1.02e+07/4.62e+08 =  2% of the original kernel matrix.

torch.Size([66211, 2])
We keep 1.05e+08/2.98e+09 =  3% of the original kernel matrix.

torch.Size([33287, 2])
We keep 1.51e+07/7.29e+08 =  2% of the original kernel matrix.

torch.Size([9515, 2])
We keep 2.27e+06/4.34e+07 =  5% of the original kernel matrix.

torch.Size([12758, 2])
We keep 2.58e+06/8.79e+07 =  2% of the original kernel matrix.

torch.Size([7149, 2])
We keep 1.69e+06/2.42e+07 =  7% of the original kernel matrix.

torch.Size([11093, 2])
We keep 2.00e+06/6.56e+07 =  3% of the original kernel matrix.

torch.Size([11570, 2])
We keep 3.76e+06/7.09e+07 =  5% of the original kernel matrix.

torch.Size([14117, 2])
We keep 3.03e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([270017, 2])
We keep 1.62e+09/6.68e+10 =  2% of the original kernel matrix.

torch.Size([66614, 2])
We keep 6.00e+07/3.45e+09 =  1% of the original kernel matrix.

torch.Size([48227, 2])
We keep 6.92e+07/1.51e+09 =  4% of the original kernel matrix.

torch.Size([28847, 2])
We keep 1.12e+07/5.18e+08 =  2% of the original kernel matrix.

torch.Size([14533, 2])
We keep 5.09e+06/1.15e+08 =  4% of the original kernel matrix.

torch.Size([15252, 2])
We keep 3.60e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([2745, 2])
We keep 1.63e+05/1.91e+06 =  8% of the original kernel matrix.

torch.Size([7403, 2])
We keep 8.33e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([10716, 2])
We keep 4.98e+06/6.16e+07 =  8% of the original kernel matrix.

torch.Size([14145, 2])
We keep 3.09e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([32089, 2])
We keep 4.59e+07/8.85e+08 =  5% of the original kernel matrix.

torch.Size([22608, 2])
We keep 9.10e+06/3.97e+08 =  2% of the original kernel matrix.

torch.Size([48925, 2])
We keep 4.51e+07/1.64e+09 =  2% of the original kernel matrix.

torch.Size([28135, 2])
We keep 1.15e+07/5.40e+08 =  2% of the original kernel matrix.

torch.Size([191979, 2])
We keep 6.36e+08/2.20e+10 =  2% of the original kernel matrix.

torch.Size([60153, 2])
We keep 3.65e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([6198, 2])
We keep 6.75e+05/1.15e+07 =  5% of the original kernel matrix.

torch.Size([10325, 2])
We keep 1.56e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([19645, 2])
We keep 6.76e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([18596, 2])
We keep 5.03e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([21584, 2])
We keep 1.14e+07/3.68e+08 =  3% of the original kernel matrix.

torch.Size([19629, 2])
We keep 6.16e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([32257, 2])
We keep 1.47e+07/6.56e+08 =  2% of the original kernel matrix.

torch.Size([24077, 2])
We keep 7.75e+06/3.42e+08 =  2% of the original kernel matrix.

torch.Size([37468, 2])
We keep 2.62e+07/8.71e+08 =  3% of the original kernel matrix.

torch.Size([25237, 2])
We keep 8.86e+06/3.94e+08 =  2% of the original kernel matrix.

torch.Size([72768, 2])
We keep 5.69e+07/2.84e+09 =  2% of the original kernel matrix.

torch.Size([35852, 2])
We keep 1.44e+07/7.12e+08 =  2% of the original kernel matrix.

torch.Size([7702, 2])
We keep 6.65e+06/3.01e+07 = 22% of the original kernel matrix.

torch.Size([11643, 2])
We keep 2.15e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([28330, 2])
We keep 1.16e+07/4.53e+08 =  2% of the original kernel matrix.

torch.Size([22724, 2])
We keep 6.45e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([1965, 2])
We keep 8.46e+04/8.37e+05 = 10% of the original kernel matrix.

torch.Size([6739, 2])
We keep 6.04e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([4234, 2])
We keep 6.70e+05/8.43e+06 =  7% of the original kernel matrix.

torch.Size([8713, 2])
We keep 1.49e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([99014, 2])
We keep 9.40e+07/5.84e+09 =  1% of the original kernel matrix.

torch.Size([42341, 2])
We keep 1.96e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([34785, 2])
We keep 3.06e+07/9.16e+08 =  3% of the original kernel matrix.

torch.Size([24016, 2])
We keep 9.15e+06/4.04e+08 =  2% of the original kernel matrix.

torch.Size([47800, 2])
We keep 2.48e+07/1.16e+09 =  2% of the original kernel matrix.

torch.Size([28711, 2])
We keep 9.77e+06/4.54e+08 =  2% of the original kernel matrix.

torch.Size([2721, 2])
We keep 1.39e+05/1.75e+06 =  7% of the original kernel matrix.

torch.Size([7364, 2])
We keep 7.84e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([4062, 2])
We keep 1.21e+06/5.95e+06 = 20% of the original kernel matrix.

torch.Size([8727, 2])
We keep 1.29e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([54824, 2])
We keep 7.43e+07/2.07e+09 =  3% of the original kernel matrix.

torch.Size([30416, 2])
We keep 1.26e+07/6.07e+08 =  2% of the original kernel matrix.

torch.Size([37601, 2])
We keep 2.45e+07/8.33e+08 =  2% of the original kernel matrix.

torch.Size([25049, 2])
We keep 8.64e+06/3.85e+08 =  2% of the original kernel matrix.

torch.Size([21375, 2])
We keep 7.78e+06/2.56e+08 =  3% of the original kernel matrix.

torch.Size([19815, 2])
We keep 5.31e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([2597, 2])
We keep 1.47e+05/1.64e+06 =  8% of the original kernel matrix.

torch.Size([7221, 2])
We keep 7.88e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([3825, 2])
We keep 3.11e+05/4.17e+06 =  7% of the original kernel matrix.

torch.Size([8443, 2])
We keep 1.08e+06/2.73e+07 =  3% of the original kernel matrix.

torch.Size([15038, 2])
We keep 5.78e+06/1.21e+08 =  4% of the original kernel matrix.

torch.Size([16343, 2])
We keep 3.87e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([13612, 2])
We keep 3.91e+06/9.49e+07 =  4% of the original kernel matrix.

torch.Size([15596, 2])
We keep 3.55e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([45070, 2])
We keep 1.06e+08/1.47e+09 =  7% of the original kernel matrix.

torch.Size([27867, 2])
We keep 1.08e+07/5.12e+08 =  2% of the original kernel matrix.

torch.Size([8831, 2])
We keep 2.49e+06/3.93e+07 =  6% of the original kernel matrix.

torch.Size([12291, 2])
We keep 2.52e+06/8.37e+07 =  3% of the original kernel matrix.

torch.Size([4562, 2])
We keep 1.10e+06/9.49e+06 = 11% of the original kernel matrix.

torch.Size([8980, 2])
We keep 1.36e+06/4.11e+07 =  3% of the original kernel matrix.

torch.Size([3260, 2])
We keep 3.62e+05/3.64e+06 =  9% of the original kernel matrix.

torch.Size([7625, 2])
We keep 9.98e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([4050, 2])
We keep 3.74e+05/5.00e+06 =  7% of the original kernel matrix.

torch.Size([8566, 2])
We keep 1.14e+06/2.99e+07 =  3% of the original kernel matrix.

torch.Size([90061, 2])
We keep 1.40e+08/5.86e+09 =  2% of the original kernel matrix.

torch.Size([40424, 2])
We keep 1.84e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([5295, 2])
We keep 6.18e+05/8.84e+06 =  6% of the original kernel matrix.

torch.Size([9720, 2])
We keep 1.32e+06/3.97e+07 =  3% of the original kernel matrix.

torch.Size([26806, 2])
We keep 1.47e+07/4.38e+08 =  3% of the original kernel matrix.

torch.Size([21669, 2])
We keep 6.69e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([89547, 2])
We keep 1.17e+08/4.86e+09 =  2% of the original kernel matrix.

torch.Size([40171, 2])
We keep 1.85e+07/9.30e+08 =  1% of the original kernel matrix.

torch.Size([19206, 2])
We keep 7.01e+06/2.03e+08 =  3% of the original kernel matrix.

torch.Size([18824, 2])
We keep 4.69e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([56969, 2])
We keep 8.37e+07/2.29e+09 =  3% of the original kernel matrix.

torch.Size([31278, 2])
We keep 1.36e+07/6.39e+08 =  2% of the original kernel matrix.

torch.Size([112650, 2])
We keep 1.39e+08/8.30e+09 =  1% of the original kernel matrix.

torch.Size([45649, 2])
We keep 2.33e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([5691, 2])
We keep 6.03e+05/9.08e+06 =  6% of the original kernel matrix.

torch.Size([10083, 2])
We keep 1.45e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([71273, 2])
We keep 9.87e+07/2.97e+09 =  3% of the original kernel matrix.

torch.Size([35244, 2])
We keep 1.51e+07/7.27e+08 =  2% of the original kernel matrix.

torch.Size([21368, 2])
We keep 7.39e+06/2.69e+08 =  2% of the original kernel matrix.

torch.Size([19889, 2])
We keep 5.38e+06/2.19e+08 =  2% of the original kernel matrix.

torch.Size([238541, 2])
We keep 9.30e+08/3.15e+10 =  2% of the original kernel matrix.

torch.Size([66042, 2])
We keep 4.23e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([151365, 2])
We keep 2.04e+08/1.44e+10 =  1% of the original kernel matrix.

torch.Size([53000, 2])
We keep 2.95e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([8889, 2])
We keep 3.80e+06/4.81e+07 =  7% of the original kernel matrix.

torch.Size([11905, 2])
We keep 2.59e+06/9.26e+07 =  2% of the original kernel matrix.

torch.Size([43817, 2])
We keep 2.95e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([27578, 2])
We keep 9.32e+06/4.27e+08 =  2% of the original kernel matrix.

torch.Size([7738, 2])
We keep 9.59e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([11614, 2])
We keep 1.96e+06/6.04e+07 =  3% of the original kernel matrix.

torch.Size([3901, 2])
We keep 4.71e+05/4.76e+06 =  9% of the original kernel matrix.

torch.Size([8420, 2])
We keep 1.06e+06/2.91e+07 =  3% of the original kernel matrix.

torch.Size([5642, 2])
We keep 6.32e+05/9.95e+06 =  6% of the original kernel matrix.

torch.Size([10044, 2])
We keep 1.49e+06/4.21e+07 =  3% of the original kernel matrix.

torch.Size([9681, 2])
We keep 2.52e+06/4.60e+07 =  5% of the original kernel matrix.

torch.Size([13028, 2])
We keep 2.74e+06/9.06e+07 =  3% of the original kernel matrix.

torch.Size([80495, 2])
We keep 9.31e+07/4.32e+09 =  2% of the original kernel matrix.

torch.Size([37916, 2])
We keep 1.75e+07/8.77e+08 =  1% of the original kernel matrix.

torch.Size([36176, 2])
We keep 3.54e+07/9.11e+08 =  3% of the original kernel matrix.

torch.Size([24966, 2])
We keep 9.11e+06/4.03e+08 =  2% of the original kernel matrix.

torch.Size([3360, 2])
We keep 3.55e+05/3.49e+06 = 10% of the original kernel matrix.

torch.Size([7875, 2])
We keep 9.54e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([19250, 2])
We keep 7.06e+06/2.22e+08 =  3% of the original kernel matrix.

torch.Size([18757, 2])
We keep 4.92e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([10006, 2])
We keep 1.81e+06/4.30e+07 =  4% of the original kernel matrix.

torch.Size([13373, 2])
We keep 2.57e+06/8.75e+07 =  2% of the original kernel matrix.

torch.Size([178829, 2])
We keep 4.94e+08/2.63e+10 =  1% of the original kernel matrix.

torch.Size([56481, 2])
We keep 3.89e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([14080, 2])
We keep 3.05e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([16191, 2])
We keep 3.66e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([9487, 2])
We keep 2.33e+06/4.34e+07 =  5% of the original kernel matrix.

torch.Size([12862, 2])
We keep 2.60e+06/8.80e+07 =  2% of the original kernel matrix.

torch.Size([47323, 2])
We keep 5.09e+07/1.43e+09 =  3% of the original kernel matrix.

torch.Size([28560, 2])
We keep 1.08e+07/5.04e+08 =  2% of the original kernel matrix.

torch.Size([77937, 2])
We keep 7.57e+07/3.48e+09 =  2% of the original kernel matrix.

torch.Size([37196, 2])
We keep 1.59e+07/7.87e+08 =  2% of the original kernel matrix.

torch.Size([4973, 2])
We keep 7.78e+05/9.49e+06 =  8% of the original kernel matrix.

torch.Size([9212, 2])
We keep 1.46e+06/4.11e+07 =  3% of the original kernel matrix.

torch.Size([56455, 2])
We keep 7.85e+07/2.03e+09 =  3% of the original kernel matrix.

torch.Size([30962, 2])
We keep 1.27e+07/6.01e+08 =  2% of the original kernel matrix.

torch.Size([8842, 2])
We keep 2.93e+06/3.61e+07 =  8% of the original kernel matrix.

torch.Size([12560, 2])
We keep 2.45e+06/8.02e+07 =  3% of the original kernel matrix.

time for making ranges is 2.043816328048706
Sorting X and nu_X
time for sorting X is 0.06728506088256836
Sorting Z and nu_Z
time for sorting Z is 0.0002567768096923828
Starting Optim
sum tnu_Z before tensor(24200756., device='cuda:0')
c= tensor(939.2990, device='cuda:0')
c= tensor(51295.8945, device='cuda:0')
c= tensor(53097.8828, device='cuda:0')
c= tensor(55348.7500, device='cuda:0')
c= tensor(408936., device='cuda:0')
c= tensor(464027.2500, device='cuda:0')
c= tensor(857712.2500, device='cuda:0')
c= tensor(1306182.2500, device='cuda:0')
c= tensor(1328873.1250, device='cuda:0')
c= tensor(4941614.5000, device='cuda:0')
c= tensor(4953223.5000, device='cuda:0')
c= tensor(12672544., device='cuda:0')
c= tensor(12692685., device='cuda:0')
c= tensor(14590588., device='cuda:0')
c= tensor(14703862., device='cuda:0')
c= tensor(15028333., device='cuda:0')
c= tensor(15278127., device='cuda:0')
c= tensor(15705138., device='cuda:0')
c= tensor(1.2961e+08, device='cuda:0')
c= tensor(1.3090e+08, device='cuda:0')
c= tensor(1.3092e+08, device='cuda:0')
c= tensor(1.7327e+08, device='cuda:0')
c= tensor(1.7338e+08, device='cuda:0')
c= tensor(1.7340e+08, device='cuda:0')
c= tensor(1.7388e+08, device='cuda:0')
c= tensor(1.7453e+08, device='cuda:0')
c= tensor(1.7455e+08, device='cuda:0')
c= tensor(1.7457e+08, device='cuda:0')
c= tensor(1.7502e+08, device='cuda:0')
c= tensor(6.7368e+08, device='cuda:0')
c= tensor(6.7368e+08, device='cuda:0')
c= tensor(6.7982e+08, device='cuda:0')
c= tensor(6.7984e+08, device='cuda:0')
c= tensor(6.7985e+08, device='cuda:0')
c= tensor(6.7985e+08, device='cuda:0')
c= tensor(6.8001e+08, device='cuda:0')
c= tensor(6.8043e+08, device='cuda:0')
c= tensor(6.8044e+08, device='cuda:0')
c= tensor(6.8044e+08, device='cuda:0')
c= tensor(6.8044e+08, device='cuda:0')
c= tensor(6.8045e+08, device='cuda:0')
c= tensor(6.8045e+08, device='cuda:0')
c= tensor(6.8045e+08, device='cuda:0')
c= tensor(6.8046e+08, device='cuda:0')
c= tensor(6.8046e+08, device='cuda:0')
c= tensor(6.8046e+08, device='cuda:0')
c= tensor(6.8047e+08, device='cuda:0')
c= tensor(6.8047e+08, device='cuda:0')
c= tensor(6.8047e+08, device='cuda:0')
c= tensor(6.8048e+08, device='cuda:0')
c= tensor(6.8049e+08, device='cuda:0')
c= tensor(6.8049e+08, device='cuda:0')
c= tensor(6.8049e+08, device='cuda:0')
c= tensor(6.8049e+08, device='cuda:0')
c= tensor(6.8050e+08, device='cuda:0')
c= tensor(6.8051e+08, device='cuda:0')
c= tensor(6.8051e+08, device='cuda:0')
c= tensor(6.8052e+08, device='cuda:0')
c= tensor(6.8052e+08, device='cuda:0')
c= tensor(6.8052e+08, device='cuda:0')
c= tensor(6.8053e+08, device='cuda:0')
c= tensor(6.8053e+08, device='cuda:0')
c= tensor(6.8053e+08, device='cuda:0')
c= tensor(6.8054e+08, device='cuda:0')
c= tensor(6.8055e+08, device='cuda:0')
c= tensor(6.8055e+08, device='cuda:0')
c= tensor(6.8055e+08, device='cuda:0')
c= tensor(6.8056e+08, device='cuda:0')
c= tensor(6.8057e+08, device='cuda:0')
c= tensor(6.8057e+08, device='cuda:0')
c= tensor(6.8057e+08, device='cuda:0')
c= tensor(6.8058e+08, device='cuda:0')
c= tensor(6.8059e+08, device='cuda:0')
c= tensor(6.8059e+08, device='cuda:0')
c= tensor(6.8059e+08, device='cuda:0')
c= tensor(6.8059e+08, device='cuda:0')
c= tensor(6.8060e+08, device='cuda:0')
c= tensor(6.8060e+08, device='cuda:0')
c= tensor(6.8060e+08, device='cuda:0')
c= tensor(6.8063e+08, device='cuda:0')
c= tensor(6.8064e+08, device='cuda:0')
c= tensor(6.8064e+08, device='cuda:0')
c= tensor(6.8064e+08, device='cuda:0')
c= tensor(6.8064e+08, device='cuda:0')
c= tensor(6.8065e+08, device='cuda:0')
c= tensor(6.8065e+08, device='cuda:0')
c= tensor(6.8066e+08, device='cuda:0')
c= tensor(6.8066e+08, device='cuda:0')
c= tensor(6.8066e+08, device='cuda:0')
c= tensor(6.8066e+08, device='cuda:0')
c= tensor(6.8067e+08, device='cuda:0')
c= tensor(6.8067e+08, device='cuda:0')
c= tensor(6.8067e+08, device='cuda:0')
c= tensor(6.8067e+08, device='cuda:0')
c= tensor(6.8067e+08, device='cuda:0')
c= tensor(6.8068e+08, device='cuda:0')
c= tensor(6.8068e+08, device='cuda:0')
c= tensor(6.8079e+08, device='cuda:0')
c= tensor(6.8079e+08, device='cuda:0')
c= tensor(6.8080e+08, device='cuda:0')
c= tensor(6.8081e+08, device='cuda:0')
c= tensor(6.8081e+08, device='cuda:0')
c= tensor(6.8082e+08, device='cuda:0')
c= tensor(6.8082e+08, device='cuda:0')
c= tensor(6.8082e+08, device='cuda:0')
c= tensor(6.8082e+08, device='cuda:0')
c= tensor(6.8083e+08, device='cuda:0')
c= tensor(6.8083e+08, device='cuda:0')
c= tensor(6.8083e+08, device='cuda:0')
c= tensor(6.8083e+08, device='cuda:0')
c= tensor(6.8083e+08, device='cuda:0')
c= tensor(6.8084e+08, device='cuda:0')
c= tensor(6.8084e+08, device='cuda:0')
c= tensor(6.8084e+08, device='cuda:0')
c= tensor(6.8084e+08, device='cuda:0')
c= tensor(6.8085e+08, device='cuda:0')
c= tensor(6.8085e+08, device='cuda:0')
c= tensor(6.8085e+08, device='cuda:0')
c= tensor(6.8086e+08, device='cuda:0')
c= tensor(6.8086e+08, device='cuda:0')
c= tensor(6.8088e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8090e+08, device='cuda:0')
c= tensor(6.8094e+08, device='cuda:0')
c= tensor(6.8094e+08, device='cuda:0')
c= tensor(6.8095e+08, device='cuda:0')
c= tensor(6.8095e+08, device='cuda:0')
c= tensor(6.8095e+08, device='cuda:0')
c= tensor(6.8095e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8097e+08, device='cuda:0')
c= tensor(6.8097e+08, device='cuda:0')
c= tensor(6.8097e+08, device='cuda:0')
c= tensor(6.8099e+08, device='cuda:0')
c= tensor(6.8100e+08, device='cuda:0')
c= tensor(6.8100e+08, device='cuda:0')
c= tensor(6.8100e+08, device='cuda:0')
c= tensor(6.8100e+08, device='cuda:0')
c= tensor(6.8100e+08, device='cuda:0')
c= tensor(6.8101e+08, device='cuda:0')
c= tensor(6.8101e+08, device='cuda:0')
c= tensor(6.8101e+08, device='cuda:0')
c= tensor(6.8101e+08, device='cuda:0')
c= tensor(6.8102e+08, device='cuda:0')
c= tensor(6.8102e+08, device='cuda:0')
c= tensor(6.8105e+08, device='cuda:0')
c= tensor(6.8105e+08, device='cuda:0')
c= tensor(6.8105e+08, device='cuda:0')
c= tensor(6.8106e+08, device='cuda:0')
c= tensor(6.8106e+08, device='cuda:0')
c= tensor(6.8107e+08, device='cuda:0')
c= tensor(6.8107e+08, device='cuda:0')
c= tensor(6.8107e+08, device='cuda:0')
c= tensor(6.8107e+08, device='cuda:0')
c= tensor(6.8107e+08, device='cuda:0')
c= tensor(6.8108e+08, device='cuda:0')
c= tensor(6.8108e+08, device='cuda:0')
c= tensor(6.8108e+08, device='cuda:0')
c= tensor(6.8109e+08, device='cuda:0')
c= tensor(6.8109e+08, device='cuda:0')
c= tensor(6.8109e+08, device='cuda:0')
c= tensor(6.8109e+08, device='cuda:0')
c= tensor(6.8111e+08, device='cuda:0')
c= tensor(6.8111e+08, device='cuda:0')
c= tensor(6.8112e+08, device='cuda:0')
c= tensor(6.8112e+08, device='cuda:0')
c= tensor(6.8113e+08, device='cuda:0')
c= tensor(6.8113e+08, device='cuda:0')
c= tensor(6.8114e+08, device='cuda:0')
c= tensor(6.8114e+08, device='cuda:0')
c= tensor(6.8115e+08, device='cuda:0')
c= tensor(6.8115e+08, device='cuda:0')
c= tensor(6.8115e+08, device='cuda:0')
c= tensor(6.8115e+08, device='cuda:0')
c= tensor(6.8116e+08, device='cuda:0')
c= tensor(6.8116e+08, device='cuda:0')
c= tensor(6.8117e+08, device='cuda:0')
c= tensor(6.8117e+08, device='cuda:0')
c= tensor(6.8117e+08, device='cuda:0')
c= tensor(6.8118e+08, device='cuda:0')
c= tensor(6.8118e+08, device='cuda:0')
c= tensor(6.8118e+08, device='cuda:0')
c= tensor(6.8118e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8120e+08, device='cuda:0')
c= tensor(6.8120e+08, device='cuda:0')
c= tensor(6.8120e+08, device='cuda:0')
c= tensor(6.8121e+08, device='cuda:0')
c= tensor(6.8121e+08, device='cuda:0')
c= tensor(6.8122e+08, device='cuda:0')
c= tensor(6.8122e+08, device='cuda:0')
c= tensor(6.8122e+08, device='cuda:0')
c= tensor(6.8122e+08, device='cuda:0')
c= tensor(6.8122e+08, device='cuda:0')
c= tensor(6.8123e+08, device='cuda:0')
c= tensor(6.8123e+08, device='cuda:0')
c= tensor(6.8123e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8129e+08, device='cuda:0')
c= tensor(6.8129e+08, device='cuda:0')
c= tensor(6.8129e+08, device='cuda:0')
c= tensor(6.8130e+08, device='cuda:0')
c= tensor(6.8130e+08, device='cuda:0')
c= tensor(6.8131e+08, device='cuda:0')
c= tensor(6.8131e+08, device='cuda:0')
c= tensor(6.8131e+08, device='cuda:0')
c= tensor(6.8131e+08, device='cuda:0')
c= tensor(6.8132e+08, device='cuda:0')
c= tensor(6.8132e+08, device='cuda:0')
c= tensor(6.8132e+08, device='cuda:0')
c= tensor(6.8132e+08, device='cuda:0')
c= tensor(6.8132e+08, device='cuda:0')
c= tensor(6.8133e+08, device='cuda:0')
c= tensor(6.8133e+08, device='cuda:0')
c= tensor(6.8133e+08, device='cuda:0')
c= tensor(6.8133e+08, device='cuda:0')
c= tensor(6.8134e+08, device='cuda:0')
c= tensor(6.8135e+08, device='cuda:0')
c= tensor(6.8135e+08, device='cuda:0')
c= tensor(6.8136e+08, device='cuda:0')
c= tensor(6.8145e+08, device='cuda:0')
c= tensor(6.8156e+08, device='cuda:0')
c= tensor(6.8157e+08, device='cuda:0')
c= tensor(6.8157e+08, device='cuda:0')
c= tensor(6.8158e+08, device='cuda:0')
c= tensor(6.8191e+08, device='cuda:0')
c= tensor(6.8303e+08, device='cuda:0')
c= tensor(6.8304e+08, device='cuda:0')
c= tensor(6.8316e+08, device='cuda:0')
c= tensor(6.8371e+08, device='cuda:0')
c= tensor(6.8375e+08, device='cuda:0')
c= tensor(7.5556e+08, device='cuda:0')
c= tensor(7.5556e+08, device='cuda:0')
c= tensor(7.5557e+08, device='cuda:0')
c= tensor(7.6081e+08, device='cuda:0')
c= tensor(8.1683e+08, device='cuda:0')
c= tensor(8.1684e+08, device='cuda:0')
c= tensor(8.1693e+08, device='cuda:0')
c= tensor(8.1710e+08, device='cuda:0')
c= tensor(8.1871e+08, device='cuda:0')
c= tensor(8.1922e+08, device='cuda:0')
c= tensor(8.2083e+08, device='cuda:0')
c= tensor(8.2138e+08, device='cuda:0')
c= tensor(8.2145e+08, device='cuda:0')
c= tensor(8.2158e+08, device='cuda:0')
c= tensor(8.3108e+08, device='cuda:0')
c= tensor(8.3110e+08, device='cuda:0')
c= tensor(8.3110e+08, device='cuda:0')
c= tensor(8.3197e+08, device='cuda:0')
c= tensor(8.3200e+08, device='cuda:0')
c= tensor(8.3642e+08, device='cuda:0')
c= tensor(8.3869e+08, device='cuda:0')
c= tensor(8.3869e+08, device='cuda:0')
c= tensor(8.3870e+08, device='cuda:0')
c= tensor(8.3871e+08, device='cuda:0')
c= tensor(8.3875e+08, device='cuda:0')
c= tensor(8.4271e+08, device='cuda:0')
c= tensor(8.4306e+08, device='cuda:0')
c= tensor(8.4347e+08, device='cuda:0')
c= tensor(8.4347e+08, device='cuda:0')
c= tensor(8.4347e+08, device='cuda:0')
c= tensor(8.4348e+08, device='cuda:0')
c= tensor(8.5302e+08, device='cuda:0')
c= tensor(8.5319e+08, device='cuda:0')
c= tensor(8.5320e+08, device='cuda:0')
c= tensor(8.7397e+08, device='cuda:0')
c= tensor(8.7408e+08, device='cuda:0')
c= tensor(8.7421e+08, device='cuda:0')
c= tensor(8.7605e+08, device='cuda:0')
c= tensor(8.7606e+08, device='cuda:0')
c= tensor(8.7634e+08, device='cuda:0')
c= tensor(8.7665e+08, device='cuda:0')
c= tensor(8.7955e+08, device='cuda:0')
c= tensor(8.7957e+08, device='cuda:0')
c= tensor(8.7958e+08, device='cuda:0')
c= tensor(8.7967e+08, device='cuda:0')
c= tensor(8.7968e+08, device='cuda:0')
c= tensor(8.7977e+08, device='cuda:0')
c= tensor(8.7983e+08, device='cuda:0')
c= tensor(8.8013e+08, device='cuda:0')
c= tensor(8.9690e+08, device='cuda:0')
c= tensor(8.9708e+08, device='cuda:0')
c= tensor(8.9710e+08, device='cuda:0')
c= tensor(8.9711e+08, device='cuda:0')
c= tensor(9.0058e+08, device='cuda:0')
c= tensor(9.0087e+08, device='cuda:0')
c= tensor(9.0088e+08, device='cuda:0')
c= tensor(9.0089e+08, device='cuda:0')
c= tensor(9.0411e+08, device='cuda:0')
c= tensor(9.0411e+08, device='cuda:0')
c= tensor(1.0272e+09, device='cuda:0')
c= tensor(1.0272e+09, device='cuda:0')
c= tensor(1.0273e+09, device='cuda:0')
c= tensor(1.0278e+09, device='cuda:0')
c= tensor(1.0279e+09, device='cuda:0')
c= tensor(1.0287e+09, device='cuda:0')
c= tensor(1.0287e+09, device='cuda:0')
c= tensor(1.0311e+09, device='cuda:0')
c= tensor(1.0343e+09, device='cuda:0')
c= tensor(1.0343e+09, device='cuda:0')
c= tensor(1.0346e+09, device='cuda:0')
c= tensor(1.0381e+09, device='cuda:0')
c= tensor(1.0475e+09, device='cuda:0')
c= tensor(1.0496e+09, device='cuda:0')
c= tensor(1.0496e+09, device='cuda:0')
c= tensor(1.0497e+09, device='cuda:0')
c= tensor(1.0498e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.0875e+09, device='cuda:0')
c= tensor(1.0931e+09, device='cuda:0')
c= tensor(1.0948e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0957e+09, device='cuda:0')
c= tensor(1.0959e+09, device='cuda:0')
c= tensor(1.0961e+09, device='cuda:0')
c= tensor(1.0961e+09, device='cuda:0')
c= tensor(1.0961e+09, device='cuda:0')
c= tensor(1.1623e+09, device='cuda:0')
c= tensor(1.1623e+09, device='cuda:0')
c= tensor(1.1636e+09, device='cuda:0')
c= tensor(1.1636e+09, device='cuda:0')
c= tensor(1.1636e+09, device='cuda:0')
c= tensor(1.1636e+09, device='cuda:0')
c= tensor(1.1641e+09, device='cuda:0')
c= tensor(1.1641e+09, device='cuda:0')
c= tensor(1.1641e+09, device='cuda:0')
c= tensor(1.1641e+09, device='cuda:0')
c= tensor(1.1641e+09, device='cuda:0')
c= tensor(1.1917e+09, device='cuda:0')
c= tensor(1.1941e+09, device='cuda:0')
c= tensor(1.1942e+09, device='cuda:0')
c= tensor(1.1948e+09, device='cuda:0')
c= tensor(1.1948e+09, device='cuda:0')
c= tensor(1.1948e+09, device='cuda:0')
c= tensor(1.1948e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.2216e+09, device='cuda:0')
c= tensor(1.2216e+09, device='cuda:0')
c= tensor(1.2218e+09, device='cuda:0')
c= tensor(1.2218e+09, device='cuda:0')
c= tensor(1.2232e+09, device='cuda:0')
c= tensor(1.2232e+09, device='cuda:0')
c= tensor(1.5192e+09, device='cuda:0')
c= tensor(1.5606e+09, device='cuda:0')
c= tensor(1.5614e+09, device='cuda:0')
c= tensor(1.5614e+09, device='cuda:0')
c= tensor(1.5614e+09, device='cuda:0')
c= tensor(1.5618e+09, device='cuda:0')
c= tensor(1.5618e+09, device='cuda:0')
c= tensor(1.5633e+09, device='cuda:0')
c= tensor(1.5633e+09, device='cuda:0')
c= tensor(1.5665e+09, device='cuda:0')
c= tensor(1.5691e+09, device='cuda:0')
c= tensor(1.5693e+09, device='cuda:0')
c= tensor(1.5694e+09, device='cuda:0')
c= tensor(1.5694e+09, device='cuda:0')
c= tensor(1.5694e+09, device='cuda:0')
c= tensor(1.5694e+09, device='cuda:0')
c= tensor(1.5715e+09, device='cuda:0')
c= tensor(1.5723e+09, device='cuda:0')
c= tensor(1.5723e+09, device='cuda:0')
c= tensor(1.5744e+09, device='cuda:0')
c= tensor(1.5745e+09, device='cuda:0')
c= tensor(1.5745e+09, device='cuda:0')
c= tensor(1.5745e+09, device='cuda:0')
c= tensor(1.5768e+09, device='cuda:0')
c= tensor(1.5781e+09, device='cuda:0')
c= tensor(1.5878e+09, device='cuda:0')
c= tensor(1.5899e+09, device='cuda:0')
c= tensor(1.5899e+09, device='cuda:0')
c= tensor(1.5906e+09, device='cuda:0')
c= tensor(1.5930e+09, device='cuda:0')
c= tensor(1.5950e+09, device='cuda:0')
c= tensor(1.5950e+09, device='cuda:0')
c= tensor(1.6172e+09, device='cuda:0')
c= tensor(1.6484e+09, device='cuda:0')
c= tensor(1.6496e+09, device='cuda:0')
c= tensor(1.6516e+09, device='cuda:0')
c= tensor(1.6516e+09, device='cuda:0')
c= tensor(1.6517e+09, device='cuda:0')
c= tensor(1.6517e+09, device='cuda:0')
c= tensor(1.6517e+09, device='cuda:0')
c= tensor(1.6554e+09, device='cuda:0')
c= tensor(1.6573e+09, device='cuda:0')
c= tensor(1.6633e+09, device='cuda:0')
c= tensor(1.6636e+09, device='cuda:0')
c= tensor(1.6643e+09, device='cuda:0')
c= tensor(1.6646e+09, device='cuda:0')
c= tensor(1.6649e+09, device='cuda:0')
c= tensor(1.6649e+09, device='cuda:0')
c= tensor(1.6649e+09, device='cuda:0')
c= tensor(1.6693e+09, device='cuda:0')
c= tensor(1.6693e+09, device='cuda:0')
c= tensor(1.6693e+09, device='cuda:0')
c= tensor(1.6693e+09, device='cuda:0')
c= tensor(1.7106e+09, device='cuda:0')
c= tensor(1.7110e+09, device='cuda:0')
c= tensor(1.7112e+09, device='cuda:0')
c= tensor(1.7114e+09, device='cuda:0')
c= tensor(1.7118e+09, device='cuda:0')
c= tensor(1.7118e+09, device='cuda:0')
c= tensor(1.7118e+09, device='cuda:0')
c= tensor(1.7120e+09, device='cuda:0')
c= tensor(1.7121e+09, device='cuda:0')
c= tensor(1.7121e+09, device='cuda:0')
c= tensor(1.7184e+09, device='cuda:0')
c= tensor(1.7184e+09, device='cuda:0')
c= tensor(1.7185e+09, device='cuda:0')
c= tensor(1.7185e+09, device='cuda:0')
c= tensor(1.7192e+09, device='cuda:0')
c= tensor(1.7192e+09, device='cuda:0')
c= tensor(1.7193e+09, device='cuda:0')
c= tensor(1.7194e+09, device='cuda:0')
c= tensor(1.7195e+09, device='cuda:0')
c= tensor(1.7197e+09, device='cuda:0')
c= tensor(1.7645e+09, device='cuda:0')
c= tensor(1.7645e+09, device='cuda:0')
c= tensor(1.7645e+09, device='cuda:0')
c= tensor(1.7692e+09, device='cuda:0')
c= tensor(1.7692e+09, device='cuda:0')
c= tensor(1.7760e+09, device='cuda:0')
c= tensor(1.7760e+09, device='cuda:0')
c= tensor(1.7773e+09, device='cuda:0')
c= tensor(1.7802e+09, device='cuda:0')
c= tensor(1.7802e+09, device='cuda:0')
c= tensor(1.8023e+09, device='cuda:0')
c= tensor(1.8023e+09, device='cuda:0')
c= tensor(1.8041e+09, device='cuda:0')
c= tensor(1.8041e+09, device='cuda:0')
c= tensor(1.8167e+09, device='cuda:0')
c= tensor(1.8167e+09, device='cuda:0')
c= tensor(1.8167e+09, device='cuda:0')
c= tensor(1.8168e+09, device='cuda:0')
c= tensor(1.8171e+09, device='cuda:0')
c= tensor(1.8175e+09, device='cuda:0')
c= tensor(1.8178e+09, device='cuda:0')
c= tensor(1.8178e+09, device='cuda:0')
c= tensor(1.8178e+09, device='cuda:0')
c= tensor(1.8179e+09, device='cuda:0')
c= tensor(1.8241e+09, device='cuda:0')
c= tensor(1.8242e+09, device='cuda:0')
c= tensor(1.8306e+09, device='cuda:0')
c= tensor(1.8310e+09, device='cuda:0')
c= tensor(1.8311e+09, device='cuda:0')
c= tensor(1.8311e+09, device='cuda:0')
c= tensor(1.8311e+09, device='cuda:0')
c= tensor(1.9324e+09, device='cuda:0')
c= tensor(1.9324e+09, device='cuda:0')
c= tensor(1.9325e+09, device='cuda:0')
c= tensor(1.9342e+09, device='cuda:0')
c= tensor(1.9358e+09, device='cuda:0')
c= tensor(1.9358e+09, device='cuda:0')
c= tensor(1.9358e+09, device='cuda:0')
c= tensor(1.9375e+09, device='cuda:0')
c= tensor(1.9376e+09, device='cuda:0')
c= tensor(1.9376e+09, device='cuda:0')
c= tensor(1.9377e+09, device='cuda:0')
c= tensor(1.9379e+09, device='cuda:0')
c= tensor(1.9381e+09, device='cuda:0')
c= tensor(1.9522e+09, device='cuda:0')
c= tensor(1.9529e+09, device='cuda:0')
c= tensor(1.9529e+09, device='cuda:0')
c= tensor(1.9539e+09, device='cuda:0')
c= tensor(1.9539e+09, device='cuda:0')
c= tensor(1.9539e+09, device='cuda:0')
c= tensor(1.9540e+09, device='cuda:0')
c= tensor(1.9540e+09, device='cuda:0')
c= tensor(1.9581e+09, device='cuda:0')
c= tensor(1.9581e+09, device='cuda:0')
c= tensor(1.9581e+09, device='cuda:0')
c= tensor(1.9582e+09, device='cuda:0')
c= tensor(1.9585e+09, device='cuda:0')
c= tensor(1.9587e+09, device='cuda:0')
c= tensor(1.9613e+09, device='cuda:0')
c= tensor(1.9613e+09, device='cuda:0')
c= tensor(1.9613e+09, device='cuda:0')
c= tensor(1.9614e+09, device='cuda:0')
c= tensor(1.9614e+09, device='cuda:0')
c= tensor(1.9614e+09, device='cuda:0')
c= tensor(1.9614e+09, device='cuda:0')
c= tensor(1.9645e+09, device='cuda:0')
c= tensor(1.9645e+09, device='cuda:0')
c= tensor(1.9645e+09, device='cuda:0')
c= tensor(1.9645e+09, device='cuda:0')
c= tensor(1.9656e+09, device='cuda:0')
c= tensor(2.0295e+09, device='cuda:0')
c= tensor(2.0298e+09, device='cuda:0')
c= tensor(2.0298e+09, device='cuda:0')
c= tensor(2.0299e+09, device='cuda:0')
c= tensor(2.0338e+09, device='cuda:0')
c= tensor(2.0338e+09, device='cuda:0')
c= tensor(2.0339e+09, device='cuda:0')
c= tensor(2.0339e+09, device='cuda:0')
c= tensor(2.0340e+09, device='cuda:0')
c= tensor(2.0352e+09, device='cuda:0')
c= tensor(2.0358e+09, device='cuda:0')
c= tensor(2.0358e+09, device='cuda:0')
c= tensor(2.0359e+09, device='cuda:0')
c= tensor(2.0359e+09, device='cuda:0')
c= tensor(2.0361e+09, device='cuda:0')
c= tensor(2.0366e+09, device='cuda:0')
c= tensor(2.0388e+09, device='cuda:0')
c= tensor(2.0390e+09, device='cuda:0')
c= tensor(2.0466e+09, device='cuda:0')
c= tensor(2.0466e+09, device='cuda:0')
c= tensor(2.0466e+09, device='cuda:0')
c= tensor(2.0466e+09, device='cuda:0')
c= tensor(2.0487e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0491e+09, device='cuda:0')
c= tensor(2.0491e+09, device='cuda:0')
c= tensor(2.0491e+09, device='cuda:0')
c= tensor(2.0491e+09, device='cuda:0')
c= tensor(2.0492e+09, device='cuda:0')
c= tensor(2.0492e+09, device='cuda:0')
c= tensor(2.0492e+09, device='cuda:0')
c= tensor(2.0492e+09, device='cuda:0')
c= tensor(2.0510e+09, device='cuda:0')
c= tensor(2.0550e+09, device='cuda:0')
c= tensor(2.0676e+09, device='cuda:0')
c= tensor(2.0843e+09, device='cuda:0')
c= tensor(2.0846e+09, device='cuda:0')
c= tensor(2.0846e+09, device='cuda:0')
c= tensor(2.0846e+09, device='cuda:0')
c= tensor(2.0864e+09, device='cuda:0')
c= tensor(2.0864e+09, device='cuda:0')
c= tensor(2.0865e+09, device='cuda:0')
c= tensor(2.0866e+09, device='cuda:0')
c= tensor(2.1101e+09, device='cuda:0')
c= tensor(2.1114e+09, device='cuda:0')
c= tensor(2.1114e+09, device='cuda:0')
c= tensor(2.1138e+09, device='cuda:0')
c= tensor(2.1140e+09, device='cuda:0')
c= tensor(2.1143e+09, device='cuda:0')
c= tensor(2.1198e+09, device='cuda:0')
c= tensor(2.1211e+09, device='cuda:0')
c= tensor(2.1221e+09, device='cuda:0')
c= tensor(2.1222e+09, device='cuda:0')
c= tensor(2.1266e+09, device='cuda:0')
c= tensor(2.1267e+09, device='cuda:0')
c= tensor(2.1271e+09, device='cuda:0')
c= tensor(2.1968e+09, device='cuda:0')
c= tensor(2.1970e+09, device='cuda:0')
c= tensor(2.1970e+09, device='cuda:0')
c= tensor(2.1970e+09, device='cuda:0')
c= tensor(2.1971e+09, device='cuda:0')
c= tensor(2.1971e+09, device='cuda:0')
c= tensor(2.1971e+09, device='cuda:0')
c= tensor(2.1972e+09, device='cuda:0')
c= tensor(2.2074e+09, device='cuda:0')
c= tensor(2.2074e+09, device='cuda:0')
c= tensor(2.3406e+09, device='cuda:0')
c= tensor(2.3407e+09, device='cuda:0')
c= tensor(2.3409e+09, device='cuda:0')
c= tensor(2.3409e+09, device='cuda:0')
c= tensor(2.3416e+09, device='cuda:0')
c= tensor(2.3465e+09, device='cuda:0')
c= tensor(2.3465e+09, device='cuda:0')
c= tensor(2.3912e+09, device='cuda:0')
c= tensor(2.3921e+09, device='cuda:0')
c= tensor(2.3922e+09, device='cuda:0')
c= tensor(2.3922e+09, device='cuda:0')
c= tensor(2.3922e+09, device='cuda:0')
c= tensor(2.3922e+09, device='cuda:0')
c= tensor(2.3922e+09, device='cuda:0')
c= tensor(2.3923e+09, device='cuda:0')
c= tensor(2.3931e+09, device='cuda:0')
c= tensor(2.8689e+09, device='cuda:0')
c= tensor(2.8801e+09, device='cuda:0')
c= tensor(2.8802e+09, device='cuda:0')
c= tensor(2.8803e+09, device='cuda:0')
c= tensor(2.8805e+09, device='cuda:0')
c= tensor(2.8805e+09, device='cuda:0')
c= tensor(2.8816e+09, device='cuda:0')
c= tensor(2.8817e+09, device='cuda:0')
c= tensor(2.9819e+09, device='cuda:0')
c= tensor(2.9819e+09, device='cuda:0')
c= tensor(2.9823e+09, device='cuda:0')
c= tensor(2.9824e+09, device='cuda:0')
c= tensor(2.9830e+09, device='cuda:0')
c= tensor(3.0098e+09, device='cuda:0')
c= tensor(3.0099e+09, device='cuda:0')
c= tensor(3.0099e+09, device='cuda:0')
c= tensor(3.0104e+09, device='cuda:0')
c= tensor(3.0104e+09, device='cuda:0')
c= tensor(3.0104e+09, device='cuda:0')
c= tensor(3.0116e+09, device='cuda:0')
c= tensor(3.1592e+09, device='cuda:0')
c= tensor(3.1593e+09, device='cuda:0')
c= tensor(3.1596e+09, device='cuda:0')
c= tensor(3.1615e+09, device='cuda:0')
c= tensor(3.1617e+09, device='cuda:0')
c= tensor(3.1618e+09, device='cuda:0')
c= tensor(3.1619e+09, device='cuda:0')
c= tensor(3.1770e+09, device='cuda:0')
c= tensor(3.1777e+09, device='cuda:0')
c= tensor(3.1802e+09, device='cuda:0')
c= tensor(3.1802e+09, device='cuda:0')
c= tensor(3.1803e+09, device='cuda:0')
c= tensor(3.1803e+09, device='cuda:0')
c= tensor(3.2472e+09, device='cuda:0')
c= tensor(3.2488e+09, device='cuda:0')
c= tensor(3.2488e+09, device='cuda:0')
c= tensor(3.2488e+09, device='cuda:0')
c= tensor(3.2490e+09, device='cuda:0')
c= tensor(3.2499e+09, device='cuda:0')
c= tensor(3.2509e+09, device='cuda:0')
c= tensor(3.2685e+09, device='cuda:0')
c= tensor(3.2685e+09, device='cuda:0')
c= tensor(3.2687e+09, device='cuda:0')
c= tensor(3.2690e+09, device='cuda:0')
c= tensor(3.2695e+09, device='cuda:0')
c= tensor(3.2700e+09, device='cuda:0')
c= tensor(3.2713e+09, device='cuda:0')
c= tensor(3.2717e+09, device='cuda:0')
c= tensor(3.2721e+09, device='cuda:0')
c= tensor(3.2721e+09, device='cuda:0')
c= tensor(3.2721e+09, device='cuda:0')
c= tensor(3.2749e+09, device='cuda:0')
c= tensor(3.2757e+09, device='cuda:0')
c= tensor(3.2762e+09, device='cuda:0')
c= tensor(3.2762e+09, device='cuda:0')
c= tensor(3.2762e+09, device='cuda:0')
c= tensor(3.2785e+09, device='cuda:0')
c= tensor(3.2790e+09, device='cuda:0')
c= tensor(3.2792e+09, device='cuda:0')
c= tensor(3.2792e+09, device='cuda:0')
c= tensor(3.2792e+09, device='cuda:0')
c= tensor(3.2793e+09, device='cuda:0')
c= tensor(3.2793e+09, device='cuda:0')
c= tensor(3.2839e+09, device='cuda:0')
c= tensor(3.2840e+09, device='cuda:0')
c= tensor(3.2841e+09, device='cuda:0')
c= tensor(3.2841e+09, device='cuda:0')
c= tensor(3.2841e+09, device='cuda:0')
c= tensor(3.2947e+09, device='cuda:0')
c= tensor(3.2948e+09, device='cuda:0')
c= tensor(3.2951e+09, device='cuda:0')
c= tensor(3.2977e+09, device='cuda:0')
c= tensor(3.2979e+09, device='cuda:0')
c= tensor(3.2995e+09, device='cuda:0')
c= tensor(3.3034e+09, device='cuda:0')
c= tensor(3.3034e+09, device='cuda:0')
c= tensor(3.3054e+09, device='cuda:0')
c= tensor(3.3056e+09, device='cuda:0')
c= tensor(3.3288e+09, device='cuda:0')
c= tensor(3.3352e+09, device='cuda:0')
c= tensor(3.3353e+09, device='cuda:0')
c= tensor(3.3361e+09, device='cuda:0')
c= tensor(3.3361e+09, device='cuda:0')
c= tensor(3.3362e+09, device='cuda:0')
c= tensor(3.3362e+09, device='cuda:0')
c= tensor(3.3362e+09, device='cuda:0')
c= tensor(3.3389e+09, device='cuda:0')
c= tensor(3.3396e+09, device='cuda:0')
c= tensor(3.3396e+09, device='cuda:0')
c= tensor(3.3398e+09, device='cuda:0')
c= tensor(3.3399e+09, device='cuda:0')
c= tensor(3.3573e+09, device='cuda:0')
c= tensor(3.3573e+09, device='cuda:0')
c= tensor(3.3574e+09, device='cuda:0')
c= tensor(3.3584e+09, device='cuda:0')
c= tensor(3.3600e+09, device='cuda:0')
c= tensor(3.3600e+09, device='cuda:0')
c= tensor(3.3617e+09, device='cuda:0')
c= tensor(3.3617e+09, device='cuda:0')
memory (bytes)
3927928832
time for making loss 2 is 14.892556190490723
p0 True
it  0 : 1023671808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
3928121344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3929079808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  47697207000.0
relative error loss 14.1884
shape of L is 
torch.Size([])
memory (bytes)
4171964416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  7% |
memory (bytes)
4171997184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  47696888000.0
relative error loss 14.188305
shape of L is 
torch.Size([])
memory (bytes)
4175192064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4175290368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  47695438000.0
relative error loss 14.187874
shape of L is 
torch.Size([])
memory (bytes)
4177354752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4177399808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  47689180000.0
relative error loss 14.186012
shape of L is 
torch.Size([])
memory (bytes)
4179283968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4179484672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  47652954000.0
relative error loss 14.175236
shape of L is 
torch.Size([])
memory (bytes)
4181565440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4181565440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  47256300000.0
relative error loss 14.057244
shape of L is 
torch.Size([])
memory (bytes)
4183613440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4183613440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  43441460000.0
relative error loss 12.92245
shape of L is 
torch.Size([])
memory (bytes)
4185845760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4185845760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  28078545000.0
relative error loss 8.352472
shape of L is 
torch.Size([])
memory (bytes)
4187832320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4187832320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  9562755000.0
relative error loss 2.844615
shape of L is 
torch.Size([])
memory (bytes)
4190109696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4190138368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  5456517000.0
relative error loss 1.6231401
time to take a step is 351.7238748073578
it  1 : 1286345728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4192219136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4192219136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  5456517000.0
relative error loss 1.6231401
shape of L is 
torch.Size([])
memory (bytes)
4194332672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4194365440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  3950363600.0
relative error loss 1.1751074
shape of L is 
torch.Size([])
memory (bytes)
4196446208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4196478976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  3477594600.0
relative error loss 1.0344737
shape of L is 
torch.Size([])
memory (bytes)
4198387712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4198580224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  3041133300.0
relative error loss 0.9046403
shape of L is 
torch.Size([])
memory (bytes)
4200706048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4200706048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  2755370000.0
relative error loss 0.8196348
shape of L is 
torch.Size([])
memory (bytes)
4202754048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4202754048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  2671147300.0
relative error loss 0.79458123
shape of L is 
torch.Size([])
memory (bytes)
4204986368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4205002752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  2365283000.0
relative error loss 0.70359635
shape of L is 
torch.Size([])
memory (bytes)
4207026176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4207026176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  2162432000.0
relative error loss 0.64325464
shape of L is 
torch.Size([])
memory (bytes)
4209004544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4209184768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  1883911300.0
relative error loss 0.5604036
shape of L is 
torch.Size([])
memory (bytes)
4211331072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4211376128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  1660272600.0
relative error loss 0.49387822
time to take a step is 336.6432502269745
it  2 : 1361320960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4213288960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4213288960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  1660272600.0
relative error loss 0.49387822
shape of L is 
torch.Size([])
memory (bytes)
4215582720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4215603200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  1498279800.0
relative error loss 0.4456905
shape of L is 
torch.Size([])
memory (bytes)
4217520128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4217520128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  1324563700.0
relative error loss 0.39401552
shape of L is 
torch.Size([])
memory (bytes)
4219809792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4219809792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  1187670000.0
relative error loss 0.35329401
shape of L is 
torch.Size([])
memory (bytes)
4221894656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4221894656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  1044330500.0
relative error loss 0.31065506
shape of L is 
torch.Size([])
memory (bytes)
4223774720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4223975424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  931202560.0
relative error loss 0.2770031
shape of L is 
torch.Size([])
memory (bytes)
4226129920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4226129920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  827160300.0
relative error loss 0.24605384
shape of L is 
torch.Size([])
memory (bytes)
4228136960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4228136960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  724404500.0
relative error loss 0.21548726
shape of L is 
torch.Size([])
memory (bytes)
4230352896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4230393856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  702236400.0
relative error loss 0.20889297
shape of L is 
torch.Size([])
memory (bytes)
4232482816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4232511488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  663223300.0
relative error loss 0.19728781
time to take a step is 337.5634980201721
it  3 : 1361844736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4234506240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4234506240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  663223300.0
relative error loss 0.19728781
shape of L is 
torch.Size([])
memory (bytes)
4236759040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4236759040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  615553300.0
relative error loss 0.1831075
shape of L is 
torch.Size([])
memory (bytes)
4238610432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4238827520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  581257200.0
relative error loss 0.1729055
shape of L is 
torch.Size([])
memory (bytes)
4241014784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4241039360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  501681660.0
relative error loss 0.14923431
shape of L is 
torch.Size([])
memory (bytes)
4243025920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4243185664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  462725630.0
relative error loss 0.13764614
shape of L is 
torch.Size([])
memory (bytes)
4245217280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4245336064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  439660800.0
relative error loss 0.13078508
shape of L is 
torch.Size([])
memory (bytes)
4247470080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4247470080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  384498700.0
relative error loss 0.11437611
shape of L is 
torch.Size([])
memory (bytes)
4249522176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4249522176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  365769730.0
relative error loss 0.108804844
shape of L is 
torch.Size([])
memory (bytes)
4251746304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4251766784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  338387700.0
relative error loss 0.100659564
shape of L is 
torch.Size([])
memory (bytes)
4253798400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4253798400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  319226880.0
relative error loss 0.09495983
time to take a step is 339.4521276950836
c= tensor(939.2990, device='cuda:0')
c= tensor(51295.8945, device='cuda:0')
c= tensor(53097.8828, device='cuda:0')
c= tensor(55348.7500, device='cuda:0')
c= tensor(408936., device='cuda:0')
c= tensor(464027.2500, device='cuda:0')
c= tensor(857712.2500, device='cuda:0')
c= tensor(1306182.2500, device='cuda:0')
c= tensor(1328873.1250, device='cuda:0')
c= tensor(4941614.5000, device='cuda:0')
c= tensor(4953223.5000, device='cuda:0')
c= tensor(12672544., device='cuda:0')
c= tensor(12692685., device='cuda:0')
c= tensor(14590588., device='cuda:0')
c= tensor(14703862., device='cuda:0')
c= tensor(15028333., device='cuda:0')
c= tensor(15278127., device='cuda:0')
c= tensor(15705138., device='cuda:0')
c= tensor(1.2961e+08, device='cuda:0')
c= tensor(1.3090e+08, device='cuda:0')
c= tensor(1.3092e+08, device='cuda:0')
c= tensor(1.7327e+08, device='cuda:0')
c= tensor(1.7338e+08, device='cuda:0')
c= tensor(1.7340e+08, device='cuda:0')
c= tensor(1.7388e+08, device='cuda:0')
c= tensor(1.7453e+08, device='cuda:0')
c= tensor(1.7455e+08, device='cuda:0')
c= tensor(1.7457e+08, device='cuda:0')
c= tensor(1.7502e+08, device='cuda:0')
c= tensor(6.7368e+08, device='cuda:0')
c= tensor(6.7368e+08, device='cuda:0')
c= tensor(6.7982e+08, device='cuda:0')
c= tensor(6.7984e+08, device='cuda:0')
c= tensor(6.7985e+08, device='cuda:0')
c= tensor(6.7985e+08, device='cuda:0')
c= tensor(6.8001e+08, device='cuda:0')
c= tensor(6.8043e+08, device='cuda:0')
c= tensor(6.8044e+08, device='cuda:0')
c= tensor(6.8044e+08, device='cuda:0')
c= tensor(6.8044e+08, device='cuda:0')
c= tensor(6.8045e+08, device='cuda:0')
c= tensor(6.8045e+08, device='cuda:0')
c= tensor(6.8045e+08, device='cuda:0')
c= tensor(6.8046e+08, device='cuda:0')
c= tensor(6.8046e+08, device='cuda:0')
c= tensor(6.8046e+08, device='cuda:0')
c= tensor(6.8047e+08, device='cuda:0')
c= tensor(6.8047e+08, device='cuda:0')
c= tensor(6.8047e+08, device='cuda:0')
c= tensor(6.8048e+08, device='cuda:0')
c= tensor(6.8049e+08, device='cuda:0')
c= tensor(6.8049e+08, device='cuda:0')
c= tensor(6.8049e+08, device='cuda:0')
c= tensor(6.8049e+08, device='cuda:0')
c= tensor(6.8050e+08, device='cuda:0')
c= tensor(6.8051e+08, device='cuda:0')
c= tensor(6.8051e+08, device='cuda:0')
c= tensor(6.8052e+08, device='cuda:0')
c= tensor(6.8052e+08, device='cuda:0')
c= tensor(6.8052e+08, device='cuda:0')
c= tensor(6.8053e+08, device='cuda:0')
c= tensor(6.8053e+08, device='cuda:0')
c= tensor(6.8053e+08, device='cuda:0')
c= tensor(6.8054e+08, device='cuda:0')
c= tensor(6.8055e+08, device='cuda:0')
c= tensor(6.8055e+08, device='cuda:0')
c= tensor(6.8055e+08, device='cuda:0')
c= tensor(6.8056e+08, device='cuda:0')
c= tensor(6.8057e+08, device='cuda:0')
c= tensor(6.8057e+08, device='cuda:0')
c= tensor(6.8057e+08, device='cuda:0')
c= tensor(6.8058e+08, device='cuda:0')
c= tensor(6.8059e+08, device='cuda:0')
c= tensor(6.8059e+08, device='cuda:0')
c= tensor(6.8059e+08, device='cuda:0')
c= tensor(6.8059e+08, device='cuda:0')
c= tensor(6.8060e+08, device='cuda:0')
c= tensor(6.8060e+08, device='cuda:0')
c= tensor(6.8060e+08, device='cuda:0')
c= tensor(6.8063e+08, device='cuda:0')
c= tensor(6.8064e+08, device='cuda:0')
c= tensor(6.8064e+08, device='cuda:0')
c= tensor(6.8064e+08, device='cuda:0')
c= tensor(6.8064e+08, device='cuda:0')
c= tensor(6.8065e+08, device='cuda:0')
c= tensor(6.8065e+08, device='cuda:0')
c= tensor(6.8066e+08, device='cuda:0')
c= tensor(6.8066e+08, device='cuda:0')
c= tensor(6.8066e+08, device='cuda:0')
c= tensor(6.8066e+08, device='cuda:0')
c= tensor(6.8067e+08, device='cuda:0')
c= tensor(6.8067e+08, device='cuda:0')
c= tensor(6.8067e+08, device='cuda:0')
c= tensor(6.8067e+08, device='cuda:0')
c= tensor(6.8067e+08, device='cuda:0')
c= tensor(6.8068e+08, device='cuda:0')
c= tensor(6.8068e+08, device='cuda:0')
c= tensor(6.8079e+08, device='cuda:0')
c= tensor(6.8079e+08, device='cuda:0')
c= tensor(6.8080e+08, device='cuda:0')
c= tensor(6.8081e+08, device='cuda:0')
c= tensor(6.8081e+08, device='cuda:0')
c= tensor(6.8082e+08, device='cuda:0')
c= tensor(6.8082e+08, device='cuda:0')
c= tensor(6.8082e+08, device='cuda:0')
c= tensor(6.8082e+08, device='cuda:0')
c= tensor(6.8083e+08, device='cuda:0')
c= tensor(6.8083e+08, device='cuda:0')
c= tensor(6.8083e+08, device='cuda:0')
c= tensor(6.8083e+08, device='cuda:0')
c= tensor(6.8083e+08, device='cuda:0')
c= tensor(6.8084e+08, device='cuda:0')
c= tensor(6.8084e+08, device='cuda:0')
c= tensor(6.8084e+08, device='cuda:0')
c= tensor(6.8084e+08, device='cuda:0')
c= tensor(6.8085e+08, device='cuda:0')
c= tensor(6.8085e+08, device='cuda:0')
c= tensor(6.8085e+08, device='cuda:0')
c= tensor(6.8086e+08, device='cuda:0')
c= tensor(6.8086e+08, device='cuda:0')
c= tensor(6.8088e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8089e+08, device='cuda:0')
c= tensor(6.8090e+08, device='cuda:0')
c= tensor(6.8094e+08, device='cuda:0')
c= tensor(6.8094e+08, device='cuda:0')
c= tensor(6.8095e+08, device='cuda:0')
c= tensor(6.8095e+08, device='cuda:0')
c= tensor(6.8095e+08, device='cuda:0')
c= tensor(6.8095e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8096e+08, device='cuda:0')
c= tensor(6.8097e+08, device='cuda:0')
c= tensor(6.8097e+08, device='cuda:0')
c= tensor(6.8097e+08, device='cuda:0')
c= tensor(6.8099e+08, device='cuda:0')
c= tensor(6.8100e+08, device='cuda:0')
c= tensor(6.8100e+08, device='cuda:0')
c= tensor(6.8100e+08, device='cuda:0')
c= tensor(6.8100e+08, device='cuda:0')
c= tensor(6.8100e+08, device='cuda:0')
c= tensor(6.8101e+08, device='cuda:0')
c= tensor(6.8101e+08, device='cuda:0')
c= tensor(6.8101e+08, device='cuda:0')
c= tensor(6.8101e+08, device='cuda:0')
c= tensor(6.8102e+08, device='cuda:0')
c= tensor(6.8102e+08, device='cuda:0')
c= tensor(6.8105e+08, device='cuda:0')
c= tensor(6.8105e+08, device='cuda:0')
c= tensor(6.8105e+08, device='cuda:0')
c= tensor(6.8106e+08, device='cuda:0')
c= tensor(6.8106e+08, device='cuda:0')
c= tensor(6.8107e+08, device='cuda:0')
c= tensor(6.8107e+08, device='cuda:0')
c= tensor(6.8107e+08, device='cuda:0')
c= tensor(6.8107e+08, device='cuda:0')
c= tensor(6.8107e+08, device='cuda:0')
c= tensor(6.8108e+08, device='cuda:0')
c= tensor(6.8108e+08, device='cuda:0')
c= tensor(6.8108e+08, device='cuda:0')
c= tensor(6.8109e+08, device='cuda:0')
c= tensor(6.8109e+08, device='cuda:0')
c= tensor(6.8109e+08, device='cuda:0')
c= tensor(6.8109e+08, device='cuda:0')
c= tensor(6.8111e+08, device='cuda:0')
c= tensor(6.8111e+08, device='cuda:0')
c= tensor(6.8112e+08, device='cuda:0')
c= tensor(6.8112e+08, device='cuda:0')
c= tensor(6.8113e+08, device='cuda:0')
c= tensor(6.8113e+08, device='cuda:0')
c= tensor(6.8114e+08, device='cuda:0')
c= tensor(6.8114e+08, device='cuda:0')
c= tensor(6.8115e+08, device='cuda:0')
c= tensor(6.8115e+08, device='cuda:0')
c= tensor(6.8115e+08, device='cuda:0')
c= tensor(6.8115e+08, device='cuda:0')
c= tensor(6.8116e+08, device='cuda:0')
c= tensor(6.8116e+08, device='cuda:0')
c= tensor(6.8117e+08, device='cuda:0')
c= tensor(6.8117e+08, device='cuda:0')
c= tensor(6.8117e+08, device='cuda:0')
c= tensor(6.8118e+08, device='cuda:0')
c= tensor(6.8118e+08, device='cuda:0')
c= tensor(6.8118e+08, device='cuda:0')
c= tensor(6.8118e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8119e+08, device='cuda:0')
c= tensor(6.8120e+08, device='cuda:0')
c= tensor(6.8120e+08, device='cuda:0')
c= tensor(6.8120e+08, device='cuda:0')
c= tensor(6.8121e+08, device='cuda:0')
c= tensor(6.8121e+08, device='cuda:0')
c= tensor(6.8122e+08, device='cuda:0')
c= tensor(6.8122e+08, device='cuda:0')
c= tensor(6.8122e+08, device='cuda:0')
c= tensor(6.8122e+08, device='cuda:0')
c= tensor(6.8122e+08, device='cuda:0')
c= tensor(6.8123e+08, device='cuda:0')
c= tensor(6.8123e+08, device='cuda:0')
c= tensor(6.8123e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8124e+08, device='cuda:0')
c= tensor(6.8129e+08, device='cuda:0')
c= tensor(6.8129e+08, device='cuda:0')
c= tensor(6.8129e+08, device='cuda:0')
c= tensor(6.8130e+08, device='cuda:0')
c= tensor(6.8130e+08, device='cuda:0')
c= tensor(6.8131e+08, device='cuda:0')
c= tensor(6.8131e+08, device='cuda:0')
c= tensor(6.8131e+08, device='cuda:0')
c= tensor(6.8131e+08, device='cuda:0')
c= tensor(6.8132e+08, device='cuda:0')
c= tensor(6.8132e+08, device='cuda:0')
c= tensor(6.8132e+08, device='cuda:0')
c= tensor(6.8132e+08, device='cuda:0')
c= tensor(6.8132e+08, device='cuda:0')
c= tensor(6.8133e+08, device='cuda:0')
c= tensor(6.8133e+08, device='cuda:0')
c= tensor(6.8133e+08, device='cuda:0')
c= tensor(6.8133e+08, device='cuda:0')
c= tensor(6.8134e+08, device='cuda:0')
c= tensor(6.8135e+08, device='cuda:0')
c= tensor(6.8135e+08, device='cuda:0')
c= tensor(6.8136e+08, device='cuda:0')
c= tensor(6.8145e+08, device='cuda:0')
c= tensor(6.8156e+08, device='cuda:0')
c= tensor(6.8157e+08, device='cuda:0')
c= tensor(6.8157e+08, device='cuda:0')
c= tensor(6.8158e+08, device='cuda:0')
c= tensor(6.8191e+08, device='cuda:0')
c= tensor(6.8303e+08, device='cuda:0')
c= tensor(6.8304e+08, device='cuda:0')
c= tensor(6.8316e+08, device='cuda:0')
c= tensor(6.8371e+08, device='cuda:0')
c= tensor(6.8375e+08, device='cuda:0')
c= tensor(7.5556e+08, device='cuda:0')
c= tensor(7.5556e+08, device='cuda:0')
c= tensor(7.5557e+08, device='cuda:0')
c= tensor(7.6081e+08, device='cuda:0')
c= tensor(8.1683e+08, device='cuda:0')
c= tensor(8.1684e+08, device='cuda:0')
c= tensor(8.1693e+08, device='cuda:0')
c= tensor(8.1710e+08, device='cuda:0')
c= tensor(8.1871e+08, device='cuda:0')
c= tensor(8.1922e+08, device='cuda:0')
c= tensor(8.2083e+08, device='cuda:0')
c= tensor(8.2138e+08, device='cuda:0')
c= tensor(8.2145e+08, device='cuda:0')
c= tensor(8.2158e+08, device='cuda:0')
c= tensor(8.3108e+08, device='cuda:0')
c= tensor(8.3110e+08, device='cuda:0')
c= tensor(8.3110e+08, device='cuda:0')
c= tensor(8.3197e+08, device='cuda:0')
c= tensor(8.3200e+08, device='cuda:0')
c= tensor(8.3642e+08, device='cuda:0')
c= tensor(8.3869e+08, device='cuda:0')
c= tensor(8.3869e+08, device='cuda:0')
c= tensor(8.3870e+08, device='cuda:0')
c= tensor(8.3871e+08, device='cuda:0')
c= tensor(8.3875e+08, device='cuda:0')
c= tensor(8.4271e+08, device='cuda:0')
c= tensor(8.4306e+08, device='cuda:0')
c= tensor(8.4347e+08, device='cuda:0')
c= tensor(8.4347e+08, device='cuda:0')
c= tensor(8.4347e+08, device='cuda:0')
c= tensor(8.4348e+08, device='cuda:0')
c= tensor(8.5302e+08, device='cuda:0')
c= tensor(8.5319e+08, device='cuda:0')
c= tensor(8.5320e+08, device='cuda:0')
c= tensor(8.7397e+08, device='cuda:0')
c= tensor(8.7408e+08, device='cuda:0')
c= tensor(8.7421e+08, device='cuda:0')
c= tensor(8.7605e+08, device='cuda:0')
c= tensor(8.7606e+08, device='cuda:0')
c= tensor(8.7634e+08, device='cuda:0')
c= tensor(8.7665e+08, device='cuda:0')
c= tensor(8.7955e+08, device='cuda:0')
c= tensor(8.7957e+08, device='cuda:0')
c= tensor(8.7958e+08, device='cuda:0')
c= tensor(8.7967e+08, device='cuda:0')
c= tensor(8.7968e+08, device='cuda:0')
c= tensor(8.7977e+08, device='cuda:0')
c= tensor(8.7983e+08, device='cuda:0')
c= tensor(8.8013e+08, device='cuda:0')
c= tensor(8.9690e+08, device='cuda:0')
c= tensor(8.9708e+08, device='cuda:0')
c= tensor(8.9710e+08, device='cuda:0')
c= tensor(8.9711e+08, device='cuda:0')
c= tensor(9.0058e+08, device='cuda:0')
c= tensor(9.0087e+08, device='cuda:0')
c= tensor(9.0088e+08, device='cuda:0')
c= tensor(9.0089e+08, device='cuda:0')
c= tensor(9.0411e+08, device='cuda:0')
c= tensor(9.0411e+08, device='cuda:0')
c= tensor(1.0272e+09, device='cuda:0')
c= tensor(1.0272e+09, device='cuda:0')
c= tensor(1.0273e+09, device='cuda:0')
c= tensor(1.0278e+09, device='cuda:0')
c= tensor(1.0279e+09, device='cuda:0')
c= tensor(1.0287e+09, device='cuda:0')
c= tensor(1.0287e+09, device='cuda:0')
c= tensor(1.0311e+09, device='cuda:0')
c= tensor(1.0343e+09, device='cuda:0')
c= tensor(1.0343e+09, device='cuda:0')
c= tensor(1.0346e+09, device='cuda:0')
c= tensor(1.0381e+09, device='cuda:0')
c= tensor(1.0475e+09, device='cuda:0')
c= tensor(1.0496e+09, device='cuda:0')
c= tensor(1.0496e+09, device='cuda:0')
c= tensor(1.0497e+09, device='cuda:0')
c= tensor(1.0498e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.0875e+09, device='cuda:0')
c= tensor(1.0931e+09, device='cuda:0')
c= tensor(1.0948e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0957e+09, device='cuda:0')
c= tensor(1.0959e+09, device='cuda:0')
c= tensor(1.0961e+09, device='cuda:0')
c= tensor(1.0961e+09, device='cuda:0')
c= tensor(1.0961e+09, device='cuda:0')
c= tensor(1.1623e+09, device='cuda:0')
c= tensor(1.1623e+09, device='cuda:0')
c= tensor(1.1636e+09, device='cuda:0')
c= tensor(1.1636e+09, device='cuda:0')
c= tensor(1.1636e+09, device='cuda:0')
c= tensor(1.1636e+09, device='cuda:0')
c= tensor(1.1641e+09, device='cuda:0')
c= tensor(1.1641e+09, device='cuda:0')
c= tensor(1.1641e+09, device='cuda:0')
c= tensor(1.1641e+09, device='cuda:0')
c= tensor(1.1641e+09, device='cuda:0')
c= tensor(1.1917e+09, device='cuda:0')
c= tensor(1.1941e+09, device='cuda:0')
c= tensor(1.1942e+09, device='cuda:0')
c= tensor(1.1948e+09, device='cuda:0')
c= tensor(1.1948e+09, device='cuda:0')
c= tensor(1.1948e+09, device='cuda:0')
c= tensor(1.1948e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.2216e+09, device='cuda:0')
c= tensor(1.2216e+09, device='cuda:0')
c= tensor(1.2218e+09, device='cuda:0')
c= tensor(1.2218e+09, device='cuda:0')
c= tensor(1.2232e+09, device='cuda:0')
c= tensor(1.2232e+09, device='cuda:0')
c= tensor(1.5192e+09, device='cuda:0')
c= tensor(1.5606e+09, device='cuda:0')
c= tensor(1.5614e+09, device='cuda:0')
c= tensor(1.5614e+09, device='cuda:0')
c= tensor(1.5614e+09, device='cuda:0')
c= tensor(1.5618e+09, device='cuda:0')
c= tensor(1.5618e+09, device='cuda:0')
c= tensor(1.5633e+09, device='cuda:0')
c= tensor(1.5633e+09, device='cuda:0')
c= tensor(1.5665e+09, device='cuda:0')
c= tensor(1.5691e+09, device='cuda:0')
c= tensor(1.5693e+09, device='cuda:0')
c= tensor(1.5694e+09, device='cuda:0')
c= tensor(1.5694e+09, device='cuda:0')
c= tensor(1.5694e+09, device='cuda:0')
c= tensor(1.5694e+09, device='cuda:0')
c= tensor(1.5715e+09, device='cuda:0')
c= tensor(1.5723e+09, device='cuda:0')
c= tensor(1.5723e+09, device='cuda:0')
c= tensor(1.5744e+09, device='cuda:0')
c= tensor(1.5745e+09, device='cuda:0')
c= tensor(1.5745e+09, device='cuda:0')
c= tensor(1.5745e+09, device='cuda:0')
c= tensor(1.5768e+09, device='cuda:0')
c= tensor(1.5781e+09, device='cuda:0')
c= tensor(1.5878e+09, device='cuda:0')
c= tensor(1.5899e+09, device='cuda:0')
c= tensor(1.5899e+09, device='cuda:0')
c= tensor(1.5906e+09, device='cuda:0')
c= tensor(1.5930e+09, device='cuda:0')
c= tensor(1.5950e+09, device='cuda:0')
c= tensor(1.5950e+09, device='cuda:0')
c= tensor(1.6172e+09, device='cuda:0')
c= tensor(1.6484e+09, device='cuda:0')
c= tensor(1.6496e+09, device='cuda:0')
c= tensor(1.6516e+09, device='cuda:0')
c= tensor(1.6516e+09, device='cuda:0')
c= tensor(1.6517e+09, device='cuda:0')
c= tensor(1.6517e+09, device='cuda:0')
c= tensor(1.6517e+09, device='cuda:0')
c= tensor(1.6554e+09, device='cuda:0')
c= tensor(1.6573e+09, device='cuda:0')
c= tensor(1.6633e+09, device='cuda:0')
c= tensor(1.6636e+09, device='cuda:0')
c= tensor(1.6643e+09, device='cuda:0')
c= tensor(1.6646e+09, device='cuda:0')
c= tensor(1.6649e+09, device='cuda:0')
c= tensor(1.6649e+09, device='cuda:0')
c= tensor(1.6649e+09, device='cuda:0')
c= tensor(1.6693e+09, device='cuda:0')
c= tensor(1.6693e+09, device='cuda:0')
c= tensor(1.6693e+09, device='cuda:0')
c= tensor(1.6693e+09, device='cuda:0')
c= tensor(1.7106e+09, device='cuda:0')
c= tensor(1.7110e+09, device='cuda:0')
c= tensor(1.7112e+09, device='cuda:0')
c= tensor(1.7114e+09, device='cuda:0')
c= tensor(1.7118e+09, device='cuda:0')
c= tensor(1.7118e+09, device='cuda:0')
c= tensor(1.7118e+09, device='cuda:0')
c= tensor(1.7120e+09, device='cuda:0')
c= tensor(1.7121e+09, device='cuda:0')
c= tensor(1.7121e+09, device='cuda:0')
c= tensor(1.7184e+09, device='cuda:0')
c= tensor(1.7184e+09, device='cuda:0')
c= tensor(1.7185e+09, device='cuda:0')
c= tensor(1.7185e+09, device='cuda:0')
c= tensor(1.7192e+09, device='cuda:0')
c= tensor(1.7192e+09, device='cuda:0')
c= tensor(1.7193e+09, device='cuda:0')
c= tensor(1.7194e+09, device='cuda:0')
c= tensor(1.7195e+09, device='cuda:0')
c= tensor(1.7197e+09, device='cuda:0')
c= tensor(1.7645e+09, device='cuda:0')
c= tensor(1.7645e+09, device='cuda:0')
c= tensor(1.7645e+09, device='cuda:0')
c= tensor(1.7692e+09, device='cuda:0')
c= tensor(1.7692e+09, device='cuda:0')
c= tensor(1.7760e+09, device='cuda:0')
c= tensor(1.7760e+09, device='cuda:0')
c= tensor(1.7773e+09, device='cuda:0')
c= tensor(1.7802e+09, device='cuda:0')
c= tensor(1.7802e+09, device='cuda:0')
c= tensor(1.8023e+09, device='cuda:0')
c= tensor(1.8023e+09, device='cuda:0')
c= tensor(1.8041e+09, device='cuda:0')
c= tensor(1.8041e+09, device='cuda:0')
c= tensor(1.8167e+09, device='cuda:0')
c= tensor(1.8167e+09, device='cuda:0')
c= tensor(1.8167e+09, device='cuda:0')
c= tensor(1.8168e+09, device='cuda:0')
c= tensor(1.8171e+09, device='cuda:0')
c= tensor(1.8175e+09, device='cuda:0')
c= tensor(1.8178e+09, device='cuda:0')
c= tensor(1.8178e+09, device='cuda:0')
c= tensor(1.8178e+09, device='cuda:0')
c= tensor(1.8179e+09, device='cuda:0')
c= tensor(1.8241e+09, device='cuda:0')
c= tensor(1.8242e+09, device='cuda:0')
c= tensor(1.8306e+09, device='cuda:0')
c= tensor(1.8310e+09, device='cuda:0')
c= tensor(1.8311e+09, device='cuda:0')
c= tensor(1.8311e+09, device='cuda:0')
c= tensor(1.8311e+09, device='cuda:0')
c= tensor(1.9324e+09, device='cuda:0')
c= tensor(1.9324e+09, device='cuda:0')
c= tensor(1.9325e+09, device='cuda:0')
c= tensor(1.9342e+09, device='cuda:0')
c= tensor(1.9358e+09, device='cuda:0')
c= tensor(1.9358e+09, device='cuda:0')
c= tensor(1.9358e+09, device='cuda:0')
c= tensor(1.9375e+09, device='cuda:0')
c= tensor(1.9376e+09, device='cuda:0')
c= tensor(1.9376e+09, device='cuda:0')
c= tensor(1.9377e+09, device='cuda:0')
c= tensor(1.9379e+09, device='cuda:0')
c= tensor(1.9381e+09, device='cuda:0')
c= tensor(1.9522e+09, device='cuda:0')
c= tensor(1.9529e+09, device='cuda:0')
c= tensor(1.9529e+09, device='cuda:0')
c= tensor(1.9539e+09, device='cuda:0')
c= tensor(1.9539e+09, device='cuda:0')
c= tensor(1.9539e+09, device='cuda:0')
c= tensor(1.9540e+09, device='cuda:0')
c= tensor(1.9540e+09, device='cuda:0')
c= tensor(1.9581e+09, device='cuda:0')
c= tensor(1.9581e+09, device='cuda:0')
c= tensor(1.9581e+09, device='cuda:0')
c= tensor(1.9582e+09, device='cuda:0')
c= tensor(1.9585e+09, device='cuda:0')
c= tensor(1.9587e+09, device='cuda:0')
c= tensor(1.9613e+09, device='cuda:0')
c= tensor(1.9613e+09, device='cuda:0')
c= tensor(1.9613e+09, device='cuda:0')
c= tensor(1.9614e+09, device='cuda:0')
c= tensor(1.9614e+09, device='cuda:0')
c= tensor(1.9614e+09, device='cuda:0')
c= tensor(1.9614e+09, device='cuda:0')
c= tensor(1.9645e+09, device='cuda:0')
c= tensor(1.9645e+09, device='cuda:0')
c= tensor(1.9645e+09, device='cuda:0')
c= tensor(1.9645e+09, device='cuda:0')
c= tensor(1.9656e+09, device='cuda:0')
c= tensor(2.0295e+09, device='cuda:0')
c= tensor(2.0298e+09, device='cuda:0')
c= tensor(2.0298e+09, device='cuda:0')
c= tensor(2.0299e+09, device='cuda:0')
c= tensor(2.0338e+09, device='cuda:0')
c= tensor(2.0338e+09, device='cuda:0')
c= tensor(2.0339e+09, device='cuda:0')
c= tensor(2.0339e+09, device='cuda:0')
c= tensor(2.0340e+09, device='cuda:0')
c= tensor(2.0352e+09, device='cuda:0')
c= tensor(2.0358e+09, device='cuda:0')
c= tensor(2.0358e+09, device='cuda:0')
c= tensor(2.0359e+09, device='cuda:0')
c= tensor(2.0359e+09, device='cuda:0')
c= tensor(2.0361e+09, device='cuda:0')
c= tensor(2.0366e+09, device='cuda:0')
c= tensor(2.0388e+09, device='cuda:0')
c= tensor(2.0390e+09, device='cuda:0')
c= tensor(2.0466e+09, device='cuda:0')
c= tensor(2.0466e+09, device='cuda:0')
c= tensor(2.0466e+09, device='cuda:0')
c= tensor(2.0466e+09, device='cuda:0')
c= tensor(2.0487e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0491e+09, device='cuda:0')
c= tensor(2.0491e+09, device='cuda:0')
c= tensor(2.0491e+09, device='cuda:0')
c= tensor(2.0491e+09, device='cuda:0')
c= tensor(2.0492e+09, device='cuda:0')
c= tensor(2.0492e+09, device='cuda:0')
c= tensor(2.0492e+09, device='cuda:0')
c= tensor(2.0492e+09, device='cuda:0')
c= tensor(2.0510e+09, device='cuda:0')
c= tensor(2.0550e+09, device='cuda:0')
c= tensor(2.0676e+09, device='cuda:0')
c= tensor(2.0843e+09, device='cuda:0')
c= tensor(2.0846e+09, device='cuda:0')
c= tensor(2.0846e+09, device='cuda:0')
c= tensor(2.0846e+09, device='cuda:0')
c= tensor(2.0864e+09, device='cuda:0')
c= tensor(2.0864e+09, device='cuda:0')
c= tensor(2.0865e+09, device='cuda:0')
c= tensor(2.0866e+09, device='cuda:0')
c= tensor(2.1101e+09, device='cuda:0')
c= tensor(2.1114e+09, device='cuda:0')
c= tensor(2.1114e+09, device='cuda:0')
c= tensor(2.1138e+09, device='cuda:0')
c= tensor(2.1140e+09, device='cuda:0')
c= tensor(2.1143e+09, device='cuda:0')
c= tensor(2.1198e+09, device='cuda:0')
c= tensor(2.1211e+09, device='cuda:0')
c= tensor(2.1221e+09, device='cuda:0')
c= tensor(2.1222e+09, device='cuda:0')
c= tensor(2.1266e+09, device='cuda:0')
c= tensor(2.1267e+09, device='cuda:0')
c= tensor(2.1271e+09, device='cuda:0')
c= tensor(2.1968e+09, device='cuda:0')
c= tensor(2.1970e+09, device='cuda:0')
c= tensor(2.1970e+09, device='cuda:0')
c= tensor(2.1970e+09, device='cuda:0')
c= tensor(2.1971e+09, device='cuda:0')
c= tensor(2.1971e+09, device='cuda:0')
c= tensor(2.1971e+09, device='cuda:0')
c= tensor(2.1972e+09, device='cuda:0')
c= tensor(2.2074e+09, device='cuda:0')
c= tensor(2.2074e+09, device='cuda:0')
c= tensor(2.3406e+09, device='cuda:0')
c= tensor(2.3407e+09, device='cuda:0')
c= tensor(2.3409e+09, device='cuda:0')
c= tensor(2.3409e+09, device='cuda:0')
c= tensor(2.3416e+09, device='cuda:0')
c= tensor(2.3465e+09, device='cuda:0')
c= tensor(2.3465e+09, device='cuda:0')
c= tensor(2.3912e+09, device='cuda:0')
c= tensor(2.3921e+09, device='cuda:0')
c= tensor(2.3922e+09, device='cuda:0')
c= tensor(2.3922e+09, device='cuda:0')
c= tensor(2.3922e+09, device='cuda:0')
c= tensor(2.3922e+09, device='cuda:0')
c= tensor(2.3922e+09, device='cuda:0')
c= tensor(2.3923e+09, device='cuda:0')
c= tensor(2.3931e+09, device='cuda:0')
c= tensor(2.8689e+09, device='cuda:0')
c= tensor(2.8801e+09, device='cuda:0')
c= tensor(2.8802e+09, device='cuda:0')
c= tensor(2.8803e+09, device='cuda:0')
c= tensor(2.8805e+09, device='cuda:0')
c= tensor(2.8805e+09, device='cuda:0')
c= tensor(2.8816e+09, device='cuda:0')
c= tensor(2.8817e+09, device='cuda:0')
c= tensor(2.9819e+09, device='cuda:0')
c= tensor(2.9819e+09, device='cuda:0')
c= tensor(2.9823e+09, device='cuda:0')
c= tensor(2.9824e+09, device='cuda:0')
c= tensor(2.9830e+09, device='cuda:0')
c= tensor(3.0098e+09, device='cuda:0')
c= tensor(3.0099e+09, device='cuda:0')
c= tensor(3.0099e+09, device='cuda:0')
c= tensor(3.0104e+09, device='cuda:0')
c= tensor(3.0104e+09, device='cuda:0')
c= tensor(3.0104e+09, device='cuda:0')
c= tensor(3.0116e+09, device='cuda:0')
c= tensor(3.1592e+09, device='cuda:0')
c= tensor(3.1593e+09, device='cuda:0')
c= tensor(3.1596e+09, device='cuda:0')
c= tensor(3.1615e+09, device='cuda:0')
c= tensor(3.1617e+09, device='cuda:0')
c= tensor(3.1618e+09, device='cuda:0')
c= tensor(3.1619e+09, device='cuda:0')
c= tensor(3.1770e+09, device='cuda:0')
c= tensor(3.1777e+09, device='cuda:0')
c= tensor(3.1802e+09, device='cuda:0')
c= tensor(3.1802e+09, device='cuda:0')
c= tensor(3.1803e+09, device='cuda:0')
c= tensor(3.1803e+09, device='cuda:0')
c= tensor(3.2472e+09, device='cuda:0')
c= tensor(3.2488e+09, device='cuda:0')
c= tensor(3.2488e+09, device='cuda:0')
c= tensor(3.2488e+09, device='cuda:0')
c= tensor(3.2490e+09, device='cuda:0')
c= tensor(3.2499e+09, device='cuda:0')
c= tensor(3.2509e+09, device='cuda:0')
c= tensor(3.2685e+09, device='cuda:0')
c= tensor(3.2685e+09, device='cuda:0')
c= tensor(3.2687e+09, device='cuda:0')
c= tensor(3.2690e+09, device='cuda:0')
c= tensor(3.2695e+09, device='cuda:0')
c= tensor(3.2700e+09, device='cuda:0')
c= tensor(3.2713e+09, device='cuda:0')
c= tensor(3.2717e+09, device='cuda:0')
c= tensor(3.2721e+09, device='cuda:0')
c= tensor(3.2721e+09, device='cuda:0')
c= tensor(3.2721e+09, device='cuda:0')
c= tensor(3.2749e+09, device='cuda:0')
c= tensor(3.2757e+09, device='cuda:0')
c= tensor(3.2762e+09, device='cuda:0')
c= tensor(3.2762e+09, device='cuda:0')
c= tensor(3.2762e+09, device='cuda:0')
c= tensor(3.2785e+09, device='cuda:0')
c= tensor(3.2790e+09, device='cuda:0')
c= tensor(3.2792e+09, device='cuda:0')
c= tensor(3.2792e+09, device='cuda:0')
c= tensor(3.2792e+09, device='cuda:0')
c= tensor(3.2793e+09, device='cuda:0')
c= tensor(3.2793e+09, device='cuda:0')
c= tensor(3.2839e+09, device='cuda:0')
c= tensor(3.2840e+09, device='cuda:0')
c= tensor(3.2841e+09, device='cuda:0')
c= tensor(3.2841e+09, device='cuda:0')
c= tensor(3.2841e+09, device='cuda:0')
c= tensor(3.2947e+09, device='cuda:0')
c= tensor(3.2948e+09, device='cuda:0')
c= tensor(3.2951e+09, device='cuda:0')
c= tensor(3.2977e+09, device='cuda:0')
c= tensor(3.2979e+09, device='cuda:0')
c= tensor(3.2995e+09, device='cuda:0')
c= tensor(3.3034e+09, device='cuda:0')
c= tensor(3.3034e+09, device='cuda:0')
c= tensor(3.3054e+09, device='cuda:0')
c= tensor(3.3056e+09, device='cuda:0')
c= tensor(3.3288e+09, device='cuda:0')
c= tensor(3.3352e+09, device='cuda:0')
c= tensor(3.3353e+09, device='cuda:0')
c= tensor(3.3361e+09, device='cuda:0')
c= tensor(3.3361e+09, device='cuda:0')
c= tensor(3.3362e+09, device='cuda:0')
c= tensor(3.3362e+09, device='cuda:0')
c= tensor(3.3362e+09, device='cuda:0')
c= tensor(3.3389e+09, device='cuda:0')
c= tensor(3.3396e+09, device='cuda:0')
c= tensor(3.3396e+09, device='cuda:0')
c= tensor(3.3398e+09, device='cuda:0')
c= tensor(3.3399e+09, device='cuda:0')
c= tensor(3.3573e+09, device='cuda:0')
c= tensor(3.3573e+09, device='cuda:0')
c= tensor(3.3574e+09, device='cuda:0')
c= tensor(3.3584e+09, device='cuda:0')
c= tensor(3.3600e+09, device='cuda:0')
c= tensor(3.3600e+09, device='cuda:0')
c= tensor(3.3617e+09, device='cuda:0')
c= tensor(3.3617e+09, device='cuda:0')
time to make c is 12.059800148010254
time for making loss is 12.05982494354248
p0 True
it  0 : 1023832576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
4255961088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  6% |
memory (bytes)
4256370688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  6% |
error is  319226880.0
relative error loss 0.09495983
shape of L is 
torch.Size([])
memory (bytes)
4288684032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  6% |
memory (bytes)
4288684032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  6% |
error is  310545660.0
relative error loss 0.09237745
shape of L is 
torch.Size([])
memory (bytes)
4292259840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  6% |
memory (bytes)
4292308992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  277551600.0
relative error loss 0.08256277
shape of L is 
torch.Size([])
memory (bytes)
4295507968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4295507968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  269290000.0
relative error loss 0.08010519
shape of L is 
torch.Size([])
memory (bytes)
4298678272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4298715136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  262747400.0
relative error loss 0.07815898
shape of L is 
torch.Size([])
memory (bytes)
4301893632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4301930496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  256459520.0
relative error loss 0.076288536
shape of L is 
torch.Size([])
memory (bytes)
4305129472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4305141760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  252495360.0
relative error loss 0.075109325
shape of L is 
torch.Size([])
memory (bytes)
4308344832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4308344832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  249914880.0
relative error loss 0.074341714
shape of L is 
torch.Size([])
memory (bytes)
4311412736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4311412736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  246857730.0
relative error loss 0.07343231
shape of L is 
torch.Size([])
memory (bytes)
4314734592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4314759168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  245542140.0
relative error loss 0.07304097
time to take a step is 462.6052939891815
it  1 : 1362966528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4317954048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4317982720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  245542140.0
relative error loss 0.07304097
shape of L is 
torch.Size([])
memory (bytes)
4321165312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4321185792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  244034050.0
relative error loss 0.072592355
shape of L is 
torch.Size([])
memory (bytes)
4324384768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4324429824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  242763260.0
relative error loss 0.072214335
shape of L is 
torch.Size([])
memory (bytes)
4327604224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4327636992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  241712900.0
relative error loss 0.07190189
shape of L is 
torch.Size([])
memory (bytes)
4330856448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4330856448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  240467970.0
relative error loss 0.071531564
shape of L is 
torch.Size([])
memory (bytes)
4334080000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4334080000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  239749890.0
relative error loss 0.071317956
shape of L is 
torch.Size([])
memory (bytes)
4337246208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4337246208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  239042050.0
relative error loss 0.071107395
shape of L is 
torch.Size([])
memory (bytes)
4340514816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4340539392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  238507780.0
relative error loss 0.07094847
shape of L is 
torch.Size([])
memory (bytes)
4343726080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4343754752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  237814020.0
relative error loss 0.07074209
shape of L is 
torch.Size([])
memory (bytes)
4346867712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4346970112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  237555200.0
relative error loss 0.070665106
time to take a step is 455.22930431365967
it  2 : 1362966528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4350189568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4350189568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  237555200.0
relative error loss 0.070665106
shape of L is 
torch.Size([])
memory (bytes)
4353404928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4353404928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  236920320.0
relative error loss 0.07047625
shape of L is 
torch.Size([])
memory (bytes)
4356624384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4356628480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  236686600.0
relative error loss 0.07040672
shape of L is 
torch.Size([])
memory (bytes)
4359782400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4359782400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  236477440.0
relative error loss 0.07034451
shape of L is 
torch.Size([])
memory (bytes)
4363055104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4363083776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  236103940.0
relative error loss 0.0702334
shape of L is 
torch.Size([])
memory (bytes)
4366274560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4366303232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  235834370.0
relative error loss 0.070153214
shape of L is 
torch.Size([])
memory (bytes)
4369494016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4369518592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  235664900.0
relative error loss 0.0701028
shape of L is 
torch.Size([])
memory (bytes)
4372611072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4372733952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  235369730.0
relative error loss 0.070015
shape of L is 
torch.Size([])
memory (bytes)
4375920640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4375953408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  235086080.0
relative error loss 0.06993062
shape of L is 
torch.Size([])
memory (bytes)
4379152384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4379168768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  234821380.0
relative error loss 0.06985188
time to take a step is 455.26211404800415
it  3 : 1363169280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4382330880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4382388224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  234821380.0
relative error loss 0.06985188
shape of L is 
torch.Size([])
memory (bytes)
4385595392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4385599488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  234670080.0
relative error loss 0.069806874
shape of L is 
torch.Size([])
memory (bytes)
4388790272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4388818944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  234498300.0
relative error loss 0.06975578
shape of L is 
torch.Size([])
memory (bytes)
4392034304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4392038400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  234230270.0
relative error loss 0.06967605
shape of L is 
torch.Size([])
memory (bytes)
4395212800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4395257856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  234103040.0
relative error loss 0.0696382
shape of L is 
torch.Size([])
memory (bytes)
4398452736
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4398477312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  233960200.0
relative error loss 0.0695957
shape of L is 
torch.Size([])
memory (bytes)
4401655808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4401692672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  233848580.0
relative error loss 0.0695625
shape of L is 
torch.Size([])
memory (bytes)
4404912128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4404916224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  233706750.0
relative error loss 0.06952032
shape of L is 
torch.Size([])
memory (bytes)
4408025088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4408127488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  233561090.0
relative error loss 0.069476984
shape of L is 
torch.Size([])
memory (bytes)
4411355136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4411359232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  233366780.0
relative error loss 0.06941918
time to take a step is 457.2285850048065
it  4 : 1362966528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4414574592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4414574592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  233366780.0
relative error loss 0.06941918
shape of L is 
torch.Size([])
memory (bytes)
4417785856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4417785856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  233275140.0
relative error loss 0.06939192
shape of L is 
torch.Size([])
memory (bytes)
4420976640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4420976640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  233187330.0
relative error loss 0.0693658
shape of L is 
torch.Size([])
memory (bytes)
4424208384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4424237056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  233132290.0
relative error loss 0.06934943
shape of L is 
torch.Size([])
memory (bytes)
4427436032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4427464704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  233017340.0
relative error loss 0.06931524
shape of L is 
torch.Size([])
memory (bytes)
4430655488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4430680064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  232961540.0
relative error loss 0.06929864
shape of L is 
torch.Size([])
memory (bytes)
4433854464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4433854464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232909570.0
relative error loss 0.06928318
shape of L is 
torch.Size([])
memory (bytes)
4437098496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4437127168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232822780.0
relative error loss 0.06925736
shape of L is 
torch.Size([])
memory (bytes)
4440330240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4440330240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232738300.0
relative error loss 0.06923223
shape of L is 
torch.Size([])
memory (bytes)
4443561984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4443561984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232684540.0
relative error loss 0.069216244
time to take a step is 460.8342227935791
it  5 : 1363067904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4446707712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4446707712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  232684540.0
relative error loss 0.069216244
shape of L is 
torch.Size([])
memory (bytes)
4449947648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4449976320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232618750.0
relative error loss 0.06919667
shape of L is 
torch.Size([])
memory (bytes)
4453179392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4453203968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  232527870.0
relative error loss 0.06916963
shape of L is 
torch.Size([])
memory (bytes)
4456378368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4456378368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  232470020.0
relative error loss 0.06915242
shape of L is 
torch.Size([])
memory (bytes)
4459622400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4459646976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  232405500.0
relative error loss 0.06913324
shape of L is 
torch.Size([])
memory (bytes)
4462866432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4462870528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232361470.0
relative error loss 0.06912014
shape of L is 
torch.Size([])
memory (bytes)
4466085888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4466089984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232309000.0
relative error loss 0.06910452
shape of L is 
torch.Size([])
memory (bytes)
4469284864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4469284864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232313860.0
relative error loss 0.069105975
shape of L is 
torch.Size([])
memory (bytes)
4472520704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4472524800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232265220.0
relative error loss 0.06909151
shape of L is 
torch.Size([])
memory (bytes)
4475744256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4475744256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232206600.0
relative error loss 0.069074064
time to take a step is 459.23713088035583
it  6 : 1363169280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4478840832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4478840832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232206600.0
relative error loss 0.069074064
shape of L is 
torch.Size([])
memory (bytes)
4482138112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4482166784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  232158200.0
relative error loss 0.06905967
shape of L is 
torch.Size([])
memory (bytes)
4485369856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  7% |
memory (bytes)
4485394432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232115460.0
relative error loss 0.06904695
shape of L is 
torch.Size([])
memory (bytes)
4488548352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4488548352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232074500.0
relative error loss 0.06903477
shape of L is 
torch.Size([])
memory (bytes)
4491812864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4491853824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  7% |
error is  232043260.0
relative error loss 0.06902548
shape of L is 
torch.Size([])
memory (bytes)
4495065088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4495065088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  232008960.0
relative error loss 0.06901528
shape of L is 
torch.Size([])
memory (bytes)
4498227200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4498227200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  231976200.0
relative error loss 0.06900553
shape of L is 
torch.Size([])
memory (bytes)
4501499904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4501499904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231949570.0
relative error loss 0.06899761
shape of L is 
torch.Size([])
memory (bytes)
4504686592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4504711168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231925000.0
relative error loss 0.0689903
shape of L is 
torch.Size([])
memory (bytes)
4507832320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4507832320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231905280.0
relative error loss 0.068984434
time to take a step is 458.6160509586334
it  7 : 1363067904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4511113216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4511137792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  7% |
error is  231905280.0
relative error loss 0.068984434
shape of L is 
torch.Size([])
memory (bytes)
4514353152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4514353152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231887360.0
relative error loss 0.06897911
shape of L is 
torch.Size([])
memory (bytes)
4517543936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4517543936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  231854600.0
relative error loss 0.068969354
shape of L is 
torch.Size([])
memory (bytes)
4520771584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4520804352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231844600.0
relative error loss 0.06896639
shape of L is 
torch.Size([])
memory (bytes)
4523999232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4524023808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  231826940.0
relative error loss 0.06896113
shape of L is 
torch.Size([])
memory (bytes)
4527149056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4527149056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  231801600.0
relative error loss 0.068953596
shape of L is 
torch.Size([])
memory (bytes)
4530475008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4530479104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231793400.0
relative error loss 0.06895116
shape of L is 
torch.Size([])
memory (bytes)
4533657600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4533682176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231770620.0
relative error loss 0.06894438
shape of L is 
torch.Size([])
memory (bytes)
4536774656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4536913920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231761150.0
relative error loss 0.06894156
shape of L is 
torch.Size([])
memory (bytes)
4540133376
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4540133376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231734780.0
relative error loss 0.06893372
time to take a step is 459.01438212394714
it  8 : 1362966528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4543332352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4543356928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231734780.0
relative error loss 0.06893372
shape of L is 
torch.Size([])
memory (bytes)
4546564096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4546564096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231703550.0
relative error loss 0.06892443
shape of L is 
torch.Size([])
memory (bytes)
4549758976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4549795840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231697400.0
relative error loss 0.0689226
shape of L is 
torch.Size([])
memory (bytes)
4552994816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4553019392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231650560.0
relative error loss 0.06890866
shape of L is 
torch.Size([])
memory (bytes)
4556206080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4556230656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  231632900.0
relative error loss 0.06890341
shape of L is 
torch.Size([])
memory (bytes)
4559417344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4559458304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231590660.0
relative error loss 0.06889085
shape of L is 
torch.Size([])
memory (bytes)
4562640896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4562669568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231564030.0
relative error loss 0.06888293
shape of L is 
torch.Size([])
memory (bytes)
4565884928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4565884928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231537920.0
relative error loss 0.06887516
shape of L is 
torch.Size([])
memory (bytes)
4569030656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4569120768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  7% |
error is  231524860.0
relative error loss 0.068871275
shape of L is 
torch.Size([])
memory (bytes)
4572303360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4572332032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231509760.0
relative error loss 0.06886678
time to take a step is 460.1964671611786
it  9 : 1363067904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4575526912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4575535104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231509760.0
relative error loss 0.06886678
shape of L is 
torch.Size([])
memory (bytes)
4578656256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4578766848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231488000.0
relative error loss 0.06886031
shape of L is 
torch.Size([])
memory (bytes)
4581965824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4581994496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231462400.0
relative error loss 0.06885269
shape of L is 
torch.Size([])
memory (bytes)
4585181184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4585209856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231443460.0
relative error loss 0.06884706
shape of L is 
torch.Size([])
memory (bytes)
4588429312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4588429312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231427330.0
relative error loss 0.06884226
shape of L is 
torch.Size([])
memory (bytes)
4591607808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4591607808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231410180.0
relative error loss 0.06883716
shape of L is 
torch.Size([])
memory (bytes)
4594847744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4594876416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231394300.0
relative error loss 0.068832435
shape of L is 
torch.Size([])
memory (bytes)
4598095872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4598095872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231381000.0
relative error loss 0.06882848
shape of L is 
torch.Size([])
memory (bytes)
4601307136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4601307136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231367420.0
relative error loss 0.06882444
shape of L is 
torch.Size([])
memory (bytes)
4604506112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4604534784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231360770.0
relative error loss 0.06882246
time to take a step is 460.82840967178345
it  10 : 1362865152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4607725568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4607754240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  231360770.0
relative error loss 0.06882246
shape of L is 
torch.Size([])
memory (bytes)
4610949120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4610969600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  231346940.0
relative error loss 0.068818346
shape of L is 
torch.Size([])
memory (bytes)
4614062080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4614201344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  7% |
error is  231342340.0
relative error loss 0.068816975
shape of L is 
torch.Size([])
memory (bytes)
4617388032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4617416704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231334140.0
relative error loss 0.06881454
shape of L is 
torch.Size([])
memory (bytes)
4620640256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4620640256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231331580.0
relative error loss 0.06881378
shape of L is 
torch.Size([])
memory (bytes)
4623847424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4623872000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231317760.0
relative error loss 0.068809666
shape of L is 
torch.Size([])
memory (bytes)
4627058688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4627091456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231307000.0
relative error loss 0.06880647
shape of L is 
torch.Size([])
memory (bytes)
4630274048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4630302720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231292670.0
relative error loss 0.0688022
shape of L is 
torch.Size([])
memory (bytes)
4633497600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4633522176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231277060.0
relative error loss 0.06879756
shape of L is 
torch.Size([])
memory (bytes)
4636667904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4636749824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231305470.0
relative error loss 0.06880601
shape of L is 
torch.Size([])
memory (bytes)
4639940608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4639965184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231266050.0
relative error loss 0.06879428
time to take a step is 506.5772271156311
it  11 : 1363169792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4643180544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4643180544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231266050.0
relative error loss 0.06879428
shape of L is 
torch.Size([])
memory (bytes)
4646400000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4646400000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231249660.0
relative error loss 0.06878941
shape of L is 
torch.Size([])
memory (bytes)
4649582592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4649582592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231240700.0
relative error loss 0.06878675
shape of L is 
torch.Size([])
memory (bytes)
4652818432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4652843008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  231226370.0
relative error loss 0.06878248
shape of L is 
torch.Size([])
memory (bytes)
4656037888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4656062464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231220740.0
relative error loss 0.0687808
shape of L is 
torch.Size([])
memory (bytes)
4659281920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4659281920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  231213310.0
relative error loss 0.0687786
shape of L is 
torch.Size([])
memory (bytes)
4662476800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4662505472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  231203580.0
relative error loss 0.0687757
shape of L is 
torch.Size([])
memory (bytes)
4665729024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4665729024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  231194620.0
relative error loss 0.06877304
shape of L is 
torch.Size([])
memory (bytes)
4668936192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4668940288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231189000.0
relative error loss 0.06877136
shape of L is 
torch.Size([])
memory (bytes)
4672061440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4672151552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231171330.0
relative error loss 0.06876611
time to take a step is 461.3808329105377
it  12 : 1363169280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4675379200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4675383296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231171330.0
relative error loss 0.06876611
shape of L is 
torch.Size([])
memory (bytes)
4678602752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4678602752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  231165180.0
relative error loss 0.06876428
shape of L is 
torch.Size([])
memory (bytes)
4681801728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4681830400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  231157500.0
relative error loss 0.068762
shape of L is 
torch.Size([])
memory (bytes)
4685029376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4685041664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231146240.0
relative error loss 0.068758644
shape of L is 
torch.Size([])
memory (bytes)
4688240640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4688265216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231131650.0
relative error loss 0.0687543
shape of L is 
torch.Size([])
memory (bytes)
4691480576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4691480576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231118850.0
relative error loss 0.06875049
shape of L is 
torch.Size([])
memory (bytes)
4694700032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4694700032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231106820.0
relative error loss 0.06874692
shape of L is 
torch.Size([])
memory (bytes)
4697894912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4697919488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231094530.0
relative error loss 0.06874326
shape of L is 
torch.Size([])
memory (bytes)
4701118464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4701147136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231084540.0
relative error loss 0.06874029
shape of L is 
torch.Size([])
memory (bytes)
4704370688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4704370688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231078140.0
relative error loss 0.068738386
time to take a step is 461.61278009414673
it  13 : 1363067904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4707475456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4707581952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231078140.0
relative error loss 0.068738386
shape of L is 
torch.Size([])
memory (bytes)
4710793216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4710793216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231072260.0
relative error loss 0.068736635
shape of L is 
torch.Size([])
memory (bytes)
4714041344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4714041344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231063800.0
relative error loss 0.068734124
shape of L is 
torch.Size([])
memory (bytes)
4717252608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4717252608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231059710.0
relative error loss 0.0687329
shape of L is 
torch.Size([])
memory (bytes)
4720443392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4720472064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231055620.0
relative error loss 0.06873169
shape of L is 
torch.Size([])
memory (bytes)
4723662848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4723691520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  231053570.0
relative error loss 0.06873108
shape of L is 
torch.Size([])
memory (bytes)
4726906880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4726910976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231050240.0
relative error loss 0.068730086
shape of L is 
torch.Size([])
memory (bytes)
4730007552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4730126336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231043580.0
relative error loss 0.068728104
shape of L is 
torch.Size([])
memory (bytes)
4733321216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4733345792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231039740.0
relative error loss 0.068726964
shape of L is 
torch.Size([])
memory (bytes)
4736552960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4736573440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231035900.0
relative error loss 0.068725824
time to take a step is 459.95738697052
it  14 : 1363067904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4739620864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4739801088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231035900.0
relative error loss 0.068725824
shape of L is 
torch.Size([])
memory (bytes)
4743012352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4743016448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231030020.0
relative error loss 0.06872407
shape of L is 
torch.Size([])
memory (bytes)
4746235904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4746235904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  231020800.0
relative error loss 0.06872133
shape of L is 
torch.Size([])
memory (bytes)
4749381632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4749381632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231027200.0
relative error loss 0.06872323
shape of L is 
torch.Size([])
memory (bytes)
4752666624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4752670720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  231017220.0
relative error loss 0.068720266
shape of L is 
torch.Size([])
memory (bytes)
4755869696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4755894272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  231012100.0
relative error loss 0.06871874
shape of L is 
torch.Size([])
memory (bytes)
4759085056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4759085056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  231003900.0
relative error loss 0.0687163
shape of L is 
torch.Size([])
memory (bytes)
4762308608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4762337280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  230998780.0
relative error loss 0.06871478
shape of L is 
torch.Size([])
memory (bytes)
4765548544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4765548544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  230990600.0
relative error loss 0.068712346
shape of L is 
torch.Size([])
memory (bytes)
4768722944
| ID | GPU | MEM |
------------------
|  0 |  6% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4768763904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  230985730.0
relative error loss 0.06871089
time to take a step is 461.8143754005432
sum tnnu_Z after tensor(11416366., device='cuda:0')
shape of features
(3338,)
shape of features
(3338,)
number of orig particles 13350
number of new particles after remove low mass 11456
tnuZ shape should be parts x labs
torch.Size([13350, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  319218600.0
relative error without small mass is  0.09495737
nnu_Z shape should be number of particles by maxV
(13350, 702)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
shape of features
(13350,)
Fri Feb 3 00:57:21 EST 2023
