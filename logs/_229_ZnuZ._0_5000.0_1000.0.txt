Wed Feb 1 19:37:59 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 36163561
numbers of Z: 24655
shape of features
(24655,)
shape of features
(24655,)
ZX	Vol	Parts	Cubes	Eps
Z	0.019996767062310003	24655	24.655	0.09325774866085716
X	0.018448774888331704	924	0.924	0.2712887930885137
X	0.01966175029997527	20909	20.909	0.09797072108943324
X	0.01975459063694303	2632	2.632	0.19579159747964242
X	0.018738622724131813	3149	3.149	0.18121257663563314
X	0.018908043734664478	30641	30.641	0.08513625736479645
X	0.018401080935237495	30303	30.303	0.08468098542742596
X	0.01885612782590026	56121	56.121	0.06951988380265237
X	0.01861592834033919	44397	44.397	0.07484745270856154
X	0.018516827866624477	8103	8.103	0.13171652066255393
X	0.018396223872190826	19466	19.466	0.0981335076384456
X	0.01851689220980626	10436	10.436	0.12106297517443655
X	0.01819428347396933	116155	116.155	0.0539054094608066
X	0.01924359702871701	6384	6.384	0.14445451656940606
X	0.018709750101177886	213471	213.471	0.044419776335058725
X	0.018166114950853306	29415	29.415	0.0851591010681083
X	0.019023824521003138	38883	38.883	0.07879754531537611
X	0.018606860039611806	55751	55.751	0.06936496046249561
X	0.018976855668945122	47908	47.908	0.0734410264023399
X	0.018680485032437123	158995	158.995	0.04897805791627518
X	0.019061827149168934	100895	100.895	0.05738058357360845
X	0.01790677629734791	9299	9.299	0.12441146004269404
X	0.019483038589833315	260389	260.389	0.04213839782905612
X	0.01851988471518285	15821	15.821	0.10539049624671697
X	0.019302289645594668	17607	17.607	0.10311167013745905
X	0.017935462594328024	30318	30.318	0.0839467740127218
X	0.01871367668974434	64270	64.27	0.06628016904130603
X	0.019410547258159768	57170	57.17	0.06976268640049506
X	0.01939161101746672	9350	9.35	0.12752659381760803
X	0.01842159667556813	58917	58.917	0.06787276600123776
X	0.019850151272875993	1365294	1365.294	0.02440689576298322
X	0.01815612114443323	7977	7.977	0.13154100630266352
X	0.019946674405459126	465835	465.835	0.03498480527602228
X	0.018544111207509026	14678	14.678	0.10810514904954555
X	0.018658358661445752	8124	8.124	0.1319373754820853
X	0.01866456919115364	6644	6.644	0.14110079113255983
X	0.01919481453182981	97943	97.943	0.05808582640738292
X	0.019878332628479645	60794	60.794	0.06889267976200172
X	0.018697110845679522	1519	1.519	0.2308902405991277
X	0.01843052413575881	4586	4.586	0.15898929316355054
X	0.019322583711940865	2236	2.236	0.2052096593420034
X	0.018512403474471983	3656	3.656	0.1717192066445873
X	0.017240682103810585	2173	2.173	0.19944886654714222
X	0.017574310275653017	2331	2.331	0.1960854362331702
X	0.018364091050884906	4623	4.623	0.15837325690938694
X	0.017928089785238306	1078	1.078	0.25525352748837155
X	0.018669303516308435	1964	1.964	0.21183394016897616
X	0.018466090367137904	4735	4.735	0.15740492776844112
X	0.018409122658105674	4272	4.272	0.16272987185958052
X	0.018267885738406316	1891	1.891	0.2129767779982745
X	0.01943871566333809	9315	9.315	0.12778942107844726
X	0.01993399090079733	12555	12.555	0.11666104066849713
X	0.01843321627341291	2140	2.14	0.20498831949479437
X	0.01839173953643895	4141	4.141	0.1643762969337148
X	0.017986116125698396	2897	2.897	0.18379328182432741
X	0.0197513637470868	6024	6.024	0.1485607356601458
X	0.018528848276561493	3545	3.545	0.17354444966428748
X	0.018690415849057635	2221	2.221	0.2034027413924422
X	0.018624461394347838	3903	3.903	0.16835591919846066
X	0.018345732881469658	3499	3.499	0.17372555498954825
X	0.018205398896986358	2319	2.319	0.19874664988976282
X	0.019673286532704837	5182	5.182	0.15600068411521742
X	0.018485296174122468	1791	1.791	0.217725990927711
X	0.019934942825091503	8860	8.86	0.1310369444359756
X	0.018871198398000694	8610	8.61	0.1298968968978367
X	0.01822760648801716	2484	2.484	0.19432381030635498
X	0.018640884471703686	2574	2.574	0.19347253589559602
X	0.018625178740678785	2661	2.661	0.1912868834974364
X	0.01819108692981527	4759	4.759	0.1563559147859403
X	0.01938018211461678	4135	4.135	0.16735066661071793
X	0.018288322622079255	2088	2.088	0.20613312790200253
X	0.018378032547429788	3383	3.383	0.17579195912055992
X	0.019046761364404575	4284	4.284	0.1644335978884734
X	0.01857002207913778	3701	3.701	0.17119764189427777
X	0.018495330881097116	1918	1.918	0.21284878108992972
X	0.018658087242197967	3076	3.076	0.1823728826494823
X	0.018552546511159104	5706	5.706	0.1481462242466218
X	0.018379430410878993	1178	1.178	0.2498785166910075
X	0.018288484417132995	936	0.936	0.2693398553963943
X	0.018620755477527524	3419	3.419	0.17594059868398407
X	0.019518116185355927	13785	13.785	0.11229067940017168
X	0.019210218404454468	3391	3.391	0.17826585681302606
X	0.016607680000787793	1527	1.527	0.22155929627158266
X	0.01973914893301128	5285	5.285	0.15515334533928168
X	0.018097275612276517	2087	2.087	0.2054456343821434
X	0.018223842427113467	1883	1.883	0.21310642514667316
X	0.01829504231565403	1718	1.718	0.2200071286124269
X	0.017918427877892457	2944	2.944	0.18258034881070795
X	0.018058473141971367	1366	1.366	0.236452245523263
X	0.018571931554997884	3654	3.654	0.17193442628463912
X	0.01863375642356107	2911	2.911	0.18567470229292848
X	0.018777942110187434	3229	3.229	0.17982910532612498
X	0.018842228981715742	1599	1.599	0.22755930804771904
X	0.019132351863811466	1636	1.636	0.22698388238915645
X	0.01932275374833589	4078	4.078	0.16796054142268835
X	0.018103606513587153	3804	3.804	0.16820601852693595
X	0.01840621100677707	4772	4.772	0.1568271753949094
X	0.0194177753574633	2805	2.805	0.190585066911868
X	0.018567551960679454	5637	5.637	0.14878833060431979
X	0.01888916997819536	4053	4.053	0.16703677265340788
X	0.01883917477933065	3135	3.135	0.18180595739978245
X	0.019187668736833648	9591	9.591	0.1260045149874352
X	0.01851648286067039	3258	3.258	0.17845791138896866
X	0.019217903334333583	6624	6.624	0.1426248586217304
X	0.017681469475601765	1677	1.677	0.2192780183047733
X	0.018686520016058594	6507	6.507	0.14213985712183352
X	0.018422858082376085	2208	2.208	0.20282398556697823
X	0.019817804224727682	5257	5.257	0.15563449193075515
X	0.017932317336225263	946	0.946	0.2666337330272583
X	0.01893555370498937	3056	3.056	0.18367140707509
X	0.01865975607737258	1693	1.693	0.22254422800390342
X	0.01856583650077727	2523	2.523	0.19450574066730675
X	0.018206699817017342	1224	1.224	0.24593288512412925
X	0.01828209355733342	1910	1.91	0.21232323371687722
X	0.018391978895998973	3912	3.912	0.1675238085786521
X	0.01882453736162711	4719	4.719	0.15859571337791212
X	0.01857011722279711	1658	1.658	0.223739865668174
X	0.01844640099220097	2344	2.344	0.19890758565723768
X	0.018684219209016077	1039	1.039	0.2619910580559737
X	0.019621071559136352	6731	6.731	0.14285026280640728
X	0.018547550695733418	1904	1.904	0.21356981637353528
X	0.019560426837104645	10771	10.771	0.12200400868908491
X	0.018330854509625655	3141	3.141	0.1800410725009076
X	0.018597860923409214	2540	2.54	0.19418235315842067
X	0.018686163180695468	2871	2.871	0.18670773690208908
X	0.019178900267581945	2043	2.043	0.21095271120114628
X	0.018125433986628726	1644	1.644	0.22256773196877058
X	0.018273902153813783	1910	1.91	0.21229151807300553
X	0.018612148855628913	3187	3.187	0.18008248899381965
X	0.019537265997409972	12981	12.981	0.1146001571467876
X	0.01851351363554305	2303	2.303	0.20032337913309123
X	0.018490213908696047	6346	6.346	0.14282819765852406
X	0.018687161631978515	1726	1.726	0.2212249896894253
X	0.0186718410505544	4351	4.351	0.16250473486981146
X	0.017047518892782283	1193	1.193	0.24266487174225948
X	0.018732160787197555	2875	2.875	0.18677411014197096
X	0.01847632366868029	2205	2.205	0.20311203430585387
X	0.01912055306316468	1565	1.565	0.23031843599148077
X	0.018455487403785886	1761	1.761	0.21883765604566188
X	0.018108797270114312	2349	2.349	0.19754628339064928
X	0.01795288717805217	1997	1.997	0.20793076222499124
X	0.01832045510810599	4065	4.065	0.1651804212590765
X	0.018103302163770534	1171	1.171	0.2491152427428301
X	0.01923169117042657	10178	10.178	0.12362842693590796
X	0.01862052389049596	7264	7.264	0.1368585770031473
X	0.019905164698236163	6220	6.22	0.14736413420961483
X	0.0184577047884143	2275	2.275	0.20093955287425652
X	0.019296440265364152	2448	2.448	0.19901627593567256
X	0.019319397890762636	4379	4.379	0.16401096376081514
X	0.01862001110677916	2092	2.092	0.207239603085203
X	0.018561482163209186	2435	2.435	0.19680580107226733
X	0.018317854626059442	2100	2.1	0.20585042648124763
X	0.019439042822188494	3244	3.244	0.18163436097081653
X	0.018841750549379803	3846	3.846	0.16983896058680584
X	0.018480453334177332	8174	8.174	0.1312479903761911
X	0.0184975589356312	2366	2.366	0.19847229569912367
X	0.01866143536781051	12253	12.253	0.11505377439949849
X	0.01852400341033206	2896	2.896	0.1856288318031063
X	0.018837947654059425	2773	2.773	0.18939182925377296
X	0.019624807087519264	3617	3.617	0.17571909688097032
X	0.01724873673983217	764	0.764	0.28263198698218794
X	0.01910426916053515	5786	5.786	0.1489077608044974
X	0.017660909764349617	1701	1.701	0.21815721762856038
X	0.018829212876840157	4973	4.973	0.1558611592511011
X	0.018119716298191948	1050	1.05	0.258416772575924
X	0.01854919934617711	3876	3.876	0.16851832485178614
X	0.018424783259962465	1820	1.82	0.21632680358377257
X	0.018685129209768347	2235	2.235	0.20295800843766937
X	0.01916822956744654	3433	3.433	0.17740644756183171
X	0.018028429285114795	4513	4.513	0.15867098547906686
X	0.01786858557528512	805	0.805	0.2810378182854323
X	0.018174919688358243	1663	1.663	0.22191845342023114
X	0.017788440914916617	1469	1.469	0.22963532491605282
X	0.019172128562939476	7047	7.047	0.13960105071150425
X	0.018633690569394436	2132	2.132	0.2059857733404271
X	0.019244317680498578	3850	3.85	0.1709807676791074
X	0.019512242274640653	3047	3.047	0.18569985951309786
X	0.019203202764004803	6177	6.177	0.14594818172233343
X	0.018157900134113898	2471	2.471	0.19441553253149998
X	0.018514760621499537	5144	5.144	0.15325184575240255
X	0.018251449685190064	3448	3.448	0.17427832187140627
X	0.019021502995996056	3739	3.739	0.17198733022400942
X	0.018228353845794416	2341	2.341	0.19820533754453565
X	0.018326789549200126	3280	3.28	0.1774478941222007
X	0.01772288137314842	3568	3.568	0.1706225761807469
X	0.018875548065027996	5484	5.484	0.15098508279711353
X	0.019549933766885245	5076	5.076	0.15675012373408742
X	0.019635969306859114	2223	2.223	0.20671450919616455
X	0.019290537106966855	3894	3.894	0.17047058737580414
X	0.019534622914563413	6853	6.853	0.14178863888703572
X	0.01972278230981492	12106	12.106	0.1176673132735555
X	0.01774755424123407	1990	1.99	0.2073775846618909
X	0.018657516650204046	577	0.577	0.31858550195334423
X	0.01862841530503664	2919	2.919	0.18548719720013188
X	0.01862657698239378	2071	2.071	0.20796215904369408
X	0.018379275939957015	2954	2.954	0.18392438070367587
X	0.018624683420632206	3865	3.865	0.16890654055702392
X	0.01777111851029255	2544	2.544	0.19116096697154564
X	0.01834853830324302	1328	1.328	0.23995757588706615
X	0.017951834154023134	2001	2.001	0.20778805581670148
X	0.018651977809745496	1294	1.294	0.2433680363341251
X	0.01948471119368326	4184	4.184	0.16699396086873333
X	0.01782411029654608	1755	1.755	0.2165593371069358
X	0.019756975283122816	7402	7.402	0.13871511680856272
X	0.018653522544349453	3575	3.575	0.17344492759821845
X	0.018557187296583476	5373	5.373	0.15115822035981655
X	0.018153766306807474	1306	1.306	0.2404406284947698
X	0.019348937314291863	4482	4.482	0.16282775003423003
X	0.01896251006405265	3524	3.524	0.1752346457090137
X	0.018721319439144398	3244	3.244	0.1793708513192006
X	0.018962206539251	7274	7.274	0.13762749080469344
X	0.01887895379039432	4248	4.248	0.16441110130373773
X	0.01847521203097354	4182	4.182	0.16408484366258466
X	0.018055045099566746	2652	2.652	0.1895287155122454
X	0.0183137328637128	2495	2.495	0.19434294295525487
X	0.01841001582909504	1039	1.039	0.2607031056579153
X	0.018213045995893372	1712	1.712	0.2199342878550027
X	0.01824367432712379	1040	1.04	0.25983221198637757
X	0.01849692491894512	785	0.785	0.28668829314713895
X	0.01856251880519256	6145	6.145	0.1445567380756321
X	0.01859197737957747	2743	2.743	0.18924885397169763
X	0.018507744341895695	2210	2.21	0.20307372693962758
X	0.01791333161665493	1496	1.496	0.22877835033484845
X	0.018516991760464067	4695	4.695	0.1579955829245945
X	0.017619665842713034	2377	2.377	0.1949797437738505
X	0.018238886901049585	4289	4.289	0.16201209128636335
X	0.019767352227616216	4225	4.225	0.16725300492651582
X	0.017927624915139675	1653	1.653	0.2213519239925937
X	0.018566475712083023	1486	1.486	0.23204393491202005
X	0.018429612707951862	4449	4.449	0.1606021177307848
X	0.018641656964024776	1015	1.015	0.26383930234315844
X	0.01788997067250324	1552	1.552	0.22589466309852607
X	0.018481568554594163	2226	2.226	0.20249045481899006
X	0.018956642695848937	3318	3.318	0.1787701539841351
X	0.019731250750245167	4217	4.217	0.16725675689834812
X	0.018438561725469368	2244	2.244	0.20179081589395892
X	0.018127200031086626	2038	2.038	0.2071932085378766
X	0.01870967010828144	3437	3.437	0.17591202687924679
X	0.01956556671368703	6086	6.086	0.14758883157589006
X	0.01836195589653926	1468	1.468	0.23212983433080275
X	0.0179129022985719	9256	9.256	0.12461802681259927
X	0.0184823563144336	67559	67.559	0.064916940265489
X	0.019118483083367967	4144	4.144	0.16647328958076787
X	0.018419626561507915	3942	3.942	0.16718144207351507
X	0.01817551617710858	1314	1.314	0.24004746799623397
X	0.018563374585075062	5247	5.247	0.15237555063358937
X	0.019292318188964944	10911	10.911	0.12092236154792392
X	0.018660360995860242	173530	173.53	0.04755342748144651
X	0.018396731080615583	3337	3.337	0.1766559017954416
X	0.0185292920116508	64729	64.729	0.06590524456743722
X	0.018740588424861898	89002	89.002	0.05949235384174824
X	0.018893895252897083	24102	24.102	0.09220535418082107
X	0.01966049297829659	121561	121.561	0.05448371507488227
X	0.019339687837269012	2219	2.219	0.20579305731186584
X	0.018987057312723363	4789	4.789	0.15827206720695347
X	0.019654510564410643	147120	147.12	0.051120705540902
X	0.018820480802022744	386483	386.483	0.03651742165032983
X	0.018811650520293172	4328	4.328	0.16319739291454236
X	0.018656461815605745	26189	26.189	0.08931069232568756
X	0.019125008027084233	43162	43.162	0.07623715632229268
X	0.01890375938274327	82942	82.942	0.061083590795020445
X	0.019236100134354887	34069	34.069	0.08265184405857927
X	0.01911144353946376	26852	26.852	0.08928372156025195
X	0.018690265127343602	24572	24.572	0.0912833672257228
X	0.018664193715394576	13983	13.983	0.11010396493673658
X	0.018544008235290253	8275	8.275	0.13086149264427058
X	0.019555303799223767	168318	168.318	0.04879530104249861
X	0.017777763526481232	3709	3.709	0.16860615732909723
X	0.01784948030677434	3909	3.909	0.1659026418623855
X	0.018526578245205684	36919	36.919	0.07946607484599844
X	0.018808534402786948	26667	26.667	0.08901439920684288
X	0.019934342396281626	258160	258.16	0.042583137193074154
X	0.01890315197292883	83936	83.936	0.06084085645723472
X	0.018326261431461505	1255	1.255	0.24442373936100437
X	0.01885381392115013	17420	17.42	0.1026716059790907
X	0.018965709735888545	8537	8.537	0.1304832021444854
X	0.01838855683157174	16043	16.043	0.10465356352361915
X	0.019000034617629905	52841	52.841	0.0711095267145459
X	0.019180570998084123	17014	17.014	0.10407626568715692
X	0.01868176265249622	37385	37.385	0.07935485243673372
X	0.017655850120113704	1031	1.031	0.2577567157005141
X	0.018762610913209618	5481	5.481	0.15071083990805884
X	0.018524998287284686	56605	56.605	0.06891299535435574
X	0.01866056157937167	51317	51.317	0.07137637429276793
X	0.019180225953731	31893	31.893	0.08440848693853109
X	0.018680893328493507	4938	4.938	0.15581724391734483
X	0.019452339446873274	240809	240.809	0.043228122657697494
X	0.018839843461732987	11283	11.283	0.11863629281922237
X	0.018328747138046633	20750	20.75	0.09594850808792725
X	0.01863936794410129	90287	90.287	0.0591019739598823
X	0.017770561815329346	2656	2.656	0.1884333181909392
X	0.018520511595148396	64061	64.061	0.06612308208914006
X	0.019905674737108785	225877	225.877	0.04450084908097098
X	0.019255404213102456	213535	213.535	0.04484298598560671
X	0.019564074127888756	15094	15.094	0.10903141574961052
X	0.018269841785827804	15850	15.85	0.10485001734114563
X	0.018239542869520377	9635	9.635	0.12370504459944337
X	0.018255695528865495	4583	4.583	0.15851955416759325
X	0.01929028484192121	51659	51.659	0.07201078554088433
X	0.019610331636881664	8716	8.716	0.131035581067345
X	0.018676725590926986	18002	18.002	0.10123406129976709
X	0.018888199161645042	34335	34.335	0.08193783800368674
X	0.018648618755093556	33950	33.95	0.08189718305264271
X	0.01866915396259866	11030	11.03	0.1191744185312146
X	0.01939931315102575	6910	6.91	0.14107046636792503
X	0.018631881930581233	92385	92.385	0.05864330256229678
X	0.018767064846572583	11125	11.125	0.11904160626960965
X	0.01867616091544508	12614	12.614	0.11397552294350055
X	0.019951810520179972	11686	11.686	0.11951951163382417
X	0.019105774599573133	329112	329.112	0.03872044240160728
X	0.019088192226703684	8252	8.252	0.13225188177206718
X	0.018865941116693735	121591	121.591	0.05373521795925759
X	0.018723176697580224	4804	4.804	0.1573712927444964
X	0.018133218630869533	27222	27.222	0.08733432260568871
X	0.01922829200637423	13109	13.109	0.11362059218127137
X	0.019328828518455566	72878	72.878	0.06424949164521426
X	0.018523389112457052	45644	45.644	0.07403644964956846
X	0.01965452389411818	4309	4.309	0.16584231239981115
X	0.019138147145939085	117519	117.519	0.054608963563716896
X	0.018697948246698744	88422	88.422	0.059576896103119455
X	0.018778263464543785	3916	3.916	0.16863106453519705
X	0.018511827421250487	79715	79.715	0.06146620699228846
X	0.019792471007048586	112934	112.934	0.055961812098254106
X	0.01982089327347961	258866	258.866	0.04246352762461287
X	0.019188295630693252	90531	90.531	0.05962289065253928
X	0.018489058652884402	3364	3.364	0.17647625173953024
X	0.018491711096113313	9036	9.036	0.12696001326840892
X	0.018539317460162238	7680	7.68	0.13414593197202496
X	0.01851571357930968	33495	33.495	0.08207045175029287
X	0.018677392424311698	7991	7.991	0.1327104242937936
X	0.019517508279510078	1976	1.976	0.21455854266052393
X	0.01928367121141601	49671	49.671	0.07295061175761385
X	0.018842779404162475	125950	125.95	0.05308627640517871
X	0.018601897273973544	44873	44.873	0.0745631153995085
X	0.01826845441004895	5278	5.278	0.15126698122655632
X	0.018653241693557532	22645	22.645	0.0937405140655291
X	0.019875248632903835	10679	10.679	0.12300629226494748
X	0.019673494462620546	8571	8.571	0.13191170611371109
X	0.019848214923808803	8760	8.76	0.13134265246195695
X	0.018625016013871867	4162	4.162	0.16479025520472326
X	0.018668674716750554	132891	132.891	0.05198435259551998
X	0.01856050014870261	33946	33.946	0.08177119715878013
X	0.01917272497437302	16706	16.706	0.10469769532747693
X	0.019473049744912256	21186	21.186	0.0972288170292219
X	0.01819744949653129	7178	7.178	0.13635435800077972
X	0.019337216337189978	395025	395.025	0.03658107451398542
X	0.018396517788750403	6911	6.911	0.13858989867717794
X	0.018844615849501676	89958	89.958	0.05939034285763041
X	0.018297427823272556	1448	1.448	0.23292020318063844
X	0.01870648746341912	4209	4.209	0.1644136132772652
X	0.019114730643888165	2844	2.844	0.18871779931282706
X	0.018964363124257685	7378	7.378	0.13698295708404604
X	0.01950774599361984	11094	11.094	0.12069973321939394
X	0.019178059153379824	29923	29.923	0.08621837642698627
X	0.018607620687305555	4507	4.507	0.1604234168551127
X	0.019323654318847402	3889	3.889	0.17064115114917203
X	0.019521745377241703	252524	252.524	0.042599581048671564
X	0.019013598469008477	32969	32.969	0.08323759327316813
X	0.01965768985069161	25701	25.701	0.09145219892849021
X	0.01959567218491452	69269	69.269	0.06564582074492832
X	0.019351870271624372	138917	138.917	0.05183890858978107
X	0.018641141557068225	3280	3.28	0.1784567119563301
X	0.01863419921275083	5304	5.304	0.15202058737898547
X	0.018524472466825835	16502	16.502	0.10392892188542599
X	0.018178507303629063	1616	1.616	0.2240641046182041
X	0.018593754669383542	5880	5.88	0.14677877432241512
X	0.01919625898872688	16311	16.311	0.10557927391679847
X	0.01794782329551268	5412	5.412	0.14912530069141253
X	0.018637358314437056	4582	4.582	0.15962825152840923
X	0.018695303961046288	4010	4.01	0.16705624653297108
X	0.01837256117278352	6703	6.703	0.13994804228596094
X	0.018981745180556342	409181	409.181	0.035931352083338745
X	0.018569241155932347	15898	15.898	0.10531346156807567
X	0.01966733331392852	63487	63.487	0.06766335961345654
X	0.019580539968732654	12573	12.573	0.11591204895681846
X	0.0190828439370419	4119	4.119	0.16670569465022436
X	0.018499485647849406	14931	14.931	0.10740477411036459
X	0.01981850176770223	305253	305.253	0.04019175400664817
X	0.01961298010411412	443858	443.858	0.03535356212479639
X	0.018679439363690497	27084	27.084	0.08835212730936141
X	0.01916332784609207	30695	30.695	0.08546751901790425
X	0.017815655942224312	2152	2.152	0.2022956371901226
X	0.018685414996995574	44898	44.898	0.07466067592586799
X	0.019263120931309743	217610	217.61	0.04456725995995898
X	0.018859612936115884	71393	71.393	0.06416414050575356
X	0.018572352557316294	5340	5.34	0.15151020555506073
X	0.018694442763468575	81002	81.002	0.061339315806219624
X	0.01954353470567739	307664	307.664	0.039900237193076074
X	0.018381976257031217	64908	64.908	0.06566964336845625
X	0.018450528707401225	8146	8.146	0.13132723580621689
X	0.019935093808048435	14322	14.322	0.11165329346451698
X	0.018902731461645755	10126	10.126	0.12312952777296536
X	0.0185335558141953	3092	3.092	0.1816518206336019
X	0.01943608927528281	129519	129.519	0.05314043855335531
X	0.018573716117454852	15965	15.965	0.10517437991589466
X	0.01841240269157422	1453	1.453	0.23313901115936123
X	0.018687261138844443	35261	35.261	0.08092518047407629
X	0.019981023795961703	15603	15.603	0.10859334465121838
X	0.01795070348707308	1244	1.244	0.24345593387456413
X	0.018501314716670755	75849	75.849	0.06248142278791658
X	0.019374599991590188	117451	117.451	0.05484352263225861
X	0.019642646279866786	69046	69.046	0.06576888674331427
X	0.019630067705672962	119181	119.181	0.05481569491253793
X	0.018522114762933146	112017	112.017	0.05488692213961293
X	0.01864665376580914	16768	16.768	0.10360321053073836
X	0.018975313308890133	42779	42.779	0.07626397583019588
X	0.018968131986930068	89479	89.479	0.05962583588078064
X	0.019452957902475476	151658	151.658	0.050432067666593514
X	0.018662669294545735	7776	7.776	0.13388723399854086
X	0.019627168367542684	43514	43.514	0.07669073149491605
X	0.019358950467814513	126826	126.826	0.05344304208701602
X	0.01893300260676942	52871	52.871	0.07101236718028578
X	0.019248097219437755	34032	34.032	0.08269897186681943
X	0.019076416066691394	32857	32.857	0.08342373426382944
X	0.018497658287771276	8455	8.455	0.12981784041166977
X	0.018219657385361594	4840	4.84	0.15556012427755872
X	0.018312961606446406	25654	25.654	0.08937193579137835
X	0.01947892287854158	132609	132.609	0.052763141575904594
X	0.01871075856546881	88562	88.562	0.05955908170293595
X	0.01883431563633008	491840	491.84	0.03370611251371364
X	0.01987490215186033	113460	113.46	0.05595266028326898
X	0.01895965046896047	72649	72.649	0.06390481206402442
X	0.018277333428286174	15206	15.206	0.10632432056409193
X	0.019456725776996	88209	88.209	0.06042068154262353
X	0.018427866590783718	3138	3.138	0.18041558051775253
X	0.018367290979280096	9488	9.488	0.12463025302904528
X	0.019542286427082372	188534	188.534	0.04697447094710338
X	0.018601471825992092	10943	10.943	0.11934483723720771
X	0.01842089463333201	3292	3.292	0.17753485534773097
X	0.01861124944985179	9686	9.686	0.12432076917655423
X	0.01948614493178527	103164	103.164	0.05737630614825993
X	0.01833939995448093	27118	27.118	0.08777600725888456
X	0.019868090507917532	78070	78.07	0.06337101309420064
X	0.01830770482142786	8290	8.29	0.13022462745530022
X	0.018657810584582758	10286	10.286	0.1219564347861102
X	0.018809468977606353	4275	4.275	0.16386271209625336
X	0.019245207931310383	12629	12.629	0.11507594451264556
X	0.019662847761168674	24459	24.459	0.09298278998655006
X	0.01839410848828673	50708	50.708	0.07131826248509524
X	0.018653427059087586	2798	2.798	0.18820752099837817
X	0.018577868052343943	126633	126.633	0.05274115780545424
X	0.0187269591155273	3023	3.023	0.18365793844053266
X	0.01985474043436003	18881	18.881	0.10169034963030507
X	0.018209524547225325	5511	5.511	0.14894390357897047
X	0.018905940173520683	88984	88.984	0.0596708354386196
X	0.019323909377719075	4949	4.949	0.1574680729598282
X	0.018528933562254428	37352	37.352	0.07916116695306594
X	0.01856349364251527	11915	11.915	0.11592805664537437
X	0.019700719071181604	15927	15.927	0.10734521726589334
X	0.01930209606706037	44672	44.672	0.07560021409427671
X	0.019681195058946787	343789	343.789	0.03854072052790454
X	0.019536282669578776	5944	5.944	0.14868062449145172
X	0.01851345876516298	9604	9.604	0.12445484375330257
X	0.01955297050421005	98139	98.139	0.05840594281801314
X	0.018515515744779037	8974	8.974	0.1273063058856642
X	0.019499063277478593	377088	377.088	0.037255495635753075
X	0.01848945015280164	1834	1.834	0.21602709303027523
X	0.01933932580228238	92605	92.605	0.05932926451863976
X	0.019614969008289775	175321	175.321	0.04818566929950422
X	0.018417617907660336	2906	2.906	0.18506002491179746
X	0.01811883951823882	185155	185.155	0.046082034492148684
X	0.018274194997954404	19287	19.287	0.09821802375521427
X	0.019969695555160805	298948	298.948	0.04057500642088092
X	0.018379283536886278	6963	6.963	0.13820085424392883
X	0.018400475798848563	35005	35.005	0.08070485416009197
X	0.018137196111291887	2163	2.163	0.20315984671892529
X	0.01990654308636127	4443	4.443	0.16485666564971138
X	0.018356881649582428	5695	5.695	0.14771855811630938
X	0.018680784168711766	24763	24.763	0.09103267018398704
X	0.01832335893969802	16516	16.516	0.10352217906231544
X	0.01921852391132491	95983	95.983	0.05850260923173049
X	0.018561431188701488	14909	14.909	0.10757738461238589
X	0.01838881145502665	4247	4.247	0.16298856382024957
X	0.01842913295972754	7485	7.485	0.13503231072959673
X	0.019583379844523508	85128	85.128	0.061273366082552315
X	0.019316963963965626	24701	24.701	0.09213149349072033
X	0.01950214145709344	154565	154.565	0.05015609481155723
X	0.01966708910078154	44209	44.209	0.07633841373375913
X	0.018394353657241604	11171	11.171	0.11808575031666826
X	0.01924201020887594	3439	3.439	0.17753040955322688
X	0.01927866680887168	7548	7.548	0.13669365895952565
X	0.019821801723211335	459696	459.696	0.03506636919365415
X	0.01872754625192289	6626	6.626	0.1413871093979567
X	0.019127467242370685	17481	17.481	0.10304582438369586
X	0.018357574640658958	80194	80.194	0.06117272243915796
X	0.01839803302204737	24388	24.388	0.09103290512595111
X	0.018449809961997438	2642	2.642	0.1911406365368124
X	0.01939042091065969	4416	4.416	0.16375179045652402
X	0.01951839966786578	210983	210.983	0.04522711802688898
X	0.018879306451925055	25095	25.095	0.09094935260412845
X	0.01836395764550712	35184	35.184	0.08051442396391276
X	0.01867868877068885	55862	55.862	0.06940807001900957
X	0.019940623750412563	72360	72.36	0.06507488265262411
X	0.0192670678279442	71161	71.161	0.06469308493635742
X	0.019464004955889316	199804	199.804	0.04601251895583801
X	0.018707979630963288	61015	61.015	0.06743159373175096
X	0.019437852276521207	4989	4.989	0.1573541704533872
X	0.018678449424841228	42045	42.045	0.07630309509414568
X	0.018673629678367557	10276	10.276	0.12203045075045922
X	0.01950474183613826	10963	10.963	0.12117236863952795
X	0.01848338259845364	14091	14.091	0.10946615896240187
X	0.018659609503933552	5689	5.689	0.14857835271846986
X	0.018746411432965415	50132	50.132	0.07204453419977992
X	0.019617648183139668	19948	19.948	0.09944490275429682
X	0.016790078587347965	2275	2.275	0.19469603595548002
X	0.019773944799700165	17718	17.718	0.10372725221329618
X	0.01858183462367541	3866	3.866	0.16876235623098315
X	0.01837074166629703	96723	96.723	0.057482058267783416
X	0.01887038621537588	29478	29.478	0.08618418745352305
X	0.01896751559007276	11028	11.028	0.11981317145313614
X	0.018139533078231754	4653	4.653	0.1573853667619855
X	0.019677766502235048	25647	25.647	0.09154748342683058
X	0.018314266864343495	3514	3.514	0.17337877374889188
X	0.018388784761726513	7054	7.054	0.13762770187971643
X	0.018682911317028227	19149	19.149	0.09918198998684387
X	0.018691643368242382	126646	126.646	0.05284679691928481
X	0.019853917581491402	6258	6.258	0.1469389341535485
X	0.017954004771245313	1830	1.83	0.2140770355387435
X	0.01873729521841207	4788	4.788	0.15758598327986267
X	0.01867872363680653	172068	172.068	0.047703366457768466
X	0.01973109892721313	434099	434.099	0.035687886542926595
X	0.018360405943312737	27487	27.487	0.08741480213743612
X	0.019709713964205084	6747	6.747	0.1429518797503692
X	0.019194868818830813	63898	63.898	0.06697292925387165
X	0.019828641936367866	117932	117.932	0.05519339416645109
X	0.01822171746821075	2445	2.445	0.19533053894719699
X	0.01848949153450986	13243	13.243	0.11176681189422427
X	0.018666146665367454	23900	23.9	0.09209136798542471
X	0.019242152634038858	82602	82.602	0.06153011084933524
X	0.018849296228208037	148316	148.316	0.050276964049236206
X	0.01870044251967104	103327	103.327	0.056564769421786724
X	0.018324042845579456	14875	14.875	0.10719835180788986
X	0.018671200690304094	4931	4.931	0.1558639751130931
X	0.018335788094333852	2997	2.997	0.18289603032630158
X	0.018637084466421202	25525	25.525	0.09004730944602207
X	0.019510696057089338	14359	14.359	0.11075997676557627
X	0.018529001837648926	41515	41.515	0.07642151811547648
X	0.01887784239721925	40267	40.267	0.07768449742543115
X	0.019625064358917654	267372	267.372	0.041869559164622254
X	0.018907541149766973	11465	11.465	0.11814635614769704
X	0.018245113542600368	3057	3.057	0.18139154666061535
X	0.019777053212452433	11385	11.385	0.12021062330887491
X	0.019764632907031277	798569	798.569	0.029142408067999552
X	0.018348301944985216	31789	31.789	0.08326062577977733
X	0.01827430754331493	4306	4.306	0.16190327532207957
X	0.01854469486625242	3070	3.07	0.18212117298272754
X	0.018660299231994432	34069	34.069	0.08181879376000685
X	0.01905154060127226	1892	1.892	0.2159416039462485
X	0.018574252646378384	37292	37.292	0.07926811990690823
X	0.019364494820936794	4153	4.153	0.16706343761430537
X	0.01929687072018067	11921	11.921	0.11741531109165998
X	0.01963666092730806	9812	9.812	0.126019194844582
X	0.018554043386096408	4379	4.379	0.16181590539105972
X	0.018662757912839566	2784	2.784	0.188553904721933
X	0.018633799385997747	145629	145.629	0.05039079181929405
X	0.019190942062390547	299898	299.898	0.03999824242410708
X	0.019139676488273104	105420	105.42	0.05662443221680865
X	0.01915066005385789	58240	58.24	0.06902198742876076
X	0.018653104246919263	19896	19.896	0.09787274786752209
X	0.018970530270050003	8165	8.165	0.13244668530496545
X	0.019742949645399667	10143	10.143	0.12485766365421073
X	0.018683759316312984	102925	102.925	0.056621468590984735
X	0.019748057761562823	59688	59.688	0.06916384368935355
X	0.019399607521191443	106082	106.082	0.05676104923971172
X	0.01956774286292265	9474	9.474	0.12735102675722978
X	0.0198594016485802	643966	643.966	0.0313593996868159
X	0.019006168213686253	17603	17.603	0.10258943229957934
X	0.019717857622228482	39660	39.66	0.07922015214790326
X	0.019929629344935477	111019	111.019	0.05641145526663901
X	0.019201625145554242	37540	37.54	0.07997383204692396
X	0.018643928665712926	146760	146.76	0.05027011859949064
X	0.019461571650604487	256882	256.882	0.04231373987442637
X	0.019594107006372593	73915	73.915	0.06423883133050835
X	0.018541752033162898	24534	24.534	0.09108792368562456
X	0.018922665385592622	12252	12.252	0.11559128912056835
X	0.0186508763352309	53217	53.217	0.07050441033135811
X	0.018343714311507373	5986	5.986	0.14525023883111834
X	0.01909899600630987	134414	134.414	0.05218215504482901
X	0.01990573008924461	721346	721.346	0.030218865871512582
X	0.01960285524582949	92527	92.527	0.05961427736737182
X	0.019327552918686073	104371	104.371	0.05699879513885954
X	0.018700663204969346	15223	15.223	0.1070990422541102
X	0.018524079118734826	28701	28.701	0.08641978858368356
X	0.019973648725495792	24568	24.568	0.09333170537778006
X	0.018882042107439555	13220	13.22	0.11261748790376806
X	0.018614962280102117	62493	62.493	0.06678469223789664
X	0.019783645024768183	251094	251.094	0.04287031341851814
X	0.018441902654730988	12539	12.539	0.11372281665394102
X	0.018300372574347242	231387	231.387	0.04292467267095982
X	0.01917104366578201	131620	131.62	0.052614803967276275
X	0.018356995060749996	37575	37.575	0.07875912222717606
X	0.018268953367875675	3066	3.066	0.1812927787264648
X	0.019762285752250888	74266	74.266	0.06432044599683567
X	0.01869926464185009	113207	113.207	0.054867741278325916
X	0.019080025889970174	3099	3.099	0.1832817098673465
X	0.019751399740009716	620171	620.171	0.03169777717460021
X	0.018637484069416375	24322	24.322	0.09150875928393715
X	0.019347190153508248	26672	26.672	0.08985055675706075
X	0.019108852536599906	6286	6.286	0.1448616443903377
X	0.018191854874962086	8899	8.899	0.12691469936082342
X	0.01829348963832195	3306	3.306	0.17687423309848058
X	0.018281891955209625	6732	6.732	0.13951653345789528
X	0.018526051158930495	16001	16.001	0.10500546712481192
X	0.019411274039693362	59238	59.238	0.06894210684278693
X	0.01952784832455782	1779171	1779.171	0.022223488568311783
X	0.01837111393136109	5032	5.032	0.1539802259925162
X	0.018468836686698582	60394	60.394	0.06737238402125056
X	0.018674818464220044	4776	4.776	0.15754236360235738
X	0.018985476417876374	19973	19.973	0.09832396855429897
X	0.018681739783307276	6707	6.707	0.14070072569029854
X	0.018403255261920094	143414	143.414	0.0504391431601319
X	0.018543574881860096	27218	27.218	0.08799251905347265
X	0.01960538640042081	902465	902.465	0.0279028359747125
X	0.01969798647639141	6520	6.52	0.1445633087451563
X	0.01888054835607738	117497	117.497	0.054366237141317754
X	0.01874003338355856	20230	20.23	0.09748208865253813
X	0.019547168907589473	113769	113.769	0.05559298261789948
X	0.019473067835407597	256862	256.862	0.042323168389774284
X	0.018412406953680713	19346	19.346	0.09836481826634072
X	0.018498951425552492	6598	6.598	0.14100838339488223
X	0.01980394679010505	65604	65.604	0.0670821762909097
X	0.018926634803176676	14662	14.662	0.10888299601904652
X	0.018876997554322563	35980	35.98	0.08065363029832631
X	0.01897750880341292	108073	108.073	0.05599819107688298
X	0.018520663613363453	17424	17.424	0.10205545590338576
X	0.019940043953867094	57402	57.402	0.07029639099440099
X	0.018032765025983718	24560	24.56	0.09021484080138412
X	0.01946424294294926	105377	105.377	0.056950459006852944
X	0.018569537654238202	319086	319.086	0.0387523264808121
X	0.018537774142106275	26275	26.275	0.08902355927794289
X	0.01952594717089199	8179	8.179	0.13365051418278334
X	0.018482338113793254	147132	147.132	0.050082183770486934
X	0.019158533897208483	43149	43.149	0.07628933785775793
X	0.01979167865952021	81317	81.317	0.062435811537863616
X	0.01858593080587339	43829	43.829	0.0751289959264537
X	0.01838486342747609	49550	49.55	0.07185752418736595
X	0.019260889418197365	35830	35.83	0.08130985312297513
X	0.018963964070228843	68037	68.037	0.06532227816661712
X	0.0187357200903297	36001	36.001	0.08043627342113754
X	0.018448362850424147	24271	24.271	0.09126201509173228
X	0.018869077305581308	4912	4.912	0.15661406734209413
X	0.019910871572893712	15893	15.893	0.10780232543558159
X	0.018730720892091227	42326	42.326	0.07620481649504103
X	0.01844783139908081	79041	79.041	0.06156931165630702
X	0.018877698759320723	108307	108.307	0.055859559727426046
X	0.01880165555913827	7856	7.856	0.13376162593036142
X	0.018496924468086096	22554	22.554	0.09360347721096436
X	0.01917601706986151	42230	42.23	0.07686213471130748
X	0.01932673932474068	200027	200.027	0.045887034219000915
X	0.01873167650740895	33217	33.217	0.08261750603315408
X	0.018598652007289218	60094	60.094	0.06764205376855442
X	0.01893964965547172	29282	29.282	0.08648160379188495
X	0.018537248248857958	16718	16.718	0.10350316933242085
X	0.01836887339001162	1755	1.755	0.2187434881925819
X	0.01968348139551496	8354	8.354	0.1330666267794698
X	0.018972971583785716	107819	107.819	0.056037663488122794
X	0.018391432899435744	40355	40.355	0.07695545408172336
X	0.018723381084196262	50105	50.105	0.0720279525994698
X	0.018035565501474907	1322	1.322	0.23894581929610537
X	0.0191601197619433	4678	4.678	0.15999730494255582
X	0.018981033685067004	95046	95.046	0.05845145664630418
X	0.01847431073653916	61099	61.099	0.06711888202041694
X	0.018738337138177415	22390	22.39	0.0942379112635597
X	0.018422505346013494	2871	2.871	0.18582543847564092
X	0.01835120114009311	3951	3.951	0.1668472892949966
X	0.01880113577427989	26123	26.123	0.08961630053576217
X	0.018522499508227135	27561	27.561	0.08759275919387295
X	0.01870703402411648	69398	69.398	0.0645980379361646
X	0.018485214710836112	4960	4.96	0.15504137368788873
X	0.01981570567396235	63743	63.743	0.06774215599156481
X	0.01851867916438757	10788	10.788	0.11973552513847915
X	0.018813750774197224	8024	8.024	0.13284997862542042
X	0.019845647004363403	242605	242.605	0.043409871973515896
X	0.01938574257956509	9987	9.987	0.1247428887920771
X	0.018627180564379328	76063	76.063	0.06256400729820318
X	0.0187247852710686	118429	118.429	0.05407372167117745
X	0.019584467794582366	44787	44.787	0.07590200830601873
X	0.019717104238769562	65279	65.279	0.06709495714667271
X	0.01939423095490709	170332	170.332	0.048468409936432054
X	0.017926185362801952	3071	3.071	0.18005396471745155
X	0.018932627938625612	43421	43.421	0.07582929641161794
X	0.018682357789528568	18743	18.743	0.09989203483952819
X	0.018620286685656875	194598	194.598	0.0457385785502853
X	0.01866581166161501	125004	125.004	0.053052720176125605
X	0.018629348221542424	33283	33.283	0.08241224251908422
X	0.01879721760484755	47068	47.068	0.07364149695461966
X	0.018518321722157123	9606	9.606	0.12445710105569652
X	0.01856602444683894	12301	12.301	0.11470776902415673
X	0.01844012377066736	22017	22.017	0.09426169171283717
X	0.01883739267592634	9931	9.931	0.12378725610066742
X	0.019438257190123	145123	145.123	0.05116508408032708
X	0.019959549105526185	222099	222.099	0.04479209487157627
X	0.018244359249366852	7664	7.664	0.13352349957218182
X	0.01954839595930012	14609	14.609	0.11019542038905315
X	0.019379103971199935	11811	11.811	0.11794576215036763
X	0.019116556443999067	245329	245.329	0.042712362293418224
X	0.018394189201445272	19962	19.962	0.0973103122250586
X	0.018891748372175773	12809	12.809	0.11382883969004527
X	0.019790836677694188	54582	54.582	0.07130799684224101
X	0.019438795540960795	84889	84.889	0.06117950771526211
X	0.018709480075285407	10263	10.263	0.12216002980170908
X	0.019642526476315893	25751	25.751	0.09136946517038157
X	0.01839828928742382	7889	7.889	0.13261271373961622
time for making epsilon is 1.8828768730163574
epsilons are
[0.2712887930885137, 0.09797072108943324, 0.19579159747964242, 0.18121257663563314, 0.08513625736479645, 0.08468098542742596, 0.06951988380265237, 0.07484745270856154, 0.13171652066255393, 0.0981335076384456, 0.12106297517443655, 0.0539054094608066, 0.14445451656940606, 0.044419776335058725, 0.0851591010681083, 0.07879754531537611, 0.06936496046249561, 0.0734410264023399, 0.04897805791627518, 0.05738058357360845, 0.12441146004269404, 0.04213839782905612, 0.10539049624671697, 0.10311167013745905, 0.0839467740127218, 0.06628016904130603, 0.06976268640049506, 0.12752659381760803, 0.06787276600123776, 0.02440689576298322, 0.13154100630266352, 0.03498480527602228, 0.10810514904954555, 0.1319373754820853, 0.14110079113255983, 0.05808582640738292, 0.06889267976200172, 0.2308902405991277, 0.15898929316355054, 0.2052096593420034, 0.1717192066445873, 0.19944886654714222, 0.1960854362331702, 0.15837325690938694, 0.25525352748837155, 0.21183394016897616, 0.15740492776844112, 0.16272987185958052, 0.2129767779982745, 0.12778942107844726, 0.11666104066849713, 0.20498831949479437, 0.1643762969337148, 0.18379328182432741, 0.1485607356601458, 0.17354444966428748, 0.2034027413924422, 0.16835591919846066, 0.17372555498954825, 0.19874664988976282, 0.15600068411521742, 0.217725990927711, 0.1310369444359756, 0.1298968968978367, 0.19432381030635498, 0.19347253589559602, 0.1912868834974364, 0.1563559147859403, 0.16735066661071793, 0.20613312790200253, 0.17579195912055992, 0.1644335978884734, 0.17119764189427777, 0.21284878108992972, 0.1823728826494823, 0.1481462242466218, 0.2498785166910075, 0.2693398553963943, 0.17594059868398407, 0.11229067940017168, 0.17826585681302606, 0.22155929627158266, 0.15515334533928168, 0.2054456343821434, 0.21310642514667316, 0.2200071286124269, 0.18258034881070795, 0.236452245523263, 0.17193442628463912, 0.18567470229292848, 0.17982910532612498, 0.22755930804771904, 0.22698388238915645, 0.16796054142268835, 0.16820601852693595, 0.1568271753949094, 0.190585066911868, 0.14878833060431979, 0.16703677265340788, 0.18180595739978245, 0.1260045149874352, 0.17845791138896866, 0.1426248586217304, 0.2192780183047733, 0.14213985712183352, 0.20282398556697823, 0.15563449193075515, 0.2666337330272583, 0.18367140707509, 0.22254422800390342, 0.19450574066730675, 0.24593288512412925, 0.21232323371687722, 0.1675238085786521, 0.15859571337791212, 0.223739865668174, 0.19890758565723768, 0.2619910580559737, 0.14285026280640728, 0.21356981637353528, 0.12200400868908491, 0.1800410725009076, 0.19418235315842067, 0.18670773690208908, 0.21095271120114628, 0.22256773196877058, 0.21229151807300553, 0.18008248899381965, 0.1146001571467876, 0.20032337913309123, 0.14282819765852406, 0.2212249896894253, 0.16250473486981146, 0.24266487174225948, 0.18677411014197096, 0.20311203430585387, 0.23031843599148077, 0.21883765604566188, 0.19754628339064928, 0.20793076222499124, 0.1651804212590765, 0.2491152427428301, 0.12362842693590796, 0.1368585770031473, 0.14736413420961483, 0.20093955287425652, 0.19901627593567256, 0.16401096376081514, 0.207239603085203, 0.19680580107226733, 0.20585042648124763, 0.18163436097081653, 0.16983896058680584, 0.1312479903761911, 0.19847229569912367, 0.11505377439949849, 0.1856288318031063, 0.18939182925377296, 0.17571909688097032, 0.28263198698218794, 0.1489077608044974, 0.21815721762856038, 0.1558611592511011, 0.258416772575924, 0.16851832485178614, 0.21632680358377257, 0.20295800843766937, 0.17740644756183171, 0.15867098547906686, 0.2810378182854323, 0.22191845342023114, 0.22963532491605282, 0.13960105071150425, 0.2059857733404271, 0.1709807676791074, 0.18569985951309786, 0.14594818172233343, 0.19441553253149998, 0.15325184575240255, 0.17427832187140627, 0.17198733022400942, 0.19820533754453565, 0.1774478941222007, 0.1706225761807469, 0.15098508279711353, 0.15675012373408742, 0.20671450919616455, 0.17047058737580414, 0.14178863888703572, 0.1176673132735555, 0.2073775846618909, 0.31858550195334423, 0.18548719720013188, 0.20796215904369408, 0.18392438070367587, 0.16890654055702392, 0.19116096697154564, 0.23995757588706615, 0.20778805581670148, 0.2433680363341251, 0.16699396086873333, 0.2165593371069358, 0.13871511680856272, 0.17344492759821845, 0.15115822035981655, 0.2404406284947698, 0.16282775003423003, 0.1752346457090137, 0.1793708513192006, 0.13762749080469344, 0.16441110130373773, 0.16408484366258466, 0.1895287155122454, 0.19434294295525487, 0.2607031056579153, 0.2199342878550027, 0.25983221198637757, 0.28668829314713895, 0.1445567380756321, 0.18924885397169763, 0.20307372693962758, 0.22877835033484845, 0.1579955829245945, 0.1949797437738505, 0.16201209128636335, 0.16725300492651582, 0.2213519239925937, 0.23204393491202005, 0.1606021177307848, 0.26383930234315844, 0.22589466309852607, 0.20249045481899006, 0.1787701539841351, 0.16725675689834812, 0.20179081589395892, 0.2071932085378766, 0.17591202687924679, 0.14758883157589006, 0.23212983433080275, 0.12461802681259927, 0.064916940265489, 0.16647328958076787, 0.16718144207351507, 0.24004746799623397, 0.15237555063358937, 0.12092236154792392, 0.04755342748144651, 0.1766559017954416, 0.06590524456743722, 0.05949235384174824, 0.09220535418082107, 0.05448371507488227, 0.20579305731186584, 0.15827206720695347, 0.051120705540902, 0.03651742165032983, 0.16319739291454236, 0.08931069232568756, 0.07623715632229268, 0.061083590795020445, 0.08265184405857927, 0.08928372156025195, 0.0912833672257228, 0.11010396493673658, 0.13086149264427058, 0.04879530104249861, 0.16860615732909723, 0.1659026418623855, 0.07946607484599844, 0.08901439920684288, 0.042583137193074154, 0.06084085645723472, 0.24442373936100437, 0.1026716059790907, 0.1304832021444854, 0.10465356352361915, 0.0711095267145459, 0.10407626568715692, 0.07935485243673372, 0.2577567157005141, 0.15071083990805884, 0.06891299535435574, 0.07137637429276793, 0.08440848693853109, 0.15581724391734483, 0.043228122657697494, 0.11863629281922237, 0.09594850808792725, 0.0591019739598823, 0.1884333181909392, 0.06612308208914006, 0.04450084908097098, 0.04484298598560671, 0.10903141574961052, 0.10485001734114563, 0.12370504459944337, 0.15851955416759325, 0.07201078554088433, 0.131035581067345, 0.10123406129976709, 0.08193783800368674, 0.08189718305264271, 0.1191744185312146, 0.14107046636792503, 0.05864330256229678, 0.11904160626960965, 0.11397552294350055, 0.11951951163382417, 0.03872044240160728, 0.13225188177206718, 0.05373521795925759, 0.1573712927444964, 0.08733432260568871, 0.11362059218127137, 0.06424949164521426, 0.07403644964956846, 0.16584231239981115, 0.054608963563716896, 0.059576896103119455, 0.16863106453519705, 0.06146620699228846, 0.055961812098254106, 0.04246352762461287, 0.05962289065253928, 0.17647625173953024, 0.12696001326840892, 0.13414593197202496, 0.08207045175029287, 0.1327104242937936, 0.21455854266052393, 0.07295061175761385, 0.05308627640517871, 0.0745631153995085, 0.15126698122655632, 0.0937405140655291, 0.12300629226494748, 0.13191170611371109, 0.13134265246195695, 0.16479025520472326, 0.05198435259551998, 0.08177119715878013, 0.10469769532747693, 0.0972288170292219, 0.13635435800077972, 0.03658107451398542, 0.13858989867717794, 0.05939034285763041, 0.23292020318063844, 0.1644136132772652, 0.18871779931282706, 0.13698295708404604, 0.12069973321939394, 0.08621837642698627, 0.1604234168551127, 0.17064115114917203, 0.042599581048671564, 0.08323759327316813, 0.09145219892849021, 0.06564582074492832, 0.05183890858978107, 0.1784567119563301, 0.15202058737898547, 0.10392892188542599, 0.2240641046182041, 0.14677877432241512, 0.10557927391679847, 0.14912530069141253, 0.15962825152840923, 0.16705624653297108, 0.13994804228596094, 0.035931352083338745, 0.10531346156807567, 0.06766335961345654, 0.11591204895681846, 0.16670569465022436, 0.10740477411036459, 0.04019175400664817, 0.03535356212479639, 0.08835212730936141, 0.08546751901790425, 0.2022956371901226, 0.07466067592586799, 0.04456725995995898, 0.06416414050575356, 0.15151020555506073, 0.061339315806219624, 0.039900237193076074, 0.06566964336845625, 0.13132723580621689, 0.11165329346451698, 0.12312952777296536, 0.1816518206336019, 0.05314043855335531, 0.10517437991589466, 0.23313901115936123, 0.08092518047407629, 0.10859334465121838, 0.24345593387456413, 0.06248142278791658, 0.05484352263225861, 0.06576888674331427, 0.05481569491253793, 0.05488692213961293, 0.10360321053073836, 0.07626397583019588, 0.05962583588078064, 0.050432067666593514, 0.13388723399854086, 0.07669073149491605, 0.05344304208701602, 0.07101236718028578, 0.08269897186681943, 0.08342373426382944, 0.12981784041166977, 0.15556012427755872, 0.08937193579137835, 0.052763141575904594, 0.05955908170293595, 0.03370611251371364, 0.05595266028326898, 0.06390481206402442, 0.10632432056409193, 0.06042068154262353, 0.18041558051775253, 0.12463025302904528, 0.04697447094710338, 0.11934483723720771, 0.17753485534773097, 0.12432076917655423, 0.05737630614825993, 0.08777600725888456, 0.06337101309420064, 0.13022462745530022, 0.1219564347861102, 0.16386271209625336, 0.11507594451264556, 0.09298278998655006, 0.07131826248509524, 0.18820752099837817, 0.05274115780545424, 0.18365793844053266, 0.10169034963030507, 0.14894390357897047, 0.0596708354386196, 0.1574680729598282, 0.07916116695306594, 0.11592805664537437, 0.10734521726589334, 0.07560021409427671, 0.03854072052790454, 0.14868062449145172, 0.12445484375330257, 0.05840594281801314, 0.1273063058856642, 0.037255495635753075, 0.21602709303027523, 0.05932926451863976, 0.04818566929950422, 0.18506002491179746, 0.046082034492148684, 0.09821802375521427, 0.04057500642088092, 0.13820085424392883, 0.08070485416009197, 0.20315984671892529, 0.16485666564971138, 0.14771855811630938, 0.09103267018398704, 0.10352217906231544, 0.05850260923173049, 0.10757738461238589, 0.16298856382024957, 0.13503231072959673, 0.061273366082552315, 0.09213149349072033, 0.05015609481155723, 0.07633841373375913, 0.11808575031666826, 0.17753040955322688, 0.13669365895952565, 0.03506636919365415, 0.1413871093979567, 0.10304582438369586, 0.06117272243915796, 0.09103290512595111, 0.1911406365368124, 0.16375179045652402, 0.04522711802688898, 0.09094935260412845, 0.08051442396391276, 0.06940807001900957, 0.06507488265262411, 0.06469308493635742, 0.04601251895583801, 0.06743159373175096, 0.1573541704533872, 0.07630309509414568, 0.12203045075045922, 0.12117236863952795, 0.10946615896240187, 0.14857835271846986, 0.07204453419977992, 0.09944490275429682, 0.19469603595548002, 0.10372725221329618, 0.16876235623098315, 0.057482058267783416, 0.08618418745352305, 0.11981317145313614, 0.1573853667619855, 0.09154748342683058, 0.17337877374889188, 0.13762770187971643, 0.09918198998684387, 0.05284679691928481, 0.1469389341535485, 0.2140770355387435, 0.15758598327986267, 0.047703366457768466, 0.035687886542926595, 0.08741480213743612, 0.1429518797503692, 0.06697292925387165, 0.05519339416645109, 0.19533053894719699, 0.11176681189422427, 0.09209136798542471, 0.06153011084933524, 0.050276964049236206, 0.056564769421786724, 0.10719835180788986, 0.1558639751130931, 0.18289603032630158, 0.09004730944602207, 0.11075997676557627, 0.07642151811547648, 0.07768449742543115, 0.041869559164622254, 0.11814635614769704, 0.18139154666061535, 0.12021062330887491, 0.029142408067999552, 0.08326062577977733, 0.16190327532207957, 0.18212117298272754, 0.08181879376000685, 0.2159416039462485, 0.07926811990690823, 0.16706343761430537, 0.11741531109165998, 0.126019194844582, 0.16181590539105972, 0.188553904721933, 0.05039079181929405, 0.03999824242410708, 0.05662443221680865, 0.06902198742876076, 0.09787274786752209, 0.13244668530496545, 0.12485766365421073, 0.056621468590984735, 0.06916384368935355, 0.05676104923971172, 0.12735102675722978, 0.0313593996868159, 0.10258943229957934, 0.07922015214790326, 0.05641145526663901, 0.07997383204692396, 0.05027011859949064, 0.04231373987442637, 0.06423883133050835, 0.09108792368562456, 0.11559128912056835, 0.07050441033135811, 0.14525023883111834, 0.05218215504482901, 0.030218865871512582, 0.05961427736737182, 0.05699879513885954, 0.1070990422541102, 0.08641978858368356, 0.09333170537778006, 0.11261748790376806, 0.06678469223789664, 0.04287031341851814, 0.11372281665394102, 0.04292467267095982, 0.052614803967276275, 0.07875912222717606, 0.1812927787264648, 0.06432044599683567, 0.054867741278325916, 0.1832817098673465, 0.03169777717460021, 0.09150875928393715, 0.08985055675706075, 0.1448616443903377, 0.12691469936082342, 0.17687423309848058, 0.13951653345789528, 0.10500546712481192, 0.06894210684278693, 0.022223488568311783, 0.1539802259925162, 0.06737238402125056, 0.15754236360235738, 0.09832396855429897, 0.14070072569029854, 0.0504391431601319, 0.08799251905347265, 0.0279028359747125, 0.1445633087451563, 0.054366237141317754, 0.09748208865253813, 0.05559298261789948, 0.042323168389774284, 0.09836481826634072, 0.14100838339488223, 0.0670821762909097, 0.10888299601904652, 0.08065363029832631, 0.05599819107688298, 0.10205545590338576, 0.07029639099440099, 0.09021484080138412, 0.056950459006852944, 0.0387523264808121, 0.08902355927794289, 0.13365051418278334, 0.050082183770486934, 0.07628933785775793, 0.062435811537863616, 0.0751289959264537, 0.07185752418736595, 0.08130985312297513, 0.06532227816661712, 0.08043627342113754, 0.09126201509173228, 0.15661406734209413, 0.10780232543558159, 0.07620481649504103, 0.06156931165630702, 0.055859559727426046, 0.13376162593036142, 0.09360347721096436, 0.07686213471130748, 0.045887034219000915, 0.08261750603315408, 0.06764205376855442, 0.08648160379188495, 0.10350316933242085, 0.2187434881925819, 0.1330666267794698, 0.056037663488122794, 0.07695545408172336, 0.0720279525994698, 0.23894581929610537, 0.15999730494255582, 0.05845145664630418, 0.06711888202041694, 0.0942379112635597, 0.18582543847564092, 0.1668472892949966, 0.08961630053576217, 0.08759275919387295, 0.0645980379361646, 0.15504137368788873, 0.06774215599156481, 0.11973552513847915, 0.13284997862542042, 0.043409871973515896, 0.1247428887920771, 0.06256400729820318, 0.05407372167117745, 0.07590200830601873, 0.06709495714667271, 0.048468409936432054, 0.18005396471745155, 0.07582929641161794, 0.09989203483952819, 0.0457385785502853, 0.053052720176125605, 0.08241224251908422, 0.07364149695461966, 0.12445710105569652, 0.11470776902415673, 0.09426169171283717, 0.12378725610066742, 0.05116508408032708, 0.04479209487157627, 0.13352349957218182, 0.11019542038905315, 0.11794576215036763, 0.042712362293418224, 0.0973103122250586, 0.11382883969004527, 0.07130799684224101, 0.06117950771526211, 0.12216002980170908, 0.09136946517038157, 0.13261271373961622]
0.09325774866085716
Making ranges
torch.Size([40688, 2])
We keep 7.27e+06/6.08e+08 =  1% of the original kernel matrix.

torch.Size([2417, 2])
We keep 6.00e+04/8.54e+05 =  7% of the original kernel matrix.

torch.Size([11771, 2])
We keep 7.51e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([32025, 2])
We keep 8.87e+06/4.37e+08 =  2% of the original kernel matrix.

torch.Size([37855, 2])
We keep 7.21e+06/5.16e+08 =  1% of the original kernel matrix.

torch.Size([5922, 2])
We keep 3.10e+05/6.93e+06 =  4% of the original kernel matrix.

torch.Size([16802, 2])
We keep 1.53e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([6334, 2])
We keep 9.53e+05/9.92e+06 =  9% of the original kernel matrix.

torch.Size([16639, 2])
We keep 1.69e+06/7.76e+07 =  2% of the original kernel matrix.

torch.Size([42266, 2])
We keep 3.25e+07/9.39e+08 =  3% of the original kernel matrix.

torch.Size([42173, 2])
We keep 9.63e+06/7.55e+08 =  1% of the original kernel matrix.

torch.Size([46598, 2])
We keep 1.47e+07/9.18e+08 =  1% of the original kernel matrix.

torch.Size([43817, 2])
We keep 9.53e+06/7.47e+08 =  1% of the original kernel matrix.

torch.Size([85976, 2])
We keep 4.25e+07/3.15e+09 =  1% of the original kernel matrix.

torch.Size([58161, 2])
We keep 1.60e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([69167, 2])
We keep 2.52e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([52825, 2])
We keep 1.30e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([14147, 2])
We keep 2.99e+06/6.57e+07 =  4% of the original kernel matrix.

torch.Size([24180, 2])
We keep 3.41e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([25934, 2])
We keep 1.67e+07/3.79e+08 =  4% of the original kernel matrix.

torch.Size([32186, 2])
We keep 6.73e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([18639, 2])
We keep 3.23e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([27542, 2])
We keep 4.12e+06/2.57e+08 =  1% of the original kernel matrix.

torch.Size([166266, 2])
We keep 1.99e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([81946, 2])
We keep 2.93e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([12873, 2])
We keep 1.22e+06/4.08e+07 =  2% of the original kernel matrix.

torch.Size([22733, 2])
We keep 2.87e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([323807, 2])
We keep 4.81e+08/4.56e+10 =  1% of the original kernel matrix.

torch.Size([119274, 2])
We keep 5.03e+07/5.26e+09 =  0% of the original kernel matrix.

torch.Size([44388, 2])
We keep 2.03e+07/8.65e+08 =  2% of the original kernel matrix.

torch.Size([42567, 2])
We keep 9.23e+06/7.25e+08 =  1% of the original kernel matrix.

torch.Size([58207, 2])
We keep 2.31e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([48928, 2])
We keep 1.17e+07/9.59e+08 =  1% of the original kernel matrix.

torch.Size([88009, 2])
We keep 4.13e+07/3.11e+09 =  1% of the original kernel matrix.

torch.Size([59003, 2])
We keep 1.56e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([69836, 2])
We keep 3.78e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([52816, 2])
We keep 1.39e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([238296, 2])
We keep 2.82e+08/2.53e+10 =  1% of the original kernel matrix.

torch.Size([100506, 2])
We keep 3.85e+07/3.92e+09 =  0% of the original kernel matrix.

torch.Size([152487, 2])
We keep 1.34e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([78914, 2])
We keep 2.61e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([14938, 2])
We keep 3.50e+06/8.65e+07 =  4% of the original kernel matrix.

torch.Size([24566, 2])
We keep 3.79e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([389329, 2])
We keep 5.62e+08/6.78e+10 =  0% of the original kernel matrix.

torch.Size([130539, 2])
We keep 5.91e+07/6.42e+09 =  0% of the original kernel matrix.

torch.Size([25883, 2])
We keep 5.38e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([32778, 2])
We keep 5.61e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([28830, 2])
We keep 1.60e+07/3.10e+08 =  5% of the original kernel matrix.

torch.Size([35446, 2])
We keep 6.22e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([24063, 2])
We keep 5.69e+07/9.19e+08 =  6% of the original kernel matrix.

torch.Size([28548, 2])
We keep 9.61e+06/7.47e+08 =  1% of the original kernel matrix.

torch.Size([102633, 2])
We keep 4.61e+07/4.13e+09 =  1% of the original kernel matrix.

torch.Size([63908, 2])
We keep 1.76e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([79065, 2])
We keep 5.93e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([56070, 2])
We keep 1.63e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([16783, 2])
We keep 3.45e+06/8.74e+07 =  3% of the original kernel matrix.

torch.Size([26276, 2])
We keep 3.80e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([76604, 2])
We keep 1.13e+08/3.47e+09 =  3% of the original kernel matrix.

torch.Size([54823, 2])
We keep 1.67e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([2314495, 2])
We keep 1.01e+10/1.86e+12 =  0% of the original kernel matrix.

torch.Size([332839, 2])
We keep 2.70e+08/3.37e+10 =  0% of the original kernel matrix.

torch.Size([13519, 2])
We keep 3.13e+06/6.36e+07 =  4% of the original kernel matrix.

torch.Size([23375, 2])
We keep 3.36e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([585441, 2])
We keep 2.78e+09/2.17e+11 =  1% of the original kernel matrix.

torch.Size([159747, 2])
We keep 1.03e+08/1.15e+10 =  0% of the original kernel matrix.

torch.Size([23764, 2])
We keep 1.11e+07/2.15e+08 =  5% of the original kernel matrix.

torch.Size([31147, 2])
We keep 5.36e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([15283, 2])
We keep 1.84e+06/6.60e+07 =  2% of the original kernel matrix.

torch.Size([24608, 2])
We keep 3.39e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([11873, 2])
We keep 1.70e+06/4.41e+07 =  3% of the original kernel matrix.

torch.Size([21880, 2])
We keep 2.96e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([123012, 2])
We keep 1.95e+08/9.59e+09 =  2% of the original kernel matrix.

torch.Size([70378, 2])
We keep 2.57e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([92161, 2])
We keep 5.27e+07/3.70e+09 =  1% of the original kernel matrix.

torch.Size([60714, 2])
We keep 1.71e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([3588, 2])
We keep 1.27e+05/2.31e+06 =  5% of the original kernel matrix.

torch.Size([13538, 2])
We keep 1.05e+06/3.75e+07 =  2% of the original kernel matrix.

torch.Size([9953, 2])
We keep 7.49e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([20545, 2])
We keep 2.23e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([4969, 2])
We keep 2.72e+05/5.00e+06 =  5% of the original kernel matrix.

torch.Size([15238, 2])
We keep 1.37e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([8027, 2])
We keep 5.06e+05/1.34e+07 =  3% of the original kernel matrix.

torch.Size([18511, 2])
We keep 1.89e+06/9.01e+07 =  2% of the original kernel matrix.

torch.Size([3685, 2])
We keep 4.38e+05/4.72e+06 =  9% of the original kernel matrix.

torch.Size([12419, 2])
We keep 1.32e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([4667, 2])
We keep 6.96e+05/5.43e+06 = 12% of the original kernel matrix.

torch.Size([14329, 2])
We keep 1.36e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([9607, 2])
We keep 8.48e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([19673, 2])
We keep 2.24e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([2800, 2])
We keep 7.50e+04/1.16e+06 =  6% of the original kernel matrix.

torch.Size([12318, 2])
We keep 8.13e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([4593, 2])
We keep 2.01e+05/3.86e+06 =  5% of the original kernel matrix.

torch.Size([14746, 2])
We keep 1.25e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([9652, 2])
We keep 9.56e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([19791, 2])
We keep 2.30e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([8981, 2])
We keep 7.13e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([19457, 2])
We keep 2.13e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([4474, 2])
We keep 2.05e+05/3.58e+06 =  5% of the original kernel matrix.

torch.Size([14492, 2])
We keep 1.19e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([15930, 2])
We keep 3.13e+06/8.68e+07 =  3% of the original kernel matrix.

torch.Size([25453, 2])
We keep 3.83e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([21212, 2])
We keep 4.21e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([30403, 2])
We keep 4.84e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([5065, 2])
We keep 2.46e+05/4.58e+06 =  5% of the original kernel matrix.

torch.Size([15272, 2])
We keep 1.29e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([8035, 2])
We keep 7.70e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([18492, 2])
We keep 2.07e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([6684, 2])
We keep 4.49e+05/8.39e+06 =  5% of the original kernel matrix.

torch.Size([17030, 2])
We keep 1.59e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([11793, 2])
We keep 1.15e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([21942, 2])
We keep 2.77e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([7385, 2])
We keep 6.10e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([17769, 2])
We keep 1.86e+06/8.74e+07 =  2% of the original kernel matrix.

torch.Size([5225, 2])
We keep 2.26e+05/4.93e+06 =  4% of the original kernel matrix.

torch.Size([15677, 2])
We keep 1.35e+06/5.48e+07 =  2% of the original kernel matrix.

torch.Size([8557, 2])
We keep 5.94e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([18827, 2])
We keep 2.00e+06/9.62e+07 =  2% of the original kernel matrix.

torch.Size([7477, 2])
We keep 5.09e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([17923, 2])
We keep 1.84e+06/8.63e+07 =  2% of the original kernel matrix.

torch.Size([5268, 2])
We keep 2.72e+05/5.38e+06 =  5% of the original kernel matrix.

torch.Size([15380, 2])
We keep 1.38e+06/5.72e+07 =  2% of the original kernel matrix.

torch.Size([10922, 2])
We keep 9.28e+05/2.69e+07 =  3% of the original kernel matrix.

torch.Size([21730, 2])
We keep 2.45e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([3972, 2])
We keep 1.74e+05/3.21e+06 =  5% of the original kernel matrix.

torch.Size([14063, 2])
We keep 1.17e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([17532, 2])
We keep 2.07e+06/7.85e+07 =  2% of the original kernel matrix.

torch.Size([27541, 2])
We keep 3.66e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([16121, 2])
We keep 2.51e+06/7.41e+07 =  3% of the original kernel matrix.

torch.Size([25549, 2])
We keep 3.54e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([5733, 2])
We keep 2.96e+05/6.17e+06 =  4% of the original kernel matrix.

torch.Size([16055, 2])
We keep 1.44e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([5776, 2])
We keep 3.04e+05/6.63e+06 =  4% of the original kernel matrix.

torch.Size([16180, 2])
We keep 1.48e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([6106, 2])
We keep 3.05e+05/7.08e+06 =  4% of the original kernel matrix.

torch.Size([16551, 2])
We keep 1.51e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([9919, 2])
We keep 8.62e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([19959, 2])
We keep 2.28e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([8573, 2])
We keep 6.92e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([19097, 2])
We keep 2.11e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([4952, 2])
We keep 2.22e+05/4.36e+06 =  5% of the original kernel matrix.

torch.Size([15210, 2])
We keep 1.28e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([6842, 2])
We keep 7.30e+05/1.14e+07 =  6% of the original kernel matrix.

torch.Size([16852, 2])
We keep 1.79e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([9031, 2])
We keep 6.97e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([19265, 2])
We keep 2.15e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([7920, 2])
We keep 5.57e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([18251, 2])
We keep 1.92e+06/9.12e+07 =  2% of the original kernel matrix.

torch.Size([4425, 2])
We keep 2.20e+05/3.68e+06 =  5% of the original kernel matrix.

torch.Size([14556, 2])
We keep 1.22e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([6500, 2])
We keep 4.22e+05/9.46e+06 =  4% of the original kernel matrix.

torch.Size([16853, 2])
We keep 1.70e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([11346, 2])
We keep 1.78e+06/3.26e+07 =  5% of the original kernel matrix.

torch.Size([21633, 2])
We keep 2.64e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([2874, 2])
We keep 8.96e+04/1.39e+06 =  6% of the original kernel matrix.

torch.Size([12389, 2])
We keep 8.80e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([2366, 2])
We keep 6.27e+04/8.76e+05 =  7% of the original kernel matrix.

torch.Size([11529, 2])
We keep 7.38e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([7845, 2])
We keep 4.79e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([18305, 2])
We keep 1.79e+06/8.43e+07 =  2% of the original kernel matrix.

torch.Size([22174, 2])
We keep 4.01e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([30292, 2])
We keep 5.18e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([7400, 2])
We keep 4.67e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([17878, 2])
We keep 1.82e+06/8.36e+07 =  2% of the original kernel matrix.

torch.Size([3433, 2])
We keep 1.65e+05/2.33e+06 =  7% of the original kernel matrix.

torch.Size([12739, 2])
We keep 1.01e+06/3.76e+07 =  2% of the original kernel matrix.

torch.Size([10300, 2])
We keep 1.10e+06/2.79e+07 =  3% of the original kernel matrix.

torch.Size([20871, 2])
We keep 2.50e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([4904, 2])
We keep 2.41e+05/4.36e+06 =  5% of the original kernel matrix.

torch.Size([15101, 2])
We keep 1.29e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([4523, 2])
We keep 1.79e+05/3.55e+06 =  5% of the original kernel matrix.

torch.Size([14748, 2])
We keep 1.19e+06/4.64e+07 =  2% of the original kernel matrix.

torch.Size([4035, 2])
We keep 1.48e+05/2.95e+06 =  4% of the original kernel matrix.

torch.Size([14102, 2])
We keep 1.11e+06/4.24e+07 =  2% of the original kernel matrix.

torch.Size([6726, 2])
We keep 4.28e+05/8.67e+06 =  4% of the original kernel matrix.

torch.Size([17015, 2])
We keep 1.62e+06/7.26e+07 =  2% of the original kernel matrix.

torch.Size([3210, 2])
We keep 1.23e+05/1.87e+06 =  6% of the original kernel matrix.

torch.Size([12623, 2])
We keep 9.63e+05/3.37e+07 =  2% of the original kernel matrix.

torch.Size([7767, 2])
We keep 5.37e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([18103, 2])
We keep 1.91e+06/9.01e+07 =  2% of the original kernel matrix.

torch.Size([6573, 2])
We keep 3.56e+05/8.47e+06 =  4% of the original kernel matrix.

torch.Size([17084, 2])
We keep 1.63e+06/7.18e+07 =  2% of the original kernel matrix.

torch.Size([7112, 2])
We keep 4.20e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([17415, 2])
We keep 1.74e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([3826, 2])
We keep 1.37e+05/2.56e+06 =  5% of the original kernel matrix.

torch.Size([13809, 2])
We keep 1.08e+06/3.94e+07 =  2% of the original kernel matrix.

torch.Size([3704, 2])
We keep 1.61e+05/2.68e+06 =  5% of the original kernel matrix.

torch.Size([13619, 2])
We keep 1.09e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([8214, 2])
We keep 7.76e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([18801, 2])
We keep 2.09e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([8155, 2])
We keep 6.14e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([18391, 2])
We keep 1.94e+06/9.38e+07 =  2% of the original kernel matrix.

torch.Size([9902, 2])
We keep 7.90e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([20086, 2])
We keep 2.29e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([6354, 2])
We keep 3.40e+05/7.87e+06 =  4% of the original kernel matrix.

torch.Size([17098, 2])
We keep 1.60e+06/6.92e+07 =  2% of the original kernel matrix.

torch.Size([10399, 2])
We keep 1.73e+06/3.18e+07 =  5% of the original kernel matrix.

torch.Size([20710, 2])
We keep 2.61e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([8393, 2])
We keep 6.61e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([18916, 2])
We keep 2.06e+06/9.99e+07 =  2% of the original kernel matrix.

torch.Size([6802, 2])
We keep 4.08e+05/9.83e+06 =  4% of the original kernel matrix.

torch.Size([17269, 2])
We keep 1.70e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([17277, 2])
We keep 2.32e+06/9.20e+07 =  2% of the original kernel matrix.

torch.Size([26622, 2])
We keep 3.90e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([7458, 2])
We keep 4.73e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([18250, 2])
We keep 1.75e+06/8.03e+07 =  2% of the original kernel matrix.

torch.Size([13043, 2])
We keep 1.30e+06/4.39e+07 =  2% of the original kernel matrix.

torch.Size([23047, 2])
We keep 2.96e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([3825, 2])
We keep 1.68e+05/2.81e+06 =  5% of the original kernel matrix.

torch.Size([13619, 2])
We keep 1.10e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([11714, 2])
We keep 2.65e+06/4.23e+07 =  6% of the original kernel matrix.

torch.Size([21669, 2])
We keep 2.87e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([5088, 2])
We keep 2.73e+05/4.88e+06 =  5% of the original kernel matrix.

torch.Size([15433, 2])
We keep 1.35e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([10841, 2])
We keep 9.19e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([21332, 2])
We keep 2.51e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([2427, 2])
We keep 7.41e+04/8.95e+05 =  8% of the original kernel matrix.

torch.Size([11801, 2])
We keep 7.40e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([6661, 2])
We keep 4.30e+05/9.34e+06 =  4% of the original kernel matrix.

torch.Size([17049, 2])
We keep 1.68e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([4133, 2])
We keep 1.70e+05/2.87e+06 =  5% of the original kernel matrix.

torch.Size([14305, 2])
We keep 1.11e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([5555, 2])
We keep 3.53e+05/6.37e+06 =  5% of the original kernel matrix.

torch.Size([15839, 2])
We keep 1.47e+06/6.22e+07 =  2% of the original kernel matrix.

torch.Size([2939, 2])
We keep 9.45e+04/1.50e+06 =  6% of the original kernel matrix.

torch.Size([12467, 2])
We keep 8.91e+05/3.02e+07 =  2% of the original kernel matrix.

torch.Size([4194, 2])
We keep 2.01e+05/3.65e+06 =  5% of the original kernel matrix.

torch.Size([14000, 2])
We keep 1.19e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([8429, 2])
We keep 6.32e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([18856, 2])
We keep 1.98e+06/9.65e+07 =  2% of the original kernel matrix.

torch.Size([9666, 2])
We keep 8.76e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([19914, 2])
We keep 2.29e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([3666, 2])
We keep 1.77e+05/2.75e+06 =  6% of the original kernel matrix.

torch.Size([13452, 2])
We keep 1.10e+06/4.09e+07 =  2% of the original kernel matrix.

torch.Size([5457, 2])
We keep 2.55e+05/5.49e+06 =  4% of the original kernel matrix.

torch.Size([15858, 2])
We keep 1.37e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([2554, 2])
We keep 6.58e+04/1.08e+06 =  6% of the original kernel matrix.

torch.Size([11949, 2])
We keep 8.16e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([13185, 2])
We keep 1.42e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([23832, 2])
We keep 3.00e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([4374, 2])
We keep 2.02e+05/3.63e+06 =  5% of the original kernel matrix.

torch.Size([14305, 2])
We keep 1.22e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([18797, 2])
We keep 2.75e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([28074, 2])
We keep 4.25e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([6692, 2])
We keep 4.62e+05/9.87e+06 =  4% of the original kernel matrix.

torch.Size([17141, 2])
We keep 1.70e+06/7.74e+07 =  2% of the original kernel matrix.

torch.Size([5563, 2])
We keep 3.24e+05/6.45e+06 =  5% of the original kernel matrix.

torch.Size([15877, 2])
We keep 1.48e+06/6.26e+07 =  2% of the original kernel matrix.

torch.Size([6345, 2])
We keep 3.77e+05/8.24e+06 =  4% of the original kernel matrix.

torch.Size([16929, 2])
We keep 1.61e+06/7.08e+07 =  2% of the original kernel matrix.

torch.Size([4604, 2])
We keep 2.37e+05/4.17e+06 =  5% of the original kernel matrix.

torch.Size([14812, 2])
We keep 1.28e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([4052, 2])
We keep 1.51e+05/2.70e+06 =  5% of the original kernel matrix.

torch.Size([14082, 2])
We keep 1.09e+06/4.05e+07 =  2% of the original kernel matrix.

torch.Size([4631, 2])
We keep 1.85e+05/3.65e+06 =  5% of the original kernel matrix.

torch.Size([14857, 2])
We keep 1.21e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([7026, 2])
We keep 4.19e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([17409, 2])
We keep 1.73e+06/7.86e+07 =  2% of the original kernel matrix.

torch.Size([21834, 2])
We keep 3.90e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([30565, 2])
We keep 4.89e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([5358, 2])
We keep 2.46e+05/5.30e+06 =  4% of the original kernel matrix.

torch.Size([15637, 2])
We keep 1.37e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([12690, 2])
We keep 1.28e+06/4.03e+07 =  3% of the original kernel matrix.

torch.Size([22593, 2])
We keep 2.84e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([3890, 2])
We keep 2.03e+05/2.98e+06 =  6% of the original kernel matrix.

torch.Size([13859, 2])
We keep 1.14e+06/4.26e+07 =  2% of the original kernel matrix.

torch.Size([8999, 2])
We keep 7.64e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([19304, 2])
We keep 2.15e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([2939, 2])
We keep 1.09e+05/1.42e+06 =  7% of the original kernel matrix.

torch.Size([12291, 2])
We keep 8.68e+05/2.94e+07 =  2% of the original kernel matrix.

torch.Size([6355, 2])
We keep 3.77e+05/8.27e+06 =  4% of the original kernel matrix.

torch.Size([16758, 2])
We keep 1.63e+06/7.09e+07 =  2% of the original kernel matrix.

torch.Size([5145, 2])
We keep 2.36e+05/4.86e+06 =  4% of the original kernel matrix.

torch.Size([15397, 2])
We keep 1.33e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([3594, 2])
We keep 1.46e+05/2.45e+06 =  5% of the original kernel matrix.

torch.Size([13514, 2])
We keep 1.08e+06/3.86e+07 =  2% of the original kernel matrix.

torch.Size([3889, 2])
We keep 1.60e+05/3.10e+06 =  5% of the original kernel matrix.

torch.Size([13735, 2])
We keep 1.14e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([5358, 2])
We keep 2.59e+05/5.52e+06 =  4% of the original kernel matrix.

torch.Size([15625, 2])
We keep 1.39e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([4545, 2])
We keep 2.25e+05/3.99e+06 =  5% of the original kernel matrix.

torch.Size([14562, 2])
We keep 1.26e+06/4.92e+07 =  2% of the original kernel matrix.

torch.Size([8689, 2])
We keep 6.32e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([19305, 2])
We keep 2.05e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([2864, 2])
We keep 9.41e+04/1.37e+06 =  6% of the original kernel matrix.

torch.Size([12332, 2])
We keep 8.72e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([17509, 2])
We keep 2.71e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([26878, 2])
We keep 4.12e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([14134, 2])
We keep 1.66e+06/5.28e+07 =  3% of the original kernel matrix.

torch.Size([24231, 2])
We keep 3.15e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([12041, 2])
We keep 1.24e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([22633, 2])
We keep 2.85e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([5295, 2])
We keep 2.38e+05/5.18e+06 =  4% of the original kernel matrix.

torch.Size([15483, 2])
We keep 1.35e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([5544, 2])
We keep 2.93e+05/5.99e+06 =  4% of the original kernel matrix.

torch.Size([15926, 2])
We keep 1.43e+06/6.04e+07 =  2% of the original kernel matrix.

torch.Size([7935, 2])
We keep 1.19e+06/1.92e+07 =  6% of the original kernel matrix.

torch.Size([18198, 2])
We keep 2.16e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([4800, 2])
We keep 2.12e+05/4.38e+06 =  4% of the original kernel matrix.

torch.Size([15068, 2])
We keep 1.29e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([5323, 2])
We keep 3.10e+05/5.93e+06 =  5% of the original kernel matrix.

torch.Size([15486, 2])
We keep 1.43e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([4885, 2])
We keep 2.23e+05/4.41e+06 =  5% of the original kernel matrix.

torch.Size([15077, 2])
We keep 1.28e+06/5.18e+07 =  2% of the original kernel matrix.

torch.Size([6999, 2])
We keep 4.62e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([17615, 2])
We keep 1.77e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([8274, 2])
We keep 5.83e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([18723, 2])
We keep 1.97e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([12824, 2])
We keep 3.54e+06/6.68e+07 =  5% of the original kernel matrix.

torch.Size([22731, 2])
We keep 3.47e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([5527, 2])
We keep 2.59e+05/5.60e+06 =  4% of the original kernel matrix.

torch.Size([15866, 2])
We keep 1.40e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([20666, 2])
We keep 4.14e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([29098, 2])
We keep 4.65e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([6570, 2])
We keep 3.37e+05/8.39e+06 =  4% of the original kernel matrix.

torch.Size([17128, 2])
We keep 1.62e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([6205, 2])
We keep 3.60e+05/7.69e+06 =  4% of the original kernel matrix.

torch.Size([16601, 2])
We keep 1.57e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([7926, 2])
We keep 5.56e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([18621, 2])
We keep 1.91e+06/8.92e+07 =  2% of the original kernel matrix.

torch.Size([1856, 2])
We keep 5.36e+04/5.84e+05 =  9% of the original kernel matrix.

torch.Size([10305, 2])
We keep 6.52e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([11580, 2])
We keep 1.22e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([21563, 2])
We keep 2.68e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([3968, 2])
We keep 1.72e+05/2.89e+06 =  5% of the original kernel matrix.

torch.Size([13851, 2])
We keep 1.10e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([10403, 2])
We keep 1.11e+06/2.47e+07 =  4% of the original kernel matrix.

torch.Size([20953, 2])
We keep 2.37e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([2576, 2])
We keep 7.16e+04/1.10e+06 =  6% of the original kernel matrix.

torch.Size([11754, 2])
We keep 8.12e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([7491, 2])
We keep 8.37e+05/1.50e+07 =  5% of the original kernel matrix.

torch.Size([17698, 2])
We keep 1.99e+06/9.56e+07 =  2% of the original kernel matrix.

torch.Size([4345, 2])
We keep 1.97e+05/3.31e+06 =  5% of the original kernel matrix.

torch.Size([14440, 2])
We keep 1.18e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([5236, 2])
We keep 2.24e+05/5.00e+06 =  4% of the original kernel matrix.

torch.Size([15631, 2])
We keep 1.33e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([7465, 2])
We keep 4.92e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([17949, 2])
We keep 1.84e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([9470, 2])
We keep 8.13e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([19667, 2])
We keep 2.21e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([2027, 2])
We keep 4.70e+04/6.48e+05 =  7% of the original kernel matrix.

torch.Size([10836, 2])
We keep 6.83e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([3638, 2])
We keep 1.71e+05/2.77e+06 =  6% of the original kernel matrix.

torch.Size([13320, 2])
We keep 1.12e+06/4.10e+07 =  2% of the original kernel matrix.

torch.Size([3356, 2])
We keep 1.34e+05/2.16e+06 =  6% of the original kernel matrix.

torch.Size([12993, 2])
We keep 9.96e+05/3.62e+07 =  2% of the original kernel matrix.

torch.Size([13583, 2])
We keep 1.50e+06/4.97e+07 =  3% of the original kernel matrix.

torch.Size([23280, 2])
We keep 3.06e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([4781, 2])
We keep 2.87e+05/4.55e+06 =  6% of the original kernel matrix.

torch.Size([15088, 2])
We keep 1.31e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([7990, 2])
We keep 6.11e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([18521, 2])
We keep 1.98e+06/9.49e+07 =  2% of the original kernel matrix.

torch.Size([6283, 2])
We keep 4.33e+05/9.28e+06 =  4% of the original kernel matrix.

torch.Size([16780, 2])
We keep 1.70e+06/7.51e+07 =  2% of the original kernel matrix.

torch.Size([12045, 2])
We keep 1.31e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([21872, 2])
We keep 2.80e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([5384, 2])
We keep 2.97e+05/6.11e+06 =  4% of the original kernel matrix.

torch.Size([15567, 2])
We keep 1.44e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([9553, 2])
We keep 1.24e+06/2.65e+07 =  4% of the original kernel matrix.

torch.Size([19696, 2])
We keep 2.42e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([7473, 2])
We keep 5.90e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([17860, 2])
We keep 1.81e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([8000, 2])
We keep 5.56e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([18694, 2])
We keep 1.93e+06/9.22e+07 =  2% of the original kernel matrix.

torch.Size([5532, 2])
We keep 2.50e+05/5.48e+06 =  4% of the original kernel matrix.

torch.Size([15891, 2])
We keep 1.38e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([6548, 2])
We keep 6.39e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([16664, 2])
We keep 1.76e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([7714, 2])
We keep 7.67e+05/1.27e+07 =  6% of the original kernel matrix.

torch.Size([18005, 2])
We keep 1.82e+06/8.80e+07 =  2% of the original kernel matrix.

torch.Size([11666, 2])
We keep 9.64e+05/3.01e+07 =  3% of the original kernel matrix.

torch.Size([22303, 2])
We keep 2.56e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([10412, 2])
We keep 8.82e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([20888, 2])
We keep 2.45e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([5030, 2])
We keep 2.35e+05/4.94e+06 =  4% of the original kernel matrix.

torch.Size([15457, 2])
We keep 1.37e+06/5.48e+07 =  2% of the original kernel matrix.

torch.Size([8073, 2])
We keep 7.91e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([18747, 2])
We keep 2.03e+06/9.60e+07 =  2% of the original kernel matrix.

torch.Size([13418, 2])
We keep 1.52e+06/4.70e+07 =  3% of the original kernel matrix.

torch.Size([23588, 2])
We keep 3.00e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([20094, 2])
We keep 3.86e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([29112, 2])
We keep 4.71e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([4640, 2])
We keep 2.02e+05/3.96e+06 =  5% of the original kernel matrix.

torch.Size([14794, 2])
We keep 1.24e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([1525, 2])
We keep 2.73e+04/3.33e+05 =  8% of the original kernel matrix.

torch.Size([9935, 2])
We keep 5.57e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([6560, 2])
We keep 3.73e+05/8.52e+06 =  4% of the original kernel matrix.

torch.Size([17055, 2])
We keep 1.64e+06/7.20e+07 =  2% of the original kernel matrix.

torch.Size([4438, 2])
We keep 2.10e+05/4.29e+06 =  4% of the original kernel matrix.

torch.Size([14558, 2])
We keep 1.28e+06/5.11e+07 =  2% of the original kernel matrix.

torch.Size([6039, 2])
We keep 5.17e+05/8.73e+06 =  5% of the original kernel matrix.

torch.Size([16156, 2])
We keep 1.63e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([8498, 2])
We keep 5.87e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([19069, 2])
We keep 1.99e+06/9.53e+07 =  2% of the original kernel matrix.

torch.Size([5880, 2])
We keep 3.00e+05/6.47e+06 =  4% of the original kernel matrix.

torch.Size([16204, 2])
We keep 1.45e+06/6.27e+07 =  2% of the original kernel matrix.

torch.Size([3096, 2])
We keep 1.19e+05/1.76e+06 =  6% of the original kernel matrix.

torch.Size([12655, 2])
We keep 9.54e+05/3.27e+07 =  2% of the original kernel matrix.

torch.Size([4513, 2])
We keep 2.17e+05/4.00e+06 =  5% of the original kernel matrix.

torch.Size([14567, 2])
We keep 1.24e+06/4.93e+07 =  2% of the original kernel matrix.

torch.Size([3393, 2])
We keep 9.23e+04/1.67e+06 =  5% of the original kernel matrix.

torch.Size([13432, 2])
We keep 9.20e+05/3.19e+07 =  2% of the original kernel matrix.

torch.Size([9271, 2])
We keep 6.12e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([20309, 2])
We keep 2.10e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([4316, 2])
We keep 1.58e+05/3.08e+06 =  5% of the original kernel matrix.

torch.Size([14389, 2])
We keep 1.13e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([14677, 2])
We keep 1.53e+06/5.48e+07 =  2% of the original kernel matrix.

torch.Size([24955, 2])
We keep 3.22e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([7635, 2])
We keep 5.30e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([18032, 2])
We keep 1.87e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([10348, 2])
We keep 1.51e+06/2.89e+07 =  5% of the original kernel matrix.

torch.Size([20601, 2])
We keep 2.49e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([3374, 2])
We keep 1.06e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([13273, 2])
We keep 9.24e+05/3.22e+07 =  2% of the original kernel matrix.

torch.Size([9441, 2])
We keep 8.31e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([20210, 2])
We keep 2.24e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([7981, 2])
We keep 4.95e+05/1.24e+07 =  3% of the original kernel matrix.

torch.Size([18891, 2])
We keep 1.86e+06/8.69e+07 =  2% of the original kernel matrix.

torch.Size([7022, 2])
We keep 4.55e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([17503, 2])
We keep 1.75e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([14378, 2])
We keep 1.47e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([24208, 2])
We keep 3.12e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([8778, 2])
We keep 6.92e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([19010, 2])
We keep 2.13e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([7735, 2])
We keep 1.20e+06/1.75e+07 =  6% of the original kernel matrix.

torch.Size([18046, 2])
We keep 2.11e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([6216, 2])
We keep 3.13e+05/7.03e+06 =  4% of the original kernel matrix.

torch.Size([16524, 2])
We keep 1.51e+06/6.54e+07 =  2% of the original kernel matrix.

torch.Size([5871, 2])
We keep 2.90e+05/6.23e+06 =  4% of the original kernel matrix.

torch.Size([16163, 2])
We keep 1.44e+06/6.15e+07 =  2% of the original kernel matrix.

torch.Size([2588, 2])
We keep 7.41e+04/1.08e+06 =  6% of the original kernel matrix.

torch.Size([11895, 2])
We keep 8.08e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([3819, 2])
We keep 1.84e+05/2.93e+06 =  6% of the original kernel matrix.

torch.Size([13607, 2])
We keep 1.12e+06/4.22e+07 =  2% of the original kernel matrix.

torch.Size([2632, 2])
We keep 7.34e+04/1.08e+06 =  6% of the original kernel matrix.

torch.Size([11915, 2])
We keep 8.01e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([2075, 2])
We keep 4.99e+04/6.16e+05 =  8% of the original kernel matrix.

torch.Size([11075, 2])
We keep 6.78e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([11990, 2])
We keep 1.98e+06/3.78e+07 =  5% of the original kernel matrix.

torch.Size([22009, 2])
We keep 2.74e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([6255, 2])
We keep 3.80e+05/7.52e+06 =  5% of the original kernel matrix.

torch.Size([16770, 2])
We keep 1.53e+06/6.76e+07 =  2% of the original kernel matrix.

torch.Size([4871, 2])
We keep 3.61e+05/4.88e+06 =  7% of the original kernel matrix.

torch.Size([15053, 2])
We keep 1.35e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([3392, 2])
We keep 1.35e+05/2.24e+06 =  6% of the original kernel matrix.

torch.Size([13049, 2])
We keep 1.02e+06/3.69e+07 =  2% of the original kernel matrix.

torch.Size([9503, 2])
We keep 8.40e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([19763, 2])
We keep 2.28e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([5211, 2])
We keep 3.65e+05/5.65e+06 =  6% of the original kernel matrix.

torch.Size([15397, 2])
We keep 1.40e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([8844, 2])
We keep 7.67e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([19165, 2])
We keep 2.13e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([8695, 2])
We keep 6.81e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([19379, 2])
We keep 2.14e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([3873, 2])
We keep 1.43e+05/2.73e+06 =  5% of the original kernel matrix.

torch.Size([13807, 2])
We keep 1.10e+06/4.08e+07 =  2% of the original kernel matrix.

torch.Size([3416, 2])
We keep 1.32e+05/2.21e+06 =  5% of the original kernel matrix.

torch.Size([13222, 2])
We keep 1.03e+06/3.66e+07 =  2% of the original kernel matrix.

torch.Size([9561, 2])
We keep 7.40e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([19931, 2])
We keep 2.20e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([2181, 2])
We keep 8.51e+04/1.03e+06 =  8% of the original kernel matrix.

torch.Size([11102, 2])
We keep 7.80e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([3765, 2])
We keep 1.42e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([13626, 2])
We keep 1.04e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([4746, 2])
We keep 2.65e+05/4.96e+06 =  5% of the original kernel matrix.

torch.Size([14838, 2])
We keep 1.34e+06/5.49e+07 =  2% of the original kernel matrix.

torch.Size([7103, 2])
We keep 4.94e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([17801, 2])
We keep 1.79e+06/8.18e+07 =  2% of the original kernel matrix.

torch.Size([9227, 2])
We keep 6.61e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([20319, 2])
We keep 2.12e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([5208, 2])
We keep 2.57e+05/5.04e+06 =  5% of the original kernel matrix.

torch.Size([15569, 2])
We keep 1.35e+06/5.53e+07 =  2% of the original kernel matrix.

torch.Size([4717, 2])
We keep 2.19e+05/4.15e+06 =  5% of the original kernel matrix.

torch.Size([14863, 2])
We keep 1.24e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([7830, 2])
We keep 5.19e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([18507, 2])
We keep 1.82e+06/8.47e+07 =  2% of the original kernel matrix.

torch.Size([12315, 2])
We keep 1.08e+06/3.70e+07 =  2% of the original kernel matrix.

torch.Size([22782, 2])
We keep 2.75e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([3472, 2])
We keep 1.19e+05/2.16e+06 =  5% of the original kernel matrix.

torch.Size([13381, 2])
We keep 1.01e+06/3.62e+07 =  2% of the original kernel matrix.

torch.Size([16376, 2])
We keep 3.43e+06/8.57e+07 =  4% of the original kernel matrix.

torch.Size([25703, 2])
We keep 3.75e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([66276, 2])
We keep 2.02e+08/4.56e+09 =  4% of the original kernel matrix.

torch.Size([49528, 2])
We keep 1.88e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([8639, 2])
We keep 6.61e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([19068, 2])
We keep 2.00e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([8456, 2])
We keep 6.27e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([18700, 2])
We keep 2.00e+06/9.72e+07 =  2% of the original kernel matrix.

torch.Size([2984, 2])
We keep 1.59e+05/1.73e+06 =  9% of the original kernel matrix.

torch.Size([12186, 2])
We keep 9.39e+05/3.24e+07 =  2% of the original kernel matrix.

torch.Size([10717, 2])
We keep 9.13e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([20848, 2])
We keep 2.46e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([18921, 2])
We keep 3.98e+06/1.19e+08 =  3% of the original kernel matrix.

torch.Size([27697, 2])
We keep 4.20e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([184397, 2])
We keep 1.31e+09/3.01e+10 =  4% of the original kernel matrix.

torch.Size([86612, 2])
We keep 4.25e+07/4.28e+09 =  0% of the original kernel matrix.

torch.Size([7592, 2])
We keep 4.45e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([18159, 2])
We keep 1.78e+06/8.23e+07 =  2% of the original kernel matrix.

torch.Size([86649, 2])
We keep 9.60e+07/4.19e+09 =  2% of the original kernel matrix.

torch.Size([57704, 2])
We keep 1.80e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([118468, 2])
We keep 1.49e+08/7.92e+09 =  1% of the original kernel matrix.

torch.Size([68376, 2])
We keep 2.32e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([34507, 2])
We keep 1.32e+07/5.81e+08 =  2% of the original kernel matrix.

torch.Size([38065, 2])
We keep 8.03e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([139139, 2])
We keep 5.86e+08/1.48e+10 =  3% of the original kernel matrix.

torch.Size([75813, 2])
We keep 3.08e+07/3.00e+09 =  1% of the original kernel matrix.

torch.Size([5080, 2])
We keep 2.82e+05/4.92e+06 =  5% of the original kernel matrix.

torch.Size([15496, 2])
We keep 1.34e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([9124, 2])
We keep 1.00e+06/2.29e+07 =  4% of the original kernel matrix.

torch.Size([19656, 2])
We keep 2.33e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([170055, 2])
We keep 3.64e+08/2.16e+10 =  1% of the original kernel matrix.

torch.Size([81929, 2])
We keep 3.67e+07/3.63e+09 =  1% of the original kernel matrix.

torch.Size([621140, 2])
We keep 1.46e+09/1.49e+11 =  0% of the original kernel matrix.

torch.Size([163094, 2])
We keep 8.50e+07/9.53e+09 =  0% of the original kernel matrix.

torch.Size([9104, 2])
We keep 9.93e+05/1.87e+07 =  5% of the original kernel matrix.

torch.Size([19584, 2])
We keep 2.14e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([39656, 2])
We keep 1.18e+07/6.86e+08 =  1% of the original kernel matrix.

torch.Size([41004, 2])
We keep 8.44e+06/6.46e+08 =  1% of the original kernel matrix.

torch.Size([54521, 2])
We keep 1.46e+08/1.86e+09 =  7% of the original kernel matrix.

torch.Size([46275, 2])
We keep 1.29e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([108283, 2])
We keep 2.54e+08/6.88e+09 =  3% of the original kernel matrix.

torch.Size([65871, 2])
We keep 2.20e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([43540, 2])
We keep 3.35e+07/1.16e+09 =  2% of the original kernel matrix.

torch.Size([41735, 2])
We keep 1.06e+07/8.40e+08 =  1% of the original kernel matrix.

torch.Size([33771, 2])
We keep 2.05e+07/7.21e+08 =  2% of the original kernel matrix.

torch.Size([36855, 2])
We keep 8.58e+06/6.62e+08 =  1% of the original kernel matrix.

torch.Size([36204, 2])
We keep 2.00e+07/6.04e+08 =  3% of the original kernel matrix.

torch.Size([39028, 2])
We keep 8.08e+06/6.06e+08 =  1% of the original kernel matrix.

torch.Size([22954, 2])
We keep 8.42e+06/1.96e+08 =  4% of the original kernel matrix.

torch.Size([30702, 2])
We keep 5.20e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([13342, 2])
We keep 6.56e+06/6.85e+07 =  9% of the original kernel matrix.

torch.Size([23167, 2])
We keep 3.43e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([148456, 2])
We keep 8.33e+08/2.83e+10 =  2% of the original kernel matrix.

torch.Size([75374, 2])
We keep 4.15e+07/4.15e+09 =  0% of the original kernel matrix.

torch.Size([8020, 2])
We keep 5.90e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([18254, 2])
We keep 1.89e+06/9.14e+07 =  2% of the original kernel matrix.

torch.Size([7833, 2])
We keep 8.12e+05/1.53e+07 =  5% of the original kernel matrix.

torch.Size([18015, 2])
We keep 1.99e+06/9.64e+07 =  2% of the original kernel matrix.

torch.Size([55356, 2])
We keep 3.04e+07/1.36e+09 =  2% of the original kernel matrix.

torch.Size([47555, 2])
We keep 1.11e+07/9.10e+08 =  1% of the original kernel matrix.

torch.Size([37079, 2])
We keep 1.66e+07/7.11e+08 =  2% of the original kernel matrix.

torch.Size([38650, 2])
We keep 8.61e+06/6.57e+08 =  1% of the original kernel matrix.

torch.Size([361090, 2])
We keep 7.60e+08/6.66e+10 =  1% of the original kernel matrix.

torch.Size([126909, 2])
We keep 5.99e+07/6.36e+09 =  0% of the original kernel matrix.

torch.Size([130854, 2])
We keep 8.01e+07/7.05e+09 =  1% of the original kernel matrix.

torch.Size([72473, 2])
We keep 2.22e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([3248, 2])
We keep 9.27e+04/1.58e+06 =  5% of the original kernel matrix.

torch.Size([13073, 2])
We keep 9.14e+05/3.09e+07 =  2% of the original kernel matrix.

torch.Size([27033, 2])
We keep 1.23e+07/3.03e+08 =  4% of the original kernel matrix.

torch.Size([33643, 2])
We keep 6.23e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([15544, 2])
We keep 2.49e+06/7.29e+07 =  3% of the original kernel matrix.

torch.Size([25285, 2])
We keep 3.54e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([25123, 2])
We keep 6.98e+06/2.57e+08 =  2% of the original kernel matrix.

torch.Size([32071, 2])
We keep 5.70e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([80800, 2])
We keep 3.44e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([56943, 2])
We keep 1.50e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([23830, 2])
We keep 2.74e+07/2.89e+08 =  9% of the original kernel matrix.

torch.Size([31209, 2])
We keep 6.18e+06/4.19e+08 =  1% of the original kernel matrix.

torch.Size([55534, 2])
We keep 2.36e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([47625, 2])
We keep 1.13e+07/9.22e+08 =  1% of the original kernel matrix.

torch.Size([2582, 2])
We keep 9.49e+04/1.06e+06 =  8% of the original kernel matrix.

torch.Size([11568, 2])
We keep 7.81e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([11511, 2])
We keep 1.02e+06/3.00e+07 =  3% of the original kernel matrix.

torch.Size([21683, 2])
We keep 2.55e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([66180, 2])
We keep 8.83e+07/3.20e+09 =  2% of the original kernel matrix.

torch.Size([50033, 2])
We keep 1.62e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([73542, 2])
We keep 5.03e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([54008, 2])
We keep 1.47e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([43037, 2])
We keep 2.41e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([41355, 2])
We keep 1.00e+07/7.86e+08 =  1% of the original kernel matrix.

torch.Size([9756, 2])
We keep 1.01e+06/2.44e+07 =  4% of the original kernel matrix.

torch.Size([19995, 2])
We keep 2.36e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([335101, 2])
We keep 4.42e+08/5.80e+10 =  0% of the original kernel matrix.

torch.Size([119745, 2])
We keep 5.53e+07/5.94e+09 =  0% of the original kernel matrix.

torch.Size([20116, 2])
We keep 2.70e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([28582, 2])
We keep 4.34e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([33197, 2])
We keep 6.95e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([37077, 2])
We keep 6.88e+06/5.12e+08 =  1% of the original kernel matrix.

torch.Size([132136, 2])
We keep 1.04e+08/8.15e+09 =  1% of the original kernel matrix.

torch.Size([72795, 2])
We keep 2.36e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([5949, 2])
We keep 4.38e+05/7.05e+06 =  6% of the original kernel matrix.

torch.Size([16364, 2])
We keep 1.51e+06/6.55e+07 =  2% of the original kernel matrix.

torch.Size([98734, 2])
We keep 6.21e+07/4.10e+09 =  1% of the original kernel matrix.

torch.Size([61987, 2])
We keep 1.76e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([285611, 2])
We keep 7.49e+08/5.10e+10 =  1% of the original kernel matrix.

torch.Size([110125, 2])
We keep 5.35e+07/5.57e+09 =  0% of the original kernel matrix.

torch.Size([278850, 2])
We keep 7.07e+08/4.56e+10 =  1% of the original kernel matrix.

torch.Size([108581, 2])
We keep 5.09e+07/5.26e+09 =  0% of the original kernel matrix.

torch.Size([23243, 2])
We keep 5.37e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([31201, 2])
We keep 5.49e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([25364, 2])
We keep 5.57e+06/2.51e+08 =  2% of the original kernel matrix.

torch.Size([32291, 2])
We keep 5.67e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([15546, 2])
We keep 3.29e+06/9.28e+07 =  3% of the original kernel matrix.

torch.Size([24895, 2])
We keep 3.89e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([9630, 2])
We keep 7.42e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([19747, 2])
We keep 2.22e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([67358, 2])
We keep 6.39e+07/2.67e+09 =  2% of the original kernel matrix.

torch.Size([51331, 2])
We keep 1.46e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([15628, 2])
We keep 2.13e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([25110, 2])
We keep 3.64e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([27515, 2])
We keep 7.34e+06/3.24e+08 =  2% of the original kernel matrix.

torch.Size([33457, 2])
We keep 6.21e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([48872, 2])
We keep 3.66e+07/1.18e+09 =  3% of the original kernel matrix.

torch.Size([44795, 2])
We keep 9.71e+06/8.47e+08 =  1% of the original kernel matrix.

torch.Size([45683, 2])
We keep 5.26e+07/1.15e+09 =  4% of the original kernel matrix.

torch.Size([42975, 2])
We keep 1.06e+07/8.37e+08 =  1% of the original kernel matrix.

torch.Size([15200, 2])
We keep 5.25e+06/1.22e+08 =  4% of the original kernel matrix.

torch.Size([24626, 2])
We keep 4.29e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([13329, 2])
We keep 1.34e+06/4.77e+07 =  2% of the original kernel matrix.

torch.Size([23227, 2])
We keep 3.02e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([60524, 2])
We keep 2.99e+08/8.53e+09 =  3% of the original kernel matrix.

torch.Size([44323, 2])
We keep 2.44e+07/2.28e+09 =  1% of the original kernel matrix.

torch.Size([17874, 2])
We keep 5.89e+06/1.24e+08 =  4% of the original kernel matrix.

torch.Size([27145, 2])
We keep 4.30e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([18789, 2])
We keep 1.45e+07/1.59e+08 =  9% of the original kernel matrix.

torch.Size([27556, 2])
We keep 4.73e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([20700, 2])
We keep 3.14e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([30211, 2])
We keep 4.56e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([442035, 2])
We keep 1.17e+09/1.08e+11 =  1% of the original kernel matrix.

torch.Size([136401, 2])
We keep 7.33e+07/8.11e+09 =  0% of the original kernel matrix.

torch.Size([14762, 2])
We keep 2.43e+06/6.81e+07 =  3% of the original kernel matrix.

torch.Size([24271, 2])
We keep 3.50e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([139859, 2])
We keep 3.38e+08/1.48e+10 =  2% of the original kernel matrix.

torch.Size([73810, 2])
We keep 3.01e+07/3.00e+09 =  1% of the original kernel matrix.

torch.Size([8400, 2])
We keep 1.20e+06/2.31e+07 =  5% of the original kernel matrix.

torch.Size([18564, 2])
We keep 2.36e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([27950, 2])
We keep 2.88e+07/7.41e+08 =  3% of the original kernel matrix.

torch.Size([31690, 2])
We keep 8.87e+06/6.71e+08 =  1% of the original kernel matrix.

torch.Size([21445, 2])
We keep 5.90e+06/1.72e+08 =  3% of the original kernel matrix.

torch.Size([29965, 2])
We keep 4.87e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([69163, 2])
We keep 1.75e+08/5.31e+09 =  3% of the original kernel matrix.

torch.Size([49087, 2])
We keep 2.01e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([64025, 2])
We keep 2.57e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([50526, 2])
We keep 1.33e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([8949, 2])
We keep 7.02e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([19388, 2])
We keep 2.17e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([176002, 2])
We keep 1.31e+08/1.38e+10 =  0% of the original kernel matrix.

torch.Size([85034, 2])
We keep 2.98e+07/2.90e+09 =  1% of the original kernel matrix.

torch.Size([138558, 2])
We keep 8.05e+07/7.82e+09 =  1% of the original kernel matrix.

torch.Size([74989, 2])
We keep 2.32e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([8107, 2])
We keep 9.56e+05/1.53e+07 =  6% of the original kernel matrix.

torch.Size([18590, 2])
We keep 1.99e+06/9.65e+07 =  2% of the original kernel matrix.

torch.Size([125101, 2])
We keep 9.99e+07/6.35e+09 =  1% of the original kernel matrix.

torch.Size([70271, 2])
We keep 2.11e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([162131, 2])
We keep 1.21e+08/1.28e+10 =  0% of the original kernel matrix.

torch.Size([84115, 2])
We keep 2.86e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([377482, 2])
We keep 5.08e+08/6.70e+10 =  0% of the original kernel matrix.

torch.Size([129795, 2])
We keep 5.95e+07/6.38e+09 =  0% of the original kernel matrix.

torch.Size([88982, 2])
We keep 5.58e+08/8.20e+09 =  6% of the original kernel matrix.

torch.Size([58626, 2])
We keep 2.45e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([6070, 2])
We keep 1.40e+06/1.13e+07 = 12% of the original kernel matrix.

torch.Size([16173, 2])
We keep 1.71e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([16629, 2])
We keep 1.97e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([25664, 2])
We keep 3.67e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([12701, 2])
We keep 2.10e+06/5.90e+07 =  3% of the original kernel matrix.

torch.Size([22801, 2])
We keep 3.22e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([49572, 2])
We keep 3.12e+07/1.12e+09 =  2% of the original kernel matrix.

torch.Size([44809, 2])
We keep 1.02e+07/8.26e+08 =  1% of the original kernel matrix.

torch.Size([14847, 2])
We keep 2.39e+06/6.39e+07 =  3% of the original kernel matrix.

torch.Size([24376, 2])
We keep 3.33e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([4770, 2])
We keep 1.96e+05/3.90e+06 =  5% of the original kernel matrix.

torch.Size([15094, 2])
We keep 1.26e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([75101, 2])
We keep 3.53e+07/2.47e+09 =  1% of the original kernel matrix.

torch.Size([54763, 2])
We keep 1.41e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([188295, 2])
We keep 1.56e+08/1.59e+10 =  0% of the original kernel matrix.

torch.Size([88161, 2])
We keep 3.16e+07/3.11e+09 =  1% of the original kernel matrix.

torch.Size([34742, 2])
We keep 9.51e+07/2.01e+09 =  4% of the original kernel matrix.

torch.Size([34930, 2])
We keep 1.33e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([10187, 2])
We keep 1.10e+06/2.79e+07 =  3% of the original kernel matrix.

torch.Size([20315, 2])
We keep 2.47e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([33680, 2])
We keep 9.49e+06/5.13e+08 =  1% of the original kernel matrix.

torch.Size([36260, 2])
We keep 7.26e+06/5.58e+08 =  1% of the original kernel matrix.

torch.Size([18822, 2])
We keep 2.86e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([28513, 2])
We keep 4.24e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([14948, 2])
We keep 2.82e+06/7.35e+07 =  3% of the original kernel matrix.

torch.Size([24903, 2])
We keep 3.55e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([15821, 2])
We keep 2.15e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([25794, 2])
We keep 3.65e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([8916, 2])
We keep 7.00e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([19347, 2])
We keep 2.08e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([151410, 2])
We keep 4.23e+08/1.77e+10 =  2% of the original kernel matrix.

torch.Size([77445, 2])
We keep 3.34e+07/3.28e+09 =  1% of the original kernel matrix.

torch.Size([53054, 2])
We keep 1.50e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([46879, 2])
We keep 1.03e+07/8.37e+08 =  1% of the original kernel matrix.

torch.Size([25758, 2])
We keep 7.44e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([32879, 2])
We keep 5.96e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([30182, 2])
We keep 1.56e+07/4.49e+08 =  3% of the original kernel matrix.

torch.Size([35492, 2])
We keep 7.33e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([12331, 2])
We keep 2.20e+06/5.15e+07 =  4% of the original kernel matrix.

torch.Size([22087, 2])
We keep 3.04e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([596422, 2])
We keep 1.51e+09/1.56e+11 =  0% of the original kernel matrix.

torch.Size([161065, 2])
We keep 8.72e+07/9.74e+09 =  0% of the original kernel matrix.

torch.Size([13528, 2])
We keep 1.58e+06/4.78e+07 =  3% of the original kernel matrix.

torch.Size([23396, 2])
We keep 3.03e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([137245, 2])
We keep 1.04e+08/8.09e+09 =  1% of the original kernel matrix.

torch.Size([74043, 2])
We keep 2.38e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([3284, 2])
We keep 1.30e+05/2.10e+06 =  6% of the original kernel matrix.

torch.Size([12860, 2])
We keep 9.98e+05/3.57e+07 =  2% of the original kernel matrix.

torch.Size([8850, 2])
We keep 7.21e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([19201, 2])
We keep 2.11e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([6230, 2])
We keep 3.62e+05/8.09e+06 =  4% of the original kernel matrix.

torch.Size([16847, 2])
We keep 1.62e+06/7.01e+07 =  2% of the original kernel matrix.

torch.Size([14316, 2])
We keep 1.43e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([24095, 2])
We keep 3.09e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([17397, 2])
We keep 1.30e+07/1.23e+08 = 10% of the original kernel matrix.

torch.Size([26846, 2])
We keep 4.36e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([36290, 2])
We keep 5.67e+07/8.95e+08 =  6% of the original kernel matrix.

torch.Size([37953, 2])
We keep 9.48e+06/7.38e+08 =  1% of the original kernel matrix.

torch.Size([9476, 2])
We keep 7.51e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([19797, 2])
We keep 2.21e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([8179, 2])
We keep 5.91e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([18637, 2])
We keep 2.00e+06/9.59e+07 =  2% of the original kernel matrix.

torch.Size([364210, 2])
We keep 7.35e+08/6.38e+10 =  1% of the original kernel matrix.

torch.Size([125410, 2])
We keep 5.79e+07/6.23e+09 =  0% of the original kernel matrix.

torch.Size([49127, 2])
We keep 1.95e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([44670, 2])
We keep 1.03e+07/8.13e+08 =  1% of the original kernel matrix.

torch.Size([34438, 2])
We keep 1.80e+07/6.61e+08 =  2% of the original kernel matrix.

torch.Size([38208, 2])
We keep 8.67e+06/6.34e+08 =  1% of the original kernel matrix.

torch.Size([99460, 2])
We keep 8.51e+07/4.80e+09 =  1% of the original kernel matrix.

torch.Size([62812, 2])
We keep 1.92e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([136865, 2])
We keep 3.16e+08/1.93e+10 =  1% of the original kernel matrix.

torch.Size([71024, 2])
We keep 3.48e+07/3.42e+09 =  1% of the original kernel matrix.

torch.Size([6899, 2])
We keep 4.43e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([17154, 2])
We keep 1.77e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([10560, 2])
We keep 1.16e+06/2.81e+07 =  4% of the original kernel matrix.

torch.Size([20644, 2])
We keep 2.46e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([22746, 2])
We keep 1.29e+07/2.72e+08 =  4% of the original kernel matrix.

torch.Size([30144, 2])
We keep 5.92e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([3734, 2])
We keep 1.50e+05/2.61e+06 =  5% of the original kernel matrix.

torch.Size([13585, 2])
We keep 1.07e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([12030, 2])
We keep 1.09e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([22112, 2])
We keep 2.69e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([23879, 2])
We keep 7.05e+06/2.66e+08 =  2% of the original kernel matrix.

torch.Size([31240, 2])
We keep 5.90e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([9372, 2])
We keep 1.61e+06/2.93e+07 =  5% of the original kernel matrix.

torch.Size([19440, 2])
We keep 2.54e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([9969, 2])
We keep 7.69e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([20413, 2])
We keep 2.25e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([8311, 2])
We keep 7.14e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([18428, 2])
We keep 2.04e+06/9.89e+07 =  2% of the original kernel matrix.

torch.Size([13117, 2])
We keep 1.56e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([23006, 2])
We keep 2.92e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([532776, 2])
We keep 2.29e+09/1.67e+11 =  1% of the original kernel matrix.

torch.Size([147579, 2])
We keep 9.00e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([23011, 2])
We keep 1.14e+07/2.53e+08 =  4% of the original kernel matrix.

torch.Size([30277, 2])
We keep 5.80e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([88197, 2])
We keep 8.84e+07/4.03e+09 =  2% of the original kernel matrix.

torch.Size([59792, 2])
We keep 1.77e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([16983, 2])
We keep 1.09e+07/1.58e+08 =  6% of the original kernel matrix.

torch.Size([26209, 2])
We keep 4.82e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([8256, 2])
We keep 6.98e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([18644, 2])
We keep 2.08e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([21681, 2])
We keep 7.26e+06/2.23e+08 =  3% of the original kernel matrix.

torch.Size([29934, 2])
We keep 5.48e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([413840, 2])
We keep 1.69e+09/9.32e+10 =  1% of the original kernel matrix.

torch.Size([131934, 2])
We keep 6.76e+07/7.53e+09 =  0% of the original kernel matrix.

torch.Size([696089, 2])
We keep 1.25e+09/1.97e+11 =  0% of the original kernel matrix.

torch.Size([175083, 2])
We keep 9.59e+07/1.09e+10 =  0% of the original kernel matrix.

torch.Size([41513, 2])
We keep 1.85e+07/7.34e+08 =  2% of the original kernel matrix.

torch.Size([41467, 2])
We keep 8.69e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([44678, 2])
We keep 2.60e+07/9.42e+08 =  2% of the original kernel matrix.

torch.Size([43337, 2])
We keep 9.81e+06/7.57e+08 =  1% of the original kernel matrix.

torch.Size([5212, 2])
We keep 2.23e+05/4.63e+06 =  4% of the original kernel matrix.

torch.Size([15533, 2])
We keep 1.30e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([40818, 2])
We keep 1.19e+08/2.02e+09 =  5% of the original kernel matrix.

torch.Size([38589, 2])
We keep 1.34e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([242326, 2])
We keep 2.03e+09/4.74e+10 =  4% of the original kernel matrix.

torch.Size([99260, 2])
We keep 5.15e+07/5.37e+09 =  0% of the original kernel matrix.

torch.Size([92931, 2])
We keep 1.76e+08/5.10e+09 =  3% of the original kernel matrix.

torch.Size([60951, 2])
We keep 1.94e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([10414, 2])
We keep 9.74e+05/2.85e+07 =  3% of the original kernel matrix.

torch.Size([20273, 2])
We keep 2.50e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([117481, 2])
We keep 1.02e+08/6.56e+09 =  1% of the original kernel matrix.

torch.Size([68602, 2])
We keep 2.16e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([433349, 2])
We keep 1.03e+09/9.47e+10 =  1% of the original kernel matrix.

torch.Size([135736, 2])
We keep 6.97e+07/7.59e+09 =  0% of the original kernel matrix.

torch.Size([91421, 2])
We keep 9.09e+07/4.21e+09 =  2% of the original kernel matrix.

torch.Size([58974, 2])
We keep 1.76e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([13597, 2])
We keep 2.59e+06/6.64e+07 =  3% of the original kernel matrix.

torch.Size([23294, 2])
We keep 3.46e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([25601, 2])
We keep 5.44e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([33857, 2])
We keep 5.30e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([18005, 2])
We keep 2.56e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([27160, 2])
We keep 4.03e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([6970, 2])
We keep 3.97e+05/9.56e+06 =  4% of the original kernel matrix.

torch.Size([17425, 2])
We keep 1.68e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([179157, 2])
We keep 3.01e+08/1.68e+10 =  1% of the original kernel matrix.

torch.Size([86405, 2])
We keep 3.26e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([23590, 2])
We keep 7.93e+06/2.55e+08 =  3% of the original kernel matrix.

torch.Size([31172, 2])
We keep 5.78e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([3522, 2])
We keep 1.18e+05/2.11e+06 =  5% of the original kernel matrix.

torch.Size([13322, 2])
We keep 1.01e+06/3.58e+07 =  2% of the original kernel matrix.

torch.Size([53549, 2])
We keep 1.90e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([47274, 2])
We keep 1.06e+07/8.69e+08 =  1% of the original kernel matrix.

torch.Size([27573, 2])
We keep 4.85e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([35408, 2])
We keep 5.66e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([3138, 2])
We keep 9.68e+04/1.55e+06 =  6% of the original kernel matrix.

torch.Size([12800, 2])
We keep 8.97e+05/3.07e+07 =  2% of the original kernel matrix.

torch.Size([102950, 2])
We keep 1.66e+08/5.75e+09 =  2% of the original kernel matrix.

torch.Size([63330, 2])
We keep 2.05e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([170829, 2])
We keep 2.04e+08/1.38e+10 =  1% of the original kernel matrix.

torch.Size([83834, 2])
We keep 2.95e+07/2.90e+09 =  1% of the original kernel matrix.

torch.Size([93762, 2])
We keep 1.93e+08/4.77e+09 =  4% of the original kernel matrix.

torch.Size([62043, 2])
We keep 1.91e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([167956, 2])
We keep 2.62e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([83454, 2])
We keep 3.03e+07/2.94e+09 =  1% of the original kernel matrix.

torch.Size([160513, 2])
We keep 1.81e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([80742, 2])
We keep 2.84e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([22774, 2])
We keep 1.02e+07/2.81e+08 =  3% of the original kernel matrix.

torch.Size([30476, 2])
We keep 6.06e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([61347, 2])
We keep 4.13e+07/1.83e+09 =  2% of the original kernel matrix.

torch.Size([49488, 2])
We keep 1.28e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([135811, 2])
We keep 8.50e+07/8.01e+09 =  1% of the original kernel matrix.

torch.Size([73775, 2])
We keep 2.34e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([226443, 2])
We keep 2.17e+08/2.30e+10 =  0% of the original kernel matrix.

torch.Size([97734, 2])
We keep 3.67e+07/3.74e+09 =  0% of the original kernel matrix.

torch.Size([15104, 2])
We keep 1.70e+06/6.05e+07 =  2% of the original kernel matrix.

torch.Size([24802, 2])
We keep 3.28e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([58443, 2])
We keep 4.13e+07/1.89e+09 =  2% of the original kernel matrix.

torch.Size([49313, 2])
We keep 1.23e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([152566, 2])
We keep 3.12e+08/1.61e+10 =  1% of the original kernel matrix.

torch.Size([78678, 2])
We keep 3.10e+07/3.13e+09 =  0% of the original kernel matrix.

torch.Size([76017, 2])
We keep 6.26e+07/2.80e+09 =  2% of the original kernel matrix.

torch.Size([55010, 2])
We keep 1.50e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([37948, 2])
We keep 3.70e+07/1.16e+09 =  3% of the original kernel matrix.

torch.Size([38673, 2])
We keep 1.04e+07/8.39e+08 =  1% of the original kernel matrix.

torch.Size([27338, 2])
We keep 1.39e+08/1.08e+09 = 12% of the original kernel matrix.

torch.Size([31472, 2])
We keep 1.04e+07/8.10e+08 =  1% of the original kernel matrix.

torch.Size([15124, 2])
We keep 2.39e+06/7.15e+07 =  3% of the original kernel matrix.

torch.Size([24701, 2])
We keep 3.54e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([10090, 2])
We keep 8.45e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([20092, 2])
We keep 2.32e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([31145, 2])
We keep 2.81e+07/6.58e+08 =  4% of the original kernel matrix.

torch.Size([34851, 2])
We keep 8.18e+06/6.32e+08 =  1% of the original kernel matrix.

torch.Size([195064, 2])
We keep 1.61e+08/1.76e+10 =  0% of the original kernel matrix.

torch.Size([90155, 2])
We keep 3.32e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([137726, 2])
We keep 8.07e+07/7.84e+09 =  1% of the original kernel matrix.

torch.Size([74880, 2])
We keep 2.32e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([712131, 2])
We keep 3.28e+09/2.42e+11 =  1% of the original kernel matrix.

torch.Size([177220, 2])
We keep 1.05e+08/1.21e+10 =  0% of the original kernel matrix.

torch.Size([138805, 2])
We keep 4.00e+08/1.29e+10 =  3% of the original kernel matrix.

torch.Size([75976, 2])
We keep 2.97e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([104541, 2])
We keep 1.23e+08/5.28e+09 =  2% of the original kernel matrix.

torch.Size([64534, 2])
We keep 1.97e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([22246, 2])
We keep 4.85e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([29860, 2])
We keep 5.40e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([131825, 2])
We keep 1.13e+08/7.78e+09 =  1% of the original kernel matrix.

torch.Size([73133, 2])
We keep 2.33e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([6211, 2])
We keep 6.05e+05/9.85e+06 =  6% of the original kernel matrix.

torch.Size([16270, 2])
We keep 1.67e+06/7.74e+07 =  2% of the original kernel matrix.

torch.Size([17076, 2])
We keep 2.79e+06/9.00e+07 =  3% of the original kernel matrix.

torch.Size([26318, 2])
We keep 3.82e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([269577, 2])
We keep 3.88e+08/3.55e+10 =  1% of the original kernel matrix.

torch.Size([107087, 2])
We keep 4.52e+07/4.65e+09 =  0% of the original kernel matrix.

torch.Size([18243, 2])
We keep 3.72e+06/1.20e+08 =  3% of the original kernel matrix.

torch.Size([27185, 2])
We keep 4.23e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([7452, 2])
We keep 4.54e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([17908, 2])
We keep 1.77e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([15607, 2])
We keep 2.80e+06/9.38e+07 =  2% of the original kernel matrix.

torch.Size([25115, 2])
We keep 3.83e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([109233, 2])
We keep 3.17e+08/1.06e+10 =  2% of the original kernel matrix.

torch.Size([65306, 2])
We keep 2.67e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([39317, 2])
We keep 2.00e+07/7.35e+08 =  2% of the original kernel matrix.

torch.Size([40040, 2])
We keep 8.66e+06/6.69e+08 =  1% of the original kernel matrix.

torch.Size([108176, 2])
We keep 1.21e+08/6.09e+09 =  1% of the original kernel matrix.

torch.Size([66194, 2])
We keep 2.13e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([11813, 2])
We keep 6.53e+06/6.87e+07 =  9% of the original kernel matrix.

torch.Size([21464, 2])
We keep 3.39e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([16738, 2])
We keep 6.86e+06/1.06e+08 =  6% of the original kernel matrix.

torch.Size([26051, 2])
We keep 4.13e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([8590, 2])
We keep 7.85e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([19109, 2])
We keep 2.14e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([18210, 2])
We keep 1.97e+07/1.59e+08 = 12% of the original kernel matrix.

torch.Size([27310, 2])
We keep 4.91e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([35690, 2])
We keep 1.03e+07/5.98e+08 =  1% of the original kernel matrix.

torch.Size([36805, 2])
We keep 7.57e+06/6.03e+08 =  1% of the original kernel matrix.

torch.Size([68770, 2])
We keep 9.36e+07/2.57e+09 =  3% of the original kernel matrix.

torch.Size([51489, 2])
We keep 1.48e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([6143, 2])
We keep 3.63e+05/7.83e+06 =  4% of the original kernel matrix.

torch.Size([16536, 2])
We keep 1.59e+06/6.90e+07 =  2% of the original kernel matrix.

torch.Size([183776, 2])
We keep 2.29e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([86981, 2])
We keep 3.16e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([6434, 2])
We keep 4.64e+05/9.14e+06 =  5% of the original kernel matrix.

torch.Size([16812, 2])
We keep 1.66e+06/7.45e+07 =  2% of the original kernel matrix.

torch.Size([28466, 2])
We keep 7.49e+06/3.56e+08 =  2% of the original kernel matrix.

torch.Size([34648, 2])
We keep 6.52e+06/4.66e+08 =  1% of the original kernel matrix.

torch.Size([9754, 2])
We keep 2.77e+06/3.04e+07 =  9% of the original kernel matrix.

torch.Size([20014, 2])
We keep 2.51e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([134875, 2])
We keep 8.77e+07/7.92e+09 =  1% of the original kernel matrix.

torch.Size([73520, 2])
We keep 2.35e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([9261, 2])
We keep 1.26e+06/2.45e+07 =  5% of the original kernel matrix.

torch.Size([19660, 2])
We keep 2.42e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([50938, 2])
We keep 4.56e+07/1.40e+09 =  3% of the original kernel matrix.

torch.Size([45033, 2])
We keep 1.15e+07/9.21e+08 =  1% of the original kernel matrix.

torch.Size([19683, 2])
We keep 4.24e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([28314, 2])
We keep 4.51e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([24053, 2])
We keep 1.03e+07/2.54e+08 =  4% of the original kernel matrix.

torch.Size([31981, 2])
We keep 5.74e+06/3.93e+08 =  1% of the original kernel matrix.

torch.Size([65196, 2])
We keep 3.35e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([51261, 2])
We keep 1.32e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([465750, 2])
We keep 1.57e+09/1.18e+11 =  1% of the original kernel matrix.

torch.Size([140719, 2])
We keep 7.71e+07/8.48e+09 =  0% of the original kernel matrix.

torch.Size([11922, 2])
We keep 1.46e+06/3.53e+07 =  4% of the original kernel matrix.

torch.Size([22334, 2])
We keep 2.75e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([14359, 2])
We keep 6.95e+06/9.22e+07 =  7% of the original kernel matrix.

torch.Size([23868, 2])
We keep 3.92e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([119031, 2])
We keep 1.91e+08/9.63e+09 =  1% of the original kernel matrix.

torch.Size([68086, 2])
We keep 2.60e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([15418, 2])
We keep 2.67e+06/8.05e+07 =  3% of the original kernel matrix.

torch.Size([24879, 2])
We keep 3.70e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([590759, 2])
We keep 1.10e+09/1.42e+11 =  0% of the original kernel matrix.

torch.Size([160329, 2])
We keep 8.33e+07/9.30e+09 =  0% of the original kernel matrix.

torch.Size([4263, 2])
We keep 1.89e+05/3.36e+06 =  5% of the original kernel matrix.

torch.Size([14441, 2])
We keep 1.18e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([131532, 2])
We keep 9.81e+07/8.58e+09 =  1% of the original kernel matrix.

torch.Size([72429, 2])
We keep 2.44e+07/2.28e+09 =  1% of the original kernel matrix.

torch.Size([251449, 2])
We keep 2.45e+08/3.07e+10 =  0% of the original kernel matrix.

torch.Size([103599, 2])
We keep 4.22e+07/4.32e+09 =  0% of the original kernel matrix.

torch.Size([6181, 2])
We keep 3.66e+05/8.44e+06 =  4% of the original kernel matrix.

torch.Size([16538, 2])
We keep 1.60e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([230365, 2])
We keep 5.47e+08/3.43e+10 =  1% of the original kernel matrix.

torch.Size([96331, 2])
We keep 4.41e+07/4.56e+09 =  0% of the original kernel matrix.

torch.Size([20013, 2])
We keep 1.54e+07/3.72e+08 =  4% of the original kernel matrix.

torch.Size([26598, 2])
We keep 6.71e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([372634, 2])
We keep 1.61e+09/8.94e+10 =  1% of the original kernel matrix.

torch.Size([125279, 2])
We keep 6.88e+07/7.37e+09 =  0% of the original kernel matrix.

torch.Size([13891, 2])
We keep 1.50e+06/4.85e+07 =  3% of the original kernel matrix.

torch.Size([23756, 2])
We keep 3.04e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([49794, 2])
We keep 2.89e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([44723, 2])
We keep 1.08e+07/8.63e+08 =  1% of the original kernel matrix.

torch.Size([4733, 2])
We keep 3.98e+05/4.68e+06 =  8% of the original kernel matrix.

torch.Size([14658, 2])
We keep 1.30e+06/5.33e+07 =  2% of the original kernel matrix.

torch.Size([8842, 2])
We keep 8.52e+05/1.97e+07 =  4% of the original kernel matrix.

torch.Size([19488, 2])
We keep 2.26e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([10428, 2])
We keep 1.78e+06/3.24e+07 =  5% of the original kernel matrix.

torch.Size([20507, 2])
We keep 2.64e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([37858, 2])
We keep 1.17e+07/6.13e+08 =  1% of the original kernel matrix.

torch.Size([39499, 2])
We keep 8.00e+06/6.11e+08 =  1% of the original kernel matrix.

torch.Size([23802, 2])
We keep 1.02e+07/2.73e+08 =  3% of the original kernel matrix.

torch.Size([30994, 2])
We keep 5.99e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([133583, 2])
We keep 1.59e+08/9.21e+09 =  1% of the original kernel matrix.

torch.Size([73502, 2])
We keep 2.51e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([24314, 2])
We keep 4.48e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([31974, 2])
We keep 5.36e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([9154, 2])
We keep 6.72e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([19447, 2])
We keep 2.10e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([13767, 2])
We keep 2.59e+06/5.60e+07 =  4% of the original kernel matrix.

torch.Size([23477, 2])
We keep 3.19e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([92406, 2])
We keep 2.35e+08/7.25e+09 =  3% of the original kernel matrix.

torch.Size([60570, 2])
We keep 2.29e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([22835, 2])
We keep 3.72e+07/6.10e+08 =  6% of the original kernel matrix.

torch.Size([29958, 2])
We keep 8.30e+06/6.09e+08 =  1% of the original kernel matrix.

torch.Size([227557, 2])
We keep 2.55e+08/2.39e+10 =  1% of the original kernel matrix.

torch.Size([98411, 2])
We keep 3.80e+07/3.81e+09 =  0% of the original kernel matrix.

torch.Size([62005, 2])
We keep 6.30e+07/1.95e+09 =  3% of the original kernel matrix.

torch.Size([50070, 2])
We keep 1.32e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([16692, 2])
We keep 5.17e+06/1.25e+08 =  4% of the original kernel matrix.

torch.Size([25833, 2])
We keep 4.27e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([7709, 2])
We keep 5.47e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([18630, 2])
We keep 1.85e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([11638, 2])
We keep 2.75e+06/5.70e+07 =  4% of the original kernel matrix.

torch.Size([21712, 2])
We keep 3.25e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([691952, 2])
We keep 2.25e+09/2.11e+11 =  1% of the original kernel matrix.

torch.Size([174270, 2])
We keep 1.00e+08/1.13e+10 =  0% of the original kernel matrix.

torch.Size([13149, 2])
We keep 1.46e+06/4.39e+07 =  3% of the original kernel matrix.

torch.Size([23052, 2])
We keep 2.94e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([27012, 2])
We keep 7.49e+06/3.06e+08 =  2% of the original kernel matrix.

torch.Size([33555, 2])
We keep 6.18e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([99753, 2])
We keep 4.21e+08/6.43e+09 =  6% of the original kernel matrix.

torch.Size([61437, 2])
We keep 2.18e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([30259, 2])
We keep 1.85e+07/5.95e+08 =  3% of the original kernel matrix.

torch.Size([34449, 2])
We keep 8.16e+06/6.01e+08 =  1% of the original kernel matrix.

torch.Size([5753, 2])
We keep 3.54e+05/6.98e+06 =  5% of the original kernel matrix.

torch.Size([16009, 2])
We keep 1.51e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([8594, 2])
We keep 9.22e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([18986, 2])
We keep 2.20e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([250733, 2])
We keep 8.50e+08/4.45e+10 =  1% of the original kernel matrix.

torch.Size([101642, 2])
We keep 5.00e+07/5.20e+09 =  0% of the original kernel matrix.

torch.Size([37868, 2])
We keep 1.24e+07/6.30e+08 =  1% of the original kernel matrix.

torch.Size([39903, 2])
We keep 8.18e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([41494, 2])
We keep 3.87e+07/1.24e+09 =  3% of the original kernel matrix.

torch.Size([39765, 2])
We keep 1.09e+07/8.67e+08 =  1% of the original kernel matrix.

torch.Size([87655, 2])
We keep 5.49e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([58547, 2])
We keep 1.58e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([94702, 2])
We keep 4.51e+08/5.24e+09 =  8% of the original kernel matrix.

torch.Size([63495, 2])
We keep 1.99e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([103297, 2])
We keep 1.30e+08/5.06e+09 =  2% of the original kernel matrix.

torch.Size([63800, 2])
We keep 1.96e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([238306, 2])
We keep 6.26e+08/3.99e+10 =  1% of the original kernel matrix.

torch.Size([99703, 2])
We keep 4.74e+07/4.93e+09 =  0% of the original kernel matrix.

torch.Size([98202, 2])
We keep 5.05e+07/3.72e+09 =  1% of the original kernel matrix.

torch.Size([62386, 2])
We keep 1.69e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([9377, 2])
We keep 1.39e+06/2.49e+07 =  5% of the original kernel matrix.

torch.Size([19968, 2])
We keep 2.41e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([64526, 2])
We keep 2.34e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([51166, 2])
We keep 1.24e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([16679, 2])
We keep 4.42e+06/1.06e+08 =  4% of the original kernel matrix.

torch.Size([26068, 2])
We keep 4.08e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([18848, 2])
We keep 5.23e+06/1.20e+08 =  4% of the original kernel matrix.

torch.Size([27817, 2])
We keep 4.38e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([21852, 2])
We keep 4.63e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([29780, 2])
We keep 5.15e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([11512, 2])
We keep 1.11e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([21595, 2])
We keep 2.63e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([60033, 2])
We keep 1.03e+08/2.51e+09 =  4% of the original kernel matrix.

torch.Size([48766, 2])
We keep 1.47e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([28723, 2])
We keep 8.28e+06/3.98e+08 =  2% of the original kernel matrix.

torch.Size([34728, 2])
We keep 6.85e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([5456, 2])
We keep 2.81e+05/5.18e+06 =  5% of the original kernel matrix.

torch.Size([15449, 2])
We keep 1.34e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([25680, 2])
We keep 7.35e+06/3.14e+08 =  2% of the original kernel matrix.

torch.Size([32556, 2])
We keep 6.31e+06/4.37e+08 =  1% of the original kernel matrix.

torch.Size([6058, 2])
We keep 1.30e+06/1.49e+07 =  8% of the original kernel matrix.

torch.Size([15806, 2])
We keep 1.89e+06/9.53e+07 =  1% of the original kernel matrix.

torch.Size([91216, 2])
We keep 3.26e+08/9.36e+09 =  3% of the original kernel matrix.

torch.Size([57036, 2])
We keep 2.56e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([31794, 2])
We keep 8.59e+07/8.69e+08 =  9% of the original kernel matrix.

torch.Size([34887, 2])
We keep 9.26e+06/7.27e+08 =  1% of the original kernel matrix.

torch.Size([18188, 2])
We keep 3.43e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([27010, 2])
We keep 4.25e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([7962, 2])
We keep 1.42e+06/2.17e+07 =  6% of the original kernel matrix.

torch.Size([18048, 2])
We keep 2.21e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([34062, 2])
We keep 1.77e+07/6.58e+08 =  2% of the original kernel matrix.

torch.Size([38163, 2])
We keep 8.71e+06/6.32e+08 =  1% of the original kernel matrix.

torch.Size([7060, 2])
We keep 5.88e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([17324, 2])
We keep 1.85e+06/8.66e+07 =  2% of the original kernel matrix.

torch.Size([13546, 2])
We keep 1.62e+06/4.98e+07 =  3% of the original kernel matrix.

torch.Size([23233, 2])
We keep 3.07e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([30179, 2])
We keep 6.64e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([35626, 2])
We keep 6.55e+06/4.72e+08 =  1% of the original kernel matrix.

torch.Size([180762, 2])
We keep 2.49e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([86267, 2])
We keep 3.19e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([12337, 2])
We keep 1.51e+06/3.92e+07 =  3% of the original kernel matrix.

torch.Size([22981, 2])
We keep 2.87e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([4480, 2])
We keep 1.92e+05/3.35e+06 =  5% of the original kernel matrix.

torch.Size([14562, 2])
We keep 1.15e+06/4.51e+07 =  2% of the original kernel matrix.

torch.Size([9247, 2])
We keep 1.03e+06/2.29e+07 =  4% of the original kernel matrix.

torch.Size([19370, 2])
We keep 2.32e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([222327, 2])
We keep 4.75e+08/2.96e+10 =  1% of the original kernel matrix.

torch.Size([96162, 2])
We keep 4.17e+07/4.24e+09 =  0% of the original kernel matrix.

torch.Size([644280, 2])
We keep 3.39e+09/1.88e+11 =  1% of the original kernel matrix.

torch.Size([167527, 2])
We keep 9.31e+07/1.07e+10 =  0% of the original kernel matrix.

torch.Size([37421, 2])
We keep 1.91e+07/7.56e+08 =  2% of the original kernel matrix.

torch.Size([38567, 2])
We keep 8.93e+06/6.78e+08 =  1% of the original kernel matrix.

torch.Size([12925, 2])
We keep 1.44e+06/4.55e+07 =  3% of the original kernel matrix.

torch.Size([23398, 2])
We keep 3.02e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([75887, 2])
We keep 1.08e+08/4.08e+09 =  2% of the original kernel matrix.

torch.Size([53653, 2])
We keep 1.80e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([172586, 2])
We keep 1.28e+08/1.39e+10 =  0% of the original kernel matrix.

torch.Size([86075, 2])
We keep 2.99e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([5717, 2])
We keep 2.82e+05/5.98e+06 =  4% of the original kernel matrix.

torch.Size([16044, 2])
We keep 1.43e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([20306, 2])
We keep 8.57e+06/1.75e+08 =  4% of the original kernel matrix.

torch.Size([28669, 2])
We keep 4.94e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([36812, 2])
We keep 8.21e+06/5.71e+08 =  1% of the original kernel matrix.

torch.Size([40573, 2])
We keep 8.00e+06/5.89e+08 =  1% of the original kernel matrix.

torch.Size([72902, 2])
We keep 3.16e+08/6.82e+09 =  4% of the original kernel matrix.

torch.Size([51454, 2])
We keep 2.22e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([166543, 2])
We keep 1.07e+09/2.20e+10 =  4% of the original kernel matrix.

torch.Size([82480, 2])
We keep 3.68e+07/3.66e+09 =  1% of the original kernel matrix.

torch.Size([157365, 2])
We keep 1.33e+08/1.07e+10 =  1% of the original kernel matrix.

torch.Size([80306, 2])
We keep 2.66e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([24807, 2])
We keep 4.45e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([32473, 2])
We keep 5.38e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([10384, 2])
We keep 1.19e+06/2.43e+07 =  4% of the original kernel matrix.

torch.Size([20625, 2])
We keep 2.39e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([6916, 2])
We keep 3.87e+05/8.98e+06 =  4% of the original kernel matrix.

torch.Size([17284, 2])
We keep 1.66e+06/7.39e+07 =  2% of the original kernel matrix.

torch.Size([37419, 2])
We keep 1.63e+07/6.52e+08 =  2% of the original kernel matrix.

torch.Size([39620, 2])
We keep 8.35e+06/6.29e+08 =  1% of the original kernel matrix.

torch.Size([23299, 2])
We keep 4.99e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([31450, 2])
We keep 5.37e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([55093, 2])
We keep 4.28e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([46528, 2])
We keep 1.23e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([60046, 2])
We keep 2.76e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([48998, 2])
We keep 1.20e+07/9.93e+08 =  1% of the original kernel matrix.

torch.Size([399144, 2])
We keep 5.34e+08/7.15e+10 =  0% of the original kernel matrix.

torch.Size([132244, 2])
We keep 6.12e+07/6.59e+09 =  0% of the original kernel matrix.

torch.Size([18450, 2])
We keep 6.20e+06/1.31e+08 =  4% of the original kernel matrix.

torch.Size([27273, 2])
We keep 4.40e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([6720, 2])
We keep 5.14e+05/9.35e+06 =  5% of the original kernel matrix.

torch.Size([17046, 2])
We keep 1.66e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([19450, 2])
We keep 3.27e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([28681, 2])
We keep 4.42e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([796771, 2])
We keep 1.84e+10/6.38e+11 =  2% of the original kernel matrix.

torch.Size([185927, 2])
We keep 1.62e+08/1.97e+10 =  0% of the original kernel matrix.

torch.Size([49630, 2])
We keep 1.65e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([45314, 2])
We keep 9.91e+06/7.84e+08 =  1% of the original kernel matrix.

torch.Size([8214, 2])
We keep 1.06e+06/1.85e+07 =  5% of the original kernel matrix.

torch.Size([18492, 2])
We keep 2.15e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([6880, 2])
We keep 5.07e+05/9.42e+06 =  5% of the original kernel matrix.

torch.Size([17213, 2])
We keep 1.68e+06/7.57e+07 =  2% of the original kernel matrix.

torch.Size([50502, 2])
We keep 2.22e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([45298, 2])
We keep 1.04e+07/8.40e+08 =  1% of the original kernel matrix.

torch.Size([4480, 2])
We keep 1.94e+05/3.58e+06 =  5% of the original kernel matrix.

torch.Size([14677, 2])
We keep 1.21e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([41462, 2])
We keep 8.94e+07/1.39e+09 =  6% of the original kernel matrix.

torch.Size([39564, 2])
We keep 1.14e+07/9.19e+08 =  1% of the original kernel matrix.

torch.Size([8584, 2])
We keep 7.04e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([19054, 2])
We keep 2.10e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([20267, 2])
We keep 3.09e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([28756, 2])
We keep 4.56e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([16808, 2])
We keep 3.09e+06/9.63e+07 =  3% of the original kernel matrix.

torch.Size([26806, 2])
We keep 3.96e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([9301, 2])
We keep 6.68e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([19625, 2])
We keep 2.16e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([6255, 2])
We keep 3.31e+05/7.75e+06 =  4% of the original kernel matrix.

torch.Size([16661, 2])
We keep 1.57e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([223140, 2])
We keep 2.00e+08/2.12e+10 =  0% of the original kernel matrix.

torch.Size([96966, 2])
We keep 3.54e+07/3.59e+09 =  0% of the original kernel matrix.

torch.Size([443032, 2])
We keep 1.02e+09/8.99e+10 =  1% of the original kernel matrix.

torch.Size([137738, 2])
We keep 6.83e+07/7.39e+09 =  0% of the original kernel matrix.

torch.Size([148151, 2])
We keep 2.08e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([77821, 2])
We keep 2.73e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([72533, 2])
We keep 2.63e+08/3.39e+09 =  7% of the original kernel matrix.

torch.Size([52314, 2])
We keep 1.63e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([32205, 2])
We keep 6.50e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([36778, 2])
We keep 6.79e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([14037, 2])
We keep 2.73e+06/6.67e+07 =  4% of the original kernel matrix.

torch.Size([23830, 2])
We keep 3.48e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([17287, 2])
We keep 3.00e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([26732, 2])
We keep 4.10e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([157002, 2])
We keep 1.05e+08/1.06e+10 =  0% of the original kernel matrix.

torch.Size([80399, 2])
We keep 2.64e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([75516, 2])
We keep 1.30e+08/3.56e+09 =  3% of the original kernel matrix.

torch.Size([55520, 2])
We keep 1.64e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([120912, 2])
We keep 4.84e+08/1.13e+10 =  4% of the original kernel matrix.

torch.Size([69427, 2])
We keep 2.68e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([15968, 2])
We keep 3.51e+06/8.98e+07 =  3% of the original kernel matrix.

torch.Size([25652, 2])
We keep 3.89e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([870867, 2])
We keep 4.19e+09/4.15e+11 =  1% of the original kernel matrix.

torch.Size([199114, 2])
We keep 1.36e+08/1.59e+10 =  0% of the original kernel matrix.

torch.Size([26987, 2])
We keep 8.73e+06/3.10e+08 =  2% of the original kernel matrix.

torch.Size([33484, 2])
We keep 6.24e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([57230, 2])
We keep 3.06e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([49532, 2])
We keep 1.20e+07/9.78e+08 =  1% of the original kernel matrix.

torch.Size([159932, 2])
We keep 1.91e+08/1.23e+10 =  1% of the original kernel matrix.

torch.Size([83656, 2])
We keep 2.86e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([57396, 2])
We keep 1.84e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([49034, 2])
We keep 1.13e+07/9.26e+08 =  1% of the original kernel matrix.

torch.Size([158714, 2])
We keep 6.99e+08/2.15e+10 =  3% of the original kernel matrix.

torch.Size([79762, 2])
We keep 3.49e+07/3.62e+09 =  0% of the original kernel matrix.

torch.Size([357557, 2])
We keep 1.08e+09/6.60e+10 =  1% of the original kernel matrix.

torch.Size([124039, 2])
We keep 5.92e+07/6.33e+09 =  0% of the original kernel matrix.

torch.Size([110131, 2])
We keep 6.13e+07/5.46e+09 =  1% of the original kernel matrix.

torch.Size([66723, 2])
We keep 2.01e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([37516, 2])
We keep 1.39e+07/6.02e+08 =  2% of the original kernel matrix.

torch.Size([39707, 2])
We keep 8.05e+06/6.05e+08 =  1% of the original kernel matrix.

torch.Size([19104, 2])
We keep 8.28e+06/1.50e+08 =  5% of the original kernel matrix.

torch.Size([27700, 2])
We keep 4.69e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([78050, 2])
We keep 6.54e+07/2.83e+09 =  2% of the original kernel matrix.

torch.Size([55649, 2])
We keep 1.49e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([11516, 2])
We keep 1.29e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([21634, 2])
We keep 2.70e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([198867, 2])
We keep 3.29e+08/1.81e+10 =  1% of the original kernel matrix.

torch.Size([90863, 2])
We keep 3.31e+07/3.31e+09 =  0% of the original kernel matrix.

torch.Size([1136601, 2])
We keep 3.75e+09/5.20e+11 =  0% of the original kernel matrix.

torch.Size([229404, 2])
We keep 1.51e+08/1.78e+10 =  0% of the original kernel matrix.

torch.Size([122592, 2])
We keep 2.82e+08/8.56e+09 =  3% of the original kernel matrix.

torch.Size([70468, 2])
We keep 2.47e+07/2.28e+09 =  1% of the original kernel matrix.

torch.Size([107426, 2])
We keep 2.05e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([63608, 2])
We keep 2.74e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([23007, 2])
We keep 8.76e+06/2.32e+08 =  3% of the original kernel matrix.

torch.Size([30900, 2])
We keep 5.50e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([34088, 2])
We keep 5.12e+07/8.24e+08 =  6% of the original kernel matrix.

torch.Size([36762, 2])
We keep 9.23e+06/7.08e+08 =  1% of the original kernel matrix.

torch.Size([39093, 2])
We keep 1.23e+07/6.04e+08 =  2% of the original kernel matrix.

torch.Size([39795, 2])
We keep 7.58e+06/6.06e+08 =  1% of the original kernel matrix.

torch.Size([18901, 2])
We keep 5.78e+06/1.75e+08 =  3% of the original kernel matrix.

torch.Size([27482, 2])
We keep 5.01e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([83913, 2])
We keep 1.20e+08/3.91e+09 =  3% of the original kernel matrix.

torch.Size([57096, 2])
We keep 1.76e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([365829, 2])
We keep 4.50e+08/6.30e+10 =  0% of the original kernel matrix.

torch.Size([126814, 2])
We keep 5.80e+07/6.19e+09 =  0% of the original kernel matrix.

torch.Size([13730, 2])
We keep 1.28e+07/1.57e+08 =  8% of the original kernel matrix.

torch.Size([23229, 2])
We keep 4.71e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([242869, 2])
We keep 1.36e+09/5.35e+10 =  2% of the original kernel matrix.

torch.Size([97695, 2])
We keep 5.32e+07/5.70e+09 =  0% of the original kernel matrix.

torch.Size([159413, 2])
We keep 6.00e+08/1.73e+10 =  3% of the original kernel matrix.

torch.Size([81115, 2])
We keep 3.34e+07/3.25e+09 =  1% of the original kernel matrix.

torch.Size([57740, 2])
We keep 2.73e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([48404, 2])
We keep 1.14e+07/9.26e+08 =  1% of the original kernel matrix.

torch.Size([6305, 2])
We keep 4.67e+05/9.40e+06 =  4% of the original kernel matrix.

torch.Size([16527, 2])
We keep 1.68e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([107798, 2])
We keep 9.45e+07/5.52e+09 =  1% of the original kernel matrix.

torch.Size([65449, 2])
We keep 2.02e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([174512, 2])
We keep 1.25e+08/1.28e+10 =  0% of the original kernel matrix.

torch.Size([84977, 2])
We keep 2.88e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([6666, 2])
We keep 4.36e+05/9.60e+06 =  4% of the original kernel matrix.

torch.Size([17157, 2])
We keep 1.69e+06/7.64e+07 =  2% of the original kernel matrix.

torch.Size([938158, 2])
We keep 2.32e+09/3.85e+11 =  0% of the original kernel matrix.

torch.Size([211627, 2])
We keep 1.31e+08/1.53e+10 =  0% of the original kernel matrix.

torch.Size([35326, 2])
We keep 1.86e+07/5.92e+08 =  3% of the original kernel matrix.

torch.Size([38019, 2])
We keep 7.83e+06/6.00e+08 =  1% of the original kernel matrix.

torch.Size([38893, 2])
We keep 1.54e+07/7.11e+08 =  2% of the original kernel matrix.

torch.Size([40737, 2])
We keep 8.74e+06/6.58e+08 =  1% of the original kernel matrix.

torch.Size([11714, 2])
We keep 1.60e+06/3.95e+07 =  4% of the original kernel matrix.

torch.Size([21845, 2])
We keep 2.83e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([16513, 2])
We keep 2.04e+06/7.92e+07 =  2% of the original kernel matrix.

torch.Size([25658, 2])
We keep 3.63e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([6909, 2])
We keep 5.52e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([17152, 2])
We keep 1.75e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([11841, 2])
We keep 2.89e+06/4.53e+07 =  6% of the original kernel matrix.

torch.Size([21744, 2])
We keep 2.93e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([25579, 2])
We keep 1.16e+07/2.56e+08 =  4% of the original kernel matrix.

torch.Size([32294, 2])
We keep 5.76e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([82970, 2])
We keep 7.69e+07/3.51e+09 =  2% of the original kernel matrix.

torch.Size([57084, 2])
We keep 1.66e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([2119455, 2])
We keep 3.89e+10/3.17e+12 =  1% of the original kernel matrix.

torch.Size([293549, 2])
We keep 3.50e+08/4.39e+10 =  0% of the original kernel matrix.

torch.Size([9767, 2])
We keep 1.24e+06/2.53e+07 =  4% of the original kernel matrix.

torch.Size([20224, 2])
We keep 2.39e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([96769, 2])
We keep 5.07e+07/3.65e+09 =  1% of the original kernel matrix.

torch.Size([61339, 2])
We keep 1.68e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([9607, 2])
We keep 8.84e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([19851, 2])
We keep 2.31e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([29961, 2])
We keep 7.49e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([35724, 2])
We keep 6.86e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([11637, 2])
We keep 1.85e+06/4.50e+07 =  4% of the original kernel matrix.

torch.Size([21766, 2])
We keep 3.00e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([121233, 2])
We keep 7.86e+08/2.06e+10 =  3% of the original kernel matrix.

torch.Size([66397, 2])
We keep 3.57e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([35844, 2])
We keep 4.62e+07/7.41e+08 =  6% of the original kernel matrix.

torch.Size([37651, 2])
We keep 8.81e+06/6.71e+08 =  1% of the original kernel matrix.

torch.Size([1519614, 2])
We keep 4.88e+09/8.14e+11 =  0% of the original kernel matrix.

torch.Size([263584, 2])
We keep 1.85e+08/2.23e+10 =  0% of the original kernel matrix.

torch.Size([12683, 2])
We keep 1.41e+06/4.25e+07 =  3% of the original kernel matrix.

torch.Size([23014, 2])
We keep 2.93e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([176186, 2])
We keep 2.12e+08/1.38e+10 =  1% of the original kernel matrix.

torch.Size([85330, 2])
We keep 2.99e+07/2.90e+09 =  1% of the original kernel matrix.

torch.Size([31015, 2])
We keep 2.20e+07/4.09e+08 =  5% of the original kernel matrix.

torch.Size([36262, 2])
We keep 6.95e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([157022, 2])
We keep 1.72e+08/1.29e+10 =  1% of the original kernel matrix.

torch.Size([80493, 2])
We keep 2.94e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([343453, 2])
We keep 7.47e+08/6.60e+10 =  1% of the original kernel matrix.

torch.Size([121187, 2])
We keep 5.93e+07/6.33e+09 =  0% of the original kernel matrix.

torch.Size([30161, 2])
We keep 7.45e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([35463, 2])
We keep 6.65e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([12911, 2])
We keep 1.60e+06/4.35e+07 =  3% of the original kernel matrix.

torch.Size([22808, 2])
We keep 2.93e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([101421, 2])
We keep 5.12e+07/4.30e+09 =  1% of the original kernel matrix.

torch.Size([65239, 2])
We keep 1.82e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([23006, 2])
We keep 9.43e+06/2.15e+08 =  4% of the original kernel matrix.

torch.Size([30949, 2])
We keep 5.41e+06/3.61e+08 =  1% of the original kernel matrix.

torch.Size([39825, 2])
We keep 5.27e+07/1.29e+09 =  4% of the original kernel matrix.

torch.Size([38568, 2])
We keep 1.11e+07/8.87e+08 =  1% of the original kernel matrix.

torch.Size([160711, 2])
We keep 1.31e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([81053, 2])
We keep 2.76e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([22856, 2])
We keep 2.70e+07/3.04e+08 =  8% of the original kernel matrix.

torch.Size([30120, 2])
We keep 6.01e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([84763, 2])
We keep 8.88e+07/3.29e+09 =  2% of the original kernel matrix.

torch.Size([60070, 2])
We keep 1.64e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([35092, 2])
We keep 1.42e+07/6.03e+08 =  2% of the original kernel matrix.

torch.Size([37522, 2])
We keep 7.92e+06/6.06e+08 =  1% of the original kernel matrix.

torch.Size([136379, 2])
We keep 2.31e+08/1.11e+10 =  2% of the original kernel matrix.

torch.Size([74143, 2])
We keep 2.76e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([426726, 2])
We keep 1.40e+09/1.02e+11 =  1% of the original kernel matrix.

torch.Size([131833, 2])
We keep 7.17e+07/7.87e+09 =  0% of the original kernel matrix.

torch.Size([40890, 2])
We keep 1.26e+07/6.90e+08 =  1% of the original kernel matrix.

torch.Size([41445, 2])
We keep 8.54e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([15097, 2])
We keep 2.14e+06/6.69e+07 =  3% of the original kernel matrix.

torch.Size([25236, 2])
We keep 3.48e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([172886, 2])
We keep 4.20e+08/2.16e+10 =  1% of the original kernel matrix.

torch.Size([82814, 2])
We keep 3.65e+07/3.63e+09 =  1% of the original kernel matrix.

torch.Size([60098, 2])
We keep 3.23e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([48742, 2])
We keep 1.28e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([114955, 2])
We keep 2.83e+08/6.61e+09 =  4% of the original kernel matrix.

torch.Size([67526, 2])
We keep 2.16e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([59087, 2])
We keep 5.64e+07/1.92e+09 =  2% of the original kernel matrix.

torch.Size([48300, 2])
We keep 1.30e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([64462, 2])
We keep 7.07e+07/2.46e+09 =  2% of the original kernel matrix.

torch.Size([49739, 2])
We keep 1.44e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([43041, 2])
We keep 4.16e+07/1.28e+09 =  3% of the original kernel matrix.

torch.Size([40811, 2])
We keep 1.11e+07/8.83e+08 =  1% of the original kernel matrix.

torch.Size([75354, 2])
We keep 1.40e+08/4.63e+09 =  3% of the original kernel matrix.

torch.Size([52936, 2])
We keep 1.87e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([47471, 2])
We keep 5.04e+07/1.30e+09 =  3% of the original kernel matrix.

torch.Size([43944, 2])
We keep 1.10e+07/8.88e+08 =  1% of the original kernel matrix.

torch.Size([36538, 2])
We keep 1.16e+07/5.89e+08 =  1% of the original kernel matrix.

torch.Size([39601, 2])
We keep 8.15e+06/5.98e+08 =  1% of the original kernel matrix.

torch.Size([9834, 2])
We keep 9.43e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([20120, 2])
We keep 2.36e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([25694, 2])
We keep 1.07e+07/2.53e+08 =  4% of the original kernel matrix.

torch.Size([33663, 2])
We keep 5.85e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([53032, 2])
We keep 7.74e+07/1.79e+09 =  4% of the original kernel matrix.

torch.Size([45429, 2])
We keep 1.27e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([114041, 2])
We keep 1.88e+08/6.25e+09 =  3% of the original kernel matrix.

torch.Size([66921, 2])
We keep 2.13e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([164237, 2])
We keep 2.11e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([82380, 2])
We keep 2.74e+07/2.67e+09 =  1% of the original kernel matrix.

torch.Size([14900, 2])
We keep 1.99e+06/6.17e+07 =  3% of the original kernel matrix.

torch.Size([24420, 2])
We keep 3.34e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([34793, 2])
We keep 7.99e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([36710, 2])
We keep 7.12e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([60971, 2])
We keep 3.63e+07/1.78e+09 =  2% of the original kernel matrix.

torch.Size([49687, 2])
We keep 1.26e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([274667, 2])
We keep 9.29e+08/4.00e+10 =  2% of the original kernel matrix.

torch.Size([109090, 2])
We keep 4.61e+07/4.93e+09 =  0% of the original kernel matrix.

torch.Size([50481, 2])
We keep 1.88e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([45761, 2])
We keep 1.03e+07/8.19e+08 =  1% of the original kernel matrix.

torch.Size([97167, 2])
We keep 4.85e+07/3.61e+09 =  1% of the original kernel matrix.

torch.Size([61650, 2])
We keep 1.66e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([31043, 2])
We keep 8.45e+07/8.57e+08 =  9% of the original kernel matrix.

torch.Size([34447, 2])
We keep 9.07e+06/7.22e+08 =  1% of the original kernel matrix.

torch.Size([27025, 2])
We keep 5.43e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([33659, 2])
We keep 5.84e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([3775, 2])
We keep 1.92e+05/3.08e+06 =  6% of the original kernel matrix.

torch.Size([13451, 2])
We keep 1.15e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([15374, 2])
We keep 2.60e+06/6.98e+07 =  3% of the original kernel matrix.

torch.Size([25431, 2])
We keep 3.49e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([166329, 2])
We keep 1.05e+08/1.16e+10 =  0% of the original kernel matrix.

torch.Size([83179, 2])
We keep 2.73e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([53991, 2])
We keep 3.27e+07/1.63e+09 =  2% of the original kernel matrix.

torch.Size([46130, 2])
We keep 1.19e+07/9.95e+08 =  1% of the original kernel matrix.

torch.Size([79028, 2])
We keep 3.04e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([55915, 2])
We keep 1.43e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([3337, 2])
We keep 9.49e+04/1.75e+06 =  5% of the original kernel matrix.

torch.Size([13199, 2])
We keep 9.34e+05/3.26e+07 =  2% of the original kernel matrix.

torch.Size([9621, 2])
We keep 9.27e+05/2.19e+07 =  4% of the original kernel matrix.

torch.Size([20034, 2])
We keep 2.31e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([136063, 2])
We keep 1.76e+08/9.03e+09 =  1% of the original kernel matrix.

torch.Size([74116, 2])
We keep 2.49e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([93581, 2])
We keep 5.96e+07/3.73e+09 =  1% of the original kernel matrix.

torch.Size([60460, 2])
We keep 1.72e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([34861, 2])
We keep 8.86e+06/5.01e+08 =  1% of the original kernel matrix.

torch.Size([38998, 2])
We keep 7.55e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([6418, 2])
We keep 3.67e+05/8.24e+06 =  4% of the original kernel matrix.

torch.Size([16876, 2])
We keep 1.61e+06/7.08e+07 =  2% of the original kernel matrix.

torch.Size([8377, 2])
We keep 7.39e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([18808, 2])
We keep 2.02e+06/9.74e+07 =  2% of the original kernel matrix.

torch.Size([38984, 2])
We keep 1.26e+07/6.82e+08 =  1% of the original kernel matrix.

torch.Size([40613, 2])
We keep 8.39e+06/6.44e+08 =  1% of the original kernel matrix.

torch.Size([41529, 2])
We keep 1.77e+07/7.60e+08 =  2% of the original kernel matrix.

torch.Size([41629, 2])
We keep 8.76e+06/6.80e+08 =  1% of the original kernel matrix.

torch.Size([89914, 2])
We keep 1.25e+08/4.82e+09 =  2% of the original kernel matrix.

torch.Size([59363, 2])
We keep 1.90e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([9435, 2])
We keep 1.44e+06/2.46e+07 =  5% of the original kernel matrix.

torch.Size([19792, 2])
We keep 2.33e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([55522, 2])
We keep 3.29e+08/4.06e+09 =  8% of the original kernel matrix.

torch.Size([45544, 2])
We keep 1.78e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([16825, 2])
We keep 4.11e+06/1.16e+08 =  3% of the original kernel matrix.

torch.Size([25976, 2])
We keep 4.24e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([13904, 2])
We keep 2.43e+06/6.44e+07 =  3% of the original kernel matrix.

torch.Size([23795, 2])
We keep 3.41e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([322173, 2])
We keep 7.92e+08/5.89e+10 =  1% of the original kernel matrix.

torch.Size([118069, 2])
We keep 5.68e+07/5.98e+09 =  0% of the original kernel matrix.

torch.Size([16520, 2])
We keep 6.11e+06/9.97e+07 =  6% of the original kernel matrix.

torch.Size([25699, 2])
We keep 3.92e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([105680, 2])
We keep 1.13e+08/5.79e+09 =  1% of the original kernel matrix.

torch.Size([64555, 2])
We keep 2.02e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([180868, 2])
We keep 2.07e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([86664, 2])
We keep 3.00e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([64505, 2])
We keep 3.51e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([51249, 2])
We keep 1.33e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([95561, 2])
We keep 8.60e+07/4.26e+09 =  2% of the original kernel matrix.

torch.Size([61427, 2])
We keep 1.81e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([250394, 2])
We keep 4.48e+08/2.90e+10 =  1% of the original kernel matrix.

torch.Size([103584, 2])
We keep 4.10e+07/4.20e+09 =  0% of the original kernel matrix.

torch.Size([6858, 2])
We keep 3.85e+05/9.43e+06 =  4% of the original kernel matrix.

torch.Size([17201, 2])
We keep 1.65e+06/7.57e+07 =  2% of the original kernel matrix.

torch.Size([64941, 2])
We keep 4.48e+07/1.89e+09 =  2% of the original kernel matrix.

torch.Size([51213, 2])
We keep 1.28e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([29953, 2])
We keep 6.37e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([35613, 2])
We keep 6.45e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([290328, 2])
We keep 5.27e+08/3.79e+10 =  1% of the original kernel matrix.

torch.Size([113197, 2])
We keep 4.60e+07/4.80e+09 =  0% of the original kernel matrix.

torch.Size([174551, 2])
We keep 2.15e+08/1.56e+10 =  1% of the original kernel matrix.

torch.Size([84786, 2])
We keep 3.14e+07/3.08e+09 =  1% of the original kernel matrix.

torch.Size([42214, 2])
We keep 2.62e+07/1.11e+09 =  2% of the original kernel matrix.

torch.Size([40754, 2])
We keep 1.04e+07/8.21e+08 =  1% of the original kernel matrix.

torch.Size([73062, 2])
We keep 3.03e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([54348, 2])
We keep 1.36e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([17157, 2])
We keep 2.22e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([26386, 2])
We keep 3.84e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([13004, 2])
We keep 1.28e+07/1.51e+08 =  8% of the original kernel matrix.

torch.Size([22138, 2])
We keep 4.67e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([28775, 2])
We keep 3.68e+07/4.85e+08 =  7% of the original kernel matrix.

torch.Size([34637, 2])
We keep 7.46e+06/5.43e+08 =  1% of the original kernel matrix.

torch.Size([17396, 2])
We keep 2.57e+06/9.86e+07 =  2% of the original kernel matrix.

torch.Size([26614, 2])
We keep 3.99e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([208064, 2])
We keep 2.57e+08/2.11e+10 =  1% of the original kernel matrix.

torch.Size([93121, 2])
We keep 3.56e+07/3.58e+09 =  0% of the original kernel matrix.

torch.Size([294050, 2])
We keep 7.22e+08/4.93e+10 =  1% of the original kernel matrix.

torch.Size([113602, 2])
We keep 5.28e+07/5.48e+09 =  0% of the original kernel matrix.

torch.Size([13705, 2])
We keep 2.67e+06/5.87e+07 =  4% of the original kernel matrix.

torch.Size([23468, 2])
We keep 3.24e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([23044, 2])
We keep 4.98e+06/2.13e+08 =  2% of the original kernel matrix.

torch.Size([30784, 2])
We keep 5.32e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([20140, 2])
We keep 3.52e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([28749, 2])
We keep 4.53e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([339704, 2])
We keep 8.88e+08/6.02e+10 =  1% of the original kernel matrix.

torch.Size([121153, 2])
We keep 5.71e+07/6.05e+09 =  0% of the original kernel matrix.

torch.Size([31616, 2])
We keep 7.44e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([36779, 2])
We keep 6.89e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([20706, 2])
We keep 4.44e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([29063, 2])
We keep 4.79e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([80949, 2])
We keep 5.34e+07/2.98e+09 =  1% of the original kernel matrix.

torch.Size([57670, 2])
We keep 1.56e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([129262, 2])
We keep 9.32e+07/7.21e+09 =  1% of the original kernel matrix.

torch.Size([71805, 2])
We keep 2.25e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([17246, 2])
We keep 3.54e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([26301, 2])
We keep 4.07e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([37281, 2])
We keep 1.88e+07/6.63e+08 =  2% of the original kernel matrix.

torch.Size([39785, 2])
We keep 8.52e+06/6.35e+08 =  1% of the original kernel matrix.

torch.Size([14429, 2])
We keep 3.32e+06/6.22e+07 =  5% of the original kernel matrix.

torch.Size([24150, 2])
We keep 3.23e+06/1.95e+08 =  1% of the original kernel matrix.

time for making ranges is 3.997680187225342
Sorting X and nu_X
time for sorting X is 0.08735370635986328
Sorting Z and nu_Z
time for sorting Z is 0.0002720355987548828
Starting Optim
sum tnu_Z before tensor(39337608., device='cuda:0')
c= tensor(1176.6621, device='cuda:0')
c= tensor(144267.8125, device='cuda:0')
c= tensor(148919.5938, device='cuda:0')
c= tensor(159175.0312, device='cuda:0')
c= tensor(710701.2500, device='cuda:0')
c= tensor(971854.6875, device='cuda:0')
c= tensor(1784429.7500, device='cuda:0')
c= tensor(2212288.7500, device='cuda:0')
c= tensor(2278791.5000, device='cuda:0')
c= tensor(5624209.5000, device='cuda:0')
c= tensor(5668916.5000, device='cuda:0')
c= tensor(12094471., device='cuda:0')
c= tensor(12109491., device='cuda:0')
c= tensor(26185400., device='cuda:0')
c= tensor(26577328., device='cuda:0')
c= tensor(26947340., device='cuda:0')
c= tensor(27778144., device='cuda:0')
c= tensor(28829474., device='cuda:0')
c= tensor(36137436., device='cuda:0')
c= tensor(39219180., device='cuda:0')
c= tensor(39267664., device='cuda:0')
c= tensor(57892636., device='cuda:0')
c= tensor(57970568., device='cuda:0')
c= tensor(58249356., device='cuda:0')
c= tensor(59762528., device='cuda:0')
c= tensor(60792884., device='cuda:0')
c= tensor(62128048., device='cuda:0')
c= tensor(62195668., device='cuda:0')
c= tensor(65464084., device='cuda:0')
c= tensor(4.4589e+08, device='cuda:0')
c= tensor(4.4593e+08, device='cuda:0')
c= tensor(5.4214e+08, device='cuda:0')
c= tensor(5.4231e+08, device='cuda:0')
c= tensor(5.4234e+08, device='cuda:0')
c= tensor(5.4236e+08, device='cuda:0')
c= tensor(5.4637e+08, device='cuda:0')
c= tensor(5.4745e+08, device='cuda:0')
c= tensor(5.4745e+08, device='cuda:0')
c= tensor(5.4746e+08, device='cuda:0')
c= tensor(5.4746e+08, device='cuda:0')
c= tensor(5.4747e+08, device='cuda:0')
c= tensor(5.4748e+08, device='cuda:0')
c= tensor(5.4748e+08, device='cuda:0')
c= tensor(5.4749e+08, device='cuda:0')
c= tensor(5.4750e+08, device='cuda:0')
c= tensor(5.4750e+08, device='cuda:0')
c= tensor(5.4751e+08, device='cuda:0')
c= tensor(5.4752e+08, device='cuda:0')
c= tensor(5.4752e+08, device='cuda:0')
c= tensor(5.4757e+08, device='cuda:0')
c= tensor(5.4763e+08, device='cuda:0')
c= tensor(5.4763e+08, device='cuda:0')
c= tensor(5.4764e+08, device='cuda:0')
c= tensor(5.4765e+08, device='cuda:0')
c= tensor(5.4766e+08, device='cuda:0')
c= tensor(5.4767e+08, device='cuda:0')
c= tensor(5.4767e+08, device='cuda:0')
c= tensor(5.4768e+08, device='cuda:0')
c= tensor(5.4769e+08, device='cuda:0')
c= tensor(5.4769e+08, device='cuda:0')
c= tensor(5.4770e+08, device='cuda:0')
c= tensor(5.4771e+08, device='cuda:0')
c= tensor(5.4773e+08, device='cuda:0')
c= tensor(5.4776e+08, device='cuda:0')
c= tensor(5.4777e+08, device='cuda:0')
c= tensor(5.4777e+08, device='cuda:0')
c= tensor(5.4777e+08, device='cuda:0')
c= tensor(5.4779e+08, device='cuda:0')
c= tensor(5.4779e+08, device='cuda:0')
c= tensor(5.4780e+08, device='cuda:0')
c= tensor(5.4781e+08, device='cuda:0')
c= tensor(5.4781e+08, device='cuda:0')
c= tensor(5.4782e+08, device='cuda:0')
c= tensor(5.4782e+08, device='cuda:0')
c= tensor(5.4783e+08, device='cuda:0')
c= tensor(5.4785e+08, device='cuda:0')
c= tensor(5.4785e+08, device='cuda:0')
c= tensor(5.4785e+08, device='cuda:0')
c= tensor(5.4786e+08, device='cuda:0')
c= tensor(5.4792e+08, device='cuda:0')
c= tensor(5.4792e+08, device='cuda:0')
c= tensor(5.4793e+08, device='cuda:0')
c= tensor(5.4794e+08, device='cuda:0')
c= tensor(5.4794e+08, device='cuda:0')
c= tensor(5.4794e+08, device='cuda:0')
c= tensor(5.4795e+08, device='cuda:0')
c= tensor(5.4795e+08, device='cuda:0')
c= tensor(5.4795e+08, device='cuda:0')
c= tensor(5.4796e+08, device='cuda:0')
c= tensor(5.4797e+08, device='cuda:0')
c= tensor(5.4797e+08, device='cuda:0')
c= tensor(5.4797e+08, device='cuda:0')
c= tensor(5.4798e+08, device='cuda:0')
c= tensor(5.4798e+08, device='cuda:0')
c= tensor(5.4799e+08, device='cuda:0')
c= tensor(5.4800e+08, device='cuda:0')
c= tensor(5.4801e+08, device='cuda:0')
c= tensor(5.4803e+08, device='cuda:0')
c= tensor(5.4803e+08, device='cuda:0')
c= tensor(5.4804e+08, device='cuda:0')
c= tensor(5.4807e+08, device='cuda:0')
c= tensor(5.4808e+08, device='cuda:0')
c= tensor(5.4809e+08, device='cuda:0')
c= tensor(5.4810e+08, device='cuda:0')
c= tensor(5.4812e+08, device='cuda:0')
c= tensor(5.4813e+08, device='cuda:0')
c= tensor(5.4814e+08, device='cuda:0')
c= tensor(5.4814e+08, device='cuda:0')
c= tensor(5.4815e+08, device='cuda:0')
c= tensor(5.4815e+08, device='cuda:0')
c= tensor(5.4815e+08, device='cuda:0')
c= tensor(5.4815e+08, device='cuda:0')
c= tensor(5.4816e+08, device='cuda:0')
c= tensor(5.4817e+08, device='cuda:0')
c= tensor(5.4818e+08, device='cuda:0')
c= tensor(5.4818e+08, device='cuda:0')
c= tensor(5.4818e+08, device='cuda:0')
c= tensor(5.4818e+08, device='cuda:0')
c= tensor(5.4820e+08, device='cuda:0')
c= tensor(5.4821e+08, device='cuda:0')
c= tensor(5.4824e+08, device='cuda:0')
c= tensor(5.4825e+08, device='cuda:0')
c= tensor(5.4825e+08, device='cuda:0')
c= tensor(5.4826e+08, device='cuda:0')
c= tensor(5.4826e+08, device='cuda:0')
c= tensor(5.4826e+08, device='cuda:0')
c= tensor(5.4826e+08, device='cuda:0')
c= tensor(5.4827e+08, device='cuda:0')
c= tensor(5.4832e+08, device='cuda:0')
c= tensor(5.4832e+08, device='cuda:0')
c= tensor(5.4834e+08, device='cuda:0')
c= tensor(5.4834e+08, device='cuda:0')
c= tensor(5.4835e+08, device='cuda:0')
c= tensor(5.4835e+08, device='cuda:0')
c= tensor(5.4836e+08, device='cuda:0')
c= tensor(5.4836e+08, device='cuda:0')
c= tensor(5.4836e+08, device='cuda:0')
c= tensor(5.4837e+08, device='cuda:0')
c= tensor(5.4837e+08, device='cuda:0')
c= tensor(5.4837e+08, device='cuda:0')
c= tensor(5.4838e+08, device='cuda:0')
c= tensor(5.4838e+08, device='cuda:0')
c= tensor(5.4842e+08, device='cuda:0')
c= tensor(5.4844e+08, device='cuda:0')
c= tensor(5.4846e+08, device='cuda:0')
c= tensor(5.4846e+08, device='cuda:0')
c= tensor(5.4847e+08, device='cuda:0')
c= tensor(5.4848e+08, device='cuda:0')
c= tensor(5.4848e+08, device='cuda:0')
c= tensor(5.4849e+08, device='cuda:0')
c= tensor(5.4849e+08, device='cuda:0')
c= tensor(5.4850e+08, device='cuda:0')
c= tensor(5.4850e+08, device='cuda:0')
c= tensor(5.4855e+08, device='cuda:0')
c= tensor(5.4855e+08, device='cuda:0')
c= tensor(5.4861e+08, device='cuda:0')
c= tensor(5.4861e+08, device='cuda:0')
c= tensor(5.4861e+08, device='cuda:0')
c= tensor(5.4862e+08, device='cuda:0')
c= tensor(5.4862e+08, device='cuda:0')
c= tensor(5.4864e+08, device='cuda:0')
c= tensor(5.4864e+08, device='cuda:0')
c= tensor(5.4865e+08, device='cuda:0')
c= tensor(5.4865e+08, device='cuda:0')
c= tensor(5.4866e+08, device='cuda:0')
c= tensor(5.4867e+08, device='cuda:0')
c= tensor(5.4867e+08, device='cuda:0')
c= tensor(5.4868e+08, device='cuda:0')
c= tensor(5.4869e+08, device='cuda:0')
c= tensor(5.4869e+08, device='cuda:0')
c= tensor(5.4869e+08, device='cuda:0')
c= tensor(5.4869e+08, device='cuda:0')
c= tensor(5.4871e+08, device='cuda:0')
c= tensor(5.4871e+08, device='cuda:0')
c= tensor(5.4872e+08, device='cuda:0')
c= tensor(5.4873e+08, device='cuda:0')
c= tensor(5.4874e+08, device='cuda:0')
c= tensor(5.4875e+08, device='cuda:0')
c= tensor(5.4877e+08, device='cuda:0')
c= tensor(5.4877e+08, device='cuda:0')
c= tensor(5.4878e+08, device='cuda:0')
c= tensor(5.4878e+08, device='cuda:0')
c= tensor(5.4879e+08, device='cuda:0')
c= tensor(5.4880e+08, device='cuda:0')
c= tensor(5.4881e+08, device='cuda:0')
c= tensor(5.4882e+08, device='cuda:0')
c= tensor(5.4883e+08, device='cuda:0')
c= tensor(5.4883e+08, device='cuda:0')
c= tensor(5.4885e+08, device='cuda:0')
c= tensor(5.4891e+08, device='cuda:0')
c= tensor(5.4891e+08, device='cuda:0')
c= tensor(5.4891e+08, device='cuda:0')
c= tensor(5.4892e+08, device='cuda:0')
c= tensor(5.4892e+08, device='cuda:0')
c= tensor(5.4893e+08, device='cuda:0')
c= tensor(5.4894e+08, device='cuda:0')
c= tensor(5.4894e+08, device='cuda:0')
c= tensor(5.4894e+08, device='cuda:0')
c= tensor(5.4895e+08, device='cuda:0')
c= tensor(5.4895e+08, device='cuda:0')
c= tensor(5.4896e+08, device='cuda:0')
c= tensor(5.4896e+08, device='cuda:0')
c= tensor(5.4898e+08, device='cuda:0')
c= tensor(5.4898e+08, device='cuda:0')
c= tensor(5.4900e+08, device='cuda:0')
c= tensor(5.4900e+08, device='cuda:0')
c= tensor(5.4901e+08, device='cuda:0')
c= tensor(5.4902e+08, device='cuda:0')
c= tensor(5.4902e+08, device='cuda:0')
c= tensor(5.4904e+08, device='cuda:0')
c= tensor(5.4905e+08, device='cuda:0')
c= tensor(5.4907e+08, device='cuda:0')
c= tensor(5.4908e+08, device='cuda:0')
c= tensor(5.4908e+08, device='cuda:0')
c= tensor(5.4908e+08, device='cuda:0')
c= tensor(5.4909e+08, device='cuda:0')
c= tensor(5.4909e+08, device='cuda:0')
c= tensor(5.4909e+08, device='cuda:0')
c= tensor(5.4911e+08, device='cuda:0')
c= tensor(5.4912e+08, device='cuda:0')
c= tensor(5.4912e+08, device='cuda:0')
c= tensor(5.4912e+08, device='cuda:0')
c= tensor(5.4913e+08, device='cuda:0')
c= tensor(5.4914e+08, device='cuda:0')
c= tensor(5.4915e+08, device='cuda:0')
c= tensor(5.4916e+08, device='cuda:0')
c= tensor(5.4916e+08, device='cuda:0')
c= tensor(5.4916e+08, device='cuda:0')
c= tensor(5.4917e+08, device='cuda:0')
c= tensor(5.4917e+08, device='cuda:0')
c= tensor(5.4918e+08, device='cuda:0')
c= tensor(5.4918e+08, device='cuda:0')
c= tensor(5.4919e+08, device='cuda:0')
c= tensor(5.4919e+08, device='cuda:0')
c= tensor(5.4920e+08, device='cuda:0')
c= tensor(5.4920e+08, device='cuda:0')
c= tensor(5.4921e+08, device='cuda:0')
c= tensor(5.4922e+08, device='cuda:0')
c= tensor(5.4922e+08, device='cuda:0')
c= tensor(5.4928e+08, device='cuda:0')
c= tensor(5.5438e+08, device='cuda:0')
c= tensor(5.5439e+08, device='cuda:0')
c= tensor(5.5440e+08, device='cuda:0')
c= tensor(5.5440e+08, device='cuda:0')
c= tensor(5.5442e+08, device='cuda:0')
c= tensor(5.5454e+08, device='cuda:0')
c= tensor(5.9484e+08, device='cuda:0')
c= tensor(5.9484e+08, device='cuda:0')
c= tensor(5.9691e+08, device='cuda:0')
c= tensor(6.0034e+08, device='cuda:0')
c= tensor(6.0064e+08, device='cuda:0')
c= tensor(6.1860e+08, device='cuda:0')
c= tensor(6.1860e+08, device='cuda:0')
c= tensor(6.1863e+08, device='cuda:0')
c= tensor(6.2667e+08, device='cuda:0')
c= tensor(6.7692e+08, device='cuda:0')
c= tensor(6.7693e+08, device='cuda:0')
c= tensor(6.7716e+08, device='cuda:0')
c= tensor(6.8698e+08, device='cuda:0')
c= tensor(6.9710e+08, device='cuda:0')
c= tensor(6.9800e+08, device='cuda:0')
c= tensor(6.9854e+08, device='cuda:0')
c= tensor(6.9897e+08, device='cuda:0')
c= tensor(6.9914e+08, device='cuda:0')
c= tensor(6.9930e+08, device='cuda:0')
c= tensor(7.2641e+08, device='cuda:0')
c= tensor(7.2642e+08, device='cuda:0')
c= tensor(7.2643e+08, device='cuda:0')
c= tensor(7.2700e+08, device='cuda:0')
c= tensor(7.2732e+08, device='cuda:0')
c= tensor(7.5329e+08, device='cuda:0')
c= tensor(7.5499e+08, device='cuda:0')
c= tensor(7.5499e+08, device='cuda:0')
c= tensor(7.5519e+08, device='cuda:0')
c= tensor(7.5522e+08, device='cuda:0')
c= tensor(7.5551e+08, device='cuda:0')
c= tensor(7.5649e+08, device='cuda:0')
c= tensor(7.5916e+08, device='cuda:0')
c= tensor(7.5968e+08, device='cuda:0')
c= tensor(7.5968e+08, device='cuda:0')
c= tensor(7.5970e+08, device='cuda:0')
c= tensor(7.6154e+08, device='cuda:0')
c= tensor(7.6301e+08, device='cuda:0')
c= tensor(7.6352e+08, device='cuda:0')
c= tensor(7.6353e+08, device='cuda:0')
c= tensor(7.9036e+08, device='cuda:0')
c= tensor(7.9040e+08, device='cuda:0')
c= tensor(7.9057e+08, device='cuda:0')
c= tensor(7.9305e+08, device='cuda:0')
c= tensor(7.9305e+08, device='cuda:0')
c= tensor(7.9432e+08, device='cuda:0')
c= tensor(8.1571e+08, device='cuda:0')
c= tensor(8.4374e+08, device='cuda:0')
c= tensor(8.4383e+08, device='cuda:0')
c= tensor(8.4393e+08, device='cuda:0')
c= tensor(8.4397e+08, device='cuda:0')
c= tensor(8.4398e+08, device='cuda:0')
c= tensor(8.4562e+08, device='cuda:0')
c= tensor(8.4566e+08, device='cuda:0')
c= tensor(8.4600e+08, device='cuda:0')
c= tensor(8.4975e+08, device='cuda:0')
c= tensor(8.5118e+08, device='cuda:0')
c= tensor(8.5131e+08, device='cuda:0')
c= tensor(8.5132e+08, device='cuda:0')
c= tensor(8.6150e+08, device='cuda:0')
c= tensor(8.6165e+08, device='cuda:0')
c= tensor(8.6205e+08, device='cuda:0')
c= tensor(8.6209e+08, device='cuda:0')
c= tensor(9.0285e+08, device='cuda:0')
c= tensor(9.0288e+08, device='cuda:0')
c= tensor(9.1069e+08, device='cuda:0')
c= tensor(9.1074e+08, device='cuda:0')
c= tensor(9.1270e+08, device='cuda:0')
c= tensor(9.1289e+08, device='cuda:0')
c= tensor(9.1742e+08, device='cuda:0')
c= tensor(9.1880e+08, device='cuda:0')
c= tensor(9.1880e+08, device='cuda:0')
c= tensor(9.2293e+08, device='cuda:0')
c= tensor(9.2471e+08, device='cuda:0')
c= tensor(9.2473e+08, device='cuda:0')
c= tensor(9.2674e+08, device='cuda:0')
c= tensor(9.3309e+08, device='cuda:0')
c= tensor(9.4797e+08, device='cuda:0')
c= tensor(9.6114e+08, device='cuda:0')
c= tensor(9.6116e+08, device='cuda:0')
c= tensor(9.6118e+08, device='cuda:0')
c= tensor(9.6134e+08, device='cuda:0')
c= tensor(9.6206e+08, device='cuda:0')
c= tensor(9.6209e+08, device='cuda:0')
c= tensor(9.6210e+08, device='cuda:0')
c= tensor(9.6285e+08, device='cuda:0')
c= tensor(9.6652e+08, device='cuda:0')
c= tensor(9.6940e+08, device='cuda:0')
c= tensor(9.6941e+08, device='cuda:0')
c= tensor(9.6965e+08, device='cuda:0')
c= tensor(9.6969e+08, device='cuda:0')
c= tensor(9.6972e+08, device='cuda:0')
c= tensor(9.6975e+08, device='cuda:0')
c= tensor(9.6976e+08, device='cuda:0')
c= tensor(9.9092e+08, device='cuda:0')
c= tensor(9.9123e+08, device='cuda:0')
c= tensor(9.9137e+08, device='cuda:0')
c= tensor(9.9184e+08, device='cuda:0')
c= tensor(9.9187e+08, device='cuda:0')
c= tensor(1.0482e+09, device='cuda:0')
c= tensor(1.0482e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0517e+09, device='cuda:0')
c= tensor(1.0527e+09, device='cuda:0')
c= tensor(1.0527e+09, device='cuda:0')
c= tensor(1.0528e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0732e+09, device='cuda:0')
c= tensor(1.0738e+09, device='cuda:0')
c= tensor(1.0757e+09, device='cuda:0')
c= tensor(1.0825e+09, device='cuda:0')
c= tensor(1.0825e+09, device='cuda:0')
c= tensor(1.0826e+09, device='cuda:0')
c= tensor(1.0828e+09, device='cuda:0')
c= tensor(1.0828e+09, device='cuda:0')
c= tensor(1.0828e+09, device='cuda:0')
c= tensor(1.0829e+09, device='cuda:0')
c= tensor(1.0829e+09, device='cuda:0')
c= tensor(1.0830e+09, device='cuda:0')
c= tensor(1.0830e+09, device='cuda:0')
c= tensor(1.0830e+09, device='cuda:0')
c= tensor(1.1624e+09, device='cuda:0')
c= tensor(1.1631e+09, device='cuda:0')
c= tensor(1.1652e+09, device='cuda:0')
c= tensor(1.1654e+09, device='cuda:0')
c= tensor(1.1654e+09, device='cuda:0')
c= tensor(1.1655e+09, device='cuda:0')
c= tensor(1.2363e+09, device='cuda:0')
c= tensor(1.2811e+09, device='cuda:0')
c= tensor(1.2816e+09, device='cuda:0')
c= tensor(1.2821e+09, device='cuda:0')
c= tensor(1.2821e+09, device='cuda:0')
c= tensor(1.2851e+09, device='cuda:0')
c= tensor(1.3413e+09, device='cuda:0')
c= tensor(1.3463e+09, device='cuda:0')
c= tensor(1.3463e+09, device='cuda:0')
c= tensor(1.3495e+09, device='cuda:0')
c= tensor(1.3818e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3837e+09, device='cuda:0')
c= tensor(1.3837e+09, device='cuda:0')
c= tensor(1.3838e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.3920e+09, device='cuda:0')
c= tensor(1.3920e+09, device='cuda:0')
c= tensor(1.3924e+09, device='cuda:0')
c= tensor(1.3924e+09, device='cuda:0')
c= tensor(1.3924e+09, device='cuda:0')
c= tensor(1.3956e+09, device='cuda:0')
c= tensor(1.4005e+09, device='cuda:0')
c= tensor(1.4042e+09, device='cuda:0')
c= tensor(1.4111e+09, device='cuda:0')
c= tensor(1.4156e+09, device='cuda:0')
c= tensor(1.4158e+09, device='cuda:0')
c= tensor(1.4167e+09, device='cuda:0')
c= tensor(1.4184e+09, device='cuda:0')
c= tensor(1.4238e+09, device='cuda:0')
c= tensor(1.4239e+09, device='cuda:0')
c= tensor(1.4290e+09, device='cuda:0')
c= tensor(1.4392e+09, device='cuda:0')
c= tensor(1.4414e+09, device='cuda:0')
c= tensor(1.4422e+09, device='cuda:0')
c= tensor(1.4465e+09, device='cuda:0')
c= tensor(1.4465e+09, device='cuda:0')
c= tensor(1.4465e+09, device='cuda:0')
c= tensor(1.4473e+09, device='cuda:0')
c= tensor(1.4508e+09, device='cuda:0')
c= tensor(1.4532e+09, device='cuda:0')
c= tensor(1.5663e+09, device='cuda:0')
c= tensor(1.5771e+09, device='cuda:0')
c= tensor(1.5805e+09, device='cuda:0')
c= tensor(1.5808e+09, device='cuda:0')
c= tensor(1.5831e+09, device='cuda:0')
c= tensor(1.5831e+09, device='cuda:0')
c= tensor(1.5831e+09, device='cuda:0')
c= tensor(1.5947e+09, device='cuda:0')
c= tensor(1.5948e+09, device='cuda:0')
c= tensor(1.5948e+09, device='cuda:0')
c= tensor(1.5949e+09, device='cuda:0')
c= tensor(1.6043e+09, device='cuda:0')
c= tensor(1.6048e+09, device='cuda:0')
c= tensor(1.6072e+09, device='cuda:0')
c= tensor(1.6073e+09, device='cuda:0')
c= tensor(1.6074e+09, device='cuda:0')
c= tensor(1.6074e+09, device='cuda:0')
c= tensor(1.6079e+09, device='cuda:0')
c= tensor(1.6081e+09, device='cuda:0')
c= tensor(1.6100e+09, device='cuda:0')
c= tensor(1.6100e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.6156e+09, device='cuda:0')
c= tensor(1.6156e+09, device='cuda:0')
c= tensor(1.6172e+09, device='cuda:0')
c= tensor(1.6172e+09, device='cuda:0')
c= tensor(1.6182e+09, device='cuda:0')
c= tensor(1.6182e+09, device='cuda:0')
c= tensor(1.6184e+09, device='cuda:0')
c= tensor(1.6194e+09, device='cuda:0')
c= tensor(1.6754e+09, device='cuda:0')
c= tensor(1.6754e+09, device='cuda:0')
c= tensor(1.6756e+09, device='cuda:0')
c= tensor(1.6818e+09, device='cuda:0')
c= tensor(1.6819e+09, device='cuda:0')
c= tensor(1.7210e+09, device='cuda:0')
c= tensor(1.7210e+09, device='cuda:0')
c= tensor(1.7233e+09, device='cuda:0')
c= tensor(1.7289e+09, device='cuda:0')
c= tensor(1.7289e+09, device='cuda:0')
c= tensor(1.7473e+09, device='cuda:0')
c= tensor(1.7481e+09, device='cuda:0')
c= tensor(1.7912e+09, device='cuda:0')
c= tensor(1.7912e+09, device='cuda:0')
c= tensor(1.7918e+09, device='cuda:0')
c= tensor(1.7918e+09, device='cuda:0')
c= tensor(1.7918e+09, device='cuda:0')
c= tensor(1.7918e+09, device='cuda:0')
c= tensor(1.7925e+09, device='cuda:0')
c= tensor(1.7928e+09, device='cuda:0')
c= tensor(1.7963e+09, device='cuda:0')
c= tensor(1.7964e+09, device='cuda:0')
c= tensor(1.7964e+09, device='cuda:0')
c= tensor(1.7964e+09, device='cuda:0')
c= tensor(1.8013e+09, device='cuda:0')
c= tensor(1.8019e+09, device='cuda:0')
c= tensor(1.8078e+09, device='cuda:0')
c= tensor(1.8090e+09, device='cuda:0')
c= tensor(1.8091e+09, device='cuda:0')
c= tensor(1.8091e+09, device='cuda:0')
c= tensor(1.8091e+09, device='cuda:0')
c= tensor(1.8748e+09, device='cuda:0')
c= tensor(1.8748e+09, device='cuda:0')
c= tensor(1.8750e+09, device='cuda:0')
c= tensor(1.8868e+09, device='cuda:0')
c= tensor(1.8877e+09, device='cuda:0')
c= tensor(1.8877e+09, device='cuda:0')
c= tensor(1.8877e+09, device='cuda:0')
c= tensor(1.9256e+09, device='cuda:0')
c= tensor(1.9258e+09, device='cuda:0')
c= tensor(1.9270e+09, device='cuda:0')
c= tensor(1.9279e+09, device='cuda:0')
c= tensor(1.9383e+09, device='cuda:0')
c= tensor(1.9411e+09, device='cuda:0')
c= tensor(1.9590e+09, device='cuda:0')
c= tensor(1.9600e+09, device='cuda:0')
c= tensor(1.9600e+09, device='cuda:0')
c= tensor(1.9607e+09, device='cuda:0')
c= tensor(1.9608e+09, device='cuda:0')
c= tensor(1.9609e+09, device='cuda:0')
c= tensor(1.9610e+09, device='cuda:0')
c= tensor(1.9610e+09, device='cuda:0')
c= tensor(1.9646e+09, device='cuda:0')
c= tensor(1.9647e+09, device='cuda:0')
c= tensor(1.9647e+09, device='cuda:0')
c= tensor(1.9648e+09, device='cuda:0')
c= tensor(1.9648e+09, device='cuda:0')
c= tensor(1.9741e+09, device='cuda:0')
c= tensor(1.9761e+09, device='cuda:0')
c= tensor(1.9762e+09, device='cuda:0')
c= tensor(1.9762e+09, device='cuda:0')
c= tensor(1.9766e+09, device='cuda:0')
c= tensor(1.9766e+09, device='cuda:0')
c= tensor(1.9767e+09, device='cuda:0')
c= tensor(1.9768e+09, device='cuda:0')
c= tensor(1.9873e+09, device='cuda:0')
c= tensor(1.9874e+09, device='cuda:0')
c= tensor(1.9874e+09, device='cuda:0')
c= tensor(1.9874e+09, device='cuda:0')
c= tensor(2.0046e+09, device='cuda:0')
c= tensor(2.1057e+09, device='cuda:0')
c= tensor(2.1062e+09, device='cuda:0')
c= tensor(2.1062e+09, device='cuda:0')
c= tensor(2.1082e+09, device='cuda:0')
c= tensor(2.1108e+09, device='cuda:0')
c= tensor(2.1108e+09, device='cuda:0')
c= tensor(2.1110e+09, device='cuda:0')
c= tensor(2.1111e+09, device='cuda:0')
c= tensor(2.1256e+09, device='cuda:0')
c= tensor(2.1597e+09, device='cuda:0')
c= tensor(2.1629e+09, device='cuda:0')
c= tensor(2.1629e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1634e+09, device='cuda:0')
c= tensor(2.1647e+09, device='cuda:0')
c= tensor(2.1653e+09, device='cuda:0')
c= tensor(2.1789e+09, device='cuda:0')
c= tensor(2.1790e+09, device='cuda:0')
c= tensor(2.1790e+09, device='cuda:0')
c= tensor(2.1791e+09, device='cuda:0')
c= tensor(2.8254e+09, device='cuda:0')
c= tensor(2.8258e+09, device='cuda:0')
c= tensor(2.8258e+09, device='cuda:0')
c= tensor(2.8258e+09, device='cuda:0')
c= tensor(2.8262e+09, device='cuda:0')
c= tensor(2.8262e+09, device='cuda:0')
c= tensor(2.8281e+09, device='cuda:0')
c= tensor(2.8281e+09, device='cuda:0')
c= tensor(2.8282e+09, device='cuda:0')
c= tensor(2.8283e+09, device='cuda:0')
c= tensor(2.8283e+09, device='cuda:0')
c= tensor(2.8283e+09, device='cuda:0')
c= tensor(2.8337e+09, device='cuda:0')
c= tensor(2.8665e+09, device='cuda:0')
c= tensor(2.8719e+09, device='cuda:0')
c= tensor(2.8781e+09, device='cuda:0')
c= tensor(2.8783e+09, device='cuda:0')
c= tensor(2.8783e+09, device='cuda:0')
c= tensor(2.8784e+09, device='cuda:0')
c= tensor(2.8804e+09, device='cuda:0')
c= tensor(2.8829e+09, device='cuda:0')
c= tensor(2.8934e+09, device='cuda:0')
c= tensor(2.8935e+09, device='cuda:0')
c= tensor(3.0378e+09, device='cuda:0')
c= tensor(3.0380e+09, device='cuda:0')
c= tensor(3.0386e+09, device='cuda:0')
c= tensor(3.0472e+09, device='cuda:0')
c= tensor(3.0475e+09, device='cuda:0')
c= tensor(3.0644e+09, device='cuda:0')
c= tensor(3.0939e+09, device='cuda:0')
c= tensor(3.0958e+09, device='cuda:0')
c= tensor(3.0961e+09, device='cuda:0')
c= tensor(3.0962e+09, device='cuda:0')
c= tensor(3.0974e+09, device='cuda:0')
c= tensor(3.0974e+09, device='cuda:0')
c= tensor(3.1047e+09, device='cuda:0')
c= tensor(3.2210e+09, device='cuda:0')
c= tensor(3.2302e+09, device='cuda:0')
c= tensor(3.2349e+09, device='cuda:0')
c= tensor(3.2350e+09, device='cuda:0')
c= tensor(3.2360e+09, device='cuda:0')
c= tensor(3.2362e+09, device='cuda:0')
c= tensor(3.2364e+09, device='cuda:0')
c= tensor(3.2386e+09, device='cuda:0')
c= tensor(3.2522e+09, device='cuda:0')
c= tensor(3.2523e+09, device='cuda:0')
c= tensor(3.2964e+09, device='cuda:0')
c= tensor(3.3167e+09, device='cuda:0')
c= tensor(3.3175e+09, device='cuda:0')
c= tensor(3.3175e+09, device='cuda:0')
c= tensor(3.3196e+09, device='cuda:0')
c= tensor(3.3224e+09, device='cuda:0')
c= tensor(3.3224e+09, device='cuda:0')
c= tensor(3.3954e+09, device='cuda:0')
c= tensor(3.3962e+09, device='cuda:0')
c= tensor(3.3965e+09, device='cuda:0')
c= tensor(3.3965e+09, device='cuda:0')
c= tensor(3.3965e+09, device='cuda:0')
c= tensor(3.3965e+09, device='cuda:0')
c= tensor(3.3966e+09, device='cuda:0')
c= tensor(3.3968e+09, device='cuda:0')
c= tensor(3.3985e+09, device='cuda:0')
c= tensor(4.9717e+09, device='cuda:0')
c= tensor(4.9718e+09, device='cuda:0')
c= tensor(4.9727e+09, device='cuda:0')
c= tensor(4.9727e+09, device='cuda:0')
c= tensor(4.9728e+09, device='cuda:0')
c= tensor(4.9728e+09, device='cuda:0')
c= tensor(4.9982e+09, device='cuda:0')
c= tensor(4.9993e+09, device='cuda:0')
c= tensor(5.1833e+09, device='cuda:0')
c= tensor(5.1833e+09, device='cuda:0')
c= tensor(5.1888e+09, device='cuda:0')
c= tensor(5.1894e+09, device='cuda:0')
c= tensor(5.1943e+09, device='cuda:0')
c= tensor(5.2209e+09, device='cuda:0')
c= tensor(5.2211e+09, device='cuda:0')
c= tensor(5.2211e+09, device='cuda:0')
c= tensor(5.2222e+09, device='cuda:0')
c= tensor(5.2223e+09, device='cuda:0')
c= tensor(5.2233e+09, device='cuda:0')
c= tensor(5.2262e+09, device='cuda:0')
c= tensor(5.2273e+09, device='cuda:0')
c= tensor(5.2294e+09, device='cuda:0')
c= tensor(5.2297e+09, device='cuda:0')
c= tensor(5.2352e+09, device='cuda:0')
c= tensor(5.2738e+09, device='cuda:0')
c= tensor(5.2740e+09, device='cuda:0')
c= tensor(5.2741e+09, device='cuda:0')
c= tensor(5.2890e+09, device='cuda:0')
c= tensor(5.2896e+09, device='cuda:0')
c= tensor(5.2950e+09, device='cuda:0')
c= tensor(5.2963e+09, device='cuda:0')
c= tensor(5.2982e+09, device='cuda:0')
c= tensor(5.2989e+09, device='cuda:0')
c= tensor(5.3052e+09, device='cuda:0')
c= tensor(5.3062e+09, device='cuda:0')
c= tensor(5.3064e+09, device='cuda:0')
c= tensor(5.3064e+09, device='cuda:0')
c= tensor(5.3069e+09, device='cuda:0')
c= tensor(5.3087e+09, device='cuda:0')
c= tensor(5.3134e+09, device='cuda:0')
c= tensor(5.3178e+09, device='cuda:0')
c= tensor(5.3178e+09, device='cuda:0')
c= tensor(5.3180e+09, device='cuda:0')
c= tensor(5.3188e+09, device='cuda:0')
c= tensor(5.3433e+09, device='cuda:0')
c= tensor(5.3437e+09, device='cuda:0')
c= tensor(5.3446e+09, device='cuda:0')
c= tensor(5.3482e+09, device='cuda:0')
c= tensor(5.3484e+09, device='cuda:0')
c= tensor(5.3484e+09, device='cuda:0')
c= tensor(5.3484e+09, device='cuda:0')
c= tensor(5.3511e+09, device='cuda:0')
c= tensor(5.3520e+09, device='cuda:0')
c= tensor(5.3525e+09, device='cuda:0')
c= tensor(5.3525e+09, device='cuda:0')
c= tensor(5.3525e+09, device='cuda:0')
c= tensor(5.3568e+09, device='cuda:0')
c= tensor(5.3581e+09, device='cuda:0')
c= tensor(5.3582e+09, device='cuda:0')
c= tensor(5.3582e+09, device='cuda:0')
c= tensor(5.3582e+09, device='cuda:0')
c= tensor(5.3585e+09, device='cuda:0')
c= tensor(5.3588e+09, device='cuda:0')
c= tensor(5.3621e+09, device='cuda:0')
c= tensor(5.3622e+09, device='cuda:0')
c= tensor(5.3787e+09, device='cuda:0')
c= tensor(5.3788e+09, device='cuda:0')
c= tensor(5.3788e+09, device='cuda:0')
c= tensor(5.4044e+09, device='cuda:0')
c= tensor(5.4044e+09, device='cuda:0')
c= tensor(5.4070e+09, device='cuda:0')
c= tensor(5.4128e+09, device='cuda:0')
c= tensor(5.4134e+09, device='cuda:0')
c= tensor(5.4151e+09, device='cuda:0')
c= tensor(5.4259e+09, device='cuda:0')
c= tensor(5.4259e+09, device='cuda:0')
c= tensor(5.4269e+09, device='cuda:0')
c= tensor(5.4270e+09, device='cuda:0')
c= tensor(5.4399e+09, device='cuda:0')
c= tensor(5.4461e+09, device='cuda:0')
c= tensor(5.4466e+09, device='cuda:0')
c= tensor(5.4474e+09, device='cuda:0')
c= tensor(5.4475e+09, device='cuda:0')
c= tensor(5.4485e+09, device='cuda:0')
c= tensor(5.4491e+09, device='cuda:0')
c= tensor(5.4492e+09, device='cuda:0')
c= tensor(5.4565e+09, device='cuda:0')
c= tensor(5.4771e+09, device='cuda:0')
c= tensor(5.4771e+09, device='cuda:0')
c= tensor(5.4772e+09, device='cuda:0')
c= tensor(5.4773e+09, device='cuda:0')
c= tensor(5.5031e+09, device='cuda:0')
c= tensor(5.5032e+09, device='cuda:0')
c= tensor(5.5033e+09, device='cuda:0')
c= tensor(5.5043e+09, device='cuda:0')
c= tensor(5.5061e+09, device='cuda:0')
c= tensor(5.5061e+09, device='cuda:0')
c= tensor(5.5065e+09, device='cuda:0')
c= tensor(5.5065e+09, device='cuda:0')
memory (bytes)
4867002368
time for making loss 2 is 12.613130807876587
p0 True
it  0 : 2079867392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 40% |
shape of L is 
torch.Size([])
memory (bytes)
4867264512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4868177920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  68002030000.0
relative error loss 12.349402
shape of L is 
torch.Size([])
memory (bytes)
4999180288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  3% | 15% |
memory (bytes)
4999364608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 15% |
error is  68001767000.0
relative error loss 12.349355
shape of L is 
torch.Size([])
memory (bytes)
5001039872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5001203712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  68001150000.0
relative error loss 12.349242
shape of L is 
torch.Size([])
memory (bytes)
5002633216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 15% |
memory (bytes)
5002661888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  67997123000.0
relative error loss 12.348512
shape of L is 
torch.Size([])
memory (bytes)
5004652544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5004820480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  67974943000.0
relative error loss 12.344483
shape of L is 
torch.Size([])
memory (bytes)
5006909440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5006909440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  67731610000.0
relative error loss 12.300294
shape of L is 
torch.Size([])
memory (bytes)
5008891904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5009031168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  66524590000.0
relative error loss 12.081095
shape of L is 
torch.Size([])
memory (bytes)
5011128320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5011144704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  60202914000.0
relative error loss 10.933056
shape of L is 
torch.Size([])
memory (bytes)
5013159936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5013286912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  33927250000.0
relative error loss 6.1613054
shape of L is 
torch.Size([])
memory (bytes)
5015429120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5015445504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  13309706000.0
relative error loss 2.4170885
time to take a step is 211.25253582000732
it  1 : 2426027008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5017505792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5017505792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  13309706000.0
relative error loss 2.4170885
shape of L is 
torch.Size([])
memory (bytes)
5019664384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 15% |
memory (bytes)
5019705344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  8091811000.0
relative error loss 1.4695007
shape of L is 
torch.Size([])
memory (bytes)
5021814784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5021814784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  6235107000.0
relative error loss 1.1323168
shape of L is 
torch.Size([])
memory (bytes)
5023637504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 15% |
memory (bytes)
5023862784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  5261817000.0
relative error loss 0.955564
shape of L is 
torch.Size([])
memory (bytes)
5026078720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5026078720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  56554185000.0
relative error loss 10.270434
shape of L is 
torch.Size([])
memory (bytes)
5028028416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5028220928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  5025717000.0
relative error loss 0.91268754
shape of L is 
torch.Size([])
memory (bytes)
5030318080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 15% |
memory (bytes)
5030318080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  4797757000.0
relative error loss 0.87128913
shape of L is 
torch.Size([])
memory (bytes)
5032398848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5032423424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  4586619400.0
relative error loss 0.8329458
shape of L is 
torch.Size([])
memory (bytes)
5034512384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5034536960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  3987520800.0
relative error loss 0.7241475
shape of L is 
torch.Size([])
memory (bytes)
5036601344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5036601344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  3803183400.0
relative error loss 0.69067115
time to take a step is 167.0513916015625
it  2 : 2702955520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5038624768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 15% |
memory (bytes)
5038624768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  3803183400.0
relative error loss 0.69067115
shape of L is 
torch.Size([])
memory (bytes)
5040623616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5040852992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  3829615600.0
relative error loss 0.69547135
shape of L is 
torch.Size([])
memory (bytes)
5042917376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5042917376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 15% |
error is  3523834400.0
relative error loss 0.63994044
shape of L is 
torch.Size([])
memory (bytes)
5045108736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 15% |
memory (bytes)
5045108736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  3366889200.0
relative error loss 0.61143863
shape of L is 
torch.Size([])
memory (bytes)
5047160832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5047160832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  3097840400.0
relative error loss 0.56257844
shape of L is 
torch.Size([])
memory (bytes)
5049077760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5049339904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2884686000.0
relative error loss 0.5238689
shape of L is 
torch.Size([])
memory (bytes)
5051224064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 15% |
memory (bytes)
5051224064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2757032400.0
relative error loss 0.5006866
shape of L is 
torch.Size([])
memory (bytes)
5053452288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 15% |
memory (bytes)
5053603840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2676096800.0
relative error loss 0.48598838
shape of L is 
torch.Size([])
memory (bytes)
5055594496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 15% |
memory (bytes)
5055750144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2498475300.0
relative error loss 0.4537317
shape of L is 
torch.Size([])
memory (bytes)
5057863680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5057880064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2263724300.0
relative error loss 0.41110012
time to take a step is 145.2733826637268
it  3 : 2702955520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5059862528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5060014080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2263724300.0
relative error loss 0.41110012
shape of L is 
torch.Size([])
memory (bytes)
5062057984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5062057984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2080234000.0
relative error loss 0.37777764
shape of L is 
torch.Size([])
memory (bytes)
5064294400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5064306688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 15% |
error is  1948529200.0
relative error loss 0.3538596
shape of L is 
torch.Size([])
memory (bytes)
5066240000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 15% |
memory (bytes)
5066448896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1812575200.0
relative error loss 0.3291699
shape of L is 
torch.Size([])
memory (bytes)
5068537856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5068537856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1595970300.0
relative error loss 0.2898337
shape of L is 
torch.Size([])
memory (bytes)
5070708736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5070708736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1492474400.0
relative error loss 0.27103847
shape of L is 
torch.Size([])
memory (bytes)
5072814080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5072814080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1605459200.0
relative error loss 0.29155692
shape of L is 
torch.Size([])
memory (bytes)
5074685952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5074898944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1376976900.0
relative error loss 0.25006375
shape of L is 
torch.Size([])
memory (bytes)
5077086208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5077086208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1274474200.0
relative error loss 0.2314489
shape of L is 
torch.Size([])
memory (bytes)
5079228416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 15% |
memory (bytes)
5079228416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1219960600.0
relative error loss 0.22154903
time to take a step is 205.05356884002686
c= tensor(1176.6621, device='cuda:0')
c= tensor(144267.8125, device='cuda:0')
c= tensor(148919.5938, device='cuda:0')
c= tensor(159175.0312, device='cuda:0')
c= tensor(710701.2500, device='cuda:0')
c= tensor(971854.6875, device='cuda:0')
c= tensor(1784429.7500, device='cuda:0')
c= tensor(2212288.7500, device='cuda:0')
c= tensor(2278791.5000, device='cuda:0')
c= tensor(5624209.5000, device='cuda:0')
c= tensor(5668916.5000, device='cuda:0')
c= tensor(12094471., device='cuda:0')
c= tensor(12109491., device='cuda:0')
c= tensor(26185400., device='cuda:0')
c= tensor(26577328., device='cuda:0')
c= tensor(26947340., device='cuda:0')
c= tensor(27778144., device='cuda:0')
c= tensor(28829474., device='cuda:0')
c= tensor(36137436., device='cuda:0')
c= tensor(39219180., device='cuda:0')
c= tensor(39267664., device='cuda:0')
c= tensor(57892636., device='cuda:0')
c= tensor(57970568., device='cuda:0')
c= tensor(58249356., device='cuda:0')
c= tensor(59762528., device='cuda:0')
c= tensor(60792884., device='cuda:0')
c= tensor(62128048., device='cuda:0')
c= tensor(62195668., device='cuda:0')
c= tensor(65464084., device='cuda:0')
c= tensor(4.4589e+08, device='cuda:0')
c= tensor(4.4593e+08, device='cuda:0')
c= tensor(5.4214e+08, device='cuda:0')
c= tensor(5.4231e+08, device='cuda:0')
c= tensor(5.4234e+08, device='cuda:0')
c= tensor(5.4236e+08, device='cuda:0')
c= tensor(5.4637e+08, device='cuda:0')
c= tensor(5.4745e+08, device='cuda:0')
c= tensor(5.4745e+08, device='cuda:0')
c= tensor(5.4746e+08, device='cuda:0')
c= tensor(5.4746e+08, device='cuda:0')
c= tensor(5.4747e+08, device='cuda:0')
c= tensor(5.4748e+08, device='cuda:0')
c= tensor(5.4748e+08, device='cuda:0')
c= tensor(5.4749e+08, device='cuda:0')
c= tensor(5.4750e+08, device='cuda:0')
c= tensor(5.4750e+08, device='cuda:0')
c= tensor(5.4751e+08, device='cuda:0')
c= tensor(5.4752e+08, device='cuda:0')
c= tensor(5.4752e+08, device='cuda:0')
c= tensor(5.4757e+08, device='cuda:0')
c= tensor(5.4763e+08, device='cuda:0')
c= tensor(5.4763e+08, device='cuda:0')
c= tensor(5.4764e+08, device='cuda:0')
c= tensor(5.4765e+08, device='cuda:0')
c= tensor(5.4766e+08, device='cuda:0')
c= tensor(5.4767e+08, device='cuda:0')
c= tensor(5.4767e+08, device='cuda:0')
c= tensor(5.4768e+08, device='cuda:0')
c= tensor(5.4769e+08, device='cuda:0')
c= tensor(5.4769e+08, device='cuda:0')
c= tensor(5.4770e+08, device='cuda:0')
c= tensor(5.4771e+08, device='cuda:0')
c= tensor(5.4773e+08, device='cuda:0')
c= tensor(5.4776e+08, device='cuda:0')
c= tensor(5.4777e+08, device='cuda:0')
c= tensor(5.4777e+08, device='cuda:0')
c= tensor(5.4777e+08, device='cuda:0')
c= tensor(5.4779e+08, device='cuda:0')
c= tensor(5.4779e+08, device='cuda:0')
c= tensor(5.4780e+08, device='cuda:0')
c= tensor(5.4781e+08, device='cuda:0')
c= tensor(5.4781e+08, device='cuda:0')
c= tensor(5.4782e+08, device='cuda:0')
c= tensor(5.4782e+08, device='cuda:0')
c= tensor(5.4783e+08, device='cuda:0')
c= tensor(5.4785e+08, device='cuda:0')
c= tensor(5.4785e+08, device='cuda:0')
c= tensor(5.4785e+08, device='cuda:0')
c= tensor(5.4786e+08, device='cuda:0')
c= tensor(5.4792e+08, device='cuda:0')
c= tensor(5.4792e+08, device='cuda:0')
c= tensor(5.4793e+08, device='cuda:0')
c= tensor(5.4794e+08, device='cuda:0')
c= tensor(5.4794e+08, device='cuda:0')
c= tensor(5.4794e+08, device='cuda:0')
c= tensor(5.4795e+08, device='cuda:0')
c= tensor(5.4795e+08, device='cuda:0')
c= tensor(5.4795e+08, device='cuda:0')
c= tensor(5.4796e+08, device='cuda:0')
c= tensor(5.4797e+08, device='cuda:0')
c= tensor(5.4797e+08, device='cuda:0')
c= tensor(5.4797e+08, device='cuda:0')
c= tensor(5.4798e+08, device='cuda:0')
c= tensor(5.4798e+08, device='cuda:0')
c= tensor(5.4799e+08, device='cuda:0')
c= tensor(5.4800e+08, device='cuda:0')
c= tensor(5.4801e+08, device='cuda:0')
c= tensor(5.4803e+08, device='cuda:0')
c= tensor(5.4803e+08, device='cuda:0')
c= tensor(5.4804e+08, device='cuda:0')
c= tensor(5.4807e+08, device='cuda:0')
c= tensor(5.4808e+08, device='cuda:0')
c= tensor(5.4809e+08, device='cuda:0')
c= tensor(5.4810e+08, device='cuda:0')
c= tensor(5.4812e+08, device='cuda:0')
c= tensor(5.4813e+08, device='cuda:0')
c= tensor(5.4814e+08, device='cuda:0')
c= tensor(5.4814e+08, device='cuda:0')
c= tensor(5.4815e+08, device='cuda:0')
c= tensor(5.4815e+08, device='cuda:0')
c= tensor(5.4815e+08, device='cuda:0')
c= tensor(5.4815e+08, device='cuda:0')
c= tensor(5.4816e+08, device='cuda:0')
c= tensor(5.4817e+08, device='cuda:0')
c= tensor(5.4818e+08, device='cuda:0')
c= tensor(5.4818e+08, device='cuda:0')
c= tensor(5.4818e+08, device='cuda:0')
c= tensor(5.4818e+08, device='cuda:0')
c= tensor(5.4820e+08, device='cuda:0')
c= tensor(5.4821e+08, device='cuda:0')
c= tensor(5.4824e+08, device='cuda:0')
c= tensor(5.4825e+08, device='cuda:0')
c= tensor(5.4825e+08, device='cuda:0')
c= tensor(5.4826e+08, device='cuda:0')
c= tensor(5.4826e+08, device='cuda:0')
c= tensor(5.4826e+08, device='cuda:0')
c= tensor(5.4826e+08, device='cuda:0')
c= tensor(5.4827e+08, device='cuda:0')
c= tensor(5.4832e+08, device='cuda:0')
c= tensor(5.4832e+08, device='cuda:0')
c= tensor(5.4834e+08, device='cuda:0')
c= tensor(5.4834e+08, device='cuda:0')
c= tensor(5.4835e+08, device='cuda:0')
c= tensor(5.4835e+08, device='cuda:0')
c= tensor(5.4836e+08, device='cuda:0')
c= tensor(5.4836e+08, device='cuda:0')
c= tensor(5.4836e+08, device='cuda:0')
c= tensor(5.4837e+08, device='cuda:0')
c= tensor(5.4837e+08, device='cuda:0')
c= tensor(5.4837e+08, device='cuda:0')
c= tensor(5.4838e+08, device='cuda:0')
c= tensor(5.4838e+08, device='cuda:0')
c= tensor(5.4842e+08, device='cuda:0')
c= tensor(5.4844e+08, device='cuda:0')
c= tensor(5.4846e+08, device='cuda:0')
c= tensor(5.4846e+08, device='cuda:0')
c= tensor(5.4847e+08, device='cuda:0')
c= tensor(5.4848e+08, device='cuda:0')
c= tensor(5.4848e+08, device='cuda:0')
c= tensor(5.4849e+08, device='cuda:0')
c= tensor(5.4849e+08, device='cuda:0')
c= tensor(5.4850e+08, device='cuda:0')
c= tensor(5.4850e+08, device='cuda:0')
c= tensor(5.4855e+08, device='cuda:0')
c= tensor(5.4855e+08, device='cuda:0')
c= tensor(5.4861e+08, device='cuda:0')
c= tensor(5.4861e+08, device='cuda:0')
c= tensor(5.4861e+08, device='cuda:0')
c= tensor(5.4862e+08, device='cuda:0')
c= tensor(5.4862e+08, device='cuda:0')
c= tensor(5.4864e+08, device='cuda:0')
c= tensor(5.4864e+08, device='cuda:0')
c= tensor(5.4865e+08, device='cuda:0')
c= tensor(5.4865e+08, device='cuda:0')
c= tensor(5.4866e+08, device='cuda:0')
c= tensor(5.4867e+08, device='cuda:0')
c= tensor(5.4867e+08, device='cuda:0')
c= tensor(5.4868e+08, device='cuda:0')
c= tensor(5.4869e+08, device='cuda:0')
c= tensor(5.4869e+08, device='cuda:0')
c= tensor(5.4869e+08, device='cuda:0')
c= tensor(5.4869e+08, device='cuda:0')
c= tensor(5.4871e+08, device='cuda:0')
c= tensor(5.4871e+08, device='cuda:0')
c= tensor(5.4872e+08, device='cuda:0')
c= tensor(5.4873e+08, device='cuda:0')
c= tensor(5.4874e+08, device='cuda:0')
c= tensor(5.4875e+08, device='cuda:0')
c= tensor(5.4877e+08, device='cuda:0')
c= tensor(5.4877e+08, device='cuda:0')
c= tensor(5.4878e+08, device='cuda:0')
c= tensor(5.4878e+08, device='cuda:0')
c= tensor(5.4879e+08, device='cuda:0')
c= tensor(5.4880e+08, device='cuda:0')
c= tensor(5.4881e+08, device='cuda:0')
c= tensor(5.4882e+08, device='cuda:0')
c= tensor(5.4883e+08, device='cuda:0')
c= tensor(5.4883e+08, device='cuda:0')
c= tensor(5.4885e+08, device='cuda:0')
c= tensor(5.4891e+08, device='cuda:0')
c= tensor(5.4891e+08, device='cuda:0')
c= tensor(5.4891e+08, device='cuda:0')
c= tensor(5.4892e+08, device='cuda:0')
c= tensor(5.4892e+08, device='cuda:0')
c= tensor(5.4893e+08, device='cuda:0')
c= tensor(5.4894e+08, device='cuda:0')
c= tensor(5.4894e+08, device='cuda:0')
c= tensor(5.4894e+08, device='cuda:0')
c= tensor(5.4895e+08, device='cuda:0')
c= tensor(5.4895e+08, device='cuda:0')
c= tensor(5.4896e+08, device='cuda:0')
c= tensor(5.4896e+08, device='cuda:0')
c= tensor(5.4898e+08, device='cuda:0')
c= tensor(5.4898e+08, device='cuda:0')
c= tensor(5.4900e+08, device='cuda:0')
c= tensor(5.4900e+08, device='cuda:0')
c= tensor(5.4901e+08, device='cuda:0')
c= tensor(5.4902e+08, device='cuda:0')
c= tensor(5.4902e+08, device='cuda:0')
c= tensor(5.4904e+08, device='cuda:0')
c= tensor(5.4905e+08, device='cuda:0')
c= tensor(5.4907e+08, device='cuda:0')
c= tensor(5.4908e+08, device='cuda:0')
c= tensor(5.4908e+08, device='cuda:0')
c= tensor(5.4908e+08, device='cuda:0')
c= tensor(5.4909e+08, device='cuda:0')
c= tensor(5.4909e+08, device='cuda:0')
c= tensor(5.4909e+08, device='cuda:0')
c= tensor(5.4911e+08, device='cuda:0')
c= tensor(5.4912e+08, device='cuda:0')
c= tensor(5.4912e+08, device='cuda:0')
c= tensor(5.4912e+08, device='cuda:0')
c= tensor(5.4913e+08, device='cuda:0')
c= tensor(5.4914e+08, device='cuda:0')
c= tensor(5.4915e+08, device='cuda:0')
c= tensor(5.4916e+08, device='cuda:0')
c= tensor(5.4916e+08, device='cuda:0')
c= tensor(5.4916e+08, device='cuda:0')
c= tensor(5.4917e+08, device='cuda:0')
c= tensor(5.4917e+08, device='cuda:0')
c= tensor(5.4918e+08, device='cuda:0')
c= tensor(5.4918e+08, device='cuda:0')
c= tensor(5.4919e+08, device='cuda:0')
c= tensor(5.4919e+08, device='cuda:0')
c= tensor(5.4920e+08, device='cuda:0')
c= tensor(5.4920e+08, device='cuda:0')
c= tensor(5.4921e+08, device='cuda:0')
c= tensor(5.4922e+08, device='cuda:0')
c= tensor(5.4922e+08, device='cuda:0')
c= tensor(5.4928e+08, device='cuda:0')
c= tensor(5.5438e+08, device='cuda:0')
c= tensor(5.5439e+08, device='cuda:0')
c= tensor(5.5440e+08, device='cuda:0')
c= tensor(5.5440e+08, device='cuda:0')
c= tensor(5.5442e+08, device='cuda:0')
c= tensor(5.5454e+08, device='cuda:0')
c= tensor(5.9484e+08, device='cuda:0')
c= tensor(5.9484e+08, device='cuda:0')
c= tensor(5.9691e+08, device='cuda:0')
c= tensor(6.0034e+08, device='cuda:0')
c= tensor(6.0064e+08, device='cuda:0')
c= tensor(6.1860e+08, device='cuda:0')
c= tensor(6.1860e+08, device='cuda:0')
c= tensor(6.1863e+08, device='cuda:0')
c= tensor(6.2667e+08, device='cuda:0')
c= tensor(6.7692e+08, device='cuda:0')
c= tensor(6.7693e+08, device='cuda:0')
c= tensor(6.7716e+08, device='cuda:0')
c= tensor(6.8698e+08, device='cuda:0')
c= tensor(6.9710e+08, device='cuda:0')
c= tensor(6.9800e+08, device='cuda:0')
c= tensor(6.9854e+08, device='cuda:0')
c= tensor(6.9897e+08, device='cuda:0')
c= tensor(6.9914e+08, device='cuda:0')
c= tensor(6.9930e+08, device='cuda:0')
c= tensor(7.2641e+08, device='cuda:0')
c= tensor(7.2642e+08, device='cuda:0')
c= tensor(7.2643e+08, device='cuda:0')
c= tensor(7.2700e+08, device='cuda:0')
c= tensor(7.2732e+08, device='cuda:0')
c= tensor(7.5329e+08, device='cuda:0')
c= tensor(7.5499e+08, device='cuda:0')
c= tensor(7.5499e+08, device='cuda:0')
c= tensor(7.5519e+08, device='cuda:0')
c= tensor(7.5522e+08, device='cuda:0')
c= tensor(7.5551e+08, device='cuda:0')
c= tensor(7.5649e+08, device='cuda:0')
c= tensor(7.5916e+08, device='cuda:0')
c= tensor(7.5968e+08, device='cuda:0')
c= tensor(7.5968e+08, device='cuda:0')
c= tensor(7.5970e+08, device='cuda:0')
c= tensor(7.6154e+08, device='cuda:0')
c= tensor(7.6301e+08, device='cuda:0')
c= tensor(7.6352e+08, device='cuda:0')
c= tensor(7.6353e+08, device='cuda:0')
c= tensor(7.9036e+08, device='cuda:0')
c= tensor(7.9040e+08, device='cuda:0')
c= tensor(7.9057e+08, device='cuda:0')
c= tensor(7.9305e+08, device='cuda:0')
c= tensor(7.9305e+08, device='cuda:0')
c= tensor(7.9432e+08, device='cuda:0')
c= tensor(8.1571e+08, device='cuda:0')
c= tensor(8.4374e+08, device='cuda:0')
c= tensor(8.4383e+08, device='cuda:0')
c= tensor(8.4393e+08, device='cuda:0')
c= tensor(8.4397e+08, device='cuda:0')
c= tensor(8.4398e+08, device='cuda:0')
c= tensor(8.4562e+08, device='cuda:0')
c= tensor(8.4566e+08, device='cuda:0')
c= tensor(8.4600e+08, device='cuda:0')
c= tensor(8.4975e+08, device='cuda:0')
c= tensor(8.5118e+08, device='cuda:0')
c= tensor(8.5131e+08, device='cuda:0')
c= tensor(8.5132e+08, device='cuda:0')
c= tensor(8.6150e+08, device='cuda:0')
c= tensor(8.6165e+08, device='cuda:0')
c= tensor(8.6205e+08, device='cuda:0')
c= tensor(8.6209e+08, device='cuda:0')
c= tensor(9.0285e+08, device='cuda:0')
c= tensor(9.0288e+08, device='cuda:0')
c= tensor(9.1069e+08, device='cuda:0')
c= tensor(9.1074e+08, device='cuda:0')
c= tensor(9.1270e+08, device='cuda:0')
c= tensor(9.1289e+08, device='cuda:0')
c= tensor(9.1742e+08, device='cuda:0')
c= tensor(9.1880e+08, device='cuda:0')
c= tensor(9.1880e+08, device='cuda:0')
c= tensor(9.2293e+08, device='cuda:0')
c= tensor(9.2471e+08, device='cuda:0')
c= tensor(9.2473e+08, device='cuda:0')
c= tensor(9.2674e+08, device='cuda:0')
c= tensor(9.3309e+08, device='cuda:0')
c= tensor(9.4797e+08, device='cuda:0')
c= tensor(9.6114e+08, device='cuda:0')
c= tensor(9.6116e+08, device='cuda:0')
c= tensor(9.6118e+08, device='cuda:0')
c= tensor(9.6134e+08, device='cuda:0')
c= tensor(9.6206e+08, device='cuda:0')
c= tensor(9.6209e+08, device='cuda:0')
c= tensor(9.6210e+08, device='cuda:0')
c= tensor(9.6285e+08, device='cuda:0')
c= tensor(9.6652e+08, device='cuda:0')
c= tensor(9.6940e+08, device='cuda:0')
c= tensor(9.6941e+08, device='cuda:0')
c= tensor(9.6965e+08, device='cuda:0')
c= tensor(9.6969e+08, device='cuda:0')
c= tensor(9.6972e+08, device='cuda:0')
c= tensor(9.6975e+08, device='cuda:0')
c= tensor(9.6976e+08, device='cuda:0')
c= tensor(9.9092e+08, device='cuda:0')
c= tensor(9.9123e+08, device='cuda:0')
c= tensor(9.9137e+08, device='cuda:0')
c= tensor(9.9184e+08, device='cuda:0')
c= tensor(9.9187e+08, device='cuda:0')
c= tensor(1.0482e+09, device='cuda:0')
c= tensor(1.0482e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0517e+09, device='cuda:0')
c= tensor(1.0527e+09, device='cuda:0')
c= tensor(1.0527e+09, device='cuda:0')
c= tensor(1.0528e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0732e+09, device='cuda:0')
c= tensor(1.0738e+09, device='cuda:0')
c= tensor(1.0757e+09, device='cuda:0')
c= tensor(1.0825e+09, device='cuda:0')
c= tensor(1.0825e+09, device='cuda:0')
c= tensor(1.0826e+09, device='cuda:0')
c= tensor(1.0828e+09, device='cuda:0')
c= tensor(1.0828e+09, device='cuda:0')
c= tensor(1.0828e+09, device='cuda:0')
c= tensor(1.0829e+09, device='cuda:0')
c= tensor(1.0829e+09, device='cuda:0')
c= tensor(1.0830e+09, device='cuda:0')
c= tensor(1.0830e+09, device='cuda:0')
c= tensor(1.0830e+09, device='cuda:0')
c= tensor(1.1624e+09, device='cuda:0')
c= tensor(1.1631e+09, device='cuda:0')
c= tensor(1.1652e+09, device='cuda:0')
c= tensor(1.1654e+09, device='cuda:0')
c= tensor(1.1654e+09, device='cuda:0')
c= tensor(1.1655e+09, device='cuda:0')
c= tensor(1.2363e+09, device='cuda:0')
c= tensor(1.2811e+09, device='cuda:0')
c= tensor(1.2816e+09, device='cuda:0')
c= tensor(1.2821e+09, device='cuda:0')
c= tensor(1.2821e+09, device='cuda:0')
c= tensor(1.2851e+09, device='cuda:0')
c= tensor(1.3413e+09, device='cuda:0')
c= tensor(1.3463e+09, device='cuda:0')
c= tensor(1.3463e+09, device='cuda:0')
c= tensor(1.3495e+09, device='cuda:0')
c= tensor(1.3818e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3837e+09, device='cuda:0')
c= tensor(1.3837e+09, device='cuda:0')
c= tensor(1.3838e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.3920e+09, device='cuda:0')
c= tensor(1.3920e+09, device='cuda:0')
c= tensor(1.3924e+09, device='cuda:0')
c= tensor(1.3924e+09, device='cuda:0')
c= tensor(1.3924e+09, device='cuda:0')
c= tensor(1.3956e+09, device='cuda:0')
c= tensor(1.4005e+09, device='cuda:0')
c= tensor(1.4042e+09, device='cuda:0')
c= tensor(1.4111e+09, device='cuda:0')
c= tensor(1.4156e+09, device='cuda:0')
c= tensor(1.4158e+09, device='cuda:0')
c= tensor(1.4167e+09, device='cuda:0')
c= tensor(1.4184e+09, device='cuda:0')
c= tensor(1.4238e+09, device='cuda:0')
c= tensor(1.4239e+09, device='cuda:0')
c= tensor(1.4290e+09, device='cuda:0')
c= tensor(1.4392e+09, device='cuda:0')
c= tensor(1.4414e+09, device='cuda:0')
c= tensor(1.4422e+09, device='cuda:0')
c= tensor(1.4465e+09, device='cuda:0')
c= tensor(1.4465e+09, device='cuda:0')
c= tensor(1.4465e+09, device='cuda:0')
c= tensor(1.4473e+09, device='cuda:0')
c= tensor(1.4508e+09, device='cuda:0')
c= tensor(1.4532e+09, device='cuda:0')
c= tensor(1.5663e+09, device='cuda:0')
c= tensor(1.5771e+09, device='cuda:0')
c= tensor(1.5805e+09, device='cuda:0')
c= tensor(1.5808e+09, device='cuda:0')
c= tensor(1.5831e+09, device='cuda:0')
c= tensor(1.5831e+09, device='cuda:0')
c= tensor(1.5831e+09, device='cuda:0')
c= tensor(1.5947e+09, device='cuda:0')
c= tensor(1.5948e+09, device='cuda:0')
c= tensor(1.5948e+09, device='cuda:0')
c= tensor(1.5949e+09, device='cuda:0')
c= tensor(1.6043e+09, device='cuda:0')
c= tensor(1.6048e+09, device='cuda:0')
c= tensor(1.6072e+09, device='cuda:0')
c= tensor(1.6073e+09, device='cuda:0')
c= tensor(1.6074e+09, device='cuda:0')
c= tensor(1.6074e+09, device='cuda:0')
c= tensor(1.6079e+09, device='cuda:0')
c= tensor(1.6081e+09, device='cuda:0')
c= tensor(1.6100e+09, device='cuda:0')
c= tensor(1.6100e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.6156e+09, device='cuda:0')
c= tensor(1.6156e+09, device='cuda:0')
c= tensor(1.6172e+09, device='cuda:0')
c= tensor(1.6172e+09, device='cuda:0')
c= tensor(1.6182e+09, device='cuda:0')
c= tensor(1.6182e+09, device='cuda:0')
c= tensor(1.6184e+09, device='cuda:0')
c= tensor(1.6194e+09, device='cuda:0')
c= tensor(1.6754e+09, device='cuda:0')
c= tensor(1.6754e+09, device='cuda:0')
c= tensor(1.6756e+09, device='cuda:0')
c= tensor(1.6818e+09, device='cuda:0')
c= tensor(1.6819e+09, device='cuda:0')
c= tensor(1.7210e+09, device='cuda:0')
c= tensor(1.7210e+09, device='cuda:0')
c= tensor(1.7233e+09, device='cuda:0')
c= tensor(1.7289e+09, device='cuda:0')
c= tensor(1.7289e+09, device='cuda:0')
c= tensor(1.7473e+09, device='cuda:0')
c= tensor(1.7481e+09, device='cuda:0')
c= tensor(1.7912e+09, device='cuda:0')
c= tensor(1.7912e+09, device='cuda:0')
c= tensor(1.7918e+09, device='cuda:0')
c= tensor(1.7918e+09, device='cuda:0')
c= tensor(1.7918e+09, device='cuda:0')
c= tensor(1.7918e+09, device='cuda:0')
c= tensor(1.7925e+09, device='cuda:0')
c= tensor(1.7928e+09, device='cuda:0')
c= tensor(1.7963e+09, device='cuda:0')
c= tensor(1.7964e+09, device='cuda:0')
c= tensor(1.7964e+09, device='cuda:0')
c= tensor(1.7964e+09, device='cuda:0')
c= tensor(1.8013e+09, device='cuda:0')
c= tensor(1.8019e+09, device='cuda:0')
c= tensor(1.8078e+09, device='cuda:0')
c= tensor(1.8090e+09, device='cuda:0')
c= tensor(1.8091e+09, device='cuda:0')
c= tensor(1.8091e+09, device='cuda:0')
c= tensor(1.8091e+09, device='cuda:0')
c= tensor(1.8748e+09, device='cuda:0')
c= tensor(1.8748e+09, device='cuda:0')
c= tensor(1.8750e+09, device='cuda:0')
c= tensor(1.8868e+09, device='cuda:0')
c= tensor(1.8877e+09, device='cuda:0')
c= tensor(1.8877e+09, device='cuda:0')
c= tensor(1.8877e+09, device='cuda:0')
c= tensor(1.9256e+09, device='cuda:0')
c= tensor(1.9258e+09, device='cuda:0')
c= tensor(1.9270e+09, device='cuda:0')
c= tensor(1.9279e+09, device='cuda:0')
c= tensor(1.9383e+09, device='cuda:0')
c= tensor(1.9411e+09, device='cuda:0')
c= tensor(1.9590e+09, device='cuda:0')
c= tensor(1.9600e+09, device='cuda:0')
c= tensor(1.9600e+09, device='cuda:0')
c= tensor(1.9607e+09, device='cuda:0')
c= tensor(1.9608e+09, device='cuda:0')
c= tensor(1.9609e+09, device='cuda:0')
c= tensor(1.9610e+09, device='cuda:0')
c= tensor(1.9610e+09, device='cuda:0')
c= tensor(1.9646e+09, device='cuda:0')
c= tensor(1.9647e+09, device='cuda:0')
c= tensor(1.9647e+09, device='cuda:0')
c= tensor(1.9648e+09, device='cuda:0')
c= tensor(1.9648e+09, device='cuda:0')
c= tensor(1.9741e+09, device='cuda:0')
c= tensor(1.9761e+09, device='cuda:0')
c= tensor(1.9762e+09, device='cuda:0')
c= tensor(1.9762e+09, device='cuda:0')
c= tensor(1.9766e+09, device='cuda:0')
c= tensor(1.9766e+09, device='cuda:0')
c= tensor(1.9767e+09, device='cuda:0')
c= tensor(1.9768e+09, device='cuda:0')
c= tensor(1.9873e+09, device='cuda:0')
c= tensor(1.9874e+09, device='cuda:0')
c= tensor(1.9874e+09, device='cuda:0')
c= tensor(1.9874e+09, device='cuda:0')
c= tensor(2.0046e+09, device='cuda:0')
c= tensor(2.1057e+09, device='cuda:0')
c= tensor(2.1062e+09, device='cuda:0')
c= tensor(2.1062e+09, device='cuda:0')
c= tensor(2.1082e+09, device='cuda:0')
c= tensor(2.1108e+09, device='cuda:0')
c= tensor(2.1108e+09, device='cuda:0')
c= tensor(2.1110e+09, device='cuda:0')
c= tensor(2.1111e+09, device='cuda:0')
c= tensor(2.1256e+09, device='cuda:0')
c= tensor(2.1597e+09, device='cuda:0')
c= tensor(2.1629e+09, device='cuda:0')
c= tensor(2.1629e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1634e+09, device='cuda:0')
c= tensor(2.1647e+09, device='cuda:0')
c= tensor(2.1653e+09, device='cuda:0')
c= tensor(2.1789e+09, device='cuda:0')
c= tensor(2.1790e+09, device='cuda:0')
c= tensor(2.1790e+09, device='cuda:0')
c= tensor(2.1791e+09, device='cuda:0')
c= tensor(2.8254e+09, device='cuda:0')
c= tensor(2.8258e+09, device='cuda:0')
c= tensor(2.8258e+09, device='cuda:0')
c= tensor(2.8258e+09, device='cuda:0')
c= tensor(2.8262e+09, device='cuda:0')
c= tensor(2.8262e+09, device='cuda:0')
c= tensor(2.8281e+09, device='cuda:0')
c= tensor(2.8281e+09, device='cuda:0')
c= tensor(2.8282e+09, device='cuda:0')
c= tensor(2.8283e+09, device='cuda:0')
c= tensor(2.8283e+09, device='cuda:0')
c= tensor(2.8283e+09, device='cuda:0')
c= tensor(2.8337e+09, device='cuda:0')
c= tensor(2.8665e+09, device='cuda:0')
c= tensor(2.8719e+09, device='cuda:0')
c= tensor(2.8781e+09, device='cuda:0')
c= tensor(2.8783e+09, device='cuda:0')
c= tensor(2.8783e+09, device='cuda:0')
c= tensor(2.8784e+09, device='cuda:0')
c= tensor(2.8804e+09, device='cuda:0')
c= tensor(2.8829e+09, device='cuda:0')
c= tensor(2.8934e+09, device='cuda:0')
c= tensor(2.8935e+09, device='cuda:0')
c= tensor(3.0378e+09, device='cuda:0')
c= tensor(3.0380e+09, device='cuda:0')
c= tensor(3.0386e+09, device='cuda:0')
c= tensor(3.0472e+09, device='cuda:0')
c= tensor(3.0475e+09, device='cuda:0')
c= tensor(3.0644e+09, device='cuda:0')
c= tensor(3.0939e+09, device='cuda:0')
c= tensor(3.0958e+09, device='cuda:0')
c= tensor(3.0961e+09, device='cuda:0')
c= tensor(3.0962e+09, device='cuda:0')
c= tensor(3.0974e+09, device='cuda:0')
c= tensor(3.0974e+09, device='cuda:0')
c= tensor(3.1047e+09, device='cuda:0')
c= tensor(3.2210e+09, device='cuda:0')
c= tensor(3.2302e+09, device='cuda:0')
c= tensor(3.2349e+09, device='cuda:0')
c= tensor(3.2350e+09, device='cuda:0')
c= tensor(3.2360e+09, device='cuda:0')
c= tensor(3.2362e+09, device='cuda:0')
c= tensor(3.2364e+09, device='cuda:0')
c= tensor(3.2386e+09, device='cuda:0')
c= tensor(3.2522e+09, device='cuda:0')
c= tensor(3.2523e+09, device='cuda:0')
c= tensor(3.2964e+09, device='cuda:0')
c= tensor(3.3167e+09, device='cuda:0')
c= tensor(3.3175e+09, device='cuda:0')
c= tensor(3.3175e+09, device='cuda:0')
c= tensor(3.3196e+09, device='cuda:0')
c= tensor(3.3224e+09, device='cuda:0')
c= tensor(3.3224e+09, device='cuda:0')
c= tensor(3.3954e+09, device='cuda:0')
c= tensor(3.3962e+09, device='cuda:0')
c= tensor(3.3965e+09, device='cuda:0')
c= tensor(3.3965e+09, device='cuda:0')
c= tensor(3.3965e+09, device='cuda:0')
c= tensor(3.3965e+09, device='cuda:0')
c= tensor(3.3966e+09, device='cuda:0')
c= tensor(3.3968e+09, device='cuda:0')
c= tensor(3.3985e+09, device='cuda:0')
c= tensor(4.9717e+09, device='cuda:0')
c= tensor(4.9718e+09, device='cuda:0')
c= tensor(4.9727e+09, device='cuda:0')
c= tensor(4.9727e+09, device='cuda:0')
c= tensor(4.9728e+09, device='cuda:0')
c= tensor(4.9728e+09, device='cuda:0')
c= tensor(4.9982e+09, device='cuda:0')
c= tensor(4.9993e+09, device='cuda:0')
c= tensor(5.1833e+09, device='cuda:0')
c= tensor(5.1833e+09, device='cuda:0')
c= tensor(5.1888e+09, device='cuda:0')
c= tensor(5.1894e+09, device='cuda:0')
c= tensor(5.1943e+09, device='cuda:0')
c= tensor(5.2209e+09, device='cuda:0')
c= tensor(5.2211e+09, device='cuda:0')
c= tensor(5.2211e+09, device='cuda:0')
c= tensor(5.2222e+09, device='cuda:0')
c= tensor(5.2223e+09, device='cuda:0')
c= tensor(5.2233e+09, device='cuda:0')
c= tensor(5.2262e+09, device='cuda:0')
c= tensor(5.2273e+09, device='cuda:0')
c= tensor(5.2294e+09, device='cuda:0')
c= tensor(5.2297e+09, device='cuda:0')
c= tensor(5.2352e+09, device='cuda:0')
c= tensor(5.2738e+09, device='cuda:0')
c= tensor(5.2740e+09, device='cuda:0')
c= tensor(5.2741e+09, device='cuda:0')
c= tensor(5.2890e+09, device='cuda:0')
c= tensor(5.2896e+09, device='cuda:0')
c= tensor(5.2950e+09, device='cuda:0')
c= tensor(5.2963e+09, device='cuda:0')
c= tensor(5.2982e+09, device='cuda:0')
c= tensor(5.2989e+09, device='cuda:0')
c= tensor(5.3052e+09, device='cuda:0')
c= tensor(5.3062e+09, device='cuda:0')
c= tensor(5.3064e+09, device='cuda:0')
c= tensor(5.3064e+09, device='cuda:0')
c= tensor(5.3069e+09, device='cuda:0')
c= tensor(5.3087e+09, device='cuda:0')
c= tensor(5.3134e+09, device='cuda:0')
c= tensor(5.3178e+09, device='cuda:0')
c= tensor(5.3178e+09, device='cuda:0')
c= tensor(5.3180e+09, device='cuda:0')
c= tensor(5.3188e+09, device='cuda:0')
c= tensor(5.3433e+09, device='cuda:0')
c= tensor(5.3437e+09, device='cuda:0')
c= tensor(5.3446e+09, device='cuda:0')
c= tensor(5.3482e+09, device='cuda:0')
c= tensor(5.3484e+09, device='cuda:0')
c= tensor(5.3484e+09, device='cuda:0')
c= tensor(5.3484e+09, device='cuda:0')
c= tensor(5.3511e+09, device='cuda:0')
c= tensor(5.3520e+09, device='cuda:0')
c= tensor(5.3525e+09, device='cuda:0')
c= tensor(5.3525e+09, device='cuda:0')
c= tensor(5.3525e+09, device='cuda:0')
c= tensor(5.3568e+09, device='cuda:0')
c= tensor(5.3581e+09, device='cuda:0')
c= tensor(5.3582e+09, device='cuda:0')
c= tensor(5.3582e+09, device='cuda:0')
c= tensor(5.3582e+09, device='cuda:0')
c= tensor(5.3585e+09, device='cuda:0')
c= tensor(5.3588e+09, device='cuda:0')
c= tensor(5.3621e+09, device='cuda:0')
c= tensor(5.3622e+09, device='cuda:0')
c= tensor(5.3787e+09, device='cuda:0')
c= tensor(5.3788e+09, device='cuda:0')
c= tensor(5.3788e+09, device='cuda:0')
c= tensor(5.4044e+09, device='cuda:0')
c= tensor(5.4044e+09, device='cuda:0')
c= tensor(5.4070e+09, device='cuda:0')
c= tensor(5.4128e+09, device='cuda:0')
c= tensor(5.4134e+09, device='cuda:0')
c= tensor(5.4151e+09, device='cuda:0')
c= tensor(5.4259e+09, device='cuda:0')
c= tensor(5.4259e+09, device='cuda:0')
c= tensor(5.4269e+09, device='cuda:0')
c= tensor(5.4270e+09, device='cuda:0')
c= tensor(5.4399e+09, device='cuda:0')
c= tensor(5.4461e+09, device='cuda:0')
c= tensor(5.4466e+09, device='cuda:0')
c= tensor(5.4474e+09, device='cuda:0')
c= tensor(5.4475e+09, device='cuda:0')
c= tensor(5.4485e+09, device='cuda:0')
c= tensor(5.4491e+09, device='cuda:0')
c= tensor(5.4492e+09, device='cuda:0')
c= tensor(5.4565e+09, device='cuda:0')
c= tensor(5.4771e+09, device='cuda:0')
c= tensor(5.4771e+09, device='cuda:0')
c= tensor(5.4772e+09, device='cuda:0')
c= tensor(5.4773e+09, device='cuda:0')
c= tensor(5.5031e+09, device='cuda:0')
c= tensor(5.5032e+09, device='cuda:0')
c= tensor(5.5033e+09, device='cuda:0')
c= tensor(5.5043e+09, device='cuda:0')
c= tensor(5.5061e+09, device='cuda:0')
c= tensor(5.5061e+09, device='cuda:0')
c= tensor(5.5065e+09, device='cuda:0')
c= tensor(5.5065e+09, device='cuda:0')
time to make c is 9.927502155303955
time for making loss is 9.92761778831482
p0 True
it  0 : 2080163840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5081411584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5081690112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1219960600.0
relative error loss 0.22154903
shape of L is 
torch.Size([])
memory (bytes)
5108109312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5108260864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1212223200.0
relative error loss 0.2201439
shape of L is 
torch.Size([])
memory (bytes)
5111775232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 15% |
memory (bytes)
5111869440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1171272700.0
relative error loss 0.21270715
shape of L is 
torch.Size([])
memory (bytes)
5115015168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5115047936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1148943400.0
relative error loss 0.20865206
shape of L is 
torch.Size([])
memory (bytes)
5118001152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5118230528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1134325800.0
relative error loss 0.20599745
shape of L is 
torch.Size([])
memory (bytes)
5121429504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5121429504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1123113000.0
relative error loss 0.20396118
shape of L is 
torch.Size([])
memory (bytes)
5124558848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5124632576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1111832600.0
relative error loss 0.20191261
shape of L is 
torch.Size([])
memory (bytes)
5127794688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5127827456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1106103800.0
relative error loss 0.20087226
shape of L is 
torch.Size([])
memory (bytes)
5130977280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 15% |
memory (bytes)
5131042816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1100543000.0
relative error loss 0.19986239
shape of L is 
torch.Size([])
memory (bytes)
5134213120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5134245888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1096330800.0
relative error loss 0.19909744
time to take a step is 269.1841309070587
it  1 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5137260544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5137448960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 15% |
error is  1096330800.0
relative error loss 0.19909744
shape of L is 
torch.Size([])
memory (bytes)
5140652032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5140652032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1093116400.0
relative error loss 0.1985137
shape of L is 
torch.Size([])
memory (bytes)
5143724032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5143724032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1089280500.0
relative error loss 0.19781709
shape of L is 
torch.Size([])
memory (bytes)
5147086848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 15% |
memory (bytes)
5147086848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1085830700.0
relative error loss 0.19719058
shape of L is 
torch.Size([])
memory (bytes)
5150244864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5150244864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1083722800.0
relative error loss 0.19680777
shape of L is 
torch.Size([])
memory (bytes)
5153423360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
5153497088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1080910800.0
relative error loss 0.19629712
shape of L is 
torch.Size([])
memory (bytes)
5156696064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5156696064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1079657000.0
relative error loss 0.19606942
shape of L is 
torch.Size([])
memory (bytes)
5159755776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5159907328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 15% |
error is  1077696500.0
relative error loss 0.19571339
shape of L is 
torch.Size([])
memory (bytes)
5163081728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5163081728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1076252200.0
relative error loss 0.1954511
shape of L is 
torch.Size([])
memory (bytes)
5166268416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5166313472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1074981400.0
relative error loss 0.1952203
time to take a step is 265.8258240222931
it  2 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5169520640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
memory (bytes)
5169520640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1074981400.0
relative error loss 0.1952203
shape of L is 
torch.Size([])
memory (bytes)
5172621312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5172744192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1072664600.0
relative error loss 0.19479957
shape of L is 
torch.Size([])
memory (bytes)
5175947264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
memory (bytes)
5175947264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1072053760.0
relative error loss 0.19468865
shape of L is 
torch.Size([])
memory (bytes)
5179076608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5179150336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1070484000.0
relative error loss 0.19440357
shape of L is 
torch.Size([])
memory (bytes)
5182324736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5182357504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1069810200.0
relative error loss 0.1942812
shape of L is 
torch.Size([])
memory (bytes)
5185433600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5185523712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1068555260.0
relative error loss 0.1940533
shape of L is 
torch.Size([])
memory (bytes)
5188734976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5188767744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1067357700.0
relative error loss 0.19383582
shape of L is 
torch.Size([])
memory (bytes)
5191950336
| ID | GPU | MEM |
------------------
|  0 | 20% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5191979008
| ID | GPU | MEM |
------------------
|  0 | 19% |  0% |
|  1 | 99% | 15% |
error is  1066461700.0
relative error loss 0.1936731
shape of L is 
torch.Size([])
memory (bytes)
5195182080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5195186176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1065669600.0
relative error loss 0.19352926
shape of L is 
torch.Size([])
memory (bytes)
5198303232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 15% |
memory (bytes)
5198397440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1064972300.0
relative error loss 0.19340263
time to take a step is 265.6759235858917
it  3 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5201604608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5201604608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1064972300.0
relative error loss 0.19340263
shape of L is 
torch.Size([])
memory (bytes)
5204766720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5204766720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1064155140.0
relative error loss 0.19325423
shape of L is 
torch.Size([])
memory (bytes)
5207887872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5208010752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1063620600.0
relative error loss 0.19315717
shape of L is 
torch.Size([])
memory (bytes)
5211217920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5211217920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1062783500.0
relative error loss 0.19300513
shape of L is 
torch.Size([])
memory (bytes)
5214392320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 15% |
memory (bytes)
5214437376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1062376960.0
relative error loss 0.19293131
shape of L is 
torch.Size([])
memory (bytes)
5217607680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5217607680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1061883900.0
relative error loss 0.19284177
shape of L is 
torch.Size([])
memory (bytes)
5220823040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5220823040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1061346300.0
relative error loss 0.19274414
shape of L is 
torch.Size([])
memory (bytes)
5224030208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5224058880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1060854800.0
relative error loss 0.19265488
shape of L is 
torch.Size([])
memory (bytes)
5227171840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5227266048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1060377600.0
relative error loss 0.19256821
shape of L is 
torch.Size([])
memory (bytes)
5230440448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5230473216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1060011500.0
relative error loss 0.19250174
time to take a step is 264.68977785110474
it  4 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5233528832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5233664000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1060011500.0
relative error loss 0.19250174
shape of L is 
torch.Size([])
memory (bytes)
5236891648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5236891648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1059788800.0
relative error loss 0.1924613
shape of L is 
torch.Size([])
memory (bytes)
5240000512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5240094720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1058925600.0
relative error loss 0.19230452
shape of L is 
torch.Size([])
memory (bytes)
5243269120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5243305984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1058644500.0
relative error loss 0.19225347
shape of L is 
torch.Size([])
memory (bytes)
5246365696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
5246521344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1058173440.0
relative error loss 0.19216794
shape of L is 
torch.Size([])
memory (bytes)
5249724416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 15% |
memory (bytes)
5249724416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1057504260.0
relative error loss 0.1920464
shape of L is 
torch.Size([])
memory (bytes)
5252911104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5252939776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1057126900.0
relative error loss 0.19197789
shape of L is 
torch.Size([])
memory (bytes)
5256134656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5256134656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1056759800.0
relative error loss 0.19191122
shape of L is 
torch.Size([])
memory (bytes)
5259239424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5259358208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1056512000.0
relative error loss 0.1918662
shape of L is 
torch.Size([])
memory (bytes)
5262524416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5262557184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1056079360.0
relative error loss 0.19178765
time to take a step is 265.34967827796936
it  5 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5265719296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5265752064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1056079360.0
relative error loss 0.19178765
shape of L is 
torch.Size([])
memory (bytes)
5268946944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5268963328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1055874050.0
relative error loss 0.19175036
shape of L is 
torch.Size([])
memory (bytes)
5272174592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5272174592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1055577100.0
relative error loss 0.19169644
shape of L is 
torch.Size([])
memory (bytes)
5275217920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5275385856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1055274000.0
relative error loss 0.19164139
shape of L is 
torch.Size([])
memory (bytes)
5278552064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5278580736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1055049200.0
relative error loss 0.19160056
shape of L is 
torch.Size([])
memory (bytes)
5281640448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5281796096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1054792200.0
relative error loss 0.19155389
shape of L is 
torch.Size([])
memory (bytes)
5284843520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5285003264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1054465540.0
relative error loss 0.19149457
shape of L is 
torch.Size([])
memory (bytes)
5288185856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5288214528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1054272000.0
relative error loss 0.19145942
shape of L is 
torch.Size([])
memory (bytes)
5291241472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5291425792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1053997060.0
relative error loss 0.19140948
shape of L is 
torch.Size([])
memory (bytes)
5294620672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 15% |
memory (bytes)
5294620672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1053815300.0
relative error loss 0.19137648
time to take a step is 265.01266527175903
it  6 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5297795072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5297831936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1053815300.0
relative error loss 0.19137648
shape of L is 
torch.Size([])
memory (bytes)
5301002240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5301035008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1053552100.0
relative error loss 0.19132869
shape of L is 
torch.Size([])
memory (bytes)
5304205312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5304238080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1053318660.0
relative error loss 0.1912863
shape of L is 
torch.Size([])
memory (bytes)
5307445248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5307445248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1053150200.0
relative error loss 0.1912557
shape of L is 
torch.Size([])
memory (bytes)
5310619648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5310619648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1052864000.0
relative error loss 0.19120373
shape of L is 
torch.Size([])
memory (bytes)
5313863680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5313867776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1052586500.0
relative error loss 0.19115333
shape of L is 
torch.Size([])
memory (bytes)
5317029888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5317062656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1052450800.0
relative error loss 0.19112869
shape of L is 
torch.Size([])
memory (bytes)
5320155136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5320273920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1052256260.0
relative error loss 0.19109336
shape of L is 
torch.Size([])
memory (bytes)
5323448320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5323481088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1052008960.0
relative error loss 0.19104844
shape of L is 
torch.Size([])
memory (bytes)
5326512128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 15% |
memory (bytes)
5326688256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1051765250.0
relative error loss 0.19100419
time to take a step is 265.00977325439453
it  7 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 13% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5329903616
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5329903616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1051765250.0
relative error loss 0.19100419
shape of L is 
torch.Size([])
memory (bytes)
5333098496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
memory (bytes)
5333098496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1051633150.0
relative error loss 0.1909802
shape of L is 
torch.Size([])
memory (bytes)
5336170496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5336326144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1051502600.0
relative error loss 0.19095649
shape of L is 
torch.Size([])
memory (bytes)
5339508736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5339533312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1051229700.0
relative error loss 0.19090693
shape of L is 
torch.Size([])
memory (bytes)
5342670848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5342715904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1051295740.0
relative error loss 0.19091892
shape of L is 
torch.Size([])
memory (bytes)
5345943552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5345943552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1051160600.0
relative error loss 0.19089438
shape of L is 
torch.Size([])
memory (bytes)
5349068800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5349154816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1051074560.0
relative error loss 0.19087875
shape of L is 
torch.Size([])
memory (bytes)
5352357888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5352357888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1050751000.0
relative error loss 0.19082
shape of L is 
torch.Size([])
memory (bytes)
5355536384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5355565056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1050756100.0
relative error loss 0.19082092
shape of L is 
torch.Size([])
memory (bytes)
5358731264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5358768128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1050643460.0
relative error loss 0.19080046
time to take a step is 265.383736371994
it  8 : 2705915904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5361954816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
memory (bytes)
5361954816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1050643460.0
relative error loss 0.19080046
shape of L is 
torch.Size([])
memory (bytes)
5365116928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5365178368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1050517000.0
relative error loss 0.1907775
shape of L is 
torch.Size([])
memory (bytes)
5368352768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5368385536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1050298900.0
relative error loss 0.19073789
shape of L is 
torch.Size([])
memory (bytes)
5371588608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5371588608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1050246660.0
relative error loss 0.1907284
shape of L is 
torch.Size([])
memory (bytes)
5374795776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5374795776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 15% |
error is  1050070500.0
relative error loss 0.19069642
shape of L is 
torch.Size([])
memory (bytes)
5377998848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5378011136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1049980900.0
relative error loss 0.19068015
shape of L is 
torch.Size([])
memory (bytes)
5381218304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5381218304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1049862140.0
relative error loss 0.19065857
shape of L is 
torch.Size([])
memory (bytes)
5384392704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5384421376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1049712100.0
relative error loss 0.19063133
shape of L is 
torch.Size([])
memory (bytes)
5387530240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5387628544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1049554940.0
relative error loss 0.19060278
shape of L is 
torch.Size([])
memory (bytes)
5390802944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5390835712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 15% |
error is  1049456100.0
relative error loss 0.19058484
time to take a step is 266.8144474029541
it  9 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 15% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5393879040
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 24% | 15% |
memory (bytes)
5394034688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1049456100.0
relative error loss 0.19058484
shape of L is 
torch.Size([])
memory (bytes)
5397233664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5397233664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1049339900.0
relative error loss 0.19056374
shape of L is 
torch.Size([])
memory (bytes)
5400358912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5400444928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1049186300.0
relative error loss 0.19053584
shape of L is 
torch.Size([])
memory (bytes)
5403643904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 15% |
memory (bytes)
5403648000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1049048060.0
relative error loss 0.19051073
shape of L is 
torch.Size([])
memory (bytes)
5406687232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5406855168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1048967700.0
relative error loss 0.19049613
shape of L is 
torch.Size([])
memory (bytes)
5410058240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5410058240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1048856060.0
relative error loss 0.19047587
shape of L is 
torch.Size([])
memory (bytes)
5413187584
| ID | GPU | MEM |
------------------
|  0 | 26% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5413269504
| ID | GPU | MEM |
------------------
|  0 | 23% |  0% |
|  1 | 99% | 15% |
error is  1048770050.0
relative error loss 0.19046025
shape of L is 
torch.Size([])
memory (bytes)
5416476672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5416476672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1048699400.0
relative error loss 0.19044742
shape of L is 
torch.Size([])
memory (bytes)
5419671552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5419671552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1048542700.0
relative error loss 0.19041896
shape of L is 
torch.Size([])
memory (bytes)
5422817280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5422862336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1048476160.0
relative error loss 0.19040687
time to take a step is 266.28608322143555
it  10 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5426085888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5426085888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1048476160.0
relative error loss 0.19040687
shape of L is 
torch.Size([])
memory (bytes)
5429186560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5429288960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1048387600.0
relative error loss 0.1903908
shape of L is 
torch.Size([])
memory (bytes)
5432475648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5432504320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1048184800.0
relative error loss 0.19035397
shape of L is 
torch.Size([])
memory (bytes)
5435568128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5435723776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1048122400.0
relative error loss 0.19034262
shape of L is 
torch.Size([])
memory (bytes)
5438922752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5438926848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1048039400.0
relative error loss 0.19032757
shape of L is 
torch.Size([])
memory (bytes)
5442101248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5442101248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047929340.0
relative error loss 0.19030757
shape of L is 
torch.Size([])
memory (bytes)
5445328896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5445328896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047863800.0
relative error loss 0.19029567
shape of L is 
torch.Size([])
memory (bytes)
5448450048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5448531968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047793660.0
relative error loss 0.19028293
shape of L is 
torch.Size([])
memory (bytes)
5451751424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5451751424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047656960.0
relative error loss 0.1902581
shape of L is 
torch.Size([])
memory (bytes)
5454954496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5454954496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047594500.0
relative error loss 0.19024676
time to take a step is 266.50123834609985
it  11 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5458067456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 15% |
memory (bytes)
5458153472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047594500.0
relative error loss 0.19024676
shape of L is 
torch.Size([])
memory (bytes)
5461327872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5461360640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047519740.0
relative error loss 0.19023319
shape of L is 
torch.Size([])
memory (bytes)
5464391680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5464576000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047456260.0
relative error loss 0.19022165
shape of L is 
torch.Size([])
memory (bytes)
5467774976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5467779072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047383550.0
relative error loss 0.19020845
shape of L is 
torch.Size([])
memory (bytes)
5470932992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5470990336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047340000.0
relative error loss 0.19020055
shape of L is 
torch.Size([])
memory (bytes)
5474205696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5474205696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1047229440.0
relative error loss 0.19018047
shape of L is 
torch.Size([])
memory (bytes)
5477310464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5477408768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047165950.0
relative error loss 0.19016893
shape of L is 
torch.Size([])
memory (bytes)
5480628224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5480628224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047107600.0
relative error loss 0.19015834
shape of L is 
torch.Size([])
memory (bytes)
5483700224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5483835392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047014400.0
relative error loss 0.19014141
shape of L is 
torch.Size([])
memory (bytes)
5487042560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 15% |
memory (bytes)
5487042560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1047042050.0
relative error loss 0.19014643
shape of L is 
torch.Size([])
memory (bytes)
5490262016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 15% |
memory (bytes)
5490262016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046943740.0
relative error loss 0.19012858
time to take a step is 293.35112714767456
it  12 : 2705915904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5493317632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 15% |
memory (bytes)
5493473280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046943740.0
relative error loss 0.19012858
shape of L is 
torch.Size([])
memory (bytes)
5496643584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5496676352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046854660.0
relative error loss 0.19011241
shape of L is 
torch.Size([])
memory (bytes)
5499727872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
5499887616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046789100.0
relative error loss 0.1901005
shape of L is 
torch.Size([])
memory (bytes)
5503086592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5503086592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046737400.0
relative error loss 0.19009112
shape of L is 
torch.Size([])
memory (bytes)
5506138112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5506301952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1046651900.0
relative error loss 0.19007559
shape of L is 
torch.Size([])
memory (bytes)
5509472256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5509505024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046622200.0
relative error loss 0.1900702
shape of L is 
torch.Size([])
memory (bytes)
5512560640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5512712192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1046578700.0
relative error loss 0.19006228
shape of L is 
torch.Size([])
memory (bytes)
5515907072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5515907072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046513660.0
relative error loss 0.19005048
shape of L is 
torch.Size([])
memory (bytes)
5519052800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5519122432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046473200.0
relative error loss 0.19004314
shape of L is 
torch.Size([])
memory (bytes)
5522337792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5522337792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1046432800.0
relative error loss 0.19003579
time to take a step is 265.65993094444275
it  13 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5525540864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5525540864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046432800.0
relative error loss 0.19003579
shape of L is 
torch.Size([])
memory (bytes)
5528702976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5528735744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046409700.0
relative error loss 0.1900316
shape of L is 
torch.Size([])
memory (bytes)
5531914240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5531942912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046366700.0
relative error loss 0.1900238
shape of L is 
torch.Size([])
memory (bytes)
5534949376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
5535158272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046321150.0
relative error loss 0.19001552
shape of L is 
torch.Size([])
memory (bytes)
5538353152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5538353152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046260200.0
relative error loss 0.19000445
shape of L is 
torch.Size([])
memory (bytes)
5541421056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5541564416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046252000.0
relative error loss 0.19000296
shape of L is 
torch.Size([])
memory (bytes)
5544767488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5544771584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046210050.0
relative error loss 0.18999535
shape of L is 
torch.Size([])
memory (bytes)
5547892736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5547974656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 15% |
error is  1046176260.0
relative error loss 0.18998921
shape of L is 
torch.Size([])
memory (bytes)
5551181824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5551181824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1046117400.0
relative error loss 0.18997851
shape of L is 
torch.Size([])
memory (bytes)
5554384896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5554384896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046052350.0
relative error loss 0.18996671
time to take a step is 266.1165189743042
it  14 : 2705915392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5557530624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5557592064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046052350.0
relative error loss 0.18996671
shape of L is 
torch.Size([])
memory (bytes)
5560791040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 15% |
memory (bytes)
5560795136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1046046700.0
relative error loss 0.18996568
shape of L is 
torch.Size([])
memory (bytes)
5564002304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5564002304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1045996000.0
relative error loss 0.18995647
shape of L is 
torch.Size([])
memory (bytes)
5567217664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5567217664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1045912600.0
relative error loss 0.18994132
shape of L is 
torch.Size([])
memory (bytes)
5570392064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 15% |
memory (bytes)
5570392064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1045859840.0
relative error loss 0.18993174
shape of L is 
torch.Size([])
memory (bytes)
5573627904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5573627904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1045790700.0
relative error loss 0.18991919
shape of L is 
torch.Size([])
memory (bytes)
5576822784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5576826880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1045742100.0
relative error loss 0.18991035
shape of L is 
torch.Size([])
memory (bytes)
5579993088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
memory (bytes)
5579993088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1045696500.0
relative error loss 0.18990208
shape of L is 
torch.Size([])
memory (bytes)
5583245312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5583245312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  1045641700.0
relative error loss 0.18989213
shape of L is 
torch.Size([])
memory (bytes)
5586321408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5586460672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  1045605400.0
relative error loss 0.18988553
time to take a step is 266.4302201271057
sum tnnu_Z after tensor(12040630., device='cuda:0')
shape of features
(6164,)
shape of features
(6164,)
number of orig particles 24655
number of new particles after remove low mass 21397
tnuZ shape should be parts x labs
torch.Size([24655, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1219849900.0
relative error without small mass is  0.22152893
nnu_Z shape should be number of particles by maxV
(24655, 702)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
shape of features
(24655,)
Wed Feb 1 20:59:24 EST 2023
