Wed Feb 1 15:07:29 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 42938961
numbers of Z: 27005
shape of features
(27005,)
shape of features
(27005,)
ZX	Vol	Parts	Cubes	Eps
Z	0.02000867682881141	27005	27.005	0.0904880847313365
X	0.018704218715161425	820	0.82	0.28360153247192516
X	0.019551957317519894	16730	16.73	0.10533307590502734
X	0.019978666848777644	6288	6.288	0.1470115051214972
X	0.018937604144568014	13174	13.174	0.11285889445746056
X	0.019674748143171373	12747	12.747	0.11556697135708612
X	0.018798389745762624	43161	43.161	0.07580125284899329
X	0.018562075260730514	55119	55.119	0.06957316060387485
X	0.018831269046165576	45942	45.942	0.07428307903073299
X	0.018678329305076968	11889	11.889	0.1162512327547322
X	0.018142465568724203	16040	16.04	0.10419110593401999
X	0.018854848824578687	6241	6.241	0.14456271322628161
X	0.01829047526288232	87958	87.958	0.05924475141347736
X	0.01890165203337689	8125	8.125	0.13250292478819606
X	0.018891964884097688	372534	372.534	0.03701437364633001
X	0.018833497018059638	24248	24.248	0.09192176360194933
X	0.018802936767969354	59434	59.434	0.06813918088890115
X	0.018873158604490056	79397	79.397	0.06194603290215655
X	0.01875794397129116	24931	24.931	0.09095261258189116
X	0.01883128340746261	147602	147.602	0.050341856314533716
X	0.019606050455618963	117441	117.441	0.055062609801588794
X	0.018592389222380684	38917	38.917	0.07817453069517552
X	0.018900284592722822	261193	261.193	0.04167116127259797
X	0.01875864453456445	11678	11.678	0.1171145878332614
X	0.01888618295430023	42142	42.142	0.0765261180248126
X	0.017782321135416612	1323	1.323	0.23776223157488705
X	0.01894137485008198	77801	77.801	0.062441803737310585
X	0.018940762649519913	58631	58.631	0.06861568238889007
X	0.01868650555300257	3773	3.773	0.17045657702026523
X	0.01875800561604648	48025	48.025	0.07309815733119049
X	0.018949256191474015	1285050	1285.05	0.024522084266761418
X	0.018778097655492813	11940	11.94	0.11629179895953531
X	0.019422229475294338	786761	786.761	0.029117366019925178
X	0.01887841141153486	25979	25.979	0.08990441210107794
X	0.01879597023728804	15340	15.34	0.10700723188998926
X	0.01851781884969015	20264	20.264	0.09704091544554852
X	0.018795298707669758	186313	186.313	0.04655167988822486
X	0.019438765195806004	110892	110.892	0.05596581328047121
X	0.018365938931893883	1394	1.394	0.23618380813635298
X	0.019519876853066515	6662	6.662	0.14309489765601344
X	0.018285860829268884	2833	2.833	0.1861899685292283
X	0.018563806474736832	5226	5.226	0.1525805615688344
X	0.017924949859208	2135	2.135	0.20324507160137623
X	0.017152038760406666	851	0.851	0.27214188990902877
X	0.018297212286088816	6755	6.755	0.1393969242065146
X	0.018237233164219123	545	0.545	0.3222459610949594
X	0.018191868178500433	923	0.923	0.2701211146237494
X	0.018450415058955096	3882	3.882	0.16813193186956465
X	0.018692167374460885	2911	2.911	0.18586851047320266
X	0.01853645094188736	1335	1.335	0.2403523873140841
X	0.018723901106963335	15139	15.139	0.1073411844638347
X	0.019678845856551073	13824	13.824	0.11249208497993742
X	0.018436759427206413	2857	2.857	0.18617646623063866
X	0.018712001079543218	7060	7.06	0.1383901492177161
X	0.01833334436851786	2414	2.414	0.19656286646412774
X	0.019606699888814437	8839	8.839	0.13041687486461723
X	0.01834230527103924	4626	4.626	0.15827637542324816
X	0.01853015183906533	2059	2.059	0.20800520875320247
X	0.018773406475802366	3987	3.987	0.16760962301228377
X	0.018681339638592462	2479	2.479	0.19605465716121492
X	0.018512097361674765	3255	3.255	0.1784986260588892
X	0.018473844930892477	3872	3.872	0.16834774950961132
X	0.0182896175898406	1535	1.535	0.2284013930621755
X	0.019937066917216126	26117	26.117	0.09139294229072205
X	0.01863080515796061	6050	6.05	0.14548741130853013
X	0.018113493156692913	1572	1.572	0.22586549508117978
X	0.018584296116368287	2770	2.77	0.18860597597680626
X	0.018277765242570305	1670	1.67	0.22202516830641528
X	0.018238636087994864	6823	6.823	0.13878387061381775
X	0.01863540888455404	3378	3.378	0.17669588152701876
X	0.018290519437214003	1445	1.445	0.23305194442268315
X	0.017797298468365703	3886	3.886	0.1660671614834398
X	0.018311477515749457	2790	2.79	0.1872289758041378
X	0.01852427226230088	2694	2.694	0.19015796511788538
X	0.018613451059586023	2287	2.287	0.20115009670870143
X	0.01863909896666093	3730	3.73	0.17096427181188542
X	0.018685022109875597	4686	4.686	0.15857343865450277
X	0.018064030304563942	551	0.551	0.32005234832826857
X	0.018539119083487366	665	0.665	0.3032185846697888
X	0.01839097130134237	2337	2.337	0.19890635156652386
X	0.019880156134822316	23982	23.982	0.09393875443501322
X	0.018098226166911383	2751	2.751	0.18737650764997005
X	0.018460129956623883	871	0.871	0.2767401676421233
X	0.019730671041826804	18321	18.321	0.10250165880574019
X	0.01825368604986813	885	0.885	0.2742431373856803
X	0.018519095012775332	3071	3.071	0.18201757100063434
X	0.018377104833642754	1268	1.268	0.24381061675539148
X	0.017997093063819646	2059	2.059	0.20599120206175253
X	0.018407656922023487	826	0.826	0.28140997529136946
X	0.019699950343526656	7398	7.398	0.13860650138848962
X	0.018344814589709848	3607	3.607	0.17197120253044282
X	0.018219149428611862	1017	1.017	0.26165902270177255
X	0.018399996760166085	1148	1.148	0.2521304425786517
X	0.018192642932724108	1249	1.249	0.2442180649303718
X	0.01837865149783564	2781	2.781	0.18765964247387934
X	0.018340515560290916	4608	4.608	0.15847704200269214
X	0.018554213244013944	5690	5.69	0.1482893946906495
X	0.018732789939095242	3141	3.141	0.1813474732814721
X	0.018516614811308917	11676	11.676	0.11661538094955315
X	0.01843320149226121	4250	4.25	0.1630812181409831
X	0.018498758949335916	2801	2.801	0.18761886087533652
X	0.019240756221720074	12008	12.008	0.11701741122174697
X	0.01850307210482131	2796	2.796	0.18774522117733
X	0.018498210906482637	4564	4.564	0.15943904962887304
X	0.018465650089698672	2067	2.067	0.20749519383884465
X	0.018396441566922783	5041	5.041	0.15395922252588717
X	0.018426790960435546	3328	3.328	0.17691125518400505
X	0.018678858147573613	5324	5.324	0.151951185380341
X	0.017943807261474893	699	0.699	0.2949935181090511
X	0.018227224621228343	1839	1.839	0.21480593279695448
X	0.018045331193731397	1022	1.022	0.26039820557877374
X	0.018423267479959437	2344	2.344	0.19882440130142717
X	0.01845549736208618	1996	1.996	0.20988838485455547
X	0.01821372605105974	1620	1.62	0.22402402466237054
X	0.01865651535437779	2337	2.337	0.19985910542521654
X	0.01808162203023659	3702	3.702	0.16966814985206424
X	0.01823755712709398	1554	1.554	0.2272506933474172
X	0.01823739740039554	1252	1.252	0.24422278397370759
X	0.018428082868392665	909	0.909	0.27267087875411644
X	0.01921866510618766	8056	8.056	0.13361890872163357
X	0.017492936415276442	983	0.983	0.26107767305071466
X	0.018702028392012553	11356	11.356	0.11809217172077956
X	0.018417871569079557	2081	2.081	0.2068501245227432
X	0.018189657230458983	1794	1.794	0.21643825582611914
X	0.018238986774446523	2149	2.149	0.20398023962860065
X	0.018039910314744865	1453	1.453	0.23155611437084694
X	0.01860942940760249	2009	2.009	0.21001535083561026
X	0.01861380952687004	1251	1.251	0.2459570744171641
X	0.018326563462879142	2260	2.26	0.20090505550620455
X	0.019653746150589405	28283	28.283	0.08857407446138814
X	0.018443674739913554	2038	2.038	0.2083920234833763
X	0.0185640681235849	6658	6.658	0.14074828796424832
X	0.01837090973365098	2117	2.117	0.2054959506482463
X	0.018607738582986356	2784	2.784	0.1883684315846394
X	0.01791992764102027	1236	1.236	0.24384054872864927
X	0.018353901021170536	4328	4.328	0.16186279835879763
X	0.018586721304272136	1562	1.562	0.22830070494067545
X	0.017767138891893783	970	0.97	0.26360193122814285
X	0.01821323258229951	1390	1.39	0.23575309943674766
X	0.018469501736611084	1664	1.664	0.22306629423311305
X	0.018123845987907262	914	0.914	0.2706666341950228
X	0.019524398546512927	5787	5.787	0.14998277819167063
X	0.018383913551645883	1218	1.218	0.2471327073665815
X	0.01981200752154262	19416	19.416	0.10067529409556657
X	0.01889133166560867	8884	8.884	0.12859320115548686
X	0.019962918223886225	15762	15.762	0.1081942634816237
X	0.018279404448758318	1305	1.305	0.24105557319991927
X	0.018369673274124185	2790	2.79	0.18742711036694862
X	0.018025274755635826	2114	2.114	0.20429557975387092
X	0.018469396678606937	1318	1.318	0.24108994594285524
X	0.018485266341618616	2806	2.806	0.1874617564547288
X	0.01814816621925566	1664	1.664	0.22176507011362412
X	0.019684413396473945	9067	9.067	0.12948504751187542
X	0.018508738102005304	4226	4.226	0.1636122331933148
X	0.018510798537755214	10538	10.538	0.12065786971332618
X	0.01836290340327274	2168	2.168	0.20384216529229085
X	0.019290937696611044	31158	31.158	0.08523041213097474
X	0.018607382486846895	1552	1.552	0.22887473580869255
X	0.018604722472445323	3195	3.195	0.17990812519465388
X	0.01855949852272204	4596	4.596	0.15924363131451097
X	0.017312315115427267	472	0.472	0.33225463638374536
X	0.018636976942817837	5559	5.559	0.14966707274760005
X	0.018264583062863948	1465	1.465	0.23187684328128308
X	0.01924952577211935	4812	4.812	0.15874429948241361
X	0.018802843031428255	1786	1.786	0.2191697885600525
X	0.01842388072211095	3048	3.048	0.18216099236917194
X	0.01824752319478109	1390	1.39	0.23590095988256493
X	0.01854371079769998	1939	1.939	0.21226233895632157
X	0.018396774839025416	2948	2.948	0.18410746771898667
X	0.01870615715620645	6899	6.899	0.1394438862526093
X	0.017767797142361944	353	0.353	0.3692209147426116
X	0.018322599071503444	1525	1.525	0.22903705192556356
X	0.017884231401101198	678	0.678	0.29767808435403265
X	0.018549428779308513	7597	7.597	0.13465716638404934
X	0.018780295643738915	1031	1.031	0.2631164008430925
X	0.01841142036223771	5066	5.066	0.1537472545755153
X	0.018717187299920567	4593	4.593	0.1597281200902861
X	0.018525956815951605	6026	6.026	0.14540650639766592
X	0.018823886465894883	2301	2.301	0.20149497752794285
X	0.018695124861937577	5379	5.379	0.15147545833786583
X	0.01834078562684049	2084	2.084	0.20646197305596387
X	0.01868353975991925	2444	2.444	0.1969938431850904
X	0.018536513275810414	2527	2.527	0.1943006565996446
X	0.018257340067146712	5404	5.404	0.1500516313943261
X	0.018788948151756298	2736	2.736	0.19007665984150218
X	0.018678755802549005	5416	5.416	0.15108560786858638
X	0.01849376163061139	3566	3.566	0.17309371762475223
X	0.018373162672886408	3477	3.477	0.17417791058716886
X	0.019317490146740845	7231	7.231	0.1387556829301086
X	0.018738302766827156	8097	8.097	0.13227223875262917
X	0.01954899826919584	15390	15.39	0.10830004309560806
X	0.01831919382525775	2123	2.123	0.20510934820028473
X	0.018471094318284465	531	0.531	0.3264369927153404
X	0.018256986552364306	1359	1.359	0.23772227601006532
X	0.01849680547591768	2688	2.688	0.19020524484783577
X	0.018678381613102625	3366	3.366	0.17704148517121537
X	0.01854249056923007	6206	6.206	0.14402971406860174
X	0.018601691862779924	1753	1.753	0.21974728092381446
X	0.01840057300549899	1283	1.283	0.24296006607310658
X	0.018120190153599454	2089	2.089	0.2054666958254069
X	0.018493631031124055	1083	1.083	0.25751226019786755
X	0.019144534196179945	4364	4.364	0.16370176203173584
X	0.018489216174094956	1254	1.254	0.245211210488489
X	0.019432727036698513	9495	9.495	0.12696370688193856
X	0.01875059669270994	9034	9.034	0.1275591653277981
X	0.018693488396371024	3720	3.72	0.1712836086767902
X	0.017546235329060366	574	0.574	0.31267321974392426
X	0.019780480377707244	9394	9.394	0.12817266848725983
X	0.018571322001870533	3903	3.903	0.16819564873194073
X	0.01851604702470569	2057	2.057	0.20801979360014797
X	0.019583868339102518	14180	14.18	0.11136295592444753
X	0.019450226352591932	6426	6.426	0.14465317802753802
X	0.01824172007745152	2066	2.066	0.20668636306566515
X	0.018645449959794926	1622	1.622	0.22568741359917896
X	0.01844116291667334	1314	1.314	0.24121129842462433
X	0.018477283288599215	864	0.864	0.277571448407615
X	0.01829810220548719	1307	1.307	0.24101467515382508
X	0.017600216383792305	725	0.725	0.2895520652303189
X	0.017912106103444132	613	0.613	0.3080080445480223
X	0.01862206421538871	6698	6.698	0.14061367501251906
X	0.018617290290977976	2286	2.286	0.20119325403627328
X	0.018249651959460284	1962	1.962	0.21030609583203919
X	0.01833785368596444	1096	1.096	0.25576786781396604
X	0.018400048149454833	3925	3.925	0.16736312104373166
X	0.018178706255642517	1955	1.955	0.21028360001280558
X	0.018420136324978163	5660	5.66	0.14819224237195144
X	0.01866826257039998	4601	4.601	0.15949627908766267
X	0.01833062040911515	1464	1.464	0.232208811581621
X	0.018236773771959182	718	0.718	0.2939507158492481
X	0.018530611033191105	7296	7.296	0.1364378828151897
X	0.018217449550657256	740	0.74	0.290905513142891
X	0.018218459454835103	1119	1.119	0.253450909862843
X	0.018441064898424463	2273	2.273	0.2009380516164805
X	0.01884687620810269	3715	3.715	0.17182783573898908
X	0.01950464565420263	6820	6.82	0.1419442801559773
X	0.01841345212188531	1700	1.7	0.22125619112734557
X	0.018379479911873137	1622	1.622	0.22460915604074774
X	0.018665930593936644	1711	1.711	0.22178553224548936
X	0.019683246167908484	8978	8.978	0.12990894055188182
X	0.018029181408995006	1084	1.084	0.2552596927402361
X	0.018756591709984782	18020	18.02	0.10134438994596691
X	0.018823285640238916	52029	52.029	0.07125521441715339
X	0.01853847929538958	3395	3.395	0.17609408808872673
X	0.01847674073661509	4614	4.614	0.15879954664603985
X	0.017998322331812408	655	0.655	0.3017613229931847
X	0.01869534394561226	4678	4.678	0.1586929916029132
X	0.019396933524000664	56216	56.216	0.07013870003122427
X	0.018820805286441197	202038	202.038	0.04533166948465791
X	0.018493730284127492	3858	3.858	0.16861159321431404
X	0.01880005572118912	135181	135.181	0.05181010755801481
X	0.01859632342120987	47782	47.782	0.07301088098960316
X	0.01859005953720533	16925	16.925	0.10317727044674581
X	0.018881262472061965	93391	93.391	0.0586915169626244
X	0.018556041362509487	1188	1.188	0.24997100163093933
X	0.018523584014673113	2339	2.339	0.19932645022769335
X	0.018856168049119194	145865	145.865	0.050563149141106836
X	0.018891497216181052	505935	505.935	0.033423907293984687
X	0.018568687151409233	5170	5.17	0.15314290652110604
X	0.018889503105990423	38258	38.258	0.07903742032078057
X	0.01833187051540161	12101	12.101	0.11484934076499752
X	0.01926202312837046	32501	32.501	0.08399789294815228
X	0.018735261502593352	97574	97.574	0.05769106417474837
X	0.01861519826456598	44947	44.947	0.07453993082710067
X	0.01879554354439863	35301	35.301	0.08105054891284277
X	0.01872818381841998	14967	14.967	0.10775902044510745
X	0.018421536565105617	4724	4.724	0.15740021497330822
X	0.018823240178270383	153303	153.303	0.049702844405578345
X	0.018336238084938353	4912	4.912	0.15512577363903685
X	0.018203851007269425	2410	2.41	0.19620741222114052
X	0.01873990383811976	23356	23.356	0.09292294465863402
X	0.018609443462877656	68165	68.165	0.06487200418065109
X	0.019685723682324487	161284	161.284	0.049604380823531226
X	0.018738191902700155	84324	84.324	0.06057017488258701
X	0.018151068588224555	923	0.923	0.26991902652741445
X	0.01872617260466051	26687	26.687	0.08886206927270221
X	0.01828246490326001	13466	13.466	0.11073002027030385
X	0.018635283535095828	35603	35.603	0.08059036762533188
X	0.0188172830067713	57262	57.262	0.06900758059807326
X	0.01857701648848868	16771	16.771	0.10346790822846004
X	0.018811641340574333	71854	71.854	0.06397229237417104
X	0.017829296569487636	764	0.764	0.2857680141312172
X	0.018762549724957073	7142	7.142	0.13798249813327068
X	0.018910660692258923	97015	97.015	0.057981476626323435
X	0.018484791515639927	44228	44.228	0.07476627639728146
X	0.01852045719006321	71818	71.818	0.0636511320707623
X	0.01874731191434859	5169	5.169	0.1536423086574587
X	0.01886906402764409	239824	239.824	0.042850172241866005
X	0.018741880360034765	9335	9.335	0.1261535897279094
X	0.01871071912608575	29002	29.002	0.0864080445699538
X	0.018862877145785863	95603	95.603	0.05821641688395811
X	0.0188509158012621	2416	2.416	0.19834069980775887
X	0.0187605926443446	98975	98.975	0.05744342782817261
X	0.019796960740700073	223817	223.817	0.04455555083794113
X	0.01894678352942958	270827	270.827	0.041204803249319806
X	0.018477690260416547	25922	25.922	0.08932912064921161
X	0.01879231835626542	21875	21.875	0.09506283601256357
X	0.018413869314448363	5104	5.104	0.15337154657546437
X	0.018662750354022317	4404	4.404	0.16182394293026586
X	0.018679959812783076	72905	72.905	0.06351449866153144
X	0.018735972043398046	7420	7.42	0.1361729276908702
X	0.018765767374468195	22807	22.807	0.09370570992468016
X	0.019112473899421785	36844	36.844	0.08034953675447953
X	0.01869699644665331	13782	13.782	0.11070142209895667
X	0.018339672673232204	17475	17.475	0.10162287217134192
X	0.019086139620302946	7025	7.025	0.13953738804518961
X	0.01856675602693776	89005	89.005	0.05930717105317358
X	0.01852859972915064	20948	20.948	0.09599162522328739
X	0.018740704461092224	16352	16.352	0.10464978597972398
X	0.019873286207704455	23197	23.197	0.09497572589848462
X	0.019017824330763194	181168	181.168	0.047172958447060716
X	0.01871391905262979	15152	15.152	0.10729140396753437
X	0.018724110405917795	194705	194.705	0.04581503573636603
X	0.018601102042949114	5052	5.052	0.15441581493596165
X	0.01841584551346961	40265	40.265	0.07704680841496858
X	0.018707621167645305	11832	11.832	0.11649844681081778
X	0.019049272696390174	141060	141.06	0.051304820576782866
X	0.01950022895936083	36392	36.392	0.08122279477220229
X	0.018768281217038427	7514	7.514	0.1356806046971732
X	0.018836715284546637	118424	118.424	0.054182014619186716
X	0.01892831339051511	133786	133.786	0.05210752147878856
X	0.01850772596720046	3024	3.024	0.18291827251359657
X	0.01890860876996969	126976	126.976	0.05300448812263204
X	0.01981437148029417	138835	138.835	0.052258921831018844
X	0.019724663469436743	341302	341.302	0.03866252878496122
X	0.01882642063060113	28463	28.463	0.08712892618654239
X	0.018066825537988315	1310	1.31	0.23981159911279126
X	0.018827747620584777	8597	8.597	0.12986251630026413
X	0.018779242343177727	12507	12.507	0.11450953178003695
X	0.018682269968511973	18787	18.787	0.09981383359800824
X	0.018928450846600185	37205	37.205	0.07983094205608628
X	0.018071643651720322	1628	1.628	0.22307338180070477
X	0.018902869404946614	69383	69.383	0.06482734268528571
X	0.01876594795924483	125004	125.004	0.05314742149975142
X	0.018543118522104916	2108	2.108	0.20642900657220833
X	0.018508626410080067	5670	5.67	0.1483418565925283
X	0.018784545503095204	28383	28.383	0.08714600140970732
X	0.01983835812319275	25123	25.123	0.09242967142605236
X	0.0192527357149197	17413	17.413	0.10340454224604534
X	0.01871694517501625	11070	11.07	0.11913218734408845
X	0.01854556241386522	3540	3.54	0.17367830903493411
X	0.018571865554243828	13015	13.015	0.11258238711073847
X	0.01882291982975995	34464	34.464	0.0817410923193829
X	0.01946658378469639	14854	14.854	0.10943311828684567
X	0.019613307964692295	41485	41.485	0.07790283293288454
X	0.019565618372075842	6712	6.712	0.14285009827926212
X	0.01918940200166629	670190	670.19	0.030592936543973975
X	0.01860918803712082	9845	9.845	0.12364329436256351
X	0.018767737158528155	87994	87.994	0.05974748342760525
X	0.01827638614670855	840	0.84	0.2791710284115574
X	0.018340209760195025	2765	2.765	0.1878897351656466
X	0.018399321801907634	4446	4.446	0.1605501758643919
X	0.018822376309534608	10944	10.944	0.11981176219576606
X	0.018849501788757564	6999	6.999	0.13913016305746936
X	0.01872211625714107	89069	89.069	0.05945788460111787
X	0.018632637944233695	4715	4.715	0.15809964060745935
X	0.018420007625747453	3564	3.564	0.17289563808240488
X	0.019222754671674735	329567	329.567	0.03878144333207712
X	0.01875653205395115	48795	48.795	0.07270970909618994
X	0.018758291630609564	41820	41.82	0.07654845300692736
X	0.019804621153635592	111337	111.337	0.05623961531269877
X	0.019362907002127748	251866	251.866	0.042520693409558025
X	0.018444178721737724	3331	3.331	0.17691373973853902
X	0.018458630963175984	5367	5.367	0.15094635511946997
X	0.01861731353130245	17695	17.695	0.10170808607419879
X	0.01795633438147792	1105	1.105	0.2532902477145475
X	0.018438874332470503	4795	4.795	0.15666858860815563
X	0.01882775669682786	11723	11.723	0.11710801081594172
X	0.018643816136063304	2204	2.204	0.20375474926027806
X	0.01851376631907046	6183	6.183	0.14413358108973548
X	0.018538433993799678	5104	5.104	0.15371660820809158
X	0.01876585008119013	6314	6.314	0.1437764937347471
X	0.01888987159239886	202678	202.678	0.04533922913254318
X	0.01854044213988225	15812	15.812	0.10544947624868795
X	0.018825487858073937	68120	68.12	0.0651364172120416
X	0.018371465585675237	2841	2.841	0.1863048624802984
X	0.01854239043738547	3944	3.944	0.16752370590385335
X	0.018795410300178415	58747	58.747	0.06839463544446721
X	0.019597933724435572	548105	548.105	0.032944443331830386
X	0.01894772787866487	495435	495.435	0.03369174344249711
X	0.018433825437468087	10657	10.657	0.1200402344359334
X	0.01961062969349936	59606	59.606	0.06903465945679221
X	0.018578547169905722	1974	1.974	0.2111323915223152
X	0.01863853104500953	3855	3.855	0.16909435118423757
X	0.0187974194974419	164820	164.82	0.04849490444446199
X	0.018949391262627045	13689	13.689	0.11144857328283068
X	0.01867493898352741	10675	10.675	0.12049354152531644
X	0.018702803557923284	45359	45.359	0.0744299533828815
X	0.019115329242020456	698212	698.212	0.030139184101841427
X	0.01853107379453762	40992	40.992	0.07674801630530236
X	0.01815761317517916	7785	7.785	0.13261725905170818
X	0.019916417536627762	34982	34.982	0.08288110431125029
X	0.018529638819811074	7674	7.674	0.13415752978191176
X	0.018494500380484974	1811	1.811	0.21695752493191794
X	0.018859771281268974	153365	153.365	0.049728274208288833
X	0.01876794742049286	14447	14.447	0.10911380526932282
X	0.018219662718688072	1264	1.264	0.24336853046663323
X	0.019272170364810912	59383	59.383	0.06872100423803755
X	0.01975201767585788	15319	15.319	0.10884129067302728
X	0.018002191824315603	545	0.545	0.32085560296628773
X	0.018854112060393315	75959	75.959	0.06284570682888654
X	0.01864479701958505	125224	125.224	0.05300172801731767
X	0.018886560866634386	139638	139.638	0.051331397436154086
X	0.019674691651130494	156673	156.673	0.05007695637871672
X	0.018758673679233603	167243	167.243	0.048226386949189785
X	0.018726043963910376	19647	19.647	0.09841242447792993
X	0.018642160009099133	11268	11.268	0.11827232410802493
X	0.018863814548013582	101863	101.863	0.056999496388132745
X	0.019494052816944114	154929	154.929	0.05010985432870626
X	0.01871163350519232	8628	8.628	0.12943960790580764
X	0.01963904456481656	52846	52.846	0.07189567162003649
X	0.019370933847810235	166637	166.637	0.048804481880220234
X	0.01884915752128307	172965	172.965	0.04776516007624721
X	0.018899392052285403	87786	87.786	0.05993412801933028
X	0.01851528666945621	46459	46.459	0.0735902357647765
X	0.018712773421884727	13457	13.457	0.11161690320138498
X	0.018731595327554453	5297	5.297	0.1523520367769613
X	0.018877075906845807	12404	12.404	0.11502466794280686
X	0.01925344617061595	125060	125.06	0.053595708565530406
X	0.018859293444009793	97350	97.35	0.05786240588431748
X	0.018931808963342164	420907	420.907	0.03556329467612566
X	0.01947156177566509	101418	101.418	0.05768929721692225
X	0.018831845473372855	68160	68.16	0.06513100306873101
X	0.01830020879683045	14366	14.366	0.10840269040359808
X	0.018890052677236268	141135	141.135	0.051152415560967945
X	0.018382304348783903	4595	4.595	0.15874673880731396
X	0.01839750272840435	10900	10.9	0.11906317496276943
X	0.01879694490631353	148945	148.945	0.0501595671269724
X	0.018774124090299132	48048	48.048	0.0731074197837932
X	0.018625872686660477	5158	5.158	0.15341874675494266
X	0.01874769227135974	15128	15.128	0.1074126505448423
X	0.01947466771557638	291257	291.257	0.04058802271978635
X	0.01865757282992979	18590	18.59	0.1001210168931764
X	0.01984578668529245	92396	92.396	0.05988780324321428
X	0.01823915876654962	1688	1.688	0.22107728399162233
X	0.018431297115645853	2067	2.067	0.2073664412298892
X	0.018547648883233795	4574	4.574	0.1594645806777202
X	0.019718469193555604	7190	7.19	0.13997423478611565
X	0.01874504655439622	41896	41.896	0.0764841282087874
X	0.01886383098043137	52609	52.609	0.07104332564962709
X	0.01832870866349275	4042	4.042	0.16551798335496132
X	0.018664544082695075	138729	138.729	0.05124089538931911
X	0.018487222304204718	4801	4.801	0.1567400517624922
X	0.01959198799117386	43086	43.086	0.07689783444388967
X	0.01867737168545152	8154	8.154	0.13182011457496634
X	0.01884845238289125	54876	54.876	0.07003219681295335
X	0.01915101585577076	7710	7.71	0.13542925183734295
X	0.018768527883407642	18483	18.483	0.10051230875627208
X	0.018857605448173224	23500	23.5	0.09292649588266308
X	0.019623874568689425	30764	30.764	0.08608232444201235
X	0.01883775232552149	71194	71.194	0.0641990442699989
X	0.01916179689991937	365819	365.819	0.03741593980020146
X	0.018741670989380142	5498	5.498	0.1504993155014148
X	0.018611869057891698	6314	6.314	0.1433821660653144
X	0.018884910676043264	84013	84.013	0.06080269286200854
X	0.018303099202138285	13486	13.486	0.1107168767384162
X	0.01888304488275725	439146	439.146	0.03503383643647976
X	0.018440947255779504	3005	3.005	0.18308225367064268
X	0.01878389388073986	102928	102.928	0.056721890524310446
X	0.01990061593316374	287631	287.631	0.04105287803842216
X	0.018070868132373023	2195	2.195	0.20192100418916342
X	0.01855833843563114	219160	219.16	0.04391294861131538
X	0.018238041286652466	16998	16.998	0.10237489198241603
X	0.018926126272094897	570478	570.478	0.0321322357114065
X	0.018392781450384	7706	7.706	0.13364092365408192
X	0.018549780119675615	10147	10.147	0.12227388237033988
X	0.01858889997498353	7695	7.695	0.13417811573728589
X	0.01858244571544921	4282	4.282	0.16311180860955135
X	0.01847953789829677	9316	9.316	0.12564750689296647
X	0.018825143742654853	24245	24.245	0.09191196213663178
X	0.0185547523131297	11009	11.009	0.11900606535335181
X	0.018845200377892436	107748	107.748	0.05592386525819591
X	0.01868923322221252	14533	14.533	0.10874569419935051
X	0.01859724231815145	2949	2.949	0.1847528978824203
X	0.01831754025790911	15900	15.9	0.10483106794982174
X	0.019824251419599547	261855	261.855	0.042303728088034065
X	0.019871994182254334	110273	110.273	0.056483823487470476
X	0.019563235663340242	316700	316.7	0.03953036955295192
X	0.01877913736296351	15968	15.968	0.10555408326042157
X	0.01847041477793403	11648	11.648	0.11661160210481827
X	0.018535425901224728	2956	2.956	0.1844021781496633
X	0.018411427665116217	6006	6.006	0.1452671376928431
X	0.019410006313247567	167780	167.78	0.04872611982386967
X	0.018837894917098152	12287	12.287	0.11530872513484233
X	0.018826049195746906	13944	13.944	0.11052417242550978
X	0.01885046330034883	28508	28.508	0.08712011221964502
X	0.018640284589127403	29480	29.48	0.08583050869447992
X	0.01871141816441668	13772	13.772	0.11075667179062165
X	0.018750198947070817	3688	3.688	0.17195111220518047
X	0.019469333652474528	295947	295.947	0.040368788428485895
X	0.01882846439569714	40813	40.813	0.07726903007224373
X	0.019425691692144264	34835	34.835	0.08231018142503421
X	0.018699094560199966	14994	14.994	0.10763851324191502
X	0.01979287612266163	120281	120.281	0.054798746651704755
X	0.01899495673550816	158238	158.238	0.04932977953520551
X	0.019392823709761767	471766	471.766	0.034512106698018706
X	0.018773657980301884	137816	137.816	0.051453677520122636
X	0.01859288332015748	9770	9.77	0.12392265992551436
X	0.019560153602420395	35656	35.656	0.08186153425053024
X	0.01861871173823681	34118	34.118	0.08171880761372599
X	0.01856909510049227	4547	4.547	0.1598411499725629
X	0.01850629607216602	11103	11.103	0.11856587809994479
X	0.018481895890858264	5749	5.749	0.14758812100382745
X	0.01872537076288736	15717	15.717	0.1060116491978021
X	0.0185712086557438	28888	28.888	0.08630598193179144
X	0.018444671685046515	3038	3.038	0.18242921382619745
X	0.018736730061185355	14366	14.366	0.10925784895385661
X	0.01827194667781726	5094	5.094	0.15307653609179658
X	0.01868116441438332	34927	34.927	0.08117349017527924
X	0.018509804095130325	7969	7.969	0.13243394976290127
X	0.01870479845948031	24435	24.435	0.09147734750979218
X	0.01830944155081485	11979	11.979	0.11519090167009888
X	0.018798152001363908	15015	15.015	0.10777795529690255
X	0.01835454020870965	4951	4.951	0.15476884008189964
X	0.018660146796088864	7929	7.929	0.13301446725366106
X	0.01879204336563596	18945	18.945	0.09973014844060535
X	0.018841225315824262	119418	119.418	0.05403557575251267
X	0.01850083897959179	5932	5.932	0.14610444436739886
X	0.01782089367770606	1395	1.395	0.23376801574702782
X	0.018348790494047904	4781	4.781	0.15656555893131083
X	0.018825218630779825	160565	160.565	0.048943653122054454
X	0.01983972337098128	408605	408.605	0.0364818885199288
X	0.01850741223837944	53295	53.295	0.0702888456048191
X	0.01945681440796461	7989	7.989	0.13454258626672358
X	0.018692388556875373	152902	152.902	0.04963071646103338
X	0.019891079127478534	140139	140.139	0.05216346639203257
X	0.017794367079659923	1939	1.939	0.20936379350179207
X	0.018405334928761907	6914	6.914	0.13859198548196563
X	0.018876664328420795	26853	26.853	0.08891550110881948
X	0.018583545327896835	88071	88.071	0.05953402076216266
X	0.018614907222994522	65001	65.001	0.0659143963809299
X	0.018903084225117533	183749	183.749	0.04685643343510554
X	0.018727209355220525	10885	10.885	0.11982521329594438
X	0.018449095663983103	2702	2.702	0.1897127761805435
X	0.0186727643706842	3528	3.528	0.17427162474082372
X	0.018565296089405884	33925	33.925	0.08179511007987338
X	0.01877862605963933	9890	9.89	0.12382904040331034
X	0.01871983984555538	18045	18.045	0.10123136218675606
X	0.018830132066647274	154620	154.62	0.04956737259936906
X	0.01967764133255759	521424	521.424	0.03354239027831012
X	0.018548318059645175	9130	9.13	0.1266517667088089
X	0.01797456747261276	1676	1.676	0.22052685205521594
X	0.019733207941598028	18118	18.118	0.1028874678549197
X	0.018784553138085126	415211	415.211	0.03563231228228858
X	0.01887668029917353	41860	41.86	0.07668471341038284
X	0.018445095414310202	3498	3.498	0.17405521244274397
X	0.017938833138831024	2701	2.701	0.18797057445944024
X	0.0187443135734103	77245	77.245	0.062373443499519
X	0.01862353993977764	2161	2.161	0.20502295525279637
X	0.01920072112448961	49156	49.156	0.0730993757328158
X	0.018352157320011418	4569	4.569	0.15896029319489205
X	0.018742960031711255	11513	11.513	0.11763861194056681
X	0.019390164024831333	7477	7.477	0.1373888117467976
X	0.018707590275540174	5228	5.228	0.15295397109269995
X	0.018086986976701774	2697	2.697	0.18857976627615555
X	0.019872306932099442	150334	150.334	0.050940557201466966
X	0.01912279430239498	311625	311.625	0.03944327982721614
X	0.018862456005592033	122222	122.222	0.053639281428606675
X	0.019662493800967913	92435	92.435	0.05969446318279603
X	0.018861269078772263	21704	21.704	0.09542826839694785
X	0.018409487221716786	6775	6.775	0.13954388099975393
X	0.01878729075735882	11204	11.204	0.11880380552056329
X	0.018908772816196462	113841	113.841	0.05496946970487932
X	0.01996130911975288	48396	48.396	0.07443774085118415
X	0.01883221042965372	87039	87.039	0.060033795548842535
X	0.0188131006855456	8248	8.248	0.13163475693377416
X	0.019742136111214868	972060	972.06	0.02728351724215895
X	0.018736946977696765	15539	15.539	0.10643682727047611
X	0.019643284900071334	58998	58.998	0.06930942171890087
X	0.01986245967591423	127710	127.71	0.05377775990588339
X	0.018804536127809378	37692	37.692	0.079311801400647
X	0.01887180231714044	100606	100.606	0.05724398142824988
X	0.019804467589530254	503656	503.656	0.034005021455034264
X	0.019357281535635693	111272	111.272	0.0558238121668386
X	0.01881530656965738	66594	66.594	0.06561836410890035
X	0.01860850284323745	7090	7.09	0.13793942119494507
X	0.018784098452046906	39573	39.573	0.07800644034273978
X	0.01844209338077027	9679	9.679	0.12397284724487591
X	0.018914194283514894	174610	174.61	0.04766938816365242
X	0.01979464046782562	862961	862.961	0.02841311285642574
X	0.019797010387063593	95671	95.671	0.05914795936426456
X	0.01867513549789481	187158	187.158	0.04638224746157977
X	0.018837212646427442	52414	52.414	0.07109784631458906
X	0.018685860382165652	23377	23.377	0.09280572637377552
X	0.01991791969299529	53790	53.79	0.07180931945824662
X	0.018599952866157756	8530	8.53	0.1296744070575207
X	0.01885682173406482	26819	26.819	0.08892188047803778
X	0.01972680812376862	283751	283.751	0.04111874079658769
X	0.01862017611989711	52792	52.792	0.0706542932233011
X	0.018768022979095356	410357	410.357	0.03576176075603896
X	0.01872372906788637	199681	199.681	0.04543095523673909
X	0.018756041326516492	34466	34.466	0.08164258851562806
X	0.01868679892040873	8837	8.837	0.12835418076811744
X	0.0188590925857345	87646	87.646	0.05992337017429906
X	0.01900985385403638	94834	94.834	0.05852457065645904
X	0.01827809961018276	6757	6.757	0.13933462089030466
X	0.019782677742475992	429647	429.647	0.03584190608916194
X	0.019126145849918928	31617	31.617	0.0845737526684312
X	0.01882904174577406	39905	39.905	0.07785149695073
X	0.018839347054986938	5526	5.526	0.15050525204324722
X	0.018801935344015294	5671	5.671	0.1491125861610219
X	0.018254710571026088	1474	1.474	0.23136224478050013
X	0.0185033126025696	4346	4.346	0.162076452122883
X	0.01918843368451324	18708	18.708	0.10084879687157121
X	0.0196871595220845	39111	39.111	0.07954777683618737
X	0.019916698088003962	3364056	3364.056	0.018090533492350267
X	0.019500824761406212	10258	10.258	0.1238787349362269
X	0.0188241127697221	77077	77.077	0.0625071797394259
X	0.01881769757348648	5680	5.68	0.14907542275393004
X	0.018737191604657765	16051	16.051	0.10529331549378664
X	0.01857491286102299	14433	14.433	0.10877357216181845
X	0.01887625642193348	206771	206.771	0.04502724894093858
X	0.01850814719136496	22038	22.038	0.0943474692624761
X	0.019317741595304173	705750	705.75	0.03013712516825439
X	0.018771430409196082	6148	6.148	0.1450734156424235
X	0.018933102147046067	192707	192.707	0.04614324107221545
X	0.018830225328564012	46164	46.164	0.07416244311035829
X	0.018747215546042403	120291	120.291	0.053814735800903166
X	0.019520143524313787	251304	251.304	0.04266723846348521
X	0.018549029163632574	18125	18.125	0.1007738203248583
X	0.018734801245228624	8943	8.943	0.12795441819962133
X	0.01978043729207751	64494	64.494	0.06743813091290862
X	0.018611649539108384	7702	7.702	0.13419215189717132
X	0.018712454324597325	16342	16.342	0.10461850645602389
X	0.018830944106250164	139483	139.483	0.051299949575879876
X	0.018572875237082745	15754	15.754	0.10564025318924612
X	0.019915974435983932	75114	75.114	0.06424315928804375
X	0.01843319306839444	26668	26.668	0.0884171900138989
X	0.01990989025608107	160656	160.656	0.049856750839395475
X	0.019059687113921694	365604	365.604	0.0373566803339807
X	0.018841157341580785	38299	38.299	0.07894174349747919
X	0.01860376700687342	8467	8.467	0.13000411843931878
X	0.018766597482840864	99831	99.831	0.057284884161139873
X	0.018791752111335384	75872	75.872	0.06280032831398075
X	0.01888899998226563	91366	91.366	0.059130032552452395
X	0.018734907335812005	50731	50.731	0.07174517656857358
X	0.018788204424742345	33912	33.912	0.08213166450080936
X	0.018785685794283177	32348	32.348	0.08343082534636956
X	0.0183055514544412	107415	107.415	0.05544205183914313
X	0.01857837798610748	65617	65.617	0.06566447474319423
X	0.01875114319138474	16156	16.156	0.10509078479387789
X	0.018488399829445158	5833	5.833	0.1468934562316988
X	0.019945426811643505	18504	18.504	0.10253195497688186
X	0.018748673877093794	67643	67.643	0.06520024556595272
X	0.0187143253636969	79864	79.864	0.06165112972427351
X	0.018815754902561415	160319	160.319	0.048960466894896665
X	0.018763526171224974	8051	8.051	0.13258311249968022
X	0.01870746119593175	21675	21.675	0.09521058837178965
X	0.019101012373076154	25260	25.26	0.09110474842938712
X	0.018933057970354827	130905	130.905	0.05249140248193809
X	0.01889003045628699	26213	26.213	0.08965447605846581
X	0.018896790427508064	62649	62.649	0.06706428536752326
X	0.01938533561520155	10814	10.814	0.12147744613595769
X	0.018707427230073403	20123	20.123	0.09759789385727527
X	0.01852358720099968	2864	2.864	0.18631623239665981
X	0.018588698571798876	11196	11.196	0.11841190419292567
X	0.0199420715438291	110235	110.235	0.05655663837233039
X	0.018618645368185895	24346	24.346	0.09144784748220096
X	0.01883030074762092	55240	55.24	0.06985559489666447
X	0.0185524220372909	1745	1.745	0.2198881006684583
X	0.019014339900575106	5929	5.929	0.1474687280059243
X	0.018710971405866784	32781	32.781	0.08295158952552675
X	0.018744330060633436	45383	45.383	0.07447186681579321
X	0.018741368429054495	36521	36.521	0.0800607300340413
X	0.018679282342999776	4641	4.641	0.15906801909072638
X	0.018668183581255826	2666	2.666	0.19131424402148584
X	0.018828420558550572	31867	31.867	0.08391203173435541
X	0.01934211849554488	26267	26.267	0.09030210105966278
X	0.018757870904508533	88684	88.684	0.05958168161532045
X	0.018465218229998075	4221	4.221	0.16354842425900243
X	0.01815075866658909	10917	10.917	0.11846693634284143
X	0.01851887004659505	44611	44.611	0.0745974831802336
X	0.01853158311039195	4690	4.69	0.15809321314978275
X	0.019401697955340982	209502	209.502	0.045242909086979456
X	0.01855207469022565	10400	10.4	0.12127921621327364
X	0.018737212722940745	84644	84.644	0.06049269543965273
X	0.01889333307750171	122350	122.35	0.053649811750688725
X	0.01882298697508782	40073	40.073	0.07773421701318715
X	0.019511141828116477	122328	122.328	0.05423158158492175
X	0.018937169429674047	137148	137.148	0.05168627040634518
X	0.01774513769443719	1037	1.037	0.2576915665987629
X	0.01875860487921497	48984	48.984	0.07261874900693044
X	0.01876203727904487	12152	12.152	0.11557859512006817
X	0.018823996548878834	88881	88.881	0.05960750823415322
X	0.01883619179049617	112054	112.054	0.05518934585713958
X	0.01840993324469661	21666	21.666	0.09471624923290255
X	0.018839965777812613	69175	69.175	0.06482019212316395
X	0.01872715611648337	13633	13.633	0.1111629675788088
X	0.018763470289913188	23912	23.912	0.09223570832766223
X	0.019032084766623616	10187	10.187	0.12316293198506649
X	0.018458819062926537	7111	7.111	0.13743301623840276
X	0.01893686930079417	164866	164.866	0.04861000811008176
X	0.019836254675682508	488422	488.422	0.03437332037937423
X	0.018667955005838562	19698	19.698	0.09822564081342873
X	0.018727835610413985	15148	15.148	0.107327438380118
X	0.01887038594482954	25274	25.274	0.09071983823691658
X	0.019424340547698354	67874	67.874	0.06589932200885248
X	0.018737443438217427	38250	38.25	0.07883026111849076
X	0.018731590214360355	7182	7.182	0.1376500634218502
X	0.01973120772552632	44255	44.255	0.07639479583049874
X	0.019028958663718242	133295	133.295	0.052263728354810225
X	0.018193473624085927	7307	7.307	0.13553730294896404
X	0.01863709584627231	62883	62.883	0.06667274388360776
X	0.018625597055256994	14478	14.478	0.10875950199672886
time for making epsilon is 2.2393758296966553
epsilons are
[0.28360153247192516, 0.10533307590502734, 0.1470115051214972, 0.11285889445746056, 0.11556697135708612, 0.07580125284899329, 0.06957316060387485, 0.07428307903073299, 0.1162512327547322, 0.10419110593401999, 0.14456271322628161, 0.05924475141347736, 0.13250292478819606, 0.03701437364633001, 0.09192176360194933, 0.06813918088890115, 0.06194603290215655, 0.09095261258189116, 0.050341856314533716, 0.055062609801588794, 0.07817453069517552, 0.04167116127259797, 0.1171145878332614, 0.0765261180248126, 0.23776223157488705, 0.062441803737310585, 0.06861568238889007, 0.17045657702026523, 0.07309815733119049, 0.024522084266761418, 0.11629179895953531, 0.029117366019925178, 0.08990441210107794, 0.10700723188998926, 0.09704091544554852, 0.04655167988822486, 0.05596581328047121, 0.23618380813635298, 0.14309489765601344, 0.1861899685292283, 0.1525805615688344, 0.20324507160137623, 0.27214188990902877, 0.1393969242065146, 0.3222459610949594, 0.2701211146237494, 0.16813193186956465, 0.18586851047320266, 0.2403523873140841, 0.1073411844638347, 0.11249208497993742, 0.18617646623063866, 0.1383901492177161, 0.19656286646412774, 0.13041687486461723, 0.15827637542324816, 0.20800520875320247, 0.16760962301228377, 0.19605465716121492, 0.1784986260588892, 0.16834774950961132, 0.2284013930621755, 0.09139294229072205, 0.14548741130853013, 0.22586549508117978, 0.18860597597680626, 0.22202516830641528, 0.13878387061381775, 0.17669588152701876, 0.23305194442268315, 0.1660671614834398, 0.1872289758041378, 0.19015796511788538, 0.20115009670870143, 0.17096427181188542, 0.15857343865450277, 0.32005234832826857, 0.3032185846697888, 0.19890635156652386, 0.09393875443501322, 0.18737650764997005, 0.2767401676421233, 0.10250165880574019, 0.2742431373856803, 0.18201757100063434, 0.24381061675539148, 0.20599120206175253, 0.28140997529136946, 0.13860650138848962, 0.17197120253044282, 0.26165902270177255, 0.2521304425786517, 0.2442180649303718, 0.18765964247387934, 0.15847704200269214, 0.1482893946906495, 0.1813474732814721, 0.11661538094955315, 0.1630812181409831, 0.18761886087533652, 0.11701741122174697, 0.18774522117733, 0.15943904962887304, 0.20749519383884465, 0.15395922252588717, 0.17691125518400505, 0.151951185380341, 0.2949935181090511, 0.21480593279695448, 0.26039820557877374, 0.19882440130142717, 0.20988838485455547, 0.22402402466237054, 0.19985910542521654, 0.16966814985206424, 0.2272506933474172, 0.24422278397370759, 0.27267087875411644, 0.13361890872163357, 0.26107767305071466, 0.11809217172077956, 0.2068501245227432, 0.21643825582611914, 0.20398023962860065, 0.23155611437084694, 0.21001535083561026, 0.2459570744171641, 0.20090505550620455, 0.08857407446138814, 0.2083920234833763, 0.14074828796424832, 0.2054959506482463, 0.1883684315846394, 0.24384054872864927, 0.16186279835879763, 0.22830070494067545, 0.26360193122814285, 0.23575309943674766, 0.22306629423311305, 0.2706666341950228, 0.14998277819167063, 0.2471327073665815, 0.10067529409556657, 0.12859320115548686, 0.1081942634816237, 0.24105557319991927, 0.18742711036694862, 0.20429557975387092, 0.24108994594285524, 0.1874617564547288, 0.22176507011362412, 0.12948504751187542, 0.1636122331933148, 0.12065786971332618, 0.20384216529229085, 0.08523041213097474, 0.22887473580869255, 0.17990812519465388, 0.15924363131451097, 0.33225463638374536, 0.14966707274760005, 0.23187684328128308, 0.15874429948241361, 0.2191697885600525, 0.18216099236917194, 0.23590095988256493, 0.21226233895632157, 0.18410746771898667, 0.1394438862526093, 0.3692209147426116, 0.22903705192556356, 0.29767808435403265, 0.13465716638404934, 0.2631164008430925, 0.1537472545755153, 0.1597281200902861, 0.14540650639766592, 0.20149497752794285, 0.15147545833786583, 0.20646197305596387, 0.1969938431850904, 0.1943006565996446, 0.1500516313943261, 0.19007665984150218, 0.15108560786858638, 0.17309371762475223, 0.17417791058716886, 0.1387556829301086, 0.13227223875262917, 0.10830004309560806, 0.20510934820028473, 0.3264369927153404, 0.23772227601006532, 0.19020524484783577, 0.17704148517121537, 0.14402971406860174, 0.21974728092381446, 0.24296006607310658, 0.2054666958254069, 0.25751226019786755, 0.16370176203173584, 0.245211210488489, 0.12696370688193856, 0.1275591653277981, 0.1712836086767902, 0.31267321974392426, 0.12817266848725983, 0.16819564873194073, 0.20801979360014797, 0.11136295592444753, 0.14465317802753802, 0.20668636306566515, 0.22568741359917896, 0.24121129842462433, 0.277571448407615, 0.24101467515382508, 0.2895520652303189, 0.3080080445480223, 0.14061367501251906, 0.20119325403627328, 0.21030609583203919, 0.25576786781396604, 0.16736312104373166, 0.21028360001280558, 0.14819224237195144, 0.15949627908766267, 0.232208811581621, 0.2939507158492481, 0.1364378828151897, 0.290905513142891, 0.253450909862843, 0.2009380516164805, 0.17182783573898908, 0.1419442801559773, 0.22125619112734557, 0.22460915604074774, 0.22178553224548936, 0.12990894055188182, 0.2552596927402361, 0.10134438994596691, 0.07125521441715339, 0.17609408808872673, 0.15879954664603985, 0.3017613229931847, 0.1586929916029132, 0.07013870003122427, 0.04533166948465791, 0.16861159321431404, 0.05181010755801481, 0.07301088098960316, 0.10317727044674581, 0.0586915169626244, 0.24997100163093933, 0.19932645022769335, 0.050563149141106836, 0.033423907293984687, 0.15314290652110604, 0.07903742032078057, 0.11484934076499752, 0.08399789294815228, 0.05769106417474837, 0.07453993082710067, 0.08105054891284277, 0.10775902044510745, 0.15740021497330822, 0.049702844405578345, 0.15512577363903685, 0.19620741222114052, 0.09292294465863402, 0.06487200418065109, 0.049604380823531226, 0.06057017488258701, 0.26991902652741445, 0.08886206927270221, 0.11073002027030385, 0.08059036762533188, 0.06900758059807326, 0.10346790822846004, 0.06397229237417104, 0.2857680141312172, 0.13798249813327068, 0.057981476626323435, 0.07476627639728146, 0.0636511320707623, 0.1536423086574587, 0.042850172241866005, 0.1261535897279094, 0.0864080445699538, 0.05821641688395811, 0.19834069980775887, 0.05744342782817261, 0.04455555083794113, 0.041204803249319806, 0.08932912064921161, 0.09506283601256357, 0.15337154657546437, 0.16182394293026586, 0.06351449866153144, 0.1361729276908702, 0.09370570992468016, 0.08034953675447953, 0.11070142209895667, 0.10162287217134192, 0.13953738804518961, 0.05930717105317358, 0.09599162522328739, 0.10464978597972398, 0.09497572589848462, 0.047172958447060716, 0.10729140396753437, 0.04581503573636603, 0.15441581493596165, 0.07704680841496858, 0.11649844681081778, 0.051304820576782866, 0.08122279477220229, 0.1356806046971732, 0.054182014619186716, 0.05210752147878856, 0.18291827251359657, 0.05300448812263204, 0.052258921831018844, 0.03866252878496122, 0.08712892618654239, 0.23981159911279126, 0.12986251630026413, 0.11450953178003695, 0.09981383359800824, 0.07983094205608628, 0.22307338180070477, 0.06482734268528571, 0.05314742149975142, 0.20642900657220833, 0.1483418565925283, 0.08714600140970732, 0.09242967142605236, 0.10340454224604534, 0.11913218734408845, 0.17367830903493411, 0.11258238711073847, 0.0817410923193829, 0.10943311828684567, 0.07790283293288454, 0.14285009827926212, 0.030592936543973975, 0.12364329436256351, 0.05974748342760525, 0.2791710284115574, 0.1878897351656466, 0.1605501758643919, 0.11981176219576606, 0.13913016305746936, 0.05945788460111787, 0.15809964060745935, 0.17289563808240488, 0.03878144333207712, 0.07270970909618994, 0.07654845300692736, 0.05623961531269877, 0.042520693409558025, 0.17691373973853902, 0.15094635511946997, 0.10170808607419879, 0.2532902477145475, 0.15666858860815563, 0.11710801081594172, 0.20375474926027806, 0.14413358108973548, 0.15371660820809158, 0.1437764937347471, 0.04533922913254318, 0.10544947624868795, 0.0651364172120416, 0.1863048624802984, 0.16752370590385335, 0.06839463544446721, 0.032944443331830386, 0.03369174344249711, 0.1200402344359334, 0.06903465945679221, 0.2111323915223152, 0.16909435118423757, 0.04849490444446199, 0.11144857328283068, 0.12049354152531644, 0.0744299533828815, 0.030139184101841427, 0.07674801630530236, 0.13261725905170818, 0.08288110431125029, 0.13415752978191176, 0.21695752493191794, 0.049728274208288833, 0.10911380526932282, 0.24336853046663323, 0.06872100423803755, 0.10884129067302728, 0.32085560296628773, 0.06284570682888654, 0.05300172801731767, 0.051331397436154086, 0.05007695637871672, 0.048226386949189785, 0.09841242447792993, 0.11827232410802493, 0.056999496388132745, 0.05010985432870626, 0.12943960790580764, 0.07189567162003649, 0.048804481880220234, 0.04776516007624721, 0.05993412801933028, 0.0735902357647765, 0.11161690320138498, 0.1523520367769613, 0.11502466794280686, 0.053595708565530406, 0.05786240588431748, 0.03556329467612566, 0.05768929721692225, 0.06513100306873101, 0.10840269040359808, 0.051152415560967945, 0.15874673880731396, 0.11906317496276943, 0.0501595671269724, 0.0731074197837932, 0.15341874675494266, 0.1074126505448423, 0.04058802271978635, 0.1001210168931764, 0.05988780324321428, 0.22107728399162233, 0.2073664412298892, 0.1594645806777202, 0.13997423478611565, 0.0764841282087874, 0.07104332564962709, 0.16551798335496132, 0.05124089538931911, 0.1567400517624922, 0.07689783444388967, 0.13182011457496634, 0.07003219681295335, 0.13542925183734295, 0.10051230875627208, 0.09292649588266308, 0.08608232444201235, 0.0641990442699989, 0.03741593980020146, 0.1504993155014148, 0.1433821660653144, 0.06080269286200854, 0.1107168767384162, 0.03503383643647976, 0.18308225367064268, 0.056721890524310446, 0.04105287803842216, 0.20192100418916342, 0.04391294861131538, 0.10237489198241603, 0.0321322357114065, 0.13364092365408192, 0.12227388237033988, 0.13417811573728589, 0.16311180860955135, 0.12564750689296647, 0.09191196213663178, 0.11900606535335181, 0.05592386525819591, 0.10874569419935051, 0.1847528978824203, 0.10483106794982174, 0.042303728088034065, 0.056483823487470476, 0.03953036955295192, 0.10555408326042157, 0.11661160210481827, 0.1844021781496633, 0.1452671376928431, 0.04872611982386967, 0.11530872513484233, 0.11052417242550978, 0.08712011221964502, 0.08583050869447992, 0.11075667179062165, 0.17195111220518047, 0.040368788428485895, 0.07726903007224373, 0.08231018142503421, 0.10763851324191502, 0.054798746651704755, 0.04932977953520551, 0.034512106698018706, 0.051453677520122636, 0.12392265992551436, 0.08186153425053024, 0.08171880761372599, 0.1598411499725629, 0.11856587809994479, 0.14758812100382745, 0.1060116491978021, 0.08630598193179144, 0.18242921382619745, 0.10925784895385661, 0.15307653609179658, 0.08117349017527924, 0.13243394976290127, 0.09147734750979218, 0.11519090167009888, 0.10777795529690255, 0.15476884008189964, 0.13301446725366106, 0.09973014844060535, 0.05403557575251267, 0.14610444436739886, 0.23376801574702782, 0.15656555893131083, 0.048943653122054454, 0.0364818885199288, 0.0702888456048191, 0.13454258626672358, 0.04963071646103338, 0.05216346639203257, 0.20936379350179207, 0.13859198548196563, 0.08891550110881948, 0.05953402076216266, 0.0659143963809299, 0.04685643343510554, 0.11982521329594438, 0.1897127761805435, 0.17427162474082372, 0.08179511007987338, 0.12382904040331034, 0.10123136218675606, 0.04956737259936906, 0.03354239027831012, 0.1266517667088089, 0.22052685205521594, 0.1028874678549197, 0.03563231228228858, 0.07668471341038284, 0.17405521244274397, 0.18797057445944024, 0.062373443499519, 0.20502295525279637, 0.0730993757328158, 0.15896029319489205, 0.11763861194056681, 0.1373888117467976, 0.15295397109269995, 0.18857976627615555, 0.050940557201466966, 0.03944327982721614, 0.053639281428606675, 0.05969446318279603, 0.09542826839694785, 0.13954388099975393, 0.11880380552056329, 0.05496946970487932, 0.07443774085118415, 0.060033795548842535, 0.13163475693377416, 0.02728351724215895, 0.10643682727047611, 0.06930942171890087, 0.05377775990588339, 0.079311801400647, 0.05724398142824988, 0.034005021455034264, 0.0558238121668386, 0.06561836410890035, 0.13793942119494507, 0.07800644034273978, 0.12397284724487591, 0.04766938816365242, 0.02841311285642574, 0.05914795936426456, 0.04638224746157977, 0.07109784631458906, 0.09280572637377552, 0.07180931945824662, 0.1296744070575207, 0.08892188047803778, 0.04111874079658769, 0.0706542932233011, 0.03576176075603896, 0.04543095523673909, 0.08164258851562806, 0.12835418076811744, 0.05992337017429906, 0.05852457065645904, 0.13933462089030466, 0.03584190608916194, 0.0845737526684312, 0.07785149695073, 0.15050525204324722, 0.1491125861610219, 0.23136224478050013, 0.162076452122883, 0.10084879687157121, 0.07954777683618737, 0.018090533492350267, 0.1238787349362269, 0.0625071797394259, 0.14907542275393004, 0.10529331549378664, 0.10877357216181845, 0.04502724894093858, 0.0943474692624761, 0.03013712516825439, 0.1450734156424235, 0.04614324107221545, 0.07416244311035829, 0.053814735800903166, 0.04266723846348521, 0.1007738203248583, 0.12795441819962133, 0.06743813091290862, 0.13419215189717132, 0.10461850645602389, 0.051299949575879876, 0.10564025318924612, 0.06424315928804375, 0.0884171900138989, 0.049856750839395475, 0.0373566803339807, 0.07894174349747919, 0.13000411843931878, 0.057284884161139873, 0.06280032831398075, 0.059130032552452395, 0.07174517656857358, 0.08213166450080936, 0.08343082534636956, 0.05544205183914313, 0.06566447474319423, 0.10509078479387789, 0.1468934562316988, 0.10253195497688186, 0.06520024556595272, 0.06165112972427351, 0.048960466894896665, 0.13258311249968022, 0.09521058837178965, 0.09110474842938712, 0.05249140248193809, 0.08965447605846581, 0.06706428536752326, 0.12147744613595769, 0.09759789385727527, 0.18631623239665981, 0.11841190419292567, 0.05655663837233039, 0.09144784748220096, 0.06985559489666447, 0.2198881006684583, 0.1474687280059243, 0.08295158952552675, 0.07447186681579321, 0.0800607300340413, 0.15906801909072638, 0.19131424402148584, 0.08391203173435541, 0.09030210105966278, 0.05958168161532045, 0.16354842425900243, 0.11846693634284143, 0.0745974831802336, 0.15809321314978275, 0.045242909086979456, 0.12127921621327364, 0.06049269543965273, 0.053649811750688725, 0.07773421701318715, 0.05423158158492175, 0.05168627040634518, 0.2576915665987629, 0.07261874900693044, 0.11557859512006817, 0.05960750823415322, 0.05518934585713958, 0.09471624923290255, 0.06482019212316395, 0.1111629675788088, 0.09223570832766223, 0.12316293198506649, 0.13743301623840276, 0.04861000811008176, 0.03437332037937423, 0.09822564081342873, 0.107327438380118, 0.09071983823691658, 0.06589932200885248, 0.07883026111849076, 0.1376500634218502, 0.07639479583049874, 0.052263728354810225, 0.13553730294896404, 0.06667274388360776, 0.10875950199672886]
0.0904880847313365
Making ranges
torch.Size([48521, 2])
We keep 7.81e+06/7.29e+08 =  1% of the original kernel matrix.

torch.Size([2332, 2])
We keep 4.58e+04/6.72e+05 =  6% of the original kernel matrix.

torch.Size([12954, 2])
We keep 7.06e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([28063, 2])
We keep 5.64e+06/2.80e+08 =  2% of the original kernel matrix.

torch.Size([38429, 2])
We keep 5.98e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([14375, 2])
We keep 1.10e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([27307, 2])
We keep 2.83e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([17655, 2])
We keep 7.25e+06/1.74e+08 =  4% of the original kernel matrix.

torch.Size([29295, 2])
We keep 4.95e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([24053, 2])
We keep 3.38e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([35461, 2])
We keep 4.80e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([75708, 2])
We keep 2.46e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([61961, 2])
We keep 1.27e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([98353, 2])
We keep 3.56e+07/3.04e+09 =  1% of the original kernel matrix.

torch.Size([69806, 2])
We keep 1.54e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([84201, 2])
We keep 2.31e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([65222, 2])
We keep 1.32e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([21543, 2])
We keep 4.12e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([33170, 2])
We keep 4.55e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([23321, 2])
We keep 9.04e+06/2.57e+08 =  3% of the original kernel matrix.

torch.Size([33888, 2])
We keep 5.63e+06/4.33e+08 =  1% of the original kernel matrix.

torch.Size([12722, 2])
We keep 1.25e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([25412, 2])
We keep 2.81e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([139735, 2])
We keep 1.31e+08/7.74e+09 =  1% of the original kernel matrix.

torch.Size([82602, 2])
We keep 2.28e+07/2.38e+09 =  0% of the original kernel matrix.

torch.Size([18359, 2])
We keep 1.51e+06/6.60e+07 =  2% of the original kernel matrix.

torch.Size([30502, 2])
We keep 3.37e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([698722, 2])
We keep 1.13e+09/1.39e+11 =  0% of the original kernel matrix.

torch.Size([194197, 2])
We keep 8.16e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([42710, 2])
We keep 7.89e+06/5.88e+08 =  1% of the original kernel matrix.

torch.Size([47723, 2])
We keep 7.92e+06/6.55e+08 =  1% of the original kernel matrix.

torch.Size([109984, 2])
We keep 4.84e+07/3.53e+09 =  1% of the original kernel matrix.

torch.Size([73696, 2])
We keep 1.65e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([140606, 2])
We keep 7.63e+07/6.30e+09 =  1% of the original kernel matrix.

torch.Size([84223, 2])
We keep 2.10e+07/2.14e+09 =  0% of the original kernel matrix.

torch.Size([42002, 2])
We keep 9.33e+06/6.22e+08 =  1% of the original kernel matrix.

torch.Size([45272, 2])
We keep 7.77e+06/6.73e+08 =  1% of the original kernel matrix.

torch.Size([248285, 2])
We keep 2.44e+08/2.18e+10 =  1% of the original kernel matrix.

torch.Size([114159, 2])
We keep 3.54e+07/3.99e+09 =  0% of the original kernel matrix.

torch.Size([199403, 2])
We keep 1.87e+08/1.38e+10 =  1% of the original kernel matrix.

torch.Size([101831, 2])
We keep 2.96e+07/3.17e+09 =  0% of the original kernel matrix.

torch.Size([50329, 2])
We keep 2.76e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([48061, 2])
We keep 1.17e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([480996, 2])
We keep 5.51e+08/6.82e+10 =  0% of the original kernel matrix.

torch.Size([162728, 2])
We keep 5.86e+07/7.05e+09 =  0% of the original kernel matrix.

torch.Size([23653, 2])
We keep 3.11e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([34895, 2])
We keep 4.39e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([66505, 2])
We keep 5.59e+07/1.78e+09 =  3% of the original kernel matrix.

torch.Size([57898, 2])
We keep 1.25e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([3107, 2])
We keep 1.58e+05/1.75e+06 =  9% of the original kernel matrix.

torch.Size([13971, 2])
We keep 9.14e+05/3.57e+07 =  2% of the original kernel matrix.

torch.Size([142515, 2])
We keep 5.49e+07/6.05e+09 =  0% of the original kernel matrix.

torch.Size([84890, 2])
We keep 2.06e+07/2.10e+09 =  0% of the original kernel matrix.

torch.Size([95353, 2])
We keep 4.70e+07/3.44e+09 =  1% of the original kernel matrix.

torch.Size([68370, 2])
We keep 1.64e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([9236, 2])
We keep 4.72e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([22241, 2])
We keep 1.93e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([63830, 2])
We keep 9.82e+07/2.31e+09 =  4% of the original kernel matrix.

torch.Size([56400, 2])
We keep 1.36e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([2666865, 2])
We keep 7.22e+09/1.65e+12 =  0% of the original kernel matrix.

torch.Size([399692, 2])
We keep 2.51e+08/3.47e+10 =  0% of the original kernel matrix.

torch.Size([20490, 2])
We keep 4.19e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([32160, 2])
We keep 4.58e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([1285483, 2])
We keep 4.91e+09/6.19e+11 =  0% of the original kernel matrix.

torch.Size([267492, 2])
We keep 1.62e+08/2.12e+10 =  0% of the original kernel matrix.

torch.Size([36268, 2])
We keep 2.00e+07/6.75e+08 =  2% of the original kernel matrix.

torch.Size([41921, 2])
We keep 8.06e+06/7.02e+08 =  1% of the original kernel matrix.

torch.Size([29164, 2])
We keep 4.05e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([39239, 2])
We keep 5.47e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([30757, 2])
We keep 7.88e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([39611, 2])
We keep 6.88e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([274929, 2])
We keep 5.17e+08/3.47e+10 =  1% of the original kernel matrix.

torch.Size([119551, 2])
We keep 4.41e+07/5.03e+09 =  0% of the original kernel matrix.

torch.Size([191324, 2])
We keep 1.82e+08/1.23e+10 =  1% of the original kernel matrix.

torch.Size([99552, 2])
We keep 2.80e+07/2.99e+09 =  0% of the original kernel matrix.

torch.Size([3456, 2])
We keep 1.01e+05/1.94e+06 =  5% of the original kernel matrix.

torch.Size([15040, 2])
We keep 9.83e+05/3.76e+07 =  2% of the original kernel matrix.

torch.Size([14264, 2])
We keep 1.30e+06/4.44e+07 =  2% of the original kernel matrix.

torch.Size([26849, 2])
We keep 2.96e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([6591, 2])
We keep 3.26e+05/8.03e+06 =  4% of the original kernel matrix.

torch.Size([19029, 2])
We keep 1.59e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([12387, 2])
We keep 8.11e+05/2.73e+07 =  2% of the original kernel matrix.

torch.Size([25195, 2])
We keep 2.46e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([4015, 2])
We keep 2.41e+05/4.56e+06 =  5% of the original kernel matrix.

torch.Size([14666, 2])
We keep 1.30e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([2002, 2])
We keep 8.47e+04/7.24e+05 = 11% of the original kernel matrix.

torch.Size([11660, 2])
We keep 6.82e+05/2.30e+07 =  2% of the original kernel matrix.

torch.Size([14756, 2])
We keep 1.25e+06/4.56e+07 =  2% of the original kernel matrix.

torch.Size([26973, 2])
We keep 2.95e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([1662, 2])
We keep 2.42e+04/2.97e+05 =  8% of the original kernel matrix.

torch.Size([11606, 2])
We keep 5.39e+05/1.47e+07 =  3% of the original kernel matrix.

torch.Size([2524, 2])
We keep 6.25e+04/8.52e+05 =  7% of the original kernel matrix.

torch.Size([13427, 2])
We keep 7.51e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([8874, 2])
We keep 5.73e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([21767, 2])
We keep 1.97e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([7231, 2])
We keep 3.28e+05/8.47e+06 =  3% of the original kernel matrix.

torch.Size([19925, 2])
We keep 1.61e+06/7.86e+07 =  2% of the original kernel matrix.

torch.Size([3624, 2])
We keep 9.59e+04/1.78e+06 =  5% of the original kernel matrix.

torch.Size([15282, 2])
We keep 9.49e+05/3.61e+07 =  2% of the original kernel matrix.

torch.Size([26039, 2])
We keep 4.85e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([36510, 2])
We keep 5.46e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([24826, 2])
We keep 3.87e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([35838, 2])
We keep 5.15e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([6865, 2])
We keep 3.24e+05/8.16e+06 =  3% of the original kernel matrix.

torch.Size([19613, 2])
We keep 1.59e+06/7.72e+07 =  2% of the original kernel matrix.

torch.Size([14011, 2])
We keep 1.40e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([26430, 2])
We keep 3.04e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([6089, 2])
We keep 2.58e+05/5.83e+06 =  4% of the original kernel matrix.

torch.Size([18496, 2])
We keep 1.41e+06/6.52e+07 =  2% of the original kernel matrix.

torch.Size([17806, 2])
We keep 1.85e+06/7.81e+07 =  2% of the original kernel matrix.

torch.Size([29946, 2])
We keep 3.63e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([9778, 2])
We keep 7.99e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([22344, 2])
We keep 2.24e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([5437, 2])
We keep 1.76e+05/4.24e+06 =  4% of the original kernel matrix.

torch.Size([17885, 2])
We keep 1.26e+06/5.56e+07 =  2% of the original kernel matrix.

torch.Size([9564, 2])
We keep 5.29e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([22403, 2])
We keep 2.02e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([6027, 2])
We keep 2.45e+05/6.15e+06 =  3% of the original kernel matrix.

torch.Size([18482, 2])
We keep 1.44e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([7368, 2])
We keep 4.07e+05/1.06e+07 =  3% of the original kernel matrix.

torch.Size([19741, 2])
We keep 1.73e+06/8.79e+07 =  1% of the original kernel matrix.

torch.Size([9550, 2])
We keep 5.20e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([22446, 2])
We keep 1.96e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([3868, 2])
We keep 1.30e+05/2.36e+06 =  5% of the original kernel matrix.

torch.Size([15552, 2])
We keep 1.04e+06/4.15e+07 =  2% of the original kernel matrix.

torch.Size([44638, 2])
We keep 9.62e+06/6.82e+08 =  1% of the original kernel matrix.

torch.Size([49560, 2])
We keep 8.48e+06/7.05e+08 =  1% of the original kernel matrix.

torch.Size([13801, 2])
We keep 1.04e+06/3.66e+07 =  2% of the original kernel matrix.

torch.Size([26399, 2])
We keep 2.71e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([3935, 2])
We keep 1.21e+05/2.47e+06 =  4% of the original kernel matrix.

torch.Size([15622, 2])
We keep 1.06e+06/4.25e+07 =  2% of the original kernel matrix.

torch.Size([6986, 2])
We keep 3.01e+05/7.67e+06 =  3% of the original kernel matrix.

torch.Size([19651, 2])
We keep 1.57e+06/7.48e+07 =  2% of the original kernel matrix.

torch.Size([4111, 2])
We keep 1.38e+05/2.79e+06 =  4% of the original kernel matrix.

torch.Size([15922, 2])
We keep 1.11e+06/4.51e+07 =  2% of the original kernel matrix.

torch.Size([15067, 2])
We keep 1.35e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([27417, 2])
We keep 2.95e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([8075, 2])
We keep 4.31e+05/1.14e+07 =  3% of the original kernel matrix.

torch.Size([20880, 2])
We keep 1.79e+06/9.12e+07 =  1% of the original kernel matrix.

torch.Size([3764, 2])
We keep 1.13e+05/2.09e+06 =  5% of the original kernel matrix.

torch.Size([15257, 2])
We keep 1.00e+06/3.90e+07 =  2% of the original kernel matrix.

torch.Size([8481, 2])
We keep 8.12e+05/1.51e+07 =  5% of the original kernel matrix.

torch.Size([20720, 2])
We keep 1.96e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([7158, 2])
We keep 2.96e+05/7.78e+06 =  3% of the original kernel matrix.

torch.Size([19883, 2])
We keep 1.55e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([6620, 2])
We keep 2.79e+05/7.26e+06 =  3% of the original kernel matrix.

torch.Size([19276, 2])
We keep 1.53e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([5444, 2])
We keep 2.33e+05/5.23e+06 =  4% of the original kernel matrix.

torch.Size([17688, 2])
We keep 1.37e+06/6.18e+07 =  2% of the original kernel matrix.

torch.Size([8442, 2])
We keep 5.14e+05/1.39e+07 =  3% of the original kernel matrix.

torch.Size([20960, 2])
We keep 1.93e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([9979, 2])
We keep 8.50e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([22594, 2])
We keep 2.26e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([1733, 2])
We keep 2.40e+04/3.04e+05 =  7% of the original kernel matrix.

torch.Size([11814, 2])
We keep 5.33e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([2013, 2])
We keep 3.41e+04/4.42e+05 =  7% of the original kernel matrix.

torch.Size([12295, 2])
We keep 6.05e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([5948, 2])
We keep 2.47e+05/5.46e+06 =  4% of the original kernel matrix.

torch.Size([18510, 2])
We keep 1.36e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([40352, 2])
We keep 8.30e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([46762, 2])
We keep 7.96e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([7106, 2])
We keep 3.05e+05/7.57e+06 =  4% of the original kernel matrix.

torch.Size([19776, 2])
We keep 1.53e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([2243, 2])
We keep 5.37e+04/7.59e+05 =  7% of the original kernel matrix.

torch.Size([12376, 2])
We keep 7.25e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([32674, 2])
We keep 5.52e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([41997, 2])
We keep 6.39e+06/4.95e+08 =  1% of the original kernel matrix.

torch.Size([2338, 2])
We keep 5.25e+04/7.83e+05 =  6% of the original kernel matrix.

torch.Size([12879, 2])
We keep 7.23e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([6995, 2])
We keep 3.88e+05/9.43e+06 =  4% of the original kernel matrix.

torch.Size([19457, 2])
We keep 1.69e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([3518, 2])
We keep 8.89e+04/1.61e+06 =  5% of the original kernel matrix.

torch.Size([15058, 2])
We keep 9.24e+05/3.42e+07 =  2% of the original kernel matrix.

torch.Size([4902, 2])
We keep 2.20e+05/4.24e+06 =  5% of the original kernel matrix.

torch.Size([16847, 2])
We keep 1.27e+06/5.56e+07 =  2% of the original kernel matrix.

torch.Size([2352, 2])
We keep 4.79e+04/6.82e+05 =  7% of the original kernel matrix.

torch.Size([13010, 2])
We keep 7.05e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([15668, 2])
We keep 1.45e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([28160, 2])
We keep 3.20e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([9014, 2])
We keep 4.69e+05/1.30e+07 =  3% of the original kernel matrix.

torch.Size([22028, 2])
We keep 1.87e+06/9.74e+07 =  1% of the original kernel matrix.

torch.Size([2908, 2])
We keep 5.93e+04/1.03e+06 =  5% of the original kernel matrix.

torch.Size([14078, 2])
We keep 7.93e+05/2.75e+07 =  2% of the original kernel matrix.

torch.Size([3082, 2])
We keep 7.24e+04/1.32e+06 =  5% of the original kernel matrix.

torch.Size([14338, 2])
We keep 8.61e+05/3.10e+07 =  2% of the original kernel matrix.

torch.Size([3558, 2])
We keep 8.16e+04/1.56e+06 =  5% of the original kernel matrix.

torch.Size([15200, 2])
We keep 8.99e+05/3.37e+07 =  2% of the original kernel matrix.

torch.Size([6502, 2])
We keep 3.32e+05/7.73e+06 =  4% of the original kernel matrix.

torch.Size([19016, 2])
We keep 1.56e+06/7.51e+07 =  2% of the original kernel matrix.

torch.Size([10617, 2])
We keep 7.19e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([23493, 2])
We keep 2.23e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([12793, 2])
We keep 8.91e+05/3.24e+07 =  2% of the original kernel matrix.

torch.Size([25625, 2])
We keep 2.58e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([7848, 2])
We keep 3.52e+05/9.87e+06 =  3% of the original kernel matrix.

torch.Size([20834, 2])
We keep 1.71e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([19718, 2])
We keep 3.55e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([31473, 2])
We keep 4.48e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([10186, 2])
We keep 6.25e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([23074, 2])
We keep 2.12e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([7269, 2])
We keep 2.97e+05/7.85e+06 =  3% of the original kernel matrix.

torch.Size([20131, 2])
We keep 1.56e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([22697, 2])
We keep 3.03e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([34055, 2])
We keep 4.58e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([6991, 2])
We keep 3.57e+05/7.82e+06 =  4% of the original kernel matrix.

torch.Size([19651, 2])
We keep 1.57e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([10813, 2])
We keep 6.43e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([23615, 2])
We keep 2.22e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([5064, 2])
We keep 2.06e+05/4.27e+06 =  4% of the original kernel matrix.

torch.Size([17128, 2])
We keep 1.27e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([10207, 2])
We keep 1.09e+06/2.54e+07 =  4% of the original kernel matrix.

torch.Size([22828, 2])
We keep 2.36e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([8040, 2])
We keep 3.83e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([20849, 2])
We keep 1.74e+06/8.99e+07 =  1% of the original kernel matrix.

torch.Size([12693, 2])
We keep 8.19e+05/2.83e+07 =  2% of the original kernel matrix.

torch.Size([25528, 2])
We keep 2.48e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([2196, 2])
We keep 3.63e+04/4.89e+05 =  7% of the original kernel matrix.

torch.Size([12698, 2])
We keep 6.10e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([4646, 2])
We keep 1.52e+05/3.38e+06 =  4% of the original kernel matrix.

torch.Size([16754, 2])
We keep 1.17e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([2816, 2])
We keep 6.21e+04/1.04e+06 =  5% of the original kernel matrix.

torch.Size([13695, 2])
We keep 7.90e+05/2.76e+07 =  2% of the original kernel matrix.

torch.Size([5488, 2])
We keep 2.36e+05/5.49e+06 =  4% of the original kernel matrix.

torch.Size([17722, 2])
We keep 1.38e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([4754, 2])
We keep 2.20e+05/3.98e+06 =  5% of the original kernel matrix.

torch.Size([16554, 2])
We keep 1.24e+06/5.39e+07 =  2% of the original kernel matrix.

torch.Size([3762, 2])
We keep 1.34e+05/2.62e+06 =  5% of the original kernel matrix.

torch.Size([15203, 2])
We keep 1.09e+06/4.37e+07 =  2% of the original kernel matrix.

torch.Size([5795, 2])
We keep 2.31e+05/5.46e+06 =  4% of the original kernel matrix.

torch.Size([18174, 2])
We keep 1.38e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([8350, 2])
We keep 5.00e+05/1.37e+07 =  3% of the original kernel matrix.

torch.Size([20938, 2])
We keep 1.90e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([3855, 2])
We keep 1.27e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([15294, 2])
We keep 1.05e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([3438, 2])
We keep 8.81e+04/1.57e+06 =  5% of the original kernel matrix.

torch.Size([14925, 2])
We keep 8.97e+05/3.38e+07 =  2% of the original kernel matrix.

torch.Size([2535, 2])
We keep 5.32e+04/8.26e+05 =  6% of the original kernel matrix.

torch.Size([13356, 2])
We keep 7.48e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([15472, 2])
We keep 1.74e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([27856, 2])
We keep 3.40e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([2689, 2])
We keep 5.78e+04/9.66e+05 =  5% of the original kernel matrix.

torch.Size([13485, 2])
We keep 7.69e+05/2.65e+07 =  2% of the original kernel matrix.

torch.Size([22942, 2])
We keep 2.50e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([34275, 2])
We keep 4.33e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([5108, 2])
We keep 1.94e+05/4.33e+06 =  4% of the original kernel matrix.

torch.Size([17208, 2])
We keep 1.27e+06/5.62e+07 =  2% of the original kernel matrix.

torch.Size([4236, 2])
We keep 1.50e+05/3.22e+06 =  4% of the original kernel matrix.

torch.Size([15909, 2])
We keep 1.16e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([5454, 2])
We keep 2.04e+05/4.62e+06 =  4% of the original kernel matrix.

torch.Size([17787, 2])
We keep 1.28e+06/5.80e+07 =  2% of the original kernel matrix.

torch.Size([3983, 2])
We keep 1.12e+05/2.11e+06 =  5% of the original kernel matrix.

torch.Size([15682, 2])
We keep 9.92e+05/3.92e+07 =  2% of the original kernel matrix.

torch.Size([5320, 2])
We keep 1.80e+05/4.04e+06 =  4% of the original kernel matrix.

torch.Size([17784, 2])
We keep 1.26e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([3472, 2])
We keep 8.48e+04/1.57e+06 =  5% of the original kernel matrix.

torch.Size([15027, 2])
We keep 9.09e+05/3.38e+07 =  2% of the original kernel matrix.

torch.Size([5646, 2])
We keep 2.49e+05/5.11e+06 =  4% of the original kernel matrix.

torch.Size([17951, 2])
We keep 1.35e+06/6.10e+07 =  2% of the original kernel matrix.

torch.Size([48728, 2])
We keep 1.02e+07/8.00e+08 =  1% of the original kernel matrix.

torch.Size([51182, 2])
We keep 8.95e+06/7.64e+08 =  1% of the original kernel matrix.

torch.Size([5349, 2])
We keep 1.90e+05/4.15e+06 =  4% of the original kernel matrix.

torch.Size([17651, 2])
We keep 1.24e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([14270, 2])
We keep 1.18e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([26848, 2])
We keep 2.91e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([5147, 2])
We keep 2.18e+05/4.48e+06 =  4% of the original kernel matrix.

torch.Size([17309, 2])
We keep 1.30e+06/5.72e+07 =  2% of the original kernel matrix.

torch.Size([6582, 2])
We keep 4.01e+05/7.75e+06 =  5% of the original kernel matrix.

torch.Size([19122, 2])
We keep 1.56e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([2986, 2])
We keep 9.02e+04/1.53e+06 =  5% of the original kernel matrix.

torch.Size([13901, 2])
We keep 9.04e+05/3.34e+07 =  2% of the original kernel matrix.

torch.Size([9717, 2])
We keep 1.09e+06/1.87e+07 =  5% of the original kernel matrix.

torch.Size([22576, 2])
We keep 2.10e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([3981, 2])
We keep 1.24e+05/2.44e+06 =  5% of the original kernel matrix.

torch.Size([15723, 2])
We keep 1.06e+06/4.22e+07 =  2% of the original kernel matrix.

torch.Size([2756, 2])
We keep 5.76e+04/9.41e+05 =  6% of the original kernel matrix.

torch.Size([13634, 2])
We keep 7.68e+05/2.62e+07 =  2% of the original kernel matrix.

torch.Size([3564, 2])
We keep 9.78e+04/1.93e+06 =  5% of the original kernel matrix.

torch.Size([15133, 2])
We keep 9.72e+05/3.75e+07 =  2% of the original kernel matrix.

torch.Size([4359, 2])
We keep 1.33e+05/2.77e+06 =  4% of the original kernel matrix.

torch.Size([16252, 2])
We keep 1.10e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([2466, 2])
We keep 5.38e+04/8.35e+05 =  6% of the original kernel matrix.

torch.Size([13190, 2])
We keep 7.41e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([11806, 2])
We keep 1.05e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([24401, 2])
We keep 2.67e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([3123, 2])
We keep 9.32e+04/1.48e+06 =  6% of the original kernel matrix.

torch.Size([14303, 2])
We keep 8.91e+05/3.29e+07 =  2% of the original kernel matrix.

torch.Size([33342, 2])
We keep 6.41e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([41978, 2])
We keep 6.65e+06/5.24e+08 =  1% of the original kernel matrix.

torch.Size([17063, 2])
We keep 1.93e+06/7.89e+07 =  2% of the original kernel matrix.

torch.Size([29172, 2])
We keep 3.62e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([28823, 2])
We keep 4.52e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([39185, 2])
We keep 5.68e+06/4.26e+08 =  1% of the original kernel matrix.

torch.Size([3487, 2])
We keep 9.07e+04/1.70e+06 =  5% of the original kernel matrix.

torch.Size([14996, 2])
We keep 9.21e+05/3.52e+07 =  2% of the original kernel matrix.

torch.Size([7249, 2])
We keep 3.30e+05/7.78e+06 =  4% of the original kernel matrix.

torch.Size([20005, 2])
We keep 1.55e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([4822, 2])
We keep 2.31e+05/4.47e+06 =  5% of the original kernel matrix.

torch.Size([16573, 2])
We keep 1.28e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([3603, 2])
We keep 1.05e+05/1.74e+06 =  6% of the original kernel matrix.

torch.Size([15282, 2])
We keep 9.35e+05/3.56e+07 =  2% of the original kernel matrix.

torch.Size([7278, 2])
We keep 3.39e+05/7.87e+06 =  4% of the original kernel matrix.

torch.Size([20129, 2])
We keep 1.57e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([4105, 2])
We keep 1.49e+05/2.77e+06 =  5% of the original kernel matrix.

torch.Size([15711, 2])
We keep 1.09e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([17922, 2])
We keep 1.98e+06/8.22e+07 =  2% of the original kernel matrix.

torch.Size([30196, 2])
We keep 3.72e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([10335, 2])
We keep 5.71e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([23309, 2])
We keep 2.10e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([16864, 2])
We keep 2.96e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([28851, 2])
We keep 4.15e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([5436, 2])
We keep 2.08e+05/4.70e+06 =  4% of the original kernel matrix.

torch.Size([17755, 2])
We keep 1.32e+06/5.85e+07 =  2% of the original kernel matrix.

torch.Size([50141, 2])
We keep 1.50e+07/9.71e+08 =  1% of the original kernel matrix.

torch.Size([51025, 2])
We keep 9.78e+06/8.41e+08 =  1% of the original kernel matrix.

torch.Size([4160, 2])
We keep 1.12e+05/2.41e+06 =  4% of the original kernel matrix.

torch.Size([16157, 2])
We keep 1.05e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([7354, 2])
We keep 4.81e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([19901, 2])
We keep 1.74e+06/8.63e+07 =  2% of the original kernel matrix.

torch.Size([10304, 2])
We keep 7.77e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([23062, 2])
We keep 2.23e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([1397, 2])
We keep 2.09e+04/2.23e+05 =  9% of the original kernel matrix.

torch.Size([10736, 2])
We keep 4.78e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([12720, 2])
We keep 9.54e+05/3.09e+07 =  3% of the original kernel matrix.

torch.Size([25498, 2])
We keep 2.54e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([3687, 2])
We keep 1.19e+05/2.15e+06 =  5% of the original kernel matrix.

torch.Size([15157, 2])
We keep 1.01e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([10430, 2])
We keep 8.63e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([23286, 2])
We keep 2.31e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([4691, 2])
We keep 1.67e+05/3.19e+06 =  5% of the original kernel matrix.

torch.Size([16897, 2])
We keep 1.18e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([6216, 2])
We keep 3.66e+05/9.29e+06 =  3% of the original kernel matrix.

torch.Size([18437, 2])
We keep 1.67e+06/8.23e+07 =  2% of the original kernel matrix.

torch.Size([3742, 2])
We keep 9.54e+04/1.93e+06 =  4% of the original kernel matrix.

torch.Size([15452, 2])
We keep 9.73e+05/3.75e+07 =  2% of the original kernel matrix.

torch.Size([5073, 2])
We keep 1.71e+05/3.76e+06 =  4% of the original kernel matrix.

torch.Size([17316, 2])
We keep 1.22e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([7124, 2])
We keep 3.39e+05/8.69e+06 =  3% of the original kernel matrix.

torch.Size([19679, 2])
We keep 1.62e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([15299, 2])
We keep 1.26e+06/4.76e+07 =  2% of the original kernel matrix.

torch.Size([27637, 2])
We keep 2.99e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([1156, 2])
We keep 1.23e+04/1.25e+05 =  9% of the original kernel matrix.

torch.Size([10108, 2])
We keep 4.06e+05/9.53e+06 =  4% of the original kernel matrix.

torch.Size([3934, 2])
We keep 1.30e+05/2.33e+06 =  5% of the original kernel matrix.

torch.Size([15553, 2])
We keep 1.05e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([1850, 2])
We keep 3.51e+04/4.60e+05 =  7% of the original kernel matrix.

torch.Size([11612, 2])
We keep 6.05e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([16643, 2])
We keep 1.47e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([29000, 2])
We keep 3.20e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([2800, 2])
We keep 5.99e+04/1.06e+06 =  5% of the original kernel matrix.

torch.Size([13884, 2])
We keep 7.91e+05/2.78e+07 =  2% of the original kernel matrix.

torch.Size([11579, 2])
We keep 8.63e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([24530, 2])
We keep 2.40e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([10463, 2])
We keep 1.30e+06/2.11e+07 =  6% of the original kernel matrix.

torch.Size([23433, 2])
We keep 2.17e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([13013, 2])
We keep 1.07e+06/3.63e+07 =  2% of the original kernel matrix.

torch.Size([25681, 2])
We keep 2.71e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([5672, 2])
We keep 2.20e+05/5.29e+06 =  4% of the original kernel matrix.

torch.Size([18159, 2])
We keep 1.38e+06/6.21e+07 =  2% of the original kernel matrix.

torch.Size([10779, 2])
We keep 1.03e+06/2.89e+07 =  3% of the original kernel matrix.

torch.Size([23221, 2])
We keep 2.52e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([5336, 2])
We keep 2.08e+05/4.34e+06 =  4% of the original kernel matrix.

torch.Size([17660, 2])
We keep 1.27e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([5902, 2])
We keep 2.45e+05/5.97e+06 =  4% of the original kernel matrix.

torch.Size([18286, 2])
We keep 1.43e+06/6.60e+07 =  2% of the original kernel matrix.

torch.Size([6208, 2])
We keep 2.77e+05/6.39e+06 =  4% of the original kernel matrix.

torch.Size([18639, 2])
We keep 1.46e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([10443, 2])
We keep 1.01e+06/2.92e+07 =  3% of the original kernel matrix.

torch.Size([22839, 2])
We keep 2.50e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([6774, 2])
We keep 2.98e+05/7.49e+06 =  3% of the original kernel matrix.

torch.Size([19408, 2])
We keep 1.54e+06/7.39e+07 =  2% of the original kernel matrix.

torch.Size([12850, 2])
We keep 8.40e+05/2.93e+07 =  2% of the original kernel matrix.

torch.Size([25675, 2])
We keep 2.50e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([8755, 2])
We keep 4.56e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([21717, 2])
We keep 1.86e+06/9.63e+07 =  1% of the original kernel matrix.

torch.Size([8106, 2])
We keep 4.57e+05/1.21e+07 =  3% of the original kernel matrix.

torch.Size([20743, 2])
We keep 1.83e+06/9.39e+07 =  1% of the original kernel matrix.

torch.Size([13770, 2])
We keep 1.70e+06/5.23e+07 =  3% of the original kernel matrix.

torch.Size([26352, 2])
We keep 3.16e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([17617, 2])
We keep 1.65e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([29796, 2])
We keep 3.36e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([26851, 2])
We keep 4.41e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([37444, 2])
We keep 5.57e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([5162, 2])
We keep 2.37e+05/4.51e+06 =  5% of the original kernel matrix.

torch.Size([17378, 2])
We keep 1.27e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([1621, 2])
We keep 2.17e+04/2.82e+05 =  7% of the original kernel matrix.

torch.Size([11511, 2])
We keep 5.31e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([3447, 2])
We keep 9.46e+04/1.85e+06 =  5% of the original kernel matrix.

torch.Size([14855, 2])
We keep 9.56e+05/3.67e+07 =  2% of the original kernel matrix.

torch.Size([6221, 2])
We keep 4.80e+05/7.23e+06 =  6% of the original kernel matrix.

torch.Size([18642, 2])
We keep 1.42e+06/7.26e+07 =  1% of the original kernel matrix.

torch.Size([6927, 2])
We keep 4.43e+05/1.13e+07 =  3% of the original kernel matrix.

torch.Size([18948, 2])
We keep 1.79e+06/9.09e+07 =  1% of the original kernel matrix.

torch.Size([13854, 2])
We keep 1.13e+06/3.85e+07 =  2% of the original kernel matrix.

torch.Size([26476, 2])
We keep 2.78e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([4833, 2])
We keep 1.40e+05/3.07e+06 =  4% of the original kernel matrix.

torch.Size([17280, 2])
We keep 1.14e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([3572, 2])
We keep 1.10e+05/1.65e+06 =  6% of the original kernel matrix.

torch.Size([15195, 2])
We keep 9.36e+05/3.46e+07 =  2% of the original kernel matrix.

torch.Size([5152, 2])
We keep 2.26e+05/4.36e+06 =  5% of the original kernel matrix.

torch.Size([17261, 2])
We keep 1.27e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([3035, 2])
We keep 6.64e+04/1.17e+06 =  5% of the original kernel matrix.

torch.Size([14214, 2])
We keep 8.30e+05/2.92e+07 =  2% of the original kernel matrix.

torch.Size([10585, 2])
We keep 5.88e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([23576, 2])
We keep 2.16e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([3549, 2])
We keep 8.42e+04/1.57e+06 =  5% of the original kernel matrix.

torch.Size([15285, 2])
We keep 9.00e+05/3.39e+07 =  2% of the original kernel matrix.

torch.Size([19935, 2])
We keep 1.92e+06/9.02e+07 =  2% of the original kernel matrix.

torch.Size([31779, 2])
We keep 3.83e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([18549, 2])
We keep 1.84e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([30699, 2])
We keep 3.66e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([8625, 2])
We keep 5.50e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([21500, 2])
We keep 1.90e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([1787, 2])
We keep 2.41e+04/3.29e+05 =  7% of the original kernel matrix.

torch.Size([11937, 2])
We keep 5.47e+05/1.55e+07 =  3% of the original kernel matrix.

torch.Size([18775, 2])
We keep 2.24e+06/8.82e+07 =  2% of the original kernel matrix.

torch.Size([31018, 2])
We keep 3.83e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([9242, 2])
We keep 5.24e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([22123, 2])
We keep 1.98e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([5219, 2])
We keep 1.91e+05/4.23e+06 =  4% of the original kernel matrix.

torch.Size([17450, 2])
We keep 1.28e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([26774, 2])
We keep 3.45e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([37487, 2])
We keep 5.20e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([13970, 2])
We keep 1.17e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([26598, 2])
We keep 2.88e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([5091, 2])
We keep 2.09e+05/4.27e+06 =  4% of the original kernel matrix.

torch.Size([17184, 2])
We keep 1.28e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([4194, 2])
We keep 1.29e+05/2.63e+06 =  4% of the original kernel matrix.

torch.Size([16083, 2])
We keep 1.09e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([3599, 2])
We keep 9.52e+04/1.73e+06 =  5% of the original kernel matrix.

torch.Size([15144, 2])
We keep 9.43e+05/3.55e+07 =  2% of the original kernel matrix.

torch.Size([2475, 2])
We keep 5.13e+04/7.46e+05 =  6% of the original kernel matrix.

torch.Size([13255, 2])
We keep 7.30e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([3020, 2])
We keep 1.05e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([13780, 2])
We keep 9.25e+05/3.53e+07 =  2% of the original kernel matrix.

torch.Size([2204, 2])
We keep 3.50e+04/5.26e+05 =  6% of the original kernel matrix.

torch.Size([12754, 2])
We keep 6.36e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([1642, 2])
We keep 2.98e+04/3.76e+05 =  7% of the original kernel matrix.

torch.Size([11242, 2])
We keep 5.75e+05/1.66e+07 =  3% of the original kernel matrix.

torch.Size([14182, 2])
We keep 1.30e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([26560, 2])
We keep 2.91e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([5447, 2])
We keep 2.31e+05/5.23e+06 =  4% of the original kernel matrix.

torch.Size([17646, 2])
We keep 1.35e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([4761, 2])
We keep 2.07e+05/3.85e+06 =  5% of the original kernel matrix.

torch.Size([16672, 2])
We keep 1.22e+06/5.30e+07 =  2% of the original kernel matrix.

torch.Size([2783, 2])
We keep 7.05e+04/1.20e+06 =  5% of the original kernel matrix.

torch.Size([13703, 2])
We keep 8.31e+05/2.96e+07 =  2% of the original kernel matrix.

torch.Size([9398, 2])
We keep 5.14e+05/1.54e+07 =  3% of the original kernel matrix.

torch.Size([22193, 2])
We keep 1.97e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([4839, 2])
We keep 1.78e+05/3.82e+06 =  4% of the original kernel matrix.

torch.Size([16908, 2])
We keep 1.22e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([11237, 2])
We keep 1.02e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([23733, 2])
We keep 2.58e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([10054, 2])
We keep 7.74e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([22765, 2])
We keep 2.24e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([4043, 2])
We keep 1.08e+05/2.14e+06 =  5% of the original kernel matrix.

torch.Size([16043, 2])
We keep 1.01e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([1719, 2])
We keep 3.69e+04/5.16e+05 =  7% of the original kernel matrix.

torch.Size([11244, 2])
We keep 6.43e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([15540, 2])
We keep 1.40e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([28014, 2])
We keep 3.14e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([2052, 2])
We keep 4.20e+04/5.48e+05 =  7% of the original kernel matrix.

torch.Size([12227, 2])
We keep 6.44e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([2874, 2])
We keep 6.83e+04/1.25e+06 =  5% of the original kernel matrix.

torch.Size([13824, 2])
We keep 8.37e+05/3.02e+07 =  2% of the original kernel matrix.

torch.Size([5439, 2])
We keep 2.27e+05/5.17e+06 =  4% of the original kernel matrix.

torch.Size([17659, 2])
We keep 1.35e+06/6.14e+07 =  2% of the original kernel matrix.

torch.Size([8538, 2])
We keep 5.17e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([21371, 2])
We keep 1.93e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([14263, 2])
We keep 1.35e+06/4.65e+07 =  2% of the original kernel matrix.

torch.Size([26916, 2])
We keep 3.00e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([4487, 2])
We keep 1.48e+05/2.89e+06 =  5% of the original kernel matrix.

torch.Size([16558, 2])
We keep 1.11e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([3865, 2])
We keep 1.51e+05/2.63e+06 =  5% of the original kernel matrix.

torch.Size([15351, 2])
We keep 1.08e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([4451, 2])
We keep 1.42e+05/2.93e+06 =  4% of the original kernel matrix.

torch.Size([16507, 2])
We keep 1.12e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([18640, 2])
We keep 1.81e+06/8.06e+07 =  2% of the original kernel matrix.

torch.Size([30849, 2])
We keep 3.68e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([2777, 2])
We keep 9.78e+04/1.18e+06 =  8% of the original kernel matrix.

torch.Size([13742, 2])
We keep 8.07e+05/2.93e+07 =  2% of the original kernel matrix.

torch.Size([30876, 2])
We keep 7.75e+06/3.25e+08 =  2% of the original kernel matrix.

torch.Size([39950, 2])
We keep 6.31e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([55176, 2])
We keep 7.15e+07/2.71e+09 =  2% of the original kernel matrix.

torch.Size([50063, 2])
We keep 1.48e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([7905, 2])
We keep 5.49e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([20789, 2])
We keep 1.73e+06/9.17e+07 =  1% of the original kernel matrix.

torch.Size([9899, 2])
We keep 9.57e+05/2.13e+07 =  4% of the original kernel matrix.

torch.Size([22628, 2])
We keep 2.24e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([1800, 2])
We keep 3.31e+04/4.29e+05 =  7% of the original kernel matrix.

torch.Size([11650, 2])
We keep 5.85e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([11185, 2])
We keep 6.84e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([23942, 2])
We keep 2.25e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([74742, 2])
We keep 4.25e+08/3.16e+09 = 13% of the original kernel matrix.

torch.Size([60495, 2])
We keep 1.60e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([245192, 2])
We keep 9.42e+08/4.08e+10 =  2% of the original kernel matrix.

torch.Size([111569, 2])
We keep 4.81e+07/5.46e+09 =  0% of the original kernel matrix.

torch.Size([9182, 2])
We keep 5.14e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([22015, 2])
We keep 1.96e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([196248, 2])
We keep 2.29e+08/1.83e+10 =  1% of the original kernel matrix.

torch.Size([99373, 2])
We keep 3.33e+07/3.65e+09 =  0% of the original kernel matrix.

torch.Size([65622, 2])
We keep 7.11e+07/2.28e+09 =  3% of the original kernel matrix.

torch.Size([56374, 2])
We keep 1.33e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([26315, 2])
We keep 8.86e+06/2.86e+08 =  3% of the original kernel matrix.

torch.Size([36413, 2])
We keep 5.99e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([108917, 2])
We keep 8.30e+08/8.72e+09 =  9% of the original kernel matrix.

torch.Size([73650, 2])
We keep 2.33e+07/2.52e+09 =  0% of the original kernel matrix.

torch.Size([3230, 2])
We keep 7.72e+04/1.41e+06 =  5% of the original kernel matrix.

torch.Size([14740, 2])
We keep 8.73e+05/3.21e+07 =  2% of the original kernel matrix.

torch.Size([5539, 2])
We keep 2.60e+05/5.47e+06 =  4% of the original kernel matrix.

torch.Size([17702, 2])
We keep 1.38e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([209135, 2])
We keep 2.45e+08/2.13e+10 =  1% of the original kernel matrix.

torch.Size([100709, 2])
We keep 3.54e+07/3.94e+09 =  0% of the original kernel matrix.

torch.Size([960463, 2])
We keep 2.04e+09/2.56e+11 =  0% of the original kernel matrix.

torch.Size([233859, 2])
We keep 1.07e+08/1.37e+10 =  0% of the original kernel matrix.

torch.Size([11092, 2])
We keep 1.56e+06/2.67e+07 =  5% of the original kernel matrix.

torch.Size([23736, 2])
We keep 2.46e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([67029, 2])
We keep 2.24e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([58976, 2])
We keep 1.14e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([19307, 2])
We keep 6.22e+06/1.46e+08 =  4% of the original kernel matrix.

torch.Size([30842, 2])
We keep 4.60e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([51096, 2])
We keep 5.88e+07/1.06e+09 =  5% of the original kernel matrix.

torch.Size([51139, 2])
We keep 9.59e+06/8.78e+08 =  1% of the original kernel matrix.

torch.Size([112075, 2])
We keep 2.21e+08/9.52e+09 =  2% of the original kernel matrix.

torch.Size([71756, 2])
We keep 2.49e+07/2.63e+09 =  0% of the original kernel matrix.

torch.Size([51069, 2])
We keep 7.85e+07/2.02e+09 =  3% of the original kernel matrix.

torch.Size([47997, 2])
We keep 1.31e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([59416, 2])
We keep 2.91e+07/1.25e+09 =  2% of the original kernel matrix.

torch.Size([55328, 2])
We keep 1.07e+07/9.53e+08 =  1% of the original kernel matrix.

torch.Size([28353, 2])
We keep 5.13e+06/2.24e+08 =  2% of the original kernel matrix.

torch.Size([38309, 2])
We keep 5.34e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([9578, 2])
We keep 3.28e+06/2.23e+07 = 14% of the original kernel matrix.

torch.Size([22484, 2])
We keep 2.24e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([167240, 2])
We keep 3.56e+08/2.35e+10 =  1% of the original kernel matrix.

torch.Size([88180, 2])
We keep 3.71e+07/4.14e+09 =  0% of the original kernel matrix.

torch.Size([9779, 2])
We keep 1.13e+06/2.41e+07 =  4% of the original kernel matrix.

torch.Size([22553, 2])
We keep 2.29e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([5832, 2])
We keep 2.53e+05/5.81e+06 =  4% of the original kernel matrix.

torch.Size([18310, 2])
We keep 1.40e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([40251, 2])
We keep 9.78e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([45625, 2])
We keep 7.57e+06/6.31e+08 =  1% of the original kernel matrix.

torch.Size([87053, 2])
We keep 9.52e+07/4.65e+09 =  2% of the original kernel matrix.

torch.Size([62186, 2])
We keep 1.86e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([246054, 2])
We keep 2.94e+08/2.60e+10 =  1% of the original kernel matrix.

torch.Size([113638, 2])
We keep 3.87e+07/4.36e+09 =  0% of the original kernel matrix.

torch.Size([156436, 2])
We keep 6.70e+07/7.11e+09 =  0% of the original kernel matrix.

torch.Size([89089, 2])
We keep 2.20e+07/2.28e+09 =  0% of the original kernel matrix.

torch.Size([2392, 2])
We keep 5.45e+04/8.52e+05 =  6% of the original kernel matrix.

torch.Size([12931, 2])
We keep 7.44e+05/2.49e+07 =  2% of the original kernel matrix.

torch.Size([43948, 2])
We keep 2.81e+07/7.12e+08 =  3% of the original kernel matrix.

torch.Size([47875, 2])
We keep 8.80e+06/7.21e+08 =  1% of the original kernel matrix.

torch.Size([23838, 2])
We keep 3.87e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([34821, 2])
We keep 4.98e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([46507, 2])
We keep 2.87e+07/1.27e+09 =  2% of the original kernel matrix.

torch.Size([47272, 2])
We keep 1.07e+07/9.61e+08 =  1% of the original kernel matrix.

torch.Size([100552, 2])
We keep 5.04e+07/3.28e+09 =  1% of the original kernel matrix.

torch.Size([71204, 2])
We keep 1.56e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([28792, 2])
We keep 5.02e+07/2.81e+08 = 17% of the original kernel matrix.

torch.Size([38521, 2])
We keep 5.49e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([123533, 2])
We keep 6.95e+07/5.16e+09 =  1% of the original kernel matrix.

torch.Size([78210, 2])
We keep 1.93e+07/1.94e+09 =  0% of the original kernel matrix.

torch.Size([2170, 2])
We keep 4.05e+04/5.84e+05 =  6% of the original kernel matrix.

torch.Size([12417, 2])
We keep 6.42e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([15155, 2])
We keep 1.36e+06/5.10e+07 =  2% of the original kernel matrix.

torch.Size([27695, 2])
We keep 3.08e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([136769, 2])
We keep 1.28e+08/9.41e+09 =  1% of the original kernel matrix.

torch.Size([81422, 2])
We keep 2.52e+07/2.62e+09 =  0% of the original kernel matrix.

torch.Size([63481, 2])
We keep 3.92e+07/1.96e+09 =  2% of the original kernel matrix.

torch.Size([55081, 2])
We keep 1.29e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([105670, 2])
We keep 8.05e+07/5.16e+09 =  1% of the original kernel matrix.

torch.Size([70459, 2])
We keep 1.94e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([10741, 2])
We keep 8.78e+05/2.67e+07 =  3% of the original kernel matrix.

torch.Size([23460, 2])
We keep 2.44e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([355912, 2])
We keep 3.77e+08/5.75e+10 =  0% of the original kernel matrix.

torch.Size([137859, 2])
We keep 5.48e+07/6.48e+09 =  0% of the original kernel matrix.

torch.Size([19845, 2])
We keep 1.82e+06/8.71e+07 =  2% of the original kernel matrix.

torch.Size([31722, 2])
We keep 3.74e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([51294, 2])
We keep 1.19e+07/8.41e+08 =  1% of the original kernel matrix.

torch.Size([51665, 2])
We keep 9.13e+06/7.83e+08 =  1% of the original kernel matrix.

torch.Size([142633, 2])
We keep 1.15e+08/9.14e+09 =  1% of the original kernel matrix.

torch.Size([84127, 2])
We keep 2.47e+07/2.58e+09 =  0% of the original kernel matrix.

torch.Size([6013, 2])
We keep 2.34e+05/5.84e+06 =  4% of the original kernel matrix.

torch.Size([18522, 2])
We keep 1.42e+06/6.52e+07 =  2% of the original kernel matrix.

torch.Size([159821, 2])
We keep 1.49e+08/9.80e+09 =  1% of the original kernel matrix.

torch.Size([89781, 2])
We keep 2.55e+07/2.67e+09 =  0% of the original kernel matrix.

torch.Size([318640, 2])
We keep 7.32e+08/5.01e+10 =  1% of the original kernel matrix.

torch.Size([129740, 2])
We keep 5.20e+07/6.04e+09 =  0% of the original kernel matrix.

torch.Size([444245, 2])
We keep 6.72e+08/7.33e+10 =  0% of the original kernel matrix.

torch.Size([154414, 2])
We keep 6.14e+07/7.31e+09 =  0% of the original kernel matrix.

torch.Size([38961, 2])
We keep 1.17e+07/6.72e+08 =  1% of the original kernel matrix.

torch.Size([44945, 2])
We keep 8.44e+06/7.00e+08 =  1% of the original kernel matrix.

torch.Size([35088, 2])
We keep 9.35e+06/4.79e+08 =  1% of the original kernel matrix.

torch.Size([42549, 2])
We keep 7.33e+06/5.91e+08 =  1% of the original kernel matrix.

torch.Size([10202, 2])
We keep 1.09e+06/2.61e+07 =  4% of the original kernel matrix.

torch.Size([22676, 2])
We keep 2.39e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([10721, 2])
We keep 6.29e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([23552, 2])
We keep 2.15e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([94274, 2])
We keep 9.04e+07/5.32e+09 =  1% of the original kernel matrix.

torch.Size([65912, 2])
We keep 1.93e+07/1.97e+09 =  0% of the original kernel matrix.

torch.Size([16343, 2])
We keep 1.39e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([28849, 2])
We keep 3.15e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([35692, 2])
We keep 9.80e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([42842, 2])
We keep 7.60e+06/6.16e+08 =  1% of the original kernel matrix.

torch.Size([56225, 2])
We keep 3.76e+07/1.36e+09 =  2% of the original kernel matrix.

torch.Size([53468, 2])
We keep 1.06e+07/9.95e+08 =  1% of the original kernel matrix.

torch.Size([25171, 2])
We keep 5.67e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([36027, 2])
We keep 4.93e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([23103, 2])
We keep 1.31e+07/3.05e+08 =  4% of the original kernel matrix.

torch.Size([33013, 2])
We keep 6.16e+06/4.72e+08 =  1% of the original kernel matrix.

torch.Size([15870, 2])
We keep 1.29e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([28317, 2])
We keep 3.05e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([60203, 2])
We keep 2.05e+08/7.92e+09 =  2% of the original kernel matrix.

torch.Size([48359, 2])
We keep 2.31e+07/2.40e+09 =  0% of the original kernel matrix.

torch.Size([26835, 2])
We keep 3.96e+07/4.39e+08 =  9% of the original kernel matrix.

torch.Size([36273, 2])
We keep 6.76e+06/5.66e+08 =  1% of the original kernel matrix.

torch.Size([22072, 2])
We keep 1.67e+07/2.67e+08 =  6% of the original kernel matrix.

torch.Size([32846, 2])
We keep 5.83e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([39724, 2])
We keep 8.03e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([46604, 2])
We keep 7.76e+06/6.26e+08 =  1% of the original kernel matrix.

torch.Size([238570, 2])
We keep 6.90e+08/3.28e+10 =  2% of the original kernel matrix.

torch.Size([110786, 2])
We keep 4.18e+07/4.89e+09 =  0% of the original kernel matrix.

torch.Size([25854, 2])
We keep 5.77e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([36391, 2])
We keep 5.45e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([257799, 2])
We keep 6.23e+08/3.79e+10 =  1% of the original kernel matrix.

torch.Size([112643, 2])
We keep 4.50e+07/5.26e+09 =  0% of the original kernel matrix.

torch.Size([9774, 2])
We keep 1.11e+06/2.55e+07 =  4% of the original kernel matrix.

torch.Size([22633, 2])
We keep 2.40e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([44184, 2])
We keep 3.87e+07/1.62e+09 =  2% of the original kernel matrix.

torch.Size([43456, 2])
We keep 1.19e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([23244, 2])
We keep 2.69e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([34572, 2])
We keep 4.46e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([171640, 2])
We keep 4.56e+08/1.99e+10 =  2% of the original kernel matrix.

torch.Size([89631, 2])
We keep 3.47e+07/3.81e+09 =  0% of the original kernel matrix.

torch.Size([54521, 2])
We keep 1.51e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([52593, 2])
We keep 1.10e+07/9.83e+08 =  1% of the original kernel matrix.

torch.Size([16426, 2])
We keep 1.42e+06/5.65e+07 =  2% of the original kernel matrix.

torch.Size([28905, 2])
We keep 3.19e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([209900, 2])
We keep 1.10e+08/1.40e+10 =  0% of the original kernel matrix.

torch.Size([104496, 2])
We keep 2.95e+07/3.20e+09 =  0% of the original kernel matrix.

torch.Size([238259, 2])
We keep 1.46e+08/1.79e+10 =  0% of the original kernel matrix.

torch.Size([111995, 2])
We keep 3.27e+07/3.61e+09 =  0% of the original kernel matrix.

torch.Size([6873, 2])
We keep 6.38e+05/9.14e+06 =  6% of the original kernel matrix.

torch.Size([19469, 2])
We keep 1.66e+06/8.17e+07 =  2% of the original kernel matrix.

torch.Size([210810, 2])
We keep 2.99e+08/1.61e+10 =  1% of the original kernel matrix.

torch.Size([104902, 2])
We keep 3.16e+07/3.43e+09 =  0% of the original kernel matrix.

torch.Size([220699, 2])
We keep 1.49e+08/1.93e+10 =  0% of the original kernel matrix.

torch.Size([107981, 2])
We keep 3.40e+07/3.75e+09 =  0% of the original kernel matrix.

torch.Size([620025, 2])
We keep 7.90e+08/1.16e+11 =  0% of the original kernel matrix.

torch.Size([183897, 2])
We keep 7.53e+07/9.22e+09 =  0% of the original kernel matrix.

torch.Size([45254, 2])
We keep 3.96e+07/8.10e+08 =  4% of the original kernel matrix.

torch.Size([48361, 2])
We keep 9.05e+06/7.69e+08 =  1% of the original kernel matrix.

torch.Size([3288, 2])
We keep 1.30e+05/1.72e+06 =  7% of the original kernel matrix.

torch.Size([14472, 2])
We keep 9.17e+05/3.54e+07 =  2% of the original kernel matrix.

torch.Size([18328, 2])
We keep 1.73e+06/7.39e+07 =  2% of the original kernel matrix.

torch.Size([30369, 2])
We keep 3.54e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([16115, 2])
We keep 7.72e+06/1.56e+08 =  4% of the original kernel matrix.

torch.Size([28249, 2])
We keep 4.59e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([34710, 2])
We keep 7.18e+06/3.53e+08 =  2% of the original kernel matrix.

torch.Size([42568, 2])
We keep 6.47e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([60504, 2])
We keep 4.69e+07/1.38e+09 =  3% of the original kernel matrix.

torch.Size([55548, 2])
We keep 1.12e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([4249, 2])
We keep 1.26e+05/2.65e+06 =  4% of the original kernel matrix.

torch.Size([16181, 2])
We keep 1.07e+06/4.40e+07 =  2% of the original kernel matrix.

torch.Size([122551, 2])
We keep 6.67e+07/4.81e+09 =  1% of the original kernel matrix.

torch.Size([78101, 2])
We keep 1.86e+07/1.87e+09 =  0% of the original kernel matrix.

torch.Size([214514, 2])
We keep 1.47e+08/1.56e+10 =  0% of the original kernel matrix.

torch.Size([105754, 2])
We keep 3.10e+07/3.38e+09 =  0% of the original kernel matrix.

torch.Size([3850, 2])
We keep 5.42e+05/4.44e+06 = 12% of the original kernel matrix.

torch.Size([15089, 2])
We keep 1.21e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([11642, 2])
We keep 1.05e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([24250, 2])
We keep 2.61e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([46968, 2])
We keep 1.17e+07/8.06e+08 =  1% of the original kernel matrix.

torch.Size([49266, 2])
We keep 8.99e+06/7.66e+08 =  1% of the original kernel matrix.

torch.Size([44575, 2])
We keep 8.42e+06/6.31e+08 =  1% of the original kernel matrix.

torch.Size([49395, 2])
We keep 8.15e+06/6.78e+08 =  1% of the original kernel matrix.

torch.Size([30489, 2])
We keep 7.35e+06/3.03e+08 =  2% of the original kernel matrix.

torch.Size([39935, 2])
We keep 6.04e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([22080, 2])
We keep 2.61e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([33645, 2])
We keep 4.25e+06/2.99e+08 =  1% of the original kernel matrix.

torch.Size([8779, 2])
We keep 4.37e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([21762, 2])
We keep 1.84e+06/9.56e+07 =  1% of the original kernel matrix.

torch.Size([22027, 2])
We keep 5.24e+06/1.69e+08 =  3% of the original kernel matrix.

torch.Size([33235, 2])
We keep 4.75e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([62425, 2])
We keep 1.36e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([57157, 2])
We keep 1.04e+07/9.31e+08 =  1% of the original kernel matrix.

torch.Size([26711, 2])
We keep 4.75e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([37274, 2])
We keep 5.36e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([62755, 2])
We keep 3.76e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([55996, 2])
We keep 1.24e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([13383, 2])
We keep 1.40e+06/4.51e+07 =  3% of the original kernel matrix.

torch.Size([25928, 2])
We keep 2.98e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([1180717, 2])
We keep 6.00e+09/4.49e+11 =  1% of the original kernel matrix.

torch.Size([259240, 2])
We keep 1.37e+08/1.81e+10 =  0% of the original kernel matrix.

torch.Size([19409, 2])
We keep 2.17e+06/9.69e+07 =  2% of the original kernel matrix.

torch.Size([31440, 2])
We keep 3.91e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([154824, 2])
We keep 8.02e+07/7.74e+09 =  1% of the original kernel matrix.

torch.Size([88690, 2])
We keep 2.28e+07/2.38e+09 =  0% of the original kernel matrix.

torch.Size([2285, 2])
We keep 4.84e+04/7.06e+05 =  6% of the original kernel matrix.

torch.Size([12723, 2])
We keep 7.03e+05/2.27e+07 =  3% of the original kernel matrix.

torch.Size([7084, 2])
We keep 3.23e+05/7.65e+06 =  4% of the original kernel matrix.

torch.Size([19709, 2])
We keep 1.54e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([10484, 2])
We keep 6.35e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([23397, 2])
We keep 2.18e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([22476, 2])
We keep 3.30e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([34017, 2])
We keep 4.12e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([15449, 2])
We keep 1.31e+06/4.90e+07 =  2% of the original kernel matrix.

torch.Size([27988, 2])
We keep 3.04e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([129481, 2])
We keep 1.85e+08/7.93e+09 =  2% of the original kernel matrix.

torch.Size([79878, 2])
We keep 2.31e+07/2.41e+09 =  0% of the original kernel matrix.

torch.Size([11192, 2])
We keep 6.95e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([24025, 2])
We keep 2.25e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([8594, 2])
We keep 4.68e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([21385, 2])
We keep 1.85e+06/9.62e+07 =  1% of the original kernel matrix.

torch.Size([568281, 2])
We keep 8.84e+08/1.09e+11 =  0% of the original kernel matrix.

torch.Size([174078, 2])
We keep 7.25e+07/8.90e+09 =  0% of the original kernel matrix.

torch.Size([79125, 2])
We keep 4.03e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([62626, 2])
We keep 1.40e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([68500, 2])
We keep 3.05e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([58779, 2])
We keep 1.23e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([176076, 2])
We keep 1.55e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([95577, 2])
We keep 2.83e+07/3.01e+09 =  0% of the original kernel matrix.

torch.Size([374583, 2])
We keep 5.54e+08/6.34e+10 =  0% of the original kernel matrix.

torch.Size([138337, 2])
We keep 5.78e+07/6.80e+09 =  0% of the original kernel matrix.

torch.Size([8179, 2])
We keep 3.86e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([20793, 2])
We keep 1.77e+06/9.00e+07 =  1% of the original kernel matrix.

torch.Size([12565, 2])
We keep 9.06e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([25313, 2])
We keep 2.47e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([24416, 2])
We keep 9.51e+06/3.13e+08 =  3% of the original kernel matrix.

torch.Size([34617, 2])
We keep 6.27e+06/4.78e+08 =  1% of the original kernel matrix.

torch.Size([2746, 2])
We keep 8.18e+04/1.22e+06 =  6% of the original kernel matrix.

torch.Size([13519, 2])
We keep 8.44e+05/2.98e+07 =  2% of the original kernel matrix.

torch.Size([11132, 2])
We keep 7.60e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([24029, 2])
We keep 2.30e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([20203, 2])
We keep 3.55e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([32014, 2])
We keep 4.49e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([5513, 2])
We keep 2.29e+05/4.86e+06 =  4% of the original kernel matrix.

torch.Size([17927, 2])
We keep 1.33e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([13564, 2])
We keep 1.09e+06/3.82e+07 =  2% of the original kernel matrix.

torch.Size([26025, 2])
We keep 2.75e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([12075, 2])
We keep 7.87e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([24876, 2])
We keep 2.39e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([13520, 2])
We keep 1.16e+06/3.99e+07 =  2% of the original kernel matrix.

torch.Size([26207, 2])
We keep 2.78e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([297720, 2])
We keep 5.42e+08/4.11e+10 =  1% of the original kernel matrix.

torch.Size([125190, 2])
We keep 4.76e+07/5.47e+09 =  0% of the original kernel matrix.

torch.Size([24128, 2])
We keep 5.68e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([34710, 2])
We keep 5.69e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([107516, 2])
We keep 7.39e+07/4.64e+09 =  1% of the original kernel matrix.

torch.Size([72661, 2])
We keep 1.83e+07/1.84e+09 =  0% of the original kernel matrix.

torch.Size([6033, 2])
We keep 6.97e+05/8.07e+06 =  8% of the original kernel matrix.

torch.Size([18326, 2])
We keep 1.50e+06/7.67e+07 =  1% of the original kernel matrix.

torch.Size([8979, 2])
We keep 5.38e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([21762, 2])
We keep 2.00e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([81940, 2])
We keep 5.69e+07/3.45e+09 =  1% of the original kernel matrix.

torch.Size([61698, 2])
We keep 1.65e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([888348, 2])
We keep 3.10e+09/3.00e+11 =  1% of the original kernel matrix.

torch.Size([224301, 2])
We keep 1.15e+08/1.48e+10 =  0% of the original kernel matrix.

torch.Size([934214, 2])
We keep 1.28e+09/2.45e+11 =  0% of the original kernel matrix.

torch.Size([230171, 2])
We keep 1.04e+08/1.34e+10 =  0% of the original kernel matrix.

torch.Size([21686, 2])
We keep 2.65e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([33288, 2])
We keep 4.12e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([101049, 2])
We keep 1.26e+08/3.55e+09 =  3% of the original kernel matrix.

torch.Size([70995, 2])
We keep 1.67e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([5175, 2])
We keep 1.80e+05/3.90e+06 =  4% of the original kernel matrix.

torch.Size([17540, 2])
We keep 1.23e+06/5.33e+07 =  2% of the original kernel matrix.

torch.Size([7400, 2])
We keep 1.02e+06/1.49e+07 =  6% of the original kernel matrix.

torch.Size([19969, 2])
We keep 1.95e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([152802, 2])
We keep 1.20e+09/2.72e+10 =  4% of the original kernel matrix.

torch.Size([79053, 2])
We keep 3.94e+07/4.45e+09 =  0% of the original kernel matrix.

torch.Size([24605, 2])
We keep 7.79e+06/1.87e+08 =  4% of the original kernel matrix.

torch.Size([35764, 2])
We keep 4.86e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([19088, 2])
We keep 7.70e+06/1.14e+08 =  6% of the original kernel matrix.

torch.Size([30914, 2])
We keep 4.21e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([73080, 2])
We keep 2.86e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([60285, 2])
We keep 1.29e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([1165679, 2])
We keep 4.20e+09/4.87e+11 =  0% of the original kernel matrix.

torch.Size([252878, 2])
We keep 1.44e+08/1.89e+10 =  0% of the original kernel matrix.

torch.Size([60449, 2])
We keep 3.15e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([53418, 2])
We keep 1.20e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([12309, 2])
We keep 2.34e+06/6.06e+07 =  3% of the original kernel matrix.

torch.Size([24073, 2])
We keep 3.27e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([62663, 2])
We keep 1.58e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([58505, 2])
We keep 1.07e+07/9.45e+08 =  1% of the original kernel matrix.

torch.Size([16085, 2])
We keep 1.36e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([28489, 2])
We keep 3.24e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([4710, 2])
We keep 2.04e+05/3.28e+06 =  6% of the original kernel matrix.

torch.Size([16816, 2])
We keep 1.16e+06/4.89e+07 =  2% of the original kernel matrix.

torch.Size([246914, 2])
We keep 3.82e+08/2.35e+10 =  1% of the original kernel matrix.

torch.Size([114217, 2])
We keep 3.68e+07/4.14e+09 =  0% of the original kernel matrix.

torch.Size([21648, 2])
We keep 7.89e+06/2.09e+08 =  3% of the original kernel matrix.

torch.Size([32637, 2])
We keep 5.29e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([3412, 2])
We keep 1.11e+05/1.60e+06 =  6% of the original kernel matrix.

torch.Size([14843, 2])
We keep 9.30e+05/3.41e+07 =  2% of the original kernel matrix.

torch.Size([108403, 2])
We keep 4.43e+07/3.53e+09 =  1% of the original kernel matrix.

torch.Size([73290, 2])
We keep 1.65e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([27578, 2])
We keep 4.56e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([38066, 2])
We keep 5.58e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([1701, 2])
We keep 2.38e+04/2.97e+05 =  8% of the original kernel matrix.

torch.Size([11620, 2])
We keep 5.28e+05/1.47e+07 =  3% of the original kernel matrix.

torch.Size([122646, 2])
We keep 1.05e+08/5.77e+09 =  1% of the original kernel matrix.

torch.Size([77716, 2])
We keep 2.02e+07/2.05e+09 =  0% of the original kernel matrix.

torch.Size([211817, 2])
We keep 1.41e+08/1.57e+10 =  0% of the original kernel matrix.

torch.Size([104827, 2])
We keep 3.07e+07/3.38e+09 =  0% of the original kernel matrix.

torch.Size([181937, 2])
We keep 4.06e+08/1.95e+10 =  2% of the original kernel matrix.

torch.Size([95381, 2])
We keep 3.44e+07/3.77e+09 =  0% of the original kernel matrix.

torch.Size([232942, 2])
We keep 4.49e+08/2.45e+10 =  1% of the original kernel matrix.

torch.Size([109556, 2])
We keep 3.82e+07/4.23e+09 =  0% of the original kernel matrix.

torch.Size([256579, 2])
We keep 3.62e+08/2.80e+10 =  1% of the original kernel matrix.

torch.Size([115424, 2])
We keep 3.99e+07/4.52e+09 =  0% of the original kernel matrix.

torch.Size([26343, 2])
We keep 1.01e+07/3.86e+08 =  2% of the original kernel matrix.

torch.Size([35851, 2])
We keep 6.69e+06/5.31e+08 =  1% of the original kernel matrix.

torch.Size([20977, 2])
We keep 4.22e+06/1.27e+08 =  3% of the original kernel matrix.

torch.Size([32646, 2])
We keep 4.18e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([184282, 2])
We keep 1.02e+08/1.04e+10 =  0% of the original kernel matrix.

torch.Size([97681, 2])
We keep 2.60e+07/2.75e+09 =  0% of the original kernel matrix.

torch.Size([270329, 2])
We keep 1.95e+08/2.40e+10 =  0% of the original kernel matrix.

torch.Size([120266, 2])
We keep 3.74e+07/4.18e+09 =  0% of the original kernel matrix.

torch.Size([17963, 2])
We keep 1.75e+06/7.44e+07 =  2% of the original kernel matrix.

torch.Size([30109, 2])
We keep 3.48e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([75855, 2])
We keep 8.82e+07/2.79e+09 =  3% of the original kernel matrix.

torch.Size([61871, 2])
We keep 1.45e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([229219, 2])
We keep 6.85e+08/2.78e+10 =  2% of the original kernel matrix.

torch.Size([108906, 2])
We keep 3.85e+07/4.50e+09 =  0% of the original kernel matrix.

torch.Size([264248, 2])
We keep 4.12e+08/2.99e+10 =  1% of the original kernel matrix.

torch.Size([118003, 2])
We keep 4.11e+07/4.67e+09 =  0% of the original kernel matrix.

torch.Size([118987, 2])
We keep 1.34e+08/7.71e+09 =  1% of the original kernel matrix.

torch.Size([75888, 2])
We keep 2.30e+07/2.37e+09 =  0% of the original kernel matrix.

torch.Size([42164, 2])
We keep 8.68e+07/2.16e+09 =  4% of the original kernel matrix.

torch.Size([41083, 2])
We keep 1.35e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([22398, 2])
We keep 5.02e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([33601, 2])
We keep 4.96e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([12523, 2])
We keep 8.00e+05/2.81e+07 =  2% of the original kernel matrix.

torch.Size([25410, 2])
We keep 2.45e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([19642, 2])
We keep 8.86e+06/1.54e+08 =  5% of the original kernel matrix.

torch.Size([31307, 2])
We keep 4.68e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([218553, 2])
We keep 1.27e+08/1.56e+10 =  0% of the original kernel matrix.

torch.Size([106861, 2])
We keep 3.10e+07/3.38e+09 =  0% of the original kernel matrix.

torch.Size([173021, 2])
We keep 8.55e+07/9.48e+09 =  0% of the original kernel matrix.

torch.Size([94309, 2])
We keep 2.50e+07/2.63e+09 =  0% of the original kernel matrix.

torch.Size([634405, 2])
We keep 1.73e+09/1.77e+11 =  0% of the original kernel matrix.

torch.Size([182303, 2])
We keep 9.00e+07/1.14e+10 =  0% of the original kernel matrix.

torch.Size([155424, 2])
We keep 1.72e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([88908, 2])
We keep 2.61e+07/2.74e+09 =  0% of the original kernel matrix.

torch.Size([111199, 2])
We keep 1.63e+08/4.65e+09 =  3% of the original kernel matrix.

torch.Size([74594, 2])
We keep 1.81e+07/1.84e+09 =  0% of the original kernel matrix.

torch.Size([21729, 2])
We keep 3.54e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([32883, 2])
We keep 5.18e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([243928, 2])
We keep 2.17e+08/1.99e+10 =  1% of the original kernel matrix.

torch.Size([113267, 2])
We keep 3.45e+07/3.81e+09 =  0% of the original kernel matrix.

torch.Size([8978, 2])
We keep 1.01e+06/2.11e+07 =  4% of the original kernel matrix.

torch.Size([21181, 2])
We keep 2.26e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([18484, 2])
We keep 9.08e+06/1.19e+08 =  7% of the original kernel matrix.

torch.Size([30272, 2])
We keep 4.08e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([257678, 2])
We keep 2.40e+08/2.22e+10 =  1% of the original kernel matrix.

torch.Size([116894, 2])
We keep 3.61e+07/4.02e+09 =  0% of the original kernel matrix.

torch.Size([81380, 2])
We keep 3.68e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([63802, 2])
We keep 1.37e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([11400, 2])
We keep 8.44e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([24163, 2])
We keep 2.42e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([26747, 2])
We keep 4.01e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([37119, 2])
We keep 5.41e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([353788, 2])
We keep 1.46e+09/8.48e+10 =  1% of the original kernel matrix.

torch.Size([132142, 2])
We keep 6.58e+07/7.87e+09 =  0% of the original kernel matrix.

torch.Size([31683, 2])
We keep 1.15e+07/3.46e+08 =  3% of the original kernel matrix.

torch.Size([40113, 2])
We keep 6.35e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([134700, 2])
We keep 1.49e+08/8.54e+09 =  1% of the original kernel matrix.

torch.Size([82219, 2])
We keep 2.40e+07/2.50e+09 =  0% of the original kernel matrix.

torch.Size([4112, 2])
We keep 1.65e+05/2.85e+06 =  5% of the original kernel matrix.

torch.Size([15827, 2])
We keep 1.09e+06/4.56e+07 =  2% of the original kernel matrix.

torch.Size([5238, 2])
We keep 2.09e+05/4.27e+06 =  4% of the original kernel matrix.

torch.Size([17523, 2])
We keep 1.28e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([10102, 2])
We keep 7.01e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([22851, 2])
We keep 2.24e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([14898, 2])
We keep 1.70e+06/5.17e+07 =  3% of the original kernel matrix.

torch.Size([27500, 2])
We keep 3.09e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([74992, 2])
We keep 4.29e+07/1.76e+09 =  2% of the original kernel matrix.

torch.Size([61898, 2])
We keep 1.24e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([79714, 2])
We keep 5.22e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([61463, 2])
We keep 1.50e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([8668, 2])
We keep 1.19e+06/1.63e+07 =  7% of the original kernel matrix.

torch.Size([21420, 2])
We keep 1.97e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([232385, 2])
We keep 2.72e+08/1.92e+10 =  1% of the original kernel matrix.

torch.Size([109737, 2])
We keep 3.38e+07/3.75e+09 =  0% of the original kernel matrix.

torch.Size([9214, 2])
We keep 9.80e+05/2.30e+07 =  4% of the original kernel matrix.

torch.Size([21883, 2])
We keep 2.33e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([73334, 2])
We keep 4.73e+07/1.86e+09 =  2% of the original kernel matrix.

torch.Size([61436, 2])
We keep 1.25e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([13193, 2])
We keep 3.09e+06/6.65e+07 =  4% of the original kernel matrix.

torch.Size([25757, 2])
We keep 3.37e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([99576, 2])
We keep 4.25e+07/3.01e+09 =  1% of the original kernel matrix.

torch.Size([70046, 2])
We keep 1.54e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([15827, 2])
We keep 1.70e+06/5.94e+07 =  2% of the original kernel matrix.

torch.Size([28287, 2])
We keep 3.29e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([31188, 2])
We keep 7.26e+06/3.42e+08 =  2% of the original kernel matrix.

torch.Size([40010, 2])
We keep 6.31e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([39655, 2])
We keep 1.00e+07/5.52e+08 =  1% of the original kernel matrix.

torch.Size([45824, 2])
We keep 7.77e+06/6.35e+08 =  1% of the original kernel matrix.

torch.Size([45372, 2])
We keep 2.56e+07/9.46e+08 =  2% of the original kernel matrix.

torch.Size([48101, 2])
We keep 9.75e+06/8.31e+08 =  1% of the original kernel matrix.

torch.Size([120805, 2])
We keep 6.30e+07/5.07e+09 =  1% of the original kernel matrix.

torch.Size([78115, 2])
We keep 1.92e+07/1.92e+09 =  0% of the original kernel matrix.

torch.Size([558935, 2])
We keep 1.80e+09/1.34e+11 =  1% of the original kernel matrix.

torch.Size([171481, 2])
We keep 8.01e+07/9.88e+09 =  0% of the original kernel matrix.

torch.Size([13001, 2])
We keep 8.72e+05/3.02e+07 =  2% of the original kernel matrix.

torch.Size([25794, 2])
We keep 2.55e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([8889, 2])
We keep 2.39e+06/3.99e+07 =  5% of the original kernel matrix.

torch.Size([20841, 2])
We keep 2.85e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([126110, 2])
We keep 1.74e+08/7.06e+09 =  2% of the original kernel matrix.

torch.Size([78651, 2])
We keep 2.21e+07/2.27e+09 =  0% of the original kernel matrix.

torch.Size([23840, 2])
We keep 4.27e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([34716, 2])
We keep 5.02e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([827318, 2])
We keep 1.13e+09/1.93e+11 =  0% of the original kernel matrix.

torch.Size([214573, 2])
We keep 9.40e+07/1.19e+10 =  0% of the original kernel matrix.

torch.Size([6824, 2])
We keep 4.24e+05/9.03e+06 =  4% of the original kernel matrix.

torch.Size([19302, 2])
We keep 1.65e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([162009, 2])
We keep 1.15e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([90339, 2])
We keep 2.61e+07/2.78e+09 =  0% of the original kernel matrix.

torch.Size([502686, 2])
We keep 5.35e+08/8.27e+10 =  0% of the original kernel matrix.

torch.Size([165800, 2])
We keep 6.47e+07/7.77e+09 =  0% of the original kernel matrix.

torch.Size([5289, 2])
We keep 2.14e+05/4.82e+06 =  4% of the original kernel matrix.

torch.Size([17271, 2])
We keep 1.31e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([265833, 2])
We keep 7.00e+08/4.80e+10 =  1% of the original kernel matrix.

torch.Size([113398, 2])
We keep 5.04e+07/5.92e+09 =  0% of the original kernel matrix.

torch.Size([19784, 2])
We keep 7.14e+06/2.89e+08 =  2% of the original kernel matrix.

torch.Size([29711, 2])
We keep 5.97e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([897571, 2])
We keep 2.73e+09/3.25e+11 =  0% of the original kernel matrix.

torch.Size([224835, 2])
We keep 1.20e+08/1.54e+10 =  0% of the original kernel matrix.

torch.Size([16733, 2])
We keep 1.48e+06/5.94e+07 =  2% of the original kernel matrix.

torch.Size([28960, 2])
We keep 3.25e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([18140, 2])
We keep 2.63e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([30076, 2])
We keep 3.97e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([10128, 2])
We keep 2.82e+06/5.92e+07 =  4% of the original kernel matrix.

torch.Size([21714, 2])
We keep 3.24e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([10392, 2])
We keep 5.84e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([23261, 2])
We keep 2.11e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([15779, 2])
We keep 2.35e+06/8.68e+07 =  2% of the original kernel matrix.

torch.Size([27902, 2])
We keep 3.78e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([40130, 2])
We keep 1.21e+07/5.88e+08 =  2% of the original kernel matrix.

torch.Size([45917, 2])
We keep 7.93e+06/6.55e+08 =  1% of the original kernel matrix.

torch.Size([15913, 2])
We keep 1.01e+07/1.21e+08 =  8% of the original kernel matrix.

torch.Size([28146, 2])
We keep 4.30e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([171651, 2])
We keep 1.44e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([93885, 2])
We keep 2.71e+07/2.91e+09 =  0% of the original kernel matrix.

torch.Size([25527, 2])
We keep 3.98e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([36202, 2])
We keep 5.29e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([6998, 2])
We keep 3.57e+05/8.70e+06 =  4% of the original kernel matrix.

torch.Size([19666, 2])
We keep 1.63e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([21076, 2])
We keep 6.90e+06/2.53e+08 =  2% of the original kernel matrix.

torch.Size([31672, 2])
We keep 5.68e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([392089, 2])
We keep 9.27e+08/6.86e+10 =  1% of the original kernel matrix.

torch.Size([145350, 2])
We keep 6.02e+07/7.07e+09 =  0% of the original kernel matrix.

torch.Size([130404, 2])
We keep 2.39e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([78063, 2])
We keep 2.83e+07/2.98e+09 =  0% of the original kernel matrix.

torch.Size([576809, 2])
We keep 9.88e+08/1.00e+11 =  0% of the original kernel matrix.

torch.Size([176695, 2])
We keep 7.06e+07/8.55e+09 =  0% of the original kernel matrix.

torch.Size([29629, 2])
We keep 8.30e+06/2.55e+08 =  3% of the original kernel matrix.

torch.Size([39284, 2])
We keep 5.64e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([20701, 2])
We keep 3.78e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([32478, 2])
We keep 4.40e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([7165, 2])
We keep 3.36e+05/8.74e+06 =  3% of the original kernel matrix.

torch.Size([19915, 2])
We keep 1.63e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([11147, 2])
We keep 1.62e+06/3.61e+07 =  4% of the original kernel matrix.

torch.Size([23769, 2])
We keep 2.62e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([280647, 2])
We keep 5.88e+08/2.82e+10 =  2% of the original kernel matrix.

torch.Size([122904, 2])
We keep 3.93e+07/4.53e+09 =  0% of the original kernel matrix.

torch.Size([22941, 2])
We keep 3.18e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([34337, 2])
We keep 4.64e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([24944, 2])
We keep 4.78e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([36005, 2])
We keep 5.16e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([44912, 2])
We keep 1.94e+07/8.13e+08 =  2% of the original kernel matrix.

torch.Size([47803, 2])
We keep 9.09e+06/7.70e+08 =  1% of the original kernel matrix.

torch.Size([40261, 2])
We keep 1.74e+07/8.69e+08 =  2% of the original kernel matrix.

torch.Size([44289, 2])
We keep 9.36e+06/7.96e+08 =  1% of the original kernel matrix.

torch.Size([19784, 2])
We keep 7.96e+06/1.90e+08 =  4% of the original kernel matrix.

torch.Size([31093, 2])
We keep 5.04e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([8061, 2])
We keep 5.30e+05/1.36e+07 =  3% of the original kernel matrix.

torch.Size([20550, 2])
We keep 1.90e+06/9.96e+07 =  1% of the original kernel matrix.

torch.Size([422149, 2])
We keep 1.16e+09/8.76e+10 =  1% of the original kernel matrix.

torch.Size([147599, 2])
We keep 6.64e+07/7.99e+09 =  0% of the original kernel matrix.

torch.Size([65423, 2])
We keep 3.18e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([57427, 2])
We keep 1.21e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([32478, 2])
We keep 8.96e+07/1.21e+09 =  7% of the original kernel matrix.

torch.Size([39525, 2])
We keep 1.08e+07/9.41e+08 =  1% of the original kernel matrix.

torch.Size([27798, 2])
We keep 5.92e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([38080, 2])
We keep 5.33e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([172612, 2])
We keep 4.50e+08/1.45e+10 =  3% of the original kernel matrix.

torch.Size([93875, 2])
We keep 3.03e+07/3.25e+09 =  0% of the original kernel matrix.

torch.Size([254711, 2])
We keep 4.17e+08/2.50e+10 =  1% of the original kernel matrix.

torch.Size([115965, 2])
We keep 3.83e+07/4.27e+09 =  0% of the original kernel matrix.

torch.Size([766356, 2])
We keep 1.95e+09/2.23e+11 =  0% of the original kernel matrix.

torch.Size([203757, 2])
We keep 1.01e+08/1.27e+10 =  0% of the original kernel matrix.

torch.Size([235171, 2])
We keep 2.81e+08/1.90e+10 =  1% of the original kernel matrix.

torch.Size([111173, 2])
We keep 3.37e+07/3.72e+09 =  0% of the original kernel matrix.

torch.Size([14375, 2])
We keep 1.14e+07/9.55e+07 = 11% of the original kernel matrix.

torch.Size([26806, 2])
We keep 3.87e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([60822, 2])
We keep 1.44e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([55921, 2])
We keep 1.07e+07/9.63e+08 =  1% of the original kernel matrix.

torch.Size([51285, 2])
We keep 2.73e+07/1.16e+09 =  2% of the original kernel matrix.

torch.Size([50681, 2])
We keep 1.05e+07/9.21e+08 =  1% of the original kernel matrix.

torch.Size([10651, 2])
We keep 7.28e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([23502, 2])
We keep 2.20e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([20037, 2])
We keep 2.79e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([31907, 2])
We keep 4.20e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([13496, 2])
We keep 9.61e+05/3.31e+07 =  2% of the original kernel matrix.

torch.Size([26337, 2])
We keep 2.59e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([18641, 2])
We keep 1.76e+07/2.47e+08 =  7% of the original kernel matrix.

torch.Size([30199, 2])
We keep 5.61e+06/4.24e+08 =  1% of the original kernel matrix.

torch.Size([44500, 2])
We keep 1.65e+07/8.35e+08 =  1% of the original kernel matrix.

torch.Size([47547, 2])
We keep 9.05e+06/7.80e+08 =  1% of the original kernel matrix.

torch.Size([5995, 2])
We keep 8.57e+05/9.23e+06 =  9% of the original kernel matrix.

torch.Size([17863, 2])
We keep 1.67e+06/8.20e+07 =  2% of the original kernel matrix.

torch.Size([26092, 2])
We keep 4.53e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([36723, 2])
We keep 5.20e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([9421, 2])
We keep 1.08e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([22013, 2])
We keep 2.37e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([33822, 2])
We keep 6.79e+07/1.22e+09 =  5% of the original kernel matrix.

torch.Size([39251, 2])
We keep 1.06e+07/9.43e+08 =  1% of the original kernel matrix.

torch.Size([14989, 2])
We keep 1.84e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([27499, 2])
We keep 3.37e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([42164, 2])
We keep 1.34e+07/5.97e+08 =  2% of the original kernel matrix.

torch.Size([47396, 2])
We keep 7.92e+06/6.60e+08 =  1% of the original kernel matrix.

torch.Size([12291, 2])
We keep 2.40e+07/1.43e+08 = 16% of the original kernel matrix.

torch.Size([23642, 2])
We keep 4.11e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([27292, 2])
We keep 5.19e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([37605, 2])
We keep 5.37e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([10432, 2])
We keep 9.65e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([23087, 2])
We keep 2.35e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([16538, 2])
We keep 1.65e+06/6.29e+07 =  2% of the original kernel matrix.

torch.Size([28921, 2])
We keep 3.34e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([35690, 2])
We keep 7.49e+06/3.59e+08 =  2% of the original kernel matrix.

torch.Size([43703, 2])
We keep 6.39e+06/5.12e+08 =  1% of the original kernel matrix.

torch.Size([192908, 2])
We keep 2.06e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([99932, 2])
We keep 2.93e+07/3.22e+09 =  0% of the original kernel matrix.

torch.Size([12967, 2])
We keep 1.10e+06/3.52e+07 =  3% of the original kernel matrix.

torch.Size([25655, 2])
We keep 2.70e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([3747, 2])
We keep 1.03e+05/1.95e+06 =  5% of the original kernel matrix.

torch.Size([15343, 2])
We keep 9.74e+05/3.77e+07 =  2% of the original kernel matrix.

torch.Size([10445, 2])
We keep 7.68e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([23215, 2])
We keep 2.30e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([231761, 2])
We keep 6.30e+08/2.58e+10 =  2% of the original kernel matrix.

torch.Size([110139, 2])
We keep 3.89e+07/4.34e+09 =  0% of the original kernel matrix.

torch.Size([703766, 2])
We keep 2.20e+09/1.67e+11 =  1% of the original kernel matrix.

torch.Size([196930, 2])
We keep 8.70e+07/1.10e+10 =  0% of the original kernel matrix.

torch.Size([66110, 2])
We keep 5.95e+07/2.84e+09 =  2% of the original kernel matrix.

torch.Size([54070, 2])
We keep 1.50e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([15043, 2])
We keep 1.94e+06/6.38e+07 =  3% of the original kernel matrix.

torch.Size([27357, 2])
We keep 3.41e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([195014, 2])
We keep 3.34e+08/2.34e+10 =  1% of the original kernel matrix.

torch.Size([97460, 2])
We keep 3.71e+07/4.13e+09 =  0% of the original kernel matrix.

torch.Size([241644, 2])
We keep 1.57e+08/1.96e+10 =  0% of the original kernel matrix.

torch.Size([113266, 2])
We keep 3.44e+07/3.78e+09 =  0% of the original kernel matrix.

torch.Size([5207, 2])
We keep 1.76e+05/3.76e+06 =  4% of the original kernel matrix.

torch.Size([17430, 2])
We keep 1.21e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([13276, 2])
We keep 1.77e+06/4.78e+07 =  3% of the original kernel matrix.

torch.Size([25975, 2])
We keep 2.95e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([48402, 2])
We keep 8.69e+06/7.21e+08 =  1% of the original kernel matrix.

torch.Size([52011, 2])
We keep 8.68e+06/7.25e+08 =  1% of the original kernel matrix.

torch.Size([89639, 2])
We keep 2.31e+08/7.76e+09 =  2% of the original kernel matrix.

torch.Size([64186, 2])
We keep 2.30e+07/2.38e+09 =  0% of the original kernel matrix.

torch.Size([76267, 2])
We keep 2.96e+08/4.23e+09 =  7% of the original kernel matrix.

torch.Size([60666, 2])
We keep 1.76e+07/1.76e+09 =  0% of the original kernel matrix.

torch.Size([308741, 2])
We keep 3.02e+08/3.38e+10 =  0% of the original kernel matrix.

torch.Size([128947, 2])
We keep 4.35e+07/4.96e+09 =  0% of the original kernel matrix.

torch.Size([22661, 2])
We keep 2.43e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([34144, 2])
We keep 4.18e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([6569, 2])
We keep 3.01e+05/7.30e+06 =  4% of the original kernel matrix.

torch.Size([19144, 2])
We keep 1.52e+06/7.30e+07 =  2% of the original kernel matrix.

torch.Size([8608, 2])
We keep 4.54e+05/1.24e+07 =  3% of the original kernel matrix.

torch.Size([21349, 2])
We keep 1.85e+06/9.53e+07 =  1% of the original kernel matrix.

torch.Size([53383, 2])
We keep 3.58e+07/1.15e+09 =  3% of the original kernel matrix.

torch.Size([51838, 2])
We keep 1.03e+07/9.16e+08 =  1% of the original kernel matrix.

torch.Size([20643, 2])
We keep 2.35e+06/9.78e+07 =  2% of the original kernel matrix.

torch.Size([32504, 2])
We keep 3.94e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([27936, 2])
We keep 7.49e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([37399, 2])
We keep 6.25e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([249024, 2])
We keep 5.02e+08/2.39e+10 =  2% of the original kernel matrix.

torch.Size([114548, 2])
We keep 3.73e+07/4.18e+09 =  0% of the original kernel matrix.

torch.Size([946232, 2])
We keep 1.63e+09/2.72e+11 =  0% of the original kernel matrix.

torch.Size([232719, 2])
We keep 1.11e+08/1.41e+10 =  0% of the original kernel matrix.

torch.Size([17944, 2])
We keep 2.31e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([29996, 2])
We keep 3.70e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([4447, 2])
We keep 1.42e+05/2.81e+06 =  5% of the original kernel matrix.

torch.Size([16327, 2])
We keep 1.10e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([31262, 2])
We keep 5.25e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([40669, 2])
We keep 6.29e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([517998, 2])
We keep 6.79e+09/1.72e+11 =  3% of the original kernel matrix.

torch.Size([167478, 2])
We keep 8.62e+07/1.12e+10 =  0% of the original kernel matrix.

torch.Size([72448, 2])
We keep 9.32e+07/1.75e+09 =  5% of the original kernel matrix.

torch.Size([60505, 2])
We keep 1.20e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([6229, 2])
We keep 2.32e+06/1.22e+07 = 18% of the original kernel matrix.

torch.Size([18204, 2])
We keep 1.76e+06/9.45e+07 =  1% of the original kernel matrix.

torch.Size([6110, 2])
We keep 3.64e+05/7.30e+06 =  4% of the original kernel matrix.

torch.Size([18274, 2])
We keep 1.53e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([119994, 2])
We keep 3.25e+08/5.97e+09 =  5% of the original kernel matrix.

torch.Size([77398, 2])
We keep 2.06e+07/2.09e+09 =  0% of the original kernel matrix.

torch.Size([5222, 2])
We keep 2.59e+05/4.67e+06 =  5% of the original kernel matrix.

torch.Size([17429, 2])
We keep 1.32e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([55164, 2])
We keep 5.36e+07/2.42e+09 =  2% of the original kernel matrix.

torch.Size([49307, 2])
We keep 1.40e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([10439, 2])
We keep 7.30e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([23191, 2])
We keep 2.22e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([23643, 2])
We keep 2.41e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([34898, 2])
We keep 4.39e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([14474, 2])
We keep 1.66e+06/5.59e+07 =  2% of the original kernel matrix.

torch.Size([26965, 2])
We keep 3.18e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([12565, 2])
We keep 7.76e+05/2.73e+07 =  2% of the original kernel matrix.

torch.Size([25472, 2])
We keep 2.45e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([6789, 2])
We keep 3.05e+05/7.27e+06 =  4% of the original kernel matrix.

torch.Size([19324, 2])
We keep 1.53e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([250350, 2])
We keep 1.97e+08/2.26e+10 =  0% of the original kernel matrix.

torch.Size([115231, 2])
We keep 3.62e+07/4.06e+09 =  0% of the original kernel matrix.

torch.Size([559843, 2])
We keep 8.58e+08/9.71e+10 =  0% of the original kernel matrix.

torch.Size([173745, 2])
We keep 6.96e+07/8.42e+09 =  0% of the original kernel matrix.

torch.Size([183919, 2])
We keep 2.83e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([97100, 2])
We keep 3.05e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([147269, 2])
We keep 2.38e+08/8.54e+09 =  2% of the original kernel matrix.

torch.Size([85404, 2])
We keep 2.40e+07/2.50e+09 =  0% of the original kernel matrix.

torch.Size([39607, 2])
We keep 7.22e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([45786, 2])
We keep 7.19e+06/5.86e+08 =  1% of the original kernel matrix.

torch.Size([12318, 2])
We keep 1.98e+06/4.59e+07 =  4% of the original kernel matrix.

torch.Size([25082, 2])
We keep 2.95e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([21544, 2])
We keep 2.95e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([33155, 2])
We keep 4.33e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([201327, 2])
We keep 1.21e+08/1.30e+10 =  0% of the original kernel matrix.

torch.Size([102544, 2])
We keep 2.86e+07/3.07e+09 =  0% of the original kernel matrix.

torch.Size([79673, 2])
We keep 4.56e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([64081, 2])
We keep 1.39e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([123086, 2])
We keep 2.28e+08/7.58e+09 =  3% of the original kernel matrix.

torch.Size([78297, 2])
We keep 2.23e+07/2.35e+09 =  0% of the original kernel matrix.

torch.Size([14931, 2])
We keep 2.26e+06/6.80e+07 =  3% of the original kernel matrix.

torch.Size([27167, 2])
We keep 3.47e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([1773172, 2])
We keep 6.54e+09/9.45e+11 =  0% of the original kernel matrix.

torch.Size([319052, 2])
We keep 1.96e+08/2.63e+10 =  0% of the original kernel matrix.

torch.Size([29364, 2])
We keep 4.99e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([39077, 2])
We keep 5.49e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([94718, 2])
We keep 5.03e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([68755, 2])
We keep 1.65e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([199602, 2])
We keep 1.76e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([102441, 2])
We keep 3.17e+07/3.45e+09 =  0% of the original kernel matrix.

torch.Size([68796, 2])
We keep 1.74e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([59864, 2])
We keep 1.11e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([136179, 2])
We keep 3.85e+08/1.01e+10 =  3% of the original kernel matrix.

torch.Size([82091, 2])
We keep 2.49e+07/2.72e+09 =  0% of the original kernel matrix.

torch.Size([772290, 2])
We keep 2.52e+09/2.54e+11 =  0% of the original kernel matrix.

torch.Size([205752, 2])
We keep 1.07e+08/1.36e+10 =  0% of the original kernel matrix.

torch.Size([191279, 2])
We keep 1.02e+08/1.24e+10 =  0% of the original kernel matrix.

torch.Size([99323, 2])
We keep 2.81e+07/3.00e+09 =  0% of the original kernel matrix.

torch.Size([108478, 2])
We keep 7.75e+07/4.43e+09 =  1% of the original kernel matrix.

torch.Size([72799, 2])
We keep 1.81e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([15112, 2])
We keep 2.02e+06/5.03e+07 =  4% of the original kernel matrix.

torch.Size([27708, 2])
We keep 2.96e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([64011, 2])
We keep 3.11e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([56815, 2])
We keep 1.16e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([17322, 2])
We keep 4.33e+06/9.37e+07 =  4% of the original kernel matrix.

torch.Size([29651, 2])
We keep 3.79e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([306244, 2])
We keep 5.18e+08/3.05e+10 =  1% of the original kernel matrix.

torch.Size([128954, 2])
We keep 4.14e+07/4.72e+09 =  0% of the original kernel matrix.

torch.Size([1673649, 2])
We keep 5.03e+09/7.45e+11 =  0% of the original kernel matrix.

torch.Size([308573, 2])
We keep 1.76e+08/2.33e+10 =  0% of the original kernel matrix.

torch.Size([132836, 2])
We keep 2.74e+08/9.15e+09 =  2% of the original kernel matrix.

torch.Size([81730, 2])
We keep 2.51e+07/2.58e+09 =  0% of the original kernel matrix.

torch.Size([275660, 2])
We keep 3.91e+08/3.50e+10 =  1% of the original kernel matrix.

torch.Size([119297, 2])
We keep 4.42e+07/5.05e+09 =  0% of the original kernel matrix.

torch.Size([65769, 2])
We keep 1.48e+08/2.75e+09 =  5% of the original kernel matrix.

torch.Size([55543, 2])
We keep 1.48e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([31654, 2])
We keep 1.65e+07/5.46e+08 =  3% of the original kernel matrix.

torch.Size([39411, 2])
We keep 7.69e+06/6.31e+08 =  1% of the original kernel matrix.

torch.Size([92586, 2])
We keep 4.21e+07/2.89e+09 =  1% of the original kernel matrix.

torch.Size([68956, 2])
We keep 1.53e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([15650, 2])
We keep 2.27e+06/7.28e+07 =  3% of the original kernel matrix.

torch.Size([27850, 2])
We keep 3.53e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([37211, 2])
We keep 6.29e+07/7.19e+08 =  8% of the original kernel matrix.

torch.Size([43446, 2])
We keep 8.69e+06/7.24e+08 =  1% of the original kernel matrix.

torch.Size([497380, 2])
We keep 5.01e+08/8.05e+10 =  0% of the original kernel matrix.

torch.Size([164852, 2])
We keep 6.39e+07/7.66e+09 =  0% of the original kernel matrix.

torch.Size([57326, 2])
We keep 5.85e+07/2.79e+09 =  2% of the original kernel matrix.

torch.Size([48559, 2])
We keep 1.50e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([538632, 2])
We keep 3.04e+09/1.68e+11 =  1% of the original kernel matrix.

torch.Size([162493, 2])
We keep 8.80e+07/1.11e+10 =  0% of the original kernel matrix.

torch.Size([235550, 2])
We keep 1.90e+09/3.99e+10 =  4% of the original kernel matrix.

torch.Size([108151, 2])
We keep 4.56e+07/5.39e+09 =  0% of the original kernel matrix.

torch.Size([59605, 2])
We keep 1.72e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([55490, 2])
We keep 1.05e+07/9.31e+08 =  1% of the original kernel matrix.

torch.Size([15625, 2])
We keep 2.75e+06/7.81e+07 =  3% of the original kernel matrix.

torch.Size([27925, 2])
We keep 3.63e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([138945, 2])
We keep 2.45e+08/7.68e+09 =  3% of the original kernel matrix.

torch.Size([83813, 2])
We keep 2.29e+07/2.37e+09 =  0% of the original kernel matrix.

torch.Size([171511, 2])
We keep 8.85e+07/8.99e+09 =  0% of the original kernel matrix.

torch.Size([93756, 2])
We keep 2.43e+07/2.56e+09 =  0% of the original kernel matrix.

torch.Size([11478, 2])
We keep 1.69e+06/4.57e+07 =  3% of the original kernel matrix.

torch.Size([23776, 2])
We keep 2.97e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([793799, 2])
We keep 1.10e+09/1.85e+11 =  0% of the original kernel matrix.

torch.Size([207601, 2])
We keep 9.21e+07/1.16e+10 =  0% of the original kernel matrix.

torch.Size([47796, 2])
We keep 3.13e+07/1.00e+09 =  3% of the original kernel matrix.

torch.Size([49606, 2])
We keep 9.67e+06/8.54e+08 =  1% of the original kernel matrix.

torch.Size([64303, 2])
We keep 3.97e+07/1.59e+09 =  2% of the original kernel matrix.

torch.Size([56835, 2])
We keep 1.19e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([12673, 2])
We keep 8.77e+05/3.05e+07 =  2% of the original kernel matrix.

torch.Size([25624, 2])
We keep 2.54e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([12160, 2])
We keep 1.03e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([24809, 2])
We keep 2.60e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([3811, 2])
We keep 1.17e+05/2.17e+06 =  5% of the original kernel matrix.

torch.Size([15321, 2])
We keep 9.97e+05/3.98e+07 =  2% of the original kernel matrix.

torch.Size([8533, 2])
We keep 1.85e+06/1.89e+07 =  9% of the original kernel matrix.

torch.Size([20939, 2])
We keep 2.13e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([32161, 2])
We keep 1.08e+07/3.50e+08 =  3% of the original kernel matrix.

torch.Size([40818, 2])
We keep 6.43e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([55447, 2])
We keep 4.18e+07/1.53e+09 =  2% of the original kernel matrix.

torch.Size([52198, 2])
We keep 1.14e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([6847711, 2])
We keep 6.73e+10/1.13e+13 =  0% of the original kernel matrix.

torch.Size([636602, 2])
We keep 6.28e+08/9.08e+10 =  0% of the original kernel matrix.

torch.Size([16217, 2])
We keep 3.67e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([28459, 2])
We keep 4.07e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([142011, 2])
We keep 5.97e+07/5.94e+09 =  1% of the original kernel matrix.

torch.Size([84197, 2])
We keep 2.04e+07/2.08e+09 =  0% of the original kernel matrix.

torch.Size([11405, 2])
We keep 1.18e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([24120, 2])
We keep 2.61e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([29944, 2])
We keep 4.47e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([39455, 2])
We keep 5.66e+06/4.33e+08 =  1% of the original kernel matrix.

torch.Size([25326, 2])
We keep 4.72e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([35932, 2])
We keep 5.25e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([253052, 2])
We keep 5.90e+08/4.28e+10 =  1% of the original kernel matrix.

torch.Size([109484, 2])
We keep 4.86e+07/5.58e+09 =  0% of the original kernel matrix.

torch.Size([30397, 2])
We keep 4.09e+07/4.86e+08 =  8% of the original kernel matrix.

torch.Size([38917, 2])
We keep 7.34e+06/5.95e+08 =  1% of the original kernel matrix.

torch.Size([1356751, 2])
We keep 2.61e+09/4.98e+11 =  0% of the original kernel matrix.

torch.Size([278241, 2])
We keep 1.44e+08/1.91e+10 =  0% of the original kernel matrix.

torch.Size([13493, 2])
We keep 1.13e+06/3.78e+07 =  2% of the original kernel matrix.

torch.Size([26167, 2])
We keep 2.72e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([325368, 2])
We keep 4.03e+08/3.71e+10 =  1% of the original kernel matrix.

torch.Size([132895, 2])
We keep 4.54e+07/5.20e+09 =  0% of the original kernel matrix.

torch.Size([67470, 2])
We keep 1.06e+08/2.13e+09 =  4% of the original kernel matrix.

torch.Size([57758, 2])
We keep 1.35e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([183732, 2])
We keep 1.90e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([96763, 2])
We keep 3.03e+07/3.25e+09 =  0% of the original kernel matrix.

torch.Size([367277, 2])
We keep 7.00e+08/6.32e+10 =  1% of the original kernel matrix.

torch.Size([139867, 2])
We keep 5.73e+07/6.79e+09 =  0% of the original kernel matrix.

torch.Size([32859, 2])
We keep 7.09e+06/3.29e+08 =  2% of the original kernel matrix.

torch.Size([41394, 2])
We keep 6.23e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([18165, 2])
We keep 2.34e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([30379, 2])
We keep 3.66e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([113293, 2])
We keep 4.70e+07/4.16e+09 =  1% of the original kernel matrix.

torch.Size([75238, 2])
We keep 1.76e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([15342, 2])
We keep 2.18e+06/5.93e+07 =  3% of the original kernel matrix.

torch.Size([27947, 2])
We keep 3.26e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([25668, 2])
We keep 7.81e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([35613, 2])
We keep 5.76e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([232366, 2])
We keep 1.95e+08/1.95e+10 =  1% of the original kernel matrix.

torch.Size([109773, 2])
We keep 3.38e+07/3.77e+09 =  0% of the original kernel matrix.

torch.Size([17437, 2])
We keep 4.25e+07/2.48e+08 = 17% of the original kernel matrix.

torch.Size([28913, 2])
We keep 5.43e+06/4.25e+08 =  1% of the original kernel matrix.

torch.Size([132316, 2])
We keep 1.03e+08/5.64e+09 =  1% of the original kernel matrix.

torch.Size([82532, 2])
We keep 2.01e+07/2.03e+09 =  0% of the original kernel matrix.

torch.Size([42471, 2])
We keep 2.03e+07/7.11e+08 =  2% of the original kernel matrix.

torch.Size([46164, 2])
We keep 8.59e+06/7.20e+08 =  1% of the original kernel matrix.

torch.Size([246718, 2])
We keep 4.15e+08/2.58e+10 =  1% of the original kernel matrix.

torch.Size([114329, 2])
We keep 3.88e+07/4.34e+09 =  0% of the original kernel matrix.

torch.Size([605963, 2])
We keep 1.46e+09/1.34e+11 =  1% of the original kernel matrix.

torch.Size([178003, 2])
We keep 8.04e+07/9.87e+09 =  0% of the original kernel matrix.

torch.Size([65955, 2])
We keep 3.13e+07/1.47e+09 =  2% of the original kernel matrix.

torch.Size([58006, 2])
We keep 1.16e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([16149, 2])
We keep 1.90e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([28543, 2])
We keep 3.48e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([128993, 2])
We keep 1.83e+08/9.97e+09 =  1% of the original kernel matrix.

torch.Size([78926, 2])
We keep 2.52e+07/2.70e+09 =  0% of the original kernel matrix.

torch.Size([119201, 2])
We keep 8.60e+07/5.76e+09 =  1% of the original kernel matrix.

torch.Size([75898, 2])
We keep 2.03e+07/2.05e+09 =  0% of the original kernel matrix.

torch.Size([121764, 2])
We keep 5.08e+08/8.35e+09 =  6% of the original kernel matrix.

torch.Size([77229, 2])
We keep 2.32e+07/2.47e+09 =  0% of the original kernel matrix.

torch.Size([70462, 2])
We keep 8.83e+07/2.57e+09 =  3% of the original kernel matrix.

torch.Size([58022, 2])
We keep 1.43e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([46888, 2])
We keep 2.97e+07/1.15e+09 =  2% of the original kernel matrix.

torch.Size([47651, 2])
We keep 1.03e+07/9.16e+08 =  1% of the original kernel matrix.

torch.Size([47083, 2])
We keep 2.93e+07/1.05e+09 =  2% of the original kernel matrix.

torch.Size([48191, 2])
We keep 1.00e+07/8.74e+08 =  1% of the original kernel matrix.

torch.Size([106847, 2])
We keep 2.60e+08/1.15e+10 =  2% of the original kernel matrix.

torch.Size([67074, 2])
We keep 2.67e+07/2.90e+09 =  0% of the original kernel matrix.

torch.Size([77275, 2])
We keep 2.16e+08/4.31e+09 =  5% of the original kernel matrix.

torch.Size([60502, 2])
We keep 1.76e+07/1.77e+09 =  0% of the original kernel matrix.

torch.Size([29566, 2])
We keep 5.11e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([39268, 2])
We keep 5.72e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([13069, 2])
We keep 1.08e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([25634, 2])
We keep 2.67e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([33263, 2])
We keep 5.89e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([42548, 2])
We keep 6.44e+06/5.00e+08 =  1% of the original kernel matrix.

torch.Size([82428, 2])
We keep 8.93e+07/4.58e+09 =  1% of the original kernel matrix.

torch.Size([61950, 2])
We keep 1.84e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([121222, 2])
We keep 1.48e+08/6.38e+09 =  2% of the original kernel matrix.

torch.Size([76864, 2])
We keep 2.12e+07/2.16e+09 =  0% of the original kernel matrix.

torch.Size([277066, 2])
We keep 4.68e+08/2.57e+10 =  1% of the original kernel matrix.

torch.Size([121534, 2])
We keep 3.84e+07/4.33e+09 =  0% of the original kernel matrix.

torch.Size([16272, 2])
We keep 1.83e+06/6.48e+07 =  2% of the original kernel matrix.

torch.Size([28657, 2])
We keep 3.38e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([37823, 2])
We keep 6.57e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([44310, 2])
We keep 7.17e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([41399, 2])
We keep 1.27e+07/6.38e+08 =  1% of the original kernel matrix.

torch.Size([45365, 2])
We keep 7.94e+06/6.82e+08 =  1% of the original kernel matrix.

torch.Size([222152, 2])
We keep 4.67e+08/1.71e+10 =  2% of the original kernel matrix.

torch.Size([108115, 2])
We keep 3.14e+07/3.54e+09 =  0% of the original kernel matrix.

torch.Size([42248, 2])
We keep 1.69e+07/6.87e+08 =  2% of the original kernel matrix.

torch.Size([46944, 2])
We keep 8.28e+06/7.08e+08 =  1% of the original kernel matrix.

torch.Size([115066, 2])
We keep 4.19e+07/3.92e+09 =  1% of the original kernel matrix.

torch.Size([75269, 2])
We keep 1.72e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([17674, 2])
We keep 6.40e+06/1.17e+08 =  5% of the original kernel matrix.

torch.Size([29776, 2])
We keep 4.18e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([35395, 2])
We keep 6.34e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([43156, 2])
We keep 6.72e+06/5.43e+08 =  1% of the original kernel matrix.

torch.Size([6152, 2])
We keep 3.72e+05/8.20e+06 =  4% of the original kernel matrix.

torch.Size([18329, 2])
We keep 1.59e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([22004, 2])
We keep 2.67e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([33529, 2])
We keep 4.29e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([187137, 2])
We keep 1.02e+08/1.22e+10 =  0% of the original kernel matrix.

torch.Size([98447, 2])
We keep 2.77e+07/2.98e+09 =  0% of the original kernel matrix.

torch.Size([32929, 2])
We keep 1.78e+07/5.93e+08 =  3% of the original kernel matrix.

torch.Size([40504, 2])
We keep 7.99e+06/6.57e+08 =  1% of the original kernel matrix.

torch.Size([103331, 2])
We keep 3.62e+07/3.05e+09 =  1% of the original kernel matrix.

torch.Size([71738, 2])
We keep 1.53e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([4871, 2])
We keep 1.39e+05/3.05e+06 =  4% of the original kernel matrix.

torch.Size([17257, 2])
We keep 1.13e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([12958, 2])
We keep 1.52e+06/3.52e+07 =  4% of the original kernel matrix.

torch.Size([25789, 2])
We keep 2.70e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([56567, 2])
We keep 3.89e+07/1.07e+09 =  3% of the original kernel matrix.

torch.Size([54156, 2])
We keep 1.00e+07/8.85e+08 =  1% of the original kernel matrix.

torch.Size([77077, 2])
We keep 2.69e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([61995, 2])
We keep 1.31e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([65898, 2])
We keep 1.77e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([58562, 2])
We keep 1.10e+07/9.86e+08 =  1% of the original kernel matrix.

torch.Size([10580, 2])
We keep 7.13e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([23520, 2])
We keep 2.26e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([5888, 2])
We keep 3.37e+05/7.11e+06 =  4% of the original kernel matrix.

torch.Size([18040, 2])
We keep 1.54e+06/7.20e+07 =  2% of the original kernel matrix.

torch.Size([52483, 2])
We keep 2.27e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([51925, 2])
We keep 9.65e+06/8.61e+08 =  1% of the original kernel matrix.

torch.Size([44971, 2])
We keep 1.08e+07/6.90e+08 =  1% of the original kernel matrix.

torch.Size([46117, 2])
We keep 7.84e+06/7.09e+08 =  1% of the original kernel matrix.

torch.Size([129144, 2])
We keep 1.77e+08/7.86e+09 =  2% of the original kernel matrix.

torch.Size([79916, 2])
We keep 2.32e+07/2.39e+09 =  0% of the original kernel matrix.

torch.Size([9494, 2])
We keep 7.18e+05/1.78e+07 =  4% of the original kernel matrix.

torch.Size([22410, 2])
We keep 2.07e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([17387, 2])
We keep 1.53e+07/1.19e+08 = 12% of the original kernel matrix.

torch.Size([28902, 2])
We keep 4.20e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([54990, 2])
We keep 4.43e+07/1.99e+09 =  2% of the original kernel matrix.

torch.Size([49942, 2])
We keep 1.31e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([9766, 2])
We keep 8.56e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([22445, 2])
We keep 2.26e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([326929, 2])
We keep 5.71e+08/4.39e+10 =  1% of the original kernel matrix.

torch.Size([134434, 2])
We keep 4.94e+07/5.66e+09 =  0% of the original kernel matrix.

torch.Size([15009, 2])
We keep 6.76e+06/1.08e+08 =  6% of the original kernel matrix.

torch.Size([27365, 2])
We keep 3.83e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([122413, 2])
We keep 1.06e+08/7.16e+09 =  1% of the original kernel matrix.

torch.Size([77245, 2])
We keep 2.18e+07/2.29e+09 =  0% of the original kernel matrix.

torch.Size([217087, 2])
We keep 1.53e+08/1.50e+10 =  1% of the original kernel matrix.

torch.Size([106634, 2])
We keep 3.05e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([67595, 2])
We keep 8.41e+07/1.61e+09 =  5% of the original kernel matrix.

torch.Size([58592, 2])
We keep 1.13e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([192289, 2])
We keep 1.98e+08/1.50e+10 =  1% of the original kernel matrix.

torch.Size([100012, 2])
We keep 3.06e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([245347, 2])
We keep 1.79e+08/1.88e+10 =  0% of the original kernel matrix.

torch.Size([113835, 2])
We keep 3.32e+07/3.70e+09 =  0% of the original kernel matrix.

torch.Size([2787, 2])
We keep 7.11e+04/1.08e+06 =  6% of the original kernel matrix.

torch.Size([13623, 2])
We keep 7.77e+05/2.80e+07 =  2% of the original kernel matrix.

torch.Size([86847, 2])
We keep 4.21e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([65881, 2])
We keep 1.41e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([24501, 2])
We keep 3.08e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([35629, 2])
We keep 4.53e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([152261, 2])
We keep 2.34e+08/7.90e+09 =  2% of the original kernel matrix.

torch.Size([87972, 2])
We keep 2.27e+07/2.40e+09 =  0% of the original kernel matrix.

torch.Size([171089, 2])
We keep 1.75e+08/1.26e+10 =  1% of the original kernel matrix.

torch.Size([93399, 2])
We keep 2.81e+07/3.03e+09 =  0% of the original kernel matrix.

torch.Size([30243, 2])
We keep 1.45e+07/4.69e+08 =  3% of the original kernel matrix.

torch.Size([38603, 2])
We keep 7.35e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([124015, 2])
We keep 7.87e+07/4.79e+09 =  1% of the original kernel matrix.

torch.Size([78802, 2])
We keep 1.87e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([25696, 2])
We keep 3.49e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([36402, 2])
We keep 5.02e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([22444, 2])
We keep 2.05e+07/5.72e+08 =  3% of the original kernel matrix.

torch.Size([31133, 2])
We keep 7.92e+06/6.46e+08 =  1% of the original kernel matrix.

torch.Size([17614, 2])
We keep 9.47e+06/1.04e+08 =  9% of the original kernel matrix.

torch.Size([29847, 2])
We keep 3.80e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([13113, 2])
We keep 1.67e+06/5.06e+07 =  3% of the original kernel matrix.

torch.Size([25335, 2])
We keep 3.02e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([275782, 2])
We keep 4.56e+08/2.72e+10 =  1% of the original kernel matrix.

torch.Size([121544, 2])
We keep 3.89e+07/4.45e+09 =  0% of the original kernel matrix.

torch.Size([829851, 2])
We keep 4.29e+09/2.39e+11 =  1% of the original kernel matrix.

torch.Size([215489, 2])
We keep 1.05e+08/1.32e+10 =  0% of the original kernel matrix.

torch.Size([28124, 2])
We keep 1.16e+07/3.88e+08 =  2% of the original kernel matrix.

torch.Size([36940, 2])
We keep 6.73e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([27492, 2])
We keep 4.33e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([37791, 2])
We keep 5.39e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([43408, 2])
We keep 1.09e+07/6.39e+08 =  1% of the original kernel matrix.

torch.Size([45320, 2])
We keep 7.67e+06/6.83e+08 =  1% of the original kernel matrix.

torch.Size([109430, 2])
We keep 1.16e+08/4.61e+09 =  2% of the original kernel matrix.

torch.Size([74153, 2])
We keep 1.82e+07/1.83e+09 =  0% of the original kernel matrix.

torch.Size([67056, 2])
We keep 2.19e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([58751, 2])
We keep 1.15e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([14955, 2])
We keep 1.77e+06/5.16e+07 =  3% of the original kernel matrix.

torch.Size([27637, 2])
We keep 3.06e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([74987, 2])
We keep 3.11e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([62052, 2])
We keep 1.29e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([235102, 2])
We keep 1.78e+08/1.78e+10 =  0% of the original kernel matrix.

torch.Size([111153, 2])
We keep 3.28e+07/3.60e+09 =  0% of the original kernel matrix.

torch.Size([14338, 2])
We keep 1.78e+06/5.34e+07 =  3% of the original kernel matrix.

torch.Size([26831, 2])
We keep 3.07e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([103620, 2])
We keep 9.99e+07/3.95e+09 =  2% of the original kernel matrix.

torch.Size([71710, 2])
We keep 1.72e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([21851, 2])
We keep 2.04e+07/2.10e+08 =  9% of the original kernel matrix.

torch.Size([33338, 2])
We keep 5.01e+06/3.91e+08 =  1% of the original kernel matrix.

time for making ranges is 6.783545255661011
Sorting X and nu_X
time for sorting X is 0.09075164794921875
Sorting Z and nu_Z
time for sorting Z is 0.00027871131896972656
Starting Optim
sum tnu_Z before tensor(31576560., device='cuda:0')
c= tensor(996.3505, device='cuda:0')
c= tensor(82457.7734, device='cuda:0')
c= tensor(96296.8828, device='cuda:0')
c= tensor(188882.3125, device='cuda:0')
c= tensor(241862.6562, device='cuda:0')
c= tensor(792405.1250, device='cuda:0')
c= tensor(1587705.5000, device='cuda:0')
c= tensor(2022298.3750, device='cuda:0')
c= tensor(2083178.6250, device='cuda:0')
c= tensor(3709692., device='cuda:0')
c= tensor(3727973.7500, device='cuda:0')
c= tensor(7435840., device='cuda:0')
c= tensor(7457749.5000, device='cuda:0')
c= tensor(44858076., device='cuda:0')
c= tensor(45071212., device='cuda:0')
c= tensor(46058012., device='cuda:0')
c= tensor(48420088., device='cuda:0')
c= tensor(48725360., device='cuda:0')
c= tensor(55846392., device='cuda:0')
c= tensor(61099936., device='cuda:0')
c= tensor(61680912., device='cuda:0')
c= tensor(80709864., device='cuda:0')
c= tensor(80753248., device='cuda:0')
c= tensor(81848368., device='cuda:0')
c= tensor(81854904., device='cuda:0')
c= tensor(83129672., device='cuda:0')
c= tensor(84541608., device='cuda:0')
c= tensor(84549680., device='cuda:0')
c= tensor(87061040., device='cuda:0')
c= tensor(3.6065e+08, device='cuda:0')
c= tensor(3.6071e+08, device='cuda:0')
c= tensor(5.8516e+08, device='cuda:0')
c= tensor(5.8563e+08, device='cuda:0')
c= tensor(5.8570e+08, device='cuda:0')
c= tensor(5.8586e+08, device='cuda:0')
c= tensor(6.0410e+08, device='cuda:0')
c= tensor(6.1095e+08, device='cuda:0')
c= tensor(6.1095e+08, device='cuda:0')
c= tensor(6.1096e+08, device='cuda:0')
c= tensor(6.1097e+08, device='cuda:0')
c= tensor(6.1098e+08, device='cuda:0')
c= tensor(6.1098e+08, device='cuda:0')
c= tensor(6.1099e+08, device='cuda:0')
c= tensor(6.1100e+08, device='cuda:0')
c= tensor(6.1100e+08, device='cuda:0')
c= tensor(6.1100e+08, device='cuda:0')
c= tensor(6.1101e+08, device='cuda:0')
c= tensor(6.1102e+08, device='cuda:0')
c= tensor(6.1102e+08, device='cuda:0')
c= tensor(6.1110e+08, device='cuda:0')
c= tensor(6.1116e+08, device='cuda:0')
c= tensor(6.1116e+08, device='cuda:0')
c= tensor(6.1119e+08, device='cuda:0')
c= tensor(6.1119e+08, device='cuda:0')
c= tensor(6.1121e+08, device='cuda:0')
c= tensor(6.1122e+08, device='cuda:0')
c= tensor(6.1123e+08, device='cuda:0')
c= tensor(6.1123e+08, device='cuda:0')
c= tensor(6.1124e+08, device='cuda:0')
c= tensor(6.1124e+08, device='cuda:0')
c= tensor(6.1125e+08, device='cuda:0')
c= tensor(6.1125e+08, device='cuda:0')
c= tensor(6.1141e+08, device='cuda:0')
c= tensor(6.1143e+08, device='cuda:0')
c= tensor(6.1143e+08, device='cuda:0')
c= tensor(6.1143e+08, device='cuda:0')
c= tensor(6.1143e+08, device='cuda:0')
c= tensor(6.1145e+08, device='cuda:0')
c= tensor(6.1146e+08, device='cuda:0')
c= tensor(6.1146e+08, device='cuda:0')
c= tensor(6.1147e+08, device='cuda:0')
c= tensor(6.1148e+08, device='cuda:0')
c= tensor(6.1148e+08, device='cuda:0')
c= tensor(6.1148e+08, device='cuda:0')
c= tensor(6.1149e+08, device='cuda:0')
c= tensor(6.1150e+08, device='cuda:0')
c= tensor(6.1150e+08, device='cuda:0')
c= tensor(6.1150e+08, device='cuda:0')
c= tensor(6.1151e+08, device='cuda:0')
c= tensor(6.1166e+08, device='cuda:0')
c= tensor(6.1167e+08, device='cuda:0')
c= tensor(6.1167e+08, device='cuda:0')
c= tensor(6.1176e+08, device='cuda:0')
c= tensor(6.1176e+08, device='cuda:0')
c= tensor(6.1176e+08, device='cuda:0')
c= tensor(6.1176e+08, device='cuda:0')
c= tensor(6.1177e+08, device='cuda:0')
c= tensor(6.1177e+08, device='cuda:0')
c= tensor(6.1179e+08, device='cuda:0')
c= tensor(6.1179e+08, device='cuda:0')
c= tensor(6.1180e+08, device='cuda:0')
c= tensor(6.1180e+08, device='cuda:0')
c= tensor(6.1180e+08, device='cuda:0')
c= tensor(6.1180e+08, device='cuda:0')
c= tensor(6.1182e+08, device='cuda:0')
c= tensor(6.1184e+08, device='cuda:0')
c= tensor(6.1184e+08, device='cuda:0')
c= tensor(6.1189e+08, device='cuda:0')
c= tensor(6.1190e+08, device='cuda:0')
c= tensor(6.1191e+08, device='cuda:0')
c= tensor(6.1195e+08, device='cuda:0')
c= tensor(6.1196e+08, device='cuda:0')
c= tensor(6.1197e+08, device='cuda:0')
c= tensor(6.1197e+08, device='cuda:0')
c= tensor(6.1199e+08, device='cuda:0')
c= tensor(6.1199e+08, device='cuda:0')
c= tensor(6.1200e+08, device='cuda:0')
c= tensor(6.1201e+08, device='cuda:0')
c= tensor(6.1201e+08, device='cuda:0')
c= tensor(6.1201e+08, device='cuda:0')
c= tensor(6.1201e+08, device='cuda:0')
c= tensor(6.1202e+08, device='cuda:0')
c= tensor(6.1202e+08, device='cuda:0')
c= tensor(6.1202e+08, device='cuda:0')
c= tensor(6.1203e+08, device='cuda:0')
c= tensor(6.1203e+08, device='cuda:0')
c= tensor(6.1203e+08, device='cuda:0')
c= tensor(6.1203e+08, device='cuda:0')
c= tensor(6.1206e+08, device='cuda:0')
c= tensor(6.1206e+08, device='cuda:0')
c= tensor(6.1210e+08, device='cuda:0')
c= tensor(6.1210e+08, device='cuda:0')
c= tensor(6.1210e+08, device='cuda:0')
c= tensor(6.1210e+08, device='cuda:0')
c= tensor(6.1211e+08, device='cuda:0')
c= tensor(6.1211e+08, device='cuda:0')
c= tensor(6.1211e+08, device='cuda:0')
c= tensor(6.1211e+08, device='cuda:0')
c= tensor(6.1230e+08, device='cuda:0')
c= tensor(6.1230e+08, device='cuda:0')
c= tensor(6.1232e+08, device='cuda:0')
c= tensor(6.1233e+08, device='cuda:0')
c= tensor(6.1233e+08, device='cuda:0')
c= tensor(6.1233e+08, device='cuda:0')
c= tensor(6.1237e+08, device='cuda:0')
c= tensor(6.1238e+08, device='cuda:0')
c= tensor(6.1238e+08, device='cuda:0')
c= tensor(6.1238e+08, device='cuda:0')
c= tensor(6.1238e+08, device='cuda:0')
c= tensor(6.1238e+08, device='cuda:0')
c= tensor(6.1240e+08, device='cuda:0')
c= tensor(6.1240e+08, device='cuda:0')
c= tensor(6.1255e+08, device='cuda:0')
c= tensor(6.1257e+08, device='cuda:0')
c= tensor(6.1264e+08, device='cuda:0')
c= tensor(6.1264e+08, device='cuda:0')
c= tensor(6.1264e+08, device='cuda:0')
c= tensor(6.1265e+08, device='cuda:0')
c= tensor(6.1265e+08, device='cuda:0')
c= tensor(6.1265e+08, device='cuda:0')
c= tensor(6.1266e+08, device='cuda:0')
c= tensor(6.1268e+08, device='cuda:0')
c= tensor(6.1269e+08, device='cuda:0')
c= tensor(6.1274e+08, device='cuda:0')
c= tensor(6.1275e+08, device='cuda:0')
c= tensor(6.1307e+08, device='cuda:0')
c= tensor(6.1307e+08, device='cuda:0')
c= tensor(6.1307e+08, device='cuda:0')
c= tensor(6.1309e+08, device='cuda:0')
c= tensor(6.1309e+08, device='cuda:0')
c= tensor(6.1310e+08, device='cuda:0')
c= tensor(6.1310e+08, device='cuda:0')
c= tensor(6.1311e+08, device='cuda:0')
c= tensor(6.1311e+08, device='cuda:0')
c= tensor(6.1313e+08, device='cuda:0')
c= tensor(6.1313e+08, device='cuda:0')
c= tensor(6.1313e+08, device='cuda:0')
c= tensor(6.1313e+08, device='cuda:0')
c= tensor(6.1315e+08, device='cuda:0')
c= tensor(6.1315e+08, device='cuda:0')
c= tensor(6.1315e+08, device='cuda:0')
c= tensor(6.1315e+08, device='cuda:0')
c= tensor(6.1317e+08, device='cuda:0')
c= tensor(6.1318e+08, device='cuda:0')
c= tensor(6.1319e+08, device='cuda:0')
c= tensor(6.1324e+08, device='cuda:0')
c= tensor(6.1326e+08, device='cuda:0')
c= tensor(6.1326e+08, device='cuda:0')
c= tensor(6.1328e+08, device='cuda:0')
c= tensor(6.1328e+08, device='cuda:0')
c= tensor(6.1328e+08, device='cuda:0')
c= tensor(6.1329e+08, device='cuda:0')
c= tensor(6.1331e+08, device='cuda:0')
c= tensor(6.1331e+08, device='cuda:0')
c= tensor(6.1332e+08, device='cuda:0')
c= tensor(6.1333e+08, device='cuda:0')
c= tensor(6.1334e+08, device='cuda:0')
c= tensor(6.1336e+08, device='cuda:0')
c= tensor(6.1338e+08, device='cuda:0')
c= tensor(6.1346e+08, device='cuda:0')
c= tensor(6.1347e+08, device='cuda:0')
c= tensor(6.1347e+08, device='cuda:0')
c= tensor(6.1347e+08, device='cuda:0')
c= tensor(6.1349e+08, device='cuda:0')
c= tensor(6.1350e+08, device='cuda:0')
c= tensor(6.1351e+08, device='cuda:0')
c= tensor(6.1351e+08, device='cuda:0')
c= tensor(6.1351e+08, device='cuda:0')
c= tensor(6.1352e+08, device='cuda:0')
c= tensor(6.1352e+08, device='cuda:0')
c= tensor(6.1353e+08, device='cuda:0')
c= tensor(6.1353e+08, device='cuda:0')
c= tensor(6.1355e+08, device='cuda:0')
c= tensor(6.1358e+08, device='cuda:0')
c= tensor(6.1359e+08, device='cuda:0')
c= tensor(6.1359e+08, device='cuda:0')
c= tensor(6.1362e+08, device='cuda:0')
c= tensor(6.1363e+08, device='cuda:0')
c= tensor(6.1363e+08, device='cuda:0')
c= tensor(6.1368e+08, device='cuda:0')
c= tensor(6.1370e+08, device='cuda:0')
c= tensor(6.1370e+08, device='cuda:0')
c= tensor(6.1370e+08, device='cuda:0')
c= tensor(6.1371e+08, device='cuda:0')
c= tensor(6.1371e+08, device='cuda:0')
c= tensor(6.1371e+08, device='cuda:0')
c= tensor(6.1371e+08, device='cuda:0')
c= tensor(6.1371e+08, device='cuda:0')
c= tensor(6.1373e+08, device='cuda:0')
c= tensor(6.1373e+08, device='cuda:0')
c= tensor(6.1374e+08, device='cuda:0')
c= tensor(6.1374e+08, device='cuda:0')
c= tensor(6.1375e+08, device='cuda:0')
c= tensor(6.1375e+08, device='cuda:0')
c= tensor(6.1376e+08, device='cuda:0')
c= tensor(6.1377e+08, device='cuda:0')
c= tensor(6.1378e+08, device='cuda:0')
c= tensor(6.1378e+08, device='cuda:0')
c= tensor(6.1380e+08, device='cuda:0')
c= tensor(6.1380e+08, device='cuda:0')
c= tensor(6.1380e+08, device='cuda:0')
c= tensor(6.1380e+08, device='cuda:0')
c= tensor(6.1381e+08, device='cuda:0')
c= tensor(6.1383e+08, device='cuda:0')
c= tensor(6.1383e+08, device='cuda:0')
c= tensor(6.1383e+08, device='cuda:0')
c= tensor(6.1383e+08, device='cuda:0')
c= tensor(6.1386e+08, device='cuda:0')
c= tensor(6.1386e+08, device='cuda:0')
c= tensor(6.1402e+08, device='cuda:0')
c= tensor(6.1592e+08, device='cuda:0')
c= tensor(6.1593e+08, device='cuda:0')
c= tensor(6.1594e+08, device='cuda:0')
c= tensor(6.1594e+08, device='cuda:0')
c= tensor(6.1595e+08, device='cuda:0')
c= tensor(6.3027e+08, device='cuda:0')
c= tensor(6.7197e+08, device='cuda:0')
c= tensor(6.7198e+08, device='cuda:0')
c= tensor(6.7761e+08, device='cuda:0')
c= tensor(6.7940e+08, device='cuda:0')
c= tensor(6.7956e+08, device='cuda:0')
c= tensor(7.0819e+08, device='cuda:0')
c= tensor(7.0819e+08, device='cuda:0')
c= tensor(7.0819e+08, device='cuda:0')
c= tensor(7.1407e+08, device='cuda:0')
c= tensor(7.9042e+08, device='cuda:0')
c= tensor(7.9045e+08, device='cuda:0')
c= tensor(7.9081e+08, device='cuda:0')
c= tensor(7.9102e+08, device='cuda:0')
c= tensor(7.9278e+08, device='cuda:0')
c= tensor(7.9826e+08, device='cuda:0')
c= tensor(7.9994e+08, device='cuda:0')
c= tensor(8.0091e+08, device='cuda:0')
c= tensor(8.0104e+08, device='cuda:0')
c= tensor(8.0110e+08, device='cuda:0')
c= tensor(8.1556e+08, device='cuda:0')
c= tensor(8.1560e+08, device='cuda:0')
c= tensor(8.1560e+08, device='cuda:0')
c= tensor(8.1579e+08, device='cuda:0')
c= tensor(8.1764e+08, device='cuda:0')
c= tensor(8.2815e+08, device='cuda:0')
c= tensor(8.2962e+08, device='cuda:0')
c= tensor(8.2963e+08, device='cuda:0')
c= tensor(8.3033e+08, device='cuda:0')
c= tensor(8.3041e+08, device='cuda:0')
c= tensor(8.3136e+08, device='cuda:0')
c= tensor(8.3402e+08, device='cuda:0')
c= tensor(8.3546e+08, device='cuda:0')
c= tensor(8.3715e+08, device='cuda:0')
c= tensor(8.3715e+08, device='cuda:0')
c= tensor(8.3717e+08, device='cuda:0')
c= tensor(8.4093e+08, device='cuda:0')
c= tensor(8.4193e+08, device='cuda:0')
c= tensor(8.4407e+08, device='cuda:0')
c= tensor(8.4409e+08, device='cuda:0')
c= tensor(8.6813e+08, device='cuda:0')
c= tensor(8.6816e+08, device='cuda:0')
c= tensor(8.6849e+08, device='cuda:0')
c= tensor(8.7191e+08, device='cuda:0')
c= tensor(8.7192e+08, device='cuda:0')
c= tensor(8.7600e+08, device='cuda:0')
c= tensor(8.9909e+08, device='cuda:0')
c= tensor(9.4073e+08, device='cuda:0')
c= tensor(9.4093e+08, device='cuda:0')
c= tensor(9.4110e+08, device='cuda:0')
c= tensor(9.4112e+08, device='cuda:0')
c= tensor(9.4112e+08, device='cuda:0')
c= tensor(9.4361e+08, device='cuda:0')
c= tensor(9.4363e+08, device='cuda:0')
c= tensor(9.4406e+08, device='cuda:0')
c= tensor(9.4776e+08, device='cuda:0')
c= tensor(9.4795e+08, device='cuda:0')
c= tensor(9.4829e+08, device='cuda:0')
c= tensor(9.4831e+08, device='cuda:0')
c= tensor(9.5451e+08, device='cuda:0')
c= tensor(9.5724e+08, device='cuda:0')
c= tensor(9.5788e+08, device='cuda:0')
c= tensor(9.5802e+08, device='cuda:0')
c= tensor(9.8873e+08, device='cuda:0')
c= tensor(9.8888e+08, device='cuda:0')
c= tensor(1.0079e+09, device='cuda:0')
c= tensor(1.0079e+09, device='cuda:0')
c= tensor(1.0123e+09, device='cuda:0')
c= tensor(1.0125e+09, device='cuda:0')
c= tensor(1.0326e+09, device='cuda:0')
c= tensor(1.0334e+09, device='cuda:0')
c= tensor(1.0335e+09, device='cuda:0')
c= tensor(1.0374e+09, device='cuda:0')
c= tensor(1.0412e+09, device='cuda:0')
c= tensor(1.0412e+09, device='cuda:0')
c= tensor(1.0475e+09, device='cuda:0')
c= tensor(1.0547e+09, device='cuda:0')
c= tensor(1.0817e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.0837e+09, device='cuda:0')
c= tensor(1.0839e+09, device='cuda:0')
c= tensor(1.0847e+09, device='cuda:0')
c= tensor(1.0847e+09, device='cuda:0')
c= tensor(1.0863e+09, device='cuda:0')
c= tensor(1.0898e+09, device='cuda:0')
c= tensor(1.0899e+09, device='cuda:0')
c= tensor(1.0899e+09, device='cuda:0')
c= tensor(1.0904e+09, device='cuda:0')
c= tensor(1.0906e+09, device='cuda:0')
c= tensor(1.0907e+09, device='cuda:0')
c= tensor(1.0907e+09, device='cuda:0')
c= tensor(1.0907e+09, device='cuda:0')
c= tensor(1.0913e+09, device='cuda:0')
c= tensor(1.0916e+09, device='cuda:0')
c= tensor(1.0917e+09, device='cuda:0')
c= tensor(1.0927e+09, device='cuda:0')
c= tensor(1.0928e+09, device='cuda:0')
c= tensor(1.4537e+09, device='cuda:0')
c= tensor(1.4538e+09, device='cuda:0')
c= tensor(1.4570e+09, device='cuda:0')
c= tensor(1.4570e+09, device='cuda:0')
c= tensor(1.4570e+09, device='cuda:0')
c= tensor(1.4570e+09, device='cuda:0')
c= tensor(1.4571e+09, device='cuda:0')
c= tensor(1.4571e+09, device='cuda:0')
c= tensor(1.4614e+09, device='cuda:0')
c= tensor(1.4614e+09, device='cuda:0')
c= tensor(1.4614e+09, device='cuda:0')
c= tensor(1.4905e+09, device='cuda:0')
c= tensor(1.4914e+09, device='cuda:0')
c= tensor(1.4922e+09, device='cuda:0')
c= tensor(1.4959e+09, device='cuda:0')
c= tensor(1.5127e+09, device='cuda:0')
c= tensor(1.5127e+09, device='cuda:0')
c= tensor(1.5127e+09, device='cuda:0')
c= tensor(1.5129e+09, device='cuda:0')
c= tensor(1.5129e+09, device='cuda:0')
c= tensor(1.5129e+09, device='cuda:0')
c= tensor(1.5129e+09, device='cuda:0')
c= tensor(1.5129e+09, device='cuda:0')
c= tensor(1.5130e+09, device='cuda:0')
c= tensor(1.5130e+09, device='cuda:0')
c= tensor(1.5130e+09, device='cuda:0')
c= tensor(1.5375e+09, device='cuda:0')
c= tensor(1.5376e+09, device='cuda:0')
c= tensor(1.5394e+09, device='cuda:0')
c= tensor(1.5394e+09, device='cuda:0')
c= tensor(1.5394e+09, device='cuda:0')
c= tensor(1.5405e+09, device='cuda:0')
c= tensor(1.6700e+09, device='cuda:0')
c= tensor(1.7194e+09, device='cuda:0')
c= tensor(1.7195e+09, device='cuda:0')
c= tensor(1.7237e+09, device='cuda:0')
c= tensor(1.7237e+09, device='cuda:0')
c= tensor(1.7238e+09, device='cuda:0')
c= tensor(1.7551e+09, device='cuda:0')
c= tensor(1.7553e+09, device='cuda:0')
c= tensor(1.7556e+09, device='cuda:0')
c= tensor(1.7567e+09, device='cuda:0')
c= tensor(1.9778e+09, device='cuda:0')
c= tensor(1.9784e+09, device='cuda:0')
c= tensor(1.9785e+09, device='cuda:0')
c= tensor(1.9788e+09, device='cuda:0')
c= tensor(1.9788e+09, device='cuda:0')
c= tensor(1.9788e+09, device='cuda:0')
c= tensor(1.9897e+09, device='cuda:0')
c= tensor(1.9898e+09, device='cuda:0')
c= tensor(1.9898e+09, device='cuda:0')
c= tensor(1.9906e+09, device='cuda:0')
c= tensor(1.9907e+09, device='cuda:0')
c= tensor(1.9907e+09, device='cuda:0')
c= tensor(1.9933e+09, device='cuda:0')
c= tensor(1.9973e+09, device='cuda:0')
c= tensor(2.0071e+09, device='cuda:0')
c= tensor(2.0193e+09, device='cuda:0')
c= tensor(2.0303e+09, device='cuda:0')
c= tensor(2.0305e+09, device='cuda:0')
c= tensor(2.0307e+09, device='cuda:0')
c= tensor(2.0328e+09, device='cuda:0')
c= tensor(2.0378e+09, device='cuda:0')
c= tensor(2.0378e+09, device='cuda:0')
c= tensor(2.0449e+09, device='cuda:0')
c= tensor(2.0954e+09, device='cuda:0')
c= tensor(2.1106e+09, device='cuda:0')
c= tensor(2.1133e+09, device='cuda:0')
c= tensor(2.1157e+09, device='cuda:0')
c= tensor(2.1158e+09, device='cuda:0')
c= tensor(2.1158e+09, device='cuda:0')
c= tensor(2.1160e+09, device='cuda:0')
c= tensor(2.1190e+09, device='cuda:0')
c= tensor(2.1218e+09, device='cuda:0')
c= tensor(2.1897e+09, device='cuda:0')
c= tensor(2.1954e+09, device='cuda:0')
c= tensor(2.1999e+09, device='cuda:0')
c= tensor(2.2002e+09, device='cuda:0')
c= tensor(2.2057e+09, device='cuda:0')
c= tensor(2.2057e+09, device='cuda:0')
c= tensor(2.2059e+09, device='cuda:0')
c= tensor(2.2137e+09, device='cuda:0')
c= tensor(2.2143e+09, device='cuda:0')
c= tensor(2.2143e+09, device='cuda:0')
c= tensor(2.2145e+09, device='cuda:0')
c= tensor(2.2592e+09, device='cuda:0')
c= tensor(2.2594e+09, device='cuda:0')
c= tensor(2.2625e+09, device='cuda:0')
c= tensor(2.2625e+09, device='cuda:0')
c= tensor(2.2625e+09, device='cuda:0')
c= tensor(2.2626e+09, device='cuda:0')
c= tensor(2.2626e+09, device='cuda:0')
c= tensor(2.2639e+09, device='cuda:0')
c= tensor(2.2652e+09, device='cuda:0')
c= tensor(2.2652e+09, device='cuda:0')
c= tensor(2.2718e+09, device='cuda:0')
c= tensor(2.2718e+09, device='cuda:0')
c= tensor(2.2731e+09, device='cuda:0')
c= tensor(2.2732e+09, device='cuda:0')
c= tensor(2.2739e+09, device='cuda:0')
c= tensor(2.2739e+09, device='cuda:0')
c= tensor(2.2741e+09, device='cuda:0')
c= tensor(2.2742e+09, device='cuda:0')
c= tensor(2.2747e+09, device='cuda:0')
c= tensor(2.2768e+09, device='cuda:0')
c= tensor(2.3422e+09, device='cuda:0')
c= tensor(2.3422e+09, device='cuda:0')
c= tensor(2.3423e+09, device='cuda:0')
c= tensor(2.3495e+09, device='cuda:0')
c= tensor(2.3495e+09, device='cuda:0')
c= tensor(2.3921e+09, device='cuda:0')
c= tensor(2.3921e+09, device='cuda:0')
c= tensor(2.3947e+09, device='cuda:0')
c= tensor(2.4091e+09, device='cuda:0')
c= tensor(2.4091e+09, device='cuda:0')
c= tensor(2.4336e+09, device='cuda:0')
c= tensor(2.4340e+09, device='cuda:0')
c= tensor(2.5333e+09, device='cuda:0')
c= tensor(2.5333e+09, device='cuda:0')
c= tensor(2.5333e+09, device='cuda:0')
c= tensor(2.5334e+09, device='cuda:0')
c= tensor(2.5334e+09, device='cuda:0')
c= tensor(2.5335e+09, device='cuda:0')
c= tensor(2.5342e+09, device='cuda:0')
c= tensor(2.5346e+09, device='cuda:0')
c= tensor(2.5382e+09, device='cuda:0')
c= tensor(2.5383e+09, device='cuda:0')
c= tensor(2.5383e+09, device='cuda:0')
c= tensor(2.5384e+09, device='cuda:0')
c= tensor(2.5749e+09, device='cuda:0')
c= tensor(2.5848e+09, device='cuda:0')
c= tensor(2.6196e+09, device='cuda:0')
c= tensor(2.6198e+09, device='cuda:0')
c= tensor(2.6199e+09, device='cuda:0')
c= tensor(2.6199e+09, device='cuda:0')
c= tensor(2.6200e+09, device='cuda:0')
c= tensor(2.6440e+09, device='cuda:0')
c= tensor(2.6440e+09, device='cuda:0')
c= tensor(2.6441e+09, device='cuda:0')
c= tensor(2.6449e+09, device='cuda:0')
c= tensor(2.6460e+09, device='cuda:0')
c= tensor(2.6461e+09, device='cuda:0')
c= tensor(2.6462e+09, device='cuda:0')
c= tensor(2.6960e+09, device='cuda:0')
c= tensor(2.6967e+09, device='cuda:0')
c= tensor(2.7005e+09, device='cuda:0')
c= tensor(2.7006e+09, device='cuda:0')
c= tensor(2.7135e+09, device='cuda:0')
c= tensor(2.7291e+09, device='cuda:0')
c= tensor(2.7983e+09, device='cuda:0')
c= tensor(2.8087e+09, device='cuda:0')
c= tensor(2.8090e+09, device='cuda:0')
c= tensor(2.8095e+09, device='cuda:0')
c= tensor(2.8100e+09, device='cuda:0')
c= tensor(2.8100e+09, device='cuda:0')
c= tensor(2.8100e+09, device='cuda:0')
c= tensor(2.8101e+09, device='cuda:0')
c= tensor(2.8111e+09, device='cuda:0')
c= tensor(2.8114e+09, device='cuda:0')
c= tensor(2.8114e+09, device='cuda:0')
c= tensor(2.8115e+09, device='cuda:0')
c= tensor(2.8115e+09, device='cuda:0')
c= tensor(2.8133e+09, device='cuda:0')
c= tensor(2.8133e+09, device='cuda:0')
c= tensor(2.8136e+09, device='cuda:0')
c= tensor(2.8142e+09, device='cuda:0')
c= tensor(2.8144e+09, device='cuda:0')
c= tensor(2.8144e+09, device='cuda:0')
c= tensor(2.8144e+09, device='cuda:0')
c= tensor(2.8146e+09, device='cuda:0')
c= tensor(2.8225e+09, device='cuda:0')
c= tensor(2.8225e+09, device='cuda:0')
c= tensor(2.8225e+09, device='cuda:0')
c= tensor(2.8225e+09, device='cuda:0')
c= tensor(2.8492e+09, device='cuda:0')
c= tensor(2.9271e+09, device='cuda:0')
c= tensor(2.9282e+09, device='cuda:0')
c= tensor(2.9282e+09, device='cuda:0')
c= tensor(2.9374e+09, device='cuda:0')
c= tensor(2.9408e+09, device='cuda:0')
c= tensor(2.9408e+09, device='cuda:0')
c= tensor(2.9408e+09, device='cuda:0')
c= tensor(2.9410e+09, device='cuda:0')
c= tensor(2.9515e+09, device='cuda:0')
c= tensor(2.9594e+09, device='cuda:0')
c= tensor(2.9687e+09, device='cuda:0')
c= tensor(2.9688e+09, device='cuda:0')
c= tensor(2.9688e+09, device='cuda:0')
c= tensor(2.9688e+09, device='cuda:0')
c= tensor(2.9696e+09, device='cuda:0')
c= tensor(2.9696e+09, device='cuda:0')
c= tensor(2.9699e+09, device='cuda:0')
c= tensor(2.9885e+09, device='cuda:0')
c= tensor(3.0373e+09, device='cuda:0')
c= tensor(3.0373e+09, device='cuda:0')
c= tensor(3.0373e+09, device='cuda:0')
c= tensor(3.0374e+09, device='cuda:0')
c= tensor(3.2767e+09, device='cuda:0')
c= tensor(3.2790e+09, device='cuda:0')
c= tensor(3.2791e+09, device='cuda:0')
c= tensor(3.2791e+09, device='cuda:0')
c= tensor(3.2900e+09, device='cuda:0')
c= tensor(3.2900e+09, device='cuda:0')
c= tensor(3.2914e+09, device='cuda:0')
c= tensor(3.2914e+09, device='cuda:0')
c= tensor(3.2914e+09, device='cuda:0')
c= tensor(3.2914e+09, device='cuda:0')
c= tensor(3.2915e+09, device='cuda:0')
c= tensor(3.2915e+09, device='cuda:0')
c= tensor(3.2970e+09, device='cuda:0')
c= tensor(3.3239e+09, device='cuda:0')
c= tensor(3.3342e+09, device='cuda:0')
c= tensor(3.3415e+09, device='cuda:0')
c= tensor(3.3417e+09, device='cuda:0')
c= tensor(3.3418e+09, device='cuda:0')
c= tensor(3.3418e+09, device='cuda:0')
c= tensor(3.3445e+09, device='cuda:0')
c= tensor(3.3454e+09, device='cuda:0')
c= tensor(3.3560e+09, device='cuda:0')
c= tensor(3.3560e+09, device='cuda:0')
c= tensor(3.6609e+09, device='cuda:0')
c= tensor(3.6610e+09, device='cuda:0')
c= tensor(3.6622e+09, device='cuda:0')
c= tensor(3.6720e+09, device='cuda:0')
c= tensor(3.6724e+09, device='cuda:0')
c= tensor(3.6812e+09, device='cuda:0')
c= tensor(3.7636e+09, device='cuda:0')
c= tensor(3.7673e+09, device='cuda:0')
c= tensor(3.7688e+09, device='cuda:0')
c= tensor(3.7688e+09, device='cuda:0')
c= tensor(3.7694e+09, device='cuda:0')
c= tensor(3.7695e+09, device='cuda:0')
c= tensor(3.7826e+09, device='cuda:0')
c= tensor(3.9404e+09, device='cuda:0')
c= tensor(3.9478e+09, device='cuda:0')
c= tensor(3.9625e+09, device='cuda:0')
c= tensor(3.9653e+09, device='cuda:0')
c= tensor(3.9657e+09, device='cuda:0')
c= tensor(3.9666e+09, device='cuda:0')
c= tensor(3.9666e+09, device='cuda:0')
c= tensor(3.9678e+09, device='cuda:0')
c= tensor(3.9833e+09, device='cuda:0')
c= tensor(3.9843e+09, device='cuda:0')
c= tensor(4.0888e+09, device='cuda:0')
c= tensor(4.1524e+09, device='cuda:0')
c= tensor(4.1529e+09, device='cuda:0')
c= tensor(4.1529e+09, device='cuda:0')
c= tensor(4.1609e+09, device='cuda:0')
c= tensor(4.1628e+09, device='cuda:0')
c= tensor(4.1629e+09, device='cuda:0')
c= tensor(4.1924e+09, device='cuda:0')
c= tensor(4.1936e+09, device='cuda:0')
c= tensor(4.1950e+09, device='cuda:0')
c= tensor(4.1950e+09, device='cuda:0')
c= tensor(4.1951e+09, device='cuda:0')
c= tensor(4.1951e+09, device='cuda:0')
c= tensor(4.1951e+09, device='cuda:0')
c= tensor(4.1953e+09, device='cuda:0')
c= tensor(4.1961e+09, device='cuda:0')
c= tensor(7.4218e+09, device='cuda:0')
c= tensor(7.4221e+09, device='cuda:0')
c= tensor(7.4232e+09, device='cuda:0')
c= tensor(7.4233e+09, device='cuda:0')
c= tensor(7.4233e+09, device='cuda:0')
c= tensor(7.4235e+09, device='cuda:0')
c= tensor(7.4438e+09, device='cuda:0')
c= tensor(7.4446e+09, device='cuda:0')
c= tensor(7.5523e+09, device='cuda:0')
c= tensor(7.5524e+09, device='cuda:0')
c= tensor(7.5661e+09, device='cuda:0')
c= tensor(7.5694e+09, device='cuda:0')
c= tensor(7.5762e+09, device='cuda:0')
c= tensor(7.6006e+09, device='cuda:0')
c= tensor(7.6007e+09, device='cuda:0')
c= tensor(7.6007e+09, device='cuda:0')
c= tensor(7.6018e+09, device='cuda:0')
c= tensor(7.6019e+09, device='cuda:0')
c= tensor(7.6021e+09, device='cuda:0')
c= tensor(7.6072e+09, device='cuda:0')
c= tensor(7.6088e+09, device='cuda:0')
c= tensor(7.6111e+09, device='cuda:0')
c= tensor(7.6116e+09, device='cuda:0')
c= tensor(7.6296e+09, device='cuda:0')
c= tensor(7.6796e+09, device='cuda:0')
c= tensor(7.6803e+09, device='cuda:0')
c= tensor(7.6804e+09, device='cuda:0')
c= tensor(7.6866e+09, device='cuda:0')
c= tensor(7.6884e+09, device='cuda:0')
c= tensor(7.6982e+09, device='cuda:0')
c= tensor(7.7005e+09, device='cuda:0')
c= tensor(7.7014e+09, device='cuda:0')
c= tensor(7.7022e+09, device='cuda:0')
c= tensor(7.7150e+09, device='cuda:0')
c= tensor(7.7207e+09, device='cuda:0')
c= tensor(7.7208e+09, device='cuda:0')
c= tensor(7.7208e+09, device='cuda:0')
c= tensor(7.7209e+09, device='cuda:0')
c= tensor(7.7243e+09, device='cuda:0')
c= tensor(7.7280e+09, device='cuda:0')
c= tensor(7.7468e+09, device='cuda:0')
c= tensor(7.7468e+09, device='cuda:0')
c= tensor(7.7470e+09, device='cuda:0')
c= tensor(7.7473e+09, device='cuda:0')
c= tensor(7.7584e+09, device='cuda:0')
c= tensor(7.7587e+09, device='cuda:0')
c= tensor(7.7595e+09, device='cuda:0')
c= tensor(7.7600e+09, device='cuda:0')
c= tensor(7.7602e+09, device='cuda:0')
c= tensor(7.7602e+09, device='cuda:0')
c= tensor(7.7602e+09, device='cuda:0')
c= tensor(7.7629e+09, device='cuda:0')
c= tensor(7.7634e+09, device='cuda:0')
c= tensor(7.7641e+09, device='cuda:0')
c= tensor(7.7641e+09, device='cuda:0')
c= tensor(7.7641e+09, device='cuda:0')
c= tensor(7.7652e+09, device='cuda:0')
c= tensor(7.7657e+09, device='cuda:0')
c= tensor(7.7661e+09, device='cuda:0')
c= tensor(7.7661e+09, device='cuda:0')
c= tensor(7.7661e+09, device='cuda:0')
c= tensor(7.7665e+09, device='cuda:0')
c= tensor(7.7667e+09, device='cuda:0')
c= tensor(7.7715e+09, device='cuda:0')
c= tensor(7.7715e+09, device='cuda:0')
c= tensor(7.7720e+09, device='cuda:0')
c= tensor(7.7731e+09, device='cuda:0')
c= tensor(7.7731e+09, device='cuda:0')
c= tensor(7.7934e+09, device='cuda:0')
c= tensor(7.7937e+09, device='cuda:0')
c= tensor(7.7964e+09, device='cuda:0')
c= tensor(7.7998e+09, device='cuda:0')
c= tensor(7.8012e+09, device='cuda:0')
c= tensor(7.8054e+09, device='cuda:0')
c= tensor(7.8098e+09, device='cuda:0')
c= tensor(7.8098e+09, device='cuda:0')
c= tensor(7.8108e+09, device='cuda:0')
c= tensor(7.8109e+09, device='cuda:0')
c= tensor(7.8154e+09, device='cuda:0')
c= tensor(7.8202e+09, device='cuda:0')
c= tensor(7.8204e+09, device='cuda:0')
c= tensor(7.8232e+09, device='cuda:0')
c= tensor(7.8233e+09, device='cuda:0')
c= tensor(7.8254e+09, device='cuda:0')
c= tensor(7.8256e+09, device='cuda:0')
c= tensor(7.8256e+09, device='cuda:0')
c= tensor(7.8385e+09, device='cuda:0')
c= tensor(8.0409e+09, device='cuda:0')
c= tensor(8.0413e+09, device='cuda:0')
c= tensor(8.0414e+09, device='cuda:0')
c= tensor(8.0416e+09, device='cuda:0')
c= tensor(8.0445e+09, device='cuda:0')
c= tensor(8.0450e+09, device='cuda:0')
c= tensor(8.0450e+09, device='cuda:0')
c= tensor(8.0457e+09, device='cuda:0')
c= tensor(8.0498e+09, device='cuda:0')
c= tensor(8.0498e+09, device='cuda:0')
c= tensor(8.0521e+09, device='cuda:0')
c= tensor(8.0524e+09, device='cuda:0')
memory (bytes)
5185474560
time for making loss 2 is 12.059659242630005
p0 True
it  0 : 2669042688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 22% |
shape of L is 
torch.Size([])
memory (bytes)
5185740800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  8% |
memory (bytes)
5186240512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  44292460000.0
relative error loss 5.5004954
shape of L is 
torch.Size([])
memory (bytes)
5339336704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 10% |
memory (bytes)
5339398144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  44292305000.0
relative error loss 5.500476
shape of L is 
torch.Size([])
memory (bytes)
5341261824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 10% |
memory (bytes)
5341261824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  44291584000.0
relative error loss 5.5003867
shape of L is 
torch.Size([])
memory (bytes)
5343133696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5343248384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  44288176000.0
relative error loss 5.4999633
shape of L is 
torch.Size([])
memory (bytes)
5345349632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5345382400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  44268884000.0
relative error loss 5.4975677
shape of L is 
torch.Size([])
memory (bytes)
5347581952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5347581952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  44162753000.0
relative error loss 5.4843874
shape of L is 
torch.Size([])
memory (bytes)
5349490688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
5349683200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  43011770000.0
relative error loss 5.3414516
shape of L is 
torch.Size([])
memory (bytes)
5351784448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5351809024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  37674426000.0
relative error loss 4.6786294
shape of L is 
torch.Size([])
memory (bytes)
5353836544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5353926656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  14318326000.0
relative error loss 1.778133
shape of L is 
torch.Size([])
memory (bytes)
5356064768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 11% |
memory (bytes)
5356064768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  10293403000.0
relative error loss 1.2782947
time to take a step is 204.11751508712769
it  1 : 3199857152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5358080000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
5358080000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  10293403000.0
relative error loss 1.2782947
shape of L is 
torch.Size([])
memory (bytes)
5360332800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5360332800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  9554964000.0
relative error loss 1.186591
shape of L is 
torch.Size([])
memory (bytes)
5362450432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5362450432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  8905507000.0
relative error loss 1.1059376
shape of L is 
torch.Size([])
memory (bytes)
5364490240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5364490240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  7730809300.0
relative error loss 0.9600569
shape of L is 
torch.Size([])
memory (bytes)
5366657024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5366689792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  10862673000.0
relative error loss 1.34899
shape of L is 
torch.Size([])
memory (bytes)
5368627200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5368856576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  7237916700.0
relative error loss 0.8988466
shape of L is 
torch.Size([])
memory (bytes)
5370957824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5370957824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  7004941000.0
relative error loss 0.8699143
shape of L is 
torch.Size([])
memory (bytes)
5373083648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
5373083648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6752941600.0
relative error loss 0.8386196
shape of L is 
torch.Size([])
memory (bytes)
5375180800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5375180800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6549697500.0
relative error loss 0.8133795
shape of L is 
torch.Size([])
memory (bytes)
5377200128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5377200128
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 12% |
error is  6440452000.0
relative error loss 0.7998128
time to take a step is 198.24694848060608
it  2 : 3351519232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5379293184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5379411968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  6440452000.0
relative error loss 0.7998128
shape of L is 
torch.Size([])
memory (bytes)
5381500928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5381509120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6134693400.0
relative error loss 0.7618419
shape of L is 
torch.Size([])
memory (bytes)
5383602176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5383602176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  5622761000.0
relative error loss 0.69826716
shape of L is 
torch.Size([])
memory (bytes)
5385748480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5385748480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  5278881000.0
relative error loss 0.6555621
shape of L is 
torch.Size([])
memory (bytes)
5387853824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5387853824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4758121500.0
relative error loss 0.5908912
shape of L is 
torch.Size([])
memory (bytes)
5389742080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5389967360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4413138000.0
relative error loss 0.54804915
shape of L is 
torch.Size([])
memory (bytes)
5392097280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5392097280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4166656500.0
relative error loss 0.51743966
shape of L is 
torch.Size([])
memory (bytes)
5394169856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5394169856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3797420300.0
relative error loss 0.47158575
shape of L is 
torch.Size([])
memory (bytes)
5396238336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5396238336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3590879700.0
relative error loss 0.44593635
shape of L is 
torch.Size([])
memory (bytes)
5398364160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5398364160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3367890000.0
relative error loss 0.41824415
time to take a step is 198.1864528656006
it  3 : 3351518720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5400600576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5400600576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 11% |
error is  3367890000.0
relative error loss 0.41824415
shape of L is 
torch.Size([])
memory (bytes)
5402492928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5402722304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  3176429000.0
relative error loss 0.39446744
shape of L is 
torch.Size([])
memory (bytes)
5404864512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5404864512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2982005200.0
relative error loss 0.37032276
shape of L is 
torch.Size([])
memory (bytes)
5406957568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5406957568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  3484795000.0
relative error loss 0.43276212
shape of L is 
torch.Size([])
memory (bytes)
5408841728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5408841728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2809840600.0
relative error loss 0.34894237
shape of L is 
torch.Size([])
memory (bytes)
5411209216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5411209216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2640599600.0
relative error loss 0.32792503
shape of L is 
torch.Size([])
memory (bytes)
5413343232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 12% |
memory (bytes)
5413376000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2508903000.0
relative error loss 0.31157017
shape of L is 
torch.Size([])
memory (bytes)
5415174144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5415440384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2273282600.0
relative error loss 0.28230944
shape of L is 
torch.Size([])
memory (bytes)
5417611264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5417611264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2096523300.0
relative error loss 0.26035845
shape of L is 
torch.Size([])
memory (bytes)
5419773952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5419773952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2007209500.0
relative error loss 0.24926695
time to take a step is 198.86245155334473
c= tensor(996.3505, device='cuda:0')
c= tensor(82457.7734, device='cuda:0')
c= tensor(96296.8828, device='cuda:0')
c= tensor(188882.3125, device='cuda:0')
c= tensor(241862.6562, device='cuda:0')
c= tensor(792405.1250, device='cuda:0')
c= tensor(1587705.5000, device='cuda:0')
c= tensor(2022298.3750, device='cuda:0')
c= tensor(2083178.6250, device='cuda:0')
c= tensor(3709692., device='cuda:0')
c= tensor(3727973.7500, device='cuda:0')
c= tensor(7435840., device='cuda:0')
c= tensor(7457749.5000, device='cuda:0')
c= tensor(44858076., device='cuda:0')
c= tensor(45071212., device='cuda:0')
c= tensor(46058012., device='cuda:0')
c= tensor(48420088., device='cuda:0')
c= tensor(48725360., device='cuda:0')
c= tensor(55846392., device='cuda:0')
c= tensor(61099936., device='cuda:0')
c= tensor(61680912., device='cuda:0')
c= tensor(80709864., device='cuda:0')
c= tensor(80753248., device='cuda:0')
c= tensor(81848368., device='cuda:0')
c= tensor(81854904., device='cuda:0')
c= tensor(83129672., device='cuda:0')
c= tensor(84541608., device='cuda:0')
c= tensor(84549680., device='cuda:0')
c= tensor(87061040., device='cuda:0')
c= tensor(3.6065e+08, device='cuda:0')
c= tensor(3.6071e+08, device='cuda:0')
c= tensor(5.8516e+08, device='cuda:0')
c= tensor(5.8563e+08, device='cuda:0')
c= tensor(5.8570e+08, device='cuda:0')
c= tensor(5.8586e+08, device='cuda:0')
c= tensor(6.0410e+08, device='cuda:0')
c= tensor(6.1095e+08, device='cuda:0')
c= tensor(6.1095e+08, device='cuda:0')
c= tensor(6.1096e+08, device='cuda:0')
c= tensor(6.1097e+08, device='cuda:0')
c= tensor(6.1098e+08, device='cuda:0')
c= tensor(6.1098e+08, device='cuda:0')
c= tensor(6.1099e+08, device='cuda:0')
c= tensor(6.1100e+08, device='cuda:0')
c= tensor(6.1100e+08, device='cuda:0')
c= tensor(6.1100e+08, device='cuda:0')
c= tensor(6.1101e+08, device='cuda:0')
c= tensor(6.1102e+08, device='cuda:0')
c= tensor(6.1102e+08, device='cuda:0')
c= tensor(6.1110e+08, device='cuda:0')
c= tensor(6.1116e+08, device='cuda:0')
c= tensor(6.1116e+08, device='cuda:0')
c= tensor(6.1119e+08, device='cuda:0')
c= tensor(6.1119e+08, device='cuda:0')
c= tensor(6.1121e+08, device='cuda:0')
c= tensor(6.1122e+08, device='cuda:0')
c= tensor(6.1123e+08, device='cuda:0')
c= tensor(6.1123e+08, device='cuda:0')
c= tensor(6.1124e+08, device='cuda:0')
c= tensor(6.1124e+08, device='cuda:0')
c= tensor(6.1125e+08, device='cuda:0')
c= tensor(6.1125e+08, device='cuda:0')
c= tensor(6.1141e+08, device='cuda:0')
c= tensor(6.1143e+08, device='cuda:0')
c= tensor(6.1143e+08, device='cuda:0')
c= tensor(6.1143e+08, device='cuda:0')
c= tensor(6.1143e+08, device='cuda:0')
c= tensor(6.1145e+08, device='cuda:0')
c= tensor(6.1146e+08, device='cuda:0')
c= tensor(6.1146e+08, device='cuda:0')
c= tensor(6.1147e+08, device='cuda:0')
c= tensor(6.1148e+08, device='cuda:0')
c= tensor(6.1148e+08, device='cuda:0')
c= tensor(6.1148e+08, device='cuda:0')
c= tensor(6.1149e+08, device='cuda:0')
c= tensor(6.1150e+08, device='cuda:0')
c= tensor(6.1150e+08, device='cuda:0')
c= tensor(6.1150e+08, device='cuda:0')
c= tensor(6.1151e+08, device='cuda:0')
c= tensor(6.1166e+08, device='cuda:0')
c= tensor(6.1167e+08, device='cuda:0')
c= tensor(6.1167e+08, device='cuda:0')
c= tensor(6.1176e+08, device='cuda:0')
c= tensor(6.1176e+08, device='cuda:0')
c= tensor(6.1176e+08, device='cuda:0')
c= tensor(6.1176e+08, device='cuda:0')
c= tensor(6.1177e+08, device='cuda:0')
c= tensor(6.1177e+08, device='cuda:0')
c= tensor(6.1179e+08, device='cuda:0')
c= tensor(6.1179e+08, device='cuda:0')
c= tensor(6.1180e+08, device='cuda:0')
c= tensor(6.1180e+08, device='cuda:0')
c= tensor(6.1180e+08, device='cuda:0')
c= tensor(6.1180e+08, device='cuda:0')
c= tensor(6.1182e+08, device='cuda:0')
c= tensor(6.1184e+08, device='cuda:0')
c= tensor(6.1184e+08, device='cuda:0')
c= tensor(6.1189e+08, device='cuda:0')
c= tensor(6.1190e+08, device='cuda:0')
c= tensor(6.1191e+08, device='cuda:0')
c= tensor(6.1195e+08, device='cuda:0')
c= tensor(6.1196e+08, device='cuda:0')
c= tensor(6.1197e+08, device='cuda:0')
c= tensor(6.1197e+08, device='cuda:0')
c= tensor(6.1199e+08, device='cuda:0')
c= tensor(6.1199e+08, device='cuda:0')
c= tensor(6.1200e+08, device='cuda:0')
c= tensor(6.1201e+08, device='cuda:0')
c= tensor(6.1201e+08, device='cuda:0')
c= tensor(6.1201e+08, device='cuda:0')
c= tensor(6.1201e+08, device='cuda:0')
c= tensor(6.1202e+08, device='cuda:0')
c= tensor(6.1202e+08, device='cuda:0')
c= tensor(6.1202e+08, device='cuda:0')
c= tensor(6.1203e+08, device='cuda:0')
c= tensor(6.1203e+08, device='cuda:0')
c= tensor(6.1203e+08, device='cuda:0')
c= tensor(6.1203e+08, device='cuda:0')
c= tensor(6.1206e+08, device='cuda:0')
c= tensor(6.1206e+08, device='cuda:0')
c= tensor(6.1210e+08, device='cuda:0')
c= tensor(6.1210e+08, device='cuda:0')
c= tensor(6.1210e+08, device='cuda:0')
c= tensor(6.1210e+08, device='cuda:0')
c= tensor(6.1211e+08, device='cuda:0')
c= tensor(6.1211e+08, device='cuda:0')
c= tensor(6.1211e+08, device='cuda:0')
c= tensor(6.1211e+08, device='cuda:0')
c= tensor(6.1230e+08, device='cuda:0')
c= tensor(6.1230e+08, device='cuda:0')
c= tensor(6.1232e+08, device='cuda:0')
c= tensor(6.1233e+08, device='cuda:0')
c= tensor(6.1233e+08, device='cuda:0')
c= tensor(6.1233e+08, device='cuda:0')
c= tensor(6.1237e+08, device='cuda:0')
c= tensor(6.1238e+08, device='cuda:0')
c= tensor(6.1238e+08, device='cuda:0')
c= tensor(6.1238e+08, device='cuda:0')
c= tensor(6.1238e+08, device='cuda:0')
c= tensor(6.1238e+08, device='cuda:0')
c= tensor(6.1240e+08, device='cuda:0')
c= tensor(6.1240e+08, device='cuda:0')
c= tensor(6.1255e+08, device='cuda:0')
c= tensor(6.1257e+08, device='cuda:0')
c= tensor(6.1264e+08, device='cuda:0')
c= tensor(6.1264e+08, device='cuda:0')
c= tensor(6.1264e+08, device='cuda:0')
c= tensor(6.1265e+08, device='cuda:0')
c= tensor(6.1265e+08, device='cuda:0')
c= tensor(6.1265e+08, device='cuda:0')
c= tensor(6.1266e+08, device='cuda:0')
c= tensor(6.1268e+08, device='cuda:0')
c= tensor(6.1269e+08, device='cuda:0')
c= tensor(6.1274e+08, device='cuda:0')
c= tensor(6.1275e+08, device='cuda:0')
c= tensor(6.1307e+08, device='cuda:0')
c= tensor(6.1307e+08, device='cuda:0')
c= tensor(6.1307e+08, device='cuda:0')
c= tensor(6.1309e+08, device='cuda:0')
c= tensor(6.1309e+08, device='cuda:0')
c= tensor(6.1310e+08, device='cuda:0')
c= tensor(6.1310e+08, device='cuda:0')
c= tensor(6.1311e+08, device='cuda:0')
c= tensor(6.1311e+08, device='cuda:0')
c= tensor(6.1313e+08, device='cuda:0')
c= tensor(6.1313e+08, device='cuda:0')
c= tensor(6.1313e+08, device='cuda:0')
c= tensor(6.1313e+08, device='cuda:0')
c= tensor(6.1315e+08, device='cuda:0')
c= tensor(6.1315e+08, device='cuda:0')
c= tensor(6.1315e+08, device='cuda:0')
c= tensor(6.1315e+08, device='cuda:0')
c= tensor(6.1317e+08, device='cuda:0')
c= tensor(6.1318e+08, device='cuda:0')
c= tensor(6.1319e+08, device='cuda:0')
c= tensor(6.1324e+08, device='cuda:0')
c= tensor(6.1326e+08, device='cuda:0')
c= tensor(6.1326e+08, device='cuda:0')
c= tensor(6.1328e+08, device='cuda:0')
c= tensor(6.1328e+08, device='cuda:0')
c= tensor(6.1328e+08, device='cuda:0')
c= tensor(6.1329e+08, device='cuda:0')
c= tensor(6.1331e+08, device='cuda:0')
c= tensor(6.1331e+08, device='cuda:0')
c= tensor(6.1332e+08, device='cuda:0')
c= tensor(6.1333e+08, device='cuda:0')
c= tensor(6.1334e+08, device='cuda:0')
c= tensor(6.1336e+08, device='cuda:0')
c= tensor(6.1338e+08, device='cuda:0')
c= tensor(6.1346e+08, device='cuda:0')
c= tensor(6.1347e+08, device='cuda:0')
c= tensor(6.1347e+08, device='cuda:0')
c= tensor(6.1347e+08, device='cuda:0')
c= tensor(6.1349e+08, device='cuda:0')
c= tensor(6.1350e+08, device='cuda:0')
c= tensor(6.1351e+08, device='cuda:0')
c= tensor(6.1351e+08, device='cuda:0')
c= tensor(6.1351e+08, device='cuda:0')
c= tensor(6.1352e+08, device='cuda:0')
c= tensor(6.1352e+08, device='cuda:0')
c= tensor(6.1353e+08, device='cuda:0')
c= tensor(6.1353e+08, device='cuda:0')
c= tensor(6.1355e+08, device='cuda:0')
c= tensor(6.1358e+08, device='cuda:0')
c= tensor(6.1359e+08, device='cuda:0')
c= tensor(6.1359e+08, device='cuda:0')
c= tensor(6.1362e+08, device='cuda:0')
c= tensor(6.1363e+08, device='cuda:0')
c= tensor(6.1363e+08, device='cuda:0')
c= tensor(6.1368e+08, device='cuda:0')
c= tensor(6.1370e+08, device='cuda:0')
c= tensor(6.1370e+08, device='cuda:0')
c= tensor(6.1370e+08, device='cuda:0')
c= tensor(6.1371e+08, device='cuda:0')
c= tensor(6.1371e+08, device='cuda:0')
c= tensor(6.1371e+08, device='cuda:0')
c= tensor(6.1371e+08, device='cuda:0')
c= tensor(6.1371e+08, device='cuda:0')
c= tensor(6.1373e+08, device='cuda:0')
c= tensor(6.1373e+08, device='cuda:0')
c= tensor(6.1374e+08, device='cuda:0')
c= tensor(6.1374e+08, device='cuda:0')
c= tensor(6.1375e+08, device='cuda:0')
c= tensor(6.1375e+08, device='cuda:0')
c= tensor(6.1376e+08, device='cuda:0')
c= tensor(6.1377e+08, device='cuda:0')
c= tensor(6.1378e+08, device='cuda:0')
c= tensor(6.1378e+08, device='cuda:0')
c= tensor(6.1380e+08, device='cuda:0')
c= tensor(6.1380e+08, device='cuda:0')
c= tensor(6.1380e+08, device='cuda:0')
c= tensor(6.1380e+08, device='cuda:0')
c= tensor(6.1381e+08, device='cuda:0')
c= tensor(6.1383e+08, device='cuda:0')
c= tensor(6.1383e+08, device='cuda:0')
c= tensor(6.1383e+08, device='cuda:0')
c= tensor(6.1383e+08, device='cuda:0')
c= tensor(6.1386e+08, device='cuda:0')
c= tensor(6.1386e+08, device='cuda:0')
c= tensor(6.1402e+08, device='cuda:0')
c= tensor(6.1592e+08, device='cuda:0')
c= tensor(6.1593e+08, device='cuda:0')
c= tensor(6.1594e+08, device='cuda:0')
c= tensor(6.1594e+08, device='cuda:0')
c= tensor(6.1595e+08, device='cuda:0')
c= tensor(6.3027e+08, device='cuda:0')
c= tensor(6.7197e+08, device='cuda:0')
c= tensor(6.7198e+08, device='cuda:0')
c= tensor(6.7761e+08, device='cuda:0')
c= tensor(6.7940e+08, device='cuda:0')
c= tensor(6.7956e+08, device='cuda:0')
c= tensor(7.0819e+08, device='cuda:0')
c= tensor(7.0819e+08, device='cuda:0')
c= tensor(7.0819e+08, device='cuda:0')
c= tensor(7.1407e+08, device='cuda:0')
c= tensor(7.9042e+08, device='cuda:0')
c= tensor(7.9045e+08, device='cuda:0')
c= tensor(7.9081e+08, device='cuda:0')
c= tensor(7.9102e+08, device='cuda:0')
c= tensor(7.9278e+08, device='cuda:0')
c= tensor(7.9826e+08, device='cuda:0')
c= tensor(7.9994e+08, device='cuda:0')
c= tensor(8.0091e+08, device='cuda:0')
c= tensor(8.0104e+08, device='cuda:0')
c= tensor(8.0110e+08, device='cuda:0')
c= tensor(8.1556e+08, device='cuda:0')
c= tensor(8.1560e+08, device='cuda:0')
c= tensor(8.1560e+08, device='cuda:0')
c= tensor(8.1579e+08, device='cuda:0')
c= tensor(8.1764e+08, device='cuda:0')
c= tensor(8.2815e+08, device='cuda:0')
c= tensor(8.2962e+08, device='cuda:0')
c= tensor(8.2963e+08, device='cuda:0')
c= tensor(8.3033e+08, device='cuda:0')
c= tensor(8.3041e+08, device='cuda:0')
c= tensor(8.3136e+08, device='cuda:0')
c= tensor(8.3402e+08, device='cuda:0')
c= tensor(8.3546e+08, device='cuda:0')
c= tensor(8.3715e+08, device='cuda:0')
c= tensor(8.3715e+08, device='cuda:0')
c= tensor(8.3717e+08, device='cuda:0')
c= tensor(8.4093e+08, device='cuda:0')
c= tensor(8.4193e+08, device='cuda:0')
c= tensor(8.4407e+08, device='cuda:0')
c= tensor(8.4409e+08, device='cuda:0')
c= tensor(8.6813e+08, device='cuda:0')
c= tensor(8.6816e+08, device='cuda:0')
c= tensor(8.6849e+08, device='cuda:0')
c= tensor(8.7191e+08, device='cuda:0')
c= tensor(8.7192e+08, device='cuda:0')
c= tensor(8.7600e+08, device='cuda:0')
c= tensor(8.9909e+08, device='cuda:0')
c= tensor(9.4073e+08, device='cuda:0')
c= tensor(9.4093e+08, device='cuda:0')
c= tensor(9.4110e+08, device='cuda:0')
c= tensor(9.4112e+08, device='cuda:0')
c= tensor(9.4112e+08, device='cuda:0')
c= tensor(9.4361e+08, device='cuda:0')
c= tensor(9.4363e+08, device='cuda:0')
c= tensor(9.4406e+08, device='cuda:0')
c= tensor(9.4776e+08, device='cuda:0')
c= tensor(9.4795e+08, device='cuda:0')
c= tensor(9.4829e+08, device='cuda:0')
c= tensor(9.4831e+08, device='cuda:0')
c= tensor(9.5451e+08, device='cuda:0')
c= tensor(9.5724e+08, device='cuda:0')
c= tensor(9.5788e+08, device='cuda:0')
c= tensor(9.5802e+08, device='cuda:0')
c= tensor(9.8873e+08, device='cuda:0')
c= tensor(9.8888e+08, device='cuda:0')
c= tensor(1.0079e+09, device='cuda:0')
c= tensor(1.0079e+09, device='cuda:0')
c= tensor(1.0123e+09, device='cuda:0')
c= tensor(1.0125e+09, device='cuda:0')
c= tensor(1.0326e+09, device='cuda:0')
c= tensor(1.0334e+09, device='cuda:0')
c= tensor(1.0335e+09, device='cuda:0')
c= tensor(1.0374e+09, device='cuda:0')
c= tensor(1.0412e+09, device='cuda:0')
c= tensor(1.0412e+09, device='cuda:0')
c= tensor(1.0475e+09, device='cuda:0')
c= tensor(1.0547e+09, device='cuda:0')
c= tensor(1.0817e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.0837e+09, device='cuda:0')
c= tensor(1.0839e+09, device='cuda:0')
c= tensor(1.0847e+09, device='cuda:0')
c= tensor(1.0847e+09, device='cuda:0')
c= tensor(1.0863e+09, device='cuda:0')
c= tensor(1.0898e+09, device='cuda:0')
c= tensor(1.0899e+09, device='cuda:0')
c= tensor(1.0899e+09, device='cuda:0')
c= tensor(1.0904e+09, device='cuda:0')
c= tensor(1.0906e+09, device='cuda:0')
c= tensor(1.0907e+09, device='cuda:0')
c= tensor(1.0907e+09, device='cuda:0')
c= tensor(1.0907e+09, device='cuda:0')
c= tensor(1.0913e+09, device='cuda:0')
c= tensor(1.0916e+09, device='cuda:0')
c= tensor(1.0917e+09, device='cuda:0')
c= tensor(1.0927e+09, device='cuda:0')
c= tensor(1.0928e+09, device='cuda:0')
c= tensor(1.4537e+09, device='cuda:0')
c= tensor(1.4538e+09, device='cuda:0')
c= tensor(1.4570e+09, device='cuda:0')
c= tensor(1.4570e+09, device='cuda:0')
c= tensor(1.4570e+09, device='cuda:0')
c= tensor(1.4570e+09, device='cuda:0')
c= tensor(1.4571e+09, device='cuda:0')
c= tensor(1.4571e+09, device='cuda:0')
c= tensor(1.4614e+09, device='cuda:0')
c= tensor(1.4614e+09, device='cuda:0')
c= tensor(1.4614e+09, device='cuda:0')
c= tensor(1.4905e+09, device='cuda:0')
c= tensor(1.4914e+09, device='cuda:0')
c= tensor(1.4922e+09, device='cuda:0')
c= tensor(1.4959e+09, device='cuda:0')
c= tensor(1.5127e+09, device='cuda:0')
c= tensor(1.5127e+09, device='cuda:0')
c= tensor(1.5127e+09, device='cuda:0')
c= tensor(1.5129e+09, device='cuda:0')
c= tensor(1.5129e+09, device='cuda:0')
c= tensor(1.5129e+09, device='cuda:0')
c= tensor(1.5129e+09, device='cuda:0')
c= tensor(1.5129e+09, device='cuda:0')
c= tensor(1.5130e+09, device='cuda:0')
c= tensor(1.5130e+09, device='cuda:0')
c= tensor(1.5130e+09, device='cuda:0')
c= tensor(1.5375e+09, device='cuda:0')
c= tensor(1.5376e+09, device='cuda:0')
c= tensor(1.5394e+09, device='cuda:0')
c= tensor(1.5394e+09, device='cuda:0')
c= tensor(1.5394e+09, device='cuda:0')
c= tensor(1.5405e+09, device='cuda:0')
c= tensor(1.6700e+09, device='cuda:0')
c= tensor(1.7194e+09, device='cuda:0')
c= tensor(1.7195e+09, device='cuda:0')
c= tensor(1.7237e+09, device='cuda:0')
c= tensor(1.7237e+09, device='cuda:0')
c= tensor(1.7238e+09, device='cuda:0')
c= tensor(1.7551e+09, device='cuda:0')
c= tensor(1.7553e+09, device='cuda:0')
c= tensor(1.7556e+09, device='cuda:0')
c= tensor(1.7567e+09, device='cuda:0')
c= tensor(1.9778e+09, device='cuda:0')
c= tensor(1.9784e+09, device='cuda:0')
c= tensor(1.9785e+09, device='cuda:0')
c= tensor(1.9788e+09, device='cuda:0')
c= tensor(1.9788e+09, device='cuda:0')
c= tensor(1.9788e+09, device='cuda:0')
c= tensor(1.9897e+09, device='cuda:0')
c= tensor(1.9898e+09, device='cuda:0')
c= tensor(1.9898e+09, device='cuda:0')
c= tensor(1.9906e+09, device='cuda:0')
c= tensor(1.9907e+09, device='cuda:0')
c= tensor(1.9907e+09, device='cuda:0')
c= tensor(1.9933e+09, device='cuda:0')
c= tensor(1.9973e+09, device='cuda:0')
c= tensor(2.0071e+09, device='cuda:0')
c= tensor(2.0193e+09, device='cuda:0')
c= tensor(2.0303e+09, device='cuda:0')
c= tensor(2.0305e+09, device='cuda:0')
c= tensor(2.0307e+09, device='cuda:0')
c= tensor(2.0328e+09, device='cuda:0')
c= tensor(2.0378e+09, device='cuda:0')
c= tensor(2.0378e+09, device='cuda:0')
c= tensor(2.0449e+09, device='cuda:0')
c= tensor(2.0954e+09, device='cuda:0')
c= tensor(2.1106e+09, device='cuda:0')
c= tensor(2.1133e+09, device='cuda:0')
c= tensor(2.1157e+09, device='cuda:0')
c= tensor(2.1158e+09, device='cuda:0')
c= tensor(2.1158e+09, device='cuda:0')
c= tensor(2.1160e+09, device='cuda:0')
c= tensor(2.1190e+09, device='cuda:0')
c= tensor(2.1218e+09, device='cuda:0')
c= tensor(2.1897e+09, device='cuda:0')
c= tensor(2.1954e+09, device='cuda:0')
c= tensor(2.1999e+09, device='cuda:0')
c= tensor(2.2002e+09, device='cuda:0')
c= tensor(2.2057e+09, device='cuda:0')
c= tensor(2.2057e+09, device='cuda:0')
c= tensor(2.2059e+09, device='cuda:0')
c= tensor(2.2137e+09, device='cuda:0')
c= tensor(2.2143e+09, device='cuda:0')
c= tensor(2.2143e+09, device='cuda:0')
c= tensor(2.2145e+09, device='cuda:0')
c= tensor(2.2592e+09, device='cuda:0')
c= tensor(2.2594e+09, device='cuda:0')
c= tensor(2.2625e+09, device='cuda:0')
c= tensor(2.2625e+09, device='cuda:0')
c= tensor(2.2625e+09, device='cuda:0')
c= tensor(2.2626e+09, device='cuda:0')
c= tensor(2.2626e+09, device='cuda:0')
c= tensor(2.2639e+09, device='cuda:0')
c= tensor(2.2652e+09, device='cuda:0')
c= tensor(2.2652e+09, device='cuda:0')
c= tensor(2.2718e+09, device='cuda:0')
c= tensor(2.2718e+09, device='cuda:0')
c= tensor(2.2731e+09, device='cuda:0')
c= tensor(2.2732e+09, device='cuda:0')
c= tensor(2.2739e+09, device='cuda:0')
c= tensor(2.2739e+09, device='cuda:0')
c= tensor(2.2741e+09, device='cuda:0')
c= tensor(2.2742e+09, device='cuda:0')
c= tensor(2.2747e+09, device='cuda:0')
c= tensor(2.2768e+09, device='cuda:0')
c= tensor(2.3422e+09, device='cuda:0')
c= tensor(2.3422e+09, device='cuda:0')
c= tensor(2.3423e+09, device='cuda:0')
c= tensor(2.3495e+09, device='cuda:0')
c= tensor(2.3495e+09, device='cuda:0')
c= tensor(2.3921e+09, device='cuda:0')
c= tensor(2.3921e+09, device='cuda:0')
c= tensor(2.3947e+09, device='cuda:0')
c= tensor(2.4091e+09, device='cuda:0')
c= tensor(2.4091e+09, device='cuda:0')
c= tensor(2.4336e+09, device='cuda:0')
c= tensor(2.4340e+09, device='cuda:0')
c= tensor(2.5333e+09, device='cuda:0')
c= tensor(2.5333e+09, device='cuda:0')
c= tensor(2.5333e+09, device='cuda:0')
c= tensor(2.5334e+09, device='cuda:0')
c= tensor(2.5334e+09, device='cuda:0')
c= tensor(2.5335e+09, device='cuda:0')
c= tensor(2.5342e+09, device='cuda:0')
c= tensor(2.5346e+09, device='cuda:0')
c= tensor(2.5382e+09, device='cuda:0')
c= tensor(2.5383e+09, device='cuda:0')
c= tensor(2.5383e+09, device='cuda:0')
c= tensor(2.5384e+09, device='cuda:0')
c= tensor(2.5749e+09, device='cuda:0')
c= tensor(2.5848e+09, device='cuda:0')
c= tensor(2.6196e+09, device='cuda:0')
c= tensor(2.6198e+09, device='cuda:0')
c= tensor(2.6199e+09, device='cuda:0')
c= tensor(2.6199e+09, device='cuda:0')
c= tensor(2.6200e+09, device='cuda:0')
c= tensor(2.6440e+09, device='cuda:0')
c= tensor(2.6440e+09, device='cuda:0')
c= tensor(2.6441e+09, device='cuda:0')
c= tensor(2.6449e+09, device='cuda:0')
c= tensor(2.6460e+09, device='cuda:0')
c= tensor(2.6461e+09, device='cuda:0')
c= tensor(2.6462e+09, device='cuda:0')
c= tensor(2.6960e+09, device='cuda:0')
c= tensor(2.6967e+09, device='cuda:0')
c= tensor(2.7005e+09, device='cuda:0')
c= tensor(2.7006e+09, device='cuda:0')
c= tensor(2.7135e+09, device='cuda:0')
c= tensor(2.7291e+09, device='cuda:0')
c= tensor(2.7983e+09, device='cuda:0')
c= tensor(2.8087e+09, device='cuda:0')
c= tensor(2.8090e+09, device='cuda:0')
c= tensor(2.8095e+09, device='cuda:0')
c= tensor(2.8100e+09, device='cuda:0')
c= tensor(2.8100e+09, device='cuda:0')
c= tensor(2.8100e+09, device='cuda:0')
c= tensor(2.8101e+09, device='cuda:0')
c= tensor(2.8111e+09, device='cuda:0')
c= tensor(2.8114e+09, device='cuda:0')
c= tensor(2.8114e+09, device='cuda:0')
c= tensor(2.8115e+09, device='cuda:0')
c= tensor(2.8115e+09, device='cuda:0')
c= tensor(2.8133e+09, device='cuda:0')
c= tensor(2.8133e+09, device='cuda:0')
c= tensor(2.8136e+09, device='cuda:0')
c= tensor(2.8142e+09, device='cuda:0')
c= tensor(2.8144e+09, device='cuda:0')
c= tensor(2.8144e+09, device='cuda:0')
c= tensor(2.8144e+09, device='cuda:0')
c= tensor(2.8146e+09, device='cuda:0')
c= tensor(2.8225e+09, device='cuda:0')
c= tensor(2.8225e+09, device='cuda:0')
c= tensor(2.8225e+09, device='cuda:0')
c= tensor(2.8225e+09, device='cuda:0')
c= tensor(2.8492e+09, device='cuda:0')
c= tensor(2.9271e+09, device='cuda:0')
c= tensor(2.9282e+09, device='cuda:0')
c= tensor(2.9282e+09, device='cuda:0')
c= tensor(2.9374e+09, device='cuda:0')
c= tensor(2.9408e+09, device='cuda:0')
c= tensor(2.9408e+09, device='cuda:0')
c= tensor(2.9408e+09, device='cuda:0')
c= tensor(2.9410e+09, device='cuda:0')
c= tensor(2.9515e+09, device='cuda:0')
c= tensor(2.9594e+09, device='cuda:0')
c= tensor(2.9687e+09, device='cuda:0')
c= tensor(2.9688e+09, device='cuda:0')
c= tensor(2.9688e+09, device='cuda:0')
c= tensor(2.9688e+09, device='cuda:0')
c= tensor(2.9696e+09, device='cuda:0')
c= tensor(2.9696e+09, device='cuda:0')
c= tensor(2.9699e+09, device='cuda:0')
c= tensor(2.9885e+09, device='cuda:0')
c= tensor(3.0373e+09, device='cuda:0')
c= tensor(3.0373e+09, device='cuda:0')
c= tensor(3.0373e+09, device='cuda:0')
c= tensor(3.0374e+09, device='cuda:0')
c= tensor(3.2767e+09, device='cuda:0')
c= tensor(3.2790e+09, device='cuda:0')
c= tensor(3.2791e+09, device='cuda:0')
c= tensor(3.2791e+09, device='cuda:0')
c= tensor(3.2900e+09, device='cuda:0')
c= tensor(3.2900e+09, device='cuda:0')
c= tensor(3.2914e+09, device='cuda:0')
c= tensor(3.2914e+09, device='cuda:0')
c= tensor(3.2914e+09, device='cuda:0')
c= tensor(3.2914e+09, device='cuda:0')
c= tensor(3.2915e+09, device='cuda:0')
c= tensor(3.2915e+09, device='cuda:0')
c= tensor(3.2970e+09, device='cuda:0')
c= tensor(3.3239e+09, device='cuda:0')
c= tensor(3.3342e+09, device='cuda:0')
c= tensor(3.3415e+09, device='cuda:0')
c= tensor(3.3417e+09, device='cuda:0')
c= tensor(3.3418e+09, device='cuda:0')
c= tensor(3.3418e+09, device='cuda:0')
c= tensor(3.3445e+09, device='cuda:0')
c= tensor(3.3454e+09, device='cuda:0')
c= tensor(3.3560e+09, device='cuda:0')
c= tensor(3.3560e+09, device='cuda:0')
c= tensor(3.6609e+09, device='cuda:0')
c= tensor(3.6610e+09, device='cuda:0')
c= tensor(3.6622e+09, device='cuda:0')
c= tensor(3.6720e+09, device='cuda:0')
c= tensor(3.6724e+09, device='cuda:0')
c= tensor(3.6812e+09, device='cuda:0')
c= tensor(3.7636e+09, device='cuda:0')
c= tensor(3.7673e+09, device='cuda:0')
c= tensor(3.7688e+09, device='cuda:0')
c= tensor(3.7688e+09, device='cuda:0')
c= tensor(3.7694e+09, device='cuda:0')
c= tensor(3.7695e+09, device='cuda:0')
c= tensor(3.7826e+09, device='cuda:0')
c= tensor(3.9404e+09, device='cuda:0')
c= tensor(3.9478e+09, device='cuda:0')
c= tensor(3.9625e+09, device='cuda:0')
c= tensor(3.9653e+09, device='cuda:0')
c= tensor(3.9657e+09, device='cuda:0')
c= tensor(3.9666e+09, device='cuda:0')
c= tensor(3.9666e+09, device='cuda:0')
c= tensor(3.9678e+09, device='cuda:0')
c= tensor(3.9833e+09, device='cuda:0')
c= tensor(3.9843e+09, device='cuda:0')
c= tensor(4.0888e+09, device='cuda:0')
c= tensor(4.1524e+09, device='cuda:0')
c= tensor(4.1529e+09, device='cuda:0')
c= tensor(4.1529e+09, device='cuda:0')
c= tensor(4.1609e+09, device='cuda:0')
c= tensor(4.1628e+09, device='cuda:0')
c= tensor(4.1629e+09, device='cuda:0')
c= tensor(4.1924e+09, device='cuda:0')
c= tensor(4.1936e+09, device='cuda:0')
c= tensor(4.1950e+09, device='cuda:0')
c= tensor(4.1950e+09, device='cuda:0')
c= tensor(4.1951e+09, device='cuda:0')
c= tensor(4.1951e+09, device='cuda:0')
c= tensor(4.1951e+09, device='cuda:0')
c= tensor(4.1953e+09, device='cuda:0')
c= tensor(4.1961e+09, device='cuda:0')
c= tensor(7.4218e+09, device='cuda:0')
c= tensor(7.4221e+09, device='cuda:0')
c= tensor(7.4232e+09, device='cuda:0')
c= tensor(7.4233e+09, device='cuda:0')
c= tensor(7.4233e+09, device='cuda:0')
c= tensor(7.4235e+09, device='cuda:0')
c= tensor(7.4438e+09, device='cuda:0')
c= tensor(7.4446e+09, device='cuda:0')
c= tensor(7.5523e+09, device='cuda:0')
c= tensor(7.5524e+09, device='cuda:0')
c= tensor(7.5661e+09, device='cuda:0')
c= tensor(7.5694e+09, device='cuda:0')
c= tensor(7.5762e+09, device='cuda:0')
c= tensor(7.6006e+09, device='cuda:0')
c= tensor(7.6007e+09, device='cuda:0')
c= tensor(7.6007e+09, device='cuda:0')
c= tensor(7.6018e+09, device='cuda:0')
c= tensor(7.6019e+09, device='cuda:0')
c= tensor(7.6021e+09, device='cuda:0')
c= tensor(7.6072e+09, device='cuda:0')
c= tensor(7.6088e+09, device='cuda:0')
c= tensor(7.6111e+09, device='cuda:0')
c= tensor(7.6116e+09, device='cuda:0')
c= tensor(7.6296e+09, device='cuda:0')
c= tensor(7.6796e+09, device='cuda:0')
c= tensor(7.6803e+09, device='cuda:0')
c= tensor(7.6804e+09, device='cuda:0')
c= tensor(7.6866e+09, device='cuda:0')
c= tensor(7.6884e+09, device='cuda:0')
c= tensor(7.6982e+09, device='cuda:0')
c= tensor(7.7005e+09, device='cuda:0')
c= tensor(7.7014e+09, device='cuda:0')
c= tensor(7.7022e+09, device='cuda:0')
c= tensor(7.7150e+09, device='cuda:0')
c= tensor(7.7207e+09, device='cuda:0')
c= tensor(7.7208e+09, device='cuda:0')
c= tensor(7.7208e+09, device='cuda:0')
c= tensor(7.7209e+09, device='cuda:0')
c= tensor(7.7243e+09, device='cuda:0')
c= tensor(7.7280e+09, device='cuda:0')
c= tensor(7.7468e+09, device='cuda:0')
c= tensor(7.7468e+09, device='cuda:0')
c= tensor(7.7470e+09, device='cuda:0')
c= tensor(7.7473e+09, device='cuda:0')
c= tensor(7.7584e+09, device='cuda:0')
c= tensor(7.7587e+09, device='cuda:0')
c= tensor(7.7595e+09, device='cuda:0')
c= tensor(7.7600e+09, device='cuda:0')
c= tensor(7.7602e+09, device='cuda:0')
c= tensor(7.7602e+09, device='cuda:0')
c= tensor(7.7602e+09, device='cuda:0')
c= tensor(7.7629e+09, device='cuda:0')
c= tensor(7.7634e+09, device='cuda:0')
c= tensor(7.7641e+09, device='cuda:0')
c= tensor(7.7641e+09, device='cuda:0')
c= tensor(7.7641e+09, device='cuda:0')
c= tensor(7.7652e+09, device='cuda:0')
c= tensor(7.7657e+09, device='cuda:0')
c= tensor(7.7661e+09, device='cuda:0')
c= tensor(7.7661e+09, device='cuda:0')
c= tensor(7.7661e+09, device='cuda:0')
c= tensor(7.7665e+09, device='cuda:0')
c= tensor(7.7667e+09, device='cuda:0')
c= tensor(7.7715e+09, device='cuda:0')
c= tensor(7.7715e+09, device='cuda:0')
c= tensor(7.7720e+09, device='cuda:0')
c= tensor(7.7731e+09, device='cuda:0')
c= tensor(7.7731e+09, device='cuda:0')
c= tensor(7.7934e+09, device='cuda:0')
c= tensor(7.7937e+09, device='cuda:0')
c= tensor(7.7964e+09, device='cuda:0')
c= tensor(7.7998e+09, device='cuda:0')
c= tensor(7.8012e+09, device='cuda:0')
c= tensor(7.8054e+09, device='cuda:0')
c= tensor(7.8098e+09, device='cuda:0')
c= tensor(7.8098e+09, device='cuda:0')
c= tensor(7.8108e+09, device='cuda:0')
c= tensor(7.8109e+09, device='cuda:0')
c= tensor(7.8154e+09, device='cuda:0')
c= tensor(7.8202e+09, device='cuda:0')
c= tensor(7.8204e+09, device='cuda:0')
c= tensor(7.8232e+09, device='cuda:0')
c= tensor(7.8233e+09, device='cuda:0')
c= tensor(7.8254e+09, device='cuda:0')
c= tensor(7.8256e+09, device='cuda:0')
c= tensor(7.8256e+09, device='cuda:0')
c= tensor(7.8385e+09, device='cuda:0')
c= tensor(8.0409e+09, device='cuda:0')
c= tensor(8.0413e+09, device='cuda:0')
c= tensor(8.0414e+09, device='cuda:0')
c= tensor(8.0416e+09, device='cuda:0')
c= tensor(8.0445e+09, device='cuda:0')
c= tensor(8.0450e+09, device='cuda:0')
c= tensor(8.0450e+09, device='cuda:0')
c= tensor(8.0457e+09, device='cuda:0')
c= tensor(8.0498e+09, device='cuda:0')
c= tensor(8.0498e+09, device='cuda:0')
c= tensor(8.0521e+09, device='cuda:0')
c= tensor(8.0524e+09, device='cuda:0')
time to make c is 9.118221044540405
time for making loss is 9.118331670761108
p0 True
it  0 : 2669367296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5421699072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 10% |
memory (bytes)
5422174208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2007209500.0
relative error loss 0.24926695
shape of L is 
torch.Size([])
memory (bytes)
5448384512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5448384512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1985839600.0
relative error loss 0.24661312
shape of L is 
torch.Size([])
memory (bytes)
5451685888
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5451898880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1933769700.0
relative error loss 0.24014677
shape of L is 
torch.Size([])
memory (bytes)
5455151104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
5455151104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1913759700.0
relative error loss 0.23766182
shape of L is 
torch.Size([])
memory (bytes)
5458321408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5458321408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1899058700.0
relative error loss 0.23583616
shape of L is 
torch.Size([])
memory (bytes)
5461536768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5461536768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1880482300.0
relative error loss 0.23352924
shape of L is 
torch.Size([])
memory (bytes)
5464522752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5464735744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1868569600.0
relative error loss 0.23204984
shape of L is 
torch.Size([])
memory (bytes)
5467934720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5467934720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1857929200.0
relative error loss 0.23072846
shape of L is 
torch.Size([])
memory (bytes)
5470912512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5471121408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1851356700.0
relative error loss 0.22991224
shape of L is 
torch.Size([])
memory (bytes)
5474332672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5474332672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1844376600.0
relative error loss 0.22904542
time to take a step is 264.71681928634644
it  1 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5477462016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5477527552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1844376600.0
relative error loss 0.22904542
shape of L is 
torch.Size([])
memory (bytes)
5480726528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5480726528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1836776000.0
relative error loss 0.22810152
shape of L is 
torch.Size([])
memory (bytes)
5483884544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5483884544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1830435800.0
relative error loss 0.22731417
shape of L is 
torch.Size([])
memory (bytes)
5487185920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5487185920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1826634800.0
relative error loss 0.22684214
shape of L is 
torch.Size([])
memory (bytes)
5490323456
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5490323456
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 95% | 12% |
error is  1822742500.0
relative error loss 0.22635877
shape of L is 
torch.Size([])
memory (bytes)
5493583872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 12% |
memory (bytes)
5493583872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1819154400.0
relative error loss 0.22591318
shape of L is 
torch.Size([])
memory (bytes)
5496795136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5496795136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1815342100.0
relative error loss 0.22543974
shape of L is 
torch.Size([])
memory (bytes)
5499969536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5499969536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1813037000.0
relative error loss 0.22515349
shape of L is 
torch.Size([])
memory (bytes)
5503209472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5503209472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1810302000.0
relative error loss 0.22481383
shape of L is 
torch.Size([])
memory (bytes)
5506228224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5506412544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1808212000.0
relative error loss 0.22455429
time to take a step is 253.04558897018433
it  2 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5509623808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5509623808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1808212000.0
relative error loss 0.22455429
shape of L is 
torch.Size([])
memory (bytes)
5512798208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5512798208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1805846000.0
relative error loss 0.22426046
shape of L is 
torch.Size([])
memory (bytes)
5516034048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5516034048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 12% |
error is  1804263400.0
relative error loss 0.22406393
shape of L is 
torch.Size([])
memory (bytes)
5519200256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5519200256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1802715100.0
relative error loss 0.22387165
shape of L is 
torch.Size([])
memory (bytes)
5522436096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5522436096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1800472600.0
relative error loss 0.22359316
shape of L is 
torch.Size([])
memory (bytes)
5525594112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5525594112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1798444000.0
relative error loss 0.22334124
shape of L is 
torch.Size([])
memory (bytes)
5528838144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5528838144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1797297200.0
relative error loss 0.22319882
shape of L is 
torch.Size([])
memory (bytes)
5531910144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5532037120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1795875300.0
relative error loss 0.22302225
shape of L is 
torch.Size([])
memory (bytes)
5535236096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 12% |
memory (bytes)
5535236096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1793653800.0
relative error loss 0.22274636
shape of L is 
torch.Size([])
memory (bytes)
5538398208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5538398208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1794004500.0
relative error loss 0.22278991
shape of L is 
torch.Size([])
memory (bytes)
5541658624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5541658624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1792309800.0
relative error loss 0.22257945
time to take a step is 280.9949963092804
it  3 : 3354760704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5544792064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 11% |
memory (bytes)
5544857600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1792309800.0
relative error loss 0.22257945
shape of L is 
torch.Size([])
memory (bytes)
5548048384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 11% |
memory (bytes)
5548048384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1790947300.0
relative error loss 0.22241026
shape of L is 
torch.Size([])
memory (bytes)
5551144960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5551255552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1789685200.0
relative error loss 0.22225353
shape of L is 
torch.Size([])
memory (bytes)
5554397184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5554397184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1788769800.0
relative error loss 0.22213984
shape of L is 
torch.Size([])
memory (bytes)
5557665792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5557665792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1787729900.0
relative error loss 0.2220107
shape of L is 
torch.Size([])
memory (bytes)
5560664064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5560864768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1787001300.0
relative error loss 0.22192022
shape of L is 
torch.Size([])
memory (bytes)
5564071936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5564071936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1786323500.0
relative error loss 0.22183605
shape of L is 
torch.Size([])
memory (bytes)
5567229952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5567229952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1785531900.0
relative error loss 0.22173774
shape of L is 
torch.Size([])
memory (bytes)
5570478080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 12% |
memory (bytes)
5570478080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1784424400.0
relative error loss 0.2216002
shape of L is 
torch.Size([])
memory (bytes)
5573685248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5573685248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1784077300.0
relative error loss 0.2215571
time to take a step is 256.71114110946655
it  4 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5576888320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5576888320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1784077300.0
relative error loss 0.2215571
shape of L is 
torch.Size([])
memory (bytes)
5579980800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
5580107776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1783441400.0
relative error loss 0.22147813
shape of L is 
torch.Size([])
memory (bytes)
5583323136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5583323136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 12% |
error is  1782556200.0
relative error loss 0.2213682
shape of L is 
torch.Size([])
memory (bytes)
5586513920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5586513920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1781584900.0
relative error loss 0.22124758
shape of L is 
torch.Size([])
memory (bytes)
5589692416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5589692416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1780901400.0
relative error loss 0.22116269
shape of L is 
torch.Size([])
memory (bytes)
5592928256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5592928256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1780084700.0
relative error loss 0.22106127
shape of L is 
torch.Size([])
memory (bytes)
5596127232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5596127232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1779331600.0
relative error loss 0.22096775
shape of L is 
torch.Size([])
memory (bytes)
5599334400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5599334400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 12% |
error is  1778361300.0
relative error loss 0.22084726
shape of L is 
torch.Size([])
memory (bytes)
5602443264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5602553856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1777991700.0
relative error loss 0.22080135
shape of L is 
torch.Size([])
memory (bytes)
5605752832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5605752832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  1777371100.0
relative error loss 0.22072428
time to take a step is 257.02813172340393
it  5 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5608955904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
5608955904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1777371100.0
relative error loss 0.22072428
shape of L is 
torch.Size([])
memory (bytes)
5612077056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 11% |
memory (bytes)
5612154880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1776832500.0
relative error loss 0.2206574
shape of L is 
torch.Size([])
memory (bytes)
5615362048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5615362048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1776272900.0
relative error loss 0.22058791
shape of L is 
torch.Size([])
memory (bytes)
5618409472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5618577408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1775246300.0
relative error loss 0.22046041
shape of L is 
torch.Size([])
memory (bytes)
5621776384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5621776384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1774951400.0
relative error loss 0.22042379
shape of L is 
torch.Size([])
memory (bytes)
5624930304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5624971264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1774554100.0
relative error loss 0.22037445
shape of L is 
torch.Size([])
memory (bytes)
5628174336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5628174336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1774149100.0
relative error loss 0.22032416
shape of L is 
torch.Size([])
memory (bytes)
5631328256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5631328256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1773890000.0
relative error loss 0.22029199
shape of L is 
torch.Size([])
memory (bytes)
5634572288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5634572288
| ID | GPU  | MEM |
-------------------
|  0 |   4% |  0% |
|  1 | 100% | 12% |
error is  1773539300.0
relative error loss 0.22024843
shape of L is 
torch.Size([])
memory (bytes)
5637791744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5637791744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1773063200.0
relative error loss 0.2201893
time to take a step is 259.41166615486145
it  6 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5640892416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5640998912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1773063200.0
relative error loss 0.2201893
shape of L is 
torch.Size([])
memory (bytes)
5644185600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5644185600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 12% |
error is  1772700200.0
relative error loss 0.22014421
shape of L is 
torch.Size([])
memory (bytes)
5647400960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5647400960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1772310000.0
relative error loss 0.22009577
shape of L is 
torch.Size([])
memory (bytes)
5650608128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5650608128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1771948500.0
relative error loss 0.22005089
shape of L is 
torch.Size([])
memory (bytes)
5653663744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5653811200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1771347500.0
relative error loss 0.21997623
shape of L is 
torch.Size([])
memory (bytes)
5657018368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5657018368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1771024900.0
relative error loss 0.21993618
shape of L is 
torch.Size([])
memory (bytes)
5660164096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5660225536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1770750500.0
relative error loss 0.2199021
shape of L is 
torch.Size([])
memory (bytes)
5663424512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5663424512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1770413600.0
relative error loss 0.21986026
shape of L is 
torch.Size([])
memory (bytes)
5666635776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5666635776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1770018300.0
relative error loss 0.21981117
shape of L is 
torch.Size([])
memory (bytes)
5669806080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5669834752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1769759200.0
relative error loss 0.219779
time to take a step is 262.0825734138489
it  7 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5673058304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5673058304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1769759200.0
relative error loss 0.219779
shape of L is 
torch.Size([])
memory (bytes)
5676154880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5676253184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1769493000.0
relative error loss 0.21974593
shape of L is 
torch.Size([])
memory (bytes)
5679472640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5679472640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1769085400.0
relative error loss 0.21969533
shape of L is 
torch.Size([])
memory (bytes)
5682683904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5682683904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1768708600.0
relative error loss 0.21964853
shape of L is 
torch.Size([])
memory (bytes)
5685899264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5685899264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1768469500.0
relative error loss 0.21961883
shape of L is 
torch.Size([])
memory (bytes)
5689077760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5689077760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1768231400.0
relative error loss 0.21958926
shape of L is 
torch.Size([])
memory (bytes)
5692317696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5692317696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1767799300.0
relative error loss 0.2195356
shape of L is 
torch.Size([])
memory (bytes)
5695508480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5695508480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1767490000.0
relative error loss 0.2194972
shape of L is 
torch.Size([])
memory (bytes)
5698719744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5698719744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1767225900.0
relative error loss 0.21946439
shape of L is 
torch.Size([])
memory (bytes)
5701869568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5701926912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1766937100.0
relative error loss 0.21942852
time to take a step is 254.82914781570435
it  8 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5705031680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5705138176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1766937100.0
relative error loss 0.21942852
shape of L is 
torch.Size([])
memory (bytes)
5708333056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5708333056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1766721000.0
relative error loss 0.2194017
shape of L is 
torch.Size([])
memory (bytes)
5711478784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 12% |
memory (bytes)
5711544320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1766446600.0
relative error loss 0.21936761
shape of L is 
torch.Size([])
memory (bytes)
5714747392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5714747392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1766028800.0
relative error loss 0.21931574
shape of L is 
torch.Size([])
memory (bytes)
5717749760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5717950464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  1766356500.0
relative error loss 0.21935642
shape of L is 
torch.Size([])
memory (bytes)
5721157632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5721157632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1765876700.0
relative error loss 0.21929684
shape of L is 
torch.Size([])
memory (bytes)
5724348416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5724348416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1765659100.0
relative error loss 0.21926983
shape of L is 
torch.Size([])
memory (bytes)
5727571968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5727571968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1765422100.0
relative error loss 0.21924038
shape of L is 
torch.Size([])
memory (bytes)
5730758656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 12% |
memory (bytes)
5730758656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1765222900.0
relative error loss 0.21921565
shape of L is 
torch.Size([])
memory (bytes)
5733974016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5733974016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1765055000.0
relative error loss 0.2191948
time to take a step is 254.619234085083
it  9 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5737046016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5737168896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 11% |
error is  1765055000.0
relative error loss 0.2191948
shape of L is 
torch.Size([])
memory (bytes)
5740277760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5740367872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1764732900.0
relative error loss 0.2191548
shape of L is 
torch.Size([])
memory (bytes)
5743579136
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5743579136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1764510700.0
relative error loss 0.21912721
shape of L is 
torch.Size([])
memory (bytes)
5746655232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5746786304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1764311000.0
relative error loss 0.21910241
shape of L is 
torch.Size([])
memory (bytes)
5749977088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5749977088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1763961300.0
relative error loss 0.21905898
shape of L is 
torch.Size([])
memory (bytes)
5753090048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 12% |
memory (bytes)
5753180160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1763771400.0
relative error loss 0.21903539
shape of L is 
torch.Size([])
memory (bytes)
5756383232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5756383232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1763542500.0
relative error loss 0.21900697
shape of L is 
torch.Size([])
memory (bytes)
5759569920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5759569920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1763329500.0
relative error loss 0.21898052
shape of L is 
torch.Size([])
memory (bytes)
5762785280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5762785280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1763172400.0
relative error loss 0.218961
shape of L is 
torch.Size([])
memory (bytes)
5765931008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5765931008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  1762997800.0
relative error loss 0.21893932
time to take a step is 255.1993567943573
it  10 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5769195520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
5769195520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1762997800.0
relative error loss 0.21893932
shape of L is 
torch.Size([])
memory (bytes)
5772382208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5772382208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1762848300.0
relative error loss 0.21892075
shape of L is 
torch.Size([])
memory (bytes)
5775536128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5775597568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1762708000.0
relative error loss 0.21890333
shape of L is 
torch.Size([])
memory (bytes)
5778804736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5778804736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1762559000.0
relative error loss 0.21888483
shape of L is 
torch.Size([])
memory (bytes)
5781946368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5782007808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1762407400.0
relative error loss 0.218866
shape of L is 
torch.Size([])
memory (bytes)
5785206784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5785206784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1762292700.0
relative error loss 0.21885176
shape of L is 
torch.Size([])
memory (bytes)
5788409856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5788409856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 12% |
error is  1762170400.0
relative error loss 0.21883658
shape of L is 
torch.Size([])
memory (bytes)
5791608832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5791608832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1762036700.0
relative error loss 0.21881998
shape of L is 
torch.Size([])
memory (bytes)
5794758656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5794820096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1761854500.0
relative error loss 0.21879734
shape of L is 
torch.Size([])
memory (bytes)
5798019072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5798019072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1761761300.0
relative error loss 0.21878576
time to take a step is 255.29692459106445
it  11 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5801213952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5801213952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1761761300.0
relative error loss 0.21878576
shape of L is 
torch.Size([])
memory (bytes)
5804417024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
5804417024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1761650700.0
relative error loss 0.21877204
shape of L is 
torch.Size([])
memory (bytes)
5807566848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5807628288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1761542100.0
relative error loss 0.21875855
shape of L is 
torch.Size([])
memory (bytes)
5810823168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5810823168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1761347100.0
relative error loss 0.21873432
shape of L is 
torch.Size([])
memory (bytes)
5814046720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5814046720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1761245200.0
relative error loss 0.21872167
shape of L is 
torch.Size([])
memory (bytes)
5817208832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 12% |
memory (bytes)
5817208832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1761141200.0
relative error loss 0.21870877
shape of L is 
torch.Size([])
memory (bytes)
5820444672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5820444672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1760940000.0
relative error loss 0.21868378
shape of L is 
torch.Size([])
memory (bytes)
5823647744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5823647744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1760832500.0
relative error loss 0.21867043
shape of L is 
torch.Size([])
memory (bytes)
5826863104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5826863104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1760677400.0
relative error loss 0.21865116
shape of L is 
torch.Size([])
memory (bytes)
5829914624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5830070272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1760609300.0
relative error loss 0.2186427
time to take a step is 254.9344675540924
it  12 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5833281536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5833281536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1760609300.0
relative error loss 0.2186427
shape of L is 
torch.Size([])
memory (bytes)
5836423168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5836480512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1760387600.0
relative error loss 0.21861517
shape of L is 
torch.Size([])
memory (bytes)
5839675392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5839675392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1760370200.0
relative error loss 0.21861301
shape of L is 
torch.Size([])
memory (bytes)
5842894848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 12% |
memory (bytes)
5842894848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1760179200.0
relative error loss 0.21858929
shape of L is 
torch.Size([])
memory (bytes)
5846052864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5846052864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1760090600.0
relative error loss 0.2185783
shape of L is 
torch.Size([])
memory (bytes)
5849309184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5849309184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1759965200.0
relative error loss 0.21856272
shape of L is 
torch.Size([])
memory (bytes)
5852450816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5852450816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  1759829000.0
relative error loss 0.21854581
shape of L is 
torch.Size([])
memory (bytes)
5855719424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5855719424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1759966200.0
relative error loss 0.21856284
shape of L is 
torch.Size([])
memory (bytes)
5858930688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5858930688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1759750700.0
relative error loss 0.21853608
shape of L is 
torch.Size([])
memory (bytes)
5862146048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5862146048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1759636500.0
relative error loss 0.2185219
time to take a step is 255.25803899765015
it  13 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5865336832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5865336832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1759636500.0
relative error loss 0.2185219
shape of L is 
torch.Size([])
memory (bytes)
5868523520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5868523520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1759545900.0
relative error loss 0.21851064
shape of L is 
torch.Size([])
memory (bytes)
5871747072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5871747072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1759383000.0
relative error loss 0.21849042
shape of L is 
torch.Size([])
memory (bytes)
5874880512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5874937856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 12% |
error is  1759327700.0
relative error loss 0.21848355
shape of L is 
torch.Size([])
memory (bytes)
5878149120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5878149120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1759194600.0
relative error loss 0.21846703
shape of L is 
torch.Size([])
memory (bytes)
5881348096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5881348096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1759121900.0
relative error loss 0.218458
shape of L is 
torch.Size([])
memory (bytes)
5884416000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5884559360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1759019500.0
relative error loss 0.21844527
shape of L is 
torch.Size([])
memory (bytes)
5887762432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5887762432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1758889000.0
relative error loss 0.21842906
shape of L is 
torch.Size([])
memory (bytes)
5890908160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5890965504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1758773800.0
relative error loss 0.21841475
shape of L is 
torch.Size([])
memory (bytes)
5894164480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 12% |
memory (bytes)
5894164480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1758677500.0
relative error loss 0.2184028
time to take a step is 254.4068958759308
it  14 : 3354760192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5897359360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5897359360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1758677500.0
relative error loss 0.2184028
shape of L is 
torch.Size([])
memory (bytes)
5900541952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5900541952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1758560300.0
relative error loss 0.21838824
shape of L is 
torch.Size([])
memory (bytes)
5903761408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5903761408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1758439400.0
relative error loss 0.21837324
shape of L is 
torch.Size([])
memory (bytes)
5906890752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% | 12% |
memory (bytes)
5906890752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1758339100.0
relative error loss 0.21836078
shape of L is 
torch.Size([])
memory (bytes)
5910155264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5910155264
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 12% |
error is  1758234100.0
relative error loss 0.21834774
shape of L is 
torch.Size([])
memory (bytes)
5913174016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5913362432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 12% |
error is  1758165500.0
relative error loss 0.21833922
shape of L is 
torch.Size([])
memory (bytes)
5916557312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5916557312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1758070300.0
relative error loss 0.21832739
shape of L is 
torch.Size([])
memory (bytes)
5919760384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 12% |
memory (bytes)
5919760384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1758069800.0
relative error loss 0.21832733
shape of L is 
torch.Size([])
memory (bytes)
5922963456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5922963456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1757991400.0
relative error loss 0.2183176
shape of L is 
torch.Size([])
memory (bytes)
5926170624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5926170624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1757889500.0
relative error loss 0.21830495
time to take a step is 262.18068647384644
sum tnnu_Z after tensor(11182684., device='cuda:0')
shape of features
(6751,)
shape of features
(6751,)
number of orig particles 27005
number of new particles after remove low mass 25287
tnuZ shape should be parts x labs
torch.Size([27005, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  2007135100.0
relative error without small mass is  0.24925771
nnu_Z shape should be number of particles by maxV
(27005, 702)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
shape of features
(27005,)
Wed Feb 1 16:28:07 EST 2023
