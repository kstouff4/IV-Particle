Thu Feb 2 05:58:06 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 34154965
numbers of Z: 21835
shape of features
(21835,)
shape of features
(21835,)
ZX	Vol	Parts	Cubes	Eps
Z	0.023938209337628122	21835	21.835	0.1031128572810163
X	0.01884692828718792	1216	1.216	0.2493268217899385
X	0.018949458608952603	19222	19.222	0.09952512794198931
X	0.018854394692680788	2005	2.005	0.21107302733430836
X	0.01899819506842014	11223	11.223	0.11917938130758524
X	0.018868642353513206	8756	8.756	0.12916503518874758
X	0.01904316145512661	18488	18.488	0.1009910852197193
X	0.019020471246989523	51431	51.431	0.0717792255101042
X	0.019097114270671598	35684	35.684	0.08118916446779477
X	0.019059561624006828	7821	7.821	0.13457060571242674
X	0.018931059418973023	23736	23.736	0.09273758776571478
X	0.019003908717291552	12395	12.395	0.11530960068597963
X	0.01910778956757441	88907	88.907	0.05989972537306325
X	0.020590593022382316	5914	5.914	0.1515639254640959
X	0.019118215148107468	317361	317.361	0.039201070744979374
X	0.01911261658586051	19889	19.889	0.09868149794468795
X	0.019327039361484805	39174	39.174	0.07901735182576033
X	0.019119937812945274	51168	51.168	0.07202711357001325
X	0.019090093353176477	29902	29.902	0.08610650032923106
X	0.019142374099195173	220018	220.018	0.044311099759724304
X	0.019111319973644632	108072	108.072	0.056129671223334336
X	0.019105197074600008	8412	8.412	0.13144702237501552
X	0.019158587268972682	223271	223.271	0.04410729131458714
X	0.01902093360446665	11672	11.672	0.11767806454022023
X	0.01969019224576957	29112	29.112	0.08777940605625928
X	0.018387335747026216	3068	3.068	0.18164403904020804
X	0.019134429283299746	67076	67.076	0.06582882738662289
X	0.0206078735098293	47648	47.648	0.07562455543714452
X	0.018797509995303826	5075	5.075	0.15472295785833087
X	0.018904784582099134	79964	79.964	0.061833772082634506
X	0.019283391318477908	1239395	1239.395	0.024964596916468483
X	0.019219752991034438	6846	6.846	0.14107074797362518
X	0.020964712447063008	592446	592.446	0.03283069270367751
X	0.01857555557921352	13568	13.568	0.11103894471222864
X	0.01909863311816301	6655	6.655	0.14210785213269989
X	0.018778347238350594	8266	8.266	0.13145809107435513
X	0.019380623898530326	113405	113.405	0.05549389186787102
X	0.01913839370956225	97719	97.719	0.05807316424458247
X	0.01822077493606141	1162	1.162	0.2502958056174207
X	0.020119982133015524	4118	4.118	0.1696864183126029
X	0.0184790403206453	2203	2.203	0.20318343833031288
X	0.018894653747614284	2999	2.999	0.1846945827136501
X	0.0180995457151704	1571	1.571	0.22585540930299855
X	0.01807719709876777	710	0.71	0.29418752202973525
X	0.0189360786508	2591	2.591	0.19406216787593297
X	0.018833846471003376	971	0.971	0.2686828411107962
X	0.018768515824300006	1811	1.811	0.21802376085856057
X	0.018562829109591242	4117	4.117	0.16520420594263427
X	0.0186377696261548	3235	3.235	0.17926955953774532
X	0.018251031926122655	1392	1.392	0.23580303822942192
X	0.018834518013859023	8289	8.289	0.13146721322164637
X	0.01884128031672571	10283	10.283	0.12236677803734257
X	0.01859787985112442	1940	1.94	0.21243230881629885
X	0.019003928088165777	5496	5.496	0.15121639656785207
X	0.01839453760165345	1965	1.965	0.21075381271711258
X	0.01878689918045869	5009	5.009	0.15537030717173564
X	0.018817761770860423	2965	2.965	0.18514603797423884
X	0.01828397696600902	1223	1.223	0.24634744905637415
X	0.019018988181352905	2911	2.911	0.18694552241529983
X	0.01863598324917902	2905	2.905	0.18580984664725977
X	0.018621891780983207	2924	2.924	0.18535976779354213
X	0.01899491511613358	3700	3.7	0.1725090476772077
X	0.01866718258089468	1699	1.699	0.2223114333542817
X	0.02050509450754677	8100	8.1	0.13628840808763124
X	0.018804424595375114	6544	6.544	0.14216922174857854
X	0.01865352068191109	3609	3.609	0.17289853375771613
X	0.019015333676415	2595	2.595	0.1942326324237231
X	0.018594355603003102	3710	3.71	0.17113377910767874
X	0.018215048121043347	3249	3.249	0.17764795148046816
X	0.018885041718245607	3330	3.33	0.1783300665651592
X	0.018336590169236906	2496	2.496	0.1943977940753682
X	0.018782379273608898	2320	2.32	0.2007956069773928
X	0.01853774713716403	3065	3.065	0.18219739293194218
X	0.018826972290419415	3638	3.638	0.1729707212081193
X	0.0188832129133876	1548	1.548	0.23019805218396724
X	0.01904712364069201	3616	3.616	0.1739937545545455
X	0.018937093889073164	4072	4.072	0.16691749342481463
X	0.01824185355183528	1135	1.135	0.25236223660699836
X	0.017795544322042955	550	0.55	0.3186516676501791
X	0.018946841551941953	2742	2.742	0.1904684836228757
X	0.018823379746135755	8354	8.354	0.13109950270804166
X	0.018532685528420578	4000	4.0	0.16670915686295162
X	0.01771573275627601	1451	1.451	0.2302663989677549
X	0.018997940491221204	5185	5.185	0.15416504738672027
X	0.01842755668775316	1790	1.79	0.21753955791925453
X	0.018399538295101863	2803	2.803	0.18723826542470215
X	0.01884290025817192	1188	1.188	0.25125252572182527
X	0.018919462513878407	3734	3.734	0.17175583240843953
X	0.01810381066325764	1194	1.194	0.24750761049136352
X	0.018647100267747605	3247	3.247	0.17907831767692453
X	0.018910013401078573	2697	2.697	0.191397809637125
X	0.01903430052251435	2703	2.703	0.19167418503678804
X	0.018593173270691033	1419	1.419	0.23575289818182876
X	0.01853619465347155	1460	1.46	0.23328630363986613
X	0.018415494820632705	2063	2.063	0.20744105934625626
X	0.019687369517200504	5214	5.214	0.15571802564342926
X	0.018902612685351032	3914	3.914	0.16903125065399024
X	0.019015714556794107	2495	2.495	0.19679498784108448
X	0.0188069675865309	9854	9.854	0.12404199410892033
X	0.01898664460862191	3323	3.323	0.17877465969963094
X	0.018664751146947756	2381	2.381	0.19864955789508165
X	0.01909019853983472	10640	10.64	0.12151305480683285
X	0.018927647757971355	2973	2.973	0.1853391834531241
X	0.018870948718150482	6844	6.844	0.14022579509666896
X	0.018104944566854134	1338	1.338	0.23829432260175515
X	0.0183048952907262	3234	3.234	0.17821424338238537
X	0.018838338726085816	1981	1.981	0.21186182482740146
X	0.019077908433939344	6646	6.646	0.1421205264950325
X	0.01841314672827903	827	0.827	0.2813244648829219
X	0.018954945692769135	2742	2.742	0.19049563614120385
X	0.01831967506883355	1406	1.406	0.23531203982838203
X	0.018462727237872525	1481	1.481	0.23187126517712253
X	0.018444199256236804	1010	1.01	0.26333756200271174
X	0.01856378528672858	1788	1.788	0.21815558745339778
X	0.018678412266075365	2945	2.945	0.18510502953444458
X	0.020109916474795313	6017	6.017	0.14951224124187068
X	0.018321970367901255	1718	1.718	0.22011501684213666
X	0.01856726770295999	1759	1.759	0.21936165795116855
X	0.0186024000169112	1090	1.09	0.25746260573075586
X	0.02048755878272199	6504	6.504	0.1465896225839175
X	0.018291244587093178	1899	1.899	0.2127678952450977
X	0.019195896837552316	10464	10.464	0.12241563567415233
X	0.0180242886387784	2160	2.16	0.20283121590819508
X	0.018161507987102	2268	2.268	0.20006430907810127
X	0.018895717327432223	2125	2.125	0.20717379254198243
X	0.018268812877341507	1484	1.484	0.23090082014339586
X	0.020196037392501854	3191	3.191	0.18497503605217744
X	0.018656058123909785	1719	1.719	0.22140189640719327
X	0.018749282255286968	2446	2.446	0.19717086034202985
X	0.020993211559137807	10901	10.901	0.124414456753489
X	0.018907872817825556	3217	3.217	0.18046675273190463
X	0.01895160472175399	7784	7.784	0.13452852714752805
X	0.018892974058650168	1434	1.434	0.23618395583485308
X	0.01863548195923454	3997	3.997	0.16705859592051778
X	0.018525830019612104	1246	1.246	0.24589698901294768
X	0.018142732189102756	2705	2.705	0.18858702383803416
X	0.01864522832468678	1589	1.589	0.22723816290779067
X	0.018744414493403683	1257	1.257	0.2461381132190709
X	0.01825345463040706	1270	1.27	0.24313479580950595
X	0.018253229078056397	2274	2.274	0.20022412970175835
X	0.018542226956828643	1519	1.519	0.2302509193288779
X	0.018700604646927053	2209	2.209	0.20380741360157903
X	0.018752630013268556	1471	1.471	0.23360555767773553
X	0.020685840171011022	12022	12.022	0.11982997822117857
X	0.018989752511090387	8751	8.751	0.12946544654882922
X	0.018807264668356926	4226	4.226	0.16448717727950127
X	0.018664318734921605	1730	1.73	0.22096424929092431
X	0.018450210290323572	1921	1.921	0.21256478535297196
X	0.02226074642348144	3738	3.738	0.1812592346245221
X	0.01863822762022445	1836	1.836	0.21652629504706816
X	0.01881104733695198	2044	2.044	0.20956111231677077
X	0.01807654627888102	2485	2.485	0.19375950728959535
X	0.018892907176529136	2523	2.523	0.1956412880495185
X	0.018970939529263448	3396	3.396	0.1774354474530445
X	0.0202497592931823	13205	13.205	0.11531712859196781
X	0.01854845171715464	2713	2.713	0.18979551988382204
X	0.021039131992096477	15025	15.025	0.11187621868517555
X	0.018867932307165623	2128	2.128	0.20697484305229513
X	0.018830787258642453	2840	2.84	0.18786680332472697
X	0.019096726457614243	4301	4.301	0.16436013712160807
X	0.017303669927890942	437	0.437	0.3408413382016029
X	0.021967930180491372	46096	46.096	0.07811035882616467
X	0.018879017516416637	2749	2.749	0.19007932593292554
X	0.019340696048905547	3332	3.332	0.17971694841328822
X	0.018400833731248276	1420	1.42	0.23488199062754853
X	0.01816009607402768	2865	2.865	0.18506793189796825
X	0.018675269744924113	1552	1.552	0.22915274053008816
X	0.01902039767885916	2180	2.18	0.2058674569485905
X	0.018429542099761987	3251	3.251	0.17830595936310922
X	0.01887389190661259	4133	4.133	0.16590724541616325
X	0.018209682743545282	582	0.582	0.31510826774556816
X	0.017538345883971008	1500	1.5	0.22696860614520606
X	0.01819717327534898	1061	1.061	0.2578870388863156
X	0.019074237636915348	5450	5.45	0.15182742586405243
X	0.01840587484965092	1306	1.306	0.2415485469257344
X	0.020486342222950607	3720	3.72	0.17659313959148917
X	0.020713096463720024	2964	2.964	0.1911857897983658
X	0.018476007649553802	3327	3.327	0.17708635987930013
X	0.022292749400382634	4899	4.899	0.16571127729668378
X	0.0188057341264848	4451	4.451	0.16166309878635565
X	0.017864732509069347	2140	2.14	0.2028589849956536
X	0.018799829367689484	1917	1.917	0.21404771607984036
X	0.019040774792296183	2624	2.624	0.19360070236677882
X	0.018761391221643876	3262	3.262	0.17916796500435334
X	0.018849247910876703	1670	1.67	0.22431544892743613
X	0.018445777407656117	4460	4.46	0.1605168770193396
X	0.018890366545081725	4415	4.415	0.16234411837024534
X	0.01814816067084306	2726	2.726	0.1881202676689612
X	0.018940557074139684	3156	3.156	0.18172663485193377
X	0.01890840183726325	5528	5.528	0.15067074290536225
X	0.01969524161334478	11478	11.478	0.11971953761921329
X	0.018354150087537498	1628	1.628	0.2242297809846367
X	0.017980321290718044	440	0.44	0.3444410653285264
X	0.018440532427892804	2487	2.487	0.19499907756725013
X	0.018934586686420903	3224	3.224	0.18042093624564284
X	0.01825635961530886	2121	2.121	0.20493894891085973
X	0.01903970364992414	3006	3.006	0.18502215544875988
X	0.018834165817479608	1824	1.824	0.21775786602111163
X	0.018608497066835483	1286	1.286	0.2436819959635979
X	0.018594465124567583	1961	1.961	0.2116583301672682
X	0.018825734596594774	948	0.948	0.27079950415636606
X	0.018563764769105522	2600	2.6	0.1925591837054581
X	0.018509576269298136	1258	1.258	0.24504091861156438
X	0.018799396670354667	5132	5.132	0.15415315628094675
X	0.0188980426878375	3501	3.501	0.1754183028568225
X	0.018833218282538366	6277	6.277	0.14423062001804904
X	0.018765620605840166	1022	1.022	0.26381774993108503
X	0.018907902911539716	4067	4.067	0.16690002320864317
X	0.018391418984586603	2503	2.503	0.19440978930627853
X	0.018614814571696878	3864	3.864	0.16889126900871682
X	0.02027462257058593	5535	5.535	0.15415054020050312
X	0.019052705405764717	5729	5.729	0.14926544385865398
X	0.018841718285853057	2415	2.415	0.19833580520013497
X	0.0185407450053527	2521	2.521	0.1944694765013671
X	0.0184261137928167	2226	2.226	0.20228772473582662
X	0.018814657291022885	1033	1.033	0.263106754117988
X	0.017981322590643112	1747	1.747	0.2175251995999002
X	0.01902069476112466	1712	1.712	0.2231383437959299
X	0.018163169006804026	735	0.735	0.29127380442969786
X	0.0185643620582784	4346	4.346	0.16225450703299715
X	0.01841398942281456	1841	1.841	0.21545902998479516
X	0.018201768587383895	2880	2.88	0.1848872801267691
X	0.018995861074919518	1641	1.641	0.22621260512636404
X	0.018870941488516875	5272	5.272	0.1529699256105791
X	0.018373274118609034	1529	1.529	0.22904792215859282
X	0.02009963703318524	4026	4.026	0.17091157700050694
X	0.01872549735735836	4725	4.725	0.15825004769322798
X	0.018296224648526104	990	0.99	0.26438821728447004
X	0.018369881422337506	1382	1.382	0.23688238661611422
X	0.018907229342343308	3815	3.815	0.17049479169512594
X	0.017790340917972488	737	0.737	0.2890051582504895
X	0.01877243565766593	1267	1.267	0.24561110737314648
X	0.01892669103579672	3692	3.692	0.17242662731475716
X	0.02192937920447052	3554	3.554	0.18341561428573863
X	0.018664310948016443	3487	3.487	0.17492558619009274
X	0.018470388537288426	1469	1.469	0.23253308057831604
X	0.018705372411047156	1456	1.456	0.23420796543410674
X	0.020099408702662625	2353	2.353	0.204418619649793
X	0.019145164076426805	4802	4.802	0.1585668184121133
X	0.0190123633876882	1267	1.267	0.2466530540187506
X	0.018674546375149405	11026	11.026	0.11920030253552998
X	0.018976807167920346	42514	42.514	0.07642411040150279
X	0.0189828582511315	6984	6.984	0.13955726767937982
X	0.018860656120157738	4013	4.013	0.16750555430633135
X	0.018468144092408493	622	0.622	0.3096546622229334
X	0.019446602480655127	8281	8.281	0.13291897512880524
X	0.018942006023123164	85970	85.97	0.06039852709825655
X	0.01934632628576849	222277	222.277	0.044316759207201725
X	0.018280054376237705	2205	2.205	0.20239027002100962
X	0.020098325102427223	79795	79.795	0.06315310808960121
X	0.018941027801691414	26571	26.571	0.0893302495825872
X	0.019544809094791477	23724	23.724	0.09374494079786039
X	0.020249316392257466	121146	121.146	0.055085051823397076
X	0.018889060034256253	1308	1.308	0.24351975657910402
X	0.018694040913327603	3529	3.529	0.17432132173996878
X	0.01904672415872803	148003	148.003	0.05048742928525169
X	0.019305318726958634	243047	243.047	0.04298618503343297
X	0.01900510441528243	3173	3.173	0.18160733941443327
X	0.020158631459538248	25859	25.859	0.09203431261223148
X	0.018800734340156573	14838	14.838	0.10820977200775603
X	0.01900232976270682	59089	59.089	0.0685120143784899
X	0.019356632106315247	69515	69.515	0.06530058765389418
X	0.020669238988507352	33464	33.464	0.08516259343303481
X	0.01950056600720341	20524	20.524	0.09830940561648735
X	0.019040215501062902	16375	16.375	0.10515506195973959
X	0.018704123332525666	1790	1.79	0.2186224593528719
X	0.019267325313250292	191399	191.399	0.0465186663357305
X	0.018658465470962112	2993	2.993	0.18404459547400925
X	0.018689184862125355	1963	1.963	0.21194508678796745
X	0.01943432011717912	38380	38.38	0.07970547003310267
X	0.018503743290351555	55374	55.374	0.06939335729324654
X	0.021720046909986978	176773	176.773	0.04971433206493904
X	0.019260076412110708	84201	84.201	0.06115710497594358
X	0.018700763314083848	1269	1.269	0.24516919656062594
X	0.018342877351363952	13441	13.441	0.11092053620862834
X	0.020416438040246364	9392	9.392	0.12954101543692864
X	0.019130553279577227	28696	28.696	0.08735787388347449
X	0.01915186863813491	56228	56.228	0.06983709610809098
X	0.018727803420594772	3726	3.726	0.17129630586267566
X	0.019149883554186887	24660	24.66	0.09191597995118814
X	0.01812750770811104	572	0.572	0.3164565019943409
X	0.018891636599136986	3521	3.521	0.17506574889127163
X	0.018916403851457195	48746	48.746	0.07294012967063739
X	0.01881297784088129	75448	75.448	0.06294142976505901
X	0.018831695291019725	41838	41.838	0.07663717738834136
X	0.019405583873941906	7566	7.566	0.13688424317283743
X	0.01953411532024096	258038	258.038	0.04230288889935335
X	0.019108562150423974	12095	12.095	0.1164682040965506
X	0.01908269156817773	21847	21.847	0.09559076577388545
X	0.019334127493958766	70035	70.035	0.06511331629812464
X	0.018758081177286882	2556	2.556	0.19433119464769594
X	0.019020847530798003	109195	109.195	0.055848181605827205
X	0.020677332896577052	186312	186.312	0.048056399646365815
X	0.01932144989957406	198474	198.474	0.04600221007474034
X	0.019022290865308426	11185	11.185	0.11936461727603861
X	0.018978077217076856	9305	9.305	0.12681733153936173
X	0.018769978420142462	9458	9.458	0.12566706956726506
X	0.018656506468464503	3869	3.869	0.1689444247598306
X	0.019124454556683487	34647	34.647	0.0820303328651036
X	0.020464160595898048	6609	6.609	0.14575363496858387
X	0.019050052971746244	30221	30.221	0.08574242961242604
X	0.019151271224136377	81835	81.835	0.0616244193268502
X	0.019067618155018288	16598	16.598	0.1047322005449664
X	0.018558986173606483	9996	9.996	0.12290682510814298
X	0.01883495899508847	4359	4.359	0.16287681565675813
X	0.019134751360886045	70469	70.469	0.06475523814740836
X	0.018964996522600926	13120	13.12	0.11306798278847346
X	0.018836132212346764	12120	12.12	0.1158323062746211
X	0.020436056850559797	8635	8.635	0.13326361398513828
X	0.020295421646587684	178488	178.488	0.04844656819559248
X	0.018938892550557408	9828	9.828	0.1244409001941672
X	0.019435581899824982	125437	125.437	0.05371026515596325
X	0.01864122760312713	4729	4.729	0.15796773763227162
X	0.019017198241069157	20684	20.684	0.09723829492502378
X	0.019088097958624903	15143	15.143	0.1080231683013642
X	0.019496785212727775	169256	169.256	0.048656363211439915
X	0.019045557424075522	34418	34.418	0.0820986506575532
X	0.01890359277297999	4012	4.012	0.16764649430394957
X	0.019115874115239916	99013	99.013	0.05779638128745492
X	0.02097739505924533	111778	111.778	0.057253020999596636
X	0.018884762406459105	11158	11.158	0.11917222723844896
X	0.019067157123266024	76367	76.367	0.06296899159409722
X	0.019109638779004623	108475	108.475	0.05605843118184096
X	0.021163407337956967	300730	300.73	0.04128601305644385
X	0.019900310054870755	23723	23.723	0.09431123052946311
X	0.018708265609665025	1239	1.239	0.2471652474399087
X	0.018905381116718536	8730	8.73	0.12937699639094022
X	0.019033237780111886	9816	9.816	0.12469796741361472
X	0.018922386521702226	16468	16.468	0.10473978635444083
X	0.018920667966159103	16247	16.247	0.10520937193050522
X	0.018702804318691046	2041	2.041	0.20926081307861422
X	0.019090921059779206	55101	55.101	0.07023535799247345
X	0.019634468229273942	122757	122.757	0.05428222310834289
X	0.018910363586516044	3835	3.835	0.17020729517918265
X	0.018874757723372645	5511	5.511	0.1507360017761294
X	0.018945149508530285	12192	12.192	0.11582643628777275
X	0.018622142655092763	7953	7.953	0.13279026031235724
X	0.018923621852238046	4595	4.595	0.1602899333925165
X	0.019313906706900825	9372	9.372	0.12725629742850636
X	0.019284350176420446	4245	4.245	0.16561859834580792
X	0.0183529248781886	18155	18.155	0.10036208564661389
X	0.019104060092695493	29971	29.971	0.08606134860008073
X	0.02070235177004655	15152	15.152	0.11096429051402841
X	0.02286570401420188	34260	34.26	0.08739080494784737
X	0.01860477167622129	7298	7.298	0.13660717023522065
X	0.019301421675375727	260304	260.304	0.0420116255214104
X	0.018629616844844005	4664	4.664	0.1586652402629431
X	0.019036378218540123	71517	71.517	0.06432676020134914
X	0.018546319474006808	1256	1.256	0.24533303549873053
X	0.019063500663567583	3743	3.743	0.17205248207588802
X	0.018878170314074405	3583	3.583	0.17400872728677555
X	0.019115652730357497	14901	14.901	0.10865704897620215
X	0.020394028862469795	3797	3.797	0.17512741778819763
X	0.02108989038477224	111532	111.532	0.05739731947735966
X	0.01980113648322306	3660	3.66	0.17555087991167698
X	0.021501978470048533	4634	4.634	0.16679158320318094
X	0.01943146398701415	261227	261.227	0.04205611514346368
X	0.019153895110080005	59965	59.965	0.06835753952830803
X	0.019680791056812294	46148	46.148	0.075271381833204
X	0.021201916785285515	181846	181.846	0.04885291218915598
X	0.020543834743705212	226643	226.643	0.04492068817256245
X	0.019973605816377357	10901	10.901	0.12236673670544301
X	0.01887681267772206	3744	3.744	0.17147373541211988
X	0.01897713337513707	15732	15.732	0.10645079047137286
X	0.01850001154830753	1159	1.159	0.2517848072738155
X	0.020301824158641395	4471	4.471	0.1655935680498544
X	0.019120503023487903	12807	12.807	0.11429238654598275
X	0.01846053232630233	3223	3.223	0.17892099508664885
X	0.019054683961293006	6575	6.575	0.14257235750881908
X	0.0182669863934465	2108	2.108	0.20539920874768502
X	0.018879772674600403	4028	4.028	0.16735387253722592
X	0.019354529916170742	286418	286.418	0.04073122201312768
X	0.018579442787408557	12082	12.082	0.11542448651729828
X	0.019227690299013177	83098	83.098	0.06139205661160917
X	0.019077361639588765	6513	6.513	0.1430800483056642
X	0.01865792673928688	17296	17.296	0.10255871457493883
X	0.018504564345288455	6861	6.861	0.13919719438615777
X	0.02027544388522797	507645	507.645	0.03418246826679555
X	0.0193725040277775	471052	471.052	0.03451747104890071
X	0.01906133557093189	17078	17.078	0.1037302535111034
X	0.019117700904856833	44994	44.994	0.07517850520691517
X	0.018699388128364716	2563	2.563	0.19395140496863034
X	0.018933502567448513	10732	10.732	0.12083240871866008
X	0.019043269089948047	42594	42.594	0.0764652933917851
X	0.020259263199442795	20259	20.259	0.10000043305576616
X	0.018144917598637535	9917	9.917	0.12230894751255329
X	0.018991805422343137	56455	56.455	0.06954852868056427
X	0.021959187801243948	384308	384.308	0.03851639358407341
X	0.018622233374725853	32095	32.095	0.08340616028214662
X	0.01886296876970106	8299	8.299	0.13148052424178955
X	0.02204552977968395	25000	25.0	0.0958944583448782
X	0.018957198535610155	7121	7.121	0.1385939877569311
X	0.018726233650839127	1989	1.989	0.2111568711960691
X	0.020905420694998792	116138	116.138	0.056462684530315235
X	0.018882044328951544	17908	17.908	0.10178113957237951
X	0.018085040308642483	1421	1.421	0.23347576176070686
X	0.019503608795074227	59133	59.133	0.06909209394490182
X	0.020060070655801114	12326	12.326	0.11762602790813753
X	0.018850960067013013	1089	1.089	0.2586833825918409
X	0.019104872610513652	73004	73.004	0.0639635407725464
X	0.019121675666901657	90896	90.896	0.059473987681624665
X	0.019209957602151634	100402	100.402	0.05762284287428156
X	0.01919620229355506	110137	110.137	0.055859111707733076
X	0.01936335891958511	163269	163.269	0.04913138281342767
X	0.018843467768224504	14637	14.637	0.10878514261698453
X	0.019013528504856134	26413	26.413	0.0896220752965267
X	0.01923938324739874	128206	128.206	0.05314074008028399
X	0.019766260398604404	119866	119.866	0.05483731300887954
X	0.018491407647709603	5307	5.307	0.15160270897530784
X	0.01950900498955644	66497	66.497	0.06644735361358048
X	0.019447319692264466	158513	158.513	0.049689537682807774
X	0.01915958074497173	109509	109.509	0.0559300742810706
X	0.01936201514057571	102280	102.28	0.05741868983857333
X	0.01895290242283891	52284	52.284	0.07130209769434032
X	0.018784263173119452	5991	5.991	0.14636310522870397
X	0.018637604706304114	3000	3.0	0.18383277676891702
X	0.01872048130703079	4749	4.749	0.15796890712618825
X	0.01943296826446746	82827	82.827	0.06167688099965918
X	0.019125726334070888	87354	87.354	0.06027146024735956
X	0.01942111022010225	326509	326.509	0.03903548970962648
X	0.0216754657418282	84893	84.893	0.06344034249139671
X	0.02190077086426521	68190	68.19	0.06848249434760993
X	0.018877110258459125	13181	13.181	0.11271863410943825
X	0.019348071065309298	98250	98.25	0.05817928392444643
X	0.017935924257984005	2373	2.373	0.19624955215404888
X	0.01889601917160477	8075	8.075	0.13276265533011034
X	0.0190964281246128	144735	144.735	0.050908799869657816
X	0.018671280467917235	4465	4.465	0.1611081632800684
X	0.018781513999612004	3617	3.617	0.17316523098064066
X	0.01911637505182797	11909	11.909	0.11708737845863466
X	0.01958106920358055	232694	232.694	0.043821146817913915
X	0.018803525217332764	16849	16.849	0.10372617954214251
X	0.0207351102758246	150862	150.862	0.05160702566114213
X	0.01833984132154621	10857	10.857	0.11909547072819027
X	0.018632499529219495	15061	15.061	0.10735091985987162
X	0.01907684118201239	5205	5.205	0.15418019008451955
X	0.01884952915634432	2714	2.714	0.1907934909466305
X	0.019138232363670005	50103	50.103	0.07255700775431438
X	0.019324584896384455	28411	28.411	0.08794433809026984
X	0.018951260638016856	3272	3.272	0.179587016908039
X	0.019098354524335462	87845	87.845	0.060130244244064004
X	0.018565084729733856	2267	2.267	0.20156500725404078
X	0.02227043418442406	50398	50.398	0.07616786702919334
X	0.018634211599160178	7596	7.596	0.1348679290840459
X	0.019102209155394474	90644	90.644	0.05950884364409008
X	0.01899628400750866	5265	5.265	0.15337577121087645
X	0.019222851735184864	18574	18.574	0.10115114231443195
X	0.019145708936821743	9336	9.336	0.12704869341575675
X	0.02062721750372686	19766	19.766	0.10143175791953121
X	0.019361723969875167	57384	57.384	0.0696173897144483
X	0.01959320000101315	191838	191.838	0.04674375293762423
X	0.018827628135485795	4734	4.734	0.1584366992720106
X	0.019076302251399606	11426	11.426	0.1186314918031201
X	0.019093882387056466	70658	70.658	0.06465135540605785
X	0.018842706808932283	10521	10.521	0.12144009268039341
X	0.01942568734208103	407071	407.071	0.03627176954415309
X	0.018803958739281815	1622	1.622	0.2263251480991186
X	0.019025343764052384	87618	87.618	0.06010534011639469
X	0.019381213747573935	105492	105.492	0.05684869265629267
X	0.0188427278648656	2176	2.176	0.20535012030492247
X	0.018617938078469107	104822	104.822	0.05621165601211979
X	0.018658131214024255	10626	10.626	0.12064225431049654
X	0.021160288672153457	587623	587.623	0.03302232145286256
X	0.019091104482789892	6652	6.652	0.1421105339794139
X	0.01898285792634425	18405	18.405	0.10103579394490711
X	0.01854437112137053	6073	6.073	0.14507846202561292
X	0.0182683243321828	4208	4.208	0.16313268867453307
X	0.018841234809035764	9141	9.141	0.12726389696278342
X	0.019050492452229055	26494	26.494	0.08958862921250332
X	0.01885821015205037	12084	12.084	0.11599250246717858
X	0.018980731051435305	61803	61.803	0.06746851109852178
X	0.01897385890997721	16224	16.224	0.1053576098732521
X	0.0186593483802202	2955	2.955	0.18483306390041324
X	0.01890437358773416	9805	9.805	0.12446241715890607
X	0.021213568390301578	221428	221.428	0.04575730072625952
X	0.0201660151537573	82222	82.222	0.06259563039940824
X	0.019365789223402704	183563	183.563	0.047251618034720645
X	0.01906184134336694	21735	21.735	0.09571978959792597
X	0.01895353004860212	7890	7.89	0.13392789255381518
X	0.019297518586344393	2110	2.11	0.20912518469541472
X	0.018901862670486807	3823	3.823	0.17035966151139595
X	0.020336650360584962	189588	189.588	0.04751413846557816
X	0.018827951642942174	7370	7.37	0.13670315214469278
X	0.019115686781342504	21670	21.67	0.09590553333941529
X	0.019142029773082055	34465	34.465	0.0821996373506389
X	0.019688033723829483	31542	31.542	0.08546160387852063
X	0.018650253706192	5536	5.536	0.1499096375741595
X	0.018361191701502893	2659	2.659	0.19042655435053596
X	0.01937923426524154	165400	165.4	0.048932834002569835
X	0.0190219940502984	32538	32.538	0.0836158089137668
X	0.01881239674008077	14821	14.821	0.10827350784659648
X	0.019120099044224743	22001	22.001	0.09542948326554498
X	0.02194771606886746	77725	77.725	0.06560589544704212
X	0.020206468230254782	140214	140.214	0.052428368645287576
X	0.02015610386209228	284314	284.314	0.04138752416837621
X	0.019128785119561266	110110	110.11	0.055798202498081664
X	0.019078264031332007	7152	7.152	0.1386874351040976
X	0.019156484324504484	33203	33.203	0.08324908990023228
X	0.01903440197045437	38410	38.41	0.07913433851927423
X	0.01899146766734806	5440	5.44	0.15170033607643268
X	0.01894347050364226	12353	12.353	0.11531762823025697
X	0.019100337267654043	7086	7.086	0.1391703211172737
X	0.018603064404087075	11665	11.665	0.11683328345049822
X	0.018895197723053286	9616	9.616	0.125252286306342
X	0.018500860371662507	1177	1.177	0.2504985141795724
X	0.01897574806173027	12177	12.177	0.11593632591004545
X	0.01912234923805456	8120	8.12	0.13304393215780924
X	0.0206668796682668	30574	30.574	0.08776220977598423
X	0.018436612578803503	5291	5.291	0.1516053250960782
X	0.01907575609023576	20841	20.841	0.09709295875222472
X	0.018276561237503798	21895	21.895	0.09415640005187717
X	0.01955904497031054	11019	11.019	0.12107890575509944
X	0.0185939559752267	3667	3.667	0.17179886730161875
X	0.018966702184048202	10396	10.396	0.12219174062048765
X	0.019025349817456058	17202	17.202	0.10341524595257084
X	0.01909042061968119	187907	187.907	0.04666137290677337
X	0.01896666851464171	5084	5.084	0.15509406205777276
X	0.018505655877982263	1316	1.316	0.24136976312927594
X	0.01864404195234768	4834	4.834	0.15682349984128954
X	0.020898789254630824	169000	169.0	0.049820889136031436
X	0.02031949638686279	223714	223.714	0.04495105878197092
X	0.018808785871249685	32532	32.532	0.08330735250430794
X	0.019485479062782307	7064	7.064	0.14024481019981225
X	0.020624720755860446	47055	47.055	0.07596159929995899
X	0.019652119599467863	120303	120.303	0.05466520490548098
X	0.018074611177543186	2212	2.212	0.20141629993796184
X	0.018864005141437063	7370	7.37	0.136790353776185
X	0.019062951748888016	17222	17.222	0.10344325740063241
X	0.019036242655355827	38583	38.583	0.07901843300737431
X	0.019088528392944757	17140	17.14	0.10365427306159355
X	0.02031483888571536	154919	154.919	0.05080458394387142
X	0.019035379356871142	7327	7.327	0.13747121832921017
X	0.018570112125172296	4429	4.429	0.16125119751766498
X	0.018656941454716167	1726	1.726	0.22110567309477594
X	0.018960435972327324	19090	19.09	0.09977325256612253
X	0.019207787461539123	12319	12.319	0.11595797665406089
X	0.018984649902509112	26339	26.339	0.08966048906769176
X	0.01907969511400473	144214	144.214	0.05095514040071891
X	0.01920857782973272	443124	443.124	0.035128260637054216
X	0.018773632264920254	6260	6.26	0.14420865440140276
X	0.018549753624864905	2038	2.038	0.20879078254824393
X	0.018902265889020237	10514	10.514	0.12159488278075994
X	0.019509386252322198	84297	84.297	0.06139653041169823
X	0.01903608424185043	23841	23.841	0.09277217034825185
X	0.018686447818642732	1756	1.756	0.21995512232224304
X	0.018363467114432257	3353	3.353	0.17626809436732543
X	0.0190303881575599	35515	35.515	0.0812229219956472
X	0.01904662484847761	1721	1.721	0.22284987038487528
X	0.018997988090768858	54620	54.62	0.0703264407956443
X	0.01878336251084385	3513	3.513	0.174863189977413
X	0.02026552032438425	13836	13.836	0.1135661932708944
X	0.019008143051734507	3883	3.883	0.16979468843370918
X	0.018918942570536484	4083	4.083	0.16671416170604628
X	0.018753478193542335	2101	2.101	0.20743653932660577
X	0.019127324788109825	123096	123.096	0.05376134151578341
X	0.019155068576836878	257524	257.524	0.042055422519235545
X	0.021458661596900724	128425	128.425	0.055078785094524255
X	0.019320285187512626	68912	68.912	0.06544948387464994
X	0.019126139568251	23013	23.013	0.09401951958211074
X	0.018964633174476837	7339	7.339	0.13722582581386084
X	0.022323700836067793	8207	8.207	0.13959201837333002
X	0.019131251931502835	109488	109.488	0.05590606898143414
X	0.020980234782058474	26748	26.748	0.0922230914844179
X	0.019409282195748324	50883	50.883	0.0725235278474801
X	0.01868478455991661	10035	10.035	0.12302411209987452
X	0.02204355571310122	547337	547.337	0.03427748482669089
X	0.019006532474602184	18806	18.806	0.10035418464581775
X	0.01968038602739959	50228	50.228	0.07317497382161998
X	0.02139828099899903	136041	136.041	0.05398043212881991
X	0.019055693721519502	27952	27.952	0.08801106819414624
X	0.01909215060134857	36750	36.75	0.08038947072978357
X	0.019163744674401493	204195	204.195	0.045444195984514046
X	0.019307382302347683	43505	43.505	0.07627719974235339
X	0.019114798871619633	28852	28.852	0.08717620042871603
X	0.01904866354289214	9754	9.754	0.1249953681130058
X	0.019332868737471608	50115	50.115	0.07279633627806707
X	0.019161500091511505	10796	10.796	0.12107529704960375
X	0.019134693098528714	122603	122.603	0.0538402169564546
X	0.02030232349892525	799989	799.989	0.02938690908567011
X	0.0194914918523216	89908	89.908	0.06007340488785043
X	0.020226555804245684	160402	160.402	0.05014612581728625
X	0.01910241261932754	42155	42.155	0.07680916479260735
X	0.019126279777413067	28439	28.439	0.08761370800515429
X	0.019128284666176595	18774	18.774	0.1006251183702912
X	0.018710143845637163	8955	8.955	0.12784110389561884
X	0.019054386171450215	7922	7.922	0.133984134410368
X	0.019606387254469677	216153	216.153	0.04493093293514711
X	0.0186707098634229	28334	28.334	0.08701971305400784
X	0.01896483184483058	166181	166.181	0.048505301611992305
X	0.020202167939470408	114601	114.601	0.056070770401411035
X	0.019220200493899397	30139	30.139	0.08607486661699433
X	0.018973621261305973	4526	4.526	0.16124214213315627
X	0.022679595709404428	55605	55.605	0.07416070011714611
X	0.01911045215416127	76691	76.691	0.06292774762297335
X	0.01897678662641641	3420	3.42	0.17703760380167954
X	0.020978498889444048	462860	462.86	0.03565399938953918
X	0.019165140360448786	35540	35.54	0.08139508735459206
X	0.018893763455168418	36527	36.527	0.08027275277507334
X	0.019097488441216427	5525	5.525	0.15119867703130127
X	0.018210436215253447	7108	7.108	0.1368330402895787
X	0.01887915229100001	2216	2.216	0.20423848387408794
X	0.017933189965204852	1283	1.283	0.24088529972122324
X	0.01907566052796674	10402	10.402	0.12240173633343941
X	0.021230057776766984	38017	38.017	0.08234889534158367
X	0.020311838224484697	2217575	2217.575	0.020922984693096003
X	0.018732827355628805	5814	5.814	0.14769850555908765
X	0.019027325776891105	36680	36.68	0.0803494316264197
X	0.019404605744536205	8305	8.305	0.13269515380880173
X	0.018674832710459374	11883	11.883	0.11626353966224329
X	0.0188001300112159	8024	8.024	0.13281791068120205
X	0.020193048884673565	208710	208.71	0.045907771661389006
X	0.01886079512125904	9160	9.16	0.12721983889617733
X	0.02092727734844756	725897	725.897	0.03066285407255059
X	0.019074497325533974	3920	3.92	0.16945550350332023
X	0.0211653084342667	159862	159.862	0.05096747515406207
X	0.019040945079555005	35497	35.497	0.0812516676226947
X	0.019299619599042427	105898	105.898	0.056696162388444694
X	0.020646260256615232	185310	185.31	0.048118732662614294
X	0.019149073710782383	13966	13.966	0.11109433433578449
X	0.01883990157619176	5301	5.301	0.15260670256038378
X	0.02145367989543056	66742	66.742	0.06850162277978299
X	0.018880951073666358	6678	6.678	0.14140317164551286
X	0.018943769567445545	11486	11.486	0.11814967466931936
X	0.019114988257969534	52264	52.264	0.07151390009750432
X	0.019127065378168186	18354	18.354	0.10138472752539351
X	0.022336120208917936	40790	40.79	0.08181218862320434
X	0.018738681121088757	21638	21.638	0.0953177908129462
X	0.019261805454516324	173843	173.843	0.04803007462285396
X	0.018979122123214537	174922	174.922	0.047695480533977624
X	0.019197900544005126	33728	33.728	0.0828745244622897
X	0.01956505073606461	7005	7.005	0.14082862735592105
X	0.018921948557595006	84262	84.262	0.060782426199567086
X	0.019224543943601107	47167	47.167	0.07414341700177557
X	0.01899335754927592	33916	33.916	0.0824262810873472
X	0.018975270420476725	21181	21.181	0.09640077037698094
X	0.01889007428844655	35204	35.204	0.08126069851304331
X	0.018926326110708512	39320	39.32	0.07837019666348259
X	0.018743254340935742	133600	133.6	0.051961240830965705
X	0.01902490064629014	44234	44.234	0.07548408493162762
X	0.020822888029632048	9796	9.796	0.12857722915228811
X	0.018953762503883313	6140	6.14	0.14560479853844543
X	0.019949905617686257	10588	10.588	0.12351194787638992
X	0.01882662627654001	46537	46.537	0.07395905830873399
X	0.019267807746112805	96029	96.029	0.05854322360991538
X	0.01942800742976961	122856	122.856	0.054076755871385586
X	0.018945073886637853	10739	10.739	0.12083075440420082
X	0.019097466746109702	21962	21.962	0.09544824740888058
X	0.01895223505453448	46003	46.003	0.07440887868414447
X	0.01921398877117728	187112	187.112	0.046827966309488794
X	0.019262820529737334	48842	48.842	0.07333458067218408
X	0.019147701427404375	62519	62.519	0.06740646334503297
X	0.01897289018780243	5635	5.635	0.14988098005589373
X	0.019143711874533755	25041	25.041	0.09143760180497072
X	0.018758585591387843	2477	2.477	0.19637733449042466
X	0.019004178712510603	5955	5.955	0.1472275595724235
X	0.019138432419397756	74466	74.466	0.06357935420105869
X	0.019009992038788504	28309	28.309	0.08756941911447061
X	0.01915242285762793	40976	40.976	0.07760649214642396
X	0.019074721388446153	1787	1.787	0.2201800082063854
X	0.018620190434887468	3234	3.234	0.17923164918126205
X	0.018887688035094486	50835	50.835	0.0718905805938985
X	0.01899846225953841	35315	35.315	0.08133043215487964
X	0.019082432205298205	22903	22.903	0.09409801274854868
X	0.018892903776126006	2839	2.839	0.18809522617876695
X	0.018597728346327558	4497	4.497	0.1605137856057316
X	0.019025193733940165	36054	36.054	0.08080877895581044
X	0.0192789146880031	21274	21.274	0.09677083212681312
X	0.019181465643725484	75177	75.177	0.06342574821103938
X	0.018909952150448966	4082	4.082	0.16670136008807113
X	0.01850633430576327	3755	3.755	0.17017802818514885
X	0.018444076135659834	10988	10.988	0.11884459049045522
X	0.018629147789870383	5434	5.434	0.15078487651556793
X	0.01932181924613883	183662	183.662	0.047207344208583135
X	0.018856355698241945	6712	6.712	0.14110268288245936
X	0.018892807125536978	59973	59.973	0.06804249640390204
X	0.019093099978224	106477	106.477	0.056390621368983415
X	0.01903351928051802	111398	111.398	0.055489894725224365
X	0.019035060964506363	139530	139.53	0.05147885664986123
X	0.019148378045382042	106835	106.835	0.05638187048229234
X	0.018723985549792325	3999	3.999	0.16729474439827244
X	0.019195072273978094	39863	39.863	0.07838024031269701
X	0.01913179728133098	19007	19.007	0.10021838469952815
X	0.019782389764004097	71064	71.064	0.06529446509610079
X	0.01870417771247881	94843	94.843	0.058207343495671575
X	0.019102560926714543	20983	20.983	0.0969187946471871
X	0.019106387116720924	63448	63.448	0.06702758086934245
X	0.019024321861333453	5815	5.815	0.14845214892177988
X	0.01892399764606943	19085	19.085	0.09971800326597627
X	0.018962215005109435	6684	6.684	0.14156336425899088
X	0.018782531622435504	8437	8.437	0.13057359452808273
X	0.01939297196277922	175030	175.03	0.04802978312270834
X	0.02179777791763995	320994	320.994	0.040797958916594004
X	0.018945132640936745	12734	12.734	0.11415920479004049
X	0.019063774673664626	21032	21.032	0.09677788075734227
X	0.019122970535879327	18383	18.383	0.10132415475818865
X	0.018972625392310788	95968	95.968	0.05825506116423338
X	0.019007988843973665	16698	16.698	0.10441364026779291
X	0.018872959217407434	9506	9.506	0.1256842131276372
X	0.019243242449358162	50378	50.378	0.07255696527079296
X	0.019217611634803206	118735	118.735	0.05449712002454474
X	0.018848927209484164	8722	8.722	0.12928759346380778
X	0.01933748421564212	75017	75.017	0.06364243093914632
X	0.019080501199843423	3779	3.779	0.17155536157563459
time for making epsilon is 1.805083990097046
epsilons are
[0.2493268217899385, 0.09952512794198931, 0.21107302733430836, 0.11917938130758524, 0.12916503518874758, 0.1009910852197193, 0.0717792255101042, 0.08118916446779477, 0.13457060571242674, 0.09273758776571478, 0.11530960068597963, 0.05989972537306325, 0.1515639254640959, 0.039201070744979374, 0.09868149794468795, 0.07901735182576033, 0.07202711357001325, 0.08610650032923106, 0.044311099759724304, 0.056129671223334336, 0.13144702237501552, 0.04410729131458714, 0.11767806454022023, 0.08777940605625928, 0.18164403904020804, 0.06582882738662289, 0.07562455543714452, 0.15472295785833087, 0.061833772082634506, 0.024964596916468483, 0.14107074797362518, 0.03283069270367751, 0.11103894471222864, 0.14210785213269989, 0.13145809107435513, 0.05549389186787102, 0.05807316424458247, 0.2502958056174207, 0.1696864183126029, 0.20318343833031288, 0.1846945827136501, 0.22585540930299855, 0.29418752202973525, 0.19406216787593297, 0.2686828411107962, 0.21802376085856057, 0.16520420594263427, 0.17926955953774532, 0.23580303822942192, 0.13146721322164637, 0.12236677803734257, 0.21243230881629885, 0.15121639656785207, 0.21075381271711258, 0.15537030717173564, 0.18514603797423884, 0.24634744905637415, 0.18694552241529983, 0.18580984664725977, 0.18535976779354213, 0.1725090476772077, 0.2223114333542817, 0.13628840808763124, 0.14216922174857854, 0.17289853375771613, 0.1942326324237231, 0.17113377910767874, 0.17764795148046816, 0.1783300665651592, 0.1943977940753682, 0.2007956069773928, 0.18219739293194218, 0.1729707212081193, 0.23019805218396724, 0.1739937545545455, 0.16691749342481463, 0.25236223660699836, 0.3186516676501791, 0.1904684836228757, 0.13109950270804166, 0.16670915686295162, 0.2302663989677549, 0.15416504738672027, 0.21753955791925453, 0.18723826542470215, 0.25125252572182527, 0.17175583240843953, 0.24750761049136352, 0.17907831767692453, 0.191397809637125, 0.19167418503678804, 0.23575289818182876, 0.23328630363986613, 0.20744105934625626, 0.15571802564342926, 0.16903125065399024, 0.19679498784108448, 0.12404199410892033, 0.17877465969963094, 0.19864955789508165, 0.12151305480683285, 0.1853391834531241, 0.14022579509666896, 0.23829432260175515, 0.17821424338238537, 0.21186182482740146, 0.1421205264950325, 0.2813244648829219, 0.19049563614120385, 0.23531203982838203, 0.23187126517712253, 0.26333756200271174, 0.21815558745339778, 0.18510502953444458, 0.14951224124187068, 0.22011501684213666, 0.21936165795116855, 0.25746260573075586, 0.1465896225839175, 0.2127678952450977, 0.12241563567415233, 0.20283121590819508, 0.20006430907810127, 0.20717379254198243, 0.23090082014339586, 0.18497503605217744, 0.22140189640719327, 0.19717086034202985, 0.124414456753489, 0.18046675273190463, 0.13452852714752805, 0.23618395583485308, 0.16705859592051778, 0.24589698901294768, 0.18858702383803416, 0.22723816290779067, 0.2461381132190709, 0.24313479580950595, 0.20022412970175835, 0.2302509193288779, 0.20380741360157903, 0.23360555767773553, 0.11982997822117857, 0.12946544654882922, 0.16448717727950127, 0.22096424929092431, 0.21256478535297196, 0.1812592346245221, 0.21652629504706816, 0.20956111231677077, 0.19375950728959535, 0.1956412880495185, 0.1774354474530445, 0.11531712859196781, 0.18979551988382204, 0.11187621868517555, 0.20697484305229513, 0.18786680332472697, 0.16436013712160807, 0.3408413382016029, 0.07811035882616467, 0.19007932593292554, 0.17971694841328822, 0.23488199062754853, 0.18506793189796825, 0.22915274053008816, 0.2058674569485905, 0.17830595936310922, 0.16590724541616325, 0.31510826774556816, 0.22696860614520606, 0.2578870388863156, 0.15182742586405243, 0.2415485469257344, 0.17659313959148917, 0.1911857897983658, 0.17708635987930013, 0.16571127729668378, 0.16166309878635565, 0.2028589849956536, 0.21404771607984036, 0.19360070236677882, 0.17916796500435334, 0.22431544892743613, 0.1605168770193396, 0.16234411837024534, 0.1881202676689612, 0.18172663485193377, 0.15067074290536225, 0.11971953761921329, 0.2242297809846367, 0.3444410653285264, 0.19499907756725013, 0.18042093624564284, 0.20493894891085973, 0.18502215544875988, 0.21775786602111163, 0.2436819959635979, 0.2116583301672682, 0.27079950415636606, 0.1925591837054581, 0.24504091861156438, 0.15415315628094675, 0.1754183028568225, 0.14423062001804904, 0.26381774993108503, 0.16690002320864317, 0.19440978930627853, 0.16889126900871682, 0.15415054020050312, 0.14926544385865398, 0.19833580520013497, 0.1944694765013671, 0.20228772473582662, 0.263106754117988, 0.2175251995999002, 0.2231383437959299, 0.29127380442969786, 0.16225450703299715, 0.21545902998479516, 0.1848872801267691, 0.22621260512636404, 0.1529699256105791, 0.22904792215859282, 0.17091157700050694, 0.15825004769322798, 0.26438821728447004, 0.23688238661611422, 0.17049479169512594, 0.2890051582504895, 0.24561110737314648, 0.17242662731475716, 0.18341561428573863, 0.17492558619009274, 0.23253308057831604, 0.23420796543410674, 0.204418619649793, 0.1585668184121133, 0.2466530540187506, 0.11920030253552998, 0.07642411040150279, 0.13955726767937982, 0.16750555430633135, 0.3096546622229334, 0.13291897512880524, 0.06039852709825655, 0.044316759207201725, 0.20239027002100962, 0.06315310808960121, 0.0893302495825872, 0.09374494079786039, 0.055085051823397076, 0.24351975657910402, 0.17432132173996878, 0.05048742928525169, 0.04298618503343297, 0.18160733941443327, 0.09203431261223148, 0.10820977200775603, 0.0685120143784899, 0.06530058765389418, 0.08516259343303481, 0.09830940561648735, 0.10515506195973959, 0.2186224593528719, 0.0465186663357305, 0.18404459547400925, 0.21194508678796745, 0.07970547003310267, 0.06939335729324654, 0.04971433206493904, 0.06115710497594358, 0.24516919656062594, 0.11092053620862834, 0.12954101543692864, 0.08735787388347449, 0.06983709610809098, 0.17129630586267566, 0.09191597995118814, 0.3164565019943409, 0.17506574889127163, 0.07294012967063739, 0.06294142976505901, 0.07663717738834136, 0.13688424317283743, 0.04230288889935335, 0.1164682040965506, 0.09559076577388545, 0.06511331629812464, 0.19433119464769594, 0.055848181605827205, 0.048056399646365815, 0.04600221007474034, 0.11936461727603861, 0.12681733153936173, 0.12566706956726506, 0.1689444247598306, 0.0820303328651036, 0.14575363496858387, 0.08574242961242604, 0.0616244193268502, 0.1047322005449664, 0.12290682510814298, 0.16287681565675813, 0.06475523814740836, 0.11306798278847346, 0.1158323062746211, 0.13326361398513828, 0.04844656819559248, 0.1244409001941672, 0.05371026515596325, 0.15796773763227162, 0.09723829492502378, 0.1080231683013642, 0.048656363211439915, 0.0820986506575532, 0.16764649430394957, 0.05779638128745492, 0.057253020999596636, 0.11917222723844896, 0.06296899159409722, 0.05605843118184096, 0.04128601305644385, 0.09431123052946311, 0.2471652474399087, 0.12937699639094022, 0.12469796741361472, 0.10473978635444083, 0.10520937193050522, 0.20926081307861422, 0.07023535799247345, 0.05428222310834289, 0.17020729517918265, 0.1507360017761294, 0.11582643628777275, 0.13279026031235724, 0.1602899333925165, 0.12725629742850636, 0.16561859834580792, 0.10036208564661389, 0.08606134860008073, 0.11096429051402841, 0.08739080494784737, 0.13660717023522065, 0.0420116255214104, 0.1586652402629431, 0.06432676020134914, 0.24533303549873053, 0.17205248207588802, 0.17400872728677555, 0.10865704897620215, 0.17512741778819763, 0.05739731947735966, 0.17555087991167698, 0.16679158320318094, 0.04205611514346368, 0.06835753952830803, 0.075271381833204, 0.04885291218915598, 0.04492068817256245, 0.12236673670544301, 0.17147373541211988, 0.10645079047137286, 0.2517848072738155, 0.1655935680498544, 0.11429238654598275, 0.17892099508664885, 0.14257235750881908, 0.20539920874768502, 0.16735387253722592, 0.04073122201312768, 0.11542448651729828, 0.06139205661160917, 0.1430800483056642, 0.10255871457493883, 0.13919719438615777, 0.03418246826679555, 0.03451747104890071, 0.1037302535111034, 0.07517850520691517, 0.19395140496863034, 0.12083240871866008, 0.0764652933917851, 0.10000043305576616, 0.12230894751255329, 0.06954852868056427, 0.03851639358407341, 0.08340616028214662, 0.13148052424178955, 0.0958944583448782, 0.1385939877569311, 0.2111568711960691, 0.056462684530315235, 0.10178113957237951, 0.23347576176070686, 0.06909209394490182, 0.11762602790813753, 0.2586833825918409, 0.0639635407725464, 0.059473987681624665, 0.05762284287428156, 0.055859111707733076, 0.04913138281342767, 0.10878514261698453, 0.0896220752965267, 0.05314074008028399, 0.05483731300887954, 0.15160270897530784, 0.06644735361358048, 0.049689537682807774, 0.0559300742810706, 0.05741868983857333, 0.07130209769434032, 0.14636310522870397, 0.18383277676891702, 0.15796890712618825, 0.06167688099965918, 0.06027146024735956, 0.03903548970962648, 0.06344034249139671, 0.06848249434760993, 0.11271863410943825, 0.05817928392444643, 0.19624955215404888, 0.13276265533011034, 0.050908799869657816, 0.1611081632800684, 0.17316523098064066, 0.11708737845863466, 0.043821146817913915, 0.10372617954214251, 0.05160702566114213, 0.11909547072819027, 0.10735091985987162, 0.15418019008451955, 0.1907934909466305, 0.07255700775431438, 0.08794433809026984, 0.179587016908039, 0.060130244244064004, 0.20156500725404078, 0.07616786702919334, 0.1348679290840459, 0.05950884364409008, 0.15337577121087645, 0.10115114231443195, 0.12704869341575675, 0.10143175791953121, 0.0696173897144483, 0.04674375293762423, 0.1584366992720106, 0.1186314918031201, 0.06465135540605785, 0.12144009268039341, 0.03627176954415309, 0.2263251480991186, 0.06010534011639469, 0.05684869265629267, 0.20535012030492247, 0.05621165601211979, 0.12064225431049654, 0.03302232145286256, 0.1421105339794139, 0.10103579394490711, 0.14507846202561292, 0.16313268867453307, 0.12726389696278342, 0.08958862921250332, 0.11599250246717858, 0.06746851109852178, 0.1053576098732521, 0.18483306390041324, 0.12446241715890607, 0.04575730072625952, 0.06259563039940824, 0.047251618034720645, 0.09571978959792597, 0.13392789255381518, 0.20912518469541472, 0.17035966151139595, 0.04751413846557816, 0.13670315214469278, 0.09590553333941529, 0.0821996373506389, 0.08546160387852063, 0.1499096375741595, 0.19042655435053596, 0.048932834002569835, 0.0836158089137668, 0.10827350784659648, 0.09542948326554498, 0.06560589544704212, 0.052428368645287576, 0.04138752416837621, 0.055798202498081664, 0.1386874351040976, 0.08324908990023228, 0.07913433851927423, 0.15170033607643268, 0.11531762823025697, 0.1391703211172737, 0.11683328345049822, 0.125252286306342, 0.2504985141795724, 0.11593632591004545, 0.13304393215780924, 0.08776220977598423, 0.1516053250960782, 0.09709295875222472, 0.09415640005187717, 0.12107890575509944, 0.17179886730161875, 0.12219174062048765, 0.10341524595257084, 0.04666137290677337, 0.15509406205777276, 0.24136976312927594, 0.15682349984128954, 0.049820889136031436, 0.04495105878197092, 0.08330735250430794, 0.14024481019981225, 0.07596159929995899, 0.05466520490548098, 0.20141629993796184, 0.136790353776185, 0.10344325740063241, 0.07901843300737431, 0.10365427306159355, 0.05080458394387142, 0.13747121832921017, 0.16125119751766498, 0.22110567309477594, 0.09977325256612253, 0.11595797665406089, 0.08966048906769176, 0.05095514040071891, 0.035128260637054216, 0.14420865440140276, 0.20879078254824393, 0.12159488278075994, 0.06139653041169823, 0.09277217034825185, 0.21995512232224304, 0.17626809436732543, 0.0812229219956472, 0.22284987038487528, 0.0703264407956443, 0.174863189977413, 0.1135661932708944, 0.16979468843370918, 0.16671416170604628, 0.20743653932660577, 0.05376134151578341, 0.042055422519235545, 0.055078785094524255, 0.06544948387464994, 0.09401951958211074, 0.13722582581386084, 0.13959201837333002, 0.05590606898143414, 0.0922230914844179, 0.0725235278474801, 0.12302411209987452, 0.03427748482669089, 0.10035418464581775, 0.07317497382161998, 0.05398043212881991, 0.08801106819414624, 0.08038947072978357, 0.045444195984514046, 0.07627719974235339, 0.08717620042871603, 0.1249953681130058, 0.07279633627806707, 0.12107529704960375, 0.0538402169564546, 0.02938690908567011, 0.06007340488785043, 0.05014612581728625, 0.07680916479260735, 0.08761370800515429, 0.1006251183702912, 0.12784110389561884, 0.133984134410368, 0.04493093293514711, 0.08701971305400784, 0.048505301611992305, 0.056070770401411035, 0.08607486661699433, 0.16124214213315627, 0.07416070011714611, 0.06292774762297335, 0.17703760380167954, 0.03565399938953918, 0.08139508735459206, 0.08027275277507334, 0.15119867703130127, 0.1368330402895787, 0.20423848387408794, 0.24088529972122324, 0.12240173633343941, 0.08234889534158367, 0.020922984693096003, 0.14769850555908765, 0.0803494316264197, 0.13269515380880173, 0.11626353966224329, 0.13281791068120205, 0.045907771661389006, 0.12721983889617733, 0.03066285407255059, 0.16945550350332023, 0.05096747515406207, 0.0812516676226947, 0.056696162388444694, 0.048118732662614294, 0.11109433433578449, 0.15260670256038378, 0.06850162277978299, 0.14140317164551286, 0.11814967466931936, 0.07151390009750432, 0.10138472752539351, 0.08181218862320434, 0.0953177908129462, 0.04803007462285396, 0.047695480533977624, 0.0828745244622897, 0.14082862735592105, 0.060782426199567086, 0.07414341700177557, 0.0824262810873472, 0.09640077037698094, 0.08126069851304331, 0.07837019666348259, 0.051961240830965705, 0.07548408493162762, 0.12857722915228811, 0.14560479853844543, 0.12351194787638992, 0.07395905830873399, 0.05854322360991538, 0.054076755871385586, 0.12083075440420082, 0.09544824740888058, 0.07440887868414447, 0.046827966309488794, 0.07333458067218408, 0.06740646334503297, 0.14988098005589373, 0.09143760180497072, 0.19637733449042466, 0.1472275595724235, 0.06357935420105869, 0.08756941911447061, 0.07760649214642396, 0.2201800082063854, 0.17923164918126205, 0.0718905805938985, 0.08133043215487964, 0.09409801274854868, 0.18809522617876695, 0.1605137856057316, 0.08080877895581044, 0.09677083212681312, 0.06342574821103938, 0.16670136008807113, 0.17017802818514885, 0.11884459049045522, 0.15078487651556793, 0.047207344208583135, 0.14110268288245936, 0.06804249640390204, 0.056390621368983415, 0.055489894725224365, 0.05147885664986123, 0.05638187048229234, 0.16729474439827244, 0.07838024031269701, 0.10021838469952815, 0.06529446509610079, 0.058207343495671575, 0.0969187946471871, 0.06702758086934245, 0.14845214892177988, 0.09971800326597627, 0.14156336425899088, 0.13057359452808273, 0.04802978312270834, 0.040797958916594004, 0.11415920479004049, 0.09677788075734227, 0.10132415475818865, 0.05825506116423338, 0.10441364026779291, 0.1256842131276372, 0.07255696527079296, 0.05449712002454474, 0.12928759346380778, 0.06364243093914632, 0.17155536157563459]
0.1031128572810163
Making ranges
torch.Size([31625, 2])
We keep 7.09e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([2773, 2])
We keep 1.01e+05/1.48e+06 =  6% of the original kernel matrix.

torch.Size([10186, 2])
We keep 9.22e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([28038, 2])
We keep 7.36e+06/3.69e+08 =  1% of the original kernel matrix.

torch.Size([29299, 2])
We keep 6.96e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([4453, 2])
We keep 2.16e+05/4.02e+06 =  5% of the original kernel matrix.

torch.Size([12292, 2])
We keep 1.28e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([13152, 2])
We keep 6.29e+06/1.26e+08 =  4% of the original kernel matrix.

torch.Size([19255, 2])
We keep 4.52e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([14918, 2])
We keep 2.50e+06/7.67e+07 =  3% of the original kernel matrix.

torch.Size([20820, 2])
We keep 3.67e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([27269, 2])
We keep 6.72e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([28863, 2])
We keep 6.75e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([73980, 2])
We keep 3.68e+07/2.65e+09 =  1% of the original kernel matrix.

torch.Size([46330, 2])
We keep 1.56e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([50943, 2])
We keep 1.74e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([39516, 2])
We keep 1.14e+07/7.79e+08 =  1% of the original kernel matrix.

torch.Size([12594, 2])
We keep 2.72e+06/6.12e+07 =  4% of the original kernel matrix.

torch.Size([19149, 2])
We keep 3.44e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([29271, 2])
We keep 2.31e+07/5.63e+08 =  4% of the original kernel matrix.

torch.Size([29220, 2])
We keep 7.93e+06/5.18e+08 =  1% of the original kernel matrix.

torch.Size([20353, 2])
We keep 3.92e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([24458, 2])
We keep 4.88e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([117561, 2])
We keep 1.30e+08/7.90e+09 =  1% of the original kernel matrix.

torch.Size([58541, 2])
We keep 2.49e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([10550, 2])
We keep 1.17e+06/3.50e+07 =  3% of the original kernel matrix.

torch.Size([17423, 2])
We keep 2.84e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([474739, 2])
We keep 8.44e+08/1.01e+11 =  0% of the original kernel matrix.

torch.Size([122914, 2])
We keep 7.67e+07/6.93e+09 =  1% of the original kernel matrix.

torch.Size([28928, 2])
We keep 6.66e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([29690, 2])
We keep 7.05e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([56537, 2])
We keep 2.38e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([41325, 2])
We keep 1.23e+07/8.55e+08 =  1% of the original kernel matrix.

torch.Size([75182, 2])
We keep 4.16e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([46933, 2])
We keep 1.54e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([41941, 2])
We keep 1.67e+07/8.94e+08 =  1% of the original kernel matrix.

torch.Size([35199, 2])
We keep 9.82e+06/6.53e+08 =  1% of the original kernel matrix.

torch.Size([305060, 2])
We keep 5.42e+08/4.84e+10 =  1% of the original kernel matrix.

torch.Size([99590, 2])
We keep 5.50e+07/4.80e+09 =  1% of the original kernel matrix.

torch.Size([157517, 2])
We keep 1.67e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([69305, 2])
We keep 2.93e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([13006, 2])
We keep 2.65e+06/7.08e+07 =  3% of the original kernel matrix.

torch.Size([19138, 2])
We keep 3.68e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([316982, 2])
We keep 5.44e+08/4.98e+10 =  1% of the original kernel matrix.

torch.Size([101767, 2])
We keep 5.42e+07/4.88e+09 =  1% of the original kernel matrix.

torch.Size([19246, 2])
We keep 3.61e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([23907, 2])
We keep 4.63e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([39203, 2])
We keep 3.88e+07/8.48e+08 =  4% of the original kernel matrix.

torch.Size([34376, 2])
We keep 9.99e+06/6.36e+08 =  1% of the original kernel matrix.

torch.Size([5565, 2])
We keep 8.23e+05/9.41e+06 =  8% of the original kernel matrix.

torch.Size([13039, 2])
We keep 1.59e+06/6.70e+07 =  2% of the original kernel matrix.

torch.Size([101024, 2])
We keep 5.10e+07/4.50e+09 =  1% of the original kernel matrix.

torch.Size([54308, 2])
We keep 1.93e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([55160, 2])
We keep 4.64e+07/2.27e+09 =  2% of the original kernel matrix.

torch.Size([39986, 2])
We keep 1.49e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([10138, 2])
We keep 8.80e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([17235, 2])
We keep 2.45e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([94500, 2])
We keep 1.98e+08/6.39e+09 =  3% of the original kernel matrix.

torch.Size([52488, 2])
We keep 2.24e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([2046861, 2])
We keep 8.68e+09/1.54e+12 =  0% of the original kernel matrix.

torch.Size([270551, 2])
We keep 2.64e+08/2.71e+10 =  0% of the original kernel matrix.

torch.Size([11317, 2])
We keep 2.54e+06/4.69e+07 =  5% of the original kernel matrix.

torch.Size([18325, 2])
We keep 3.14e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([775419, 2])
We keep 3.62e+09/3.51e+11 =  1% of the original kernel matrix.

torch.Size([162297, 2])
We keep 1.37e+08/1.29e+10 =  1% of the original kernel matrix.

torch.Size([20756, 2])
We keep 7.03e+06/1.84e+08 =  3% of the original kernel matrix.

torch.Size([24677, 2])
We keep 5.31e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([12881, 2])
We keep 1.51e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([19307, 2])
We keep 2.98e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([14094, 2])
We keep 2.28e+06/6.83e+07 =  3% of the original kernel matrix.

torch.Size([20086, 2])
We keep 3.58e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([151411, 2])
We keep 2.87e+08/1.29e+10 =  2% of the original kernel matrix.

torch.Size([67897, 2])
We keep 3.12e+07/2.48e+09 =  1% of the original kernel matrix.

torch.Size([142922, 2])
We keep 1.70e+08/9.55e+09 =  1% of the original kernel matrix.

torch.Size([65550, 2])
We keep 2.71e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([2640, 2])
We keep 8.88e+04/1.35e+06 =  6% of the original kernel matrix.

torch.Size([9914, 2])
We keep 8.93e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([7743, 2])
We keep 7.09e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([15289, 2])
We keep 2.16e+06/8.99e+07 =  2% of the original kernel matrix.

torch.Size([4647, 2])
We keep 2.61e+05/4.85e+06 =  5% of the original kernel matrix.

torch.Size([12242, 2])
We keep 1.39e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([6539, 2])
We keep 3.94e+05/8.99e+06 =  4% of the original kernel matrix.

torch.Size([14427, 2])
We keep 1.68e+06/6.55e+07 =  2% of the original kernel matrix.

torch.Size([2967, 2])
We keep 1.87e+05/2.47e+06 =  7% of the original kernel matrix.

torch.Size([9955, 2])
We keep 1.07e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([1666, 2])
We keep 4.07e+04/5.04e+05 =  8% of the original kernel matrix.

torch.Size([8273, 2])
We keep 6.32e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([5214, 2])
We keep 3.28e+05/6.71e+06 =  4% of the original kernel matrix.

torch.Size([12701, 2])
We keep 1.55e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([2335, 2])
We keep 6.65e+04/9.43e+05 =  7% of the original kernel matrix.

torch.Size([9683, 2])
We keep 7.82e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([3876, 2])
We keep 1.83e+05/3.28e+06 =  5% of the original kernel matrix.

torch.Size([11527, 2])
We keep 1.19e+06/3.95e+07 =  3% of the original kernel matrix.

torch.Size([8360, 2])
We keep 7.33e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([15826, 2])
We keep 2.12e+06/8.99e+07 =  2% of the original kernel matrix.

torch.Size([6406, 2])
We keep 4.48e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([13793, 2])
We keep 1.78e+06/7.06e+07 =  2% of the original kernel matrix.

torch.Size([3399, 2])
We keep 1.30e+05/1.94e+06 =  6% of the original kernel matrix.

torch.Size([10894, 2])
We keep 9.92e+05/3.04e+07 =  3% of the original kernel matrix.

torch.Size([14155, 2])
We keep 2.35e+06/6.87e+07 =  3% of the original kernel matrix.

torch.Size([20126, 2])
We keep 3.60e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([16633, 2])
We keep 2.95e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([22046, 2])
We keep 4.27e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([4201, 2])
We keep 1.95e+05/3.76e+06 =  5% of the original kernel matrix.

torch.Size([11724, 2])
We keep 1.25e+06/4.24e+07 =  2% of the original kernel matrix.

torch.Size([10001, 2])
We keep 1.22e+06/3.02e+07 =  4% of the original kernel matrix.

torch.Size([17100, 2])
We keep 2.64e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([4080, 2])
We keep 2.03e+05/3.86e+06 =  5% of the original kernel matrix.

torch.Size([11654, 2])
We keep 1.27e+06/4.29e+07 =  2% of the original kernel matrix.

torch.Size([10224, 2])
We keep 8.77e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([17221, 2])
We keep 2.45e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([5766, 2])
We keep 4.78e+05/8.79e+06 =  5% of the original kernel matrix.

torch.Size([13338, 2])
We keep 1.70e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([3072, 2])
We keep 9.20e+04/1.50e+06 =  6% of the original kernel matrix.

torch.Size([10679, 2])
We keep 8.97e+05/2.67e+07 =  3% of the original kernel matrix.

torch.Size([5778, 2])
We keep 3.83e+05/8.47e+06 =  4% of the original kernel matrix.

torch.Size([13383, 2])
We keep 1.66e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([5840, 2])
We keep 3.77e+05/8.44e+06 =  4% of the original kernel matrix.

torch.Size([13457, 2])
We keep 1.65e+06/6.34e+07 =  2% of the original kernel matrix.

torch.Size([5831, 2])
We keep 4.61e+05/8.55e+06 =  5% of the original kernel matrix.

torch.Size([13518, 2])
We keep 1.68e+06/6.38e+07 =  2% of the original kernel matrix.

torch.Size([7458, 2])
We keep 5.55e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([15088, 2])
We keep 1.96e+06/8.08e+07 =  2% of the original kernel matrix.

torch.Size([3869, 2])
We keep 1.67e+05/2.89e+06 =  5% of the original kernel matrix.

torch.Size([11482, 2])
We keep 1.12e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([13047, 2])
We keep 2.22e+06/6.56e+07 =  3% of the original kernel matrix.

torch.Size([19502, 2])
We keep 3.62e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([12578, 2])
We keep 1.31e+06/4.28e+07 =  3% of the original kernel matrix.

torch.Size([19025, 2])
We keep 3.02e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([6987, 2])
We keep 5.14e+05/1.30e+07 =  3% of the original kernel matrix.

torch.Size([14411, 2])
We keep 1.92e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([5372, 2])
We keep 3.01e+05/6.73e+06 =  4% of the original kernel matrix.

torch.Size([13116, 2])
We keep 1.53e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([7650, 2])
We keep 6.19e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([15147, 2])
We keep 1.98e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([6916, 2])
We keep 4.65e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([14388, 2])
We keep 1.79e+06/7.09e+07 =  2% of the original kernel matrix.

torch.Size([6883, 2])
We keep 4.74e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([14512, 2])
We keep 1.80e+06/7.27e+07 =  2% of the original kernel matrix.

torch.Size([5401, 2])
We keep 3.03e+05/6.23e+06 =  4% of the original kernel matrix.

torch.Size([13015, 2])
We keep 1.49e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([4490, 2])
We keep 3.25e+05/5.38e+06 =  6% of the original kernel matrix.

torch.Size([11933, 2])
We keep 1.44e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([6367, 2])
We keep 4.31e+05/9.39e+06 =  4% of the original kernel matrix.

torch.Size([13791, 2])
We keep 1.74e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([7289, 2])
We keep 5.32e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([14724, 2])
We keep 1.94e+06/7.94e+07 =  2% of the original kernel matrix.

torch.Size([3302, 2])
We keep 1.42e+05/2.40e+06 =  5% of the original kernel matrix.

torch.Size([10733, 2])
We keep 1.08e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([6945, 2])
We keep 5.58e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([14414, 2])
We keep 1.94e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([7832, 2])
We keep 7.95e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([15226, 2])
We keep 2.10e+06/8.89e+07 =  2% of the original kernel matrix.

torch.Size([2802, 2])
We keep 8.07e+04/1.29e+06 =  6% of the original kernel matrix.

torch.Size([10351, 2])
We keep 8.48e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([1500, 2])
We keep 2.59e+04/3.02e+05 =  8% of the original kernel matrix.

torch.Size([8295, 2])
We keep 5.38e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([5933, 2])
We keep 3.37e+05/7.52e+06 =  4% of the original kernel matrix.

torch.Size([13648, 2])
We keep 1.60e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([14935, 2])
We keep 1.95e+06/6.98e+07 =  2% of the original kernel matrix.

torch.Size([20613, 2])
We keep 3.64e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([8011, 2])
We keep 6.55e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([15250, 2])
We keep 2.09e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([2958, 2])
We keep 1.54e+05/2.11e+06 =  7% of the original kernel matrix.

torch.Size([10030, 2])
We keep 1.02e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([10014, 2])
We keep 1.03e+06/2.69e+07 =  3% of the original kernel matrix.

torch.Size([17204, 2])
We keep 2.54e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([3814, 2])
We keep 1.85e+05/3.20e+06 =  5% of the original kernel matrix.

torch.Size([11248, 2])
We keep 1.19e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([5716, 2])
We keep 3.76e+05/7.86e+06 =  4% of the original kernel matrix.

torch.Size([13327, 2])
We keep 1.61e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([2705, 2])
We keep 8.91e+04/1.41e+06 =  6% of the original kernel matrix.

torch.Size([10147, 2])
We keep 8.91e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([7380, 2])
We keep 6.84e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([14910, 2])
We keep 1.99e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([3123, 2])
We keep 9.21e+04/1.43e+06 =  6% of the original kernel matrix.

torch.Size([10783, 2])
We keep 8.86e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([6301, 2])
We keep 4.84e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([13948, 2])
We keep 1.79e+06/7.09e+07 =  2% of the original kernel matrix.

torch.Size([5934, 2])
We keep 3.39e+05/7.27e+06 =  4% of the original kernel matrix.

torch.Size([13657, 2])
We keep 1.57e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([5735, 2])
We keep 3.63e+05/7.31e+06 =  4% of the original kernel matrix.

torch.Size([13467, 2])
We keep 1.59e+06/5.90e+07 =  2% of the original kernel matrix.

torch.Size([3400, 2])
We keep 1.14e+05/2.01e+06 =  5% of the original kernel matrix.

torch.Size([11061, 2])
We keep 1.00e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([3400, 2])
We keep 1.26e+05/2.13e+06 =  5% of the original kernel matrix.

torch.Size([10976, 2])
We keep 1.03e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([4351, 2])
We keep 2.18e+05/4.26e+06 =  5% of the original kernel matrix.

torch.Size([11937, 2])
We keep 1.29e+06/4.50e+07 =  2% of the original kernel matrix.

torch.Size([9647, 2])
We keep 1.07e+06/2.72e+07 =  3% of the original kernel matrix.

torch.Size([16688, 2])
We keep 2.59e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([7854, 2])
We keep 6.02e+05/1.53e+07 =  3% of the original kernel matrix.

torch.Size([15376, 2])
We keep 2.06e+06/8.55e+07 =  2% of the original kernel matrix.

torch.Size([5315, 2])
We keep 3.05e+05/6.23e+06 =  4% of the original kernel matrix.

torch.Size([12912, 2])
We keep 1.51e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([14572, 2])
We keep 3.68e+06/9.71e+07 =  3% of the original kernel matrix.

torch.Size([20417, 2])
We keep 4.15e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([6486, 2])
We keep 4.81e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([13953, 2])
We keep 1.84e+06/7.26e+07 =  2% of the original kernel matrix.

torch.Size([5281, 2])
We keep 2.68e+05/5.67e+06 =  4% of the original kernel matrix.

torch.Size([13104, 2])
We keep 1.43e+06/5.20e+07 =  2% of the original kernel matrix.

torch.Size([17879, 2])
We keep 2.92e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([22822, 2])
We keep 4.37e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([6140, 2])
We keep 4.12e+05/8.84e+06 =  4% of the original kernel matrix.

torch.Size([13845, 2])
We keep 1.69e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([12543, 2])
We keep 1.49e+06/4.68e+07 =  3% of the original kernel matrix.

torch.Size([18953, 2])
We keep 3.09e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([2855, 2])
We keep 1.17e+05/1.79e+06 =  6% of the original kernel matrix.

torch.Size([10145, 2])
We keep 9.68e+05/2.92e+07 =  3% of the original kernel matrix.

torch.Size([6532, 2])
We keep 4.50e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([14118, 2])
We keep 1.77e+06/7.06e+07 =  2% of the original kernel matrix.

torch.Size([4337, 2])
We keep 2.17e+05/3.92e+06 =  5% of the original kernel matrix.

torch.Size([12062, 2])
We keep 1.26e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([13021, 2])
We keep 1.36e+06/4.42e+07 =  3% of the original kernel matrix.

torch.Size([19341, 2])
We keep 3.03e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([1898, 2])
We keep 5.25e+04/6.84e+05 =  7% of the original kernel matrix.

torch.Size([8793, 2])
We keep 7.08e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([5703, 2])
We keep 3.66e+05/7.52e+06 =  4% of the original kernel matrix.

torch.Size([13492, 2])
We keep 1.59e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([3398, 2])
We keep 1.22e+05/1.98e+06 =  6% of the original kernel matrix.

torch.Size([10970, 2])
We keep 1.01e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([3240, 2])
We keep 1.26e+05/2.19e+06 =  5% of the original kernel matrix.

torch.Size([10687, 2])
We keep 1.04e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([2358, 2])
We keep 7.93e+04/1.02e+06 =  7% of the original kernel matrix.

torch.Size([9567, 2])
We keep 7.99e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([3638, 2])
We keep 1.98e+05/3.20e+06 =  6% of the original kernel matrix.

torch.Size([10984, 2])
We keep 1.20e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([6065, 2])
We keep 3.62e+05/8.67e+06 =  4% of the original kernel matrix.

torch.Size([13920, 2])
We keep 1.65e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([10979, 2])
We keep 1.26e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([17840, 2])
We keep 2.87e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([3689, 2])
We keep 1.76e+05/2.95e+06 =  5% of the original kernel matrix.

torch.Size([11190, 2])
We keep 1.16e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([3880, 2])
We keep 1.71e+05/3.09e+06 =  5% of the original kernel matrix.

torch.Size([11565, 2])
We keep 1.16e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([2706, 2])
We keep 7.93e+04/1.19e+06 =  6% of the original kernel matrix.

torch.Size([10150, 2])
We keep 8.41e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([10997, 2])
We keep 1.63e+06/4.23e+07 =  3% of the original kernel matrix.

torch.Size([17847, 2])
We keep 3.05e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([4202, 2])
We keep 1.93e+05/3.61e+06 =  5% of the original kernel matrix.

torch.Size([11851, 2])
We keep 1.23e+06/4.15e+07 =  2% of the original kernel matrix.

torch.Size([17061, 2])
We keep 2.77e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([22359, 2])
We keep 4.28e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([4771, 2])
We keep 2.45e+05/4.67e+06 =  5% of the original kernel matrix.

torch.Size([12504, 2])
We keep 1.34e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([4565, 2])
We keep 2.67e+05/5.14e+06 =  5% of the original kernel matrix.

torch.Size([12110, 2])
We keep 1.40e+06/4.95e+07 =  2% of the original kernel matrix.

torch.Size([4755, 2])
We keep 2.36e+05/4.52e+06 =  5% of the original kernel matrix.

torch.Size([12561, 2])
We keep 1.32e+06/4.64e+07 =  2% of the original kernel matrix.

torch.Size([3510, 2])
We keep 1.33e+05/2.20e+06 =  6% of the original kernel matrix.

torch.Size([10972, 2])
We keep 1.04e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([6474, 2])
We keep 4.65e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([14264, 2])
We keep 1.81e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([3994, 2])
We keep 1.67e+05/2.95e+06 =  5% of the original kernel matrix.

torch.Size([11635, 2])
We keep 1.15e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([5253, 2])
We keep 3.44e+05/5.98e+06 =  5% of the original kernel matrix.

torch.Size([12996, 2])
We keep 1.48e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([17584, 2])
We keep 2.91e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([22597, 2])
We keep 4.50e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([6675, 2])
We keep 4.44e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([14172, 2])
We keep 1.78e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([14063, 2])
We keep 1.83e+06/6.06e+07 =  3% of the original kernel matrix.

torch.Size([20113, 2])
We keep 3.43e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([2989, 2])
We keep 1.31e+05/2.06e+06 =  6% of the original kernel matrix.

torch.Size([10290, 2])
We keep 1.01e+06/3.13e+07 =  3% of the original kernel matrix.

torch.Size([7626, 2])
We keep 6.68e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([14931, 2])
We keep 2.08e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([2739, 2])
We keep 1.24e+05/1.55e+06 =  7% of the original kernel matrix.

torch.Size([9900, 2])
We keep 9.25e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([5852, 2])
We keep 3.52e+05/7.32e+06 =  4% of the original kernel matrix.

torch.Size([13467, 2])
We keep 1.57e+06/5.91e+07 =  2% of the original kernel matrix.

torch.Size([3385, 2])
We keep 1.54e+05/2.52e+06 =  6% of the original kernel matrix.

torch.Size([10949, 2])
We keep 1.08e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([2950, 2])
We keep 9.70e+04/1.58e+06 =  6% of the original kernel matrix.

torch.Size([10313, 2])
We keep 9.33e+05/2.74e+07 =  3% of the original kernel matrix.

torch.Size([2761, 2])
We keep 1.01e+05/1.61e+06 =  6% of the original kernel matrix.

torch.Size([10012, 2])
We keep 9.40e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([4530, 2])
We keep 2.66e+05/5.17e+06 =  5% of the original kernel matrix.

torch.Size([12101, 2])
We keep 1.41e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([3540, 2])
We keep 1.27e+05/2.31e+06 =  5% of the original kernel matrix.

torch.Size([11258, 2])
We keep 1.03e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([4687, 2])
We keep 2.51e+05/4.88e+06 =  5% of the original kernel matrix.

torch.Size([12366, 2])
We keep 1.38e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([3516, 2])
We keep 1.27e+05/2.16e+06 =  5% of the original kernel matrix.

torch.Size([11209, 2])
We keep 1.04e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([17589, 2])
We keep 3.72e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([22726, 2])
We keep 4.93e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([14947, 2])
We keep 2.10e+06/7.66e+07 =  2% of the original kernel matrix.

torch.Size([20787, 2])
We keep 3.74e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([8431, 2])
We keep 7.05e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([15677, 2])
We keep 2.18e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([3807, 2])
We keep 1.70e+05/2.99e+06 =  5% of the original kernel matrix.

torch.Size([11400, 2])
We keep 1.16e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([4315, 2])
We keep 2.14e+05/3.69e+06 =  5% of the original kernel matrix.

torch.Size([12137, 2])
We keep 1.24e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([6664, 2])
We keep 6.77e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([14650, 2])
We keep 2.04e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([4072, 2])
We keep 1.85e+05/3.37e+06 =  5% of the original kernel matrix.

torch.Size([11761, 2])
We keep 1.21e+06/4.01e+07 =  3% of the original kernel matrix.

torch.Size([4014, 2])
We keep 2.39e+05/4.18e+06 =  5% of the original kernel matrix.

torch.Size([11676, 2])
We keep 1.30e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([5205, 2])
We keep 3.05e+05/6.18e+06 =  4% of the original kernel matrix.

torch.Size([12712, 2])
We keep 1.48e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([5109, 2])
We keep 3.18e+05/6.37e+06 =  4% of the original kernel matrix.

torch.Size([12755, 2])
We keep 1.51e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([7094, 2])
We keep 5.01e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([14734, 2])
We keep 1.86e+06/7.42e+07 =  2% of the original kernel matrix.

torch.Size([16533, 2])
We keep 5.61e+06/1.74e+08 =  3% of the original kernel matrix.

torch.Size([21777, 2])
We keep 5.25e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([5565, 2])
We keep 3.60e+05/7.36e+06 =  4% of the original kernel matrix.

torch.Size([13062, 2])
We keep 1.58e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([20993, 2])
We keep 5.49e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([25198, 2])
We keep 5.86e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([4961, 2])
We keep 2.29e+05/4.53e+06 =  5% of the original kernel matrix.

torch.Size([12725, 2])
We keep 1.33e+06/4.65e+07 =  2% of the original kernel matrix.

torch.Size([5464, 2])
We keep 4.99e+05/8.07e+06 =  6% of the original kernel matrix.

torch.Size([12938, 2])
We keep 1.64e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([8744, 2])
We keep 7.86e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([16170, 2])
We keep 2.21e+06/9.39e+07 =  2% of the original kernel matrix.

torch.Size([1260, 2])
We keep 2.03e+04/1.91e+05 = 10% of the original kernel matrix.

torch.Size([7665, 2])
We keep 4.61e+05/9.54e+06 =  4% of the original kernel matrix.

torch.Size([60842, 2])
We keep 2.73e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([44346, 2])
We keep 1.45e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([5382, 2])
We keep 4.40e+05/7.56e+06 =  5% of the original kernel matrix.

torch.Size([12924, 2])
We keep 1.61e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([6368, 2])
We keep 4.84e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([13948, 2])
We keep 1.86e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([3329, 2])
We keep 1.26e+05/2.02e+06 =  6% of the original kernel matrix.

torch.Size([10805, 2])
We keep 1.02e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([5684, 2])
We keep 3.72e+05/8.21e+06 =  4% of the original kernel matrix.

torch.Size([13204, 2])
We keep 1.63e+06/6.26e+07 =  2% of the original kernel matrix.

torch.Size([3464, 2])
We keep 1.37e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([10988, 2])
We keep 1.07e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([4721, 2])
We keep 2.57e+05/4.75e+06 =  5% of the original kernel matrix.

torch.Size([12547, 2])
We keep 1.31e+06/4.76e+07 =  2% of the original kernel matrix.

torch.Size([6894, 2])
We keep 4.59e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([14526, 2])
We keep 1.78e+06/7.10e+07 =  2% of the original kernel matrix.

torch.Size([8409, 2])
We keep 6.79e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([15685, 2])
We keep 2.14e+06/9.02e+07 =  2% of the original kernel matrix.

torch.Size([1518, 2])
We keep 3.02e+04/3.39e+05 =  8% of the original kernel matrix.

torch.Size([8285, 2])
We keep 5.64e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([3263, 2])
We keep 1.40e+05/2.25e+06 =  6% of the original kernel matrix.

torch.Size([10568, 2])
We keep 1.02e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([2537, 2])
We keep 7.37e+04/1.13e+06 =  6% of the original kernel matrix.

torch.Size([9760, 2])
We keep 8.23e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([10284, 2])
We keep 1.05e+06/2.97e+07 =  3% of the original kernel matrix.

torch.Size([17174, 2])
We keep 2.61e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([2824, 2])
We keep 1.14e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([10145, 2])
We keep 9.43e+05/2.85e+07 =  3% of the original kernel matrix.

torch.Size([7118, 2])
We keep 6.02e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([14928, 2])
We keep 2.00e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([5820, 2])
We keep 4.14e+05/8.79e+06 =  4% of the original kernel matrix.

torch.Size([13662, 2])
We keep 1.71e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([7064, 2])
We keep 4.85e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([14542, 2])
We keep 1.81e+06/7.26e+07 =  2% of the original kernel matrix.

torch.Size([8895, 2])
We keep 9.25e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([16231, 2])
We keep 2.52e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([8339, 2])
We keep 8.70e+05/1.98e+07 =  4% of the original kernel matrix.

torch.Size([15612, 2])
We keep 2.24e+06/9.72e+07 =  2% of the original kernel matrix.

torch.Size([4775, 2])
We keep 2.30e+05/4.58e+06 =  5% of the original kernel matrix.

torch.Size([12385, 2])
We keep 1.33e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([4202, 2])
We keep 1.92e+05/3.67e+06 =  5% of the original kernel matrix.

torch.Size([11853, 2])
We keep 1.23e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([5128, 2])
We keep 3.47e+05/6.89e+06 =  5% of the original kernel matrix.

torch.Size([12711, 2])
We keep 1.56e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([5980, 2])
We keep 5.40e+05/1.06e+07 =  5% of the original kernel matrix.

torch.Size([13500, 2])
We keep 1.82e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([3958, 2])
We keep 1.59e+05/2.79e+06 =  5% of the original kernel matrix.

torch.Size([11715, 2])
We keep 1.13e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([9185, 2])
We keep 7.07e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([16451, 2])
We keep 2.24e+06/9.74e+07 =  2% of the original kernel matrix.

torch.Size([8744, 2])
We keep 7.78e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([16050, 2])
We keep 2.23e+06/9.64e+07 =  2% of the original kernel matrix.

torch.Size([5829, 2])
We keep 3.41e+05/7.43e+06 =  4% of the original kernel matrix.

torch.Size([13476, 2])
We keep 1.58e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([6100, 2])
We keep 4.77e+05/9.96e+06 =  4% of the original kernel matrix.

torch.Size([13663, 2])
We keep 1.76e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([10606, 2])
We keep 1.09e+06/3.06e+07 =  3% of the original kernel matrix.

torch.Size([17497, 2])
We keep 2.69e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([17338, 2])
We keep 3.86e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([22569, 2])
We keep 4.69e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([3516, 2])
We keep 1.52e+05/2.65e+06 =  5% of the original kernel matrix.

torch.Size([11026, 2])
We keep 1.11e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([1236, 2])
We keep 1.81e+04/1.94e+05 =  9% of the original kernel matrix.

torch.Size([7658, 2])
We keep 4.58e+05/9.61e+06 =  4% of the original kernel matrix.

torch.Size([5434, 2])
We keep 2.96e+05/6.19e+06 =  4% of the original kernel matrix.

torch.Size([13253, 2])
We keep 1.47e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([6718, 2])
We keep 4.45e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([14330, 2])
We keep 1.78e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([4592, 2])
We keep 2.58e+05/4.50e+06 =  5% of the original kernel matrix.

torch.Size([12185, 2])
We keep 1.32e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([6097, 2])
We keep 4.23e+05/9.04e+06 =  4% of the original kernel matrix.

torch.Size([13817, 2])
We keep 1.70e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([3794, 2])
We keep 1.73e+05/3.33e+06 =  5% of the original kernel matrix.

torch.Size([11406, 2])
We keep 1.20e+06/3.98e+07 =  3% of the original kernel matrix.

torch.Size([2724, 2])
We keep 1.28e+05/1.65e+06 =  7% of the original kernel matrix.

torch.Size([9901, 2])
We keep 9.60e+05/2.81e+07 =  3% of the original kernel matrix.

torch.Size([4306, 2])
We keep 2.05e+05/3.85e+06 =  5% of the original kernel matrix.

torch.Size([11873, 2])
We keep 1.26e+06/4.28e+07 =  2% of the original kernel matrix.

torch.Size([2292, 2])
We keep 6.89e+04/8.99e+05 =  7% of the original kernel matrix.

torch.Size([9563, 2])
We keep 7.65e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([5497, 2])
We keep 3.12e+05/6.76e+06 =  4% of the original kernel matrix.

torch.Size([13170, 2])
We keep 1.53e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([3005, 2])
We keep 9.54e+04/1.58e+06 =  6% of the original kernel matrix.

torch.Size([10427, 2])
We keep 9.33e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([10436, 2])
We keep 9.05e+05/2.63e+07 =  3% of the original kernel matrix.

torch.Size([17337, 2])
We keep 2.50e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([7250, 2])
We keep 5.19e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([14815, 2])
We keep 1.90e+06/7.64e+07 =  2% of the original kernel matrix.

torch.Size([11555, 2])
We keep 1.25e+06/3.94e+07 =  3% of the original kernel matrix.

torch.Size([18241, 2])
We keep 2.90e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([2258, 2])
We keep 7.06e+04/1.04e+06 =  6% of the original kernel matrix.

torch.Size([9421, 2])
We keep 8.17e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([6819, 2])
We keep 8.02e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([14253, 2])
We keep 2.12e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([5239, 2])
We keep 3.22e+05/6.27e+06 =  5% of the original kernel matrix.

torch.Size([12867, 2])
We keep 1.49e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([7739, 2])
We keep 6.48e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([15182, 2])
We keep 2.02e+06/8.44e+07 =  2% of the original kernel matrix.

torch.Size([10001, 2])
We keep 1.09e+06/3.06e+07 =  3% of the original kernel matrix.

torch.Size([17100, 2])
We keep 2.71e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([10753, 2])
We keep 1.15e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([17612, 2])
We keep 2.73e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([5194, 2])
We keep 3.15e+05/5.83e+06 =  5% of the original kernel matrix.

torch.Size([12911, 2])
We keep 1.45e+06/5.27e+07 =  2% of the original kernel matrix.

torch.Size([5229, 2])
We keep 2.97e+05/6.36e+06 =  4% of the original kernel matrix.

torch.Size([12888, 2])
We keep 1.50e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([4859, 2])
We keep 2.60e+05/4.96e+06 =  5% of the original kernel matrix.

torch.Size([12510, 2])
We keep 1.37e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([2403, 2])
We keep 7.72e+04/1.07e+06 =  7% of the original kernel matrix.

torch.Size([9656, 2])
We keep 8.15e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([3636, 2])
We keep 1.79e+05/3.05e+06 =  5% of the original kernel matrix.

torch.Size([11054, 2])
We keep 1.17e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([3836, 2])
We keep 1.73e+05/2.93e+06 =  5% of the original kernel matrix.

torch.Size([11532, 2])
We keep 1.13e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([1990, 2])
We keep 4.55e+04/5.40e+05 =  8% of the original kernel matrix.

torch.Size([9085, 2])
We keep 6.48e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([8176, 2])
We keep 7.76e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([15476, 2])
We keep 2.22e+06/9.49e+07 =  2% of the original kernel matrix.

torch.Size([4284, 2])
We keep 1.85e+05/3.39e+06 =  5% of the original kernel matrix.

torch.Size([12046, 2])
We keep 1.21e+06/4.02e+07 =  2% of the original kernel matrix.

torch.Size([4908, 2])
We keep 5.05e+05/8.29e+06 =  6% of the original kernel matrix.

torch.Size([12141, 2])
We keep 1.65e+06/6.29e+07 =  2% of the original kernel matrix.

torch.Size([3624, 2])
We keep 1.52e+05/2.69e+06 =  5% of the original kernel matrix.

torch.Size([11308, 2])
We keep 1.10e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([9663, 2])
We keep 9.87e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([16604, 2])
We keep 2.57e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([3188, 2])
We keep 1.38e+05/2.34e+06 =  5% of the original kernel matrix.

torch.Size([10526, 2])
We keep 1.04e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([7269, 2])
We keep 7.32e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([14637, 2])
We keep 2.15e+06/8.79e+07 =  2% of the original kernel matrix.

torch.Size([8968, 2])
We keep 9.50e+05/2.23e+07 =  4% of the original kernel matrix.

torch.Size([16124, 2])
We keep 2.38e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([2160, 2])
We keep 6.88e+04/9.80e+05 =  7% of the original kernel matrix.

torch.Size([9104, 2])
We keep 7.96e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([3105, 2])
We keep 1.23e+05/1.91e+06 =  6% of the original kernel matrix.

torch.Size([10548, 2])
We keep 9.92e+05/3.02e+07 =  3% of the original kernel matrix.

torch.Size([7675, 2])
We keep 5.96e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([15132, 2])
We keep 2.03e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([1988, 2])
We keep 4.44e+04/5.43e+05 =  8% of the original kernel matrix.

torch.Size([8971, 2])
We keep 6.54e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([2926, 2])
We keep 9.93e+04/1.61e+06 =  6% of the original kernel matrix.

torch.Size([10363, 2])
We keep 9.24e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([6915, 2])
We keep 6.70e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([14404, 2])
We keep 1.99e+06/8.06e+07 =  2% of the original kernel matrix.

torch.Size([6719, 2])
We keep 6.18e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([14553, 2])
We keep 1.99e+06/7.76e+07 =  2% of the original kernel matrix.

torch.Size([7235, 2])
We keep 5.15e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([14850, 2])
We keep 1.89e+06/7.61e+07 =  2% of the original kernel matrix.

torch.Size([3445, 2])
We keep 1.29e+05/2.16e+06 =  5% of the original kernel matrix.

torch.Size([10902, 2])
We keep 1.03e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([3360, 2])
We keep 1.33e+05/2.12e+06 =  6% of the original kernel matrix.

torch.Size([10836, 2])
We keep 1.04e+06/3.18e+07 =  3% of the original kernel matrix.

torch.Size([5112, 2])
We keep 2.70e+05/5.54e+06 =  4% of the original kernel matrix.

torch.Size([13059, 2])
We keep 1.44e+06/5.14e+07 =  2% of the original kernel matrix.

torch.Size([9658, 2])
We keep 8.50e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([16938, 2])
We keep 2.35e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([2996, 2])
We keep 1.08e+05/1.61e+06 =  6% of the original kernel matrix.

torch.Size([10493, 2])
We keep 9.47e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([17232, 2])
We keep 4.22e+06/1.22e+08 =  3% of the original kernel matrix.

torch.Size([22492, 2])
We keep 4.52e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([48114, 2])
We keep 6.87e+07/1.81e+09 =  3% of the original kernel matrix.

torch.Size([36912, 2])
We keep 1.34e+07/9.28e+08 =  1% of the original kernel matrix.

torch.Size([11299, 2])
We keep 3.36e+06/4.88e+07 =  6% of the original kernel matrix.

torch.Size([18211, 2])
We keep 2.91e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([7629, 2])
We keep 6.94e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([15058, 2])
We keep 2.08e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([1739, 2])
We keep 3.25e+04/3.87e+05 =  8% of the original kernel matrix.

torch.Size([8638, 2])
We keep 5.88e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([14574, 2])
We keep 1.88e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([20557, 2])
We keep 3.61e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([96280, 2])
We keep 5.87e+08/7.39e+09 =  7% of the original kernel matrix.

torch.Size([52646, 2])
We keep 2.43e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([249097, 2])
We keep 1.41e+09/4.94e+10 =  2% of the original kernel matrix.

torch.Size([87463, 2])
We keep 5.61e+07/4.85e+09 =  1% of the original kernel matrix.

torch.Size([4876, 2])
We keep 2.38e+05/4.86e+06 =  4% of the original kernel matrix.

torch.Size([12675, 2])
We keep 1.36e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([100726, 2])
We keep 1.13e+08/6.37e+09 =  1% of the original kernel matrix.

torch.Size([53619, 2])
We keep 2.30e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([36508, 2])
We keep 2.06e+07/7.06e+08 =  2% of the original kernel matrix.

torch.Size([32664, 2])
We keep 8.84e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([30371, 2])
We keep 1.72e+07/5.63e+08 =  3% of the original kernel matrix.

torch.Size([29922, 2])
We keep 8.24e+06/5.18e+08 =  1% of the original kernel matrix.

torch.Size([145016, 2])
We keep 6.65e+08/1.47e+10 =  4% of the original kernel matrix.

torch.Size([65967, 2])
We keep 3.28e+07/2.65e+09 =  1% of the original kernel matrix.

torch.Size([3122, 2])
We keep 1.04e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([10649, 2])
We keep 9.49e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([7033, 2])
We keep 5.55e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([14577, 2])
We keep 1.92e+06/7.71e+07 =  2% of the original kernel matrix.

torch.Size([203575, 2])
We keep 2.48e+08/2.19e+10 =  1% of the original kernel matrix.

torch.Size([78802, 2])
We keep 3.89e+07/3.23e+09 =  1% of the original kernel matrix.

torch.Size([345888, 2])
We keep 5.83e+08/5.91e+10 =  0% of the original kernel matrix.

torch.Size([105898, 2])
We keep 5.99e+07/5.31e+09 =  1% of the original kernel matrix.

torch.Size([6551, 2])
We keep 5.00e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([14190, 2])
We keep 1.78e+06/6.93e+07 =  2% of the original kernel matrix.

torch.Size([35601, 2])
We keep 1.22e+07/6.69e+08 =  1% of the original kernel matrix.

torch.Size([33321, 2])
We keep 8.90e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([22070, 2])
We keep 6.10e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([25418, 2])
We keep 5.66e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([75714, 2])
We keep 2.12e+08/3.49e+09 =  6% of the original kernel matrix.

torch.Size([46681, 2])
We keep 1.77e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([80959, 2])
We keep 9.73e+07/4.83e+09 =  2% of the original kernel matrix.

torch.Size([47447, 2])
We keep 2.03e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([37289, 2])
We keep 3.93e+07/1.12e+09 =  3% of the original kernel matrix.

torch.Size([32761, 2])
We keep 1.11e+07/7.31e+08 =  1% of the original kernel matrix.

torch.Size([28321, 2])
We keep 1.07e+07/4.21e+08 =  2% of the original kernel matrix.

torch.Size([29165, 2])
We keep 7.16e+06/4.48e+08 =  1% of the original kernel matrix.

torch.Size([24867, 2])
We keep 6.91e+06/2.68e+08 =  2% of the original kernel matrix.

torch.Size([27485, 2])
We keep 6.11e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([3608, 2])
We keep 2.00e+05/3.20e+06 =  6% of the original kernel matrix.

torch.Size([11058, 2])
We keep 1.19e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([183011, 2])
We keep 8.88e+08/3.66e+10 =  2% of the original kernel matrix.

torch.Size([72834, 2])
We keep 4.85e+07/4.18e+09 =  1% of the original kernel matrix.

torch.Size([6329, 2])
We keep 4.20e+05/8.96e+06 =  4% of the original kernel matrix.

torch.Size([14171, 2])
We keep 1.70e+06/6.54e+07 =  2% of the original kernel matrix.

torch.Size([4423, 2])
We keep 1.97e+05/3.85e+06 =  5% of the original kernel matrix.

torch.Size([12195, 2])
We keep 1.26e+06/4.29e+07 =  2% of the original kernel matrix.

torch.Size([50812, 2])
We keep 2.87e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([38917, 2])
We keep 1.21e+07/8.38e+08 =  1% of the original kernel matrix.

torch.Size([71049, 2])
We keep 5.78e+07/3.07e+09 =  1% of the original kernel matrix.

torch.Size([44127, 2])
We keep 1.67e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([210740, 2])
We keep 3.70e+08/3.12e+10 =  1% of the original kernel matrix.

torch.Size([81124, 2])
We keep 4.60e+07/3.86e+09 =  1% of the original kernel matrix.

torch.Size([120567, 2])
We keep 9.29e+07/7.09e+09 =  1% of the original kernel matrix.

torch.Size([59518, 2])
We keep 2.37e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([3065, 2])
We keep 1.02e+05/1.61e+06 =  6% of the original kernel matrix.

torch.Size([10646, 2])
We keep 9.18e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([21081, 2])
We keep 1.29e+07/1.81e+08 =  7% of the original kernel matrix.

torch.Size([24847, 2])
We keep 5.21e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([14845, 2])
We keep 2.75e+06/8.82e+07 =  3% of the original kernel matrix.

torch.Size([20829, 2])
We keep 4.03e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([38124, 2])
We keep 2.22e+07/8.23e+08 =  2% of the original kernel matrix.

torch.Size([33724, 2])
We keep 9.60e+06/6.27e+08 =  1% of the original kernel matrix.

torch.Size([81014, 2])
We keep 3.96e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([48883, 2])
We keep 1.62e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([7723, 2])
We keep 5.58e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([15294, 2])
We keep 1.97e+06/8.14e+07 =  2% of the original kernel matrix.

torch.Size([34927, 2])
We keep 1.14e+07/6.08e+08 =  1% of the original kernel matrix.

torch.Size([32661, 2])
We keep 8.60e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([1516, 2])
We keep 2.90e+04/3.27e+05 =  8% of the original kernel matrix.

torch.Size([8165, 2])
We keep 5.55e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([7404, 2])
We keep 4.98e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([14873, 2])
We keep 1.89e+06/7.69e+07 =  2% of the original kernel matrix.

torch.Size([55076, 2])
We keep 5.16e+07/2.38e+09 =  2% of the original kernel matrix.

torch.Size([39001, 2])
We keep 1.50e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([92203, 2])
We keep 1.15e+08/5.69e+09 =  2% of the original kernel matrix.

torch.Size([50855, 2])
We keep 2.16e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([53914, 2])
We keep 3.23e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([39128, 2])
We keep 1.31e+07/9.14e+08 =  1% of the original kernel matrix.

torch.Size([12212, 2])
We keep 2.69e+06/5.72e+07 =  4% of the original kernel matrix.

torch.Size([18778, 2])
We keep 3.38e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([349239, 2])
We keep 5.07e+08/6.66e+10 =  0% of the original kernel matrix.

torch.Size([105989, 2])
We keep 6.22e+07/5.63e+09 =  1% of the original kernel matrix.

torch.Size([19476, 2])
We keep 3.09e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([24080, 2])
We keep 4.70e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([32348, 2])
We keep 7.78e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([31486, 2])
We keep 7.66e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([90825, 2])
We keep 6.77e+07/4.90e+09 =  1% of the original kernel matrix.

torch.Size([51482, 2])
We keep 2.00e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([5325, 2])
We keep 3.18e+05/6.53e+06 =  4% of the original kernel matrix.

torch.Size([13054, 2])
We keep 1.50e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([151425, 2])
We keep 1.90e+08/1.19e+10 =  1% of the original kernel matrix.

torch.Size([67434, 2])
We keep 2.99e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([232015, 2])
We keep 6.16e+08/3.47e+10 =  1% of the original kernel matrix.

torch.Size([85544, 2])
We keep 4.84e+07/4.07e+09 =  1% of the original kernel matrix.

torch.Size([262856, 2])
We keep 4.05e+08/3.94e+10 =  1% of the original kernel matrix.

torch.Size([91345, 2])
We keep 5.00e+07/4.33e+09 =  1% of the original kernel matrix.

torch.Size([16481, 2])
We keep 3.18e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([21738, 2])
We keep 4.57e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([15312, 2])
We keep 2.62e+06/8.66e+07 =  3% of the original kernel matrix.

torch.Size([21032, 2])
We keep 3.97e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([13896, 2])
We keep 2.99e+06/8.95e+07 =  3% of the original kernel matrix.

torch.Size([19675, 2])
We keep 4.02e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([8049, 2])
We keep 5.69e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([15440, 2])
We keep 2.03e+06/8.45e+07 =  2% of the original kernel matrix.

torch.Size([41116, 2])
We keep 3.36e+07/1.20e+09 =  2% of the original kernel matrix.

torch.Size([34571, 2])
We keep 1.10e+07/7.57e+08 =  1% of the original kernel matrix.

torch.Size([11666, 2])
We keep 1.46e+06/4.37e+07 =  3% of the original kernel matrix.

torch.Size([18523, 2])
We keep 3.05e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([40516, 2])
We keep 2.17e+07/9.13e+08 =  2% of the original kernel matrix.

torch.Size([34827, 2])
We keep 9.92e+06/6.60e+08 =  1% of the original kernel matrix.

torch.Size([91488, 2])
We keep 2.08e+08/6.70e+09 =  3% of the original kernel matrix.

torch.Size([52415, 2])
We keep 1.96e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([23876, 2])
We keep 7.31e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([26998, 2])
We keep 5.64e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([13297, 2])
We keep 4.23e+06/9.99e+07 =  4% of the original kernel matrix.

torch.Size([19481, 2])
We keep 4.16e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([8843, 2])
We keep 7.38e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([16272, 2])
We keep 2.18e+06/9.52e+07 =  2% of the original kernel matrix.

torch.Size([47939, 2])
We keep 1.61e+08/4.97e+09 =  3% of the original kernel matrix.

torch.Size([35496, 2])
We keep 1.98e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([19745, 2])
We keep 4.97e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([24082, 2])
We keep 4.93e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([17384, 2])
We keep 8.60e+06/1.47e+08 =  5% of the original kernel matrix.

torch.Size([22444, 2])
We keep 4.87e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([14129, 2])
We keep 2.31e+06/7.46e+07 =  3% of the original kernel matrix.

torch.Size([20300, 2])
We keep 3.78e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([209528, 2])
We keep 9.58e+08/3.19e+10 =  3% of the original kernel matrix.

torch.Size([80707, 2])
We keep 4.57e+07/3.90e+09 =  1% of the original kernel matrix.

torch.Size([16790, 2])
We keep 3.06e+06/9.66e+07 =  3% of the original kernel matrix.

torch.Size([22015, 2])
We keep 4.12e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([143408, 2])
We keep 3.08e+08/1.57e+10 =  1% of the original kernel matrix.

torch.Size([64410, 2])
We keep 3.38e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([8183, 2])
We keep 1.12e+06/2.24e+07 =  5% of the original kernel matrix.

torch.Size([15668, 2])
We keep 2.33e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([19385, 2])
We keep 1.48e+07/4.28e+08 =  3% of the original kernel matrix.

torch.Size([22232, 2])
We keep 7.48e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([23213, 2])
We keep 6.46e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([26351, 2])
We keep 5.35e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([150923, 2])
We keep 7.83e+08/2.86e+10 =  2% of the original kernel matrix.

torch.Size([64330, 2])
We keep 4.41e+07/3.70e+09 =  1% of the original kernel matrix.

torch.Size([46761, 2])
We keep 1.63e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([37209, 2])
We keep 1.10e+07/7.52e+08 =  1% of the original kernel matrix.

torch.Size([7997, 2])
We keep 6.56e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([15379, 2])
We keep 2.09e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([142748, 2])
We keep 9.73e+07/9.80e+09 =  0% of the original kernel matrix.

torch.Size([65530, 2])
We keep 2.71e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([148276, 2])
We keep 1.24e+08/1.25e+10 =  0% of the original kernel matrix.

torch.Size([67085, 2])
We keep 3.03e+07/2.44e+09 =  1% of the original kernel matrix.

torch.Size([16369, 2])
We keep 6.78e+06/1.25e+08 =  5% of the original kernel matrix.

torch.Size([21915, 2])
We keep 4.62e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([107626, 2])
We keep 1.50e+08/5.83e+09 =  2% of the original kernel matrix.

torch.Size([55987, 2])
We keep 2.15e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([150921, 2])
We keep 1.19e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([67773, 2])
We keep 2.94e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([402713, 2])
We keep 9.38e+08/9.04e+10 =  1% of the original kernel matrix.

torch.Size([114079, 2])
We keep 7.29e+07/6.57e+09 =  1% of the original kernel matrix.

torch.Size([31474, 2])
We keep 4.91e+07/5.63e+08 =  8% of the original kernel matrix.

torch.Size([30715, 2])
We keep 8.19e+06/5.18e+08 =  1% of the original kernel matrix.

torch.Size([3001, 2])
We keep 9.59e+04/1.54e+06 =  6% of the original kernel matrix.

torch.Size([10616, 2])
We keep 8.98e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([15494, 2])
We keep 1.97e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([21109, 2])
We keep 3.71e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([14171, 2])
We keep 4.02e+06/9.64e+07 =  4% of the original kernel matrix.

torch.Size([20530, 2])
We keep 3.90e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([24320, 2])
We keep 5.65e+06/2.71e+08 =  2% of the original kernel matrix.

torch.Size([27256, 2])
We keep 6.15e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([24615, 2])
We keep 9.37e+06/2.64e+08 =  3% of the original kernel matrix.

torch.Size([27337, 2])
We keep 6.11e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([4164, 2])
We keep 2.21e+05/4.17e+06 =  5% of the original kernel matrix.

torch.Size([11813, 2])
We keep 1.28e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([81187, 2])
We keep 5.21e+07/3.04e+09 =  1% of the original kernel matrix.

torch.Size([48705, 2])
We keep 1.66e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([170729, 2])
We keep 1.58e+08/1.51e+10 =  1% of the original kernel matrix.

torch.Size([72528, 2])
We keep 3.32e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([7066, 2])
We keep 1.41e+06/1.47e+07 =  9% of the original kernel matrix.

torch.Size([14656, 2])
We keep 1.82e+06/8.37e+07 =  2% of the original kernel matrix.

torch.Size([9895, 2])
We keep 1.30e+06/3.04e+07 =  4% of the original kernel matrix.

torch.Size([16886, 2])
We keep 2.66e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([19161, 2])
We keep 3.85e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([23624, 2])
We keep 4.79e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([13909, 2])
We keep 1.92e+06/6.33e+07 =  3% of the original kernel matrix.

torch.Size([20075, 2])
We keep 3.48e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([9217, 2])
We keep 7.79e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([16491, 2])
We keep 2.31e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([16377, 2])
We keep 2.24e+06/8.78e+07 =  2% of the original kernel matrix.

torch.Size([21815, 2])
We keep 3.94e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([8698, 2])
We keep 7.77e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([16168, 2])
We keep 2.18e+06/9.27e+07 =  2% of the original kernel matrix.

torch.Size([24798, 2])
We keep 1.10e+07/3.30e+08 =  3% of the original kernel matrix.

torch.Size([26821, 2])
We keep 6.56e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([44131, 2])
We keep 1.27e+07/8.98e+08 =  1% of the original kernel matrix.

torch.Size([36772, 2])
We keep 9.73e+06/6.54e+08 =  1% of the original kernel matrix.

torch.Size([22751, 2])
We keep 4.74e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([26215, 2])
We keep 5.69e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([37929, 2])
We keep 4.45e+07/1.17e+09 =  3% of the original kernel matrix.

torch.Size([33687, 2])
We keep 1.15e+07/7.48e+08 =  1% of the original kernel matrix.

torch.Size([11747, 2])
We keep 2.24e+06/5.33e+07 =  4% of the original kernel matrix.

torch.Size([18367, 2])
We keep 3.28e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([363325, 2])
We keep 6.43e+08/6.78e+10 =  0% of the original kernel matrix.

torch.Size([107972, 2])
We keep 6.37e+07/5.68e+09 =  1% of the original kernel matrix.

torch.Size([9171, 2])
We keep 8.24e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([16402, 2])
We keep 2.32e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([101199, 2])
We keep 6.27e+07/5.11e+09 =  1% of the original kernel matrix.

torch.Size([54099, 2])
We keep 2.06e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([2832, 2])
We keep 1.08e+05/1.58e+06 =  6% of the original kernel matrix.

torch.Size([10243, 2])
We keep 8.96e+05/2.74e+07 =  3% of the original kernel matrix.

torch.Size([7494, 2])
We keep 6.79e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([15035, 2])
We keep 1.99e+06/8.17e+07 =  2% of the original kernel matrix.

torch.Size([7330, 2])
We keep 5.35e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([14861, 2])
We keep 1.93e+06/7.82e+07 =  2% of the original kernel matrix.

torch.Size([23152, 2])
We keep 7.47e+06/2.22e+08 =  3% of the original kernel matrix.

torch.Size([26408, 2])
We keep 5.21e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([7452, 2])
We keep 6.23e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([15246, 2])
We keep 2.07e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([107821, 2])
We keep 3.39e+08/1.24e+10 =  2% of the original kernel matrix.

torch.Size([54774, 2])
We keep 3.14e+07/2.44e+09 =  1% of the original kernel matrix.

torch.Size([7110, 2])
We keep 5.48e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([14531, 2])
We keep 1.99e+06/7.99e+07 =  2% of the original kernel matrix.

torch.Size([7991, 2])
We keep 9.39e+05/2.15e+07 =  4% of the original kernel matrix.

torch.Size([15387, 2])
We keep 2.39e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([368486, 2])
We keep 6.47e+08/6.82e+10 =  0% of the original kernel matrix.

torch.Size([109003, 2])
We keep 6.41e+07/5.70e+09 =  1% of the original kernel matrix.

torch.Size([82802, 2])
We keep 6.45e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([49090, 2])
We keep 1.79e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([57562, 2])
We keep 5.10e+07/2.13e+09 =  2% of the original kernel matrix.

torch.Size([40998, 2])
We keep 1.44e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([226995, 2])
We keep 4.93e+08/3.31e+10 =  1% of the original kernel matrix.

torch.Size([84918, 2])
We keep 4.74e+07/3.97e+09 =  1% of the original kernel matrix.

torch.Size([219569, 2])
We keep 6.39e+08/5.14e+10 =  1% of the original kernel matrix.

torch.Size([79928, 2])
We keep 5.74e+07/4.95e+09 =  1% of the original kernel matrix.

torch.Size([17192, 2])
We keep 2.82e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([22491, 2])
We keep 4.47e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([7910, 2])
We keep 5.69e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([15362, 2])
We keep 1.99e+06/8.18e+07 =  2% of the original kernel matrix.

torch.Size([21440, 2])
We keep 1.11e+07/2.47e+08 =  4% of the original kernel matrix.

torch.Size([25111, 2])
We keep 5.95e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([2526, 2])
We keep 9.32e+04/1.34e+06 =  6% of the original kernel matrix.

torch.Size([9660, 2])
We keep 8.72e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([8593, 2])
We keep 8.76e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([15972, 2])
We keep 2.32e+06/9.76e+07 =  2% of the original kernel matrix.

torch.Size([19093, 2])
We keep 4.25e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([23886, 2])
We keep 5.02e+06/2.80e+08 =  1% of the original kernel matrix.

torch.Size([5905, 2])
We keep 5.06e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([13385, 2])
We keep 1.75e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([12439, 2])
We keep 1.40e+06/4.32e+07 =  3% of the original kernel matrix.

torch.Size([18882, 2])
We keep 3.03e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([4768, 2])
We keep 2.43e+05/4.44e+06 =  5% of the original kernel matrix.

torch.Size([12379, 2])
We keep 1.31e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([7808, 2])
We keep 6.51e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([15330, 2])
We keep 2.08e+06/8.80e+07 =  2% of the original kernel matrix.

torch.Size([381822, 2])
We keep 1.08e+09/8.20e+10 =  1% of the original kernel matrix.

torch.Size([110256, 2])
We keep 6.93e+07/6.25e+09 =  1% of the original kernel matrix.

torch.Size([16068, 2])
We keep 4.82e+06/1.46e+08 =  3% of the original kernel matrix.

torch.Size([21334, 2])
We keep 4.83e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([107717, 2])
We keep 1.11e+08/6.91e+09 =  1% of the original kernel matrix.

torch.Size([55774, 2])
We keep 2.37e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([10384, 2])
We keep 2.07e+06/4.24e+07 =  4% of the original kernel matrix.

torch.Size([17508, 2])
We keep 2.97e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([18842, 2])
We keep 1.63e+07/2.99e+08 =  5% of the original kernel matrix.

torch.Size([22524, 2])
We keep 6.11e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([11875, 2])
We keep 1.83e+06/4.71e+07 =  3% of the original kernel matrix.

torch.Size([18526, 2])
We keep 3.11e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([651482, 2])
We keep 4.00e+09/2.58e+11 =  1% of the original kernel matrix.

torch.Size([145283, 2])
We keep 1.12e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([717343, 2])
We keep 1.56e+09/2.22e+11 =  0% of the original kernel matrix.

torch.Size([153493, 2])
We keep 1.06e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([26126, 2])
We keep 7.86e+06/2.92e+08 =  2% of the original kernel matrix.

torch.Size([27812, 2])
We keep 6.10e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([64008, 2])
We keep 9.24e+07/2.02e+09 =  4% of the original kernel matrix.

torch.Size([43587, 2])
We keep 1.40e+07/9.82e+08 =  1% of the original kernel matrix.

torch.Size([5483, 2])
We keep 3.04e+05/6.57e+06 =  4% of the original kernel matrix.

torch.Size([13132, 2])
We keep 1.50e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([13811, 2])
We keep 1.20e+07/1.15e+08 = 10% of the original kernel matrix.

torch.Size([19884, 2])
We keep 4.30e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([47477, 2])
We keep 6.10e+07/1.81e+09 =  3% of the original kernel matrix.

torch.Size([36581, 2])
We keep 1.31e+07/9.30e+08 =  1% of the original kernel matrix.

torch.Size([28564, 2])
We keep 9.48e+06/4.10e+08 =  2% of the original kernel matrix.

torch.Size([29517, 2])
We keep 7.01e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([16369, 2])
We keep 4.58e+06/9.83e+07 =  4% of the original kernel matrix.

torch.Size([21756, 2])
We keep 4.15e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([76142, 2])
We keep 5.07e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([46973, 2])
We keep 1.68e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([470077, 2])
We keep 1.85e+09/1.48e+11 =  1% of the original kernel matrix.

torch.Size([121997, 2])
We keep 9.28e+07/8.39e+09 =  1% of the original kernel matrix.

torch.Size([43029, 2])
We keep 1.93e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([35078, 2])
We keep 1.04e+07/7.01e+08 =  1% of the original kernel matrix.

torch.Size([12908, 2])
We keep 2.45e+06/6.89e+07 =  3% of the original kernel matrix.

torch.Size([18973, 2])
We keep 3.62e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([30967, 2])
We keep 1.66e+07/6.25e+08 =  2% of the original kernel matrix.

torch.Size([31156, 2])
We keep 8.95e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([12994, 2])
We keep 1.47e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([19341, 2])
We keep 3.19e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([4526, 2])
We keep 2.04e+05/3.96e+06 =  5% of the original kernel matrix.

torch.Size([12423, 2])
We keep 1.25e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([148030, 2])
We keep 2.02e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([66972, 2])
We keep 3.17e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([25155, 2])
We keep 8.08e+06/3.21e+08 =  2% of the original kernel matrix.

torch.Size([27608, 2])
We keep 6.60e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([3263, 2])
We keep 1.27e+05/2.02e+06 =  6% of the original kernel matrix.

torch.Size([10551, 2])
We keep 1.02e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([87119, 2])
We keep 5.16e+07/3.50e+09 =  1% of the original kernel matrix.

torch.Size([50230, 2])
We keep 1.74e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([19128, 2])
We keep 3.74e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([23894, 2])
We keep 4.94e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([2363, 2])
We keep 8.52e+04/1.19e+06 =  7% of the original kernel matrix.

torch.Size([9458, 2])
We keep 8.56e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([98800, 2])
We keep 1.20e+08/5.33e+09 =  2% of the original kernel matrix.

torch.Size([53193, 2])
We keep 2.11e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([129651, 2])
We keep 9.81e+07/8.26e+09 =  1% of the original kernel matrix.

torch.Size([62316, 2])
We keep 2.52e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([126863, 2])
We keep 2.87e+08/1.01e+10 =  2% of the original kernel matrix.

torch.Size([61323, 2])
We keep 2.82e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([150501, 2])
We keep 2.75e+08/1.21e+10 =  2% of the original kernel matrix.

torch.Size([67346, 2])
We keep 3.03e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([203756, 2])
We keep 4.55e+08/2.67e+10 =  1% of the original kernel matrix.

torch.Size([79088, 2])
We keep 4.27e+07/3.56e+09 =  1% of the original kernel matrix.

torch.Size([20036, 2])
We keep 7.41e+06/2.14e+08 =  3% of the original kernel matrix.

torch.Size([24059, 2])
We keep 5.67e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([32994, 2])
We keep 2.10e+07/6.98e+08 =  3% of the original kernel matrix.

torch.Size([31100, 2])
We keep 8.86e+06/5.77e+08 =  1% of the original kernel matrix.

torch.Size([185664, 2])
We keep 1.69e+08/1.64e+10 =  1% of the original kernel matrix.

torch.Size([75887, 2])
We keep 3.40e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([168666, 2])
We keep 1.39e+08/1.44e+10 =  0% of the original kernel matrix.

torch.Size([72157, 2])
We keep 3.16e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([10175, 2])
We keep 1.00e+06/2.82e+07 =  3% of the original kernel matrix.

torch.Size([17139, 2])
We keep 2.57e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([80383, 2])
We keep 1.14e+08/4.42e+09 =  2% of the original kernel matrix.

torch.Size([49010, 2])
We keep 1.70e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([201347, 2])
We keep 3.37e+08/2.51e+10 =  1% of the original kernel matrix.

torch.Size([79283, 2])
We keep 3.84e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([149971, 2])
We keep 2.24e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([67603, 2])
We keep 2.98e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([109294, 2])
We keep 2.15e+08/1.05e+10 =  2% of the original kernel matrix.

torch.Size([56074, 2])
We keep 2.77e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([42272, 2])
We keep 1.30e+08/2.73e+09 =  4% of the original kernel matrix.

torch.Size([32448, 2])
We keep 1.60e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([10953, 2])
We keep 1.38e+06/3.59e+07 =  3% of the original kernel matrix.

torch.Size([17856, 2])
We keep 2.84e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([6323, 2])
We keep 4.22e+05/9.00e+06 =  4% of the original kernel matrix.

torch.Size([13912, 2])
We keep 1.70e+06/6.55e+07 =  2% of the original kernel matrix.

torch.Size([8003, 2])
We keep 1.06e+06/2.26e+07 =  4% of the original kernel matrix.

torch.Size([15217, 2])
We keep 2.39e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([123562, 2])
We keep 7.70e+07/6.86e+09 =  1% of the original kernel matrix.

torch.Size([60131, 2])
We keep 2.32e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([129292, 2])
We keep 8.23e+07/7.63e+09 =  1% of the original kernel matrix.

torch.Size([62088, 2])
We keep 2.40e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([445245, 2])
We keep 1.02e+09/1.07e+11 =  0% of the original kernel matrix.

torch.Size([118487, 2])
We keep 7.82e+07/7.13e+09 =  1% of the original kernel matrix.

torch.Size([102960, 2])
We keep 1.72e+08/7.21e+09 =  2% of the original kernel matrix.

torch.Size([54937, 2])
We keep 2.45e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([84383, 2])
We keep 7.65e+07/4.65e+09 =  1% of the original kernel matrix.

torch.Size([49644, 2])
We keep 2.03e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([19227, 2])
We keep 3.70e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([23671, 2])
We keep 5.04e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([142167, 2])
We keep 1.19e+08/9.65e+09 =  1% of the original kernel matrix.

torch.Size([65679, 2])
We keep 2.70e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([4844, 2])
We keep 3.40e+05/5.63e+06 =  6% of the original kernel matrix.

torch.Size([12344, 2])
We keep 1.44e+06/5.18e+07 =  2% of the original kernel matrix.

torch.Size([13064, 2])
We keep 2.48e+06/6.52e+07 =  3% of the original kernel matrix.

torch.Size([19197, 2])
We keep 3.57e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([206167, 2])
We keep 2.56e+08/2.09e+10 =  1% of the original kernel matrix.

torch.Size([80080, 2])
We keep 3.79e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([8903, 2])
We keep 8.64e+05/1.99e+07 =  4% of the original kernel matrix.

torch.Size([16252, 2])
We keep 2.24e+06/9.75e+07 =  2% of the original kernel matrix.

torch.Size([6838, 2])
We keep 5.72e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([14254, 2])
We keep 1.95e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([18669, 2])
We keep 3.38e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([23440, 2])
We keep 4.68e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([222975, 2])
We keep 1.05e+09/5.41e+10 =  1% of the original kernel matrix.

torch.Size([81350, 2])
We keep 5.84e+07/5.08e+09 =  1% of the original kernel matrix.

torch.Size([25496, 2])
We keep 5.86e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([27173, 2])
We keep 6.13e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([191529, 2])
We keep 3.16e+08/2.28e+10 =  1% of the original kernel matrix.

torch.Size([76783, 2])
We keep 3.99e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([12532, 2])
We keep 8.40e+06/1.18e+08 =  7% of the original kernel matrix.

torch.Size([18477, 2])
We keep 4.31e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([20229, 2])
We keep 1.18e+07/2.27e+08 =  5% of the original kernel matrix.

torch.Size([24061, 2])
We keep 5.66e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([9924, 2])
We keep 9.98e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([16995, 2])
We keep 2.54e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([5661, 2])
We keep 3.89e+05/7.37e+06 =  5% of the original kernel matrix.

torch.Size([13412, 2])
We keep 1.58e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([72627, 2])
We keep 5.67e+07/2.51e+09 =  2% of the original kernel matrix.

torch.Size([46176, 2])
We keep 1.53e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([36544, 2])
We keep 2.00e+07/8.07e+08 =  2% of the original kernel matrix.

torch.Size([32880, 2])
We keep 9.66e+06/6.20e+08 =  1% of the original kernel matrix.

torch.Size([6391, 2])
We keep 5.06e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([13889, 2])
We keep 1.81e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([126923, 2])
We keep 1.21e+08/7.72e+09 =  1% of the original kernel matrix.

torch.Size([61463, 2])
We keep 2.44e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([4501, 2])
We keep 2.70e+05/5.14e+06 =  5% of the original kernel matrix.

torch.Size([12141, 2])
We keep 1.37e+06/4.95e+07 =  2% of the original kernel matrix.

torch.Size([62114, 2])
We keep 5.30e+07/2.54e+09 =  2% of the original kernel matrix.

torch.Size([43246, 2])
We keep 1.58e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([12468, 2])
We keep 2.12e+06/5.77e+07 =  3% of the original kernel matrix.

torch.Size([19124, 2])
We keep 3.41e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([132903, 2])
We keep 1.01e+08/8.22e+09 =  1% of the original kernel matrix.

torch.Size([62832, 2])
We keep 2.51e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([9187, 2])
We keep 1.78e+06/2.77e+07 =  6% of the original kernel matrix.

torch.Size([16342, 2])
We keep 2.64e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([26654, 2])
We keep 8.34e+06/3.45e+08 =  2% of the original kernel matrix.

torch.Size([28228, 2])
We keep 6.77e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([15814, 2])
We keep 2.59e+06/8.72e+07 =  2% of the original kernel matrix.

torch.Size([21351, 2])
We keep 3.93e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([26099, 2])
We keep 8.51e+06/3.91e+08 =  2% of the original kernel matrix.

torch.Size([28295, 2])
We keep 7.27e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([80415, 2])
We keep 5.26e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([48549, 2])
We keep 1.72e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([242150, 2])
We keep 5.40e+08/3.68e+10 =  1% of the original kernel matrix.

torch.Size([87010, 2])
We keep 4.91e+07/4.19e+09 =  1% of the original kernel matrix.

torch.Size([9389, 2])
We keep 9.23e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([16602, 2])
We keep 2.37e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([16601, 2])
We keep 4.50e+06/1.31e+08 =  3% of the original kernel matrix.

torch.Size([21890, 2])
We keep 4.62e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([99500, 2])
We keep 1.38e+08/4.99e+09 =  2% of the original kernel matrix.

torch.Size([53829, 2])
We keep 2.03e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([16809, 2])
We keep 3.48e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([21962, 2])
We keep 4.34e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([618759, 2])
We keep 1.16e+09/1.66e+11 =  0% of the original kernel matrix.

torch.Size([142046, 2])
We keep 9.49e+07/8.89e+09 =  1% of the original kernel matrix.

torch.Size([3832, 2])
We keep 1.50e+05/2.63e+06 =  5% of the original kernel matrix.

torch.Size([11532, 2])
We keep 1.11e+06/3.54e+07 =  3% of the original kernel matrix.

torch.Size([120385, 2])
We keep 9.99e+07/7.68e+09 =  1% of the original kernel matrix.

torch.Size([59480, 2])
We keep 2.46e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([151821, 2])
We keep 1.10e+08/1.11e+10 =  0% of the original kernel matrix.

torch.Size([68264, 2])
We keep 2.86e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([4862, 2])
We keep 2.54e+05/4.73e+06 =  5% of the original kernel matrix.

torch.Size([12633, 2])
We keep 1.35e+06/4.75e+07 =  2% of the original kernel matrix.

torch.Size([124470, 2])
We keep 1.90e+08/1.10e+10 =  1% of the original kernel matrix.

torch.Size([60155, 2])
We keep 2.88e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([13907, 2])
We keep 3.88e+06/1.13e+08 =  3% of the original kernel matrix.

torch.Size([19918, 2])
We keep 4.39e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([751198, 2])
We keep 3.56e+09/3.45e+11 =  1% of the original kernel matrix.

torch.Size([159743, 2])
We keep 1.36e+08/1.28e+10 =  1% of the original kernel matrix.

torch.Size([12368, 2])
We keep 1.41e+06/4.42e+07 =  3% of the original kernel matrix.

torch.Size([18850, 2])
We keep 3.03e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([25979, 2])
We keep 8.08e+06/3.39e+08 =  2% of the original kernel matrix.

torch.Size([27951, 2])
We keep 6.57e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([8871, 2])
We keep 1.82e+06/3.69e+07 =  4% of the original kernel matrix.

torch.Size([16050, 2])
We keep 2.86e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([8483, 2])
We keep 6.78e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([15810, 2])
We keep 2.16e+06/9.19e+07 =  2% of the original kernel matrix.

torch.Size([13538, 2])
We keep 2.94e+06/8.36e+07 =  3% of the original kernel matrix.

torch.Size([19638, 2])
We keep 3.92e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([37514, 2])
We keep 1.25e+07/7.02e+08 =  1% of the original kernel matrix.

torch.Size([33614, 2])
We keep 8.60e+06/5.78e+08 =  1% of the original kernel matrix.

torch.Size([17988, 2])
We keep 1.00e+07/1.46e+08 =  6% of the original kernel matrix.

torch.Size([22769, 2])
We keep 4.91e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([90028, 2])
We keep 6.59e+07/3.82e+09 =  1% of the original kernel matrix.

torch.Size([50982, 2])
We keep 1.82e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([24231, 2])
We keep 5.17e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([27070, 2])
We keep 6.08e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([6138, 2])
We keep 4.02e+05/8.73e+06 =  4% of the original kernel matrix.

torch.Size([13865, 2])
We keep 1.69e+06/6.45e+07 =  2% of the original kernel matrix.

torch.Size([14733, 2])
We keep 4.49e+06/9.61e+07 =  4% of the original kernel matrix.

torch.Size([20538, 2])
We keep 4.16e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([230755, 2])
We keep 8.57e+08/4.90e+10 =  1% of the original kernel matrix.

torch.Size([84962, 2])
We keep 5.64e+07/4.83e+09 =  1% of the original kernel matrix.

torch.Size([89752, 2])
We keep 1.76e+08/6.76e+09 =  2% of the original kernel matrix.

torch.Size([50509, 2])
We keep 2.37e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([257685, 2])
We keep 4.08e+08/3.37e+10 =  1% of the original kernel matrix.

torch.Size([91149, 2])
We keep 4.68e+07/4.01e+09 =  1% of the original kernel matrix.

torch.Size([31797, 2])
We keep 1.22e+07/4.72e+08 =  2% of the original kernel matrix.

torch.Size([31163, 2])
We keep 7.34e+06/4.75e+08 =  1% of the original kernel matrix.

torch.Size([12577, 2])
We keep 2.09e+06/6.23e+07 =  3% of the original kernel matrix.

torch.Size([19240, 2])
We keep 3.42e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([4424, 2])
We keep 2.20e+05/4.45e+06 =  4% of the original kernel matrix.

torch.Size([12226, 2])
We keep 1.33e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([7165, 2])
We keep 7.26e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([14730, 2])
We keep 2.06e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([252460, 2])
We keep 5.01e+08/3.59e+10 =  1% of the original kernel matrix.

torch.Size([90234, 2])
We keep 4.55e+07/4.14e+09 =  1% of the original kernel matrix.

torch.Size([12901, 2])
We keep 1.70e+06/5.43e+07 =  3% of the original kernel matrix.

torch.Size([19280, 2])
We keep 3.30e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([31241, 2])
We keep 1.01e+07/4.70e+08 =  2% of the original kernel matrix.

torch.Size([30920, 2])
We keep 7.67e+06/4.73e+08 =  1% of the original kernel matrix.

torch.Size([45542, 2])
We keep 2.87e+07/1.19e+09 =  2% of the original kernel matrix.

torch.Size([36590, 2])
We keep 1.10e+07/7.53e+08 =  1% of the original kernel matrix.

torch.Size([38367, 2])
We keep 2.37e+07/9.95e+08 =  2% of the original kernel matrix.

torch.Size([33236, 2])
We keep 1.03e+07/6.89e+08 =  1% of the original kernel matrix.

torch.Size([10288, 2])
We keep 1.34e+06/3.06e+07 =  4% of the original kernel matrix.

torch.Size([17455, 2])
We keep 2.63e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([5560, 2])
We keep 3.61e+05/7.07e+06 =  5% of the original kernel matrix.

torch.Size([13125, 2])
We keep 1.56e+06/5.81e+07 =  2% of the original kernel matrix.

torch.Size([199570, 2])
We keep 4.60e+08/2.74e+10 =  1% of the original kernel matrix.

torch.Size([78601, 2])
We keep 4.30e+07/3.61e+09 =  1% of the original kernel matrix.

torch.Size([42999, 2])
We keep 2.38e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([35907, 2])
We keep 1.07e+07/7.10e+08 =  1% of the original kernel matrix.

torch.Size([19759, 2])
We keep 2.40e+07/2.20e+08 = 10% of the original kernel matrix.

torch.Size([23785, 2])
We keep 5.57e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([31616, 2])
We keep 1.26e+07/4.84e+08 =  2% of the original kernel matrix.

torch.Size([31148, 2])
We keep 7.51e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([93016, 2])
We keep 1.47e+08/6.04e+09 =  2% of the original kernel matrix.

torch.Size([52998, 2])
We keep 2.29e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([178130, 2])
We keep 3.38e+08/1.97e+10 =  1% of the original kernel matrix.

torch.Size([74042, 2])
We keep 3.72e+07/3.06e+09 =  1% of the original kernel matrix.

torch.Size([384004, 2])
We keep 8.83e+08/8.08e+10 =  1% of the original kernel matrix.

torch.Size([110867, 2])
We keep 6.99e+07/6.21e+09 =  1% of the original kernel matrix.

torch.Size([155955, 2])
We keep 1.94e+08/1.21e+10 =  1% of the original kernel matrix.

torch.Size([68920, 2])
We keep 2.99e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([11218, 2])
We keep 6.01e+06/5.12e+07 = 11% of the original kernel matrix.

torch.Size([18179, 2])
We keep 3.20e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([47540, 2])
We keep 1.57e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([37714, 2])
We keep 1.04e+07/7.25e+08 =  1% of the original kernel matrix.

torch.Size([44609, 2])
We keep 4.25e+07/1.48e+09 =  2% of the original kernel matrix.

torch.Size([35920, 2])
We keep 1.23e+07/8.39e+08 =  1% of the original kernel matrix.

torch.Size([10428, 2])
We keep 1.24e+06/2.96e+07 =  4% of the original kernel matrix.

torch.Size([17394, 2])
We keep 2.64e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([18874, 2])
We keep 3.71e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([23332, 2])
We keep 4.86e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([12688, 2])
We keep 1.51e+06/5.02e+07 =  3% of the original kernel matrix.

torch.Size([19175, 2])
We keep 3.15e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([17696, 2])
We keep 7.43e+06/1.36e+08 =  5% of the original kernel matrix.

torch.Size([22824, 2])
We keep 4.67e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([14468, 2])
We keep 2.60e+06/9.25e+07 =  2% of the original kernel matrix.

torch.Size([20171, 2])
We keep 4.06e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([2570, 2])
We keep 8.99e+04/1.39e+06 =  6% of the original kernel matrix.

torch.Size([9712, 2])
We keep 8.99e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([19114, 2])
We keep 3.59e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([23605, 2])
We keep 4.87e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([10881, 2])
We keep 3.00e+06/6.59e+07 =  4% of the original kernel matrix.

torch.Size([17644, 2])
We keep 3.20e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([32139, 2])
We keep 3.01e+07/9.35e+08 =  3% of the original kernel matrix.

torch.Size([30238, 2])
We keep 9.90e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([9261, 2])
We keep 1.22e+06/2.80e+07 =  4% of the original kernel matrix.

torch.Size([16278, 2])
We keep 2.57e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([30651, 2])
We keep 1.08e+07/4.34e+08 =  2% of the original kernel matrix.

torch.Size([30555, 2])
We keep 7.42e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([20395, 2])
We keep 3.97e+07/4.79e+08 =  8% of the original kernel matrix.

torch.Size([23464, 2])
We keep 7.61e+06/4.78e+08 =  1% of the original kernel matrix.

torch.Size([16698, 2])
We keep 3.84e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([22066, 2])
We keep 4.45e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([7376, 2])
We keep 6.96e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([14844, 2])
We keep 1.98e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([16832, 2])
We keep 2.76e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([22117, 2])
We keep 4.31e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([26732, 2])
We keep 6.17e+06/2.96e+08 =  2% of the original kernel matrix.

torch.Size([27451, 2])
We keep 5.95e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([249097, 2])
We keep 5.05e+08/3.53e+10 =  1% of the original kernel matrix.

torch.Size([88895, 2])
We keep 4.81e+07/4.10e+09 =  1% of the original kernel matrix.

torch.Size([9890, 2])
We keep 9.91e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([16961, 2])
We keep 2.50e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([3005, 2])
We keep 1.02e+05/1.73e+06 =  5% of the original kernel matrix.

torch.Size([10474, 2])
We keep 9.50e+05/2.87e+07 =  3% of the original kernel matrix.

torch.Size([8575, 2])
We keep 1.07e+06/2.34e+07 =  4% of the original kernel matrix.

torch.Size([15771, 2])
We keep 2.41e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([208785, 2])
We keep 4.85e+08/2.86e+10 =  1% of the original kernel matrix.

torch.Size([80754, 2])
We keep 4.37e+07/3.69e+09 =  1% of the original kernel matrix.

torch.Size([294807, 2])
We keep 5.81e+08/5.00e+10 =  1% of the original kernel matrix.

torch.Size([97658, 2])
We keep 5.54e+07/4.88e+09 =  1% of the original kernel matrix.

torch.Size([39270, 2])
We keep 2.50e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([33307, 2])
We keep 1.08e+07/7.10e+08 =  1% of the original kernel matrix.

torch.Size([11858, 2])
We keep 1.97e+06/4.99e+07 =  3% of the original kernel matrix.

torch.Size([18513, 2])
We keep 3.22e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([54568, 2])
We keep 4.99e+07/2.21e+09 =  2% of the original kernel matrix.

torch.Size([39797, 2])
We keep 1.48e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([167487, 2])
We keep 1.37e+08/1.45e+10 =  0% of the original kernel matrix.

torch.Size([71811, 2])
We keep 3.22e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([4660, 2])
We keep 2.49e+05/4.89e+06 =  5% of the original kernel matrix.

torch.Size([12326, 2])
We keep 1.36e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([11835, 2])
We keep 1.92e+06/5.43e+07 =  3% of the original kernel matrix.

torch.Size([18654, 2])
We keep 3.26e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([26553, 2])
We keep 5.07e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([27402, 2])
We keep 5.96e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([44018, 2])
We keep 5.59e+07/1.49e+09 =  3% of the original kernel matrix.

torch.Size([35690, 2])
We keep 1.24e+07/8.42e+08 =  1% of the original kernel matrix.

torch.Size([23722, 2])
We keep 8.03e+06/2.94e+08 =  2% of the original kernel matrix.

torch.Size([26072, 2])
We keep 6.04e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([205594, 2])
We keep 2.76e+08/2.40e+10 =  1% of the original kernel matrix.

torch.Size([80018, 2])
We keep 4.08e+07/3.38e+09 =  1% of the original kernel matrix.

torch.Size([13904, 2])
We keep 1.53e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([20019, 2])
We keep 3.24e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([8841, 2])
We keep 7.15e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([16083, 2])
We keep 2.23e+06/9.67e+07 =  2% of the original kernel matrix.

torch.Size([3948, 2])
We keep 1.65e+05/2.98e+06 =  5% of the original kernel matrix.

torch.Size([11633, 2])
We keep 1.17e+06/3.77e+07 =  3% of the original kernel matrix.

torch.Size([27568, 2])
We keep 9.48e+06/3.64e+08 =  2% of the original kernel matrix.

torch.Size([28700, 2])
We keep 6.87e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([20237, 2])
We keep 3.98e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([24401, 2])
We keep 4.83e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([34326, 2])
We keep 1.77e+07/6.94e+08 =  2% of the original kernel matrix.

torch.Size([31761, 2])
We keep 8.90e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([196193, 2])
We keep 4.60e+08/2.08e+10 =  2% of the original kernel matrix.

torch.Size([77731, 2])
We keep 3.83e+07/3.15e+09 =  1% of the original kernel matrix.

torch.Size([683141, 2])
We keep 1.48e+09/1.96e+11 =  0% of the original kernel matrix.

torch.Size([148774, 2])
We keep 1.03e+08/9.68e+09 =  1% of the original kernel matrix.

torch.Size([11435, 2])
We keep 1.40e+06/3.92e+07 =  3% of the original kernel matrix.

torch.Size([18159, 2])
We keep 2.90e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([4111, 2])
We keep 2.15e+05/4.15e+06 =  5% of the original kernel matrix.

torch.Size([11690, 2])
We keep 1.30e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([17993, 2])
We keep 2.67e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([22904, 2])
We keep 4.29e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([116124, 2])
We keep 8.57e+08/7.11e+09 = 12% of the original kernel matrix.

torch.Size([58949, 2])
We keep 2.21e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([33651, 2])
We keep 1.08e+07/5.68e+08 =  1% of the original kernel matrix.

torch.Size([31818, 2])
We keep 8.26e+06/5.21e+08 =  1% of the original kernel matrix.

torch.Size([3870, 2])
We keep 1.72e+05/3.08e+06 =  5% of the original kernel matrix.

torch.Size([11570, 2])
We keep 1.14e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([7096, 2])
We keep 5.35e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([14778, 2])
We keep 1.83e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([48603, 2])
We keep 1.03e+08/1.26e+09 =  8% of the original kernel matrix.

torch.Size([38092, 2])
We keep 1.15e+07/7.75e+08 =  1% of the original kernel matrix.

torch.Size([3939, 2])
We keep 1.73e+05/2.96e+06 =  5% of the original kernel matrix.

torch.Size([11662, 2])
We keep 1.15e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([66810, 2])
We keep 6.76e+07/2.98e+09 =  2% of the original kernel matrix.

torch.Size([43221, 2])
We keep 1.67e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([7163, 2])
We keep 5.19e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([14773, 2])
We keep 1.90e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([21445, 2])
We keep 4.04e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([25319, 2])
We keep 5.32e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([7163, 2])
We keep 6.85e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([14614, 2])
We keep 2.04e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([8261, 2])
We keep 6.52e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([15646, 2])
We keep 2.12e+06/8.92e+07 =  2% of the original kernel matrix.

torch.Size([4646, 2])
We keep 2.26e+05/4.41e+06 =  5% of the original kernel matrix.

torch.Size([12320, 2])
We keep 1.32e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([177887, 2])
We keep 1.58e+08/1.52e+10 =  1% of the original kernel matrix.

torch.Size([74214, 2])
We keep 3.27e+07/2.69e+09 =  1% of the original kernel matrix.

torch.Size([374397, 2])
We keep 5.96e+08/6.63e+10 =  0% of the original kernel matrix.

torch.Size([109971, 2])
We keep 6.32e+07/5.62e+09 =  1% of the original kernel matrix.

torch.Size([156349, 2])
We keep 2.79e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([69029, 2])
We keep 3.47e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([95982, 2])
We keep 8.78e+07/4.75e+09 =  1% of the original kernel matrix.

torch.Size([52835, 2])
We keep 2.01e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([34516, 2])
We keep 9.77e+06/5.30e+08 =  1% of the original kernel matrix.

torch.Size([32488, 2])
We keep 7.92e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([12441, 2])
We keep 1.99e+06/5.39e+07 =  3% of the original kernel matrix.

torch.Size([18961, 2])
We keep 3.31e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([12519, 2])
We keep 2.54e+06/6.74e+07 =  3% of the original kernel matrix.

torch.Size([19366, 2])
We keep 3.74e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([156561, 2])
We keep 1.22e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([69444, 2])
We keep 2.97e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([33670, 2])
We keep 2.35e+07/7.15e+08 =  3% of the original kernel matrix.

torch.Size([31883, 2])
We keep 9.24e+06/5.84e+08 =  1% of the original kernel matrix.

torch.Size([68493, 2])
We keep 1.07e+08/2.59e+09 =  4% of the original kernel matrix.

torch.Size([44856, 2])
We keep 1.54e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([15433, 2])
We keep 3.62e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([21083, 2])
We keep 4.15e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([633456, 2])
We keep 3.48e+09/3.00e+11 =  1% of the original kernel matrix.

torch.Size([143254, 2])
We keep 1.28e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([26911, 2])
We keep 1.08e+07/3.54e+08 =  3% of the original kernel matrix.

torch.Size([28489, 2])
We keep 6.65e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([68419, 2])
We keep 4.80e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([44853, 2])
We keep 1.54e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([161731, 2])
We keep 2.37e+08/1.85e+10 =  1% of the original kernel matrix.

torch.Size([70240, 2])
We keep 3.61e+07/2.97e+09 =  1% of the original kernel matrix.

torch.Size([39291, 2])
We keep 1.13e+07/7.81e+08 =  1% of the original kernel matrix.

torch.Size([34890, 2])
We keep 9.20e+06/6.10e+08 =  1% of the original kernel matrix.

torch.Size([49673, 2])
We keep 8.13e+07/1.35e+09 =  6% of the original kernel matrix.

torch.Size([38370, 2])
We keep 1.13e+07/8.02e+08 =  1% of the original kernel matrix.

torch.Size([263997, 2])
We keep 6.48e+08/4.17e+10 =  1% of the original kernel matrix.

torch.Size([91640, 2])
We keep 5.21e+07/4.46e+09 =  1% of the original kernel matrix.

torch.Size([62012, 2])
We keep 2.51e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([42971, 2])
We keep 1.35e+07/9.50e+08 =  1% of the original kernel matrix.

torch.Size([39141, 2])
We keep 1.70e+07/8.32e+08 =  2% of the original kernel matrix.

torch.Size([34489, 2])
We keep 9.63e+06/6.30e+08 =  1% of the original kernel matrix.

torch.Size([15677, 2])
We keep 3.84e+06/9.51e+07 =  4% of the original kernel matrix.

torch.Size([21388, 2])
We keep 4.04e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([66395, 2])
We keep 6.10e+07/2.51e+09 =  2% of the original kernel matrix.

torch.Size([44014, 2])
We keep 1.52e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([17380, 2])
We keep 5.39e+06/1.17e+08 =  4% of the original kernel matrix.

torch.Size([22670, 2])
We keep 4.32e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([177197, 2])
We keep 2.49e+08/1.50e+10 =  1% of the original kernel matrix.

torch.Size([73819, 2])
We keep 3.28e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([1218975, 2])
We keep 4.43e+09/6.40e+11 =  0% of the original kernel matrix.

torch.Size([202424, 2])
We keep 1.79e+08/1.75e+10 =  1% of the original kernel matrix.

torch.Size([116473, 2])
We keep 2.90e+08/8.08e+09 =  3% of the original kernel matrix.

torch.Size([58575, 2])
We keep 2.57e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([158821, 2])
We keep 4.09e+08/2.57e+10 =  1% of the original kernel matrix.

torch.Size([67590, 2])
We keep 4.24e+07/3.50e+09 =  1% of the original kernel matrix.

torch.Size([49254, 2])
We keep 7.71e+07/1.78e+09 =  4% of the original kernel matrix.

torch.Size([37790, 2])
We keep 1.33e+07/9.20e+08 =  1% of the original kernel matrix.

torch.Size([34323, 2])
We keep 2.76e+07/8.09e+08 =  3% of the original kernel matrix.

torch.Size([31678, 2])
We keep 9.58e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([26197, 2])
We keep 9.69e+06/3.52e+08 =  2% of the original kernel matrix.

torch.Size([27992, 2])
We keep 6.94e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([14018, 2])
We keep 2.79e+06/8.02e+07 =  3% of the original kernel matrix.

torch.Size([19894, 2])
We keep 3.72e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([14159, 2])
We keep 2.06e+06/6.28e+07 =  3% of the original kernel matrix.

torch.Size([20263, 2])
We keep 3.44e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([300002, 2])
We keep 3.54e+08/4.67e+10 =  0% of the original kernel matrix.

torch.Size([98807, 2])
We keep 5.37e+07/4.72e+09 =  1% of the original kernel matrix.

torch.Size([26592, 2])
We keep 2.29e+07/8.03e+08 =  2% of the original kernel matrix.

torch.Size([26986, 2])
We keep 9.62e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([189995, 2])
We keep 5.35e+08/2.76e+10 =  1% of the original kernel matrix.

torch.Size([75291, 2])
We keep 4.28e+07/3.63e+09 =  1% of the original kernel matrix.

torch.Size([134832, 2])
We keep 3.21e+08/1.31e+10 =  2% of the original kernel matrix.

torch.Size([63788, 2])
We keep 3.16e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([43710, 2])
We keep 1.35e+07/9.08e+08 =  1% of the original kernel matrix.

torch.Size([36254, 2])
We keep 9.89e+06/6.58e+08 =  1% of the original kernel matrix.

torch.Size([8454, 2])
We keep 9.49e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([15819, 2])
We keep 2.31e+06/9.88e+07 =  2% of the original kernel matrix.

torch.Size([69677, 2])
We keep 1.22e+08/3.09e+09 =  3% of the original kernel matrix.

torch.Size([46843, 2])
We keep 1.70e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([114298, 2])
We keep 7.10e+07/5.88e+09 =  1% of the original kernel matrix.

torch.Size([57703, 2])
We keep 2.18e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([6983, 2])
We keep 5.22e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([14685, 2])
We keep 1.86e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([657897, 2])
We keep 1.62e+09/2.14e+11 =  0% of the original kernel matrix.

torch.Size([145682, 2])
We keep 1.07e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([47680, 2])
We keep 2.56e+07/1.26e+09 =  2% of the original kernel matrix.

torch.Size([37890, 2])
We keep 1.04e+07/7.76e+08 =  1% of the original kernel matrix.

torch.Size([51652, 2])
We keep 4.64e+07/1.33e+09 =  3% of the original kernel matrix.

torch.Size([38729, 2])
We keep 1.18e+07/7.98e+08 =  1% of the original kernel matrix.

torch.Size([10465, 2])
We keep 1.03e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([17668, 2])
We keep 2.56e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([13023, 2])
We keep 1.53e+06/5.05e+07 =  3% of the original kernel matrix.

torch.Size([19300, 2])
We keep 3.18e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([4950, 2])
We keep 2.50e+05/4.91e+06 =  5% of the original kernel matrix.

torch.Size([12668, 2])
We keep 1.37e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([2918, 2])
We keep 1.08e+05/1.65e+06 =  6% of the original kernel matrix.

torch.Size([10286, 2])
We keep 9.32e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([16642, 2])
We keep 4.61e+06/1.08e+08 =  4% of the original kernel matrix.

torch.Size([22031, 2])
We keep 4.31e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([45186, 2])
We keep 3.38e+07/1.45e+09 =  2% of the original kernel matrix.

torch.Size([36756, 2])
We keep 1.23e+07/8.30e+08 =  1% of the original kernel matrix.

torch.Size([2905696, 2])
We keep 4.17e+10/4.92e+12 =  0% of the original kernel matrix.

torch.Size([297165, 2])
We keep 4.66e+08/4.84e+10 =  0% of the original kernel matrix.

torch.Size([10052, 2])
We keep 1.49e+06/3.38e+07 =  4% of the original kernel matrix.

torch.Size([17073, 2])
We keep 2.75e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([53349, 2])
We keep 1.95e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([40140, 2])
We keep 1.16e+07/8.01e+08 =  1% of the original kernel matrix.

torch.Size([13204, 2])
We keep 3.01e+06/6.90e+07 =  4% of the original kernel matrix.

torch.Size([19643, 2])
We keep 3.67e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([19781, 2])
We keep 3.44e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([24130, 2])
We keep 4.76e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([12619, 2])
We keep 2.54e+06/6.44e+07 =  3% of the original kernel matrix.

torch.Size([19102, 2])
We keep 3.57e+06/1.75e+08 =  2% of the original kernel matrix.

torch.Size([242461, 2])
We keep 6.87e+08/4.36e+10 =  1% of the original kernel matrix.

torch.Size([87051, 2])
We keep 5.33e+07/4.56e+09 =  1% of the original kernel matrix.

torch.Size([15335, 2])
We keep 3.41e+06/8.39e+07 =  4% of the original kernel matrix.

torch.Size([21025, 2])
We keep 3.87e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([1033260, 2])
We keep 3.44e+09/5.27e+11 =  0% of the original kernel matrix.

torch.Size([188912, 2])
We keep 1.62e+08/1.58e+10 =  1% of the original kernel matrix.

torch.Size([8002, 2])
We keep 5.94e+05/1.54e+07 =  3% of the original kernel matrix.

torch.Size([15468, 2])
We keep 2.06e+06/8.56e+07 =  2% of the original kernel matrix.

torch.Size([207066, 2])
We keep 3.57e+08/2.56e+10 =  1% of the original kernel matrix.

torch.Size([80448, 2])
We keep 4.21e+07/3.49e+09 =  1% of the original kernel matrix.

torch.Size([45598, 2])
We keep 9.53e+07/1.26e+09 =  7% of the original kernel matrix.

torch.Size([36817, 2])
We keep 1.13e+07/7.75e+08 =  1% of the original kernel matrix.

torch.Size([141810, 2])
We keep 1.70e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([65266, 2])
We keep 2.90e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([225228, 2])
We keep 4.11e+08/3.43e+10 =  1% of the original kernel matrix.

torch.Size([83741, 2])
We keep 4.77e+07/4.05e+09 =  1% of the original kernel matrix.

torch.Size([21796, 2])
We keep 4.61e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([25389, 2])
We keep 5.40e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([9902, 2])
We keep 1.21e+06/2.81e+07 =  4% of the original kernel matrix.

torch.Size([16844, 2])
We keep 2.60e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([90437, 2])
We keep 5.52e+07/4.45e+09 =  1% of the original kernel matrix.

torch.Size([51536, 2])
We keep 1.97e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([12244, 2])
We keep 1.54e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([18993, 2])
We keep 3.03e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([14472, 2])
We keep 4.42e+06/1.32e+08 =  3% of the original kernel matrix.

torch.Size([19656, 2])
We keep 4.63e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([73445, 2])
We keep 4.06e+07/2.73e+09 =  1% of the original kernel matrix.

torch.Size([45949, 2])
We keep 1.58e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([22631, 2])
We keep 1.64e+07/3.37e+08 =  4% of the original kernel matrix.

torch.Size([25868, 2])
We keep 6.12e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([48021, 2])
We keep 4.18e+07/1.66e+09 =  2% of the original kernel matrix.

torch.Size([38207, 2])
We keep 1.34e+07/8.91e+08 =  1% of the original kernel matrix.

torch.Size([29074, 2])
We keep 1.68e+07/4.68e+08 =  3% of the original kernel matrix.

torch.Size([29198, 2])
We keep 7.67e+06/4.72e+08 =  1% of the original kernel matrix.

torch.Size([235694, 2])
We keep 5.58e+08/3.02e+10 =  1% of the original kernel matrix.

torch.Size([86124, 2])
We keep 4.49e+07/3.80e+09 =  1% of the original kernel matrix.

torch.Size([222357, 2])
We keep 6.72e+08/3.06e+10 =  2% of the original kernel matrix.

torch.Size([82988, 2])
We keep 4.48e+07/3.82e+09 =  1% of the original kernel matrix.

torch.Size([47256, 2])
We keep 2.41e+07/1.14e+09 =  2% of the original kernel matrix.

torch.Size([37699, 2])
We keep 1.11e+07/7.36e+08 =  1% of the original kernel matrix.

torch.Size([12304, 2])
We keep 1.64e+06/4.91e+07 =  3% of the original kernel matrix.

torch.Size([18955, 2])
We keep 3.20e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([104032, 2])
We keep 1.66e+08/7.10e+09 =  2% of the original kernel matrix.

torch.Size([54907, 2])
We keep 2.37e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([62546, 2])
We keep 3.80e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([42220, 2])
We keep 1.46e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([44575, 2])
We keep 3.71e+07/1.15e+09 =  3% of the original kernel matrix.

torch.Size([36331, 2])
We keep 1.10e+07/7.41e+08 =  1% of the original kernel matrix.

torch.Size([28925, 2])
We keep 1.52e+07/4.49e+08 =  3% of the original kernel matrix.

torch.Size([29222, 2])
We keep 7.46e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([42547, 2])
We keep 3.68e+07/1.24e+09 =  2% of the original kernel matrix.

torch.Size([34993, 2])
We keep 1.15e+07/7.69e+08 =  1% of the original kernel matrix.

torch.Size([50948, 2])
We keep 4.93e+07/1.55e+09 =  3% of the original kernel matrix.

torch.Size([38605, 2])
We keep 1.23e+07/8.59e+08 =  1% of the original kernel matrix.

torch.Size([125386, 2])
We keep 3.94e+08/1.78e+10 =  2% of the original kernel matrix.

torch.Size([57949, 2])
We keep 3.57e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([54320, 2])
We keep 7.16e+07/1.96e+09 =  3% of the original kernel matrix.

torch.Size([39541, 2])
We keep 1.39e+07/9.66e+08 =  1% of the original kernel matrix.

torch.Size([15214, 2])
We keep 2.65e+06/9.60e+07 =  2% of the original kernel matrix.

torch.Size([20965, 2])
We keep 4.20e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([11488, 2])
We keep 1.44e+06/3.77e+07 =  3% of the original kernel matrix.

torch.Size([18219, 2])
We keep 2.85e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([16066, 2])
We keep 5.27e+06/1.12e+08 =  4% of the original kernel matrix.

torch.Size([21762, 2])
We keep 4.39e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([55271, 2])
We keep 4.86e+07/2.17e+09 =  2% of the original kernel matrix.

torch.Size([39232, 2])
We keep 1.43e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([127906, 2])
We keep 1.56e+08/9.22e+09 =  1% of the original kernel matrix.

torch.Size([61699, 2])
We keep 2.67e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([172659, 2])
We keep 3.31e+08/1.51e+10 =  2% of the original kernel matrix.

torch.Size([72828, 2])
We keep 3.28e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([17754, 2])
We keep 3.13e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([22785, 2])
We keep 4.40e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([32195, 2])
We keep 7.88e+06/4.82e+08 =  1% of the original kernel matrix.

torch.Size([31298, 2])
We keep 7.60e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([65232, 2])
We keep 3.82e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([43581, 2])
We keep 1.42e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([246425, 2])
We keep 9.10e+08/3.50e+10 =  2% of the original kernel matrix.

torch.Size([88628, 2])
We keep 4.69e+07/4.09e+09 =  1% of the original kernel matrix.

torch.Size([68097, 2])
We keep 5.18e+07/2.39e+09 =  2% of the original kernel matrix.

torch.Size([44546, 2])
We keep 1.51e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([94041, 2])
We keep 4.90e+07/3.91e+09 =  1% of the original kernel matrix.

torch.Size([52261, 2])
We keep 1.81e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([9584, 2])
We keep 2.70e+06/3.18e+07 =  8% of the original kernel matrix.

torch.Size([16699, 2])
We keep 2.79e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([34111, 2])
We keep 1.28e+07/6.27e+08 =  2% of the original kernel matrix.

torch.Size([32294, 2])
We keep 8.28e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([5402, 2])
We keep 3.00e+05/6.14e+06 =  4% of the original kernel matrix.

torch.Size([13203, 2])
We keep 1.46e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([10693, 2])
We keep 1.24e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([17548, 2])
We keep 2.82e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([110374, 2])
We keep 6.50e+07/5.55e+09 =  1% of the original kernel matrix.

torch.Size([56663, 2])
We keep 2.08e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([36604, 2])
We keep 1.97e+07/8.01e+08 =  2% of the original kernel matrix.

torch.Size([32781, 2])
We keep 9.36e+06/6.18e+08 =  1% of the original kernel matrix.

torch.Size([59729, 2])
We keep 2.12e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([42429, 2])
We keep 1.26e+07/8.95e+08 =  1% of the original kernel matrix.

torch.Size([3858, 2])
We keep 1.73e+05/3.19e+06 =  5% of the original kernel matrix.

torch.Size([11604, 2])
We keep 1.18e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([6503, 2])
We keep 4.67e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([14146, 2])
We keep 1.77e+06/7.06e+07 =  2% of the original kernel matrix.

torch.Size([69665, 2])
We keep 5.62e+07/2.58e+09 =  2% of the original kernel matrix.

torch.Size([44737, 2])
We keep 1.52e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([48150, 2])
We keep 1.89e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([37836, 2])
We keep 1.14e+07/7.71e+08 =  1% of the original kernel matrix.

torch.Size([34183, 2])
We keep 9.23e+06/5.25e+08 =  1% of the original kernel matrix.

torch.Size([32409, 2])
We keep 7.93e+06/5.00e+08 =  1% of the original kernel matrix.

torch.Size([6001, 2])
We keep 3.66e+05/8.06e+06 =  4% of the original kernel matrix.

torch.Size([13758, 2])
We keep 1.64e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([9025, 2])
We keep 8.05e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([16225, 2])
We keep 2.28e+06/9.82e+07 =  2% of the original kernel matrix.

torch.Size([49221, 2])
We keep 3.09e+07/1.30e+09 =  2% of the original kernel matrix.

torch.Size([38364, 2])
We keep 1.17e+07/7.87e+08 =  1% of the original kernel matrix.

torch.Size([30252, 2])
We keep 8.00e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([30326, 2])
We keep 7.48e+06/4.65e+08 =  1% of the original kernel matrix.

torch.Size([102602, 2])
We keep 1.26e+08/5.65e+09 =  2% of the original kernel matrix.

torch.Size([55081, 2])
We keep 2.17e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([7698, 2])
We keep 7.33e+05/1.67e+07 =  4% of the original kernel matrix.

torch.Size([15359, 2])
We keep 2.04e+06/8.91e+07 =  2% of the original kernel matrix.

torch.Size([7248, 2])
We keep 6.90e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([14643, 2])
We keep 2.01e+06/8.20e+07 =  2% of the original kernel matrix.

torch.Size([16067, 2])
We keep 4.21e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([21300, 2])
We keep 4.50e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([9961, 2])
We keep 1.16e+06/2.95e+07 =  3% of the original kernel matrix.

torch.Size([16943, 2])
We keep 2.58e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([253489, 2])
We keep 3.74e+08/3.37e+10 =  1% of the original kernel matrix.

torch.Size([90122, 2])
We keep 4.49e+07/4.01e+09 =  1% of the original kernel matrix.

torch.Size([12524, 2])
We keep 1.72e+06/4.51e+07 =  3% of the original kernel matrix.

torch.Size([19141, 2])
We keep 2.96e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([80931, 2])
We keep 6.37e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([48200, 2])
We keep 1.77e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([151649, 2])
We keep 1.40e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([68107, 2])
We keep 2.89e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([144349, 2])
We keep 3.88e+08/1.24e+10 =  3% of the original kernel matrix.

torch.Size([66018, 2])
We keep 3.02e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([192111, 2])
We keep 3.13e+08/1.95e+10 =  1% of the original kernel matrix.

torch.Size([76668, 2])
We keep 3.70e+07/3.05e+09 =  1% of the original kernel matrix.

torch.Size([151799, 2])
We keep 1.29e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([68025, 2])
We keep 2.88e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([7871, 2])
We keep 6.49e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([15274, 2])
We keep 2.06e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([55855, 2])
We keep 2.33e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([40928, 2])
We keep 1.27e+07/8.70e+08 =  1% of the original kernel matrix.

torch.Size([27640, 2])
We keep 6.11e+06/3.61e+08 =  1% of the original kernel matrix.

torch.Size([29067, 2])
We keep 6.71e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([99642, 2])
We keep 1.08e+08/5.05e+09 =  2% of the original kernel matrix.

torch.Size([53447, 2])
We keep 2.03e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([128231, 2])
We keep 1.33e+08/9.00e+09 =  1% of the original kernel matrix.

torch.Size([61876, 2])
We keep 2.62e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([26435, 2])
We keep 1.20e+07/4.40e+08 =  2% of the original kernel matrix.

torch.Size([27725, 2])
We keep 7.40e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([93204, 2])
We keep 6.52e+07/4.03e+09 =  1% of the original kernel matrix.

torch.Size([52302, 2])
We keep 1.85e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([10692, 2])
We keep 1.05e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([17607, 2])
We keep 2.74e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([20485, 2])
We keep 1.32e+07/3.64e+08 =  3% of the original kernel matrix.

torch.Size([23901, 2])
We keep 6.89e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([11839, 2])
We keep 5.15e+06/4.47e+07 = 11% of the original kernel matrix.

torch.Size([18649, 2])
We keep 2.96e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([14407, 2])
We keep 2.31e+06/7.12e+07 =  3% of the original kernel matrix.

torch.Size([20460, 2])
We keep 3.66e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([236709, 2])
We keep 5.22e+08/3.06e+10 =  1% of the original kernel matrix.

torch.Size([86718, 2])
We keep 4.53e+07/3.82e+09 =  1% of the original kernel matrix.

torch.Size([370225, 2])
We keep 2.49e+09/1.03e+11 =  2% of the original kernel matrix.

torch.Size([108235, 2])
We keep 7.86e+07/7.01e+09 =  1% of the original kernel matrix.

torch.Size([16663, 2])
We keep 5.53e+06/1.62e+08 =  3% of the original kernel matrix.

torch.Size([21871, 2])
We keep 5.16e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([29931, 2])
We keep 7.86e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([30207, 2])
We keep 7.13e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([27070, 2])
We keep 7.37e+06/3.38e+08 =  2% of the original kernel matrix.

torch.Size([28966, 2])
We keep 6.74e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([125735, 2])
We keep 1.70e+08/9.21e+09 =  1% of the original kernel matrix.

torch.Size([61061, 2])
We keep 2.61e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([24478, 2])
We keep 6.40e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([27580, 2])
We keep 6.31e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([15418, 2])
We keep 3.29e+06/9.04e+07 =  3% of the original kernel matrix.

torch.Size([21202, 2])
We keep 3.96e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([70792, 2])
We keep 6.03e+07/2.54e+09 =  2% of the original kernel matrix.

torch.Size([45817, 2])
We keep 1.51e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([167199, 2])
We keep 1.67e+08/1.41e+10 =  1% of the original kernel matrix.

torch.Size([71568, 2])
We keep 3.21e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([13926, 2])
We keep 2.61e+06/7.61e+07 =  3% of the original kernel matrix.

torch.Size([20042, 2])
We keep 3.74e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([100027, 2])
We keep 1.12e+08/5.63e+09 =  1% of the original kernel matrix.

torch.Size([53863, 2])
We keep 2.16e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([7691, 2])
We keep 6.91e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([15406, 2])
We keep 1.92e+06/8.25e+07 =  2% of the original kernel matrix.

time for making ranges is 3.5673153400421143
Sorting X and nu_X
time for sorting X is 0.08365130424499512
Sorting Z and nu_Z
time for sorting Z is 0.00027370452880859375
Starting Optim
sum tnu_Z before tensor(32129912., device='cuda:0')
c= tensor(1647.2866, device='cuda:0')
c= tensor(114254.7969, device='cuda:0')
c= tensor(117417.6719, device='cuda:0')
c= tensor(198030.8750, device='cuda:0')
c= tensor(232885.9062, device='cuda:0')
c= tensor(347175.0312, device='cuda:0')
c= tensor(1107679.3750, device='cuda:0')
c= tensor(1404002., device='cuda:0')
c= tensor(1442800., device='cuda:0')
c= tensor(4848110., device='cuda:0')
c= tensor(4918783., device='cuda:0')
c= tensor(8839366., device='cuda:0')
c= tensor(8852883., device='cuda:0')
c= tensor(34468760., device='cuda:0')
c= tensor(34621648., device='cuda:0')
c= tensor(35059280., device='cuda:0')
c= tensor(36072048., device='cuda:0')
c= tensor(36487248., device='cuda:0')
c= tensor(51512712., device='cuda:0')
c= tensor(56249052., device='cuda:0')
c= tensor(56287208., device='cuda:0')
c= tensor(76043320., device='cuda:0')
c= tensor(76087520., device='cuda:0')
c= tensor(76785984., device='cuda:0')
c= tensor(76806928., device='cuda:0')
c= tensor(77878856., device='cuda:0')
c= tensor(79042104., device='cuda:0')
c= tensor(79055424., device='cuda:0')
c= tensor(84337680., device='cuda:0')
c= tensor(3.9931e+08, device='cuda:0')
c= tensor(3.9934e+08, device='cuda:0')
c= tensor(5.5440e+08, device='cuda:0')
c= tensor(5.5451e+08, device='cuda:0')
c= tensor(5.5453e+08, device='cuda:0')
c= tensor(5.5456e+08, device='cuda:0')
c= tensor(5.6317e+08, device='cuda:0')
c= tensor(5.6842e+08, device='cuda:0')
c= tensor(5.6842e+08, device='cuda:0')
c= tensor(5.6843e+08, device='cuda:0')
c= tensor(5.6843e+08, device='cuda:0')
c= tensor(5.6844e+08, device='cuda:0')
c= tensor(5.6844e+08, device='cuda:0')
c= tensor(5.6844e+08, device='cuda:0')
c= tensor(5.6845e+08, device='cuda:0')
c= tensor(5.6845e+08, device='cuda:0')
c= tensor(5.6845e+08, device='cuda:0')
c= tensor(5.6846e+08, device='cuda:0')
c= tensor(5.6847e+08, device='cuda:0')
c= tensor(5.6847e+08, device='cuda:0')
c= tensor(5.6850e+08, device='cuda:0')
c= tensor(5.6854e+08, device='cuda:0')
c= tensor(5.6854e+08, device='cuda:0')
c= tensor(5.6856e+08, device='cuda:0')
c= tensor(5.6856e+08, device='cuda:0')
c= tensor(5.6857e+08, device='cuda:0')
c= tensor(5.6858e+08, device='cuda:0')
c= tensor(5.6858e+08, device='cuda:0')
c= tensor(5.6858e+08, device='cuda:0')
c= tensor(5.6859e+08, device='cuda:0')
c= tensor(5.6859e+08, device='cuda:0')
c= tensor(5.6860e+08, device='cuda:0')
c= tensor(5.6860e+08, device='cuda:0')
c= tensor(5.6863e+08, device='cuda:0')
c= tensor(5.6865e+08, device='cuda:0')
c= tensor(5.6866e+08, device='cuda:0')
c= tensor(5.6866e+08, device='cuda:0')
c= tensor(5.6867e+08, device='cuda:0')
c= tensor(5.6867e+08, device='cuda:0')
c= tensor(5.6868e+08, device='cuda:0')
c= tensor(5.6868e+08, device='cuda:0')
c= tensor(5.6869e+08, device='cuda:0')
c= tensor(5.6869e+08, device='cuda:0')
c= tensor(5.6870e+08, device='cuda:0')
c= tensor(5.6870e+08, device='cuda:0')
c= tensor(5.6871e+08, device='cuda:0')
c= tensor(5.6872e+08, device='cuda:0')
c= tensor(5.6872e+08, device='cuda:0')
c= tensor(5.6872e+08, device='cuda:0')
c= tensor(5.6872e+08, device='cuda:0')
c= tensor(5.6875e+08, device='cuda:0')
c= tensor(5.6876e+08, device='cuda:0')
c= tensor(5.6876e+08, device='cuda:0')
c= tensor(5.6877e+08, device='cuda:0')
c= tensor(5.6878e+08, device='cuda:0')
c= tensor(5.6878e+08, device='cuda:0')
c= tensor(5.6878e+08, device='cuda:0')
c= tensor(5.6879e+08, device='cuda:0')
c= tensor(5.6879e+08, device='cuda:0')
c= tensor(5.6880e+08, device='cuda:0')
c= tensor(5.6880e+08, device='cuda:0')
c= tensor(5.6881e+08, device='cuda:0')
c= tensor(5.6881e+08, device='cuda:0')
c= tensor(5.6881e+08, device='cuda:0')
c= tensor(5.6881e+08, device='cuda:0')
c= tensor(5.6883e+08, device='cuda:0')
c= tensor(5.6884e+08, device='cuda:0')
c= tensor(5.6884e+08, device='cuda:0')
c= tensor(5.6889e+08, device='cuda:0')
c= tensor(5.6889e+08, device='cuda:0')
c= tensor(5.6890e+08, device='cuda:0')
c= tensor(5.6893e+08, device='cuda:0')
c= tensor(5.6894e+08, device='cuda:0')
c= tensor(5.6896e+08, device='cuda:0')
c= tensor(5.6896e+08, device='cuda:0')
c= tensor(5.6897e+08, device='cuda:0')
c= tensor(5.6897e+08, device='cuda:0')
c= tensor(5.6899e+08, device='cuda:0')
c= tensor(5.6899e+08, device='cuda:0')
c= tensor(5.6899e+08, device='cuda:0')
c= tensor(5.6900e+08, device='cuda:0')
c= tensor(5.6900e+08, device='cuda:0')
c= tensor(5.6900e+08, device='cuda:0')
c= tensor(5.6900e+08, device='cuda:0')
c= tensor(5.6901e+08, device='cuda:0')
c= tensor(5.6902e+08, device='cuda:0')
c= tensor(5.6902e+08, device='cuda:0')
c= tensor(5.6903e+08, device='cuda:0')
c= tensor(5.6903e+08, device='cuda:0')
c= tensor(5.6905e+08, device='cuda:0')
c= tensor(5.6905e+08, device='cuda:0')
c= tensor(5.6908e+08, device='cuda:0')
c= tensor(5.6909e+08, device='cuda:0')
c= tensor(5.6909e+08, device='cuda:0')
c= tensor(5.6909e+08, device='cuda:0')
c= tensor(5.6910e+08, device='cuda:0')
c= tensor(5.6910e+08, device='cuda:0')
c= tensor(5.6910e+08, device='cuda:0')
c= tensor(5.6911e+08, device='cuda:0')
c= tensor(5.6914e+08, device='cuda:0')
c= tensor(5.6915e+08, device='cuda:0')
c= tensor(5.6918e+08, device='cuda:0')
c= tensor(5.6918e+08, device='cuda:0')
c= tensor(5.6919e+08, device='cuda:0')
c= tensor(5.6919e+08, device='cuda:0')
c= tensor(5.6919e+08, device='cuda:0')
c= tensor(5.6920e+08, device='cuda:0')
c= tensor(5.6920e+08, device='cuda:0')
c= tensor(5.6920e+08, device='cuda:0')
c= tensor(5.6920e+08, device='cuda:0')
c= tensor(5.6921e+08, device='cuda:0')
c= tensor(5.6921e+08, device='cuda:0')
c= tensor(5.6921e+08, device='cuda:0')
c= tensor(5.6927e+08, device='cuda:0')
c= tensor(5.6930e+08, device='cuda:0')
c= tensor(5.6931e+08, device='cuda:0')
c= tensor(5.6931e+08, device='cuda:0')
c= tensor(5.6931e+08, device='cuda:0')
c= tensor(5.6932e+08, device='cuda:0')
c= tensor(5.6932e+08, device='cuda:0')
c= tensor(5.6933e+08, device='cuda:0')
c= tensor(5.6933e+08, device='cuda:0')
c= tensor(5.6933e+08, device='cuda:0')
c= tensor(5.6934e+08, device='cuda:0')
c= tensor(5.6942e+08, device='cuda:0')
c= tensor(5.6943e+08, device='cuda:0')
c= tensor(5.6951e+08, device='cuda:0')
c= tensor(5.6951e+08, device='cuda:0')
c= tensor(5.6952e+08, device='cuda:0')
c= tensor(5.6953e+08, device='cuda:0')
c= tensor(5.6953e+08, device='cuda:0')
c= tensor(5.6993e+08, device='cuda:0')
c= tensor(5.6993e+08, device='cuda:0')
c= tensor(5.6994e+08, device='cuda:0')
c= tensor(5.6994e+08, device='cuda:0')
c= tensor(5.6995e+08, device='cuda:0')
c= tensor(5.6995e+08, device='cuda:0')
c= tensor(5.6995e+08, device='cuda:0')
c= tensor(5.6996e+08, device='cuda:0')
c= tensor(5.6997e+08, device='cuda:0')
c= tensor(5.6997e+08, device='cuda:0')
c= tensor(5.6997e+08, device='cuda:0')
c= tensor(5.6997e+08, device='cuda:0')
c= tensor(5.6998e+08, device='cuda:0')
c= tensor(5.6999e+08, device='cuda:0')
c= tensor(5.6999e+08, device='cuda:0')
c= tensor(5.7000e+08, device='cuda:0')
c= tensor(5.7001e+08, device='cuda:0')
c= tensor(5.7002e+08, device='cuda:0')
c= tensor(5.7003e+08, device='cuda:0')
c= tensor(5.7003e+08, device='cuda:0')
c= tensor(5.7003e+08, device='cuda:0')
c= tensor(5.7004e+08, device='cuda:0')
c= tensor(5.7005e+08, device='cuda:0')
c= tensor(5.7005e+08, device='cuda:0')
c= tensor(5.7006e+08, device='cuda:0')
c= tensor(5.7007e+08, device='cuda:0')
c= tensor(5.7007e+08, device='cuda:0')
c= tensor(5.7008e+08, device='cuda:0')
c= tensor(5.7009e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7018e+08, device='cuda:0')
c= tensor(5.7018e+08, device='cuda:0')
c= tensor(5.7019e+08, device='cuda:0')
c= tensor(5.7019e+08, device='cuda:0')
c= tensor(5.7019e+08, device='cuda:0')
c= tensor(5.7020e+08, device='cuda:0')
c= tensor(5.7020e+08, device='cuda:0')
c= tensor(5.7020e+08, device='cuda:0')
c= tensor(5.7020e+08, device='cuda:0')
c= tensor(5.7021e+08, device='cuda:0')
c= tensor(5.7022e+08, device='cuda:0')
c= tensor(5.7024e+08, device='cuda:0')
c= tensor(5.7024e+08, device='cuda:0')
c= tensor(5.7025e+08, device='cuda:0')
c= tensor(5.7025e+08, device='cuda:0')
c= tensor(5.7026e+08, device='cuda:0')
c= tensor(5.7027e+08, device='cuda:0')
c= tensor(5.7029e+08, device='cuda:0')
c= tensor(5.7029e+08, device='cuda:0')
c= tensor(5.7029e+08, device='cuda:0')
c= tensor(5.7030e+08, device='cuda:0')
c= tensor(5.7030e+08, device='cuda:0')
c= tensor(5.7030e+08, device='cuda:0')
c= tensor(5.7030e+08, device='cuda:0')
c= tensor(5.7031e+08, device='cuda:0')
c= tensor(5.7032e+08, device='cuda:0')
c= tensor(5.7032e+08, device='cuda:0')
c= tensor(5.7033e+08, device='cuda:0')
c= tensor(5.7033e+08, device='cuda:0')
c= tensor(5.7034e+08, device='cuda:0')
c= tensor(5.7034e+08, device='cuda:0')
c= tensor(5.7035e+08, device='cuda:0')
c= tensor(5.7036e+08, device='cuda:0')
c= tensor(5.7036e+08, device='cuda:0')
c= tensor(5.7037e+08, device='cuda:0')
c= tensor(5.7037e+08, device='cuda:0')
c= tensor(5.7037e+08, device='cuda:0')
c= tensor(5.7038e+08, device='cuda:0')
c= tensor(5.7038e+08, device='cuda:0')
c= tensor(5.7039e+08, device='cuda:0')
c= tensor(5.7040e+08, device='cuda:0')
c= tensor(5.7040e+08, device='cuda:0')
c= tensor(5.7040e+08, device='cuda:0')
c= tensor(5.7041e+08, device='cuda:0')
c= tensor(5.7042e+08, device='cuda:0')
c= tensor(5.7042e+08, device='cuda:0')
c= tensor(5.7049e+08, device='cuda:0')
c= tensor(5.7216e+08, device='cuda:0')
c= tensor(5.7222e+08, device='cuda:0')
c= tensor(5.7223e+08, device='cuda:0')
c= tensor(5.7223e+08, device='cuda:0')
c= tensor(5.7225e+08, device='cuda:0')
c= tensor(5.9616e+08, device='cuda:0')
c= tensor(6.6538e+08, device='cuda:0')
c= tensor(6.6538e+08, device='cuda:0')
c= tensor(6.6768e+08, device='cuda:0')
c= tensor(6.6811e+08, device='cuda:0')
c= tensor(6.6853e+08, device='cuda:0')
c= tensor(6.8821e+08, device='cuda:0')
c= tensor(6.8821e+08, device='cuda:0')
c= tensor(6.8822e+08, device='cuda:0')
c= tensor(6.9455e+08, device='cuda:0')
c= tensor(7.1222e+08, device='cuda:0')
c= tensor(7.1223e+08, device='cuda:0')
c= tensor(7.1242e+08, device='cuda:0')
c= tensor(7.1255e+08, device='cuda:0')
c= tensor(7.2010e+08, device='cuda:0')
c= tensor(7.2247e+08, device='cuda:0')
c= tensor(7.2332e+08, device='cuda:0')
c= tensor(7.2365e+08, device='cuda:0')
c= tensor(7.2379e+08, device='cuda:0')
c= tensor(7.2379e+08, device='cuda:0')
c= tensor(7.6018e+08, device='cuda:0')
c= tensor(7.6019e+08, device='cuda:0')
c= tensor(7.6019e+08, device='cuda:0')
c= tensor(7.6075e+08, device='cuda:0')
c= tensor(7.6189e+08, device='cuda:0')
c= tensor(7.7392e+08, device='cuda:0')
c= tensor(7.7593e+08, device='cuda:0')
c= tensor(7.7593e+08, device='cuda:0')
c= tensor(7.7621e+08, device='cuda:0')
c= tensor(7.7625e+08, device='cuda:0')
c= tensor(7.7696e+08, device='cuda:0')
c= tensor(7.7815e+08, device='cuda:0')
c= tensor(7.7817e+08, device='cuda:0')
c= tensor(7.7839e+08, device='cuda:0')
c= tensor(7.7839e+08, device='cuda:0')
c= tensor(7.7840e+08, device='cuda:0')
c= tensor(7.7956e+08, device='cuda:0')
c= tensor(7.8295e+08, device='cuda:0')
c= tensor(7.8360e+08, device='cuda:0')
c= tensor(7.8366e+08, device='cuda:0')
c= tensor(8.1454e+08, device='cuda:0')
c= tensor(8.1459e+08, device='cuda:0')
c= tensor(8.1477e+08, device='cuda:0')
c= tensor(8.1645e+08, device='cuda:0')
c= tensor(8.1646e+08, device='cuda:0')
c= tensor(8.2197e+08, device='cuda:0')
c= tensor(8.4223e+08, device='cuda:0')
c= tensor(8.6482e+08, device='cuda:0')
c= tensor(8.6486e+08, device='cuda:0')
c= tensor(8.6490e+08, device='cuda:0')
c= tensor(8.6494e+08, device='cuda:0')
c= tensor(8.6495e+08, device='cuda:0')
c= tensor(8.6574e+08, device='cuda:0')
c= tensor(8.6577e+08, device='cuda:0')
c= tensor(8.6643e+08, device='cuda:0')
c= tensor(8.8544e+08, device='cuda:0')
c= tensor(8.8573e+08, device='cuda:0')
c= tensor(8.8581e+08, device='cuda:0')
c= tensor(8.8582e+08, device='cuda:0')
c= tensor(8.9082e+08, device='cuda:0')
c= tensor(8.9095e+08, device='cuda:0')
c= tensor(8.9130e+08, device='cuda:0')
c= tensor(8.9134e+08, device='cuda:0')
c= tensor(9.3563e+08, device='cuda:0')
c= tensor(9.3568e+08, device='cuda:0')
c= tensor(9.4371e+08, device='cuda:0')
c= tensor(9.4374e+08, device='cuda:0')
c= tensor(9.4484e+08, device='cuda:0')
c= tensor(9.4532e+08, device='cuda:0')
c= tensor(9.7582e+08, device='cuda:0')
c= tensor(9.7660e+08, device='cuda:0')
c= tensor(9.7661e+08, device='cuda:0')
c= tensor(9.7974e+08, device='cuda:0')
c= tensor(9.8259e+08, device='cuda:0')
c= tensor(9.8270e+08, device='cuda:0')
c= tensor(9.8548e+08, device='cuda:0')
c= tensor(9.9061e+08, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0231e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0280e+09, device='cuda:0')
c= tensor(1.0280e+09, device='cuda:0')
c= tensor(1.0281e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0283e+09, device='cuda:0')
c= tensor(1.0287e+09, device='cuda:0')
c= tensor(1.0290e+09, device='cuda:0')
c= tensor(1.0291e+09, device='cuda:0')
c= tensor(1.0301e+09, device='cuda:0')
c= tensor(1.0301e+09, device='cuda:0')
c= tensor(1.0519e+09, device='cuda:0')
c= tensor(1.0519e+09, device='cuda:0')
c= tensor(1.0539e+09, device='cuda:0')
c= tensor(1.0539e+09, device='cuda:0')
c= tensor(1.0539e+09, device='cuda:0')
c= tensor(1.0539e+09, device='cuda:0')
c= tensor(1.0542e+09, device='cuda:0')
c= tensor(1.0542e+09, device='cuda:0')
c= tensor(1.0626e+09, device='cuda:0')
c= tensor(1.0626e+09, device='cuda:0')
c= tensor(1.0626e+09, device='cuda:0')
c= tensor(1.0807e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.0836e+09, device='cuda:0')
c= tensor(1.0982e+09, device='cuda:0')
c= tensor(1.1167e+09, device='cuda:0')
c= tensor(1.1168e+09, device='cuda:0')
c= tensor(1.1168e+09, device='cuda:0')
c= tensor(1.1170e+09, device='cuda:0')
c= tensor(1.1170e+09, device='cuda:0')
c= tensor(1.1170e+09, device='cuda:0')
c= tensor(1.1171e+09, device='cuda:0')
c= tensor(1.1171e+09, device='cuda:0')
c= tensor(1.1171e+09, device='cuda:0')
c= tensor(1.1171e+09, device='cuda:0')
c= tensor(1.1171e+09, device='cuda:0')
c= tensor(1.1566e+09, device='cuda:0')
c= tensor(1.1567e+09, device='cuda:0')
c= tensor(1.1595e+09, device='cuda:0')
c= tensor(1.1595e+09, device='cuda:0')
c= tensor(1.1598e+09, device='cuda:0')
c= tensor(1.1598e+09, device='cuda:0')
c= tensor(1.3900e+09, device='cuda:0')
c= tensor(1.4433e+09, device='cuda:0')
c= tensor(1.4435e+09, device='cuda:0')
c= tensor(1.4468e+09, device='cuda:0')
c= tensor(1.4468e+09, device='cuda:0')
c= tensor(1.4471e+09, device='cuda:0')
c= tensor(1.4484e+09, device='cuda:0')
c= tensor(1.4486e+09, device='cuda:0')
c= tensor(1.4487e+09, device='cuda:0')
c= tensor(1.4503e+09, device='cuda:0')
c= tensor(1.5248e+09, device='cuda:0')
c= tensor(1.5253e+09, device='cuda:0')
c= tensor(1.5253e+09, device='cuda:0')
c= tensor(1.5257e+09, device='cuda:0')
c= tensor(1.5257e+09, device='cuda:0')
c= tensor(1.5257e+09, device='cuda:0')
c= tensor(1.5307e+09, device='cuda:0')
c= tensor(1.5308e+09, device='cuda:0')
c= tensor(1.5308e+09, device='cuda:0')
c= tensor(1.5317e+09, device='cuda:0')
c= tensor(1.5317e+09, device='cuda:0')
c= tensor(1.5317e+09, device='cuda:0')
c= tensor(1.5344e+09, device='cuda:0')
c= tensor(1.5368e+09, device='cuda:0')
c= tensor(1.5431e+09, device='cuda:0')
c= tensor(1.5497e+09, device='cuda:0')
c= tensor(1.5620e+09, device='cuda:0')
c= tensor(1.5622e+09, device='cuda:0')
c= tensor(1.5626e+09, device='cuda:0')
c= tensor(1.5665e+09, device='cuda:0')
c= tensor(1.5698e+09, device='cuda:0')
c= tensor(1.5698e+09, device='cuda:0')
c= tensor(1.5804e+09, device='cuda:0')
c= tensor(1.6023e+09, device='cuda:0')
c= tensor(1.6087e+09, device='cuda:0')
c= tensor(1.6148e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6192e+09, device='cuda:0')
c= tensor(1.6215e+09, device='cuda:0')
c= tensor(1.6640e+09, device='cuda:0')
c= tensor(1.6695e+09, device='cuda:0')
c= tensor(1.6713e+09, device='cuda:0')
c= tensor(1.6715e+09, device='cuda:0')
c= tensor(1.6740e+09, device='cuda:0')
c= tensor(1.6740e+09, device='cuda:0')
c= tensor(1.6740e+09, device='cuda:0')
c= tensor(1.6825e+09, device='cuda:0')
c= tensor(1.6825e+09, device='cuda:0')
c= tensor(1.6825e+09, device='cuda:0')
c= tensor(1.6826e+09, device='cuda:0')
c= tensor(1.7170e+09, device='cuda:0')
c= tensor(1.7171e+09, device='cuda:0')
c= tensor(1.7250e+09, device='cuda:0')
c= tensor(1.7252e+09, device='cuda:0')
c= tensor(1.7253e+09, device='cuda:0')
c= tensor(1.7253e+09, device='cuda:0')
c= tensor(1.7253e+09, device='cuda:0')
c= tensor(1.7266e+09, device='cuda:0')
c= tensor(1.7270e+09, device='cuda:0')
c= tensor(1.7270e+09, device='cuda:0')
c= tensor(1.7298e+09, device='cuda:0')
c= tensor(1.7298e+09, device='cuda:0')
c= tensor(1.7311e+09, device='cuda:0')
c= tensor(1.7312e+09, device='cuda:0')
c= tensor(1.7330e+09, device='cuda:0')
c= tensor(1.7330e+09, device='cuda:0')
c= tensor(1.7332e+09, device='cuda:0')
c= tensor(1.7332e+09, device='cuda:0')
c= tensor(1.7334e+09, device='cuda:0')
c= tensor(1.7348e+09, device='cuda:0')
c= tensor(1.7541e+09, device='cuda:0')
c= tensor(1.7541e+09, device='cuda:0')
c= tensor(1.7543e+09, device='cuda:0')
c= tensor(1.7590e+09, device='cuda:0')
c= tensor(1.7591e+09, device='cuda:0')
c= tensor(1.7992e+09, device='cuda:0')
c= tensor(1.7992e+09, device='cuda:0')
c= tensor(1.8013e+09, device='cuda:0')
c= tensor(1.8035e+09, device='cuda:0')
c= tensor(1.8035e+09, device='cuda:0')
c= tensor(1.8094e+09, device='cuda:0')
c= tensor(1.8095e+09, device='cuda:0')
c= tensor(1.9274e+09, device='cuda:0')
c= tensor(1.9275e+09, device='cuda:0')
c= tensor(1.9276e+09, device='cuda:0')
c= tensor(1.9277e+09, device='cuda:0')
c= tensor(1.9277e+09, device='cuda:0')
c= tensor(1.9277e+09, device='cuda:0')
c= tensor(1.9283e+09, device='cuda:0')
c= tensor(1.9285e+09, device='cuda:0')
c= tensor(1.9298e+09, device='cuda:0')
c= tensor(1.9299e+09, device='cuda:0')
c= tensor(1.9299e+09, device='cuda:0')
c= tensor(1.9299e+09, device='cuda:0')
c= tensor(1.9621e+09, device='cuda:0')
c= tensor(1.9676e+09, device='cuda:0')
c= tensor(1.9805e+09, device='cuda:0')
c= tensor(1.9808e+09, device='cuda:0')
c= tensor(1.9808e+09, device='cuda:0')
c= tensor(1.9808e+09, device='cuda:0')
c= tensor(1.9809e+09, device='cuda:0')
c= tensor(2.0122e+09, device='cuda:0')
c= tensor(2.0122e+09, device='cuda:0')
c= tensor(2.0124e+09, device='cuda:0')
c= tensor(2.0133e+09, device='cuda:0')
c= tensor(2.0144e+09, device='cuda:0')
c= tensor(2.0144e+09, device='cuda:0')
c= tensor(2.0144e+09, device='cuda:0')
c= tensor(2.0337e+09, device='cuda:0')
c= tensor(2.0342e+09, device='cuda:0')
c= tensor(2.0348e+09, device='cuda:0')
c= tensor(2.0351e+09, device='cuda:0')
c= tensor(2.0387e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0760e+09, device='cuda:0')
c= tensor(2.0822e+09, device='cuda:0')
c= tensor(2.0823e+09, device='cuda:0')
c= tensor(2.0828e+09, device='cuda:0')
c= tensor(2.0837e+09, device='cuda:0')
c= tensor(2.0837e+09, device='cuda:0')
c= tensor(2.0838e+09, device='cuda:0')
c= tensor(2.0838e+09, device='cuda:0')
c= tensor(2.0843e+09, device='cuda:0')
c= tensor(2.0843e+09, device='cuda:0')
c= tensor(2.0843e+09, device='cuda:0')
c= tensor(2.0844e+09, device='cuda:0')
c= tensor(2.0845e+09, device='cuda:0')
c= tensor(2.0854e+09, device='cuda:0')
c= tensor(2.0854e+09, device='cuda:0')
c= tensor(2.0856e+09, device='cuda:0')
c= tensor(2.0863e+09, device='cuda:0')
c= tensor(2.0864e+09, device='cuda:0')
c= tensor(2.0864e+09, device='cuda:0')
c= tensor(2.0864e+09, device='cuda:0')
c= tensor(2.0865e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1238e+09, device='cuda:0')
c= tensor(2.1406e+09, device='cuda:0')
c= tensor(2.1410e+09, device='cuda:0')
c= tensor(2.1410e+09, device='cuda:0')
c= tensor(2.1420e+09, device='cuda:0')
c= tensor(2.1448e+09, device='cuda:0')
c= tensor(2.1448e+09, device='cuda:0')
c= tensor(2.1449e+09, device='cuda:0')
c= tensor(2.1450e+09, device='cuda:0')
c= tensor(2.1482e+09, device='cuda:0')
c= tensor(2.1485e+09, device='cuda:0')
c= tensor(2.1555e+09, device='cuda:0')
c= tensor(2.1556e+09, device='cuda:0')
c= tensor(2.1556e+09, device='cuda:0')
c= tensor(2.1556e+09, device='cuda:0')
c= tensor(2.1558e+09, device='cuda:0')
c= tensor(2.1558e+09, device='cuda:0')
c= tensor(2.1568e+09, device='cuda:0')
c= tensor(2.1698e+09, device='cuda:0')
c= tensor(2.2098e+09, device='cuda:0')
c= tensor(2.2098e+09, device='cuda:0')
c= tensor(2.2098e+09, device='cuda:0')
c= tensor(2.2099e+09, device='cuda:0')
c= tensor(2.2296e+09, device='cuda:0')
c= tensor(2.2298e+09, device='cuda:0')
c= tensor(2.2298e+09, device='cuda:0')
c= tensor(2.2298e+09, device='cuda:0')
c= tensor(2.2331e+09, device='cuda:0')
c= tensor(2.2331e+09, device='cuda:0')
c= tensor(2.2346e+09, device='cuda:0')
c= tensor(2.2346e+09, device='cuda:0')
c= tensor(2.2347e+09, device='cuda:0')
c= tensor(2.2347e+09, device='cuda:0')
c= tensor(2.2347e+09, device='cuda:0')
c= tensor(2.2347e+09, device='cuda:0')
c= tensor(2.2387e+09, device='cuda:0')
c= tensor(2.2566e+09, device='cuda:0')
c= tensor(2.2657e+09, device='cuda:0')
c= tensor(2.2678e+09, device='cuda:0')
c= tensor(2.2681e+09, device='cuda:0')
c= tensor(2.2681e+09, device='cuda:0')
c= tensor(2.2681e+09, device='cuda:0')
c= tensor(2.2708e+09, device='cuda:0')
c= tensor(2.2712e+09, device='cuda:0')
c= tensor(2.2741e+09, device='cuda:0')
c= tensor(2.2741e+09, device='cuda:0')
c= tensor(2.3974e+09, device='cuda:0')
c= tensor(2.3975e+09, device='cuda:0')
c= tensor(2.3986e+09, device='cuda:0')
c= tensor(2.4091e+09, device='cuda:0')
c= tensor(2.4093e+09, device='cuda:0')
c= tensor(2.4108e+09, device='cuda:0')
c= tensor(2.4284e+09, device='cuda:0')
c= tensor(2.4290e+09, device='cuda:0')
c= tensor(2.4293e+09, device='cuda:0')
c= tensor(2.4294e+09, device='cuda:0')
c= tensor(2.4309e+09, device='cuda:0')
c= tensor(2.4311e+09, device='cuda:0')
c= tensor(2.4367e+09, device='cuda:0')
c= tensor(2.5790e+09, device='cuda:0')
c= tensor(2.5869e+09, device='cuda:0')
c= tensor(2.5984e+09, device='cuda:0')
c= tensor(2.6000e+09, device='cuda:0')
c= tensor(2.6006e+09, device='cuda:0')
c= tensor(2.6008e+09, device='cuda:0')
c= tensor(2.6008e+09, device='cuda:0')
c= tensor(2.6009e+09, device='cuda:0')
c= tensor(2.6107e+09, device='cuda:0')
c= tensor(2.6111e+09, device='cuda:0')
c= tensor(2.6323e+09, device='cuda:0')
c= tensor(2.6405e+09, device='cuda:0')
c= tensor(2.6408e+09, device='cuda:0')
c= tensor(2.6408e+09, device='cuda:0')
c= tensor(2.6446e+09, device='cuda:0')
c= tensor(2.6459e+09, device='cuda:0')
c= tensor(2.6459e+09, device='cuda:0')
c= tensor(2.6884e+09, device='cuda:0')
c= tensor(2.6898e+09, device='cuda:0')
c= tensor(2.6914e+09, device='cuda:0')
c= tensor(2.6914e+09, device='cuda:0')
c= tensor(2.6914e+09, device='cuda:0')
c= tensor(2.6914e+09, device='cuda:0')
c= tensor(2.6914e+09, device='cuda:0')
c= tensor(2.6915e+09, device='cuda:0')
c= tensor(2.6922e+09, device='cuda:0')
c= tensor(4.4860e+09, device='cuda:0')
c= tensor(4.4860e+09, device='cuda:0')
c= tensor(4.4863e+09, device='cuda:0')
c= tensor(4.4864e+09, device='cuda:0')
c= tensor(4.4864e+09, device='cuda:0')
c= tensor(4.4865e+09, device='cuda:0')
c= tensor(4.5113e+09, device='cuda:0')
c= tensor(4.5114e+09, device='cuda:0')
c= tensor(4.6404e+09, device='cuda:0')
c= tensor(4.6404e+09, device='cuda:0')
c= tensor(4.6511e+09, device='cuda:0')
c= tensor(4.6538e+09, device='cuda:0')
c= tensor(4.6590e+09, device='cuda:0')
c= tensor(4.6733e+09, device='cuda:0')
c= tensor(4.6733e+09, device='cuda:0')
c= tensor(4.6734e+09, device='cuda:0')
c= tensor(4.6744e+09, device='cuda:0')
c= tensor(4.6745e+09, device='cuda:0')
c= tensor(4.6746e+09, device='cuda:0')
c= tensor(4.6754e+09, device='cuda:0')
c= tensor(4.6763e+09, device='cuda:0')
c= tensor(4.6771e+09, device='cuda:0')
c= tensor(4.6776e+09, device='cuda:0')
c= tensor(4.6982e+09, device='cuda:0')
c= tensor(4.7178e+09, device='cuda:0')
c= tensor(4.7183e+09, device='cuda:0')
c= tensor(4.7184e+09, device='cuda:0')
c= tensor(4.7251e+09, device='cuda:0')
c= tensor(4.7258e+09, device='cuda:0')
c= tensor(4.7264e+09, device='cuda:0')
c= tensor(4.7268e+09, device='cuda:0')
c= tensor(4.7279e+09, device='cuda:0')
c= tensor(4.7294e+09, device='cuda:0')
c= tensor(4.7429e+09, device='cuda:0')
c= tensor(4.7446e+09, device='cuda:0')
c= tensor(4.7446e+09, device='cuda:0')
c= tensor(4.7447e+09, device='cuda:0')
c= tensor(4.7447e+09, device='cuda:0')
c= tensor(4.7462e+09, device='cuda:0')
c= tensor(4.7507e+09, device='cuda:0')
c= tensor(4.7624e+09, device='cuda:0')
c= tensor(4.7624e+09, device='cuda:0')
c= tensor(4.7626e+09, device='cuda:0')
c= tensor(4.7634e+09, device='cuda:0')
c= tensor(4.7869e+09, device='cuda:0')
c= tensor(4.7878e+09, device='cuda:0')
c= tensor(4.7889e+09, device='cuda:0')
c= tensor(4.7892e+09, device='cuda:0')
c= tensor(4.7895e+09, device='cuda:0')
c= tensor(4.7895e+09, device='cuda:0')
c= tensor(4.7895e+09, device='cuda:0')
c= tensor(4.7911e+09, device='cuda:0')
c= tensor(4.7918e+09, device='cuda:0')
c= tensor(4.7921e+09, device='cuda:0')
c= tensor(4.7922e+09, device='cuda:0')
c= tensor(4.7922e+09, device='cuda:0')
c= tensor(4.7932e+09, device='cuda:0')
c= tensor(4.7936e+09, device='cuda:0')
c= tensor(4.7938e+09, device='cuda:0')
c= tensor(4.7938e+09, device='cuda:0')
c= tensor(4.7938e+09, device='cuda:0')
c= tensor(4.7944e+09, device='cuda:0')
c= tensor(4.7945e+09, device='cuda:0')
c= tensor(4.7983e+09, device='cuda:0')
c= tensor(4.7983e+09, device='cuda:0')
c= tensor(4.7984e+09, device='cuda:0')
c= tensor(4.7984e+09, device='cuda:0')
c= tensor(4.7984e+09, device='cuda:0')
c= tensor(4.8135e+09, device='cuda:0')
c= tensor(4.8136e+09, device='cuda:0')
c= tensor(4.8149e+09, device='cuda:0')
c= tensor(4.8179e+09, device='cuda:0')
c= tensor(4.8260e+09, device='cuda:0')
c= tensor(4.8336e+09, device='cuda:0')
c= tensor(4.8365e+09, device='cuda:0')
c= tensor(4.8365e+09, device='cuda:0')
c= tensor(4.8369e+09, device='cuda:0')
c= tensor(4.8370e+09, device='cuda:0')
c= tensor(4.8388e+09, device='cuda:0')
c= tensor(4.8425e+09, device='cuda:0')
c= tensor(4.8427e+09, device='cuda:0')
c= tensor(4.8445e+09, device='cuda:0')
c= tensor(4.8445e+09, device='cuda:0')
c= tensor(4.8457e+09, device='cuda:0')
c= tensor(4.8457e+09, device='cuda:0')
c= tensor(4.8458e+09, device='cuda:0')
c= tensor(4.8605e+09, device='cuda:0')
c= tensor(4.9590e+09, device='cuda:0')
c= tensor(4.9591e+09, device='cuda:0')
c= tensor(4.9593e+09, device='cuda:0')
c= tensor(4.9595e+09, device='cuda:0')
c= tensor(4.9637e+09, device='cuda:0')
c= tensor(4.9638e+09, device='cuda:0')
c= tensor(4.9639e+09, device='cuda:0')
c= tensor(4.9650e+09, device='cuda:0')
c= tensor(4.9685e+09, device='cuda:0')
c= tensor(4.9685e+09, device='cuda:0')
c= tensor(4.9711e+09, device='cuda:0')
c= tensor(4.9711e+09, device='cuda:0')
memory (bytes)
4700610560
time for making loss 2 is 14.728419542312622
p0 True
it  0 : 1866953728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 57% |
shape of L is 
torch.Size([])
memory (bytes)
4700819456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4701470720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  51570856000.0
relative error loss 10.374152
shape of L is 
torch.Size([])
memory (bytes)
4892844032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 11% |
memory (bytes)
4892848128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  51570635000.0
relative error loss 10.374108
shape of L is 
torch.Size([])
memory (bytes)
4894556160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4894556160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  51569623000.0
relative error loss 10.373904
shape of L is 
torch.Size([])
memory (bytes)
4895617024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4895617024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  51564030000.0
relative error loss 10.37278
shape of L is 
torch.Size([])
memory (bytes)
4896731136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4896768000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  51502547000.0
relative error loss 10.360412
shape of L is 
torch.Size([])
memory (bytes)
4898529280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4898660352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  50891550000.0
relative error loss 10.237501
shape of L is 
torch.Size([])
memory (bytes)
4900737024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4900737024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  47926330000.0
relative error loss 9.641008
shape of L is 
torch.Size([])
memory (bytes)
4902862848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4902899712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  34160476000.0
relative error loss 6.8718266
shape of L is 
torch.Size([])
memory (bytes)
4904886272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4904886272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  11959087000.0
relative error loss 2.405727
shape of L is 
torch.Size([])
memory (bytes)
4907139072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4907139072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  7350651000.0
relative error loss 1.4786797
time to take a step is 230.98574447631836
it  1 : 2296147456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4909273088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4909309952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  7350651000.0
relative error loss 1.4786797
shape of L is 
torch.Size([])
memory (bytes)
4911439872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4911480832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  5736462300.0
relative error loss 1.1539645
shape of L is 
torch.Size([])
memory (bytes)
4913545216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4913586176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 11% |
error is  5011284000.0
relative error loss 1.0080854
shape of L is 
torch.Size([])
memory (bytes)
4915712000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4915712000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4812527600.0
relative error loss 0.96810293
shape of L is 
torch.Size([])
memory (bytes)
4917682176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4917891072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4695310300.0
relative error loss 0.9445231
shape of L is 
torch.Size([])
memory (bytes)
4919992320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
4919992320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4517384700.0
relative error loss 0.90873104
shape of L is 
torch.Size([])
memory (bytes)
4922048512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4922048512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4326854700.0
relative error loss 0.87040347
shape of L is 
torch.Size([])
memory (bytes)
4924174336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4924174336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4098397000.0
relative error loss 0.8244462
shape of L is 
torch.Size([])
memory (bytes)
4926169088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4926337024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3874440400.0
relative error loss 0.7793944
shape of L is 
torch.Size([])
memory (bytes)
4928372736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4928372736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3560909600.0
relative error loss 0.71632355
time to take a step is 288.6565933227539
it  2 : 2418774528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4930224128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4930224128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3560909600.0
relative error loss 0.71632355
shape of L is 
torch.Size([])
memory (bytes)
4932640768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
4932640768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3953876500.0
relative error loss 0.795374
shape of L is 
torch.Size([])
memory (bytes)
4934754304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4934754304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3414995200.0
relative error loss 0.68697095
shape of L is 
torch.Size([])
memory (bytes)
4936884224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4936884224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3088115700.0
relative error loss 0.62121487
shape of L is 
torch.Size([])
memory (bytes)
4938964992
| ID | GPU | MEM |
------------------
|  0 |  4% |  0% |
|  1 | 23% | 11% |
memory (bytes)
4939005952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  2917213200.0
relative error loss 0.5868356
shape of L is 
torch.Size([])
memory (bytes)
4941074432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4941111296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2725273300.0
relative error loss 0.5482244
shape of L is 
torch.Size([])
memory (bytes)
4943048704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4943224832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2522439200.0
relative error loss 0.5074217
shape of L is 
torch.Size([])
memory (bytes)
4945309696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4945350656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2324998000.0
relative error loss 0.46770376
shape of L is 
torch.Size([])
memory (bytes)
4947378176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
4947378176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2145691600.0
relative error loss 0.43163395
shape of L is 
torch.Size([])
memory (bytes)
4949602304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4949639168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2027966500.0
relative error loss 0.40795198
time to take a step is 222.46524500846863
it  3 : 2418774528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4951568384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4951785472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2027966500.0
relative error loss 0.40795198
shape of L is 
torch.Size([])
memory (bytes)
4953772032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4953772032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 11% |
error is  1916653000.0
relative error loss 0.38555986
shape of L is 
torch.Size([])
memory (bytes)
4956024832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4956024832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1749025800.0
relative error loss 0.35183942
shape of L is 
torch.Size([])
memory (bytes)
4958162944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4958162944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1621723900.0
relative error loss 0.32623097
shape of L is 
torch.Size([])
memory (bytes)
4960329728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4960329728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1494417900.0
relative error loss 0.30062172
shape of L is 
torch.Size([])
memory (bytes)
4962377728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4962377728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 11% |
error is  1366602200.0
relative error loss 0.2749099
shape of L is 
torch.Size([])
memory (bytes)
4964622336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4964622336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1282260700.0
relative error loss 0.25794354
shape of L is 
torch.Size([])
memory (bytes)
4966703104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4966703104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1211385100.0
relative error loss 0.24368596
shape of L is 
torch.Size([])
memory (bytes)
4968865792
| ID | GPU | MEM |
------------------
|  0 | 18% |  0% |
|  1 | 23% | 11% |
memory (bytes)
4968898560
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 95% | 11% |
error is  1106633500.0
relative error loss 0.22261381
shape of L is 
torch.Size([])
memory (bytes)
4970991616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4970991616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% | 11% |
error is  1056095200.0
relative error loss 0.21244738
time to take a step is 220.94589948654175
c= tensor(1647.2866, device='cuda:0')
c= tensor(114254.7969, device='cuda:0')
c= tensor(117417.6719, device='cuda:0')
c= tensor(198030.8750, device='cuda:0')
c= tensor(232885.9062, device='cuda:0')
c= tensor(347175.0312, device='cuda:0')
c= tensor(1107679.3750, device='cuda:0')
c= tensor(1404002., device='cuda:0')
c= tensor(1442800., device='cuda:0')
c= tensor(4848110., device='cuda:0')
c= tensor(4918783., device='cuda:0')
c= tensor(8839366., device='cuda:0')
c= tensor(8852883., device='cuda:0')
c= tensor(34468760., device='cuda:0')
c= tensor(34621648., device='cuda:0')
c= tensor(35059280., device='cuda:0')
c= tensor(36072048., device='cuda:0')
c= tensor(36487248., device='cuda:0')
c= tensor(51512712., device='cuda:0')
c= tensor(56249052., device='cuda:0')
c= tensor(56287208., device='cuda:0')
c= tensor(76043320., device='cuda:0')
c= tensor(76087520., device='cuda:0')
c= tensor(76785984., device='cuda:0')
c= tensor(76806928., device='cuda:0')
c= tensor(77878856., device='cuda:0')
c= tensor(79042104., device='cuda:0')
c= tensor(79055424., device='cuda:0')
c= tensor(84337680., device='cuda:0')
c= tensor(3.9931e+08, device='cuda:0')
c= tensor(3.9934e+08, device='cuda:0')
c= tensor(5.5440e+08, device='cuda:0')
c= tensor(5.5451e+08, device='cuda:0')
c= tensor(5.5453e+08, device='cuda:0')
c= tensor(5.5456e+08, device='cuda:0')
c= tensor(5.6317e+08, device='cuda:0')
c= tensor(5.6842e+08, device='cuda:0')
c= tensor(5.6842e+08, device='cuda:0')
c= tensor(5.6843e+08, device='cuda:0')
c= tensor(5.6843e+08, device='cuda:0')
c= tensor(5.6844e+08, device='cuda:0')
c= tensor(5.6844e+08, device='cuda:0')
c= tensor(5.6844e+08, device='cuda:0')
c= tensor(5.6845e+08, device='cuda:0')
c= tensor(5.6845e+08, device='cuda:0')
c= tensor(5.6845e+08, device='cuda:0')
c= tensor(5.6846e+08, device='cuda:0')
c= tensor(5.6847e+08, device='cuda:0')
c= tensor(5.6847e+08, device='cuda:0')
c= tensor(5.6850e+08, device='cuda:0')
c= tensor(5.6854e+08, device='cuda:0')
c= tensor(5.6854e+08, device='cuda:0')
c= tensor(5.6856e+08, device='cuda:0')
c= tensor(5.6856e+08, device='cuda:0')
c= tensor(5.6857e+08, device='cuda:0')
c= tensor(5.6858e+08, device='cuda:0')
c= tensor(5.6858e+08, device='cuda:0')
c= tensor(5.6858e+08, device='cuda:0')
c= tensor(5.6859e+08, device='cuda:0')
c= tensor(5.6859e+08, device='cuda:0')
c= tensor(5.6860e+08, device='cuda:0')
c= tensor(5.6860e+08, device='cuda:0')
c= tensor(5.6863e+08, device='cuda:0')
c= tensor(5.6865e+08, device='cuda:0')
c= tensor(5.6866e+08, device='cuda:0')
c= tensor(5.6866e+08, device='cuda:0')
c= tensor(5.6867e+08, device='cuda:0')
c= tensor(5.6867e+08, device='cuda:0')
c= tensor(5.6868e+08, device='cuda:0')
c= tensor(5.6868e+08, device='cuda:0')
c= tensor(5.6869e+08, device='cuda:0')
c= tensor(5.6869e+08, device='cuda:0')
c= tensor(5.6870e+08, device='cuda:0')
c= tensor(5.6870e+08, device='cuda:0')
c= tensor(5.6871e+08, device='cuda:0')
c= tensor(5.6872e+08, device='cuda:0')
c= tensor(5.6872e+08, device='cuda:0')
c= tensor(5.6872e+08, device='cuda:0')
c= tensor(5.6872e+08, device='cuda:0')
c= tensor(5.6875e+08, device='cuda:0')
c= tensor(5.6876e+08, device='cuda:0')
c= tensor(5.6876e+08, device='cuda:0')
c= tensor(5.6877e+08, device='cuda:0')
c= tensor(5.6878e+08, device='cuda:0')
c= tensor(5.6878e+08, device='cuda:0')
c= tensor(5.6878e+08, device='cuda:0')
c= tensor(5.6879e+08, device='cuda:0')
c= tensor(5.6879e+08, device='cuda:0')
c= tensor(5.6880e+08, device='cuda:0')
c= tensor(5.6880e+08, device='cuda:0')
c= tensor(5.6881e+08, device='cuda:0')
c= tensor(5.6881e+08, device='cuda:0')
c= tensor(5.6881e+08, device='cuda:0')
c= tensor(5.6881e+08, device='cuda:0')
c= tensor(5.6883e+08, device='cuda:0')
c= tensor(5.6884e+08, device='cuda:0')
c= tensor(5.6884e+08, device='cuda:0')
c= tensor(5.6889e+08, device='cuda:0')
c= tensor(5.6889e+08, device='cuda:0')
c= tensor(5.6890e+08, device='cuda:0')
c= tensor(5.6893e+08, device='cuda:0')
c= tensor(5.6894e+08, device='cuda:0')
c= tensor(5.6896e+08, device='cuda:0')
c= tensor(5.6896e+08, device='cuda:0')
c= tensor(5.6897e+08, device='cuda:0')
c= tensor(5.6897e+08, device='cuda:0')
c= tensor(5.6899e+08, device='cuda:0')
c= tensor(5.6899e+08, device='cuda:0')
c= tensor(5.6899e+08, device='cuda:0')
c= tensor(5.6900e+08, device='cuda:0')
c= tensor(5.6900e+08, device='cuda:0')
c= tensor(5.6900e+08, device='cuda:0')
c= tensor(5.6900e+08, device='cuda:0')
c= tensor(5.6901e+08, device='cuda:0')
c= tensor(5.6902e+08, device='cuda:0')
c= tensor(5.6902e+08, device='cuda:0')
c= tensor(5.6903e+08, device='cuda:0')
c= tensor(5.6903e+08, device='cuda:0')
c= tensor(5.6905e+08, device='cuda:0')
c= tensor(5.6905e+08, device='cuda:0')
c= tensor(5.6908e+08, device='cuda:0')
c= tensor(5.6909e+08, device='cuda:0')
c= tensor(5.6909e+08, device='cuda:0')
c= tensor(5.6909e+08, device='cuda:0')
c= tensor(5.6910e+08, device='cuda:0')
c= tensor(5.6910e+08, device='cuda:0')
c= tensor(5.6910e+08, device='cuda:0')
c= tensor(5.6911e+08, device='cuda:0')
c= tensor(5.6914e+08, device='cuda:0')
c= tensor(5.6915e+08, device='cuda:0')
c= tensor(5.6918e+08, device='cuda:0')
c= tensor(5.6918e+08, device='cuda:0')
c= tensor(5.6919e+08, device='cuda:0')
c= tensor(5.6919e+08, device='cuda:0')
c= tensor(5.6919e+08, device='cuda:0')
c= tensor(5.6920e+08, device='cuda:0')
c= tensor(5.6920e+08, device='cuda:0')
c= tensor(5.6920e+08, device='cuda:0')
c= tensor(5.6920e+08, device='cuda:0')
c= tensor(5.6921e+08, device='cuda:0')
c= tensor(5.6921e+08, device='cuda:0')
c= tensor(5.6921e+08, device='cuda:0')
c= tensor(5.6927e+08, device='cuda:0')
c= tensor(5.6930e+08, device='cuda:0')
c= tensor(5.6931e+08, device='cuda:0')
c= tensor(5.6931e+08, device='cuda:0')
c= tensor(5.6931e+08, device='cuda:0')
c= tensor(5.6932e+08, device='cuda:0')
c= tensor(5.6932e+08, device='cuda:0')
c= tensor(5.6933e+08, device='cuda:0')
c= tensor(5.6933e+08, device='cuda:0')
c= tensor(5.6933e+08, device='cuda:0')
c= tensor(5.6934e+08, device='cuda:0')
c= tensor(5.6942e+08, device='cuda:0')
c= tensor(5.6943e+08, device='cuda:0')
c= tensor(5.6951e+08, device='cuda:0')
c= tensor(5.6951e+08, device='cuda:0')
c= tensor(5.6952e+08, device='cuda:0')
c= tensor(5.6953e+08, device='cuda:0')
c= tensor(5.6953e+08, device='cuda:0')
c= tensor(5.6993e+08, device='cuda:0')
c= tensor(5.6993e+08, device='cuda:0')
c= tensor(5.6994e+08, device='cuda:0')
c= tensor(5.6994e+08, device='cuda:0')
c= tensor(5.6995e+08, device='cuda:0')
c= tensor(5.6995e+08, device='cuda:0')
c= tensor(5.6995e+08, device='cuda:0')
c= tensor(5.6996e+08, device='cuda:0')
c= tensor(5.6997e+08, device='cuda:0')
c= tensor(5.6997e+08, device='cuda:0')
c= tensor(5.6997e+08, device='cuda:0')
c= tensor(5.6997e+08, device='cuda:0')
c= tensor(5.6998e+08, device='cuda:0')
c= tensor(5.6999e+08, device='cuda:0')
c= tensor(5.6999e+08, device='cuda:0')
c= tensor(5.7000e+08, device='cuda:0')
c= tensor(5.7001e+08, device='cuda:0')
c= tensor(5.7002e+08, device='cuda:0')
c= tensor(5.7003e+08, device='cuda:0')
c= tensor(5.7003e+08, device='cuda:0')
c= tensor(5.7003e+08, device='cuda:0')
c= tensor(5.7004e+08, device='cuda:0')
c= tensor(5.7005e+08, device='cuda:0')
c= tensor(5.7005e+08, device='cuda:0')
c= tensor(5.7006e+08, device='cuda:0')
c= tensor(5.7007e+08, device='cuda:0')
c= tensor(5.7007e+08, device='cuda:0')
c= tensor(5.7008e+08, device='cuda:0')
c= tensor(5.7009e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7018e+08, device='cuda:0')
c= tensor(5.7018e+08, device='cuda:0')
c= tensor(5.7019e+08, device='cuda:0')
c= tensor(5.7019e+08, device='cuda:0')
c= tensor(5.7019e+08, device='cuda:0')
c= tensor(5.7020e+08, device='cuda:0')
c= tensor(5.7020e+08, device='cuda:0')
c= tensor(5.7020e+08, device='cuda:0')
c= tensor(5.7020e+08, device='cuda:0')
c= tensor(5.7021e+08, device='cuda:0')
c= tensor(5.7022e+08, device='cuda:0')
c= tensor(5.7024e+08, device='cuda:0')
c= tensor(5.7024e+08, device='cuda:0')
c= tensor(5.7025e+08, device='cuda:0')
c= tensor(5.7025e+08, device='cuda:0')
c= tensor(5.7026e+08, device='cuda:0')
c= tensor(5.7027e+08, device='cuda:0')
c= tensor(5.7029e+08, device='cuda:0')
c= tensor(5.7029e+08, device='cuda:0')
c= tensor(5.7029e+08, device='cuda:0')
c= tensor(5.7030e+08, device='cuda:0')
c= tensor(5.7030e+08, device='cuda:0')
c= tensor(5.7030e+08, device='cuda:0')
c= tensor(5.7030e+08, device='cuda:0')
c= tensor(5.7031e+08, device='cuda:0')
c= tensor(5.7032e+08, device='cuda:0')
c= tensor(5.7032e+08, device='cuda:0')
c= tensor(5.7033e+08, device='cuda:0')
c= tensor(5.7033e+08, device='cuda:0')
c= tensor(5.7034e+08, device='cuda:0')
c= tensor(5.7034e+08, device='cuda:0')
c= tensor(5.7035e+08, device='cuda:0')
c= tensor(5.7036e+08, device='cuda:0')
c= tensor(5.7036e+08, device='cuda:0')
c= tensor(5.7037e+08, device='cuda:0')
c= tensor(5.7037e+08, device='cuda:0')
c= tensor(5.7037e+08, device='cuda:0')
c= tensor(5.7038e+08, device='cuda:0')
c= tensor(5.7038e+08, device='cuda:0')
c= tensor(5.7039e+08, device='cuda:0')
c= tensor(5.7040e+08, device='cuda:0')
c= tensor(5.7040e+08, device='cuda:0')
c= tensor(5.7040e+08, device='cuda:0')
c= tensor(5.7041e+08, device='cuda:0')
c= tensor(5.7042e+08, device='cuda:0')
c= tensor(5.7042e+08, device='cuda:0')
c= tensor(5.7049e+08, device='cuda:0')
c= tensor(5.7216e+08, device='cuda:0')
c= tensor(5.7222e+08, device='cuda:0')
c= tensor(5.7223e+08, device='cuda:0')
c= tensor(5.7223e+08, device='cuda:0')
c= tensor(5.7225e+08, device='cuda:0')
c= tensor(5.9616e+08, device='cuda:0')
c= tensor(6.6538e+08, device='cuda:0')
c= tensor(6.6538e+08, device='cuda:0')
c= tensor(6.6768e+08, device='cuda:0')
c= tensor(6.6811e+08, device='cuda:0')
c= tensor(6.6853e+08, device='cuda:0')
c= tensor(6.8821e+08, device='cuda:0')
c= tensor(6.8821e+08, device='cuda:0')
c= tensor(6.8822e+08, device='cuda:0')
c= tensor(6.9455e+08, device='cuda:0')
c= tensor(7.1222e+08, device='cuda:0')
c= tensor(7.1223e+08, device='cuda:0')
c= tensor(7.1242e+08, device='cuda:0')
c= tensor(7.1255e+08, device='cuda:0')
c= tensor(7.2010e+08, device='cuda:0')
c= tensor(7.2247e+08, device='cuda:0')
c= tensor(7.2332e+08, device='cuda:0')
c= tensor(7.2365e+08, device='cuda:0')
c= tensor(7.2379e+08, device='cuda:0')
c= tensor(7.2379e+08, device='cuda:0')
c= tensor(7.6018e+08, device='cuda:0')
c= tensor(7.6019e+08, device='cuda:0')
c= tensor(7.6019e+08, device='cuda:0')
c= tensor(7.6075e+08, device='cuda:0')
c= tensor(7.6189e+08, device='cuda:0')
c= tensor(7.7392e+08, device='cuda:0')
c= tensor(7.7593e+08, device='cuda:0')
c= tensor(7.7593e+08, device='cuda:0')
c= tensor(7.7621e+08, device='cuda:0')
c= tensor(7.7625e+08, device='cuda:0')
c= tensor(7.7696e+08, device='cuda:0')
c= tensor(7.7815e+08, device='cuda:0')
c= tensor(7.7817e+08, device='cuda:0')
c= tensor(7.7839e+08, device='cuda:0')
c= tensor(7.7839e+08, device='cuda:0')
c= tensor(7.7840e+08, device='cuda:0')
c= tensor(7.7956e+08, device='cuda:0')
c= tensor(7.8295e+08, device='cuda:0')
c= tensor(7.8360e+08, device='cuda:0')
c= tensor(7.8366e+08, device='cuda:0')
c= tensor(8.1454e+08, device='cuda:0')
c= tensor(8.1459e+08, device='cuda:0')
c= tensor(8.1477e+08, device='cuda:0')
c= tensor(8.1645e+08, device='cuda:0')
c= tensor(8.1646e+08, device='cuda:0')
c= tensor(8.2197e+08, device='cuda:0')
c= tensor(8.4223e+08, device='cuda:0')
c= tensor(8.6482e+08, device='cuda:0')
c= tensor(8.6486e+08, device='cuda:0')
c= tensor(8.6490e+08, device='cuda:0')
c= tensor(8.6494e+08, device='cuda:0')
c= tensor(8.6495e+08, device='cuda:0')
c= tensor(8.6574e+08, device='cuda:0')
c= tensor(8.6577e+08, device='cuda:0')
c= tensor(8.6643e+08, device='cuda:0')
c= tensor(8.8544e+08, device='cuda:0')
c= tensor(8.8573e+08, device='cuda:0')
c= tensor(8.8581e+08, device='cuda:0')
c= tensor(8.8582e+08, device='cuda:0')
c= tensor(8.9082e+08, device='cuda:0')
c= tensor(8.9095e+08, device='cuda:0')
c= tensor(8.9130e+08, device='cuda:0')
c= tensor(8.9134e+08, device='cuda:0')
c= tensor(9.3563e+08, device='cuda:0')
c= tensor(9.3568e+08, device='cuda:0')
c= tensor(9.4371e+08, device='cuda:0')
c= tensor(9.4374e+08, device='cuda:0')
c= tensor(9.4484e+08, device='cuda:0')
c= tensor(9.4532e+08, device='cuda:0')
c= tensor(9.7582e+08, device='cuda:0')
c= tensor(9.7660e+08, device='cuda:0')
c= tensor(9.7661e+08, device='cuda:0')
c= tensor(9.7974e+08, device='cuda:0')
c= tensor(9.8259e+08, device='cuda:0')
c= tensor(9.8270e+08, device='cuda:0')
c= tensor(9.8548e+08, device='cuda:0')
c= tensor(9.9061e+08, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0231e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0280e+09, device='cuda:0')
c= tensor(1.0280e+09, device='cuda:0')
c= tensor(1.0281e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0283e+09, device='cuda:0')
c= tensor(1.0287e+09, device='cuda:0')
c= tensor(1.0290e+09, device='cuda:0')
c= tensor(1.0291e+09, device='cuda:0')
c= tensor(1.0301e+09, device='cuda:0')
c= tensor(1.0301e+09, device='cuda:0')
c= tensor(1.0519e+09, device='cuda:0')
c= tensor(1.0519e+09, device='cuda:0')
c= tensor(1.0539e+09, device='cuda:0')
c= tensor(1.0539e+09, device='cuda:0')
c= tensor(1.0539e+09, device='cuda:0')
c= tensor(1.0539e+09, device='cuda:0')
c= tensor(1.0542e+09, device='cuda:0')
c= tensor(1.0542e+09, device='cuda:0')
c= tensor(1.0626e+09, device='cuda:0')
c= tensor(1.0626e+09, device='cuda:0')
c= tensor(1.0626e+09, device='cuda:0')
c= tensor(1.0807e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.0836e+09, device='cuda:0')
c= tensor(1.0982e+09, device='cuda:0')
c= tensor(1.1167e+09, device='cuda:0')
c= tensor(1.1168e+09, device='cuda:0')
c= tensor(1.1168e+09, device='cuda:0')
c= tensor(1.1170e+09, device='cuda:0')
c= tensor(1.1170e+09, device='cuda:0')
c= tensor(1.1170e+09, device='cuda:0')
c= tensor(1.1171e+09, device='cuda:0')
c= tensor(1.1171e+09, device='cuda:0')
c= tensor(1.1171e+09, device='cuda:0')
c= tensor(1.1171e+09, device='cuda:0')
c= tensor(1.1171e+09, device='cuda:0')
c= tensor(1.1566e+09, device='cuda:0')
c= tensor(1.1567e+09, device='cuda:0')
c= tensor(1.1595e+09, device='cuda:0')
c= tensor(1.1595e+09, device='cuda:0')
c= tensor(1.1598e+09, device='cuda:0')
c= tensor(1.1598e+09, device='cuda:0')
c= tensor(1.3900e+09, device='cuda:0')
c= tensor(1.4433e+09, device='cuda:0')
c= tensor(1.4435e+09, device='cuda:0')
c= tensor(1.4468e+09, device='cuda:0')
c= tensor(1.4468e+09, device='cuda:0')
c= tensor(1.4471e+09, device='cuda:0')
c= tensor(1.4484e+09, device='cuda:0')
c= tensor(1.4486e+09, device='cuda:0')
c= tensor(1.4487e+09, device='cuda:0')
c= tensor(1.4503e+09, device='cuda:0')
c= tensor(1.5248e+09, device='cuda:0')
c= tensor(1.5253e+09, device='cuda:0')
c= tensor(1.5253e+09, device='cuda:0')
c= tensor(1.5257e+09, device='cuda:0')
c= tensor(1.5257e+09, device='cuda:0')
c= tensor(1.5257e+09, device='cuda:0')
c= tensor(1.5307e+09, device='cuda:0')
c= tensor(1.5308e+09, device='cuda:0')
c= tensor(1.5308e+09, device='cuda:0')
c= tensor(1.5317e+09, device='cuda:0')
c= tensor(1.5317e+09, device='cuda:0')
c= tensor(1.5317e+09, device='cuda:0')
c= tensor(1.5344e+09, device='cuda:0')
c= tensor(1.5368e+09, device='cuda:0')
c= tensor(1.5431e+09, device='cuda:0')
c= tensor(1.5497e+09, device='cuda:0')
c= tensor(1.5620e+09, device='cuda:0')
c= tensor(1.5622e+09, device='cuda:0')
c= tensor(1.5626e+09, device='cuda:0')
c= tensor(1.5665e+09, device='cuda:0')
c= tensor(1.5698e+09, device='cuda:0')
c= tensor(1.5698e+09, device='cuda:0')
c= tensor(1.5804e+09, device='cuda:0')
c= tensor(1.6023e+09, device='cuda:0')
c= tensor(1.6087e+09, device='cuda:0')
c= tensor(1.6148e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6178e+09, device='cuda:0')
c= tensor(1.6192e+09, device='cuda:0')
c= tensor(1.6215e+09, device='cuda:0')
c= tensor(1.6640e+09, device='cuda:0')
c= tensor(1.6695e+09, device='cuda:0')
c= tensor(1.6713e+09, device='cuda:0')
c= tensor(1.6715e+09, device='cuda:0')
c= tensor(1.6740e+09, device='cuda:0')
c= tensor(1.6740e+09, device='cuda:0')
c= tensor(1.6740e+09, device='cuda:0')
c= tensor(1.6825e+09, device='cuda:0')
c= tensor(1.6825e+09, device='cuda:0')
c= tensor(1.6825e+09, device='cuda:0')
c= tensor(1.6826e+09, device='cuda:0')
c= tensor(1.7170e+09, device='cuda:0')
c= tensor(1.7171e+09, device='cuda:0')
c= tensor(1.7250e+09, device='cuda:0')
c= tensor(1.7252e+09, device='cuda:0')
c= tensor(1.7253e+09, device='cuda:0')
c= tensor(1.7253e+09, device='cuda:0')
c= tensor(1.7253e+09, device='cuda:0')
c= tensor(1.7266e+09, device='cuda:0')
c= tensor(1.7270e+09, device='cuda:0')
c= tensor(1.7270e+09, device='cuda:0')
c= tensor(1.7298e+09, device='cuda:0')
c= tensor(1.7298e+09, device='cuda:0')
c= tensor(1.7311e+09, device='cuda:0')
c= tensor(1.7312e+09, device='cuda:0')
c= tensor(1.7330e+09, device='cuda:0')
c= tensor(1.7330e+09, device='cuda:0')
c= tensor(1.7332e+09, device='cuda:0')
c= tensor(1.7332e+09, device='cuda:0')
c= tensor(1.7334e+09, device='cuda:0')
c= tensor(1.7348e+09, device='cuda:0')
c= tensor(1.7541e+09, device='cuda:0')
c= tensor(1.7541e+09, device='cuda:0')
c= tensor(1.7543e+09, device='cuda:0')
c= tensor(1.7590e+09, device='cuda:0')
c= tensor(1.7591e+09, device='cuda:0')
c= tensor(1.7992e+09, device='cuda:0')
c= tensor(1.7992e+09, device='cuda:0')
c= tensor(1.8013e+09, device='cuda:0')
c= tensor(1.8035e+09, device='cuda:0')
c= tensor(1.8035e+09, device='cuda:0')
c= tensor(1.8094e+09, device='cuda:0')
c= tensor(1.8095e+09, device='cuda:0')
c= tensor(1.9274e+09, device='cuda:0')
c= tensor(1.9275e+09, device='cuda:0')
c= tensor(1.9276e+09, device='cuda:0')
c= tensor(1.9277e+09, device='cuda:0')
c= tensor(1.9277e+09, device='cuda:0')
c= tensor(1.9277e+09, device='cuda:0')
c= tensor(1.9283e+09, device='cuda:0')
c= tensor(1.9285e+09, device='cuda:0')
c= tensor(1.9298e+09, device='cuda:0')
c= tensor(1.9299e+09, device='cuda:0')
c= tensor(1.9299e+09, device='cuda:0')
c= tensor(1.9299e+09, device='cuda:0')
c= tensor(1.9621e+09, device='cuda:0')
c= tensor(1.9676e+09, device='cuda:0')
c= tensor(1.9805e+09, device='cuda:0')
c= tensor(1.9808e+09, device='cuda:0')
c= tensor(1.9808e+09, device='cuda:0')
c= tensor(1.9808e+09, device='cuda:0')
c= tensor(1.9809e+09, device='cuda:0')
c= tensor(2.0122e+09, device='cuda:0')
c= tensor(2.0122e+09, device='cuda:0')
c= tensor(2.0124e+09, device='cuda:0')
c= tensor(2.0133e+09, device='cuda:0')
c= tensor(2.0144e+09, device='cuda:0')
c= tensor(2.0144e+09, device='cuda:0')
c= tensor(2.0144e+09, device='cuda:0')
c= tensor(2.0337e+09, device='cuda:0')
c= tensor(2.0342e+09, device='cuda:0')
c= tensor(2.0348e+09, device='cuda:0')
c= tensor(2.0351e+09, device='cuda:0')
c= tensor(2.0387e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0760e+09, device='cuda:0')
c= tensor(2.0822e+09, device='cuda:0')
c= tensor(2.0823e+09, device='cuda:0')
c= tensor(2.0828e+09, device='cuda:0')
c= tensor(2.0837e+09, device='cuda:0')
c= tensor(2.0837e+09, device='cuda:0')
c= tensor(2.0838e+09, device='cuda:0')
c= tensor(2.0838e+09, device='cuda:0')
c= tensor(2.0843e+09, device='cuda:0')
c= tensor(2.0843e+09, device='cuda:0')
c= tensor(2.0843e+09, device='cuda:0')
c= tensor(2.0844e+09, device='cuda:0')
c= tensor(2.0845e+09, device='cuda:0')
c= tensor(2.0854e+09, device='cuda:0')
c= tensor(2.0854e+09, device='cuda:0')
c= tensor(2.0856e+09, device='cuda:0')
c= tensor(2.0863e+09, device='cuda:0')
c= tensor(2.0864e+09, device='cuda:0')
c= tensor(2.0864e+09, device='cuda:0')
c= tensor(2.0864e+09, device='cuda:0')
c= tensor(2.0865e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1238e+09, device='cuda:0')
c= tensor(2.1406e+09, device='cuda:0')
c= tensor(2.1410e+09, device='cuda:0')
c= tensor(2.1410e+09, device='cuda:0')
c= tensor(2.1420e+09, device='cuda:0')
c= tensor(2.1448e+09, device='cuda:0')
c= tensor(2.1448e+09, device='cuda:0')
c= tensor(2.1449e+09, device='cuda:0')
c= tensor(2.1450e+09, device='cuda:0')
c= tensor(2.1482e+09, device='cuda:0')
c= tensor(2.1485e+09, device='cuda:0')
c= tensor(2.1555e+09, device='cuda:0')
c= tensor(2.1556e+09, device='cuda:0')
c= tensor(2.1556e+09, device='cuda:0')
c= tensor(2.1556e+09, device='cuda:0')
c= tensor(2.1558e+09, device='cuda:0')
c= tensor(2.1558e+09, device='cuda:0')
c= tensor(2.1568e+09, device='cuda:0')
c= tensor(2.1698e+09, device='cuda:0')
c= tensor(2.2098e+09, device='cuda:0')
c= tensor(2.2098e+09, device='cuda:0')
c= tensor(2.2098e+09, device='cuda:0')
c= tensor(2.2099e+09, device='cuda:0')
c= tensor(2.2296e+09, device='cuda:0')
c= tensor(2.2298e+09, device='cuda:0')
c= tensor(2.2298e+09, device='cuda:0')
c= tensor(2.2298e+09, device='cuda:0')
c= tensor(2.2331e+09, device='cuda:0')
c= tensor(2.2331e+09, device='cuda:0')
c= tensor(2.2346e+09, device='cuda:0')
c= tensor(2.2346e+09, device='cuda:0')
c= tensor(2.2347e+09, device='cuda:0')
c= tensor(2.2347e+09, device='cuda:0')
c= tensor(2.2347e+09, device='cuda:0')
c= tensor(2.2347e+09, device='cuda:0')
c= tensor(2.2387e+09, device='cuda:0')
c= tensor(2.2566e+09, device='cuda:0')
c= tensor(2.2657e+09, device='cuda:0')
c= tensor(2.2678e+09, device='cuda:0')
c= tensor(2.2681e+09, device='cuda:0')
c= tensor(2.2681e+09, device='cuda:0')
c= tensor(2.2681e+09, device='cuda:0')
c= tensor(2.2708e+09, device='cuda:0')
c= tensor(2.2712e+09, device='cuda:0')
c= tensor(2.2741e+09, device='cuda:0')
c= tensor(2.2741e+09, device='cuda:0')
c= tensor(2.3974e+09, device='cuda:0')
c= tensor(2.3975e+09, device='cuda:0')
c= tensor(2.3986e+09, device='cuda:0')
c= tensor(2.4091e+09, device='cuda:0')
c= tensor(2.4093e+09, device='cuda:0')
c= tensor(2.4108e+09, device='cuda:0')
c= tensor(2.4284e+09, device='cuda:0')
c= tensor(2.4290e+09, device='cuda:0')
c= tensor(2.4293e+09, device='cuda:0')
c= tensor(2.4294e+09, device='cuda:0')
c= tensor(2.4309e+09, device='cuda:0')
c= tensor(2.4311e+09, device='cuda:0')
c= tensor(2.4367e+09, device='cuda:0')
c= tensor(2.5790e+09, device='cuda:0')
c= tensor(2.5869e+09, device='cuda:0')
c= tensor(2.5984e+09, device='cuda:0')
c= tensor(2.6000e+09, device='cuda:0')
c= tensor(2.6006e+09, device='cuda:0')
c= tensor(2.6008e+09, device='cuda:0')
c= tensor(2.6008e+09, device='cuda:0')
c= tensor(2.6009e+09, device='cuda:0')
c= tensor(2.6107e+09, device='cuda:0')
c= tensor(2.6111e+09, device='cuda:0')
c= tensor(2.6323e+09, device='cuda:0')
c= tensor(2.6405e+09, device='cuda:0')
c= tensor(2.6408e+09, device='cuda:0')
c= tensor(2.6408e+09, device='cuda:0')
c= tensor(2.6446e+09, device='cuda:0')
c= tensor(2.6459e+09, device='cuda:0')
c= tensor(2.6459e+09, device='cuda:0')
c= tensor(2.6884e+09, device='cuda:0')
c= tensor(2.6898e+09, device='cuda:0')
c= tensor(2.6914e+09, device='cuda:0')
c= tensor(2.6914e+09, device='cuda:0')
c= tensor(2.6914e+09, device='cuda:0')
c= tensor(2.6914e+09, device='cuda:0')
c= tensor(2.6914e+09, device='cuda:0')
c= tensor(2.6915e+09, device='cuda:0')
c= tensor(2.6922e+09, device='cuda:0')
c= tensor(4.4860e+09, device='cuda:0')
c= tensor(4.4860e+09, device='cuda:0')
c= tensor(4.4863e+09, device='cuda:0')
c= tensor(4.4864e+09, device='cuda:0')
c= tensor(4.4864e+09, device='cuda:0')
c= tensor(4.4865e+09, device='cuda:0')
c= tensor(4.5113e+09, device='cuda:0')
c= tensor(4.5114e+09, device='cuda:0')
c= tensor(4.6404e+09, device='cuda:0')
c= tensor(4.6404e+09, device='cuda:0')
c= tensor(4.6511e+09, device='cuda:0')
c= tensor(4.6538e+09, device='cuda:0')
c= tensor(4.6590e+09, device='cuda:0')
c= tensor(4.6733e+09, device='cuda:0')
c= tensor(4.6733e+09, device='cuda:0')
c= tensor(4.6734e+09, device='cuda:0')
c= tensor(4.6744e+09, device='cuda:0')
c= tensor(4.6745e+09, device='cuda:0')
c= tensor(4.6746e+09, device='cuda:0')
c= tensor(4.6754e+09, device='cuda:0')
c= tensor(4.6763e+09, device='cuda:0')
c= tensor(4.6771e+09, device='cuda:0')
c= tensor(4.6776e+09, device='cuda:0')
c= tensor(4.6982e+09, device='cuda:0')
c= tensor(4.7178e+09, device='cuda:0')
c= tensor(4.7183e+09, device='cuda:0')
c= tensor(4.7184e+09, device='cuda:0')
c= tensor(4.7251e+09, device='cuda:0')
c= tensor(4.7258e+09, device='cuda:0')
c= tensor(4.7264e+09, device='cuda:0')
c= tensor(4.7268e+09, device='cuda:0')
c= tensor(4.7279e+09, device='cuda:0')
c= tensor(4.7294e+09, device='cuda:0')
c= tensor(4.7429e+09, device='cuda:0')
c= tensor(4.7446e+09, device='cuda:0')
c= tensor(4.7446e+09, device='cuda:0')
c= tensor(4.7447e+09, device='cuda:0')
c= tensor(4.7447e+09, device='cuda:0')
c= tensor(4.7462e+09, device='cuda:0')
c= tensor(4.7507e+09, device='cuda:0')
c= tensor(4.7624e+09, device='cuda:0')
c= tensor(4.7624e+09, device='cuda:0')
c= tensor(4.7626e+09, device='cuda:0')
c= tensor(4.7634e+09, device='cuda:0')
c= tensor(4.7869e+09, device='cuda:0')
c= tensor(4.7878e+09, device='cuda:0')
c= tensor(4.7889e+09, device='cuda:0')
c= tensor(4.7892e+09, device='cuda:0')
c= tensor(4.7895e+09, device='cuda:0')
c= tensor(4.7895e+09, device='cuda:0')
c= tensor(4.7895e+09, device='cuda:0')
c= tensor(4.7911e+09, device='cuda:0')
c= tensor(4.7918e+09, device='cuda:0')
c= tensor(4.7921e+09, device='cuda:0')
c= tensor(4.7922e+09, device='cuda:0')
c= tensor(4.7922e+09, device='cuda:0')
c= tensor(4.7932e+09, device='cuda:0')
c= tensor(4.7936e+09, device='cuda:0')
c= tensor(4.7938e+09, device='cuda:0')
c= tensor(4.7938e+09, device='cuda:0')
c= tensor(4.7938e+09, device='cuda:0')
c= tensor(4.7944e+09, device='cuda:0')
c= tensor(4.7945e+09, device='cuda:0')
c= tensor(4.7983e+09, device='cuda:0')
c= tensor(4.7983e+09, device='cuda:0')
c= tensor(4.7984e+09, device='cuda:0')
c= tensor(4.7984e+09, device='cuda:0')
c= tensor(4.7984e+09, device='cuda:0')
c= tensor(4.8135e+09, device='cuda:0')
c= tensor(4.8136e+09, device='cuda:0')
c= tensor(4.8149e+09, device='cuda:0')
c= tensor(4.8179e+09, device='cuda:0')
c= tensor(4.8260e+09, device='cuda:0')
c= tensor(4.8336e+09, device='cuda:0')
c= tensor(4.8365e+09, device='cuda:0')
c= tensor(4.8365e+09, device='cuda:0')
c= tensor(4.8369e+09, device='cuda:0')
c= tensor(4.8370e+09, device='cuda:0')
c= tensor(4.8388e+09, device='cuda:0')
c= tensor(4.8425e+09, device='cuda:0')
c= tensor(4.8427e+09, device='cuda:0')
c= tensor(4.8445e+09, device='cuda:0')
c= tensor(4.8445e+09, device='cuda:0')
c= tensor(4.8457e+09, device='cuda:0')
c= tensor(4.8457e+09, device='cuda:0')
c= tensor(4.8458e+09, device='cuda:0')
c= tensor(4.8605e+09, device='cuda:0')
c= tensor(4.9590e+09, device='cuda:0')
c= tensor(4.9591e+09, device='cuda:0')
c= tensor(4.9593e+09, device='cuda:0')
c= tensor(4.9595e+09, device='cuda:0')
c= tensor(4.9637e+09, device='cuda:0')
c= tensor(4.9638e+09, device='cuda:0')
c= tensor(4.9639e+09, device='cuda:0')
c= tensor(4.9650e+09, device='cuda:0')
c= tensor(4.9685e+09, device='cuda:0')
c= tensor(4.9685e+09, device='cuda:0')
c= tensor(4.9711e+09, device='cuda:0')
c= tensor(4.9711e+09, device='cuda:0')
time to make c is 11.285720348358154
time for making loss is 11.285849571228027
p0 True
it  0 : 1867216384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4973203456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
4973379584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1056095200.0
relative error loss 0.21244738
shape of L is 
torch.Size([])
memory (bytes)
5000286208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5000286208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1039557600.0
relative error loss 0.20912062
shape of L is 
torch.Size([])
memory (bytes)
5003784192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5003984896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1015482600.0
relative error loss 0.20427762
shape of L is 
torch.Size([])
memory (bytes)
5007122432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5007179776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1000558600.0
relative error loss 0.20127545
shape of L is 
torch.Size([])
memory (bytes)
5010272256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5010272256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  984762900.0
relative error loss 0.19809794
shape of L is 
torch.Size([])
memory (bytes)
5013520384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5013577728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  974880500.0
relative error loss 0.19610998
shape of L is 
torch.Size([])
memory (bytes)
5016743936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5016768512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  965510400.0
relative error loss 0.19422506
shape of L is 
torch.Size([])
memory (bytes)
5019934720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5019934720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  959567360.0
relative error loss 0.19302954
shape of L is 
torch.Size([])
memory (bytes)
5023047680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5023047680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  954106600.0
relative error loss 0.19193104
shape of L is 
torch.Size([])
memory (bytes)
5026340864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5026398208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  948908000.0
relative error loss 0.19088528
time to take a step is 287.40763688087463
it  1 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5029502976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
5029502976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  948908000.0
relative error loss 0.19088528
shape of L is 
torch.Size([])
memory (bytes)
5032845312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5032845312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  944469000.0
relative error loss 0.1899923
shape of L is 
torch.Size([])
memory (bytes)
5036060672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5036060672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  941491700.0
relative error loss 0.18939339
shape of L is 
torch.Size([])
memory (bytes)
5039214592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5039271936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  938795260.0
relative error loss 0.18885095
shape of L is 
torch.Size([])
memory (bytes)
5042368512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5042479104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  935938300.0
relative error loss 0.18827623
shape of L is 
torch.Size([])
memory (bytes)
5045665792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5045690368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  935064600.0
relative error loss 0.18810047
shape of L is 
torch.Size([])
memory (bytes)
5048643584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5048643584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  932533250.0
relative error loss 0.18759127
shape of L is 
torch.Size([])
memory (bytes)
5052055552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5052108800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  931307800.0
relative error loss 0.18734474
shape of L is 
torch.Size([])
memory (bytes)
5055148032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5055148032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  929948700.0
relative error loss 0.18707134
shape of L is 
torch.Size([])
memory (bytes)
5058478080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5058531328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  927555840.0
relative error loss 0.18659
time to take a step is 282.996052980423
it  2 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5061644288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5061644288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  927555840.0
relative error loss 0.18659
shape of L is 
torch.Size([])
memory (bytes)
5064953856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5064953856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 11% |
error is  925761300.0
relative error loss 0.18622899
shape of L is 
torch.Size([])
memory (bytes)
5068083200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5068083200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  924491500.0
relative error loss 0.18597357
shape of L is 
torch.Size([])
memory (bytes)
5071376384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5071376384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  923178000.0
relative error loss 0.18570933
shape of L is 
torch.Size([])
memory (bytes)
5074493440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5074591744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  920810750.0
relative error loss 0.18523313
shape of L is 
torch.Size([])
memory (bytes)
5077569536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5077794816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  923545340.0
relative error loss 0.18578324
shape of L is 
torch.Size([])
memory (bytes)
5081001984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5081001984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  919914240.0
relative error loss 0.18505278
shape of L is 
torch.Size([])
memory (bytes)
5084147712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5084147712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  918847000.0
relative error loss 0.1848381
shape of L is 
torch.Size([])
memory (bytes)
5087379456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5087432704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  917896700.0
relative error loss 0.18464693
shape of L is 
torch.Size([])
memory (bytes)
5090459648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5090639872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  916856300.0
relative error loss 0.18443765
time to take a step is 283.860497713089
it  3 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5093789696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5093789696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  916856300.0
relative error loss 0.18443765
shape of L is 
torch.Size([])
memory (bytes)
5096927232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5096927232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  916034300.0
relative error loss 0.18427229
shape of L is 
torch.Size([])
memory (bytes)
5100249088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5100249088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  915418900.0
relative error loss 0.18414849
shape of L is 
torch.Size([])
memory (bytes)
5103411200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5103411200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  914916100.0
relative error loss 0.18404734
shape of L is 
torch.Size([])
memory (bytes)
5106630656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5106630656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  914390300.0
relative error loss 0.18394157
shape of L is 
torch.Size([])
memory (bytes)
5109850112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5109850112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  914065900.0
relative error loss 0.18387632
shape of L is 
torch.Size([])
memory (bytes)
5113106432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5113106432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  913591800.0
relative error loss 0.18378095
shape of L is 
torch.Size([])
memory (bytes)
5116256256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5116256256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  913271550.0
relative error loss 0.18371652
shape of L is 
torch.Size([])
memory (bytes)
5119541248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5119541248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  912963100.0
relative error loss 0.18365447
shape of L is 
torch.Size([])
memory (bytes)
5122646016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5122646016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  912511000.0
relative error loss 0.18356353
time to take a step is 284.6011965274811
it  4 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5125914624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5125967872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  912511000.0
relative error loss 0.18356353
shape of L is 
torch.Size([])
memory (bytes)
5129031680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5129175040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  911868700.0
relative error loss 0.18343432
shape of L is 
torch.Size([])
memory (bytes)
5132390400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5132390400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 12% |
error is  911480060.0
relative error loss 0.18335614
shape of L is 
torch.Size([])
memory (bytes)
5135548416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5135605760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  911134700.0
relative error loss 0.18328667
shape of L is 
torch.Size([])
memory (bytes)
5138759680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5138759680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  910471200.0
relative error loss 0.1831532
shape of L is 
torch.Size([])
memory (bytes)
5141893120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5142020096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  909909760.0
relative error loss 0.18304026
shape of L is 
torch.Size([])
memory (bytes)
5145206784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5145206784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  909547260.0
relative error loss 0.18296733
shape of L is 
torch.Size([])
memory (bytes)
5148385280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5148385280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  909224960.0
relative error loss 0.1829025
shape of L is 
torch.Size([])
memory (bytes)
5151580160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5151637504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  908771840.0
relative error loss 0.18281135
shape of L is 
torch.Size([])
memory (bytes)
5154811904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5154856960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  908403200.0
relative error loss 0.18273719
time to take a step is 285.0578033924103
it  5 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5158002688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5158060032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  908403200.0
relative error loss 0.18273719
shape of L is 
torch.Size([])
memory (bytes)
5161267200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5161267200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  907953400.0
relative error loss 0.1826467
shape of L is 
torch.Size([])
memory (bytes)
5164417024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 12% |
memory (bytes)
5164474368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  907499800.0
relative error loss 0.18255545
shape of L is 
torch.Size([])
memory (bytes)
5167693824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5167693824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  907120900.0
relative error loss 0.18247923
shape of L is 
torch.Size([])
memory (bytes)
5170810880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5170810880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  907134200.0
relative error loss 0.18248191
shape of L is 
torch.Size([])
memory (bytes)
5174091776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5174116352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  906680300.0
relative error loss 0.18239062
shape of L is 
torch.Size([])
memory (bytes)
5177262080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5177262080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  906052860.0
relative error loss 0.18226439
shape of L is 
torch.Size([])
memory (bytes)
5180542976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5180542976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  905728260.0
relative error loss 0.18219909
shape of L is 
torch.Size([])
memory (bytes)
5183692800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5183692800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  905372400.0
relative error loss 0.1821275
shape of L is 
torch.Size([])
memory (bytes)
5186908160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5186965504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  904880400.0
relative error loss 0.18202853
time to take a step is 286.34505796432495
it  6 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5190103040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5190176768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  904880400.0
relative error loss 0.18202853
shape of L is 
torch.Size([])
memory (bytes)
5193342976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5193383936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  905000200.0
relative error loss 0.18205263
shape of L is 
torch.Size([])
memory (bytes)
5196578816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5196578816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  904656640.0
relative error loss 0.18198352
shape of L is 
torch.Size([])
memory (bytes)
5199753216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5199806464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  904373000.0
relative error loss 0.18192646
shape of L is 
torch.Size([])
memory (bytes)
5202956288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5202956288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  904144640.0
relative error loss 0.18188053
shape of L is 
torch.Size([])
memory (bytes)
5206237184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5206241280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  903923200.0
relative error loss 0.18183598
shape of L is 
torch.Size([])
memory (bytes)
5209350144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5209350144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  903700000.0
relative error loss 0.18179108
shape of L is 
torch.Size([])
memory (bytes)
5212672000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5212676096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  903546100.0
relative error loss 0.18176013
shape of L is 
torch.Size([])
memory (bytes)
5215698944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5215698944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  903392260.0
relative error loss 0.18172917
shape of L is 
torch.Size([])
memory (bytes)
5219098624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5219098624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  903298800.0
relative error loss 0.18171038
time to take a step is 286.53072142601013
it  7 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5222154240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5222313984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  903298800.0
relative error loss 0.18171038
shape of L is 
torch.Size([])
memory (bytes)
5225426944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5225521152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  903156200.0
relative error loss 0.1816817
shape of L is 
torch.Size([])
memory (bytes)
5228683264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5228683264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  903107100.0
relative error loss 0.18167181
shape of L is 
torch.Size([])
memory (bytes)
5231849472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5231943680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  903047940.0
relative error loss 0.1816599
shape of L is 
torch.Size([])
memory (bytes)
5235093504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5235150848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  902953200.0
relative error loss 0.18164086
shape of L is 
torch.Size([])
memory (bytes)
5238263808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5238366208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  902895100.0
relative error loss 0.18162917
shape of L is 
torch.Size([])
memory (bytes)
5241536512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5241536512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  902777100.0
relative error loss 0.18160543
shape of L is 
torch.Size([])
memory (bytes)
5244772352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5244772352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  902683900.0
relative error loss 0.18158668
shape of L is 
torch.Size([])
memory (bytes)
5248004096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5248004096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  902575600.0
relative error loss 0.1815649
shape of L is 
torch.Size([])
memory (bytes)
5251174400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5251174400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  902421760.0
relative error loss 0.18153395
time to take a step is 286.1887664794922
it  8 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5254434816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5254434816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  902421760.0
relative error loss 0.18153395
shape of L is 
torch.Size([])
memory (bytes)
5257568256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5257568256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  902223100.0
relative error loss 0.18149398
shape of L is 
torch.Size([])
memory (bytes)
5260869632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5260873728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  902004500.0
relative error loss 0.18145001
shape of L is 
torch.Size([])
memory (bytes)
5264027648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5264027648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  901819900.0
relative error loss 0.18141288
shape of L is 
torch.Size([])
memory (bytes)
5267243008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5267300352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  901865200.0
relative error loss 0.181422
shape of L is 
torch.Size([])
memory (bytes)
5270454272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5270454272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  901665540.0
relative error loss 0.18138182
shape of L is 
torch.Size([])
memory (bytes)
5273731072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5273731072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  901447940.0
relative error loss 0.18133806
shape of L is 
torch.Size([])
memory (bytes)
5276893184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5276893184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  901306100.0
relative error loss 0.18130952
shape of L is 
torch.Size([])
memory (bytes)
5280059392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5280157696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  901162240.0
relative error loss 0.18128058
shape of L is 
torch.Size([])
memory (bytes)
5283356672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5283368960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  900987400.0
relative error loss 0.1812454
time to take a step is 285.54966497421265
it  9 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5286424576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5286588416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  900987400.0
relative error loss 0.1812454
shape of L is 
torch.Size([])
memory (bytes)
5289795584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5289795584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  900865540.0
relative error loss 0.18122089
shape of L is 
torch.Size([])
memory (bytes)
5292916736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5293015040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  900726300.0
relative error loss 0.18119287
shape of L is 
torch.Size([])
memory (bytes)
5296209920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5296209920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  900628740.0
relative error loss 0.18117325
shape of L is 
torch.Size([])
memory (bytes)
5299376128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5299376128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  900470800.0
relative error loss 0.18114148
shape of L is 
torch.Size([])
memory (bytes)
5302632448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5302640640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  900518400.0
relative error loss 0.18115106
shape of L is 
torch.Size([])
memory (bytes)
5305786368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5305786368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  900366850.0
relative error loss 0.18112057
shape of L is 
torch.Size([])
memory (bytes)
5309063168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5309063168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  900232960.0
relative error loss 0.18109365
shape of L is 
torch.Size([])
memory (bytes)
5312196608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5312196608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  900073500.0
relative error loss 0.18106155
shape of L is 
torch.Size([])
memory (bytes)
5315473408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5315477504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  899932900.0
relative error loss 0.18103328
time to take a step is 285.73485922813416
it  10 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5318549504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5318549504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  899932900.0
relative error loss 0.18103328
shape of L is 
torch.Size([])
memory (bytes)
5321900032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5321900032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  899759360.0
relative error loss 0.18099837
shape of L is 
torch.Size([])
memory (bytes)
5325074432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5325074432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  899555300.0
relative error loss 0.18095733
shape of L is 
torch.Size([])
memory (bytes)
5328240640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5328240640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  899490560.0
relative error loss 0.1809443
shape of L is 
torch.Size([])
memory (bytes)
5331492864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5331546112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  899242500.0
relative error loss 0.18089439
shape of L is 
torch.Size([])
memory (bytes)
5334671360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5334753280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  899154700.0
relative error loss 0.18087673
shape of L is 
torch.Size([])
memory (bytes)
5337911296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5337968640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  899002600.0
relative error loss 0.18084614
shape of L is 
torch.Size([])
memory (bytes)
5341052928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5341179904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  898964200.0
relative error loss 0.18083842
shape of L is 
torch.Size([])
memory (bytes)
5344387072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5344387072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  898740200.0
relative error loss 0.18079336
shape of L is 
torch.Size([])
memory (bytes)
5347500032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5347598336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  898630900.0
relative error loss 0.18077137
time to take a step is 286.379985332489
it  11 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5350797312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5350809600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  898630900.0
relative error loss 0.18077137
shape of L is 
torch.Size([])
memory (bytes)
5353967616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5353967616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  898530800.0
relative error loss 0.18075123
shape of L is 
torch.Size([])
memory (bytes)
5357232128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5357236224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  898356740.0
relative error loss 0.18071622
shape of L is 
torch.Size([])
memory (bytes)
5360418816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5360418816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  898608640.0
relative error loss 0.1807669
shape of L is 
torch.Size([])
memory (bytes)
5363666944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5363666944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  898255360.0
relative error loss 0.18069582
shape of L is 
torch.Size([])
memory (bytes)
5366771712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5366771712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  898093060.0
relative error loss 0.18066317
shape of L is 
torch.Size([])
memory (bytes)
5370077184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5370077184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897985300.0
relative error loss 0.18064149
shape of L is 
torch.Size([])
memory (bytes)
5373202432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5373202432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897830140.0
relative error loss 0.18061028
shape of L is 
torch.Size([])
memory (bytes)
5376503808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5376503808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897715700.0
relative error loss 0.18058726
shape of L is 
torch.Size([])
memory (bytes)
5379620864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5379620864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897647600.0
relative error loss 0.18057357
time to take a step is 287.1067359447479
it  12 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5382926336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5382926336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897647600.0
relative error loss 0.18057357
shape of L is 
torch.Size([])
memory (bytes)
5386051584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5386051584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897563900.0
relative error loss 0.18055673
shape of L is 
torch.Size([])
memory (bytes)
5389348864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5389348864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897460740.0
relative error loss 0.18053597
shape of L is 
torch.Size([])
memory (bytes)
5392506880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5392506880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897427460.0
relative error loss 0.18052928
shape of L is 
torch.Size([])
memory (bytes)
5395771392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5395771392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897287200.0
relative error loss 0.18050106
shape of L is 
torch.Size([])
memory (bytes)
5398908928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5398908928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897240600.0
relative error loss 0.18049169
shape of L is 
torch.Size([])
memory (bytes)
5402198016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5402198016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897176800.0
relative error loss 0.18047886
shape of L is 
torch.Size([])
memory (bytes)
5405413376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5405413376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  897089300.0
relative error loss 0.18046124
shape of L is 
torch.Size([])
memory (bytes)
5408563200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5408616448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896992500.0
relative error loss 0.18044178
shape of L is 
torch.Size([])
memory (bytes)
5411811328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5411835904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896900860.0
relative error loss 0.18042335
time to take a step is 287.54883551597595
it  13 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5415010304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5415010304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896900860.0
relative error loss 0.18042335
shape of L is 
torch.Size([])
memory (bytes)
5418156032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5418254336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896817660.0
relative error loss 0.18040662
shape of L is 
torch.Size([])
memory (bytes)
5421473792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5421473792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896730400.0
relative error loss 0.18038905
shape of L is 
torch.Size([])
memory (bytes)
5424570368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5424689152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896664600.0
relative error loss 0.18037581
shape of L is 
torch.Size([])
memory (bytes)
5427838976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5427892224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896603650.0
relative error loss 0.18036355
shape of L is 
torch.Size([])
memory (bytes)
5431050240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5431107584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896548350.0
relative error loss 0.18035243
shape of L is 
torch.Size([])
memory (bytes)
5434310656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5434310656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896490500.0
relative error loss 0.1803408
shape of L is 
torch.Size([])
memory (bytes)
5437440000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5437521920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896461060.0
relative error loss 0.18033487
shape of L is 
torch.Size([])
memory (bytes)
5440675840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5440729088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896421400.0
relative error loss 0.1803269
shape of L is 
torch.Size([])
memory (bytes)
5443858432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5443944448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896391200.0
relative error loss 0.18032081
time to take a step is 287.07193636894226
it  14 : 2421396480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5447159808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5447159808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896391200.0
relative error loss 0.18032081
shape of L is 
torch.Size([])
memory (bytes)
5450264576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5450362880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896359700.0
relative error loss 0.18031448
shape of L is 
torch.Size([])
memory (bytes)
5453524992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5453578240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896339460.0
relative error loss 0.18031041
shape of L is 
torch.Size([])
memory (bytes)
5456728064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5456728064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896319740.0
relative error loss 0.18030645
shape of L is 
torch.Size([])
memory (bytes)
5460004864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5460004864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896283400.0
relative error loss 0.18029913
shape of L is 
torch.Size([])
memory (bytes)
5463146496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5463146496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896267260.0
relative error loss 0.18029588
shape of L is 
torch.Size([])
memory (bytes)
5466374144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5466431488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896240900.0
relative error loss 0.18029058
shape of L is 
torch.Size([])
memory (bytes)
5469519872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5469519872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896216800.0
relative error loss 0.18028575
shape of L is 
torch.Size([])
memory (bytes)
5472792576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5472845824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896175600.0
relative error loss 0.18027745
shape of L is 
torch.Size([])
memory (bytes)
5476061184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  3% | 12% |
memory (bytes)
5476061184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  896150800.0
relative error loss 0.18027246
time to take a step is 287.44672107696533
sum tnnu_Z after tensor(11560329., device='cuda:0')
shape of features
(5459,)
shape of features
(5459,)
number of orig particles 21835
number of new particles after remove low mass 20725
tnuZ shape should be parts x labs
torch.Size([21835, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1056047100.0
relative error without small mass is  0.21243769
nnu_Z shape should be number of particles by maxV
(21835, 702)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
shape of features
(21835,)
Thu Feb 2 07:27:59 EST 2023
