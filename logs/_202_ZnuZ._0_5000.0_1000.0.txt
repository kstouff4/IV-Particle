Tue Jan 31 05:05:57 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 18795893
numbers of Z: 9631
shape of features
(9631,)
shape of features
(9631,)
ZX	Vol	Parts	Cubes	Eps
Z	0.009471602477144564	9631	9.631	0.09944524602372098
X	0.0077662782018876894	997	0.997	0.19823146707966027
X	0.009409037772977797	11051	11.051	0.09477952860696714
X	0.008875693917193587	1632	1.632	0.17585742951581557
X	0.00797016721158684	9368	9.368	0.09475602858116436
X	0.009337434697026997	5281	5.281	0.120921577263166
X	0.00825412717165968	25019	25.019	0.06909825335643237
X	0.00832595226821784	25883	25.883	0.06851827722216734
X	0.008210869929554374	18015	18.015	0.07695762126200374
X	0.00824604377833935	6748	6.748	0.106911277921199
X	0.008094100122837115	6454	6.454	0.10783999781540375
X	0.008041606502478128	4360	4.36	0.12263622576647473
X	0.008117539779258	27968	27.968	0.06620960620827747
X	0.007957511487554145	3638	3.638	0.129809016603846
X	0.0084301519226632	221867	221.867	0.03361869853112305
X	0.00823212824578671	9761	9.761	0.0944798666399607
X	0.008236746175501195	27122	27.122	0.06721682434966572
X	0.00826793923702188	34575	34.575	0.062069516512881145
X	0.008210214669111092	12056	12.056	0.08797995950791324
X	0.00823502995750417	55358	55.358	0.052986056390924074
X	0.00819423640156055	49761	49.761	0.05481168206021961
X	0.009122821649586686	42197	42.197	0.06001814055111843
X	0.008765117457276504	74884	74.884	0.04891659683487232
X	0.008242583879616933	5113	5.113	0.1172544122895054
X	0.00818302153410833	30033	30.033	0.06482934881631154
X	0.007481349417110777	980	0.98	0.19690252607382153
X	0.008328260399613655	23766	23.766	0.07050166944638844
X	0.008364019339827574	22064	22.064	0.07237305852004956
X	0.007843369228621501	2516	2.516	0.146082213575855
X	0.008010287287058946	10006	10.006	0.0928529838752048
X	0.0087481200256692	456391	456.391	0.02676245112318649
X	0.00788490378367484	4708	4.708	0.11875538573988112
X	0.009406135684250859	371552	371.552	0.02936278275509684
X	0.00843952152922859	10115	10.115	0.09414212117309569
X	0.008171303674126202	4676	4.676	0.12044967043923475
X	0.008053075543716865	13108	13.108	0.08501099776329503
X	0.008248623086980348	66641	66.641	0.04983639866925594
X	0.009031665320500199	37479	37.479	0.06222906567341928
X	0.008015267057013636	671	0.671	0.22859417466433926
X	0.007840239405309287	2304	2.304	0.1504119297434416
X	0.007766543972777607	1360	1.36	0.17874324264939853
X	0.009017752114116725	1205	1.205	0.19560070480938904
X	0.007599163744030867	2723	2.723	0.1407902954907061
X	0.007921345617574802	481	0.481	0.25442002758423093
X	0.007490656372071475	2001	2.001	0.15527120687665188
X	0.00779561014402001	398	0.398	0.26956014418249213
X	0.007618465784438256	385	0.385	0.27048027374948846
X	0.007661056767585624	1248	1.248	0.18310127355205288
X	0.008081762686733603	3388	3.388	0.13361469684867727
X	0.008168733961711	915	0.915	0.2074489396809135
X	0.008233489075104804	7874	7.874	0.10149925028278826
X	0.009148454709787644	10466	10.466	0.09561418715860753
X	0.008788376264607467	921	0.921	0.21210439654711305
X	0.00816892558087254	6263	6.263	0.10925991105850441
X	0.007905640346570894	1266	1.266	0.1841483290432834
X	0.008147354858416622	2444	2.444	0.1493843644091565
X	0.008374605759707438	5388	5.388	0.11583654480069659
X	0.009140643138747536	1025	1.025	0.2073724042426985
X	0.007556577758540727	1508	1.508	0.1711228237495073
X	0.00791964035638776	1577	1.577	0.1712476375543114
X	0.007813199680985424	1161	1.161	0.18879865728011822
X	0.008298227727571439	2750	2.75	0.14450544721475217
X	0.008008496793679144	944	0.944	0.2039512339536903
X	0.008174730320924957	2644	2.644	0.14568136248779334
X	0.00809516283978716	5757	5.757	0.1120322659260388
X	0.007932436407911761	1297	1.297	0.18287550610120692
X	0.007834031675019199	1348	1.348	0.17978983906227472
X	0.00783584067620562	1672	1.672	0.16734642588392548
X	0.007983694025467274	3179	3.179	0.13592659947290942
X	0.008103190456169319	3774	3.774	0.1290084181927642
X	0.007711358110865246	2035	2.035	0.15590336395349633
X	0.00790475798599497	1438	1.438	0.17648585258770047
X	0.00805911011682975	1387	1.387	0.17977813726579084
X	0.00826182005748438	1610	1.61	0.17248445147927643
X	0.007953666133342158	1544	1.544	0.17270570622440462
X	0.007921740133966019	1568	1.568	0.17158981701199397
X	0.007679851462361624	3283	3.283	0.13274779394400077
X	0.0073262428196015335	314	0.314	0.28574845235642543
X	0.0081488341360104	779	0.779	0.21870195914169166
X	0.00816711226895978	1839	1.839	0.1643724135515686
X	0.007651657912898856	5910	5.91	0.10899067816843973
X	0.00814500834097888	1036	1.036	0.1988431308639978
X	0.007489346266908373	884	0.884	0.20385930932493787
X	0.00906220313932584	2101	2.101	0.16278057575437127
X	0.0079036254906134	731	0.731	0.22112422582941305
X	0.007474266566849744	947	0.947	0.1991007371270075
X	0.008067248861601636	796	0.796	0.2164067463752352
X	0.00782932413253876	1139	1.139	0.1901370929907939
X	0.007530422662609325	512	0.512	0.2450096165205443
X	0.00917798564793954	2001	2.001	0.16614990419718012
X	0.008069652324037476	1851	1.851	0.1633614081667855
X	0.007665213222833969	1042	1.042	0.19448472943949893
X	0.008060840898587236	798	0.798	0.21616853832917557
X	0.008234101225871643	763	0.763	0.22098555576660633
X	0.007728346949440096	1524	1.524	0.1718042413972507
X	0.007630269299060719	1717	1.717	0.16440809238483198
X	0.008606971154072477	2643	2.643	0.1482237266427294
X	0.008557713823676165	1582	1.582	0.1755430985971622
X	0.007983216880843065	3159	3.159	0.13621013833940643
X	0.00764920691124026	1594	1.594	0.16867188453330106
X	0.00805133888144145	1668	1.668	0.1690015495982526
X	0.007790864630385558	4573	4.573	0.11943404804168646
X	0.007995492489287161	1193	1.193	0.1885391704882882
X	0.00761158014861128	2205	2.205	0.15113164529666065
X	0.007863152652929969	1065	1.065	0.19472230406974309
X	0.007808854093257312	2714	2.714	0.1422304186251073
X	0.007796260409642114	1417	1.417	0.17653825476644652
X	0.008697625842484354	1370	1.37	0.18516549108672667
X	0.007812370649184615	714	0.714	0.22200437988202454
X	0.007653861505020249	1178	1.178	0.18660005597236934
X	0.007560740947673384	711	0.711	0.21990321853869682
X	0.007948543177395809	955	0.955	0.20265689703928458
X	0.007611314710509961	845	0.845	0.2080659387081234
X	0.00791032091496075	1087	1.087	0.19378560786135968
X	0.0077717687738648586	1255	1.255	0.18363637664315113
X	0.00784050704542347	2464	2.464	0.14708480711507965
X	0.00778806618328206	2038	2.038	0.1563418156584213
X	0.007991646687729422	1461	1.461	0.17619571937968384
X	0.007950652528115327	290	0.29	0.3015330532235862
X	0.008440907469037136	4995	4.995	0.11911082031503956
X	0.007265514029801759	554	0.554	0.23582265633443733
X	0.008022269322745981	4828	4.828	0.11844316179131252
X	0.007523025005958338	1111	1.111	0.18918766430739134
X	0.007664316732890359	969	0.969	0.19924306882540038
X	0.008053948229917632	881	0.881	0.20909531804154863
X	0.00779609610048446	799	0.799	0.21368636015454895
X	0.0078980825527402	939	0.939	0.20336928363553075
X	0.007182306437703239	678	0.678	0.21962313113380677
X	0.00791598250873479	1300	1.3	0.18260829066297776
X	0.008196997719322231	4284	4.284	0.12414668213367629
X	0.00794707335460335	1173	1.173	0.18922117568155453
X	0.008245705551331348	4302	4.302	0.12421836408768573
X	0.008106862361478156	1031	1.031	0.1988526477777471
X	0.008112856376122766	1834	1.834	0.16415652578233592
X	0.00768146487721491	1277	1.277	0.18186580556170182
X	0.008091653191379736	1620	1.62	0.17093883850569366
X	0.0077185386378627084	795	0.795	0.21333199003015818
X	0.007434057931690513	1195	1.195	0.18391592902874915
X	0.008173020728146756	1114	1.114	0.19431178283965503
X	0.0076241869898119215	781	0.781	0.21372114969984127
X	0.0075612656815557365	806	0.806	0.21090481042792417
X	0.00810552834221343	1511	1.511	0.17505410046178788
X	0.007833444525392565	348	0.348	0.2823522000729597
X	0.008922897250039053	4703	4.703	0.12379705243634222
X	0.009332405618481115	2697	2.697	0.15125307754635484
X	0.00793643691646677	2614	2.614	0.14480158925498332
X	0.007612720532953701	621	0.621	0.2305770590524071
X	0.007808178398729679	1283	1.283	0.1825748799180271
X	0.007879718367833742	918	0.918	0.20474941586511222
X	0.007651236960998561	768	0.768	0.21517417381620416
X	0.008062520419736612	1037	1.037	0.19810587589179057
X	0.007616235013147839	1784	1.784	0.16222402108952896
X	0.009097518310309714	2374	2.374	0.15648789232490357
X	0.0074730007474669525	1552	1.552	0.16886297497071018
X	0.008195555774390531	4308	4.308	0.12390844454269775
X	0.0076681446161244605	1824	1.824	0.16139440938928562
X	0.00806425697520408	9672	9.672	0.09412016724700764
X	0.008091325178468849	1530	1.53	0.1742245727282291
X	0.008227559624198712	2530	2.53	0.14815517950911816
X	0.007455019233438936	1235	1.235	0.18207931193445176
X	0.007280275769760917	329	0.329	0.28074841351226687
X	0.007865931215735715	3860	3.86	0.12678103120682488
X	0.007345316644934359	748	0.748	0.21414255240328833
X	0.007878431439605725	1671	1.671	0.1676825114807135
X	0.0071807212892804366	498	0.498	0.24339570438433134
X	0.007499630767839406	2328	2.328	0.14769041201412764
X	0.00821853137502151	962	0.962	0.20442768714360363
X	0.008281156715840352	928	0.928	0.20741858908529792
X	0.007909234762277168	2029	2.029	0.15738045859679647
X	0.00826196965311335	2066	2.066	0.1587271040724005
X	0.0073352229298791136	433	0.433	0.25682766717499816
X	0.007401792055257852	265	0.265	0.30341031079318026
X	0.00750809740257888	481	0.481	0.2499165060344637
X	0.009137728911842691	3451	3.451	0.13834530871512682
X	0.008034611560914928	773	0.773	0.21823704061519272
X	0.008136517246114175	2063	2.063	0.15799612675695818
X	0.008314170174743521	1262	1.262	0.1874648512130902
X	0.007952022616532058	3976	3.976	0.12599222443540364
X	0.00780412496295822	1301	1.301	0.18169750876112337
X	0.007907066231821051	3054	3.054	0.13731419903024286
X	0.007836742566503518	2110	2.11	0.15486420299135112
X	0.008130363258492073	2022	2.022	0.15901677191345334
X	0.008122134366621948	1701	1.701	0.16839219269623798
X	0.007479000970154728	3019	3.019	0.13530904396333823
X	0.007563157283301681	868	0.868	0.2057758806355164
X	0.008382398797232809	1808	1.808	0.1667464631472644
X	0.00911214770498956	1851	1.851	0.17011323781744161
X	0.00826811880203794	1111	1.111	0.1952379398945486
X	0.008040803126765682	2783	2.783	0.14242801036990976
X	0.00812989797771525	2823	2.823	0.14227373707025914
X	0.0079462219716785	1852	1.852	0.16249496303601005
X	0.007775784800263959	1122	1.122	0.19065599183377957
X	0.007874326655680261	399	0.399	0.2702382648750163
X	0.007589400910667729	1174	1.174	0.18628582066457314
X	0.008111972832313555	2876	2.876	0.1412903611632172
X	0.008125713869591716	3999	3.999	0.12665919022568337
X	0.007579189878461409	1608	1.608	0.16766622294651906
X	0.007524346668113729	1076	1.076	0.1912283047760425
X	0.008018745191480605	772	0.772	0.21818741869229985
X	0.0077366995758024115	1461	1.461	0.17430178675178765
X	0.00803322084905916	724	0.724	0.22304049153482078
X	0.00934647608399015	3054	3.054	0.14518651768517257
X	0.008084671875062238	977	0.977	0.20226587218380307
X	0.00940593657686199	3718	3.718	0.13625869342230415
X	0.007642036664731871	1489	1.489	0.17249299903718066
X	0.008142480694497021	2230	2.23	0.15398696457808714
X	0.00812609649565519	892	0.892	0.20885219321210582
X	0.008040558277114728	2690	2.69	0.14404935168000313
X	0.008530366510652185	1552	1.552	0.17647857625448
X	0.007676234769749699	776	0.776	0.21466546217821783
X	0.009041160267536853	3205	3.205	0.14129670835438649
X	0.007494659432117221	2383	2.383	0.14651292996002283
X	0.007871057309192481	4402	4.402	0.1213745628572116
X	0.007482782611111144	1990	1.99	0.1555022523826036
X	0.007690643670749655	2274	2.274	0.15010347380652528
X	0.007270311654646859	349	0.349	0.27515402303818576
X	0.007332636270080724	830	0.83	0.2067254836973372
X	0.007274665985128959	330	0.33	0.2803924869051964
X	0.0078130074536325	694	0.694	0.224122911738621
X	0.008104571879338975	2812	2.812	0.14231092909403417
X	0.008194093125539098	2350	2.35	0.15163903466348375
X	0.007927069367625125	1884	1.884	0.16143979102225273
X	0.007924150613287238	629	0.629	0.23268444880684458
X	0.00791189605087911	2249	2.249	0.15208873200096862
X	0.009036230670676734	2607	2.607	0.151338267946668
X	0.007836181967614106	1886	1.886	0.16076355748805052
X	0.008062083886049471	1425	1.425	0.17818760180343965
X	0.007796847534204822	738	0.738	0.21942573409706803
X	0.0075227474144149405	686	0.686	0.22216933825679275
X	0.0077097657141857	1690	1.69	0.16585102455251302
X	0.00766251821206025	411	0.411	0.2651611533042572
X	0.008023266850487416	1465	1.465	0.17626707753252382
X	0.008076146905594859	1476	1.476	0.1762135433673253
X	0.008639023500176365	1178	1.178	0.19428523111887133
X	0.008422905856955381	3011	3.011	0.14090196951958445
X	0.008062789598054192	1378	1.378	0.18019609273365025
X	0.007821129049360505	1945	1.945	0.15901934077457128
X	0.009375107502614338	4085	4.085	0.13190504259903948
X	0.008158364314663616	3275	3.275	0.13555969932414363
X	0.007672307721559027	807	0.807	0.21184464618935142
X	0.008086608726043477	12577	12.577	0.08631039875511079
X	0.008236072968902145	41561	41.561	0.05830127676905809
X	0.008122931202338068	1724	1.724	0.16764547583186154
X	0.007890319408736806	3046	3.046	0.1373372125152975
X	0.0071221394913466	294	0.294	0.2893483754192361
X	0.007967327200930645	2491	2.491	0.14733739097565346
X	0.008217238648855988	3679	3.679	0.13071698956793007
X	0.008713508986711499	146551	146.551	0.03903025892739307
X	0.008159358314595718	1323	1.323	0.18338566609754536
X	0.008466934306966933	96495	96.495	0.04443656293644923
X	0.00827103132294109	6597	6.597	0.10782953370183637
X	0.008139330909110242	12408	12.408	0.08688850140008642
X	0.008551465379329175	9886	9.886	0.09528107430701029
X	0.00762337174069877	1010	1.01	0.19615915169838213
X	0.007939342699564193	2311	2.311	0.1508903737800133
X	0.00872418723305257	71051	71.051	0.04970326605271484
X	0.008491095308311198	151453	151.453	0.03827324438030359
X	0.008245511898974883	2063	2.063	0.1586984913706301
X	0.008301877558105249	16381	16.381	0.07972820218422058
X	0.008993393198247393	6050	6.05	0.11412726303952672
X	0.008090547406800357	9309	9.309	0.09543147262180746
X	0.008926530172275573	68113	68.113	0.05079460133699357
X	0.007698469437425478	15747	15.747	0.07877749875209254
X	0.008230998087998691	10358	10.358	0.0926244263693341
X	0.008172996909261597	9061	9.061	0.09662030440871386
X	0.0075899246458761005	1569	1.569	0.1691238436410363
X	0.008420499996357434	101717	101.717	0.043582764955472725
X	0.00812172872331	1631	1.631	0.17076472881569932
X	0.007987376443844951	1480	1.48	0.17540722553367197
X	0.008236834417117306	13246	13.246	0.08535425871870292
X	0.0085815141496808	40292	40.292	0.05971934455913374
X	0.009321815988253978	79677	79.677	0.048909045132664954
X	0.00917081884703147	34559	34.559	0.06426122238990402
X	0.0075725702842986	1654	1.654	0.16604887436687019
X	0.008304654918017925	16356	16.356	0.0797776972399239
X	0.007582119823614199	2417	2.417	0.14638772596402955
X	0.008206765695739518	21824	21.824	0.07217928247615074
X	0.00834201582390462	19062	19.062	0.07592197807613453
X	0.008255548769724279	3112	3.112	0.1384316683667924
X	0.007968789602742391	25209	25.209	0.06812067051002393
X	0.007310284452873451	403	0.403	0.26275021296552425
X	0.008502183716711997	5227	5.227	0.11760505652413386
X	0.008172876363694084	63552	63.552	0.05047564790052455
X	0.008312298367582153	19762	19.762	0.07492557357154267
X	0.00795026500924108	60873	60.873	0.05073632032323922
X	0.007716346486982797	1184	1.184	0.18678987840127323
X	0.008576871654577906	101837	101.837	0.043833663710261234
X	0.008274647603228135	5652	5.652	0.11354877450427181
X	0.008340512180088858	12849	12.849	0.08658469482928821
X	0.009021094390924439	34368	34.368	0.0640277620747422
X	0.007988012388453238	886	0.886	0.20813015258475134
X	0.00788481452741871	49609	49.609	0.05416810838061305
X	0.008747389233553022	93394	93.394	0.04541365870743541
X	0.008594836344280097	163816	163.816	0.037436372916595816
X	0.009259742554920345	33166	33.166	0.06535847926357118
X	0.008048215589885482	14755	14.755	0.08170590834852204
X	0.007527946715926064	7227	7.227	0.10136923302465202
X	0.0080885347139851	1969	1.969	0.16015532356472922
X	0.008380018838655672	39361	39.361	0.05971172585219522
X	0.008211884133449998	3541	3.541	0.13236473527747977
X	0.008217959507618248	11273	11.273	0.08999979012266891
X	0.008387700358042219	15145	15.145	0.08212168968402407
X	0.008245730728602919	5360	5.36	0.11543959282253553
X	0.0076383799333450565	6475	6.475	0.10566240573841199
X	0.00904273249464953	2599	2.599	0.15152971323977477
X	0.00817037368443924	49951	49.951	0.0546889070751946
X	0.008100629693531123	5361	5.361	0.11475131140475883
X	0.008747528135169042	9389	9.389	0.09766868406615499
X	0.007988550772722659	4227	4.227	0.12363608676442919
X	0.007964478656855064	28549	28.549	0.06534145747267324
X	0.00821378322520084	10990	10.99	0.09075039308164766
X	0.008334091846293341	79410	79.41	0.04716947350368964
X	0.007801652652203476	3747	3.747	0.1276931613704074
X	0.008658714491427943	21465	21.465	0.07388804497881231
X	0.008948231872695929	3713	3.713	0.1340717958607087
X	0.00825835378014222	70551	70.551	0.04891740796761788
X	0.008117223153141801	14167	14.167	0.08305705547639554
X	0.008249217104377333	5853	5.853	0.11211870420562636
X	0.0083360450806119	50710	50.71	0.05478001514834881
X	0.00870200407742626	58220	58.22	0.05306999260536354
X	0.00795215700803362	1359	1.359	0.18020015622763746
X	0.008636581069888861	82847	82.847	0.047063948026902634
X	0.009398100048075878	61474	61.474	0.05347074031846407
X	0.00925582181824369	143667	143.667	0.04008845517073851
X	0.00838960201031296	8642	8.642	0.099016833186808
X	0.00764533822564734	1420	1.42	0.1752680626881323
X	0.008248290088372558	4840	4.84	0.11944634220911825
X	0.008282304615888554	4505	4.505	0.12250452612667803
X	0.00817897512017508	8019	8.019	0.10066060999902204
X	0.008850873428703852	16020	16.02	0.08205553337532313
X	0.007725696177259587	565	0.565	0.23912761136093177
X	0.008281341741572598	38956	38.956	0.05968182797728931
X	0.008297749493945879	59199	59.199	0.05194558596488951
X	0.007395533276918721	887	0.887	0.20277541173532132
X	0.007831182105253753	2898	2.898	0.13928669941724967
X	0.008262169148277399	15778	15.778	0.08060228924878626
X	0.009157007400097214	5083	5.083	0.12167774480001559
X	0.008414306977393897	2445	2.445	0.1509778244203859
X	0.009211018111821222	4584	4.584	0.12618885772852512
X	0.007881893753251	1765	1.765	0.16467536025505922
X	0.008058486617099778	4946	4.946	0.1176701911628471
X	0.008370405131628334	18030	18.03	0.07743136535345313
X	0.008723433180442679	10252	10.252	0.09476026802525928
X	0.008387429235062851	17542	17.542	0.0781957688451827
X	0.00768408163402512	3714	3.714	0.12742364484668026
X	0.00873745210690523	273167	273.167	0.03174330863488534
X	0.008210311999922821	5137	5.137	0.11691853503739132
X	0.008656981012569199	50310	50.31	0.055620819399338835
X	0.007730239728778351	934	0.934	0.20227802069848375
X	0.007984789636485554	2067	2.067	0.15690652375163264
X	0.008158003383252862	1696	1.696	0.16880527394131262
X	0.008304475909602725	4931	4.931	0.11897592108298058
X	0.008673319841230005	1509	1.509	0.17912878247711375
X	0.008015720016676971	62126	62.126	0.05053082423286994
X	0.00789600935862905	2327	2.327	0.15026935548826306
X	0.007860969617874932	1977	1.977	0.1584247934156567
X	0.008918781888950194	88734	88.734	0.04649488322784859
X	0.00839120400835656	25665	25.665	0.06889076441887738
X	0.00944300731303453	17538	17.538	0.0813535958127138
X	0.008722693941572242	46287	46.287	0.05733201425876367
X	0.009031314758594432	116571	116.571	0.04263047308180675
X	0.007892104100059081	2894	2.894	0.13971126537882436
X	0.00827857055612748	2837	2.837	0.1428999267720916
X	0.008279540348184894	8997	8.997	0.09726789160602298
X	0.007544875263838569	939	0.939	0.20029133317389725
X	0.007827524546647648	2900	2.9	0.13923298908538345
X	0.008207121313534686	9018	9.018	0.09690812868529868
X	0.00781692726017952	1604	1.604	0.16954197406952334
X	0.007587393970458271	2192	2.192	0.15126926443652436
X	0.008228258770944635	4512	4.512	0.12217423175126951
X	0.00822712756371218	2115	2.115	0.15727002403716436
X	0.008873590577473714	75808	75.808	0.04891718411717188
X	0.007765962102663596	6278	6.278	0.107347347699225
X	0.00943878340944905	37601	37.601	0.06308201273886188
X	0.00801241755874285	911	0.911	0.20641839601181175
X	0.0081203781719049	2337	2.337	0.151462753433246
X	0.008150391197501489	72250	72.25	0.0483185100900461
X	0.008584686442348582	83341	83.341	0.04687650705416453
X	0.00834840994066879	156279	156.279	0.037661797822655246
X	0.008745561368567183	5730	5.73	0.1151359697481406
X	0.008520829542902574	19520	19.52	0.07585787768155071
X	0.00816733353279208	1748	1.748	0.1671781908181054
X	0.008110208099744526	1342	1.342	0.18214887531812055
X	0.00823208196959437	27607	27.607	0.06680825940645406
X	0.00836876118728122	7707	7.707	0.10278394250143101
X	0.007673185711852475	2901	2.901	0.1382959069801717
X	0.008174678462604721	21082	21.082	0.07292105727275688
X	0.009450097094403405	360598	360.598	0.029703273834106272
X	0.007781776123860493	15245	15.245	0.07991907240574561
X	0.00809352313419146	7150	7.15	0.10421826932210192
X	0.009369717926037593	8617	8.617	0.10283086157627395
X	0.00814506741771723	2842	2.842	0.1420442102830602
X	0.0077807882355178555	1649	1.649	0.16772622790342212
X	0.009286475713107839	65142	65.142	0.052239211494235445
X	0.007792214043632372	10053	10.053	0.09185900361456949
X	0.0072994031310621025	1170	1.17	0.18409153934999206
X	0.0083778455601186	34100	34.1	0.06263147751002793
X	0.009306367933981386	4761	4.761	0.12503377563617085
X	0.0074336535714527195	401	0.401	0.26465858229397077
X	0.008208782714320849	32873	32.873	0.06297185306918936
X	0.00919626172353196	38499	38.499	0.062046976649393897
X	0.009348126998151882	111739	111.739	0.04373608864891334
X	0.008469882413411872	76469	76.469	0.048024625853201774
X	0.008921304013775047	67494	67.494	0.050939466322480774
X	0.007743464624625498	14407	14.407	0.0813056240431906
X	0.00814803657312512	5630	5.63	0.11311360876194404
X	0.008915133188122969	58332	58.332	0.053465502902510655
X	0.008376611612659757	43436	43.436	0.05777494202646096
X	0.008279057192793338	10587	10.587	0.09213033632270912
X	0.008358089786578614	21160	21.16	0.07337201670869559
X	0.008356241364022228	48682	48.682	0.0555751770047317
X	0.0087722913438144	81763	81.763	0.04751733510865979
X	0.008586947199615435	58317	58.317	0.05280574887029382
X	0.008255748950996438	28149	28.149	0.06644023639311115
X	0.00778866045935328	9096	9.096	0.09495928588040785
X	0.00797989405654182	3184	3.184	0.13583385372064685
X	0.00818958922545647	2371	2.371	0.15116231056992716
X	0.008526629270721479	45431	45.431	0.05725433324387464
X	0.00927954791124452	53576	53.576	0.05574238041537258
X	0.008349679726167623	155719	155.719	0.03770880216331437
X	0.008227150881529423	52440	52.44	0.05393395391213693
X	0.008596944735878721	28029	28.029	0.06743916386772363
X	0.008086946841023094	6244	6.244	0.10900353054280415
X	0.00835605783008386	49297	49.297	0.05534269645321036
X	0.007978807848834417	3457	3.457	0.132153765526885
X	0.007662469199783356	4294	4.294	0.12129295236372327
X	0.00889593660277697	51122	51.122	0.055829184107444474
X	0.008325322157521513	20118	20.118	0.07451986876468039
X	0.007917557898546647	1746	1.746	0.16551943791543686
X	0.008285689637440564	8717	8.717	0.09832271717158962
X	0.009356679468855376	289824	289.824	0.031841667427941374
X	0.007862203849592707	8891	8.891	0.09598379047425001
X	0.008628445388414487	70679	70.679	0.0496075051497677
X	0.00804220468049559	1757	1.757	0.16603556340394068
X	0.007795445091615074	2085	2.085	0.15520711246037702
X	0.007807943554767701	2842	2.842	0.140056799270519
X	0.00800966545916394	1391	1.391	0.17923758446329024
X	0.008891533735135867	9677	9.677	0.09721769349894074
X	0.008228610087549378	23487	23.487	0.0704962981114308
X	0.008944255329483662	1301	1.301	0.19014677501288751
X	0.008208250689329703	44222	44.222	0.05704322583384505
X	0.007860028607675323	2447	2.447	0.14754688748646902
X	0.009167691571776559	15669	15.669	0.08363850045266041
X	0.00806418750282782	4711	4.711	0.11962331616335185
X	0.008293868325813898	57108	57.108	0.05256379688529968
X	0.008577145010279164	3410	3.41	0.13599704059071227
X	0.009098805670730958	10719	10.719	0.09468402386722825
X	0.00827114423749257	14786	14.786	0.0823957790193331
X	0.007851955838738276	11993	11.993	0.08683270658219426
X	0.008455102266604593	28974	28.974	0.06632900304348205
X	0.00827234530835328	151891	151.891	0.03790520686374267
X	0.008341377623557716	5488	5.488	0.1149761847342969
X	0.007851426930828485	2714	2.714	0.14248842411831422
X	0.008176439845013058	20920	20.92	0.07311405233048036
X	0.00791082354333317	10218	10.218	0.09182309625792047
X	0.009441606640676265	200294	200.294	0.03612373555288612
X	0.008242276601369723	3109	3.109	0.13840193252370378
X	0.008028074276619179	51588	51.588	0.05378826843619694
X	0.009420876857067136	120570	120.57	0.0427514341947479
X	0.0075050192414612	1720	1.72	0.16340842886664367
X	0.0079231583349534	59496	59.496	0.05106658963519135
X	0.007569066212054867	7657	7.657	0.09961572171109691
X	0.008964101228206126	309695	309.695	0.030703709813772448
X	0.008150802746703668	6488	6.488	0.10790208764383259
X	0.007777553445075701	6971	6.971	0.10371684733917812
X	0.00792359483712282	5333	5.333	0.114108209872716
X	0.008645429251645208	1574	1.574	0.17643873865437637
X	0.007995865124626623	5654	5.654	0.11224574458835258
X	0.00832831546421627	12575	12.575	0.08716652336513976
X	0.00788720808762968	1716	1.716	0.16626544733654636
X	0.008705563250433276	60932	60.932	0.052277781262821295
X	0.00811072703711582	6836	6.836	0.10586504274440757
X	0.007630932783321323	2553	2.553	0.1440485655097576
X	0.007972290636044176	7538	7.538	0.1018847059994389
X	0.008945774180500887	106387	106.387	0.04381032310982233
X	0.009015114944149895	71015	71.015	0.05025821490004594
X	0.008657232040173601	141793	141.793	0.039376890893115225
X	0.008655247104037074	5122	5.122	0.11910956331045522
X	0.008111528845320914	3440	3.44	0.13310103260887718
X	0.008127755987316559	1467	1.467	0.1769484845812667
X	0.00772794974495434	2416	2.416	0.14734060645439118
X	0.008303396933597761	36986	36.986	0.06077703062549294
X	0.007902331137043198	3716	3.716	0.1285957176135326
X	0.008250584623587246	7422	7.422	0.10359081685929449
X	0.008105925966805745	13506	13.506	0.0843513354078545
X	0.007857831135561995	14033	14.033	0.08242343733940477
X	0.008161293605701614	10550	10.55	0.09179848505805535
X	0.007512841271040017	2146	2.146	0.15184185423554467
X	0.0086034557486516	127896	127.896	0.04066979554333761
X	0.008208519372053049	26325	26.325	0.0678107918582896
X	0.007894546847315295	3709	3.709	0.1286343010225542
X	0.008316155836652	4122	4.122	0.12635862086776173
X	0.008080927253043326	17885	17.885	0.07673451471304545
X	0.008243128651967142	44303	44.303	0.05708907164454724
X	0.008740775360875603	213021	213.021	0.03449126953715599
X	0.009356238950559238	63654	63.654	0.05277463054589627
X	0.008027029315001495	1976	1.976	0.1595594888088345
X	0.008280329945219999	12106	12.106	0.08810806380163409
X	0.008111017425643057	22022	22.022	0.07168135265629862
X	0.008090757303644287	2157	2.157	0.1553744757448496
X	0.008142389248776175	5676	5.676	0.11278114284241202
X	0.008098619527501858	2229	2.229	0.1537329552948433
X	0.008128065822521378	7812	7.812	0.10133084379922216
X	0.007880965684711097	22743	22.743	0.07023882347636543
X	0.00727305475129115	1345	1.345	0.1755219859904319
X	0.008196015129067799	8058	8.058	0.10056769496569033
X	0.008038660173427704	3617	3.617	0.13050036317826655
X	0.008182758104742317	11389	11.389	0.08956494014292808
X	0.00794778990126866	2828	2.828	0.14112013751356692
X	0.008274006146218408	9965	9.965	0.09398953739103766
X	0.00768417829667532	668	0.668	0.2257392045005752
X	0.007794081407244228	5763	5.763	0.1105873469766394
X	0.007947519552631854	2865	2.865	0.14050841224441687
X	0.00797573994295427	5024	5.024	0.11665600998531014
X	0.008239359388325457	7084	7.084	0.1051650981129071
X	0.0092123597665603	59469	59.469	0.05370648771632022
X	0.008240504292477338	3012	3.012	0.13986195916288321
X	0.007579523909319888	755	0.755	0.21572393082634334
X	0.0078893400420323	2903	2.903	0.13955044095410413
X	0.00833239040893175	43297	43.297	0.057734747161266704
X	0.008717486726006005	65265	65.265	0.05111758378167241
X	0.007774818528664441	45079	45.079	0.055663690398144706
X	0.007939896018294536	3415	3.415	0.13247726971563462
X	0.0082969990772348	137397	137.397	0.03923267589471424
X	0.009105056442215712	59244	59.244	0.053564792024498945
X	0.007595887766010441	1025	1.025	0.19496271222811876
X	0.007652092036014649	2238	2.238	0.15065146786835742
X	0.00829746136745932	13080	13.08	0.08592360666304893
X	0.008627535536198432	22549	22.549	0.07259719208791746
X	0.00852678162031923	15127	15.127	0.08260584495137073
X	0.008535158711148317	77062	77.062	0.048023864874365024
X	0.00817826551288816	7051	7.051	0.10506792703372735
X	0.007873730373063095	1507	1.507	0.17352248644430354
X	0.007489388670030166	1633	1.633	0.16614479040874688
X	0.007926329890520294	10442	10.442	0.0912212778290338
X	0.008874756912696296	5026	5.026	0.12086799760747098
X	0.00828929594493897	5896	5.896	0.11202631661392741
X	0.008631086167990272	43282	43.282	0.05842329458419019
X	0.008969920052970782	183696	183.696	0.0365507498351344
X	0.008129429863304359	4093	4.093	0.1257012070296219
X	0.007692560361078569	798	0.798	0.2128250155442817
X	0.009350077671477285	13311	13.311	0.08889312591668959
X	0.00828421939893536	11944	11.944	0.08851847687964637
X	0.008190645587548619	11223	11.223	0.0900332768709158
X	0.00736064203517321	947	0.947	0.1980866646621251
X	0.00748384721954735	1824	1.824	0.16009091726326302
X	0.00812661114625044	9614	9.614	0.09455150671857908
X	0.008157114375740128	1944	1.944	0.16129222405114274
X	0.008838889034766	40994	40.994	0.059964257664536766
X	0.00807061587431088	3118	3.118	0.1373019974959429
X	0.008319480089304407	5742	5.742	0.11315602063183261
X	0.00815528270057472	5282	5.282	0.11557933937789983
X	0.008250141414568273	3211	3.211	0.13696418862206883
X	0.008009275288151454	3898	3.898	0.12713070697586557
X	0.008371467357177542	65046	65.046	0.05048863907842429
X	0.0083737127227176	156542	156.542	0.037678684054062876
X	0.009389404362656567	50969	50.969	0.05689978734315619
X	0.008645365898289499	38455	38.455	0.06080559198498753
X	0.008197904207401777	7429	7.429	0.10333739126138018
X	0.008270417940377176	3871	3.871	0.12879571919030058
X	0.00821039056653717	9392	9.392	0.09561702880345707
X	0.008932919619637396	47263	47.263	0.05738839080260336
X	0.009236167946690904	19012	19.012	0.07861184790811777
X	0.008295456443717006	7693	7.693	0.10254508552619683
X	0.008594693384827271	6543	6.543	0.10951774404510566
X	0.009123727677129479	480673	480.673	0.0266751904447988
X	0.008093126718182474	5871	5.871	0.11129306081641038
X	0.009165510243511974	23863	23.863	0.07269042397914538
X	0.008360434956863099	46882	46.882	0.05628693241251709
X	0.008314918385299335	14756	14.756	0.08259677959582697
X	0.008214742655205005	11321	11.321	0.08986068484763354
X	0.009260641967689137	278158	278.158	0.03216991002989519
X	0.008339084300846138	60828	60.828	0.051563011686142184
X	0.008361841736312499	38634	38.634	0.06004047083761623
X	0.00818153534159	2492	2.492	0.14862627419628982
X	0.008261001987530046	15971	15.971	0.08027251543450009
X	0.007917126277886932	1474	1.474	0.17512844687377296
X	0.008341728798338034	51345	51.345	0.05456564618941139
X	0.00944279200104675	542316	542.316	0.025918866330533016
X	0.00859086016979392	48302	48.302	0.05623722021833549
X	0.008359734433141246	79310	79.31	0.047237638375587566
X	0.00869056218817695	17850	17.85	0.0786689252595272
X	0.008028341618080928	27985	27.985	0.06595284143964837
X	0.008730277235695793	16134	16.134	0.08148831310564371
X	0.0077140476591401165	6832	6.832	0.10413054786009127
X	0.00891443465948287	5944	5.944	0.11446472479197926
X	0.009245735578220619	149340	149.34	0.03955989290786289
X	0.008100012126016227	42724	42.724	0.05744751592659918
X	0.008174743499659572	101257	101.257	0.043219827198593075
X	0.008184659848292983	110623	110.623	0.04198089897972337
X	0.008278726861932576	24431	24.431	0.06971733382424293
X	0.007954973206740873	4958	4.958	0.1170695816143286
X	0.008718923507851127	29645	29.645	0.06650237225658391
X	0.008770300886303014	45323	45.323	0.057840487957376716
X	0.008158403285806798	7358	7.358	0.10350193330267966
X	0.009372644428593794	178000	178.0	0.037481248290896246
X	0.008243843823294324	9146	9.146	0.0965975666004748
X	0.007906286553784501	22478	22.478	0.07058920563088694
X	0.00799699916083842	1335	1.335	0.18161369521606877
X	0.007996084799421783	6697	6.697	0.10608786207588444
X	0.007671097602944493	950	0.95	0.20062172844967452
X	0.00798415513269797	1001	1.001	0.19980129392606388
X	0.008544554865571725	13383	13.383	0.08610836074718344
X	0.00814698365395248	10777	10.777	0.0910960726570632
X	0.00945177305770957	1759530	1759.53	0.017513470015757024
X	0.008132888420174508	4590	4.59	0.12100706002351226
X	0.008672995148416568	89259	89.259	0.045973301759218585
X	0.008064259678255359	3571	3.571	0.13119729611819034
X	0.008325169841092949	7395	7.395	0.10402833350628861
X	0.00826849208041857	4621	4.621	0.1214031947439839
X	0.008743064071324816	135881	135.881	0.04007144240961164
X	0.008214658110169056	2293	2.293	0.15301305261864845
X	0.008983690873064673	322082	322.082	0.030326999799545194
X	0.007900801710722908	2357	2.357	0.14965935181135326
X	0.00828549801664942	74020	74.02	0.048193652920542875
X	0.008138592871438298	12796	12.796	0.0859986626035075
X	0.008180321468313264	56461	56.461	0.052521920945112696
X	0.00825953917447582	64632	64.632	0.05036970500855117
X	0.009246892654729935	7577	7.577	0.1068643486242172
X	0.0078035126643422395	6262	6.262	0.10761158432957764
X	0.009400785051518713	29386	29.386	0.06839240609572192
X	0.00808089968042955	4016	4.016	0.12624727235742397
X	0.008077578677983288	5446	5.446	0.11404279517884199
X	0.008588774030971785	41804	41.804	0.059007123752419316
X	0.008190471462296544	3738	3.738	0.12988426312289592
X	0.00870961859811193	35873	35.873	0.062384564344582755
X	0.007837148817382554	7006	7.006	0.10380764058258389
X	0.008682031386092972	63557	63.557	0.051501433880289556
X	0.009220610073288147	169577	169.577	0.03788473347629028
X	0.008300874496423452	26466	26.466	0.06794307208261845
X	0.00794468231750167	4927	4.927	0.11726398744613119
X	0.00822868763734621	27774	27.774	0.06666492300476466
X	0.008710159055354619	32181	32.181	0.0646857845799837
X	0.0079222174804784	4766	4.766	0.11845795769960527
X	0.008225536991977395	13846	13.846	0.08406463097267801
X	0.007798704255652384	10510	10.51	0.09053293048660249
X	0.008216174455844051	25454	25.454	0.06859690754217136
X	0.007813203575107754	39029	39.029	0.058498837895877426
X	0.008245655532767616	27206	27.206	0.06717177605360476
X	0.00822226798790855	7710	7.71	0.10216741766982629
X	0.007780464881556914	2005	2.005	0.15714372969832766
X	0.00779224349334904	2081	2.081	0.15528522831224956
X	0.00792389492349024	36983	36.983	0.059838246266379465
X	0.0079912523314008	24430	24.43	0.0689017827756258
X	0.008280017297206368	24268	24.268	0.06987670522440652
X	0.008492343239765951	4697	4.697	0.12182478668239492
X	0.008356075203147245	8989	8.989	0.0975956183030137
X	0.00866880667322604	15585	15.585	0.08224020237454627
X	0.009402724192601417	14821	14.821	0.08592615858600261
X	0.00794127128987783	8485	8.485	0.09781663728212546
X	0.008216750170377329	26309	26.309	0.06784719757346931
X	0.008099400785132218	5319	5.319	0.11504673548560539
X	0.008186262034985004	13833	13.833	0.08395690526092106
X	0.007596497057385146	765	0.765	0.21494003384717095
X	0.008602746593357448	3137	3.137	0.13997190409717902
X	0.008376665550053347	51916	51.916	0.05444065194787189
X	0.008182150382315666	8994	8.994	0.09689577886683805
X	0.008347703785244676	29112	29.112	0.06594243740700684
X	0.007875312632381478	1284	1.284	0.18304909800940478
X	0.00770893123525899	2829	2.829	0.1396755557673136
X	0.009469532746209479	11384	11.384	0.09404691506936143
X	0.008022522792650939	33068	33.068	0.062368841468226105
X	0.009389461909061791	17504	17.504	0.08125207609928073
X	0.007970844321873097	1699	1.699	0.16740572884962424
X	0.008221643844759709	4413	4.413	0.12304809960399966
X	0.008338979574696748	23493	23.493	0.07080405712520155
X	0.008200399223572649	9299	9.299	0.09589579915052794
X	0.008179788965192355	26507	26.507	0.0675762114915968
X	0.007908215015123938	1680	1.68	0.1675932008804696
X	0.007593258457330141	2781	2.781	0.13976842283344876
X	0.008038774812660758	26174	26.174	0.06746935965666838
X	0.00767929012200307	2522	2.522	0.14494124843325237
X	0.00893217702763092	85855	85.855	0.04703241107972704
X	0.008186042298097145	1895	1.895	0.16286268485045655
X	0.00822011482738655	32718	32.718	0.0631001481251049
X	0.00833591691018927	44694	44.694	0.05713488849391394
X	0.008300031522867388	10695	10.695	0.09189664387561593
X	0.008724981870151036	64566	64.566	0.051316089093605625
X	0.00828077662537108	39135	39.135	0.05958934025821389
X	0.007847117104497921	538	0.538	0.24432952613140332
X	0.008612682911126119	20315	20.315	0.07512316506772354
X	0.0082734796432578	5851	5.851	0.11224130345993029
X	0.008276300271085394	21182	21.182	0.07310657282983993
X	0.008185160447608109	34170	34.17	0.06210512011366074
X	0.007800936617803291	14540	14.54	0.08125699939101
X	0.008247652658133367	22525	22.525	0.07154102986271553
X	0.008202783984774798	5969	5.969	0.11117826661316639
X	0.008136739934845245	19614	19.614	0.0745809804135084
X	0.008156862854466481	2073	2.073	0.1578730330709804
X	0.0077847436257172395	4027	4.027	0.12457213134102517
X	0.008536230901758418	43510	43.51	0.058106630239746285
X	0.008957067304991415	123148	123.148	0.041742599545719655
X	0.007669332255757441	8924	8.924	0.09507492623187579
X	0.00828339264163632	6255	6.255	0.10981465747108585
X	0.008307955333393069	10063	10.063	0.09381138765597638
X	0.009436997872204924	25256	25.256	0.07202607729467986
X	0.008590178993627875	20977	20.977	0.07425965804640892
X	0.008087875112607137	2719	2.719	0.14381639878354038
X	0.009070350773302633	16027	16.027	0.08271620773518185
X	0.008484954710659976	56273	56.273	0.0532251000687634
X	0.008630016880474071	4884	4.884	0.12089601802134722
X	0.00822228550996433	27823	27.823	0.06660848141843145
X	0.00805220870764797	3845	3.845	0.12793996461855917
time for making epsilon is 1.0223140716552734
epsilons are
[0.19823146707966027, 0.09477952860696714, 0.17585742951581557, 0.09475602858116436, 0.120921577263166, 0.06909825335643237, 0.06851827722216734, 0.07695762126200374, 0.106911277921199, 0.10783999781540375, 0.12263622576647473, 0.06620960620827747, 0.129809016603846, 0.03361869853112305, 0.0944798666399607, 0.06721682434966572, 0.062069516512881145, 0.08797995950791324, 0.052986056390924074, 0.05481168206021961, 0.06001814055111843, 0.04891659683487232, 0.1172544122895054, 0.06482934881631154, 0.19690252607382153, 0.07050166944638844, 0.07237305852004956, 0.146082213575855, 0.0928529838752048, 0.02676245112318649, 0.11875538573988112, 0.02936278275509684, 0.09414212117309569, 0.12044967043923475, 0.08501099776329503, 0.04983639866925594, 0.06222906567341928, 0.22859417466433926, 0.1504119297434416, 0.17874324264939853, 0.19560070480938904, 0.1407902954907061, 0.25442002758423093, 0.15527120687665188, 0.26956014418249213, 0.27048027374948846, 0.18310127355205288, 0.13361469684867727, 0.2074489396809135, 0.10149925028278826, 0.09561418715860753, 0.21210439654711305, 0.10925991105850441, 0.1841483290432834, 0.1493843644091565, 0.11583654480069659, 0.2073724042426985, 0.1711228237495073, 0.1712476375543114, 0.18879865728011822, 0.14450544721475217, 0.2039512339536903, 0.14568136248779334, 0.1120322659260388, 0.18287550610120692, 0.17978983906227472, 0.16734642588392548, 0.13592659947290942, 0.1290084181927642, 0.15590336395349633, 0.17648585258770047, 0.17977813726579084, 0.17248445147927643, 0.17270570622440462, 0.17158981701199397, 0.13274779394400077, 0.28574845235642543, 0.21870195914169166, 0.1643724135515686, 0.10899067816843973, 0.1988431308639978, 0.20385930932493787, 0.16278057575437127, 0.22112422582941305, 0.1991007371270075, 0.2164067463752352, 0.1901370929907939, 0.2450096165205443, 0.16614990419718012, 0.1633614081667855, 0.19448472943949893, 0.21616853832917557, 0.22098555576660633, 0.1718042413972507, 0.16440809238483198, 0.1482237266427294, 0.1755430985971622, 0.13621013833940643, 0.16867188453330106, 0.1690015495982526, 0.11943404804168646, 0.1885391704882882, 0.15113164529666065, 0.19472230406974309, 0.1422304186251073, 0.17653825476644652, 0.18516549108672667, 0.22200437988202454, 0.18660005597236934, 0.21990321853869682, 0.20265689703928458, 0.2080659387081234, 0.19378560786135968, 0.18363637664315113, 0.14708480711507965, 0.1563418156584213, 0.17619571937968384, 0.3015330532235862, 0.11911082031503956, 0.23582265633443733, 0.11844316179131252, 0.18918766430739134, 0.19924306882540038, 0.20909531804154863, 0.21368636015454895, 0.20336928363553075, 0.21962313113380677, 0.18260829066297776, 0.12414668213367629, 0.18922117568155453, 0.12421836408768573, 0.1988526477777471, 0.16415652578233592, 0.18186580556170182, 0.17093883850569366, 0.21333199003015818, 0.18391592902874915, 0.19431178283965503, 0.21372114969984127, 0.21090481042792417, 0.17505410046178788, 0.2823522000729597, 0.12379705243634222, 0.15125307754635484, 0.14480158925498332, 0.2305770590524071, 0.1825748799180271, 0.20474941586511222, 0.21517417381620416, 0.19810587589179057, 0.16222402108952896, 0.15648789232490357, 0.16886297497071018, 0.12390844454269775, 0.16139440938928562, 0.09412016724700764, 0.1742245727282291, 0.14815517950911816, 0.18207931193445176, 0.28074841351226687, 0.12678103120682488, 0.21414255240328833, 0.1676825114807135, 0.24339570438433134, 0.14769041201412764, 0.20442768714360363, 0.20741858908529792, 0.15738045859679647, 0.1587271040724005, 0.25682766717499816, 0.30341031079318026, 0.2499165060344637, 0.13834530871512682, 0.21823704061519272, 0.15799612675695818, 0.1874648512130902, 0.12599222443540364, 0.18169750876112337, 0.13731419903024286, 0.15486420299135112, 0.15901677191345334, 0.16839219269623798, 0.13530904396333823, 0.2057758806355164, 0.1667464631472644, 0.17011323781744161, 0.1952379398945486, 0.14242801036990976, 0.14227373707025914, 0.16249496303601005, 0.19065599183377957, 0.2702382648750163, 0.18628582066457314, 0.1412903611632172, 0.12665919022568337, 0.16766622294651906, 0.1912283047760425, 0.21818741869229985, 0.17430178675178765, 0.22304049153482078, 0.14518651768517257, 0.20226587218380307, 0.13625869342230415, 0.17249299903718066, 0.15398696457808714, 0.20885219321210582, 0.14404935168000313, 0.17647857625448, 0.21466546217821783, 0.14129670835438649, 0.14651292996002283, 0.1213745628572116, 0.1555022523826036, 0.15010347380652528, 0.27515402303818576, 0.2067254836973372, 0.2803924869051964, 0.224122911738621, 0.14231092909403417, 0.15163903466348375, 0.16143979102225273, 0.23268444880684458, 0.15208873200096862, 0.151338267946668, 0.16076355748805052, 0.17818760180343965, 0.21942573409706803, 0.22216933825679275, 0.16585102455251302, 0.2651611533042572, 0.17626707753252382, 0.1762135433673253, 0.19428523111887133, 0.14090196951958445, 0.18019609273365025, 0.15901934077457128, 0.13190504259903948, 0.13555969932414363, 0.21184464618935142, 0.08631039875511079, 0.05830127676905809, 0.16764547583186154, 0.1373372125152975, 0.2893483754192361, 0.14733739097565346, 0.13071698956793007, 0.03903025892739307, 0.18338566609754536, 0.04443656293644923, 0.10782953370183637, 0.08688850140008642, 0.09528107430701029, 0.19615915169838213, 0.1508903737800133, 0.04970326605271484, 0.03827324438030359, 0.1586984913706301, 0.07972820218422058, 0.11412726303952672, 0.09543147262180746, 0.05079460133699357, 0.07877749875209254, 0.0926244263693341, 0.09662030440871386, 0.1691238436410363, 0.043582764955472725, 0.17076472881569932, 0.17540722553367197, 0.08535425871870292, 0.05971934455913374, 0.048909045132664954, 0.06426122238990402, 0.16604887436687019, 0.0797776972399239, 0.14638772596402955, 0.07217928247615074, 0.07592197807613453, 0.1384316683667924, 0.06812067051002393, 0.26275021296552425, 0.11760505652413386, 0.05047564790052455, 0.07492557357154267, 0.05073632032323922, 0.18678987840127323, 0.043833663710261234, 0.11354877450427181, 0.08658469482928821, 0.0640277620747422, 0.20813015258475134, 0.05416810838061305, 0.04541365870743541, 0.037436372916595816, 0.06535847926357118, 0.08170590834852204, 0.10136923302465202, 0.16015532356472922, 0.05971172585219522, 0.13236473527747977, 0.08999979012266891, 0.08212168968402407, 0.11543959282253553, 0.10566240573841199, 0.15152971323977477, 0.0546889070751946, 0.11475131140475883, 0.09766868406615499, 0.12363608676442919, 0.06534145747267324, 0.09075039308164766, 0.04716947350368964, 0.1276931613704074, 0.07388804497881231, 0.1340717958607087, 0.04891740796761788, 0.08305705547639554, 0.11211870420562636, 0.05478001514834881, 0.05306999260536354, 0.18020015622763746, 0.047063948026902634, 0.05347074031846407, 0.04008845517073851, 0.099016833186808, 0.1752680626881323, 0.11944634220911825, 0.12250452612667803, 0.10066060999902204, 0.08205553337532313, 0.23912761136093177, 0.05968182797728931, 0.05194558596488951, 0.20277541173532132, 0.13928669941724967, 0.08060228924878626, 0.12167774480001559, 0.1509778244203859, 0.12618885772852512, 0.16467536025505922, 0.1176701911628471, 0.07743136535345313, 0.09476026802525928, 0.0781957688451827, 0.12742364484668026, 0.03174330863488534, 0.11691853503739132, 0.055620819399338835, 0.20227802069848375, 0.15690652375163264, 0.16880527394131262, 0.11897592108298058, 0.17912878247711375, 0.05053082423286994, 0.15026935548826306, 0.1584247934156567, 0.04649488322784859, 0.06889076441887738, 0.0813535958127138, 0.05733201425876367, 0.04263047308180675, 0.13971126537882436, 0.1428999267720916, 0.09726789160602298, 0.20029133317389725, 0.13923298908538345, 0.09690812868529868, 0.16954197406952334, 0.15126926443652436, 0.12217423175126951, 0.15727002403716436, 0.04891718411717188, 0.107347347699225, 0.06308201273886188, 0.20641839601181175, 0.151462753433246, 0.0483185100900461, 0.04687650705416453, 0.037661797822655246, 0.1151359697481406, 0.07585787768155071, 0.1671781908181054, 0.18214887531812055, 0.06680825940645406, 0.10278394250143101, 0.1382959069801717, 0.07292105727275688, 0.029703273834106272, 0.07991907240574561, 0.10421826932210192, 0.10283086157627395, 0.1420442102830602, 0.16772622790342212, 0.052239211494235445, 0.09185900361456949, 0.18409153934999206, 0.06263147751002793, 0.12503377563617085, 0.26465858229397077, 0.06297185306918936, 0.062046976649393897, 0.04373608864891334, 0.048024625853201774, 0.050939466322480774, 0.0813056240431906, 0.11311360876194404, 0.053465502902510655, 0.05777494202646096, 0.09213033632270912, 0.07337201670869559, 0.0555751770047317, 0.04751733510865979, 0.05280574887029382, 0.06644023639311115, 0.09495928588040785, 0.13583385372064685, 0.15116231056992716, 0.05725433324387464, 0.05574238041537258, 0.03770880216331437, 0.05393395391213693, 0.06743916386772363, 0.10900353054280415, 0.05534269645321036, 0.132153765526885, 0.12129295236372327, 0.055829184107444474, 0.07451986876468039, 0.16551943791543686, 0.09832271717158962, 0.031841667427941374, 0.09598379047425001, 0.0496075051497677, 0.16603556340394068, 0.15520711246037702, 0.140056799270519, 0.17923758446329024, 0.09721769349894074, 0.0704962981114308, 0.19014677501288751, 0.05704322583384505, 0.14754688748646902, 0.08363850045266041, 0.11962331616335185, 0.05256379688529968, 0.13599704059071227, 0.09468402386722825, 0.0823957790193331, 0.08683270658219426, 0.06632900304348205, 0.03790520686374267, 0.1149761847342969, 0.14248842411831422, 0.07311405233048036, 0.09182309625792047, 0.03612373555288612, 0.13840193252370378, 0.05378826843619694, 0.0427514341947479, 0.16340842886664367, 0.05106658963519135, 0.09961572171109691, 0.030703709813772448, 0.10790208764383259, 0.10371684733917812, 0.114108209872716, 0.17643873865437637, 0.11224574458835258, 0.08716652336513976, 0.16626544733654636, 0.052277781262821295, 0.10586504274440757, 0.1440485655097576, 0.1018847059994389, 0.04381032310982233, 0.05025821490004594, 0.039376890893115225, 0.11910956331045522, 0.13310103260887718, 0.1769484845812667, 0.14734060645439118, 0.06077703062549294, 0.1285957176135326, 0.10359081685929449, 0.0843513354078545, 0.08242343733940477, 0.09179848505805535, 0.15184185423554467, 0.04066979554333761, 0.0678107918582896, 0.1286343010225542, 0.12635862086776173, 0.07673451471304545, 0.05708907164454724, 0.03449126953715599, 0.05277463054589627, 0.1595594888088345, 0.08810806380163409, 0.07168135265629862, 0.1553744757448496, 0.11278114284241202, 0.1537329552948433, 0.10133084379922216, 0.07023882347636543, 0.1755219859904319, 0.10056769496569033, 0.13050036317826655, 0.08956494014292808, 0.14112013751356692, 0.09398953739103766, 0.2257392045005752, 0.1105873469766394, 0.14050841224441687, 0.11665600998531014, 0.1051650981129071, 0.05370648771632022, 0.13986195916288321, 0.21572393082634334, 0.13955044095410413, 0.057734747161266704, 0.05111758378167241, 0.055663690398144706, 0.13247726971563462, 0.03923267589471424, 0.053564792024498945, 0.19496271222811876, 0.15065146786835742, 0.08592360666304893, 0.07259719208791746, 0.08260584495137073, 0.048023864874365024, 0.10506792703372735, 0.17352248644430354, 0.16614479040874688, 0.0912212778290338, 0.12086799760747098, 0.11202631661392741, 0.05842329458419019, 0.0365507498351344, 0.1257012070296219, 0.2128250155442817, 0.08889312591668959, 0.08851847687964637, 0.0900332768709158, 0.1980866646621251, 0.16009091726326302, 0.09455150671857908, 0.16129222405114274, 0.059964257664536766, 0.1373019974959429, 0.11315602063183261, 0.11557933937789983, 0.13696418862206883, 0.12713070697586557, 0.05048863907842429, 0.037678684054062876, 0.05689978734315619, 0.06080559198498753, 0.10333739126138018, 0.12879571919030058, 0.09561702880345707, 0.05738839080260336, 0.07861184790811777, 0.10254508552619683, 0.10951774404510566, 0.0266751904447988, 0.11129306081641038, 0.07269042397914538, 0.05628693241251709, 0.08259677959582697, 0.08986068484763354, 0.03216991002989519, 0.051563011686142184, 0.06004047083761623, 0.14862627419628982, 0.08027251543450009, 0.17512844687377296, 0.05456564618941139, 0.025918866330533016, 0.05623722021833549, 0.047237638375587566, 0.0786689252595272, 0.06595284143964837, 0.08148831310564371, 0.10413054786009127, 0.11446472479197926, 0.03955989290786289, 0.05744751592659918, 0.043219827198593075, 0.04198089897972337, 0.06971733382424293, 0.1170695816143286, 0.06650237225658391, 0.057840487957376716, 0.10350193330267966, 0.037481248290896246, 0.0965975666004748, 0.07058920563088694, 0.18161369521606877, 0.10608786207588444, 0.20062172844967452, 0.19980129392606388, 0.08610836074718344, 0.0910960726570632, 0.017513470015757024, 0.12100706002351226, 0.045973301759218585, 0.13119729611819034, 0.10402833350628861, 0.1214031947439839, 0.04007144240961164, 0.15301305261864845, 0.030326999799545194, 0.14965935181135326, 0.048193652920542875, 0.0859986626035075, 0.052521920945112696, 0.05036970500855117, 0.1068643486242172, 0.10761158432957764, 0.06839240609572192, 0.12624727235742397, 0.11404279517884199, 0.059007123752419316, 0.12988426312289592, 0.062384564344582755, 0.10380764058258389, 0.051501433880289556, 0.03788473347629028, 0.06794307208261845, 0.11726398744613119, 0.06666492300476466, 0.0646857845799837, 0.11845795769960527, 0.08406463097267801, 0.09053293048660249, 0.06859690754217136, 0.058498837895877426, 0.06717177605360476, 0.10216741766982629, 0.15714372969832766, 0.15528522831224956, 0.059838246266379465, 0.0689017827756258, 0.06987670522440652, 0.12182478668239492, 0.0975956183030137, 0.08224020237454627, 0.08592615858600261, 0.09781663728212546, 0.06784719757346931, 0.11504673548560539, 0.08395690526092106, 0.21494003384717095, 0.13997190409717902, 0.05444065194787189, 0.09689577886683805, 0.06594243740700684, 0.18304909800940478, 0.1396755557673136, 0.09404691506936143, 0.062368841468226105, 0.08125207609928073, 0.16740572884962424, 0.12304809960399966, 0.07080405712520155, 0.09589579915052794, 0.0675762114915968, 0.1675932008804696, 0.13976842283344876, 0.06746935965666838, 0.14494124843325237, 0.04703241107972704, 0.16286268485045655, 0.0631001481251049, 0.05713488849391394, 0.09189664387561593, 0.051316089093605625, 0.05958934025821389, 0.24432952613140332, 0.07512316506772354, 0.11224130345993029, 0.07310657282983993, 0.06210512011366074, 0.08125699939101, 0.07154102986271553, 0.11117826661316639, 0.0745809804135084, 0.1578730330709804, 0.12457213134102517, 0.058106630239746285, 0.041742599545719655, 0.09507492623187579, 0.10981465747108585, 0.09381138765597638, 0.07202607729467986, 0.07425965804640892, 0.14381639878354038, 0.08271620773518185, 0.0532251000687634, 0.12089601802134722, 0.06660848141843145, 0.12793996461855917]
0.09944524602372098
Making ranges
torch.Size([13430, 2])
We keep 2.90e+06/9.28e+07 =  3% of the original kernel matrix.

torch.Size([2208, 2])
We keep 1.09e+05/9.94e+05 = 10% of the original kernel matrix.

torch.Size([5765, 2])
We keep 5.81e+05/9.60e+06 =  6% of the original kernel matrix.

torch.Size([13326, 2])
We keep 4.78e+06/1.22e+08 =  3% of the original kernel matrix.

torch.Size([13870, 2])
We keep 3.72e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([3362, 2])
We keep 2.43e+05/2.66e+06 =  9% of the original kernel matrix.

torch.Size([7053, 2])
We keep 8.53e+05/1.57e+07 =  5% of the original kernel matrix.

torch.Size([12598, 2])
We keep 4.43e+06/8.78e+07 =  5% of the original kernel matrix.

torch.Size([13349, 2])
We keep 3.11e+06/9.02e+07 =  3% of the original kernel matrix.

torch.Size([7691, 2])
We keep 1.46e+06/2.79e+07 =  5% of the original kernel matrix.

torch.Size([10486, 2])
We keep 2.07e+06/5.09e+07 =  4% of the original kernel matrix.

torch.Size([36552, 2])
We keep 1.80e+07/6.26e+08 =  2% of the original kernel matrix.

torch.Size([22306, 2])
We keep 7.13e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([35738, 2])
We keep 2.08e+07/6.70e+08 =  3% of the original kernel matrix.

torch.Size([21996, 2])
We keep 7.45e+06/2.49e+08 =  2% of the original kernel matrix.

torch.Size([26714, 2])
We keep 1.03e+07/3.25e+08 =  3% of the original kernel matrix.

torch.Size([19347, 2])
We keep 5.40e+06/1.74e+08 =  3% of the original kernel matrix.

torch.Size([9690, 2])
We keep 2.22e+06/4.55e+07 =  4% of the original kernel matrix.

torch.Size([11772, 2])
We keep 2.45e+06/6.50e+07 =  3% of the original kernel matrix.

torch.Size([9747, 2])
We keep 2.40e+06/4.17e+07 =  5% of the original kernel matrix.

torch.Size([11691, 2])
We keep 2.30e+06/6.22e+07 =  3% of the original kernel matrix.

torch.Size([7522, 2])
We keep 1.07e+06/1.90e+07 =  5% of the original kernel matrix.

torch.Size([10225, 2])
We keep 1.70e+06/4.20e+07 =  4% of the original kernel matrix.

torch.Size([35841, 2])
We keep 4.56e+07/7.82e+08 =  5% of the original kernel matrix.

torch.Size([21883, 2])
We keep 7.54e+06/2.69e+08 =  2% of the original kernel matrix.

torch.Size([6636, 2])
We keep 8.25e+05/1.32e+07 =  6% of the original kernel matrix.

torch.Size([9535, 2])
We keep 1.51e+06/3.50e+07 =  4% of the original kernel matrix.

torch.Size([320599, 2])
We keep 7.50e+08/4.92e+10 =  1% of the original kernel matrix.

torch.Size([71535, 2])
We keep 4.94e+07/2.14e+09 =  2% of the original kernel matrix.

torch.Size([13412, 2])
We keep 3.52e+06/9.53e+07 =  3% of the original kernel matrix.

torch.Size([13862, 2])
We keep 3.28e+06/9.40e+07 =  3% of the original kernel matrix.

torch.Size([39628, 2])
We keep 2.17e+07/7.36e+08 =  2% of the original kernel matrix.

torch.Size([23028, 2])
We keep 7.61e+06/2.61e+08 =  2% of the original kernel matrix.

torch.Size([49237, 2])
We keep 3.41e+07/1.20e+09 =  2% of the original kernel matrix.

torch.Size([26207, 2])
We keep 9.43e+06/3.33e+08 =  2% of the original kernel matrix.

torch.Size([17043, 2])
We keep 5.36e+06/1.45e+08 =  3% of the original kernel matrix.

torch.Size([15512, 2])
We keep 3.91e+06/1.16e+08 =  3% of the original kernel matrix.

torch.Size([74061, 2])
We keep 7.97e+07/3.06e+09 =  2% of the original kernel matrix.

torch.Size([32564, 2])
We keep 1.43e+07/5.33e+08 =  2% of the original kernel matrix.

torch.Size([70010, 2])
We keep 8.69e+07/2.48e+09 =  3% of the original kernel matrix.

torch.Size([31694, 2])
We keep 1.26e+07/4.79e+08 =  2% of the original kernel matrix.

torch.Size([47527, 2])
We keep 5.05e+07/1.78e+09 =  2% of the original kernel matrix.

torch.Size([24979, 2])
We keep 1.15e+07/4.06e+08 =  2% of the original kernel matrix.

torch.Size([99050, 2])
We keep 1.04e+08/5.61e+09 =  1% of the original kernel matrix.

torch.Size([38617, 2])
We keep 1.81e+07/7.21e+08 =  2% of the original kernel matrix.

torch.Size([8160, 2])
We keep 1.33e+06/2.61e+07 =  5% of the original kernel matrix.

torch.Size([10751, 2])
We keep 1.98e+06/4.92e+07 =  4% of the original kernel matrix.

torch.Size([40089, 2])
We keep 4.71e+07/9.02e+08 =  5% of the original kernel matrix.

torch.Size([23485, 2])
We keep 8.59e+06/2.89e+08 =  2% of the original kernel matrix.

torch.Size([2179, 2])
We keep 1.02e+05/9.60e+05 = 10% of the original kernel matrix.

torch.Size([5719, 2])
We keep 5.68e+05/9.44e+06 =  6% of the original kernel matrix.

torch.Size([34292, 2])
We keep 1.50e+07/5.65e+08 =  2% of the original kernel matrix.

torch.Size([21867, 2])
We keep 6.75e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([30740, 2])
We keep 1.52e+07/4.87e+08 =  3% of the original kernel matrix.

torch.Size([20692, 2])
We keep 6.46e+06/2.12e+08 =  3% of the original kernel matrix.

torch.Size([5011, 2])
We keep 4.40e+05/6.33e+06 =  6% of the original kernel matrix.

torch.Size([8257, 2])
We keep 1.13e+06/2.42e+07 =  4% of the original kernel matrix.

torch.Size([14197, 2])
We keep 4.29e+06/1.00e+08 =  4% of the original kernel matrix.

torch.Size([14021, 2])
We keep 3.25e+06/9.64e+07 =  3% of the original kernel matrix.

torch.Size([697518, 2])
We keep 2.68e+09/2.08e+11 =  1% of the original kernel matrix.

torch.Size([106737, 2])
We keep 9.54e+07/4.40e+09 =  2% of the original kernel matrix.

torch.Size([7730, 2])
We keep 1.24e+06/2.22e+07 =  5% of the original kernel matrix.

torch.Size([10387, 2])
We keep 1.78e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([495682, 2])
We keep 2.05e+09/1.38e+11 =  1% of the original kernel matrix.

torch.Size([89330, 2])
We keep 7.96e+07/3.58e+09 =  2% of the original kernel matrix.

torch.Size([13342, 2])
We keep 4.29e+06/1.02e+08 =  4% of the original kernel matrix.

torch.Size([13917, 2])
We keep 3.34e+06/9.74e+07 =  3% of the original kernel matrix.

torch.Size([7707, 2])
We keep 1.15e+06/2.19e+07 =  5% of the original kernel matrix.

torch.Size([10377, 2])
We keep 1.79e+06/4.50e+07 =  3% of the original kernel matrix.

torch.Size([17753, 2])
We keep 5.81e+06/1.72e+08 =  3% of the original kernel matrix.

torch.Size([15868, 2])
We keep 4.18e+06/1.26e+08 =  3% of the original kernel matrix.

torch.Size([90876, 2])
We keep 1.45e+08/4.44e+09 =  3% of the original kernel matrix.

torch.Size([36546, 2])
We keep 1.70e+07/6.42e+08 =  2% of the original kernel matrix.

torch.Size([50147, 2])
We keep 4.00e+07/1.40e+09 =  2% of the original kernel matrix.

torch.Size([26425, 2])
We keep 1.03e+07/3.61e+08 =  2% of the original kernel matrix.

torch.Size([1494, 2])
We keep 5.84e+04/4.50e+05 = 12% of the original kernel matrix.

torch.Size([4940, 2])
We keep 4.46e+05/6.46e+06 =  6% of the original kernel matrix.

torch.Size([4312, 2])
We keep 4.14e+05/5.31e+06 =  7% of the original kernel matrix.

torch.Size([7660, 2])
We keep 1.06e+06/2.22e+07 =  4% of the original kernel matrix.

torch.Size([2810, 2])
We keep 1.77e+05/1.85e+06 =  9% of the original kernel matrix.

torch.Size([6318, 2])
We keep 7.28e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([2460, 2])
We keep 1.53e+05/1.45e+06 = 10% of the original kernel matrix.

torch.Size([6182, 2])
We keep 6.98e+05/1.16e+07 =  6% of the original kernel matrix.

torch.Size([4491, 2])
We keep 5.44e+05/7.41e+06 =  7% of the original kernel matrix.

torch.Size([7585, 2])
We keep 1.21e+06/2.62e+07 =  4% of the original kernel matrix.

torch.Size([1135, 2])
We keep 3.34e+04/2.31e+05 = 14% of the original kernel matrix.

torch.Size([4525, 2])
We keep 3.57e+05/4.63e+06 =  7% of the original kernel matrix.

torch.Size([3943, 2])
We keep 3.91e+05/4.00e+06 =  9% of the original kernel matrix.

torch.Size([7285, 2])
We keep 9.48e+05/1.93e+07 =  4% of the original kernel matrix.

torch.Size([1034, 2])
We keep 2.19e+04/1.58e+05 = 13% of the original kernel matrix.

torch.Size([4430, 2])
We keep 3.03e+05/3.83e+06 =  7% of the original kernel matrix.

torch.Size([1009, 2])
We keep 2.35e+04/1.48e+05 = 15% of the original kernel matrix.

torch.Size([4316, 2])
We keep 3.05e+05/3.71e+06 =  8% of the original kernel matrix.

torch.Size([2682, 2])
We keep 1.58e+05/1.56e+06 = 10% of the original kernel matrix.

torch.Size([6202, 2])
We keep 6.84e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([6136, 2])
We keep 7.24e+05/1.15e+07 =  6% of the original kernel matrix.

torch.Size([9220, 2])
We keep 1.43e+06/3.26e+07 =  4% of the original kernel matrix.

torch.Size([1942, 2])
We keep 9.64e+04/8.37e+05 = 11% of the original kernel matrix.

torch.Size([5494, 2])
We keep 5.53e+05/8.81e+06 =  6% of the original kernel matrix.

torch.Size([10381, 2])
We keep 2.69e+06/6.20e+07 =  4% of the original kernel matrix.

torch.Size([12368, 2])
We keep 2.84e+06/7.58e+07 =  3% of the original kernel matrix.

torch.Size([13613, 2])
We keep 4.05e+06/1.10e+08 =  3% of the original kernel matrix.

torch.Size([14121, 2])
We keep 3.54e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([1887, 2])
We keep 1.02e+05/8.48e+05 = 12% of the original kernel matrix.

torch.Size([5441, 2])
We keep 5.75e+05/8.87e+06 =  6% of the original kernel matrix.

torch.Size([9127, 2])
We keep 1.93e+06/3.92e+07 =  4% of the original kernel matrix.

torch.Size([11328, 2])
We keep 2.29e+06/6.03e+07 =  3% of the original kernel matrix.

torch.Size([2753, 2])
We keep 1.61e+05/1.60e+06 = 10% of the original kernel matrix.

torch.Size([6344, 2])
We keep 6.99e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([4831, 2])
We keep 4.66e+05/5.97e+06 =  7% of the original kernel matrix.

torch.Size([8073, 2])
We keep 1.12e+06/2.35e+07 =  4% of the original kernel matrix.

torch.Size([7980, 2])
We keep 1.51e+06/2.90e+07 =  5% of the original kernel matrix.

torch.Size([10579, 2])
We keep 2.04e+06/5.19e+07 =  3% of the original kernel matrix.

torch.Size([2070, 2])
We keep 1.17e+05/1.05e+06 = 11% of the original kernel matrix.

torch.Size([5707, 2])
We keep 6.08e+05/9.87e+06 =  6% of the original kernel matrix.

torch.Size([3176, 2])
We keep 1.99e+05/2.27e+06 =  8% of the original kernel matrix.

torch.Size([6669, 2])
We keep 7.69e+05/1.45e+07 =  5% of the original kernel matrix.

torch.Size([3476, 2])
We keep 2.20e+05/2.49e+06 =  8% of the original kernel matrix.

torch.Size([7022, 2])
We keep 8.10e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([2169, 2])
We keep 1.46e+05/1.35e+06 = 10% of the original kernel matrix.

torch.Size([5585, 2])
We keep 6.52e+05/1.12e+07 =  5% of the original kernel matrix.

torch.Size([4933, 2])
We keep 5.12e+05/7.56e+06 =  6% of the original kernel matrix.

torch.Size([8296, 2])
We keep 1.22e+06/2.65e+07 =  4% of the original kernel matrix.

torch.Size([2143, 2])
We keep 9.39e+04/8.91e+05 = 10% of the original kernel matrix.

torch.Size([5793, 2])
We keep 5.58e+05/9.09e+06 =  6% of the original kernel matrix.

torch.Size([5055, 2])
We keep 5.19e+05/6.99e+06 =  7% of the original kernel matrix.

torch.Size([8305, 2])
We keep 1.19e+06/2.55e+07 =  4% of the original kernel matrix.

torch.Size([9129, 2])
We keep 1.52e+06/3.31e+07 =  4% of the original kernel matrix.

torch.Size([11376, 2])
We keep 2.14e+06/5.54e+07 =  3% of the original kernel matrix.

torch.Size([2680, 2])
We keep 1.60e+05/1.68e+06 =  9% of the original kernel matrix.

torch.Size([6276, 2])
We keep 7.08e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([2803, 2])
We keep 1.78e+05/1.82e+06 =  9% of the original kernel matrix.

torch.Size([6418, 2])
We keep 7.22e+05/1.30e+07 =  5% of the original kernel matrix.

torch.Size([3362, 2])
We keep 2.32e+05/2.80e+06 =  8% of the original kernel matrix.

torch.Size([6902, 2])
We keep 8.27e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([5793, 2])
We keep 6.90e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([8837, 2])
We keep 1.35e+06/3.06e+07 =  4% of the original kernel matrix.

torch.Size([6705, 2])
We keep 8.19e+05/1.42e+07 =  5% of the original kernel matrix.

torch.Size([9576, 2])
We keep 1.54e+06/3.63e+07 =  4% of the original kernel matrix.

torch.Size([4171, 2])
We keep 3.53e+05/4.14e+06 =  8% of the original kernel matrix.

torch.Size([7550, 2])
We keep 9.84e+05/1.96e+07 =  5% of the original kernel matrix.

torch.Size([2961, 2])
We keep 3.66e+05/2.07e+06 = 17% of the original kernel matrix.

torch.Size([6552, 2])
We keep 7.25e+05/1.38e+07 =  5% of the original kernel matrix.

torch.Size([2814, 2])
We keep 1.92e+05/1.92e+06 =  9% of the original kernel matrix.

torch.Size([6369, 2])
We keep 7.46e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([3176, 2])
We keep 2.32e+05/2.59e+06 =  8% of the original kernel matrix.

torch.Size([6711, 2])
We keep 8.21e+05/1.55e+07 =  5% of the original kernel matrix.

torch.Size([3147, 2])
We keep 2.25e+05/2.38e+06 =  9% of the original kernel matrix.

torch.Size([6618, 2])
We keep 8.06e+05/1.49e+07 =  5% of the original kernel matrix.

torch.Size([3171, 2])
We keep 2.30e+05/2.46e+06 =  9% of the original kernel matrix.

torch.Size([6680, 2])
We keep 8.16e+05/1.51e+07 =  5% of the original kernel matrix.

torch.Size([5803, 2])
We keep 7.12e+05/1.08e+07 =  6% of the original kernel matrix.

torch.Size([8830, 2])
We keep 1.39e+06/3.16e+07 =  4% of the original kernel matrix.

torch.Size([866, 2])
We keep 1.67e+04/9.86e+04 = 16% of the original kernel matrix.

torch.Size([4074, 2])
We keep 2.65e+05/3.02e+06 =  8% of the original kernel matrix.

torch.Size([1772, 2])
We keep 7.42e+04/6.07e+05 = 12% of the original kernel matrix.

torch.Size([5385, 2])
We keep 4.89e+05/7.50e+06 =  6% of the original kernel matrix.

torch.Size([3885, 2])
We keep 2.77e+05/3.38e+06 =  8% of the original kernel matrix.

torch.Size([7455, 2])
We keep 9.01e+05/1.77e+07 =  5% of the original kernel matrix.

torch.Size([9461, 2])
We keep 1.82e+06/3.49e+07 =  5% of the original kernel matrix.

torch.Size([11471, 2])
We keep 2.14e+06/5.69e+07 =  3% of the original kernel matrix.

torch.Size([2160, 2])
We keep 1.25e+05/1.07e+06 = 11% of the original kernel matrix.

torch.Size([5700, 2])
We keep 6.15e+05/9.98e+06 =  6% of the original kernel matrix.

torch.Size([1907, 2])
We keep 9.39e+04/7.81e+05 = 12% of the original kernel matrix.

torch.Size([5392, 2])
We keep 5.37e+05/8.51e+06 =  6% of the original kernel matrix.

torch.Size([3662, 2])
We keep 4.07e+05/4.41e+06 =  9% of the original kernel matrix.

torch.Size([7148, 2])
We keep 1.05e+06/2.02e+07 =  5% of the original kernel matrix.

torch.Size([1506, 2])
We keep 7.15e+04/5.34e+05 = 13% of the original kernel matrix.

torch.Size([4901, 2])
We keep 4.76e+05/7.04e+06 =  6% of the original kernel matrix.

torch.Size([1959, 2])
We keep 9.79e+04/8.97e+05 = 10% of the original kernel matrix.

torch.Size([5399, 2])
We keep 5.55e+05/9.12e+06 =  6% of the original kernel matrix.

torch.Size([1742, 2])
We keep 7.60e+04/6.34e+05 = 11% of the original kernel matrix.

torch.Size([5364, 2])
We keep 5.05e+05/7.67e+06 =  6% of the original kernel matrix.

torch.Size([2197, 2])
We keep 1.59e+05/1.30e+06 = 12% of the original kernel matrix.

torch.Size([5637, 2])
We keep 6.53e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([1216, 2])
We keep 3.68e+04/2.62e+05 = 14% of the original kernel matrix.

torch.Size([4500, 2])
We keep 3.71e+05/4.93e+06 =  7% of the original kernel matrix.

torch.Size([3402, 2])
We keep 3.55e+05/4.00e+06 =  8% of the original kernel matrix.

torch.Size([6893, 2])
We keep 1.01e+06/1.93e+07 =  5% of the original kernel matrix.

torch.Size([3894, 2])
We keep 2.99e+05/3.43e+06 =  8% of the original kernel matrix.

torch.Size([7497, 2])
We keep 9.19e+05/1.78e+07 =  5% of the original kernel matrix.

torch.Size([2468, 2])
We keep 1.13e+05/1.09e+06 = 10% of the original kernel matrix.

torch.Size([6072, 2])
We keep 5.96e+05/1.00e+07 =  5% of the original kernel matrix.

torch.Size([1821, 2])
We keep 7.22e+04/6.37e+05 = 11% of the original kernel matrix.

torch.Size([5451, 2])
We keep 5.02e+05/7.69e+06 =  6% of the original kernel matrix.

torch.Size([1657, 2])
We keep 6.69e+04/5.82e+05 = 11% of the original kernel matrix.

torch.Size([5254, 2])
We keep 4.88e+05/7.35e+06 =  6% of the original kernel matrix.

torch.Size([3209, 2])
We keep 2.17e+05/2.32e+06 =  9% of the original kernel matrix.

torch.Size([6773, 2])
We keep 7.95e+05/1.47e+07 =  5% of the original kernel matrix.

torch.Size([3383, 2])
We keep 2.68e+05/2.95e+06 =  9% of the original kernel matrix.

torch.Size([6807, 2])
We keep 8.56e+05/1.65e+07 =  5% of the original kernel matrix.

torch.Size([4686, 2])
We keep 5.00e+05/6.99e+06 =  7% of the original kernel matrix.

torch.Size([8060, 2])
We keep 1.19e+06/2.55e+07 =  4% of the original kernel matrix.

torch.Size([3296, 2])
We keep 2.32e+05/2.50e+06 =  9% of the original kernel matrix.

torch.Size([6847, 2])
We keep 8.28e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([5463, 2])
We keep 7.24e+05/9.98e+06 =  7% of the original kernel matrix.

torch.Size([8472, 2])
We keep 1.37e+06/3.04e+07 =  4% of the original kernel matrix.

torch.Size([3199, 2])
We keep 2.35e+05/2.54e+06 =  9% of the original kernel matrix.

torch.Size([6621, 2])
We keep 8.20e+05/1.54e+07 =  5% of the original kernel matrix.

torch.Size([3474, 2])
We keep 2.44e+05/2.78e+06 =  8% of the original kernel matrix.

torch.Size([7030, 2])
We keep 8.35e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([7936, 2])
We keep 1.16e+06/2.09e+07 =  5% of the original kernel matrix.

torch.Size([10520, 2])
We keep 1.79e+06/4.40e+07 =  4% of the original kernel matrix.

torch.Size([2381, 2])
We keep 1.42e+05/1.42e+06 =  9% of the original kernel matrix.

torch.Size([5917, 2])
We keep 6.65e+05/1.15e+07 =  5% of the original kernel matrix.

torch.Size([4165, 2])
We keep 3.66e+05/4.86e+06 =  7% of the original kernel matrix.

torch.Size([7532, 2])
We keep 1.02e+06/2.12e+07 =  4% of the original kernel matrix.

torch.Size([2196, 2])
We keep 1.32e+05/1.13e+06 = 11% of the original kernel matrix.

torch.Size([5711, 2])
We keep 6.25e+05/1.03e+07 =  6% of the original kernel matrix.

torch.Size([5018, 2])
We keep 5.26e+05/7.37e+06 =  7% of the original kernel matrix.

torch.Size([8250, 2])
We keep 1.19e+06/2.61e+07 =  4% of the original kernel matrix.

torch.Size([2978, 2])
We keep 1.80e+05/2.01e+06 =  8% of the original kernel matrix.

torch.Size([6575, 2])
We keep 7.36e+05/1.36e+07 =  5% of the original kernel matrix.

torch.Size([2759, 2])
We keep 1.78e+05/1.88e+06 =  9% of the original kernel matrix.

torch.Size([6405, 2])
We keep 7.50e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([1592, 2])
We keep 6.16e+04/5.10e+05 = 12% of the original kernel matrix.

torch.Size([5131, 2])
We keep 4.49e+05/6.88e+06 =  6% of the original kernel matrix.

torch.Size([2490, 2])
We keep 1.53e+05/1.39e+06 = 11% of the original kernel matrix.

torch.Size([6018, 2])
We keep 6.64e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([1601, 2])
We keep 5.90e+04/5.06e+05 = 11% of the original kernel matrix.

torch.Size([5159, 2])
We keep 4.53e+05/6.85e+06 =  6% of the original kernel matrix.

torch.Size([1997, 2])
We keep 9.69e+04/9.12e+05 = 10% of the original kernel matrix.

torch.Size([5586, 2])
We keep 5.56e+05/9.20e+06 =  6% of the original kernel matrix.

torch.Size([1727, 2])
We keep 8.98e+04/7.14e+05 = 12% of the original kernel matrix.

torch.Size([5087, 2])
We keep 5.22e+05/8.14e+06 =  6% of the original kernel matrix.

torch.Size([2252, 2])
We keep 1.34e+05/1.18e+06 = 11% of the original kernel matrix.

torch.Size([5791, 2])
We keep 6.25e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([2449, 2])
We keep 1.59e+05/1.58e+06 = 10% of the original kernel matrix.

torch.Size([6000, 2])
We keep 6.78e+05/1.21e+07 =  5% of the original kernel matrix.

torch.Size([4254, 2])
We keep 4.76e+05/6.07e+06 =  7% of the original kernel matrix.

torch.Size([7518, 2])
We keep 1.13e+06/2.37e+07 =  4% of the original kernel matrix.

torch.Size([3855, 2])
We keep 3.61e+05/4.15e+06 =  8% of the original kernel matrix.

torch.Size([7184, 2])
We keep 9.75e+05/1.96e+07 =  4% of the original kernel matrix.

torch.Size([2968, 2])
We keep 1.98e+05/2.13e+06 =  9% of the original kernel matrix.

torch.Size([6597, 2])
We keep 7.57e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([709, 2])
We keep 1.43e+04/8.41e+04 = 16% of the original kernel matrix.

torch.Size([3769, 2])
We keep 2.50e+05/2.79e+06 =  8% of the original kernel matrix.

torch.Size([8197, 2])
We keep 1.28e+06/2.50e+07 =  5% of the original kernel matrix.

torch.Size([10781, 2])
We keep 1.94e+06/4.81e+07 =  4% of the original kernel matrix.

torch.Size([1295, 2])
We keep 4.36e+04/3.07e+05 = 14% of the original kernel matrix.

torch.Size([4620, 2])
We keep 3.90e+05/5.34e+06 =  7% of the original kernel matrix.

torch.Size([8101, 2])
We keep 1.20e+06/2.33e+07 =  5% of the original kernel matrix.

torch.Size([10640, 2])
We keep 1.86e+06/4.65e+07 =  4% of the original kernel matrix.

torch.Size([2211, 2])
We keep 1.28e+05/1.23e+06 = 10% of the original kernel matrix.

torch.Size([5656, 2])
We keep 6.21e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([1980, 2])
We keep 1.05e+05/9.39e+05 = 11% of the original kernel matrix.

torch.Size([5440, 2])
We keep 5.73e+05/9.33e+06 =  6% of the original kernel matrix.

torch.Size([2014, 2])
We keep 8.82e+04/7.76e+05 = 11% of the original kernel matrix.

torch.Size([5689, 2])
We keep 5.34e+05/8.48e+06 =  6% of the original kernel matrix.

torch.Size([1766, 2])
We keep 7.81e+04/6.38e+05 = 12% of the original kernel matrix.

torch.Size([5291, 2])
We keep 5.02e+05/7.70e+06 =  6% of the original kernel matrix.

torch.Size([2018, 2])
We keep 9.45e+04/8.82e+05 = 10% of the original kernel matrix.

torch.Size([5577, 2])
We keep 5.56e+05/9.04e+06 =  6% of the original kernel matrix.

torch.Size([1611, 2])
We keep 5.76e+04/4.60e+05 = 12% of the original kernel matrix.

torch.Size([5064, 2])
We keep 4.38e+05/6.53e+06 =  6% of the original kernel matrix.

torch.Size([2709, 2])
We keep 1.73e+05/1.69e+06 = 10% of the original kernel matrix.

torch.Size([6256, 2])
We keep 7.15e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([7198, 2])
We keep 1.08e+06/1.84e+07 =  5% of the original kernel matrix.

torch.Size([9979, 2])
We keep 1.73e+06/4.13e+07 =  4% of the original kernel matrix.

torch.Size([2307, 2])
We keep 1.51e+05/1.38e+06 = 10% of the original kernel matrix.

torch.Size([5771, 2])
We keep 6.62e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([7177, 2])
We keep 1.04e+06/1.85e+07 =  5% of the original kernel matrix.

torch.Size([9996, 2])
We keep 1.72e+06/4.14e+07 =  4% of the original kernel matrix.

torch.Size([2104, 2])
We keep 1.20e+05/1.06e+06 = 11% of the original kernel matrix.

torch.Size([5610, 2])
We keep 6.02e+05/9.93e+06 =  6% of the original kernel matrix.

torch.Size([3798, 2])
We keep 2.80e+05/3.36e+06 =  8% of the original kernel matrix.

torch.Size([7340, 2])
We keep 9.01e+05/1.77e+07 =  5% of the original kernel matrix.

torch.Size([2582, 2])
We keep 1.65e+05/1.63e+06 = 10% of the original kernel matrix.

torch.Size([6003, 2])
We keep 6.95e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([3276, 2])
We keep 2.38e+05/2.62e+06 =  9% of the original kernel matrix.

torch.Size([6804, 2])
We keep 8.33e+05/1.56e+07 =  5% of the original kernel matrix.

torch.Size([1811, 2])
We keep 7.40e+04/6.32e+05 = 11% of the original kernel matrix.

torch.Size([5384, 2])
We keep 4.97e+05/7.66e+06 =  6% of the original kernel matrix.

torch.Size([2796, 2])
We keep 1.43e+05/1.43e+06 =  9% of the original kernel matrix.

torch.Size([6385, 2])
We keep 6.57e+05/1.15e+07 =  5% of the original kernel matrix.

torch.Size([2329, 2])
We keep 1.38e+05/1.24e+06 = 11% of the original kernel matrix.

torch.Size([5887, 2])
We keep 6.37e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([1871, 2])
We keep 7.04e+04/6.10e+05 = 11% of the original kernel matrix.

torch.Size([5414, 2])
We keep 4.89e+05/7.52e+06 =  6% of the original kernel matrix.

torch.Size([1867, 2])
We keep 7.33e+04/6.50e+05 = 11% of the original kernel matrix.

torch.Size([5486, 2])
We keep 5.03e+05/7.76e+06 =  6% of the original kernel matrix.

torch.Size([3001, 2])
We keep 2.13e+05/2.28e+06 =  9% of the original kernel matrix.

torch.Size([6596, 2])
We keep 8.00e+05/1.46e+07 =  5% of the original kernel matrix.

torch.Size([895, 2])
We keep 1.88e+04/1.21e+05 = 15% of the original kernel matrix.

torch.Size([4194, 2])
We keep 2.85e+05/3.35e+06 =  8% of the original kernel matrix.

torch.Size([6848, 2])
We keep 1.31e+06/2.21e+07 =  5% of the original kernel matrix.

torch.Size([9734, 2])
We keep 1.89e+06/4.53e+07 =  4% of the original kernel matrix.

torch.Size([4389, 2])
We keep 5.92e+05/7.27e+06 =  8% of the original kernel matrix.

torch.Size([7708, 2])
We keep 1.27e+06/2.60e+07 =  4% of the original kernel matrix.

torch.Size([4963, 2])
We keep 5.18e+05/6.83e+06 =  7% of the original kernel matrix.

torch.Size([8214, 2])
We keep 1.19e+06/2.52e+07 =  4% of the original kernel matrix.

torch.Size([1442, 2])
We keep 4.63e+04/3.86e+05 = 12% of the original kernel matrix.

torch.Size([4963, 2])
We keep 4.10e+05/5.98e+06 =  6% of the original kernel matrix.

torch.Size([2718, 2])
We keep 1.73e+05/1.65e+06 = 10% of the original kernel matrix.

torch.Size([6308, 2])
We keep 6.94e+05/1.24e+07 =  5% of the original kernel matrix.

torch.Size([1994, 2])
We keep 8.88e+04/8.43e+05 = 10% of the original kernel matrix.

torch.Size([5467, 2])
We keep 5.40e+05/8.84e+06 =  6% of the original kernel matrix.

torch.Size([1605, 2])
We keep 7.61e+04/5.90e+05 = 12% of the original kernel matrix.

torch.Size([5086, 2])
We keep 4.80e+05/7.40e+06 =  6% of the original kernel matrix.

torch.Size([2122, 2])
We keep 1.18e+05/1.08e+06 = 10% of the original kernel matrix.

torch.Size([5620, 2])
We keep 6.02e+05/9.99e+06 =  6% of the original kernel matrix.

torch.Size([3443, 2])
We keep 2.88e+05/3.18e+06 =  9% of the original kernel matrix.

torch.Size([6913, 2])
We keep 8.86e+05/1.72e+07 =  5% of the original kernel matrix.

torch.Size([4299, 2])
We keep 4.63e+05/5.64e+06 =  8% of the original kernel matrix.

torch.Size([7705, 2])
We keep 1.14e+06/2.29e+07 =  4% of the original kernel matrix.

torch.Size([3325, 2])
We keep 2.13e+05/2.41e+06 =  8% of the original kernel matrix.

torch.Size([6825, 2])
We keep 7.88e+05/1.49e+07 =  5% of the original kernel matrix.

torch.Size([6520, 2])
We keep 1.18e+06/1.86e+07 =  6% of the original kernel matrix.

torch.Size([9321, 2])
We keep 1.75e+06/4.15e+07 =  4% of the original kernel matrix.

torch.Size([3676, 2])
We keep 2.86e+05/3.33e+06 =  8% of the original kernel matrix.

torch.Size([7065, 2])
We keep 8.89e+05/1.76e+07 =  5% of the original kernel matrix.

torch.Size([12186, 2])
We keep 3.89e+06/9.35e+07 =  4% of the original kernel matrix.

torch.Size([12985, 2])
We keep 3.30e+06/9.32e+07 =  3% of the original kernel matrix.

torch.Size([3279, 2])
We keep 2.06e+05/2.34e+06 =  8% of the original kernel matrix.

torch.Size([6880, 2])
We keep 7.85e+05/1.47e+07 =  5% of the original kernel matrix.

torch.Size([4725, 2])
We keep 5.15e+05/6.40e+06 =  8% of the original kernel matrix.

torch.Size([8055, 2])
We keep 1.17e+06/2.44e+07 =  4% of the original kernel matrix.

torch.Size([2709, 2])
We keep 1.58e+05/1.53e+06 = 10% of the original kernel matrix.

torch.Size([6225, 2])
We keep 6.77e+05/1.19e+07 =  5% of the original kernel matrix.

torch.Size([871, 2])
We keep 1.76e+04/1.08e+05 = 16% of the original kernel matrix.

torch.Size([4113, 2])
We keep 2.72e+05/3.17e+06 =  8% of the original kernel matrix.

torch.Size([6412, 2])
We keep 9.31e+05/1.49e+07 =  6% of the original kernel matrix.

torch.Size([9285, 2])
We keep 1.58e+06/3.72e+07 =  4% of the original kernel matrix.

torch.Size([1744, 2])
We keep 6.59e+04/5.60e+05 = 11% of the original kernel matrix.

torch.Size([5222, 2])
We keep 4.73e+05/7.20e+06 =  6% of the original kernel matrix.

torch.Size([3101, 2])
We keep 2.45e+05/2.79e+06 =  8% of the original kernel matrix.

torch.Size([6560, 2])
We keep 8.41e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([1276, 2])
We keep 3.61e+04/2.48e+05 = 14% of the original kernel matrix.

torch.Size([4674, 2])
We keep 3.60e+05/4.80e+06 =  7% of the original kernel matrix.

torch.Size([4282, 2])
We keep 4.04e+05/5.42e+06 =  7% of the original kernel matrix.

torch.Size([7569, 2])
We keep 1.04e+06/2.24e+07 =  4% of the original kernel matrix.

torch.Size([2101, 2])
We keep 1.01e+05/9.25e+05 = 10% of the original kernel matrix.

torch.Size([5732, 2])
We keep 5.73e+05/9.27e+06 =  6% of the original kernel matrix.

torch.Size([2066, 2])
We keep 8.88e+04/8.61e+05 = 10% of the original kernel matrix.

torch.Size([5750, 2])
We keep 5.53e+05/8.94e+06 =  6% of the original kernel matrix.

torch.Size([4141, 2])
We keep 3.48e+05/4.12e+06 =  8% of the original kernel matrix.

torch.Size([7498, 2])
We keep 9.80e+05/1.95e+07 =  5% of the original kernel matrix.

torch.Size([3657, 2])
We keep 3.43e+05/4.27e+06 =  8% of the original kernel matrix.

torch.Size([7044, 2])
We keep 9.90e+05/1.99e+07 =  4% of the original kernel matrix.

torch.Size([1099, 2])
We keep 2.73e+04/1.87e+05 = 14% of the original kernel matrix.

torch.Size([4445, 2])
We keep 3.26e+05/4.17e+06 =  7% of the original kernel matrix.

torch.Size([766, 2])
We keep 1.28e+04/7.02e+04 = 18% of the original kernel matrix.

torch.Size([3940, 2])
We keep 2.39e+05/2.55e+06 =  9% of the original kernel matrix.

torch.Size([1076, 2])
We keep 3.22e+04/2.31e+05 = 13% of the original kernel matrix.

torch.Size([4311, 2])
We keep 3.47e+05/4.63e+06 =  7% of the original kernel matrix.

torch.Size([5743, 2])
We keep 8.11e+05/1.19e+07 =  6% of the original kernel matrix.

torch.Size([8846, 2])
We keep 1.49e+06/3.32e+07 =  4% of the original kernel matrix.

torch.Size([1822, 2])
We keep 6.64e+04/5.98e+05 = 11% of the original kernel matrix.

torch.Size([5459, 2])
We keep 4.83e+05/7.44e+06 =  6% of the original kernel matrix.

torch.Size([3893, 2])
We keep 3.57e+05/4.26e+06 =  8% of the original kernel matrix.

torch.Size([7321, 2])
We keep 9.96e+05/1.99e+07 =  5% of the original kernel matrix.

torch.Size([2558, 2])
We keep 1.48e+05/1.59e+06 =  9% of the original kernel matrix.

torch.Size([6187, 2])
We keep 6.86e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([6510, 2])
We keep 9.55e+05/1.58e+07 =  6% of the original kernel matrix.

torch.Size([9403, 2])
We keep 1.59e+06/3.83e+07 =  4% of the original kernel matrix.

torch.Size([2567, 2])
We keep 1.62e+05/1.69e+06 =  9% of the original kernel matrix.

torch.Size([6012, 2])
We keep 6.98e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([5289, 2])
We keep 6.83e+05/9.33e+06 =  7% of the original kernel matrix.

torch.Size([8452, 2])
We keep 1.32e+06/2.94e+07 =  4% of the original kernel matrix.

torch.Size([4244, 2])
We keep 3.39e+05/4.45e+06 =  7% of the original kernel matrix.

torch.Size([7643, 2])
We keep 9.95e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([3898, 2])
We keep 3.23e+05/4.09e+06 =  7% of the original kernel matrix.

torch.Size([7339, 2])
We keep 9.61e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([3418, 2])
We keep 2.54e+05/2.89e+06 =  8% of the original kernel matrix.

torch.Size([7015, 2])
We keep 8.65e+05/1.64e+07 =  5% of the original kernel matrix.

torch.Size([5520, 2])
We keep 6.54e+05/9.11e+06 =  7% of the original kernel matrix.

torch.Size([8611, 2])
We keep 1.31e+06/2.91e+07 =  4% of the original kernel matrix.

torch.Size([2014, 2])
We keep 8.08e+04/7.53e+05 = 10% of the original kernel matrix.

torch.Size([5549, 2])
We keep 5.20e+05/8.36e+06 =  6% of the original kernel matrix.

torch.Size([3497, 2])
We keep 2.79e+05/3.27e+06 =  8% of the original kernel matrix.

torch.Size([7056, 2])
We keep 8.90e+05/1.74e+07 =  5% of the original kernel matrix.

torch.Size([3447, 2])
We keep 3.02e+05/3.43e+06 =  8% of the original kernel matrix.

torch.Size([6961, 2])
We keep 9.34e+05/1.78e+07 =  5% of the original kernel matrix.

torch.Size([2290, 2])
We keep 1.33e+05/1.23e+06 = 10% of the original kernel matrix.

torch.Size([5891, 2])
We keep 6.44e+05/1.07e+07 =  6% of the original kernel matrix.

torch.Size([5046, 2])
We keep 5.80e+05/7.75e+06 =  7% of the original kernel matrix.

torch.Size([8255, 2])
We keep 1.25e+06/2.68e+07 =  4% of the original kernel matrix.

torch.Size([5352, 2])
We keep 5.65e+05/7.97e+06 =  7% of the original kernel matrix.

torch.Size([8596, 2])
We keep 1.25e+06/2.72e+07 =  4% of the original kernel matrix.

torch.Size([3688, 2])
We keep 2.93e+05/3.43e+06 =  8% of the original kernel matrix.

torch.Size([7193, 2])
We keep 9.14e+05/1.78e+07 =  5% of the original kernel matrix.

torch.Size([2286, 2])
We keep 1.28e+05/1.26e+06 = 10% of the original kernel matrix.

torch.Size([5812, 2])
We keep 6.33e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([996, 2])
We keep 2.46e+04/1.59e+05 = 15% of the original kernel matrix.

torch.Size([4252, 2])
We keep 3.09e+05/3.84e+06 =  8% of the original kernel matrix.

torch.Size([2752, 2])
We keep 1.27e+05/1.38e+06 =  9% of the original kernel matrix.

torch.Size([6426, 2])
We keep 6.38e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([5118, 2])
We keep 5.55e+05/8.27e+06 =  6% of the original kernel matrix.

torch.Size([8427, 2])
We keep 1.24e+06/2.77e+07 =  4% of the original kernel matrix.

torch.Size([6669, 2])
We keep 9.63e+05/1.60e+07 =  6% of the original kernel matrix.

torch.Size([9574, 2])
We keep 1.63e+06/3.85e+07 =  4% of the original kernel matrix.

torch.Size([3160, 2])
We keep 2.27e+05/2.59e+06 =  8% of the original kernel matrix.

torch.Size([6613, 2])
We keep 8.10e+05/1.55e+07 =  5% of the original kernel matrix.

torch.Size([2397, 2])
We keep 1.17e+05/1.16e+06 = 10% of the original kernel matrix.

torch.Size([5951, 2])
We keep 6.03e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([1691, 2])
We keep 6.56e+04/5.96e+05 = 11% of the original kernel matrix.

torch.Size([5326, 2])
We keep 4.91e+05/7.44e+06 =  6% of the original kernel matrix.

torch.Size([3293, 2])
We keep 1.92e+05/2.13e+06 =  8% of the original kernel matrix.

torch.Size([6842, 2])
We keep 7.58e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([1741, 2])
We keep 6.32e+04/5.24e+05 = 12% of the original kernel matrix.

torch.Size([5337, 2])
We keep 4.67e+05/6.97e+06 =  6% of the original kernel matrix.

torch.Size([5277, 2])
We keep 6.09e+05/9.33e+06 =  6% of the original kernel matrix.

torch.Size([8617, 2])
We keep 1.34e+06/2.94e+07 =  4% of the original kernel matrix.

torch.Size([2070, 2])
We keep 9.44e+04/9.55e+05 =  9% of the original kernel matrix.

torch.Size([5684, 2])
We keep 5.67e+05/9.41e+06 =  6% of the original kernel matrix.

torch.Size([6250, 2])
We keep 8.82e+05/1.38e+07 =  6% of the original kernel matrix.

torch.Size([9319, 2])
We keep 1.59e+06/3.58e+07 =  4% of the original kernel matrix.

torch.Size([3068, 2])
We keep 2.17e+05/2.22e+06 =  9% of the original kernel matrix.

torch.Size([6550, 2])
We keep 7.83e+05/1.43e+07 =  5% of the original kernel matrix.

torch.Size([4191, 2])
We keep 3.91e+05/4.97e+06 =  7% of the original kernel matrix.

torch.Size([7631, 2])
We keep 1.05e+06/2.15e+07 =  4% of the original kernel matrix.

torch.Size([1911, 2])
We keep 8.93e+04/7.96e+05 = 11% of the original kernel matrix.

torch.Size([5421, 2])
We keep 5.41e+05/8.59e+06 =  6% of the original kernel matrix.

torch.Size([4860, 2])
We keep 5.30e+05/7.24e+06 =  7% of the original kernel matrix.

torch.Size([8176, 2])
We keep 1.21e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([3032, 2])
We keep 2.34e+05/2.41e+06 =  9% of the original kernel matrix.

torch.Size([6601, 2])
We keep 8.17e+05/1.49e+07 =  5% of the original kernel matrix.

torch.Size([1877, 2])
We keep 6.82e+04/6.02e+05 = 11% of the original kernel matrix.

torch.Size([5466, 2])
We keep 4.82e+05/7.47e+06 =  6% of the original kernel matrix.

torch.Size([5352, 2])
We keep 6.97e+05/1.03e+07 =  6% of the original kernel matrix.

torch.Size([8559, 2])
We keep 1.40e+06/3.09e+07 =  4% of the original kernel matrix.

torch.Size([4444, 2])
We keep 4.36e+05/5.68e+06 =  7% of the original kernel matrix.

torch.Size([7697, 2])
We keep 1.08e+06/2.30e+07 =  4% of the original kernel matrix.

torch.Size([7135, 2])
We keep 1.14e+06/1.94e+07 =  5% of the original kernel matrix.

torch.Size([9905, 2])
We keep 1.73e+06/4.24e+07 =  4% of the original kernel matrix.

torch.Size([4251, 2])
We keep 3.10e+05/3.96e+06 =  7% of the original kernel matrix.

torch.Size([7639, 2])
We keep 9.47e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([4690, 2])
We keep 3.88e+05/5.17e+06 =  7% of the original kernel matrix.

torch.Size([7989, 2])
We keep 1.05e+06/2.19e+07 =  4% of the original kernel matrix.

torch.Size([844, 2])
We keep 2.01e+04/1.22e+05 = 16% of the original kernel matrix.

torch.Size([3925, 2])
We keep 2.83e+05/3.36e+06 =  8% of the original kernel matrix.

torch.Size([1979, 2])
We keep 7.83e+04/6.89e+05 = 11% of the original kernel matrix.

torch.Size([5517, 2])
We keep 5.04e+05/7.99e+06 =  6% of the original kernel matrix.

torch.Size([877, 2])
We keep 1.84e+04/1.09e+05 = 16% of the original kernel matrix.

torch.Size([4131, 2])
We keep 2.77e+05/3.18e+06 =  8% of the original kernel matrix.

torch.Size([1615, 2])
We keep 5.93e+04/4.82e+05 = 12% of the original kernel matrix.

torch.Size([5212, 2])
We keep 4.57e+05/6.68e+06 =  6% of the original kernel matrix.

torch.Size([5119, 2])
We keep 6.95e+05/7.91e+06 =  8% of the original kernel matrix.

torch.Size([8363, 2])
We keep 1.26e+06/2.71e+07 =  4% of the original kernel matrix.

torch.Size([4460, 2])
We keep 3.96e+05/5.52e+06 =  7% of the original kernel matrix.

torch.Size([7781, 2])
We keep 1.08e+06/2.26e+07 =  4% of the original kernel matrix.

torch.Size([3725, 2])
We keep 3.02e+05/3.55e+06 =  8% of the original kernel matrix.

torch.Size([7151, 2])
We keep 9.25e+05/1.81e+07 =  5% of the original kernel matrix.

torch.Size([1289, 2])
We keep 4.96e+04/3.96e+05 = 12% of the original kernel matrix.

torch.Size([4734, 2])
We keep 4.31e+05/6.06e+06 =  7% of the original kernel matrix.

torch.Size([4146, 2])
We keep 4.28e+05/5.06e+06 =  8% of the original kernel matrix.

torch.Size([7507, 2])
We keep 1.07e+06/2.17e+07 =  4% of the original kernel matrix.

torch.Size([4280, 2])
We keep 5.33e+05/6.80e+06 =  7% of the original kernel matrix.

torch.Size([7584, 2])
We keep 1.21e+06/2.51e+07 =  4% of the original kernel matrix.

torch.Size([3328, 2])
We keep 3.20e+05/3.56e+06 =  9% of the original kernel matrix.

torch.Size([6700, 2])
We keep 9.30e+05/1.82e+07 =  5% of the original kernel matrix.

torch.Size([2757, 2])
We keep 2.00e+05/2.03e+06 =  9% of the original kernel matrix.

torch.Size([6219, 2])
We keep 7.41e+05/1.37e+07 =  5% of the original kernel matrix.

torch.Size([1663, 2])
We keep 6.29e+04/5.45e+05 = 11% of the original kernel matrix.

torch.Size([5192, 2])
We keep 4.66e+05/7.11e+06 =  6% of the original kernel matrix.

torch.Size([1605, 2])
We keep 5.81e+04/4.71e+05 = 12% of the original kernel matrix.

torch.Size([5103, 2])
We keep 4.45e+05/6.61e+06 =  6% of the original kernel matrix.

torch.Size([3304, 2])
We keep 2.60e+05/2.86e+06 =  9% of the original kernel matrix.

torch.Size([6806, 2])
We keep 8.52e+05/1.63e+07 =  5% of the original kernel matrix.

torch.Size([1013, 2])
We keep 2.60e+04/1.69e+05 = 15% of the original kernel matrix.

torch.Size([4313, 2])
We keep 3.16e+05/3.96e+06 =  7% of the original kernel matrix.

torch.Size([2941, 2])
We keep 2.18e+05/2.15e+06 = 10% of the original kernel matrix.

torch.Size([6452, 2])
We keep 7.66e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([2738, 2])
We keep 2.11e+05/2.18e+06 =  9% of the original kernel matrix.

torch.Size([6175, 2])
We keep 7.75e+05/1.42e+07 =  5% of the original kernel matrix.

torch.Size([2425, 2])
We keep 1.42e+05/1.39e+06 = 10% of the original kernel matrix.

torch.Size([6045, 2])
We keep 6.75e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([5384, 2])
We keep 7.01e+05/9.07e+06 =  7% of the original kernel matrix.

torch.Size([8584, 2])
We keep 1.30e+06/2.90e+07 =  4% of the original kernel matrix.

torch.Size([2723, 2])
We keep 1.90e+05/1.90e+06 = 10% of the original kernel matrix.

torch.Size([6248, 2])
We keep 7.44e+05/1.33e+07 =  5% of the original kernel matrix.

torch.Size([3626, 2])
We keep 3.26e+05/3.78e+06 =  8% of the original kernel matrix.

torch.Size([6999, 2])
We keep 9.49e+05/1.87e+07 =  5% of the original kernel matrix.

torch.Size([6260, 2])
We keep 9.84e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([9404, 2])
We keep 1.70e+06/3.93e+07 =  4% of the original kernel matrix.

torch.Size([6189, 2])
We keep 6.67e+05/1.07e+07 =  6% of the original kernel matrix.

torch.Size([9256, 2])
We keep 1.38e+06/3.15e+07 =  4% of the original kernel matrix.

torch.Size([1828, 2])
We keep 7.29e+04/6.51e+05 = 11% of the original kernel matrix.

torch.Size([5394, 2])
We keep 4.96e+05/7.77e+06 =  6% of the original kernel matrix.

torch.Size([16903, 2])
We keep 6.91e+06/1.58e+08 =  4% of the original kernel matrix.

torch.Size([15466, 2])
We keep 4.08e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([50235, 2])
We keep 5.46e+07/1.73e+09 =  3% of the original kernel matrix.

torch.Size([26266, 2])
We keep 1.11e+07/4.00e+08 =  2% of the original kernel matrix.

torch.Size([3368, 2])
We keep 3.24e+05/2.97e+06 = 10% of the original kernel matrix.

torch.Size([6908, 2])
We keep 8.20e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([5384, 2])
We keep 6.88e+05/9.28e+06 =  7% of the original kernel matrix.

torch.Size([8520, 2])
We keep 1.32e+06/2.93e+07 =  4% of the original kernel matrix.

torch.Size([789, 2])
We keep 1.41e+04/8.64e+04 = 16% of the original kernel matrix.

torch.Size([3897, 2])
We keep 2.46e+05/2.83e+06 =  8% of the original kernel matrix.

torch.Size([4595, 2])
We keep 4.46e+05/6.21e+06 =  7% of the original kernel matrix.

torch.Size([7916, 2])
We keep 1.12e+06/2.40e+07 =  4% of the original kernel matrix.

torch.Size([6286, 2])
We keep 8.31e+05/1.35e+07 =  6% of the original kernel matrix.

torch.Size([9265, 2])
We keep 1.52e+06/3.54e+07 =  4% of the original kernel matrix.

torch.Size([192638, 2])
We keep 5.07e+08/2.15e+10 =  2% of the original kernel matrix.

torch.Size([53533, 2])
We keep 3.36e+07/1.41e+09 =  2% of the original kernel matrix.

torch.Size([2725, 2])
We keep 1.67e+05/1.75e+06 =  9% of the original kernel matrix.

torch.Size([6366, 2])
We keep 7.15e+05/1.27e+07 =  5% of the original kernel matrix.

torch.Size([121498, 2])
We keep 2.07e+08/9.31e+09 =  2% of the original kernel matrix.

torch.Size([42614, 2])
We keep 2.32e+07/9.29e+08 =  2% of the original kernel matrix.

torch.Size([9468, 2])
We keep 2.29e+06/4.35e+07 =  5% of the original kernel matrix.

torch.Size([11535, 2])
We keep 2.37e+06/6.35e+07 =  3% of the original kernel matrix.

torch.Size([16621, 2])
We keep 8.34e+06/1.54e+08 =  5% of the original kernel matrix.

torch.Size([15231, 2])
We keep 3.91e+06/1.20e+08 =  3% of the original kernel matrix.

torch.Size([13729, 2])
We keep 4.76e+06/9.77e+07 =  4% of the original kernel matrix.

torch.Size([13976, 2])
We keep 3.30e+06/9.52e+07 =  3% of the original kernel matrix.

torch.Size([2337, 2])
We keep 1.09e+05/1.02e+06 = 10% of the original kernel matrix.

torch.Size([5974, 2])
We keep 5.85e+05/9.73e+06 =  6% of the original kernel matrix.

torch.Size([4232, 2])
We keep 4.59e+05/5.34e+06 =  8% of the original kernel matrix.

torch.Size([7571, 2])
We keep 1.09e+06/2.23e+07 =  4% of the original kernel matrix.

torch.Size([90909, 2])
We keep 1.14e+08/5.05e+09 =  2% of the original kernel matrix.

torch.Size([36578, 2])
We keep 1.78e+07/6.84e+08 =  2% of the original kernel matrix.

torch.Size([220031, 2])
We keep 6.33e+08/2.29e+10 =  2% of the original kernel matrix.

torch.Size([57467, 2])
We keep 3.36e+07/1.46e+09 =  2% of the original kernel matrix.

torch.Size([3832, 2])
We keep 3.51e+05/4.26e+06 =  8% of the original kernel matrix.

torch.Size([7332, 2])
We keep 9.91e+05/1.99e+07 =  4% of the original kernel matrix.

torch.Size([22593, 2])
We keep 9.29e+06/2.68e+08 =  3% of the original kernel matrix.

torch.Size([18046, 2])
We keep 5.06e+06/1.58e+08 =  3% of the original kernel matrix.

torch.Size([8632, 2])
We keep 2.36e+06/3.66e+07 =  6% of the original kernel matrix.

torch.Size([11000, 2])
We keep 2.25e+06/5.83e+07 =  3% of the original kernel matrix.

torch.Size([13488, 2])
We keep 4.73e+06/8.67e+07 =  5% of the original kernel matrix.

torch.Size([13891, 2])
We keep 3.21e+06/8.97e+07 =  3% of the original kernel matrix.

torch.Size([79233, 2])
We keep 1.18e+08/4.64e+09 =  2% of the original kernel matrix.

torch.Size([33677, 2])
We keep 1.74e+07/6.56e+08 =  2% of the original kernel matrix.

torch.Size([19516, 2])
We keep 1.53e+07/2.48e+08 =  6% of the original kernel matrix.

torch.Size([15828, 2])
We keep 4.81e+06/1.52e+08 =  3% of the original kernel matrix.

torch.Size([14960, 2])
We keep 4.34e+06/1.07e+08 =  4% of the original kernel matrix.

torch.Size([14760, 2])
We keep 3.38e+06/9.98e+07 =  3% of the original kernel matrix.

torch.Size([12941, 2])
We keep 4.27e+06/8.21e+07 =  5% of the original kernel matrix.

torch.Size([13561, 2])
We keep 3.16e+06/8.73e+07 =  3% of the original kernel matrix.

torch.Size([3337, 2])
We keep 2.25e+05/2.46e+06 =  9% of the original kernel matrix.

torch.Size([6851, 2])
We keep 7.88e+05/1.51e+07 =  5% of the original kernel matrix.

torch.Size([124287, 2])
We keep 2.26e+08/1.03e+10 =  2% of the original kernel matrix.

torch.Size([42688, 2])
We keep 2.41e+07/9.80e+08 =  2% of the original kernel matrix.

torch.Size([3133, 2])
We keep 2.60e+05/2.66e+06 =  9% of the original kernel matrix.

torch.Size([6669, 2])
We keep 8.37e+05/1.57e+07 =  5% of the original kernel matrix.

torch.Size([3006, 2])
We keep 2.06e+05/2.19e+06 =  9% of the original kernel matrix.

torch.Size([6556, 2])
We keep 7.76e+05/1.43e+07 =  5% of the original kernel matrix.

torch.Size([18181, 2])
We keep 7.57e+06/1.75e+08 =  4% of the original kernel matrix.

torch.Size([16123, 2])
We keep 4.27e+06/1.28e+08 =  3% of the original kernel matrix.

torch.Size([46283, 2])
We keep 5.31e+07/1.62e+09 =  3% of the original kernel matrix.

torch.Size([24289, 2])
We keep 1.09e+07/3.88e+08 =  2% of the original kernel matrix.

torch.Size([94846, 2])
We keep 2.41e+08/6.35e+09 =  3% of the original kernel matrix.

torch.Size([37835, 2])
We keep 1.98e+07/7.67e+08 =  2% of the original kernel matrix.

torch.Size([44922, 2])
We keep 3.26e+07/1.19e+09 =  2% of the original kernel matrix.

torch.Size([24809, 2])
We keep 9.47e+06/3.33e+08 =  2% of the original kernel matrix.

torch.Size([3326, 2])
We keep 2.46e+05/2.74e+06 =  8% of the original kernel matrix.

torch.Size([6797, 2])
We keep 8.22e+05/1.59e+07 =  5% of the original kernel matrix.

torch.Size([20479, 2])
We keep 1.59e+07/2.68e+08 =  5% of the original kernel matrix.

torch.Size([16816, 2])
We keep 5.12e+06/1.58e+08 =  3% of the original kernel matrix.

torch.Size([4295, 2])
We keep 5.08e+05/5.84e+06 =  8% of the original kernel matrix.

torch.Size([7477, 2])
We keep 1.11e+06/2.33e+07 =  4% of the original kernel matrix.

torch.Size([29854, 2])
We keep 1.53e+07/4.76e+08 =  3% of the original kernel matrix.

torch.Size([20302, 2])
We keep 6.36e+06/2.10e+08 =  3% of the original kernel matrix.

torch.Size([26892, 2])
We keep 1.06e+07/3.63e+08 =  2% of the original kernel matrix.

torch.Size([19424, 2])
We keep 5.65e+06/1.84e+08 =  3% of the original kernel matrix.

torch.Size([5373, 2])
We keep 6.72e+05/9.68e+06 =  6% of the original kernel matrix.

torch.Size([8578, 2])
We keep 1.33e+06/3.00e+07 =  4% of the original kernel matrix.

torch.Size([37514, 2])
We keep 2.28e+07/6.35e+08 =  3% of the original kernel matrix.

torch.Size([22632, 2])
We keep 7.02e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([1096, 2])
We keep 2.39e+04/1.62e+05 = 14% of the original kernel matrix.

torch.Size([4441, 2])
We keep 3.05e+05/3.88e+06 =  7% of the original kernel matrix.

torch.Size([8019, 2])
We keep 1.44e+06/2.73e+07 =  5% of the original kernel matrix.

torch.Size([10665, 2])
We keep 2.00e+06/5.03e+07 =  3% of the original kernel matrix.

torch.Size([82008, 2])
We keep 9.12e+07/4.04e+09 =  2% of the original kernel matrix.

torch.Size([34261, 2])
We keep 1.60e+07/6.12e+08 =  2% of the original kernel matrix.

torch.Size([25053, 2])
We keep 1.61e+07/3.91e+08 =  4% of the original kernel matrix.

torch.Size([18358, 2])
We keep 5.95e+06/1.90e+08 =  3% of the original kernel matrix.

torch.Size([77019, 2])
We keep 8.64e+07/3.71e+09 =  2% of the original kernel matrix.

torch.Size([32313, 2])
We keep 1.54e+07/5.86e+08 =  2% of the original kernel matrix.

torch.Size([2502, 2])
We keep 1.53e+05/1.40e+06 = 10% of the original kernel matrix.

torch.Size([6064, 2])
We keep 6.59e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([123305, 2])
We keep 1.87e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([42863, 2])
We keep 2.41e+07/9.81e+08 =  2% of the original kernel matrix.

torch.Size([8971, 2])
We keep 1.44e+06/3.19e+07 =  4% of the original kernel matrix.

torch.Size([11336, 2])
We keep 2.11e+06/5.44e+07 =  3% of the original kernel matrix.

torch.Size([17451, 2])
We keep 6.58e+06/1.65e+08 =  3% of the original kernel matrix.

torch.Size([15741, 2])
We keep 4.12e+06/1.24e+08 =  3% of the original kernel matrix.

torch.Size([42192, 2])
We keep 3.67e+07/1.18e+09 =  3% of the original kernel matrix.

torch.Size([23989, 2])
We keep 9.57e+06/3.31e+08 =  2% of the original kernel matrix.

torch.Size([1849, 2])
We keep 8.47e+04/7.85e+05 = 10% of the original kernel matrix.

torch.Size([5333, 2])
We keep 5.32e+05/8.53e+06 =  6% of the original kernel matrix.

torch.Size([68486, 2])
We keep 6.48e+07/2.46e+09 =  2% of the original kernel matrix.

torch.Size([31167, 2])
We keep 1.30e+07/4.78e+08 =  2% of the original kernel matrix.

torch.Size([112518, 2])
We keep 2.54e+08/8.72e+09 =  2% of the original kernel matrix.

torch.Size([41065, 2])
We keep 2.32e+07/8.99e+08 =  2% of the original kernel matrix.

torch.Size([222460, 2])
We keep 4.82e+08/2.68e+10 =  1% of the original kernel matrix.

torch.Size([57809, 2])
We keep 3.74e+07/1.58e+09 =  2% of the original kernel matrix.

torch.Size([39671, 2])
We keep 3.50e+07/1.10e+09 =  3% of the original kernel matrix.

torch.Size([23107, 2])
We keep 9.34e+06/3.19e+08 =  2% of the original kernel matrix.

torch.Size([19916, 2])
We keep 8.47e+06/2.18e+08 =  3% of the original kernel matrix.

torch.Size([16611, 2])
We keep 4.64e+06/1.42e+08 =  3% of the original kernel matrix.

torch.Size([9752, 2])
We keep 4.21e+06/5.22e+07 =  8% of the original kernel matrix.

torch.Size([11594, 2])
We keep 2.68e+06/6.96e+07 =  3% of the original kernel matrix.

torch.Size([3833, 2])
We keep 3.11e+05/3.88e+06 =  8% of the original kernel matrix.

torch.Size([7318, 2])
We keep 9.50e+05/1.90e+07 =  5% of the original kernel matrix.

torch.Size([45476, 2])
We keep 6.17e+07/1.55e+09 =  3% of the original kernel matrix.

torch.Size([24759, 2])
We keep 1.05e+07/3.79e+08 =  2% of the original kernel matrix.

torch.Size([5836, 2])
We keep 7.40e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([8995, 2])
We keep 1.45e+06/3.41e+07 =  4% of the original kernel matrix.

torch.Size([14884, 2])
We keep 7.35e+06/1.27e+08 =  5% of the original kernel matrix.

torch.Size([14419, 2])
We keep 3.76e+06/1.09e+08 =  3% of the original kernel matrix.

torch.Size([20457, 2])
We keep 2.01e+07/2.29e+08 =  8% of the original kernel matrix.

torch.Size([17030, 2])
We keep 4.38e+06/1.46e+08 =  3% of the original kernel matrix.

torch.Size([8689, 2])
We keep 2.02e+06/2.87e+07 =  7% of the original kernel matrix.

torch.Size([11078, 2])
We keep 2.00e+06/5.16e+07 =  3% of the original kernel matrix.

torch.Size([9223, 2])
We keep 2.12e+06/4.19e+07 =  5% of the original kernel matrix.

torch.Size([11157, 2])
We keep 2.37e+06/6.24e+07 =  3% of the original kernel matrix.

torch.Size([4562, 2])
We keep 4.82e+05/6.75e+06 =  7% of the original kernel matrix.

torch.Size([7873, 2])
We keep 1.19e+06/2.50e+07 =  4% of the original kernel matrix.

torch.Size([32110, 2])
We keep 1.54e+08/2.50e+09 =  6% of the original kernel matrix.

torch.Size([20001, 2])
We keep 1.30e+07/4.81e+08 =  2% of the original kernel matrix.

torch.Size([7974, 2])
We keep 2.00e+06/2.87e+07 =  6% of the original kernel matrix.

torch.Size([10499, 2])
We keep 2.05e+06/5.16e+07 =  3% of the original kernel matrix.

torch.Size([12063, 2])
We keep 1.49e+07/8.82e+07 = 16% of the original kernel matrix.

torch.Size([13185, 2])
We keep 3.28e+06/9.04e+07 =  3% of the original kernel matrix.

torch.Size([7243, 2])
We keep 9.87e+05/1.79e+07 =  5% of the original kernel matrix.

torch.Size([10015, 2])
We keep 1.69e+06/4.07e+07 =  4% of the original kernel matrix.

torch.Size([36541, 2])
We keep 3.21e+07/8.15e+08 =  3% of the original kernel matrix.

torch.Size([22328, 2])
We keep 7.77e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([15389, 2])
We keep 5.06e+06/1.21e+08 =  4% of the original kernel matrix.

torch.Size([14950, 2])
We keep 3.68e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([96755, 2])
We keep 1.69e+08/6.31e+09 =  2% of the original kernel matrix.

torch.Size([37532, 2])
We keep 1.95e+07/7.65e+08 =  2% of the original kernel matrix.

torch.Size([5870, 2])
We keep 8.94e+05/1.40e+07 =  6% of the original kernel matrix.

torch.Size([9080, 2])
We keep 1.52e+06/3.61e+07 =  4% of the original kernel matrix.

torch.Size([23981, 2])
We keep 1.75e+07/4.61e+08 =  3% of the original kernel matrix.

torch.Size([17612, 2])
We keep 6.44e+06/2.07e+08 =  3% of the original kernel matrix.

torch.Size([6219, 2])
We keep 7.88e+05/1.38e+07 =  5% of the original kernel matrix.

torch.Size([9352, 2])
We keep 1.54e+06/3.58e+07 =  4% of the original kernel matrix.

torch.Size([82569, 2])
We keep 2.20e+08/4.98e+09 =  4% of the original kernel matrix.

torch.Size([34247, 2])
We keep 1.79e+07/6.79e+08 =  2% of the original kernel matrix.

torch.Size([17738, 2])
We keep 6.48e+06/2.01e+08 =  3% of the original kernel matrix.

torch.Size([15699, 2])
We keep 4.40e+06/1.36e+08 =  3% of the original kernel matrix.

torch.Size([9127, 2])
We keep 1.55e+06/3.43e+07 =  4% of the original kernel matrix.

torch.Size([11377, 2])
We keep 2.16e+06/5.64e+07 =  3% of the original kernel matrix.

torch.Size([71010, 2])
We keep 5.37e+07/2.57e+09 =  2% of the original kernel matrix.

torch.Size([31995, 2])
We keep 1.30e+07/4.88e+08 =  2% of the original kernel matrix.

torch.Size([77743, 2])
We keep 7.27e+07/3.39e+09 =  2% of the original kernel matrix.

torch.Size([33767, 2])
We keep 1.49e+07/5.61e+08 =  2% of the original kernel matrix.

torch.Size([2741, 2])
We keep 2.04e+05/1.85e+06 = 11% of the original kernel matrix.

torch.Size([6349, 2])
We keep 7.30e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([108535, 2])
We keep 2.21e+08/6.86e+09 =  3% of the original kernel matrix.

torch.Size([40467, 2])
We keep 1.98e+07/7.98e+08 =  2% of the original kernel matrix.

torch.Size([75653, 2])
We keep 8.16e+07/3.78e+09 =  2% of the original kernel matrix.

torch.Size([33579, 2])
We keep 1.57e+07/5.92e+08 =  2% of the original kernel matrix.

torch.Size([191385, 2])
We keep 3.51e+08/2.06e+10 =  1% of the original kernel matrix.

torch.Size([54171, 2])
We keep 3.29e+07/1.38e+09 =  2% of the original kernel matrix.

torch.Size([12322, 2])
We keep 6.13e+06/7.47e+07 =  8% of the original kernel matrix.

torch.Size([12860, 2])
We keep 2.89e+06/8.32e+07 =  3% of the original kernel matrix.

torch.Size([2934, 2])
We keep 1.87e+05/2.02e+06 =  9% of the original kernel matrix.

torch.Size([6468, 2])
We keep 7.43e+05/1.37e+07 =  5% of the original kernel matrix.

torch.Size([8273, 2])
We keep 1.21e+06/2.34e+07 =  5% of the original kernel matrix.

torch.Size([10726, 2])
We keep 1.89e+06/4.66e+07 =  4% of the original kernel matrix.

torch.Size([7211, 2])
We keep 1.17e+06/2.03e+07 =  5% of the original kernel matrix.

torch.Size([10064, 2])
We keep 1.71e+06/4.34e+07 =  3% of the original kernel matrix.

torch.Size([11828, 2])
We keep 2.97e+06/6.43e+07 =  4% of the original kernel matrix.

torch.Size([13027, 2])
We keep 2.77e+06/7.72e+07 =  3% of the original kernel matrix.

torch.Size([21213, 2])
We keep 1.42e+07/2.57e+08 =  5% of the original kernel matrix.

torch.Size([17400, 2])
We keep 4.92e+06/1.54e+08 =  3% of the original kernel matrix.

torch.Size([1337, 2])
We keep 4.56e+04/3.19e+05 = 14% of the original kernel matrix.

torch.Size([4751, 2])
We keep 3.96e+05/5.44e+06 =  7% of the original kernel matrix.

torch.Size([55576, 2])
We keep 5.87e+07/1.52e+09 =  3% of the original kernel matrix.

torch.Size([28040, 2])
We keep 1.07e+07/3.75e+08 =  2% of the original kernel matrix.

torch.Size([80462, 2])
We keep 7.94e+07/3.50e+09 =  2% of the original kernel matrix.

torch.Size([34127, 2])
We keep 1.52e+07/5.70e+08 =  2% of the original kernel matrix.

torch.Size([1671, 2])
We keep 1.69e+05/7.87e+05 = 21% of the original kernel matrix.

torch.Size([5064, 2])
We keep 4.69e+05/8.54e+06 =  5% of the original kernel matrix.

torch.Size([5393, 2])
We keep 5.67e+05/8.40e+06 =  6% of the original kernel matrix.

torch.Size([8548, 2])
We keep 1.27e+06/2.79e+07 =  4% of the original kernel matrix.

torch.Size([21805, 2])
We keep 7.96e+06/2.49e+08 =  3% of the original kernel matrix.

torch.Size([17527, 2])
We keep 4.79e+06/1.52e+08 =  3% of the original kernel matrix.

torch.Size([7642, 2])
We keep 1.36e+06/2.58e+07 =  5% of the original kernel matrix.

torch.Size([10429, 2])
We keep 2.00e+06/4.90e+07 =  4% of the original kernel matrix.

torch.Size([4479, 2])
We keep 4.39e+05/5.98e+06 =  7% of the original kernel matrix.

torch.Size([7896, 2])
We keep 1.11e+06/2.35e+07 =  4% of the original kernel matrix.

torch.Size([6903, 2])
We keep 1.24e+06/2.10e+07 =  5% of the original kernel matrix.

torch.Size([9735, 2])
We keep 1.87e+06/4.41e+07 =  4% of the original kernel matrix.

torch.Size([3760, 2])
We keep 2.50e+05/3.12e+06 =  8% of the original kernel matrix.

torch.Size([7316, 2])
We keep 8.59e+05/1.70e+07 =  5% of the original kernel matrix.

torch.Size([7571, 2])
We keep 1.37e+06/2.45e+07 =  5% of the original kernel matrix.

torch.Size([10212, 2])
We keep 1.93e+06/4.76e+07 =  4% of the original kernel matrix.

torch.Size([25438, 2])
We keep 9.41e+06/3.25e+08 =  2% of the original kernel matrix.

torch.Size([18988, 2])
We keep 5.35e+06/1.74e+08 =  3% of the original kernel matrix.

torch.Size([13882, 2])
We keep 4.83e+06/1.05e+08 =  4% of the original kernel matrix.

torch.Size([14200, 2])
We keep 3.49e+06/9.87e+07 =  3% of the original kernel matrix.

torch.Size([22283, 2])
We keep 1.56e+07/3.08e+08 =  5% of the original kernel matrix.

torch.Size([17552, 2])
We keep 5.20e+06/1.69e+08 =  3% of the original kernel matrix.

torch.Size([6490, 2])
We keep 8.58e+05/1.38e+07 =  6% of the original kernel matrix.

torch.Size([9394, 2])
We keep 1.54e+06/3.58e+07 =  4% of the original kernel matrix.

torch.Size([371890, 2])
We keep 1.26e+09/7.46e+10 =  1% of the original kernel matrix.

torch.Size([77312, 2])
We keep 5.84e+07/2.63e+09 =  2% of the original kernel matrix.

torch.Size([7961, 2])
We keep 1.42e+06/2.64e+07 =  5% of the original kernel matrix.

torch.Size([10628, 2])
We keep 2.00e+06/4.95e+07 =  4% of the original kernel matrix.

torch.Size([67041, 2])
We keep 6.09e+07/2.53e+09 =  2% of the original kernel matrix.

torch.Size([30891, 2])
We keep 1.31e+07/4.85e+08 =  2% of the original kernel matrix.

torch.Size([2041, 2])
We keep 1.02e+05/8.72e+05 = 11% of the original kernel matrix.

torch.Size([5571, 2])
We keep 5.60e+05/9.00e+06 =  6% of the original kernel matrix.

torch.Size([4097, 2])
We keep 3.86e+05/4.27e+06 =  9% of the original kernel matrix.

torch.Size([7496, 2])
We keep 9.98e+05/1.99e+07 =  5% of the original kernel matrix.

torch.Size([3340, 2])
We keep 2.46e+05/2.88e+06 =  8% of the original kernel matrix.

torch.Size([6865, 2])
We keep 8.58e+05/1.63e+07 =  5% of the original kernel matrix.

torch.Size([8433, 2])
We keep 1.18e+06/2.43e+07 =  4% of the original kernel matrix.

torch.Size([10862, 2])
We keep 1.84e+06/4.75e+07 =  3% of the original kernel matrix.

torch.Size([2980, 2])
We keep 2.09e+05/2.28e+06 =  9% of the original kernel matrix.

torch.Size([6591, 2])
We keep 7.91e+05/1.45e+07 =  5% of the original kernel matrix.

torch.Size([80318, 2])
We keep 1.46e+08/3.86e+09 =  3% of the original kernel matrix.

torch.Size([33946, 2])
We keep 1.60e+07/5.98e+08 =  2% of the original kernel matrix.

torch.Size([4594, 2])
We keep 4.07e+05/5.41e+06 =  7% of the original kernel matrix.

torch.Size([7869, 2])
We keep 1.08e+06/2.24e+07 =  4% of the original kernel matrix.

torch.Size([3713, 2])
We keep 3.21e+05/3.91e+06 =  8% of the original kernel matrix.

torch.Size([7144, 2])
We keep 9.53e+05/1.90e+07 =  5% of the original kernel matrix.

torch.Size([108486, 2])
We keep 1.74e+08/7.87e+09 =  2% of the original kernel matrix.

torch.Size([40392, 2])
We keep 2.18e+07/8.55e+08 =  2% of the original kernel matrix.

torch.Size([35268, 2])
We keep 2.24e+07/6.59e+08 =  3% of the original kernel matrix.

torch.Size([21925, 2])
We keep 7.36e+06/2.47e+08 =  2% of the original kernel matrix.

torch.Size([21010, 2])
We keep 1.39e+07/3.08e+08 =  4% of the original kernel matrix.

torch.Size([17243, 2])
We keep 5.41e+06/1.69e+08 =  3% of the original kernel matrix.

torch.Size([60913, 2])
We keep 6.00e+07/2.14e+09 =  2% of the original kernel matrix.

torch.Size([29510, 2])
We keep 1.22e+07/4.46e+08 =  2% of the original kernel matrix.

torch.Size([148883, 2])
We keep 2.45e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([47815, 2])
We keep 2.76e+07/1.12e+09 =  2% of the original kernel matrix.

torch.Size([5169, 2])
We keep 5.72e+05/8.38e+06 =  6% of the original kernel matrix.

torch.Size([8430, 2])
We keep 1.27e+06/2.79e+07 =  4% of the original kernel matrix.

torch.Size([5362, 2])
We keep 5.47e+05/8.05e+06 =  6% of the original kernel matrix.

torch.Size([8557, 2])
We keep 1.24e+06/2.73e+07 =  4% of the original kernel matrix.

torch.Size([11924, 2])
We keep 4.34e+06/8.09e+07 =  5% of the original kernel matrix.

torch.Size([13113, 2])
We keep 3.12e+06/8.67e+07 =  3% of the original kernel matrix.

torch.Size([1972, 2])
We keep 9.38e+04/8.82e+05 = 10% of the original kernel matrix.

torch.Size([5494, 2])
We keep 5.51e+05/9.04e+06 =  6% of the original kernel matrix.

torch.Size([5483, 2])
We keep 5.92e+05/8.41e+06 =  7% of the original kernel matrix.

torch.Size([8647, 2])
We keep 1.27e+06/2.79e+07 =  4% of the original kernel matrix.

torch.Size([12660, 2])
We keep 4.22e+06/8.13e+07 =  5% of the original kernel matrix.

torch.Size([13687, 2])
We keep 3.14e+06/8.69e+07 =  3% of the original kernel matrix.

torch.Size([3389, 2])
We keep 2.11e+05/2.57e+06 =  8% of the original kernel matrix.

torch.Size([7002, 2])
We keep 8.05e+05/1.54e+07 =  5% of the original kernel matrix.

torch.Size([4458, 2])
We keep 3.75e+05/4.80e+06 =  7% of the original kernel matrix.

torch.Size([7744, 2])
We keep 1.02e+06/2.11e+07 =  4% of the original kernel matrix.

torch.Size([7638, 2])
We keep 1.12e+06/2.04e+07 =  5% of the original kernel matrix.

torch.Size([10272, 2])
We keep 1.79e+06/4.35e+07 =  4% of the original kernel matrix.

torch.Size([3942, 2])
We keep 3.53e+05/4.47e+06 =  7% of the original kernel matrix.

torch.Size([7456, 2])
We keep 1.00e+06/2.04e+07 =  4% of the original kernel matrix.

torch.Size([94347, 2])
We keep 1.20e+08/5.75e+09 =  2% of the original kernel matrix.

torch.Size([37233, 2])
We keep 1.87e+07/7.30e+08 =  2% of the original kernel matrix.

torch.Size([8901, 2])
We keep 2.14e+06/3.94e+07 =  5% of the original kernel matrix.

torch.Size([11002, 2])
We keep 2.34e+06/6.05e+07 =  3% of the original kernel matrix.

torch.Size([43480, 2])
We keep 5.47e+07/1.41e+09 =  3% of the original kernel matrix.

torch.Size([24439, 2])
We keep 1.05e+07/3.62e+08 =  2% of the original kernel matrix.

torch.Size([2133, 2])
We keep 8.41e+04/8.30e+05 = 10% of the original kernel matrix.

torch.Size([5890, 2])
We keep 5.30e+05/8.77e+06 =  6% of the original kernel matrix.

torch.Size([4296, 2])
We keep 4.21e+05/5.46e+06 =  7% of the original kernel matrix.

torch.Size([7570, 2])
We keep 1.09e+06/2.25e+07 =  4% of the original kernel matrix.

torch.Size([95292, 2])
We keep 1.12e+08/5.22e+09 =  2% of the original kernel matrix.

torch.Size([37301, 2])
We keep 1.80e+07/6.96e+08 =  2% of the original kernel matrix.

torch.Size([100080, 2])
We keep 3.47e+08/6.95e+09 =  4% of the original kernel matrix.

torch.Size([38833, 2])
We keep 1.85e+07/8.03e+08 =  2% of the original kernel matrix.

torch.Size([231834, 2])
We keep 3.68e+08/2.44e+10 =  1% of the original kernel matrix.

torch.Size([59476, 2])
We keep 3.52e+07/1.51e+09 =  2% of the original kernel matrix.

torch.Size([9010, 2])
We keep 1.56e+06/3.28e+07 =  4% of the original kernel matrix.

torch.Size([11374, 2])
We keep 2.15e+06/5.52e+07 =  3% of the original kernel matrix.

torch.Size([27185, 2])
We keep 1.24e+07/3.81e+08 =  3% of the original kernel matrix.

torch.Size([19624, 2])
We keep 5.86e+06/1.88e+08 =  3% of the original kernel matrix.

torch.Size([3455, 2])
We keep 2.78e+05/3.06e+06 =  9% of the original kernel matrix.

torch.Size([6945, 2])
We keep 8.72e+05/1.68e+07 =  5% of the original kernel matrix.

torch.Size([2865, 2])
We keep 1.67e+05/1.80e+06 =  9% of the original kernel matrix.

torch.Size([6500, 2])
We keep 7.16e+05/1.29e+07 =  5% of the original kernel matrix.

torch.Size([35971, 2])
We keep 3.38e+07/7.62e+08 =  4% of the original kernel matrix.

torch.Size([22145, 2])
We keep 7.52e+06/2.66e+08 =  2% of the original kernel matrix.

torch.Size([11552, 2])
We keep 2.58e+06/5.94e+07 =  4% of the original kernel matrix.

torch.Size([13075, 2])
We keep 2.74e+06/7.42e+07 =  3% of the original kernel matrix.

torch.Size([5503, 2])
We keep 5.83e+05/8.42e+06 =  6% of the original kernel matrix.

torch.Size([8664, 2])
We keep 1.26e+06/2.79e+07 =  4% of the original kernel matrix.

torch.Size([28559, 2])
We keep 1.62e+07/4.44e+08 =  3% of the original kernel matrix.

torch.Size([19874, 2])
We keep 6.16e+06/2.03e+08 =  3% of the original kernel matrix.

torch.Size([431019, 2])
We keep 2.01e+09/1.30e+11 =  1% of the original kernel matrix.

torch.Size([80834, 2])
We keep 7.77e+07/3.47e+09 =  2% of the original kernel matrix.

torch.Size([20051, 2])
We keep 8.10e+06/2.32e+08 =  3% of the original kernel matrix.

torch.Size([16248, 2])
We keep 4.66e+06/1.47e+08 =  3% of the original kernel matrix.

torch.Size([9005, 2])
We keep 2.80e+06/5.11e+07 =  5% of the original kernel matrix.

torch.Size([10943, 2])
We keep 2.58e+06/6.89e+07 =  3% of the original kernel matrix.

torch.Size([10852, 2])
We keep 3.77e+06/7.43e+07 =  5% of the original kernel matrix.

torch.Size([12599, 2])
We keep 3.02e+06/8.30e+07 =  3% of the original kernel matrix.

torch.Size([5231, 2])
We keep 5.24e+05/8.08e+06 =  6% of the original kernel matrix.

torch.Size([8473, 2])
We keep 1.24e+06/2.74e+07 =  4% of the original kernel matrix.

torch.Size([3459, 2])
We keep 2.18e+05/2.72e+06 =  8% of the original kernel matrix.

torch.Size([7022, 2])
We keep 8.12e+05/1.59e+07 =  5% of the original kernel matrix.

torch.Size([78487, 2])
We keep 1.14e+08/4.24e+09 =  2% of the original kernel matrix.

torch.Size([33800, 2])
We keep 1.64e+07/6.27e+08 =  2% of the original kernel matrix.

torch.Size([13290, 2])
We keep 6.22e+06/1.01e+08 =  6% of the original kernel matrix.

torch.Size([13529, 2])
We keep 3.43e+06/9.68e+07 =  3% of the original kernel matrix.

torch.Size([2657, 2])
We keep 1.79e+05/1.37e+06 = 13% of the original kernel matrix.

torch.Size([6285, 2])
We keep 6.56e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([51368, 2])
We keep 3.23e+07/1.16e+09 =  2% of the original kernel matrix.

torch.Size([26604, 2])
We keep 9.13e+06/3.28e+08 =  2% of the original kernel matrix.

torch.Size([7138, 2])
We keep 1.34e+06/2.27e+07 =  5% of the original kernel matrix.

torch.Size([9977, 2])
We keep 1.93e+06/4.59e+07 =  4% of the original kernel matrix.

torch.Size([1025, 2])
We keep 2.53e+04/1.61e+05 = 15% of the original kernel matrix.

torch.Size([4325, 2])
We keep 3.14e+05/3.86e+06 =  8% of the original kernel matrix.

torch.Size([46321, 2])
We keep 3.96e+07/1.08e+09 =  3% of the original kernel matrix.

torch.Size([25048, 2])
We keep 8.98e+06/3.17e+08 =  2% of the original kernel matrix.

torch.Size([48812, 2])
We keep 4.11e+07/1.48e+09 =  2% of the original kernel matrix.

torch.Size([26150, 2])
We keep 1.04e+07/3.71e+08 =  2% of the original kernel matrix.

torch.Size([128270, 2])
We keep 3.46e+08/1.25e+10 =  2% of the original kernel matrix.

torch.Size([44064, 2])
We keep 2.75e+07/1.08e+09 =  2% of the original kernel matrix.

torch.Size([91495, 2])
We keep 2.77e+08/5.85e+09 =  4% of the original kernel matrix.

torch.Size([36453, 2])
We keep 1.94e+07/7.36e+08 =  2% of the original kernel matrix.

torch.Size([75494, 2])
We keep 1.47e+08/4.56e+09 =  3% of the original kernel matrix.

torch.Size([32432, 2])
We keep 1.71e+07/6.50e+08 =  2% of the original kernel matrix.

torch.Size([18170, 2])
We keep 1.03e+07/2.08e+08 =  4% of the original kernel matrix.

torch.Size([15644, 2])
We keep 4.60e+06/1.39e+08 =  3% of the original kernel matrix.

torch.Size([8643, 2])
We keep 1.73e+06/3.17e+07 =  5% of the original kernel matrix.

torch.Size([11046, 2])
We keep 2.13e+06/5.42e+07 =  3% of the original kernel matrix.

torch.Size([78320, 2])
We keep 7.08e+07/3.40e+09 =  2% of the original kernel matrix.

torch.Size([33768, 2])
We keep 1.49e+07/5.62e+08 =  2% of the original kernel matrix.

torch.Size([62264, 2])
We keep 4.28e+07/1.89e+09 =  2% of the original kernel matrix.

torch.Size([29775, 2])
We keep 1.12e+07/4.18e+08 =  2% of the original kernel matrix.

torch.Size([14858, 2])
We keep 4.04e+06/1.12e+08 =  3% of the original kernel matrix.

torch.Size([14739, 2])
We keep 3.47e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([26314, 2])
We keep 2.77e+07/4.48e+08 =  6% of the original kernel matrix.

torch.Size([19211, 2])
We keep 5.78e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([64493, 2])
We keep 5.90e+07/2.37e+09 =  2% of the original kernel matrix.

torch.Size([30484, 2])
We keep 1.25e+07/4.69e+08 =  2% of the original kernel matrix.

torch.Size([103411, 2])
We keep 1.71e+08/6.69e+09 =  2% of the original kernel matrix.

torch.Size([39439, 2])
We keep 2.04e+07/7.87e+08 =  2% of the original kernel matrix.

torch.Size([76444, 2])
We keep 7.88e+07/3.40e+09 =  2% of the original kernel matrix.

torch.Size([33414, 2])
We keep 1.48e+07/5.62e+08 =  2% of the original kernel matrix.

torch.Size([34074, 2])
We keep 4.14e+07/7.92e+08 =  5% of the original kernel matrix.

torch.Size([21185, 2])
We keep 7.68e+06/2.71e+08 =  2% of the original kernel matrix.

torch.Size([11903, 2])
We keep 5.19e+06/8.27e+07 =  6% of the original kernel matrix.

torch.Size([12687, 2])
We keep 3.14e+06/8.76e+07 =  3% of the original kernel matrix.

torch.Size([5952, 2])
We keep 6.38e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([9084, 2])
We keep 1.36e+06/3.07e+07 =  4% of the original kernel matrix.

torch.Size([4206, 2])
We keep 4.50e+05/5.62e+06 =  8% of the original kernel matrix.

torch.Size([7490, 2])
We keep 1.11e+06/2.28e+07 =  4% of the original kernel matrix.

torch.Size([63637, 2])
We keep 4.53e+07/2.06e+09 =  2% of the original kernel matrix.

torch.Size([30235, 2])
We keep 1.18e+07/4.38e+08 =  2% of the original kernel matrix.

torch.Size([66378, 2])
We keep 6.50e+07/2.87e+09 =  2% of the original kernel matrix.

torch.Size([30874, 2])
We keep 1.39e+07/5.16e+08 =  2% of the original kernel matrix.

torch.Size([207637, 2])
We keep 4.61e+08/2.42e+10 =  1% of the original kernel matrix.

torch.Size([55375, 2])
We keep 3.60e+07/1.50e+09 =  2% of the original kernel matrix.

torch.Size([69056, 2])
We keep 8.34e+07/2.75e+09 =  3% of the original kernel matrix.

torch.Size([31566, 2])
We keep 1.37e+07/5.05e+08 =  2% of the original kernel matrix.

torch.Size([37890, 2])
We keep 2.26e+07/7.86e+08 =  2% of the original kernel matrix.

torch.Size([22973, 2])
We keep 7.90e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([9154, 2])
We keep 1.92e+06/3.90e+07 =  4% of the original kernel matrix.

torch.Size([11218, 2])
We keep 2.29e+06/6.01e+07 =  3% of the original kernel matrix.

torch.Size([69211, 2])
We keep 7.04e+07/2.43e+09 =  2% of the original kernel matrix.

torch.Size([31718, 2])
We keep 1.30e+07/4.75e+08 =  2% of the original kernel matrix.

torch.Size([5562, 2])
We keep 8.44e+05/1.20e+07 =  7% of the original kernel matrix.

torch.Size([8657, 2])
We keep 1.47e+06/3.33e+07 =  4% of the original kernel matrix.

torch.Size([7019, 2])
We keep 1.12e+06/1.84e+07 =  6% of the original kernel matrix.

torch.Size([9787, 2])
We keep 1.70e+06/4.14e+07 =  4% of the original kernel matrix.

torch.Size([65288, 2])
We keep 6.52e+07/2.61e+09 =  2% of the original kernel matrix.

torch.Size([30435, 2])
We keep 1.31e+07/4.92e+08 =  2% of the original kernel matrix.

torch.Size([28644, 2])
We keep 1.60e+07/4.05e+08 =  3% of the original kernel matrix.

torch.Size([19950, 2])
We keep 5.92e+06/1.94e+08 =  3% of the original kernel matrix.

torch.Size([3464, 2])
We keep 2.66e+05/3.05e+06 =  8% of the original kernel matrix.

torch.Size([6959, 2])
We keep 8.72e+05/1.68e+07 =  5% of the original kernel matrix.

torch.Size([12559, 2])
We keep 3.05e+06/7.60e+07 =  4% of the original kernel matrix.

torch.Size([13330, 2])
We keep 3.00e+06/8.40e+07 =  3% of the original kernel matrix.

torch.Size([324207, 2])
We keep 1.88e+09/8.40e+10 =  2% of the original kernel matrix.

torch.Size([70381, 2])
We keep 6.31e+07/2.79e+09 =  2% of the original kernel matrix.

torch.Size([13128, 2])
We keep 3.62e+06/7.90e+07 =  4% of the original kernel matrix.

torch.Size([13576, 2])
We keep 3.07e+06/8.56e+07 =  3% of the original kernel matrix.

torch.Size([89958, 2])
We keep 1.26e+08/5.00e+09 =  2% of the original kernel matrix.

torch.Size([36441, 2])
We keep 1.79e+07/6.81e+08 =  2% of the original kernel matrix.

torch.Size([3560, 2])
We keep 2.72e+05/3.09e+06 =  8% of the original kernel matrix.

torch.Size([7019, 2])
We keep 8.79e+05/1.69e+07 =  5% of the original kernel matrix.

torch.Size([4132, 2])
We keep 3.55e+05/4.35e+06 =  8% of the original kernel matrix.

torch.Size([7538, 2])
We keep 9.89e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([5149, 2])
We keep 6.37e+05/8.08e+06 =  7% of the original kernel matrix.

torch.Size([8317, 2])
We keep 1.26e+06/2.74e+07 =  4% of the original kernel matrix.

torch.Size([2806, 2])
We keep 1.89e+05/1.93e+06 =  9% of the original kernel matrix.

torch.Size([6325, 2])
We keep 7.47e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([12493, 2])
We keep 4.36e+06/9.36e+07 =  4% of the original kernel matrix.

torch.Size([13719, 2])
We keep 3.42e+06/9.32e+07 =  3% of the original kernel matrix.

torch.Size([32030, 2])
We keep 2.11e+07/5.52e+08 =  3% of the original kernel matrix.

torch.Size([20749, 2])
We keep 6.95e+06/2.26e+08 =  3% of the original kernel matrix.

torch.Size([2382, 2])
We keep 1.72e+05/1.69e+06 = 10% of the original kernel matrix.

torch.Size([5973, 2])
We keep 7.22e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([59596, 2])
We keep 8.93e+07/1.96e+09 =  4% of the original kernel matrix.

torch.Size([29066, 2])
We keep 1.12e+07/4.26e+08 =  2% of the original kernel matrix.

torch.Size([4488, 2])
We keep 4.35e+05/5.99e+06 =  7% of the original kernel matrix.

torch.Size([7996, 2])
We keep 1.07e+06/2.36e+07 =  4% of the original kernel matrix.

torch.Size([19656, 2])
We keep 1.21e+07/2.46e+08 =  4% of the original kernel matrix.

torch.Size([16788, 2])
We keep 5.02e+06/1.51e+08 =  3% of the original kernel matrix.

torch.Size([7458, 2])
We keep 1.53e+06/2.22e+07 =  6% of the original kernel matrix.

torch.Size([10183, 2])
We keep 1.89e+06/4.54e+07 =  4% of the original kernel matrix.

torch.Size([81568, 2])
We keep 7.49e+07/3.26e+09 =  2% of the original kernel matrix.

torch.Size([34364, 2])
We keep 1.46e+07/5.50e+08 =  2% of the original kernel matrix.

torch.Size([5310, 2])
We keep 1.24e+06/1.16e+07 = 10% of the original kernel matrix.

torch.Size([8466, 2])
We keep 1.49e+06/3.28e+07 =  4% of the original kernel matrix.

torch.Size([13291, 2])
We keep 6.42e+06/1.15e+08 =  5% of the original kernel matrix.

torch.Size([13702, 2])
We keep 3.55e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([20952, 2])
We keep 7.75e+06/2.19e+08 =  3% of the original kernel matrix.

torch.Size([17435, 2])
We keep 4.66e+06/1.42e+08 =  3% of the original kernel matrix.

torch.Size([16281, 2])
We keep 6.84e+06/1.44e+08 =  4% of the original kernel matrix.

torch.Size([15101, 2])
We keep 3.94e+06/1.16e+08 =  3% of the original kernel matrix.

torch.Size([39468, 2])
We keep 2.46e+07/8.39e+08 =  2% of the original kernel matrix.

torch.Size([23387, 2])
We keep 8.15e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([184768, 2])
We keep 1.14e+09/2.31e+10 =  4% of the original kernel matrix.

torch.Size([52673, 2])
We keep 3.28e+07/1.46e+09 =  2% of the original kernel matrix.

torch.Size([8979, 2])
We keep 1.41e+06/3.01e+07 =  4% of the original kernel matrix.

torch.Size([11367, 2])
We keep 2.08e+06/5.29e+07 =  3% of the original kernel matrix.

torch.Size([4550, 2])
We keep 8.41e+05/7.37e+06 = 11% of the original kernel matrix.

torch.Size([7848, 2])
We keep 1.22e+06/2.61e+07 =  4% of the original kernel matrix.

torch.Size([29748, 2])
We keep 1.77e+07/4.38e+08 =  4% of the original kernel matrix.

torch.Size([20223, 2])
We keep 6.10e+06/2.01e+08 =  3% of the original kernel matrix.

torch.Size([14421, 2])
We keep 4.57e+06/1.04e+08 =  4% of the original kernel matrix.

torch.Size([14390, 2])
We keep 3.44e+06/9.84e+07 =  3% of the original kernel matrix.

torch.Size([262878, 2])
We keep 6.25e+08/4.01e+10 =  1% of the original kernel matrix.

torch.Size([63763, 2])
We keep 4.47e+07/1.93e+09 =  2% of the original kernel matrix.

torch.Size([5413, 2])
We keep 8.05e+05/9.67e+06 =  8% of the original kernel matrix.

torch.Size([8653, 2])
We keep 1.38e+06/2.99e+07 =  4% of the original kernel matrix.

torch.Size([70058, 2])
We keep 6.56e+07/2.66e+09 =  2% of the original kernel matrix.

torch.Size([31577, 2])
We keep 1.34e+07/4.97e+08 =  2% of the original kernel matrix.

torch.Size([150254, 2])
We keep 2.35e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([48299, 2])
We keep 2.83e+07/1.16e+09 =  2% of the original kernel matrix.

torch.Size([3666, 2])
We keep 2.61e+05/2.96e+06 =  8% of the original kernel matrix.

torch.Size([7125, 2])
We keep 8.57e+05/1.66e+07 =  5% of the original kernel matrix.

torch.Size([60859, 2])
We keep 2.03e+08/3.54e+09 =  5% of the original kernel matrix.

torch.Size([28784, 2])
We keep 1.43e+07/5.73e+08 =  2% of the original kernel matrix.

torch.Size([9281, 2])
We keep 3.68e+06/5.86e+07 =  6% of the original kernel matrix.

torch.Size([10820, 2])
We keep 2.65e+06/7.37e+07 =  3% of the original kernel matrix.

torch.Size([425896, 2])
We keep 1.40e+09/9.59e+10 =  1% of the original kernel matrix.

torch.Size([83356, 2])
We keep 6.74e+07/2.98e+09 =  2% of the original kernel matrix.

torch.Size([9534, 2])
We keep 1.91e+06/4.21e+07 =  4% of the original kernel matrix.

torch.Size([11684, 2])
We keep 2.38e+06/6.25e+07 =  3% of the original kernel matrix.

torch.Size([10508, 2])
We keep 2.26e+06/4.86e+07 =  4% of the original kernel matrix.

torch.Size([12144, 2])
We keep 2.50e+06/6.71e+07 =  3% of the original kernel matrix.

torch.Size([7073, 2])
We keep 1.95e+06/2.84e+07 =  6% of the original kernel matrix.

torch.Size([9697, 2])
We keep 2.08e+06/5.14e+07 =  4% of the original kernel matrix.

torch.Size([3074, 2])
We keep 2.23e+05/2.48e+06 =  9% of the original kernel matrix.

torch.Size([6780, 2])
We keep 8.32e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([8201, 2])
We keep 1.66e+06/3.20e+07 =  5% of the original kernel matrix.

torch.Size([10560, 2])
We keep 2.15e+06/5.45e+07 =  3% of the original kernel matrix.

torch.Size([17823, 2])
We keep 5.65e+06/1.58e+08 =  3% of the original kernel matrix.

torch.Size([15948, 2])
We keep 3.86e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([3513, 2])
We keep 2.36e+05/2.94e+06 =  8% of the original kernel matrix.

torch.Size([7085, 2])
We keep 8.48e+05/1.65e+07 =  5% of the original kernel matrix.

torch.Size([79383, 2])
We keep 9.17e+07/3.71e+09 =  2% of the original kernel matrix.

torch.Size([34245, 2])
We keep 1.56e+07/5.87e+08 =  2% of the original kernel matrix.

torch.Size([10474, 2])
We keep 2.01e+06/4.67e+07 =  4% of the original kernel matrix.

torch.Size([12328, 2])
We keep 2.48e+06/6.58e+07 =  3% of the original kernel matrix.

torch.Size([4910, 2])
We keep 5.20e+05/6.52e+06 =  7% of the original kernel matrix.

torch.Size([8187, 2])
We keep 1.16e+06/2.46e+07 =  4% of the original kernel matrix.

torch.Size([10494, 2])
We keep 2.57e+06/5.68e+07 =  4% of the original kernel matrix.

torch.Size([12448, 2])
We keep 2.74e+06/7.26e+07 =  3% of the original kernel matrix.

torch.Size([131570, 2])
We keep 2.23e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([44708, 2])
We keep 2.56e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([82038, 2])
We keep 1.29e+08/5.04e+09 =  2% of the original kernel matrix.

torch.Size([34496, 2])
We keep 1.83e+07/6.84e+08 =  2% of the original kernel matrix.

torch.Size([202711, 2])
We keep 3.55e+08/2.01e+10 =  1% of the original kernel matrix.

torch.Size([55360, 2])
We keep 3.27e+07/1.37e+09 =  2% of the original kernel matrix.

torch.Size([8399, 2])
We keep 1.33e+06/2.62e+07 =  5% of the original kernel matrix.

torch.Size([10922, 2])
We keep 1.92e+06/4.93e+07 =  3% of the original kernel matrix.

torch.Size([5635, 2])
We keep 7.59e+05/1.18e+07 =  6% of the original kernel matrix.

torch.Size([8862, 2])
We keep 1.45e+06/3.31e+07 =  4% of the original kernel matrix.

torch.Size([3001, 2])
We keep 1.96e+05/2.15e+06 =  9% of the original kernel matrix.

torch.Size([6666, 2])
We keep 7.74e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([4278, 2])
We keep 5.82e+05/5.84e+06 =  9% of the original kernel matrix.

torch.Size([7721, 2])
We keep 1.11e+06/2.33e+07 =  4% of the original kernel matrix.

torch.Size([53383, 2])
We keep 7.16e+07/1.37e+09 =  5% of the original kernel matrix.

torch.Size([27542, 2])
We keep 9.54e+06/3.56e+08 =  2% of the original kernel matrix.

torch.Size([6290, 2])
We keep 8.94e+05/1.38e+07 =  6% of the original kernel matrix.

torch.Size([9249, 2])
We keep 1.54e+06/3.58e+07 =  4% of the original kernel matrix.

torch.Size([11002, 2])
We keep 2.52e+06/5.51e+07 =  4% of the original kernel matrix.

torch.Size([12548, 2])
We keep 2.66e+06/7.15e+07 =  3% of the original kernel matrix.

torch.Size([18552, 2])
We keep 7.37e+06/1.82e+08 =  4% of the original kernel matrix.

torch.Size([16087, 2])
We keep 4.19e+06/1.30e+08 =  3% of the original kernel matrix.

torch.Size([18362, 2])
We keep 8.43e+06/1.97e+08 =  4% of the original kernel matrix.

torch.Size([15756, 2])
We keep 4.44e+06/1.35e+08 =  3% of the original kernel matrix.

torch.Size([13817, 2])
We keep 5.46e+06/1.11e+08 =  4% of the original kernel matrix.

torch.Size([13933, 2])
We keep 3.56e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([3985, 2])
We keep 3.99e+05/4.61e+06 =  8% of the original kernel matrix.

torch.Size([7317, 2])
We keep 1.02e+06/2.07e+07 =  4% of the original kernel matrix.

torch.Size([157898, 2])
We keep 3.54e+08/1.64e+10 =  2% of the original kernel matrix.

torch.Size([48214, 2])
We keep 3.02e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([36668, 2])
We keep 2.29e+07/6.93e+08 =  3% of the original kernel matrix.

torch.Size([22270, 2])
We keep 7.42e+06/2.54e+08 =  2% of the original kernel matrix.

torch.Size([5956, 2])
We keep 1.14e+06/1.38e+07 =  8% of the original kernel matrix.

torch.Size([9114, 2])
We keep 1.49e+06/3.57e+07 =  4% of the original kernel matrix.

torch.Size([7033, 2])
We keep 9.27e+05/1.70e+07 =  5% of the original kernel matrix.

torch.Size([9877, 2])
We keep 1.63e+06/3.97e+07 =  4% of the original kernel matrix.

torch.Size([25661, 2])
We keep 1.41e+07/3.20e+08 =  4% of the original kernel matrix.

torch.Size([18937, 2])
We keep 5.41e+06/1.72e+08 =  3% of the original kernel matrix.

torch.Size([61064, 2])
We keep 6.35e+07/1.96e+09 =  3% of the original kernel matrix.

torch.Size([29333, 2])
We keep 1.18e+07/4.27e+08 =  2% of the original kernel matrix.

torch.Size([290019, 2])
We keep 8.43e+08/4.54e+10 =  1% of the original kernel matrix.

torch.Size([67340, 2])
We keep 4.80e+07/2.05e+09 =  2% of the original kernel matrix.

torch.Size([78020, 2])
We keep 1.19e+08/4.05e+09 =  2% of the original kernel matrix.

torch.Size([33814, 2])
We keep 1.65e+07/6.13e+08 =  2% of the original kernel matrix.

torch.Size([3618, 2])
We keep 2.92e+05/3.90e+06 =  7% of the original kernel matrix.

torch.Size([7127, 2])
We keep 9.33e+05/1.90e+07 =  4% of the original kernel matrix.

torch.Size([16841, 2])
We keep 5.10e+06/1.47e+08 =  3% of the original kernel matrix.

torch.Size([15510, 2])
We keep 3.91e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([29527, 2])
We keep 1.80e+07/4.85e+08 =  3% of the original kernel matrix.

torch.Size([19933, 2])
We keep 6.33e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([4272, 2])
We keep 3.76e+05/4.65e+06 =  8% of the original kernel matrix.

torch.Size([7782, 2])
We keep 1.03e+06/2.08e+07 =  4% of the original kernel matrix.

torch.Size([8328, 2])
We keep 1.63e+06/3.22e+07 =  5% of the original kernel matrix.

torch.Size([10694, 2])
We keep 2.13e+06/5.47e+07 =  3% of the original kernel matrix.

torch.Size([4313, 2])
We keep 3.70e+05/4.97e+06 =  7% of the original kernel matrix.

torch.Size([7740, 2])
We keep 1.04e+06/2.15e+07 =  4% of the original kernel matrix.

torch.Size([10967, 2])
We keep 1.04e+07/6.10e+07 = 17% of the original kernel matrix.

torch.Size([12609, 2])
We keep 2.82e+06/7.52e+07 =  3% of the original kernel matrix.

torch.Size([29881, 2])
We keep 1.97e+07/5.17e+08 =  3% of the original kernel matrix.

torch.Size([19605, 2])
We keep 6.70e+06/2.19e+08 =  3% of the original kernel matrix.

torch.Size([2409, 2])
We keep 2.96e+05/1.81e+06 = 16% of the original kernel matrix.

torch.Size([5790, 2])
We keep 7.28e+05/1.30e+07 =  5% of the original kernel matrix.

torch.Size([11376, 2])
We keep 2.69e+06/6.49e+07 =  4% of the original kernel matrix.

torch.Size([12817, 2])
We keep 2.85e+06/7.76e+07 =  3% of the original kernel matrix.

torch.Size([6053, 2])
We keep 9.84e+05/1.31e+07 =  7% of the original kernel matrix.

torch.Size([9118, 2])
We keep 1.50e+06/3.48e+07 =  4% of the original kernel matrix.

torch.Size([11961, 2])
We keep 8.65e+06/1.30e+08 =  6% of the original kernel matrix.

torch.Size([12588, 2])
We keep 3.46e+06/1.10e+08 =  3% of the original kernel matrix.

torch.Size([5059, 2])
We keep 5.85e+05/8.00e+06 =  7% of the original kernel matrix.

torch.Size([8261, 2])
We keep 1.26e+06/2.72e+07 =  4% of the original kernel matrix.

torch.Size([14390, 2])
We keep 4.66e+06/9.93e+07 =  4% of the original kernel matrix.

torch.Size([14465, 2])
We keep 3.40e+06/9.60e+07 =  3% of the original kernel matrix.

torch.Size([1613, 2])
We keep 5.63e+04/4.46e+05 = 12% of the original kernel matrix.

torch.Size([5119, 2])
We keep 4.32e+05/6.43e+06 =  6% of the original kernel matrix.

torch.Size([9161, 2])
We keep 1.59e+06/3.32e+07 =  4% of the original kernel matrix.

torch.Size([11378, 2])
We keep 2.10e+06/5.55e+07 =  3% of the original kernel matrix.

torch.Size([5285, 2])
We keep 6.28e+05/8.21e+06 =  7% of the original kernel matrix.

torch.Size([8468, 2])
We keep 1.28e+06/2.76e+07 =  4% of the original kernel matrix.

torch.Size([7882, 2])
We keep 1.37e+06/2.52e+07 =  5% of the original kernel matrix.

torch.Size([10453, 2])
We keep 1.95e+06/4.84e+07 =  4% of the original kernel matrix.

torch.Size([10708, 2])
We keep 1.97e+06/5.02e+07 =  3% of the original kernel matrix.

torch.Size([12530, 2])
We keep 2.52e+06/6.82e+07 =  3% of the original kernel matrix.

torch.Size([72684, 2])
We keep 1.04e+08/3.54e+09 =  2% of the original kernel matrix.

torch.Size([32450, 2])
We keep 1.54e+07/5.73e+08 =  2% of the original kernel matrix.

torch.Size([5665, 2])
We keep 5.94e+05/9.07e+06 =  6% of the original kernel matrix.

torch.Size([8858, 2])
We keep 1.31e+06/2.90e+07 =  4% of the original kernel matrix.

torch.Size([1702, 2])
We keep 6.59e+04/5.70e+05 = 11% of the original kernel matrix.

torch.Size([5168, 2])
We keep 4.72e+05/7.27e+06 =  6% of the original kernel matrix.

torch.Size([5302, 2])
We keep 6.02e+05/8.43e+06 =  7% of the original kernel matrix.

torch.Size([8462, 2])
We keep 1.28e+06/2.80e+07 =  4% of the original kernel matrix.

torch.Size([58168, 2])
We keep 5.85e+07/1.87e+09 =  3% of the original kernel matrix.

torch.Size([28665, 2])
We keep 1.11e+07/4.17e+08 =  2% of the original kernel matrix.

torch.Size([84938, 2])
We keep 9.75e+07/4.26e+09 =  2% of the original kernel matrix.

torch.Size([35424, 2])
We keep 1.65e+07/6.29e+08 =  2% of the original kernel matrix.

torch.Size([57605, 2])
We keep 5.63e+07/2.03e+09 =  2% of the original kernel matrix.

torch.Size([27493, 2])
We keep 1.19e+07/4.34e+08 =  2% of the original kernel matrix.

torch.Size([5585, 2])
We keep 8.04e+05/1.17e+07 =  6% of the original kernel matrix.

torch.Size([8603, 2])
We keep 1.45e+06/3.29e+07 =  4% of the original kernel matrix.

torch.Size([185305, 2])
We keep 3.94e+08/1.89e+10 =  2% of the original kernel matrix.

torch.Size([52384, 2])
We keep 3.24e+07/1.32e+09 =  2% of the original kernel matrix.

torch.Size([76850, 2])
We keep 7.35e+07/3.51e+09 =  2% of the original kernel matrix.

torch.Size([33430, 2])
We keep 1.51e+07/5.71e+08 =  2% of the original kernel matrix.

torch.Size([2306, 2])
We keep 1.12e+05/1.05e+06 = 10% of the original kernel matrix.

torch.Size([5870, 2])
We keep 5.91e+05/9.87e+06 =  5% of the original kernel matrix.

torch.Size([4197, 2])
We keep 3.87e+05/5.01e+06 =  7% of the original kernel matrix.

torch.Size([7630, 2])
We keep 1.03e+06/2.16e+07 =  4% of the original kernel matrix.

torch.Size([18952, 2])
We keep 5.16e+06/1.71e+08 =  3% of the original kernel matrix.

torch.Size([16709, 2])
We keep 4.11e+06/1.26e+08 =  3% of the original kernel matrix.

torch.Size([23856, 2])
We keep 2.35e+07/5.08e+08 =  4% of the original kernel matrix.

torch.Size([17900, 2])
We keep 6.63e+06/2.17e+08 =  3% of the original kernel matrix.

torch.Size([19933, 2])
We keep 9.05e+06/2.29e+08 =  3% of the original kernel matrix.

torch.Size([16739, 2])
We keep 4.64e+06/1.46e+08 =  3% of the original kernel matrix.

torch.Size([101400, 2])
We keep 1.27e+08/5.94e+09 =  2% of the original kernel matrix.

torch.Size([38879, 2])
We keep 1.90e+07/7.42e+08 =  2% of the original kernel matrix.

torch.Size([10613, 2])
We keep 2.02e+06/4.97e+07 =  4% of the original kernel matrix.

torch.Size([12415, 2])
We keep 2.51e+06/6.79e+07 =  3% of the original kernel matrix.

torch.Size([3242, 2])
We keep 2.01e+05/2.27e+06 =  8% of the original kernel matrix.

torch.Size([6853, 2])
We keep 7.79e+05/1.45e+07 =  5% of the original kernel matrix.

torch.Size([3510, 2])
We keep 2.34e+05/2.67e+06 =  8% of the original kernel matrix.

torch.Size([6953, 2])
We keep 8.21e+05/1.57e+07 =  5% of the original kernel matrix.

torch.Size([15051, 2])
We keep 6.27e+06/1.09e+08 =  5% of the original kernel matrix.

torch.Size([14653, 2])
We keep 3.47e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([7640, 2])
We keep 1.26e+06/2.53e+07 =  5% of the original kernel matrix.

torch.Size([10383, 2])
We keep 1.96e+06/4.84e+07 =  4% of the original kernel matrix.

torch.Size([8733, 2])
We keep 1.66e+06/3.48e+07 =  4% of the original kernel matrix.

torch.Size([10916, 2])
We keep 2.15e+06/5.68e+07 =  3% of the original kernel matrix.

torch.Size([57655, 2])
We keep 5.49e+07/1.87e+09 =  2% of the original kernel matrix.

torch.Size([28548, 2])
We keep 1.16e+07/4.17e+08 =  2% of the original kernel matrix.

torch.Size([260216, 2])
We keep 5.38e+08/3.37e+10 =  1% of the original kernel matrix.

torch.Size([62852, 2])
We keep 4.13e+07/1.77e+09 =  2% of the original kernel matrix.

torch.Size([6381, 2])
We keep 1.19e+06/1.68e+07 =  7% of the original kernel matrix.

torch.Size([9305, 2])
We keep 1.70e+06/3.94e+07 =  4% of the original kernel matrix.

torch.Size([1844, 2])
We keep 7.25e+04/6.37e+05 = 11% of the original kernel matrix.

torch.Size([5409, 2])
We keep 4.93e+05/7.69e+06 =  6% of the original kernel matrix.

torch.Size([16709, 2])
We keep 6.16e+06/1.77e+08 =  3% of the original kernel matrix.

torch.Size([15683, 2])
We keep 4.27e+06/1.28e+08 =  3% of the original kernel matrix.

torch.Size([17222, 2])
We keep 4.62e+06/1.43e+08 =  3% of the original kernel matrix.

torch.Size([15774, 2])
We keep 3.80e+06/1.15e+08 =  3% of the original kernel matrix.

torch.Size([15598, 2])
We keep 6.32e+06/1.26e+08 =  5% of the original kernel matrix.

torch.Size([14905, 2])
We keep 3.56e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([2223, 2])
We keep 1.02e+05/8.97e+05 = 11% of the original kernel matrix.

torch.Size([5842, 2])
We keep 5.55e+05/9.12e+06 =  6% of the original kernel matrix.

torch.Size([3563, 2])
We keep 3.22e+05/3.33e+06 =  9% of the original kernel matrix.

torch.Size([6930, 2])
We keep 9.04e+05/1.76e+07 =  5% of the original kernel matrix.

torch.Size([13586, 2])
We keep 4.54e+06/9.24e+07 =  4% of the original kernel matrix.

torch.Size([13914, 2])
We keep 3.10e+06/9.26e+07 =  3% of the original kernel matrix.

torch.Size([3700, 2])
We keep 3.91e+05/3.78e+06 = 10% of the original kernel matrix.

torch.Size([7146, 2])
We keep 9.61e+05/1.87e+07 =  5% of the original kernel matrix.

torch.Size([47826, 2])
We keep 6.84e+07/1.68e+09 =  4% of the original kernel matrix.

torch.Size([25361, 2])
We keep 1.12e+07/3.95e+08 =  2% of the original kernel matrix.

torch.Size([5164, 2])
We keep 7.81e+05/9.72e+06 =  8% of the original kernel matrix.

torch.Size([8335, 2])
We keep 1.38e+06/3.00e+07 =  4% of the original kernel matrix.

torch.Size([8857, 2])
We keep 1.51e+06/3.30e+07 =  4% of the original kernel matrix.

torch.Size([11301, 2])
We keep 2.16e+06/5.53e+07 =  3% of the original kernel matrix.

torch.Size([8422, 2])
We keep 1.61e+06/2.79e+07 =  5% of the original kernel matrix.

torch.Size([10855, 2])
We keep 2.03e+06/5.09e+07 =  3% of the original kernel matrix.

torch.Size([5489, 2])
We keep 6.24e+05/1.03e+07 =  6% of the original kernel matrix.

torch.Size([8703, 2])
We keep 1.36e+06/3.09e+07 =  4% of the original kernel matrix.

torch.Size([6684, 2])
We keep 8.93e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([9544, 2])
We keep 1.57e+06/3.75e+07 =  4% of the original kernel matrix.

torch.Size([90058, 2])
We keep 9.66e+07/4.23e+09 =  2% of the original kernel matrix.

torch.Size([36488, 2])
We keep 1.63e+07/6.26e+08 =  2% of the original kernel matrix.

torch.Size([230289, 2])
We keep 4.25e+08/2.45e+10 =  1% of the original kernel matrix.

torch.Size([58878, 2])
We keep 3.57e+07/1.51e+09 =  2% of the original kernel matrix.

torch.Size([60830, 2])
We keep 9.77e+07/2.60e+09 =  3% of the original kernel matrix.

torch.Size([29454, 2])
We keep 1.35e+07/4.91e+08 =  2% of the original kernel matrix.

torch.Size([49153, 2])
We keep 5.11e+07/1.48e+09 =  3% of the original kernel matrix.

torch.Size([26082, 2])
We keep 1.06e+07/3.70e+08 =  2% of the original kernel matrix.

torch.Size([11566, 2])
We keep 2.34e+06/5.52e+07 =  4% of the original kernel matrix.

torch.Size([12906, 2])
We keep 2.60e+06/7.15e+07 =  3% of the original kernel matrix.

torch.Size([5971, 2])
We keep 1.06e+06/1.50e+07 =  7% of the original kernel matrix.

torch.Size([9041, 2])
We keep 1.53e+06/3.73e+07 =  4% of the original kernel matrix.

torch.Size([12911, 2])
We keep 3.78e+06/8.82e+07 =  4% of the original kernel matrix.

torch.Size([13480, 2])
We keep 3.23e+06/9.05e+07 =  3% of the original kernel matrix.

torch.Size([60543, 2])
We keep 5.36e+07/2.23e+09 =  2% of the original kernel matrix.

torch.Size([29673, 2])
We keep 1.25e+07/4.55e+08 =  2% of the original kernel matrix.

torch.Size([23719, 2])
We keep 2.04e+07/3.61e+08 =  5% of the original kernel matrix.

torch.Size([18306, 2])
We keep 5.84e+06/1.83e+08 =  3% of the original kernel matrix.

torch.Size([11237, 2])
We keep 2.98e+06/5.92e+07 =  5% of the original kernel matrix.

torch.Size([12799, 2])
We keep 2.75e+06/7.41e+07 =  3% of the original kernel matrix.

torch.Size([9741, 2])
We keep 2.06e+06/4.28e+07 =  4% of the original kernel matrix.

torch.Size([11827, 2])
We keep 2.43e+06/6.30e+07 =  3% of the original kernel matrix.

torch.Size([677193, 2])
We keep 3.30e+09/2.31e+11 =  1% of the original kernel matrix.

torch.Size([104928, 2])
We keep 1.02e+08/4.63e+09 =  2% of the original kernel matrix.

torch.Size([9606, 2])
We keep 1.83e+06/3.45e+07 =  5% of the original kernel matrix.

torch.Size([11660, 2])
We keep 2.20e+06/5.65e+07 =  3% of the original kernel matrix.

torch.Size([29475, 2])
We keep 1.94e+07/5.69e+08 =  3% of the original kernel matrix.

torch.Size([20067, 2])
We keep 6.97e+06/2.30e+08 =  3% of the original kernel matrix.

torch.Size([61589, 2])
We keep 5.59e+07/2.20e+09 =  2% of the original kernel matrix.

torch.Size([29594, 2])
We keep 1.21e+07/4.52e+08 =  2% of the original kernel matrix.

torch.Size([20837, 2])
We keep 6.41e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([17505, 2])
We keep 4.54e+06/1.42e+08 =  3% of the original kernel matrix.

torch.Size([15874, 2])
We keep 5.28e+06/1.28e+08 =  4% of the original kernel matrix.

torch.Size([15044, 2])
We keep 3.72e+06/1.09e+08 =  3% of the original kernel matrix.

torch.Size([344675, 2])
We keep 1.45e+09/7.74e+10 =  1% of the original kernel matrix.

torch.Size([74499, 2])
We keep 6.15e+07/2.68e+09 =  2% of the original kernel matrix.

torch.Size([83864, 2])
We keep 7.49e+07/3.70e+09 =  2% of the original kernel matrix.

torch.Size([35113, 2])
We keep 1.53e+07/5.86e+08 =  2% of the original kernel matrix.

torch.Size([53475, 2])
We keep 4.14e+07/1.49e+09 =  2% of the original kernel matrix.

torch.Size([27323, 2])
We keep 1.04e+07/3.72e+08 =  2% of the original kernel matrix.

torch.Size([4442, 2])
We keep 5.64e+05/6.21e+06 =  9% of the original kernel matrix.

torch.Size([7913, 2])
We keep 1.15e+06/2.40e+07 =  4% of the original kernel matrix.

torch.Size([21850, 2])
We keep 1.30e+07/2.55e+08 =  5% of the original kernel matrix.

torch.Size([17578, 2])
We keep 5.05e+06/1.54e+08 =  3% of the original kernel matrix.

torch.Size([3106, 2])
We keep 1.82e+05/2.17e+06 =  8% of the original kernel matrix.

torch.Size([6812, 2])
We keep 7.57e+05/1.42e+07 =  5% of the original kernel matrix.

torch.Size([72682, 2])
We keep 9.74e+07/2.64e+09 =  3% of the original kernel matrix.

torch.Size([32485, 2])
We keep 1.31e+07/4.95e+08 =  2% of the original kernel matrix.

torch.Size([776366, 2])
We keep 4.16e+09/2.94e+11 =  1% of the original kernel matrix.

torch.Size([113799, 2])
We keep 1.15e+08/5.22e+09 =  2% of the original kernel matrix.

torch.Size([59274, 2])
We keep 1.68e+08/2.33e+09 =  7% of the original kernel matrix.

torch.Size([28866, 2])
We keep 1.30e+07/4.65e+08 =  2% of the original kernel matrix.

torch.Size([101850, 2])
We keep 1.31e+08/6.29e+09 =  2% of the original kernel matrix.

torch.Size([38963, 2])
We keep 1.97e+07/7.64e+08 =  2% of the original kernel matrix.

torch.Size([23620, 2])
We keep 2.07e+07/3.19e+08 =  6% of the original kernel matrix.

torch.Size([18231, 2])
We keep 5.43e+06/1.72e+08 =  3% of the original kernel matrix.

torch.Size([37266, 2])
We keep 3.12e+07/7.83e+08 =  3% of the original kernel matrix.

torch.Size([22411, 2])
We keep 7.99e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([20368, 2])
We keep 1.61e+07/2.60e+08 =  6% of the original kernel matrix.

torch.Size([16974, 2])
We keep 5.06e+06/1.55e+08 =  3% of the original kernel matrix.

torch.Size([9592, 2])
We keep 2.37e+06/4.67e+07 =  5% of the original kernel matrix.

torch.Size([11448, 2])
We keep 2.42e+06/6.58e+07 =  3% of the original kernel matrix.

torch.Size([8710, 2])
We keep 2.34e+06/3.53e+07 =  6% of the original kernel matrix.

torch.Size([11093, 2])
We keep 2.23e+06/5.72e+07 =  3% of the original kernel matrix.

torch.Size([199175, 2])
We keep 3.62e+08/2.23e+10 =  1% of the original kernel matrix.

torch.Size([55148, 2])
We keep 3.43e+07/1.44e+09 =  2% of the original kernel matrix.

torch.Size([56084, 2])
We keep 4.64e+07/1.83e+09 =  2% of the original kernel matrix.

torch.Size([27833, 2])
We keep 1.14e+07/4.11e+08 =  2% of the original kernel matrix.

torch.Size([119381, 2])
We keep 3.62e+08/1.03e+10 =  3% of the original kernel matrix.

torch.Size([41318, 2])
We keep 2.45e+07/9.75e+08 =  2% of the original kernel matrix.

torch.Size([138186, 2])
We keep 3.23e+08/1.22e+10 =  2% of the original kernel matrix.

torch.Size([44821, 2])
We keep 2.66e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([33537, 2])
We keep 1.81e+07/5.97e+08 =  3% of the original kernel matrix.

torch.Size([21506, 2])
We keep 7.01e+06/2.35e+08 =  2% of the original kernel matrix.

torch.Size([7031, 2])
We keep 1.92e+06/2.46e+07 =  7% of the original kernel matrix.

torch.Size([9792, 2])
We keep 1.99e+06/4.78e+07 =  4% of the original kernel matrix.

torch.Size([39010, 2])
We keep 5.03e+07/8.79e+08 =  5% of the original kernel matrix.

torch.Size([23368, 2])
We keep 8.50e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([61763, 2])
We keep 4.92e+07/2.05e+09 =  2% of the original kernel matrix.

torch.Size([29655, 2])
We keep 1.19e+07/4.37e+08 =  2% of the original kernel matrix.

torch.Size([10239, 2])
We keep 2.67e+06/5.41e+07 =  4% of the original kernel matrix.

torch.Size([12040, 2])
We keep 2.65e+06/7.09e+07 =  3% of the original kernel matrix.

torch.Size([238976, 2])
We keep 5.29e+08/3.17e+10 =  1% of the original kernel matrix.

torch.Size([60164, 2])
We keep 4.05e+07/1.71e+09 =  2% of the original kernel matrix.

torch.Size([12859, 2])
We keep 4.78e+06/8.36e+07 =  5% of the original kernel matrix.

torch.Size([13579, 2])
We keep 2.90e+06/8.81e+07 =  3% of the original kernel matrix.

torch.Size([31868, 2])
We keep 1.74e+07/5.05e+08 =  3% of the original kernel matrix.

torch.Size([20875, 2])
We keep 6.52e+06/2.16e+08 =  3% of the original kernel matrix.

torch.Size([2871, 2])
We keep 1.63e+05/1.78e+06 =  9% of the original kernel matrix.

torch.Size([6513, 2])
We keep 7.06e+05/1.29e+07 =  5% of the original kernel matrix.

torch.Size([10242, 2])
We keep 1.98e+06/4.48e+07 =  4% of the original kernel matrix.

torch.Size([12186, 2])
We keep 2.45e+06/6.45e+07 =  3% of the original kernel matrix.

torch.Size([1897, 2])
We keep 1.00e+05/9.02e+05 = 11% of the original kernel matrix.

torch.Size([5372, 2])
We keep 5.57e+05/9.15e+06 =  6% of the original kernel matrix.

torch.Size([1943, 2])
We keep 1.10e+05/1.00e+06 = 10% of the original kernel matrix.

torch.Size([5440, 2])
We keep 5.93e+05/9.64e+06 =  6% of the original kernel matrix.

torch.Size([17364, 2])
We keep 9.68e+06/1.79e+08 =  5% of the original kernel matrix.

torch.Size([15790, 2])
We keep 4.33e+06/1.29e+08 =  3% of the original kernel matrix.

torch.Size([14646, 2])
We keep 5.97e+06/1.16e+08 =  5% of the original kernel matrix.

torch.Size([14373, 2])
We keep 3.57e+06/1.04e+08 =  3% of the original kernel matrix.

torch.Size([2977993, 2])
We keep 3.48e+10/3.10e+12 =  1% of the original kernel matrix.

torch.Size([228679, 2])
We keep 3.45e+08/1.69e+10 =  2% of the original kernel matrix.

torch.Size([6696, 2])
We keep 1.54e+06/2.11e+07 =  7% of the original kernel matrix.

torch.Size([9562, 2])
We keep 1.85e+06/4.42e+07 =  4% of the original kernel matrix.

torch.Size([119225, 2])
We keep 1.42e+08/7.97e+09 =  1% of the original kernel matrix.

torch.Size([42717, 2])
We keep 2.14e+07/8.60e+08 =  2% of the original kernel matrix.

torch.Size([6195, 2])
We keep 8.15e+05/1.28e+07 =  6% of the original kernel matrix.

torch.Size([9188, 2])
We keep 1.48e+06/3.44e+07 =  4% of the original kernel matrix.

torch.Size([10182, 2])
We keep 2.56e+06/5.47e+07 =  4% of the original kernel matrix.

torch.Size([12063, 2])
We keep 2.62e+06/7.12e+07 =  3% of the original kernel matrix.

torch.Size([7070, 2])
We keep 1.44e+06/2.14e+07 =  6% of the original kernel matrix.

torch.Size([9946, 2])
We keep 1.84e+06/4.45e+07 =  4% of the original kernel matrix.

torch.Size([172377, 2])
We keep 3.89e+08/1.85e+10 =  2% of the original kernel matrix.

torch.Size([50525, 2])
We keep 3.17e+07/1.31e+09 =  2% of the original kernel matrix.

torch.Size([4195, 2])
We keep 3.77e+05/5.26e+06 =  7% of the original kernel matrix.

torch.Size([7594, 2])
We keep 1.06e+06/2.21e+07 =  4% of the original kernel matrix.

torch.Size([458906, 2])
We keep 1.46e+09/1.04e+11 =  1% of the original kernel matrix.

torch.Size([86368, 2])
We keep 6.92e+07/3.10e+09 =  2% of the original kernel matrix.

torch.Size([4675, 2])
We keep 4.31e+05/5.56e+06 =  7% of the original kernel matrix.

torch.Size([7973, 2])
We keep 1.09e+06/2.27e+07 =  4% of the original kernel matrix.

torch.Size([103364, 2])
We keep 1.22e+08/5.48e+09 =  2% of the original kernel matrix.

torch.Size([39383, 2])
We keep 1.81e+07/7.13e+08 =  2% of the original kernel matrix.

torch.Size([17902, 2])
We keep 7.02e+06/1.64e+08 =  4% of the original kernel matrix.

torch.Size([15958, 2])
We keep 3.93e+06/1.23e+08 =  3% of the original kernel matrix.

torch.Size([75149, 2])
We keep 7.48e+07/3.19e+09 =  2% of the original kernel matrix.

torch.Size([32859, 2])
We keep 1.45e+07/5.44e+08 =  2% of the original kernel matrix.

torch.Size([71575, 2])
We keep 1.84e+08/4.18e+09 =  4% of the original kernel matrix.

torch.Size([32109, 2])
We keep 1.56e+07/6.22e+08 =  2% of the original kernel matrix.

torch.Size([10584, 2])
We keep 3.24e+06/5.74e+07 =  5% of the original kernel matrix.

torch.Size([12300, 2])
We keep 2.65e+06/7.30e+07 =  3% of the original kernel matrix.

torch.Size([9330, 2])
We keep 1.91e+06/3.92e+07 =  4% of the original kernel matrix.

torch.Size([11400, 2])
We keep 2.30e+06/6.03e+07 =  3% of the original kernel matrix.

torch.Size([37819, 2])
We keep 2.35e+07/8.64e+08 =  2% of the original kernel matrix.

torch.Size([23124, 2])
We keep 8.29e+06/2.83e+08 =  2% of the original kernel matrix.

torch.Size([6664, 2])
We keep 8.62e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([9627, 2])
We keep 1.57e+06/3.87e+07 =  4% of the original kernel matrix.

torch.Size([8484, 2])
We keep 1.50e+06/2.97e+07 =  5% of the original kernel matrix.

torch.Size([10914, 2])
We keep 2.07e+06/5.25e+07 =  3% of the original kernel matrix.

torch.Size([55280, 2])
We keep 4.73e+07/1.75e+09 =  2% of the original kernel matrix.

torch.Size([27770, 2])
We keep 1.11e+07/4.03e+08 =  2% of the original kernel matrix.

torch.Size([6297, 2])
We keep 8.96e+05/1.40e+07 =  6% of the original kernel matrix.

torch.Size([9388, 2])
We keep 1.54e+06/3.60e+07 =  4% of the original kernel matrix.

torch.Size([45622, 2])
We keep 4.47e+07/1.29e+09 =  3% of the original kernel matrix.

torch.Size([25060, 2])
We keep 9.98e+06/3.45e+08 =  2% of the original kernel matrix.

torch.Size([9821, 2])
We keep 4.39e+06/4.91e+07 =  8% of the original kernel matrix.

torch.Size([11576, 2])
We keep 2.39e+06/6.75e+07 =  3% of the original kernel matrix.

torch.Size([79597, 2])
We keep 1.37e+08/4.04e+09 =  3% of the original kernel matrix.

torch.Size([33831, 2])
We keep 1.64e+07/6.12e+08 =  2% of the original kernel matrix.

torch.Size([212470, 2])
We keep 5.75e+08/2.88e+10 =  2% of the original kernel matrix.

torch.Size([55828, 2])
We keep 3.89e+07/1.63e+09 =  2% of the original kernel matrix.

torch.Size([38810, 2])
We keep 2.31e+07/7.00e+08 =  3% of the original kernel matrix.

torch.Size([22886, 2])
We keep 7.47e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([7863, 2])
We keep 1.24e+06/2.43e+07 =  5% of the original kernel matrix.

torch.Size([10547, 2])
We keep 1.90e+06/4.75e+07 =  4% of the original kernel matrix.

torch.Size([36226, 2])
We keep 3.08e+07/7.71e+08 =  3% of the original kernel matrix.

torch.Size([22128, 2])
We keep 7.90e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([40812, 2])
We keep 3.45e+07/1.04e+09 =  3% of the original kernel matrix.

torch.Size([23174, 2])
We keep 9.00e+06/3.10e+08 =  2% of the original kernel matrix.

torch.Size([7825, 2])
We keep 2.16e+06/2.27e+07 =  9% of the original kernel matrix.

torch.Size([10377, 2])
We keep 1.86e+06/4.59e+07 =  4% of the original kernel matrix.

torch.Size([19258, 2])
We keep 7.23e+06/1.92e+08 =  3% of the original kernel matrix.

torch.Size([16437, 2])
We keep 4.35e+06/1.33e+08 =  3% of the original kernel matrix.

torch.Size([13648, 2])
We keep 8.59e+06/1.10e+08 =  7% of the original kernel matrix.

torch.Size([13607, 2])
We keep 3.55e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([34796, 2])
We keep 2.73e+07/6.48e+08 =  4% of the original kernel matrix.

torch.Size([21836, 2])
We keep 7.03e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([40477, 2])
We keep 6.50e+07/1.52e+09 =  4% of the original kernel matrix.

torch.Size([22339, 2])
We keep 1.05e+07/3.76e+08 =  2% of the original kernel matrix.

torch.Size([31165, 2])
We keep 6.57e+07/7.40e+08 =  8% of the original kernel matrix.

torch.Size([20466, 2])
We keep 7.91e+06/2.62e+08 =  3% of the original kernel matrix.

torch.Size([11343, 2])
We keep 2.52e+06/5.94e+07 =  4% of the original kernel matrix.

torch.Size([13106, 2])
We keep 2.80e+06/7.43e+07 =  3% of the original kernel matrix.

torch.Size([3940, 2])
We keep 3.22e+05/4.02e+06 =  8% of the original kernel matrix.

torch.Size([7315, 2])
We keep 9.53e+05/1.93e+07 =  4% of the original kernel matrix.

torch.Size([4232, 2])
We keep 3.67e+05/4.33e+06 =  8% of the original kernel matrix.

torch.Size([7737, 2])
We keep 9.74e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([44739, 2])
We keep 4.38e+07/1.37e+09 =  3% of the original kernel matrix.

torch.Size([24268, 2])
We keep 9.88e+06/3.56e+08 =  2% of the original kernel matrix.

torch.Size([33032, 2])
We keep 2.71e+07/5.97e+08 =  4% of the original kernel matrix.

torch.Size([21040, 2])
We keep 6.94e+06/2.35e+08 =  2% of the original kernel matrix.

torch.Size([36066, 2])
We keep 1.79e+07/5.89e+08 =  3% of the original kernel matrix.

torch.Size([22258, 2])
We keep 6.88e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([7242, 2])
We keep 1.22e+06/2.21e+07 =  5% of the original kernel matrix.

torch.Size([10051, 2])
We keep 1.87e+06/4.52e+07 =  4% of the original kernel matrix.

torch.Size([12977, 2])
We keep 3.16e+06/8.08e+07 =  3% of the original kernel matrix.

torch.Size([13770, 2])
We keep 3.10e+06/8.66e+07 =  3% of the original kernel matrix.

torch.Size([20820, 2])
We keep 9.18e+06/2.43e+08 =  3% of the original kernel matrix.

torch.Size([17170, 2])
We keep 4.85e+06/1.50e+08 =  3% of the original kernel matrix.

torch.Size([19325, 2])
We keep 7.03e+06/2.20e+08 =  3% of the original kernel matrix.

torch.Size([16944, 2])
We keep 4.62e+06/1.43e+08 =  3% of the original kernel matrix.

torch.Size([12095, 2])
We keep 4.10e+06/7.20e+07 =  5% of the original kernel matrix.

torch.Size([13274, 2])
We keep 3.04e+06/8.17e+07 =  3% of the original kernel matrix.

torch.Size([39389, 2])
We keep 1.96e+07/6.92e+08 =  2% of the original kernel matrix.

torch.Size([23174, 2])
We keep 7.46e+06/2.53e+08 =  2% of the original kernel matrix.

torch.Size([8629, 2])
We keep 2.45e+06/2.83e+07 =  8% of the original kernel matrix.

torch.Size([11030, 2])
We keep 2.06e+06/5.12e+07 =  4% of the original kernel matrix.

torch.Size([19578, 2])
We keep 6.54e+06/1.91e+08 =  3% of the original kernel matrix.

torch.Size([16782, 2])
We keep 4.34e+06/1.33e+08 =  3% of the original kernel matrix.

torch.Size([1721, 2])
We keep 6.84e+04/5.85e+05 = 11% of the original kernel matrix.

torch.Size([5238, 2])
We keep 4.81e+05/7.37e+06 =  6% of the original kernel matrix.

torch.Size([5791, 2])
We keep 6.96e+05/9.84e+06 =  7% of the original kernel matrix.

torch.Size([8969, 2])
We keep 1.37e+06/3.02e+07 =  4% of the original kernel matrix.

torch.Size([72851, 2])
We keep 5.59e+07/2.70e+09 =  2% of the original kernel matrix.

torch.Size([32672, 2])
We keep 1.32e+07/5.00e+08 =  2% of the original kernel matrix.

torch.Size([11941, 2])
We keep 4.31e+06/8.09e+07 =  5% of the original kernel matrix.

torch.Size([12882, 2])
We keep 3.14e+06/8.66e+07 =  3% of the original kernel matrix.

torch.Size([42625, 2])
We keep 2.16e+07/8.48e+08 =  2% of the original kernel matrix.

torch.Size([24188, 2])
We keep 8.09e+06/2.80e+08 =  2% of the original kernel matrix.

torch.Size([2856, 2])
We keep 1.65e+05/1.65e+06 = 10% of the original kernel matrix.

torch.Size([6451, 2])
We keep 7.03e+05/1.24e+07 =  5% of the original kernel matrix.

torch.Size([5436, 2])
We keep 6.87e+05/8.00e+06 =  8% of the original kernel matrix.

torch.Size([8650, 2])
We keep 1.21e+06/2.72e+07 =  4% of the original kernel matrix.

torch.Size([14060, 2])
We keep 6.03e+06/1.30e+08 =  4% of the original kernel matrix.

torch.Size([14267, 2])
We keep 3.77e+06/1.10e+08 =  3% of the original kernel matrix.

torch.Size([47515, 2])
We keep 3.12e+07/1.09e+09 =  2% of the original kernel matrix.

torch.Size([25384, 2])
We keep 9.10e+06/3.18e+08 =  2% of the original kernel matrix.

torch.Size([22112, 2])
We keep 9.94e+06/3.06e+08 =  3% of the original kernel matrix.

torch.Size([17909, 2])
We keep 5.40e+06/1.69e+08 =  3% of the original kernel matrix.

torch.Size([3332, 2])
We keep 2.85e+05/2.89e+06 =  9% of the original kernel matrix.

torch.Size([6799, 2])
We keep 8.70e+05/1.64e+07 =  5% of the original kernel matrix.

torch.Size([6948, 2])
We keep 1.14e+06/1.95e+07 =  5% of the original kernel matrix.

torch.Size([9741, 2])
We keep 1.78e+06/4.25e+07 =  4% of the original kernel matrix.

torch.Size([32720, 2])
We keep 2.46e+07/5.52e+08 =  4% of the original kernel matrix.

torch.Size([21072, 2])
We keep 6.97e+06/2.26e+08 =  3% of the original kernel matrix.

torch.Size([13712, 2])
We keep 3.19e+06/8.65e+07 =  3% of the original kernel matrix.

torch.Size([14068, 2])
We keep 3.12e+06/8.96e+07 =  3% of the original kernel matrix.

torch.Size([37007, 2])
We keep 3.06e+07/7.03e+08 =  4% of the original kernel matrix.

torch.Size([22471, 2])
We keep 7.46e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([3340, 2])
We keep 2.65e+05/2.82e+06 =  9% of the original kernel matrix.

torch.Size([6865, 2])
We keep 8.53e+05/1.62e+07 =  5% of the original kernel matrix.

torch.Size([4865, 2])
We keep 7.44e+05/7.73e+06 =  9% of the original kernel matrix.

torch.Size([8042, 2])
We keep 1.24e+06/2.68e+07 =  4% of the original kernel matrix.

torch.Size([34221, 2])
We keep 2.39e+07/6.85e+08 =  3% of the original kernel matrix.

torch.Size([21011, 2])
We keep 7.59e+06/2.52e+08 =  3% of the original kernel matrix.

torch.Size([4221, 2])
We keep 7.20e+05/6.36e+06 = 11% of the original kernel matrix.

torch.Size([7594, 2])
We keep 1.15e+06/2.43e+07 =  4% of the original kernel matrix.

torch.Size([108850, 2])
We keep 1.74e+08/7.37e+09 =  2% of the original kernel matrix.

torch.Size([40491, 2])
We keep 2.07e+07/8.27e+08 =  2% of the original kernel matrix.

torch.Size([3863, 2])
We keep 2.96e+05/3.59e+06 =  8% of the original kernel matrix.

torch.Size([7421, 2])
We keep 9.30e+05/1.83e+07 =  5% of the original kernel matrix.

torch.Size([42238, 2])
We keep 4.40e+07/1.07e+09 =  4% of the original kernel matrix.

torch.Size([23858, 2])
We keep 9.17e+06/3.15e+08 =  2% of the original kernel matrix.

torch.Size([63236, 2])
We keep 5.29e+07/2.00e+09 =  2% of the original kernel matrix.

torch.Size([30200, 2])
We keep 1.18e+07/4.30e+08 =  2% of the original kernel matrix.

torch.Size([14921, 2])
We keep 5.67e+06/1.14e+08 =  4% of the original kernel matrix.

torch.Size([14632, 2])
We keep 3.60e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([84909, 2])
We keep 9.85e+07/4.17e+09 =  2% of the original kernel matrix.

torch.Size([35155, 2])
We keep 1.64e+07/6.22e+08 =  2% of the original kernel matrix.

torch.Size([56776, 2])
We keep 3.92e+07/1.53e+09 =  2% of the original kernel matrix.

torch.Size([28337, 2])
We keep 1.03e+07/3.77e+08 =  2% of the original kernel matrix.

torch.Size([1325, 2])
We keep 3.85e+04/2.89e+05 = 13% of the original kernel matrix.

torch.Size([4856, 2])
We keep 3.82e+05/5.18e+06 =  7% of the original kernel matrix.

torch.Size([28346, 2])
We keep 1.32e+07/4.13e+08 =  3% of the original kernel matrix.

torch.Size([19862, 2])
We keep 6.05e+06/1.96e+08 =  3% of the original kernel matrix.

torch.Size([9309, 2])
We keep 1.53e+06/3.42e+07 =  4% of the original kernel matrix.

torch.Size([11510, 2])
We keep 2.18e+06/5.64e+07 =  3% of the original kernel matrix.

torch.Size([31128, 2])
We keep 1.48e+07/4.49e+08 =  3% of the original kernel matrix.

torch.Size([20870, 2])
We keep 6.29e+06/2.04e+08 =  3% of the original kernel matrix.

torch.Size([43105, 2])
We keep 5.72e+07/1.17e+09 =  4% of the original kernel matrix.

torch.Size([24543, 2])
We keep 8.99e+06/3.29e+08 =  2% of the original kernel matrix.

torch.Size([19596, 2])
We keep 9.77e+06/2.11e+08 =  4% of the original kernel matrix.

torch.Size([16385, 2])
We keep 4.43e+06/1.40e+08 =  3% of the original kernel matrix.

torch.Size([33241, 2])
We keep 1.47e+07/5.07e+08 =  2% of the original kernel matrix.

torch.Size([21493, 2])
We keep 6.47e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([9205, 2])
We keep 1.64e+06/3.56e+07 =  4% of the original kernel matrix.

torch.Size([11405, 2])
We keep 2.20e+06/5.75e+07 =  3% of the original kernel matrix.

torch.Size([23044, 2])
We keep 1.56e+07/3.85e+08 =  4% of the original kernel matrix.

torch.Size([17469, 2])
We keep 5.67e+06/1.89e+08 =  3% of the original kernel matrix.

torch.Size([4256, 2])
We keep 3.28e+05/4.30e+06 =  7% of the original kernel matrix.

torch.Size([7725, 2])
We keep 9.85e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([6637, 2])
We keep 1.06e+06/1.62e+07 =  6% of the original kernel matrix.

torch.Size([9507, 2])
We keep 1.65e+06/3.88e+07 =  4% of the original kernel matrix.

torch.Size([58200, 2])
We keep 6.14e+07/1.89e+09 =  3% of the original kernel matrix.

torch.Size([28607, 2])
We keep 1.16e+07/4.19e+08 =  2% of the original kernel matrix.

torch.Size([157174, 2])
We keep 3.19e+08/1.52e+10 =  2% of the original kernel matrix.

torch.Size([48874, 2])
We keep 2.90e+07/1.19e+09 =  2% of the original kernel matrix.

torch.Size([11811, 2])
We keep 3.41e+06/7.96e+07 =  4% of the original kernel matrix.

torch.Size([12755, 2])
We keep 3.06e+06/8.59e+07 =  3% of the original kernel matrix.

torch.Size([9319, 2])
We keep 1.71e+06/3.91e+07 =  4% of the original kernel matrix.

torch.Size([11519, 2])
We keep 2.29e+06/6.02e+07 =  3% of the original kernel matrix.

torch.Size([14306, 2])
We keep 4.01e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([14497, 2])
We keep 3.38e+06/9.69e+07 =  3% of the original kernel matrix.

torch.Size([30064, 2])
We keep 2.69e+07/6.38e+08 =  4% of the original kernel matrix.

torch.Size([20419, 2])
We keep 7.34e+06/2.43e+08 =  3% of the original kernel matrix.

torch.Size([29514, 2])
We keep 1.31e+07/4.40e+08 =  2% of the original kernel matrix.

torch.Size([20237, 2])
We keep 6.15e+06/2.02e+08 =  3% of the original kernel matrix.

torch.Size([4777, 2])
We keep 4.76e+05/7.39e+06 =  6% of the original kernel matrix.

torch.Size([8171, 2])
We keep 1.20e+06/2.62e+07 =  4% of the original kernel matrix.

torch.Size([20836, 2])
We keep 1.17e+07/2.57e+08 =  4% of the original kernel matrix.

torch.Size([17376, 2])
We keep 4.94e+06/1.54e+08 =  3% of the original kernel matrix.

torch.Size([77774, 2])
We keep 7.28e+07/3.17e+09 =  2% of the original kernel matrix.

torch.Size([33622, 2])
We keep 1.44e+07/5.42e+08 =  2% of the original kernel matrix.

torch.Size([7010, 2])
We keep 1.78e+06/2.39e+07 =  7% of the original kernel matrix.

torch.Size([9754, 2])
We keep 1.94e+06/4.70e+07 =  4% of the original kernel matrix.

torch.Size([39242, 2])
We keep 3.68e+07/7.74e+08 =  4% of the original kernel matrix.

torch.Size([23249, 2])
We keep 8.02e+06/2.68e+08 =  2% of the original kernel matrix.

torch.Size([6663, 2])
We keep 8.58e+05/1.48e+07 =  5% of the original kernel matrix.

torch.Size([9637, 2])
We keep 1.54e+06/3.70e+07 =  4% of the original kernel matrix.

time for making ranges is 1.9891488552093506
Sorting X and nu_X
time for sorting X is 0.06842398643493652
Sorting Z and nu_Z
time for sorting Z is 0.00024962425231933594
Starting Optim
sum tnu_Z before tensor(15886817., device='cuda:0')
c= tensor(1530.1538, device='cuda:0')
c= tensor(77121.5312, device='cuda:0')
c= tensor(80017.5547, device='cuda:0')
c= tensor(153384.0781, device='cuda:0')
c= tensor(178951.9062, device='cuda:0')
c= tensor(523188.1562, device='cuda:0')
c= tensor(895564., device='cuda:0')
c= tensor(1076414., device='cuda:0')
c= tensor(1122592.3750, device='cuda:0')
c= tensor(1453876.2500, device='cuda:0')
c= tensor(1470151., device='cuda:0')
c= tensor(2798312., device='cuda:0')
c= tensor(2808487.2500, device='cuda:0')
c= tensor(26804028., device='cuda:0')
c= tensor(26881100., device='cuda:0')
c= tensor(27257470., device='cuda:0')
c= tensor(28032448., device='cuda:0')
c= tensor(28224020., device='cuda:0')
c= tensor(30576476., device='cuda:0')
c= tensor(33109908., device='cuda:0')
c= tensor(34138128., device='cuda:0')
c= tensor(37371892., device='cuda:0')
c= tensor(37388680., device='cuda:0')
c= tensor(38306972., device='cuda:0')
c= tensor(38311936., device='cuda:0')
c= tensor(38659580., device='cuda:0')
c= tensor(39093800., device='cuda:0')
c= tensor(39099348., device='cuda:0')
c= tensor(39313784., device='cuda:0')
c= tensor(1.3081e+08, device='cuda:0')
c= tensor(1.3083e+08, device='cuda:0')
c= tensor(2.0718e+08, device='cuda:0')
c= tensor(2.0727e+08, device='cuda:0')
c= tensor(2.0729e+08, device='cuda:0')
c= tensor(2.0739e+08, device='cuda:0')
c= tensor(2.1086e+08, device='cuda:0')
c= tensor(2.1187e+08, device='cuda:0')
c= tensor(2.1187e+08, device='cuda:0')
c= tensor(2.1188e+08, device='cuda:0')
c= tensor(2.1188e+08, device='cuda:0')
c= tensor(2.1188e+08, device='cuda:0')
c= tensor(2.1189e+08, device='cuda:0')
c= tensor(2.1189e+08, device='cuda:0')
c= tensor(2.1190e+08, device='cuda:0')
c= tensor(2.1190e+08, device='cuda:0')
c= tensor(2.1190e+08, device='cuda:0')
c= tensor(2.1190e+08, device='cuda:0')
c= tensor(2.1191e+08, device='cuda:0')
c= tensor(2.1191e+08, device='cuda:0')
c= tensor(2.1196e+08, device='cuda:0')
c= tensor(2.1203e+08, device='cuda:0')
c= tensor(2.1203e+08, device='cuda:0')
c= tensor(2.1206e+08, device='cuda:0')
c= tensor(2.1206e+08, device='cuda:0')
c= tensor(2.1206e+08, device='cuda:0')
c= tensor(2.1209e+08, device='cuda:0')
c= tensor(2.1209e+08, device='cuda:0')
c= tensor(2.1209e+08, device='cuda:0')
c= tensor(2.1209e+08, device='cuda:0')
c= tensor(2.1210e+08, device='cuda:0')
c= tensor(2.1210e+08, device='cuda:0')
c= tensor(2.1210e+08, device='cuda:0')
c= tensor(2.1211e+08, device='cuda:0')
c= tensor(2.1213e+08, device='cuda:0')
c= tensor(2.1213e+08, device='cuda:0')
c= tensor(2.1214e+08, device='cuda:0')
c= tensor(2.1214e+08, device='cuda:0')
c= tensor(2.1215e+08, device='cuda:0')
c= tensor(2.1216e+08, device='cuda:0')
c= tensor(2.1216e+08, device='cuda:0')
c= tensor(2.1217e+08, device='cuda:0')
c= tensor(2.1217e+08, device='cuda:0')
c= tensor(2.1217e+08, device='cuda:0')
c= tensor(2.1218e+08, device='cuda:0')
c= tensor(2.1218e+08, device='cuda:0')
c= tensor(2.1219e+08, device='cuda:0')
c= tensor(2.1219e+08, device='cuda:0')
c= tensor(2.1219e+08, device='cuda:0')
c= tensor(2.1219e+08, device='cuda:0')
c= tensor(2.1222e+08, device='cuda:0')
c= tensor(2.1222e+08, device='cuda:0')
c= tensor(2.1222e+08, device='cuda:0')
c= tensor(2.1223e+08, device='cuda:0')
c= tensor(2.1223e+08, device='cuda:0')
c= tensor(2.1223e+08, device='cuda:0')
c= tensor(2.1223e+08, device='cuda:0')
c= tensor(2.1224e+08, device='cuda:0')
c= tensor(2.1224e+08, device='cuda:0')
c= tensor(2.1224e+08, device='cuda:0')
c= tensor(2.1224e+08, device='cuda:0')
c= tensor(2.1225e+08, device='cuda:0')
c= tensor(2.1225e+08, device='cuda:0')
c= tensor(2.1225e+08, device='cuda:0')
c= tensor(2.1225e+08, device='cuda:0')
c= tensor(2.1225e+08, device='cuda:0')
c= tensor(2.1226e+08, device='cuda:0')
c= tensor(2.1227e+08, device='cuda:0')
c= tensor(2.1227e+08, device='cuda:0')
c= tensor(2.1228e+08, device='cuda:0')
c= tensor(2.1228e+08, device='cuda:0')
c= tensor(2.1230e+08, device='cuda:0')
c= tensor(2.1230e+08, device='cuda:0')
c= tensor(2.1230e+08, device='cuda:0')
c= tensor(2.1231e+08, device='cuda:0')
c= tensor(2.1231e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1233e+08, device='cuda:0')
c= tensor(2.1233e+08, device='cuda:0')
c= tensor(2.1233e+08, device='cuda:0')
c= tensor(2.1234e+08, device='cuda:0')
c= tensor(2.1234e+08, device='cuda:0')
c= tensor(2.1234e+08, device='cuda:0')
c= tensor(2.1236e+08, device='cuda:0')
c= tensor(2.1236e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1239e+08, device='cuda:0')
c= tensor(2.1240e+08, device='cuda:0')
c= tensor(2.1240e+08, device='cuda:0')
c= tensor(2.1242e+08, device='cuda:0')
c= tensor(2.1242e+08, device='cuda:0')
c= tensor(2.1242e+08, device='cuda:0')
c= tensor(2.1242e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1244e+08, device='cuda:0')
c= tensor(2.1244e+08, device='cuda:0')
c= tensor(2.1246e+08, device='cuda:0')
c= tensor(2.1247e+08, device='cuda:0')
c= tensor(2.1247e+08, device='cuda:0')
c= tensor(2.1247e+08, device='cuda:0')
c= tensor(2.1247e+08, device='cuda:0')
c= tensor(2.1248e+08, device='cuda:0')
c= tensor(2.1248e+08, device='cuda:0')
c= tensor(2.1248e+08, device='cuda:0')
c= tensor(2.1248e+08, device='cuda:0')
c= tensor(2.1249e+08, device='cuda:0')
c= tensor(2.1249e+08, device='cuda:0')
c= tensor(2.1251e+08, device='cuda:0')
c= tensor(2.1251e+08, device='cuda:0')
c= tensor(2.1257e+08, device='cuda:0')
c= tensor(2.1258e+08, device='cuda:0')
c= tensor(2.1258e+08, device='cuda:0')
c= tensor(2.1258e+08, device='cuda:0')
c= tensor(2.1258e+08, device='cuda:0')
c= tensor(2.1260e+08, device='cuda:0')
c= tensor(2.1260e+08, device='cuda:0')
c= tensor(2.1261e+08, device='cuda:0')
c= tensor(2.1261e+08, device='cuda:0')
c= tensor(2.1262e+08, device='cuda:0')
c= tensor(2.1262e+08, device='cuda:0')
c= tensor(2.1262e+08, device='cuda:0')
c= tensor(2.1263e+08, device='cuda:0')
c= tensor(2.1263e+08, device='cuda:0')
c= tensor(2.1263e+08, device='cuda:0')
c= tensor(2.1263e+08, device='cuda:0')
c= tensor(2.1263e+08, device='cuda:0')
c= tensor(2.1264e+08, device='cuda:0')
c= tensor(2.1264e+08, device='cuda:0')
c= tensor(2.1265e+08, device='cuda:0')
c= tensor(2.1265e+08, device='cuda:0')
c= tensor(2.1266e+08, device='cuda:0')
c= tensor(2.1267e+08, device='cuda:0')
c= tensor(2.1267e+08, device='cuda:0')
c= tensor(2.1268e+08, device='cuda:0')
c= tensor(2.1268e+08, device='cuda:0')
c= tensor(2.1269e+08, device='cuda:0')
c= tensor(2.1269e+08, device='cuda:0')
c= tensor(2.1270e+08, device='cuda:0')
c= tensor(2.1270e+08, device='cuda:0')
c= tensor(2.1270e+08, device='cuda:0')
c= tensor(2.1270e+08, device='cuda:0')
c= tensor(2.1271e+08, device='cuda:0')
c= tensor(2.1272e+08, device='cuda:0')
c= tensor(2.1272e+08, device='cuda:0')
c= tensor(2.1272e+08, device='cuda:0')
c= tensor(2.1272e+08, device='cuda:0')
c= tensor(2.1273e+08, device='cuda:0')
c= tensor(2.1274e+08, device='cuda:0')
c= tensor(2.1275e+08, device='cuda:0')
c= tensor(2.1275e+08, device='cuda:0')
c= tensor(2.1276e+08, device='cuda:0')
c= tensor(2.1276e+08, device='cuda:0')
c= tensor(2.1276e+08, device='cuda:0')
c= tensor(2.1276e+08, device='cuda:0')
c= tensor(2.1277e+08, device='cuda:0')
c= tensor(2.1278e+08, device='cuda:0')
c= tensor(2.1278e+08, device='cuda:0')
c= tensor(2.1279e+08, device='cuda:0')
c= tensor(2.1279e+08, device='cuda:0')
c= tensor(2.1279e+08, device='cuda:0')
c= tensor(2.1280e+08, device='cuda:0')
c= tensor(2.1280e+08, device='cuda:0')
c= tensor(2.1280e+08, device='cuda:0')
c= tensor(2.1281e+08, device='cuda:0')
c= tensor(2.1282e+08, device='cuda:0')
c= tensor(2.1284e+08, device='cuda:0')
c= tensor(2.1284e+08, device='cuda:0')
c= tensor(2.1284e+08, device='cuda:0')
c= tensor(2.1285e+08, device='cuda:0')
c= tensor(2.1285e+08, device='cuda:0')
c= tensor(2.1285e+08, device='cuda:0')
c= tensor(2.1285e+08, device='cuda:0')
c= tensor(2.1286e+08, device='cuda:0')
c= tensor(2.1286e+08, device='cuda:0')
c= tensor(2.1286e+08, device='cuda:0')
c= tensor(2.1287e+08, device='cuda:0')
c= tensor(2.1287e+08, device='cuda:0')
c= tensor(2.1288e+08, device='cuda:0')
c= tensor(2.1288e+08, device='cuda:0')
c= tensor(2.1288e+08, device='cuda:0')
c= tensor(2.1288e+08, device='cuda:0')
c= tensor(2.1289e+08, device='cuda:0')
c= tensor(2.1289e+08, device='cuda:0')
c= tensor(2.1289e+08, device='cuda:0')
c= tensor(2.1289e+08, device='cuda:0')
c= tensor(2.1290e+08, device='cuda:0')
c= tensor(2.1290e+08, device='cuda:0')
c= tensor(2.1291e+08, device='cuda:0')
c= tensor(2.1291e+08, device='cuda:0')
c= tensor(2.1291e+08, device='cuda:0')
c= tensor(2.1294e+08, device='cuda:0')
c= tensor(2.1295e+08, device='cuda:0')
c= tensor(2.1295e+08, device='cuda:0')
c= tensor(2.1310e+08, device='cuda:0')
c= tensor(2.1478e+08, device='cuda:0')
c= tensor(2.1479e+08, device='cuda:0')
c= tensor(2.1480e+08, device='cuda:0')
c= tensor(2.1480e+08, device='cuda:0')
c= tensor(2.1480e+08, device='cuda:0')
c= tensor(2.1482e+08, device='cuda:0')
c= tensor(2.3189e+08, device='cuda:0')
c= tensor(2.3190e+08, device='cuda:0')
c= tensor(2.3819e+08, device='cuda:0')
c= tensor(2.3826e+08, device='cuda:0')
c= tensor(2.3844e+08, device='cuda:0')
c= tensor(2.3878e+08, device='cuda:0')
c= tensor(2.3878e+08, device='cuda:0')
c= tensor(2.3879e+08, device='cuda:0')
c= tensor(2.4173e+08, device='cuda:0')
c= tensor(2.6353e+08, device='cuda:0')
c= tensor(2.6354e+08, device='cuda:0')
c= tensor(2.6368e+08, device='cuda:0')
c= tensor(2.6373e+08, device='cuda:0')
c= tensor(2.6382e+08, device='cuda:0')
c= tensor(2.6741e+08, device='cuda:0')
c= tensor(2.6778e+08, device='cuda:0')
c= tensor(2.6790e+08, device='cuda:0')
c= tensor(2.6800e+08, device='cuda:0')
c= tensor(2.6801e+08, device='cuda:0')
c= tensor(2.7718e+08, device='cuda:0')
c= tensor(2.7719e+08, device='cuda:0')
c= tensor(2.7720e+08, device='cuda:0')
c= tensor(2.7732e+08, device='cuda:0')
c= tensor(2.7842e+08, device='cuda:0')
c= tensor(2.8602e+08, device='cuda:0')
c= tensor(2.8670e+08, device='cuda:0')
c= tensor(2.8671e+08, device='cuda:0')
c= tensor(2.8701e+08, device='cuda:0')
c= tensor(2.8701e+08, device='cuda:0')
c= tensor(2.8745e+08, device='cuda:0')
c= tensor(2.8775e+08, device='cuda:0')
c= tensor(2.8776e+08, device='cuda:0')
c= tensor(2.8831e+08, device='cuda:0')
c= tensor(2.8831e+08, device='cuda:0')
c= tensor(2.8833e+08, device='cuda:0')
c= tensor(2.9068e+08, device='cuda:0')
c= tensor(2.9106e+08, device='cuda:0')
c= tensor(2.9340e+08, device='cuda:0')
c= tensor(2.9341e+08, device='cuda:0')
c= tensor(3.0406e+08, device='cuda:0')
c= tensor(3.0409e+08, device='cuda:0')
c= tensor(3.0426e+08, device='cuda:0')
c= tensor(3.0506e+08, device='cuda:0')
c= tensor(3.0506e+08, device='cuda:0')
c= tensor(3.0642e+08, device='cuda:0')
c= tensor(3.1345e+08, device='cuda:0')
c= tensor(3.3653e+08, device='cuda:0')
c= tensor(3.3721e+08, device='cuda:0')
c= tensor(3.3738e+08, device='cuda:0')
c= tensor(3.3746e+08, device='cuda:0')
c= tensor(3.3746e+08, device='cuda:0')
c= tensor(3.3935e+08, device='cuda:0')
c= tensor(3.3937e+08, device='cuda:0')
c= tensor(3.3959e+08, device='cuda:0')
c= tensor(3.4126e+08, device='cuda:0')
c= tensor(3.4131e+08, device='cuda:0')
c= tensor(3.4137e+08, device='cuda:0')
c= tensor(3.4137e+08, device='cuda:0')
c= tensor(3.4601e+08, device='cuda:0')
c= tensor(3.4606e+08, device='cuda:0')
c= tensor(3.4651e+08, device='cuda:0')
c= tensor(3.4652e+08, device='cuda:0')
c= tensor(3.4773e+08, device='cuda:0')
c= tensor(3.4781e+08, device='cuda:0')
c= tensor(3.5244e+08, device='cuda:0')
c= tensor(3.5249e+08, device='cuda:0')
c= tensor(3.5422e+08, device='cuda:0')
c= tensor(3.5424e+08, device='cuda:0')
c= tensor(3.6021e+08, device='cuda:0')
c= tensor(3.6059e+08, device='cuda:0')
c= tensor(3.6063e+08, device='cuda:0')
c= tensor(3.6258e+08, device='cuda:0')
c= tensor(3.6428e+08, device='cuda:0')
c= tensor(3.6428e+08, device='cuda:0')
c= tensor(3.7010e+08, device='cuda:0')
c= tensor(3.7380e+08, device='cuda:0')
c= tensor(3.8543e+08, device='cuda:0')
c= tensor(3.8555e+08, device='cuda:0')
c= tensor(3.8555e+08, device='cuda:0')
c= tensor(3.8556e+08, device='cuda:0')
c= tensor(3.8563e+08, device='cuda:0')
c= tensor(3.8571e+08, device='cuda:0')
c= tensor(3.8598e+08, device='cuda:0')
c= tensor(3.8598e+08, device='cuda:0')
c= tensor(3.8738e+08, device='cuda:0')
c= tensor(3.8916e+08, device='cuda:0')
c= tensor(3.8917e+08, device='cuda:0')
c= tensor(3.8918e+08, device='cuda:0')
c= tensor(3.8952e+08, device='cuda:0')
c= tensor(3.8954e+08, device='cuda:0')
c= tensor(3.8955e+08, device='cuda:0')
c= tensor(3.8957e+08, device='cuda:0')
c= tensor(3.8957e+08, device='cuda:0')
c= tensor(3.8962e+08, device='cuda:0')
c= tensor(3.8986e+08, device='cuda:0')
c= tensor(3.8996e+08, device='cuda:0')
c= tensor(3.9045e+08, device='cuda:0')
c= tensor(3.9046e+08, device='cuda:0')
c= tensor(4.4177e+08, device='cuda:0')
c= tensor(4.4179e+08, device='cuda:0')
c= tensor(4.4461e+08, device='cuda:0')
c= tensor(4.4461e+08, device='cuda:0')
c= tensor(4.4462e+08, device='cuda:0')
c= tensor(4.4462e+08, device='cuda:0')
c= tensor(4.4464e+08, device='cuda:0')
c= tensor(4.4465e+08, device='cuda:0')
c= tensor(4.4877e+08, device='cuda:0')
c= tensor(4.4877e+08, device='cuda:0')
c= tensor(4.4877e+08, device='cuda:0')
c= tensor(4.5315e+08, device='cuda:0')
c= tensor(4.5365e+08, device='cuda:0')
c= tensor(4.5399e+08, device='cuda:0')
c= tensor(4.5541e+08, device='cuda:0')
c= tensor(4.6180e+08, device='cuda:0')
c= tensor(4.6181e+08, device='cuda:0')
c= tensor(4.6181e+08, device='cuda:0')
c= tensor(4.6189e+08, device='cuda:0')
c= tensor(4.6189e+08, device='cuda:0')
c= tensor(4.6190e+08, device='cuda:0')
c= tensor(4.6198e+08, device='cuda:0')
c= tensor(4.6198e+08, device='cuda:0')
c= tensor(4.6199e+08, device='cuda:0')
c= tensor(4.6200e+08, device='cuda:0')
c= tensor(4.6201e+08, device='cuda:0')
c= tensor(4.6863e+08, device='cuda:0')
c= tensor(4.6866e+08, device='cuda:0')
c= tensor(4.6990e+08, device='cuda:0')
c= tensor(4.6991e+08, device='cuda:0')
c= tensor(4.6991e+08, device='cuda:0')
c= tensor(4.7265e+08, device='cuda:0')
c= tensor(4.8683e+08, device='cuda:0')
c= tensor(5.0001e+08, device='cuda:0')
c= tensor(5.0004e+08, device='cuda:0')
c= tensor(5.0028e+08, device='cuda:0')
c= tensor(5.0029e+08, device='cuda:0')
c= tensor(5.0029e+08, device='cuda:0')
c= tensor(5.0114e+08, device='cuda:0')
c= tensor(5.0120e+08, device='cuda:0')
c= tensor(5.0121e+08, device='cuda:0')
c= tensor(5.0191e+08, device='cuda:0')
c= tensor(5.7307e+08, device='cuda:0')
c= tensor(5.7326e+08, device='cuda:0')
c= tensor(5.7331e+08, device='cuda:0')
c= tensor(5.7338e+08, device='cuda:0')
c= tensor(5.7339e+08, device='cuda:0')
c= tensor(5.7339e+08, device='cuda:0')
c= tensor(5.7652e+08, device='cuda:0')
c= tensor(5.7663e+08, device='cuda:0')
c= tensor(5.7663e+08, device='cuda:0')
c= tensor(5.7720e+08, device='cuda:0')
c= tensor(5.7723e+08, device='cuda:0')
c= tensor(5.7723e+08, device='cuda:0')
c= tensor(5.7800e+08, device='cuda:0')
c= tensor(5.7905e+08, device='cuda:0')
c= tensor(5.8844e+08, device='cuda:0')
c= tensor(5.9531e+08, device='cuda:0')
c= tensor(5.9891e+08, device='cuda:0')
c= tensor(5.9909e+08, device='cuda:0')
c= tensor(5.9913e+08, device='cuda:0')
c= tensor(6.0063e+08, device='cuda:0')
c= tensor(6.0164e+08, device='cuda:0')
c= tensor(6.0172e+08, device='cuda:0')
c= tensor(6.0444e+08, device='cuda:0')
c= tensor(6.0640e+08, device='cuda:0')
c= tensor(6.1145e+08, device='cuda:0')
c= tensor(6.1344e+08, device='cuda:0')
c= tensor(6.1484e+08, device='cuda:0')
c= tensor(6.1495e+08, device='cuda:0')
c= tensor(6.1496e+08, device='cuda:0')
c= tensor(6.1497e+08, device='cuda:0')
c= tensor(6.1596e+08, device='cuda:0')
c= tensor(6.1820e+08, device='cuda:0')
c= tensor(6.3713e+08, device='cuda:0')
c= tensor(6.3990e+08, device='cuda:0')
c= tensor(6.4068e+08, device='cuda:0')
c= tensor(6.4081e+08, device='cuda:0')
c= tensor(6.4240e+08, device='cuda:0')
c= tensor(6.4241e+08, device='cuda:0')
c= tensor(6.4243e+08, device='cuda:0')
c= tensor(6.4431e+08, device='cuda:0')
c= tensor(6.4461e+08, device='cuda:0')
c= tensor(6.4461e+08, device='cuda:0')
c= tensor(6.4470e+08, device='cuda:0')
c= tensor(7.2208e+08, device='cuda:0')
c= tensor(7.2216e+08, device='cuda:0')
c= tensor(7.2526e+08, device='cuda:0')
c= tensor(7.2526e+08, device='cuda:0')
c= tensor(7.2527e+08, device='cuda:0')
c= tensor(7.2528e+08, device='cuda:0')
c= tensor(7.2528e+08, device='cuda:0')
c= tensor(7.2535e+08, device='cuda:0')
c= tensor(7.2568e+08, device='cuda:0')
c= tensor(7.2569e+08, device='cuda:0')
c= tensor(7.2779e+08, device='cuda:0')
c= tensor(7.2780e+08, device='cuda:0')
c= tensor(7.2812e+08, device='cuda:0')
c= tensor(7.2816e+08, device='cuda:0')
c= tensor(7.2966e+08, device='cuda:0')
c= tensor(7.2968e+08, device='cuda:0')
c= tensor(7.2981e+08, device='cuda:0')
c= tensor(7.2998e+08, device='cuda:0')
c= tensor(7.3012e+08, device='cuda:0')
c= tensor(7.3085e+08, device='cuda:0')
c= tensor(7.7392e+08, device='cuda:0')
c= tensor(7.7394e+08, device='cuda:0')
c= tensor(7.7395e+08, device='cuda:0')
c= tensor(7.7445e+08, device='cuda:0')
c= tensor(7.7452e+08, device='cuda:0')
c= tensor(7.9729e+08, device='cuda:0')
c= tensor(7.9731e+08, device='cuda:0')
c= tensor(7.9881e+08, device='cuda:0')
c= tensor(8.0475e+08, device='cuda:0')
c= tensor(8.0475e+08, device='cuda:0')
c= tensor(8.1152e+08, device='cuda:0')
c= tensor(8.1173e+08, device='cuda:0')
c= tensor(8.5724e+08, device='cuda:0')
c= tensor(8.5726e+08, device='cuda:0')
c= tensor(8.5730e+08, device='cuda:0')
c= tensor(8.5738e+08, device='cuda:0')
c= tensor(8.5739e+08, device='cuda:0')
c= tensor(8.5741e+08, device='cuda:0')
c= tensor(8.5771e+08, device='cuda:0')
c= tensor(8.5772e+08, device='cuda:0')
c= tensor(8.6011e+08, device='cuda:0')
c= tensor(8.6014e+08, device='cuda:0')
c= tensor(8.6015e+08, device='cuda:0')
c= tensor(8.6018e+08, device='cuda:0')
c= tensor(8.6603e+08, device='cuda:0')
c= tensor(8.6915e+08, device='cuda:0')
c= tensor(8.7878e+08, device='cuda:0')
c= tensor(8.7881e+08, device='cuda:0')
c= tensor(8.7883e+08, device='cuda:0')
c= tensor(8.7883e+08, device='cuda:0')
c= tensor(8.7885e+08, device='cuda:0')
c= tensor(8.8196e+08, device='cuda:0')
c= tensor(8.8197e+08, device='cuda:0')
c= tensor(8.8202e+08, device='cuda:0')
c= tensor(8.8250e+08, device='cuda:0')
c= tensor(8.8300e+08, device='cuda:0')
c= tensor(8.8311e+08, device='cuda:0')
c= tensor(8.8312e+08, device='cuda:0')
c= tensor(9.0070e+08, device='cuda:0')
c= tensor(9.0118e+08, device='cuda:0')
c= tensor(9.0125e+08, device='cuda:0')
c= tensor(9.0127e+08, device='cuda:0')
c= tensor(9.0165e+08, device='cuda:0')
c= tensor(9.0308e+08, device='cuda:0')
c= tensor(9.2926e+08, device='cuda:0')
c= tensor(9.3201e+08, device='cuda:0')
c= tensor(9.3201e+08, device='cuda:0')
c= tensor(9.3217e+08, device='cuda:0')
c= tensor(9.3256e+08, device='cuda:0')
c= tensor(9.3256e+08, device='cuda:0')
c= tensor(9.3261e+08, device='cuda:0')
c= tensor(9.3261e+08, device='cuda:0')
c= tensor(9.3302e+08, device='cuda:0')
c= tensor(9.3343e+08, device='cuda:0')
c= tensor(9.3343e+08, device='cuda:0')
c= tensor(9.3347e+08, device='cuda:0')
c= tensor(9.3350e+08, device='cuda:0')
c= tensor(9.3385e+08, device='cuda:0')
c= tensor(9.3386e+08, device='cuda:0')
c= tensor(9.3394e+08, device='cuda:0')
c= tensor(9.3394e+08, device='cuda:0')
c= tensor(9.3399e+08, device='cuda:0')
c= tensor(9.3399e+08, device='cuda:0')
c= tensor(9.3401e+08, device='cuda:0')
c= tensor(9.3404e+08, device='cuda:0')
c= tensor(9.3779e+08, device='cuda:0')
c= tensor(9.3780e+08, device='cuda:0')
c= tensor(9.3780e+08, device='cuda:0')
c= tensor(9.3781e+08, device='cuda:0')
c= tensor(9.3992e+08, device='cuda:0')
c= tensor(9.4307e+08, device='cuda:0')
c= tensor(9.4443e+08, device='cuda:0')
c= tensor(9.4444e+08, device='cuda:0')
c= tensor(9.5562e+08, device='cuda:0')
c= tensor(9.5709e+08, device='cuda:0')
c= tensor(9.5710e+08, device='cuda:0')
c= tensor(9.5711e+08, device='cuda:0')
c= tensor(9.5719e+08, device='cuda:0')
c= tensor(9.5951e+08, device='cuda:0')
c= tensor(9.5993e+08, device='cuda:0')
c= tensor(9.6346e+08, device='cuda:0')
c= tensor(9.6348e+08, device='cuda:0')
c= tensor(9.6349e+08, device='cuda:0')
c= tensor(9.6349e+08, device='cuda:0')
c= tensor(9.6363e+08, device='cuda:0')
c= tensor(9.6365e+08, device='cuda:0')
c= tensor(9.6373e+08, device='cuda:0')
c= tensor(9.6510e+08, device='cuda:0')
c= tensor(9.7902e+08, device='cuda:0')
c= tensor(9.7903e+08, device='cuda:0')
c= tensor(9.7903e+08, device='cuda:0')
c= tensor(9.7919e+08, device='cuda:0')
c= tensor(9.7932e+08, device='cuda:0')
c= tensor(9.7945e+08, device='cuda:0')
c= tensor(9.7945e+08, device='cuda:0')
c= tensor(9.7946e+08, device='cuda:0')
c= tensor(9.7956e+08, device='cuda:0')
c= tensor(9.7956e+08, device='cuda:0')
c= tensor(9.8125e+08, device='cuda:0')
c= tensor(9.8126e+08, device='cuda:0')
c= tensor(9.8128e+08, device='cuda:0')
c= tensor(9.8131e+08, device='cuda:0')
c= tensor(9.8133e+08, device='cuda:0')
c= tensor(9.8134e+08, device='cuda:0')
c= tensor(9.8417e+08, device='cuda:0')
c= tensor(9.9736e+08, device='cuda:0')
c= tensor(9.9980e+08, device='cuda:0')
c= tensor(1.0017e+09, device='cuda:0')
c= tensor(1.0018e+09, device='cuda:0')
c= tensor(1.0018e+09, device='cuda:0')
c= tensor(1.0019e+09, device='cuda:0')
c= tensor(1.0029e+09, device='cuda:0')
c= tensor(1.0033e+09, device='cuda:0')
c= tensor(1.0034e+09, device='cuda:0')
c= tensor(1.0034e+09, device='cuda:0')
c= tensor(1.1212e+09, device='cuda:0')
c= tensor(1.1212e+09, device='cuda:0')
c= tensor(1.1216e+09, device='cuda:0')
c= tensor(1.1254e+09, device='cuda:0')
c= tensor(1.1255e+09, device='cuda:0')
c= tensor(1.1257e+09, device='cuda:0')
c= tensor(1.1750e+09, device='cuda:0')
c= tensor(1.1785e+09, device='cuda:0')
c= tensor(1.1794e+09, device='cuda:0')
c= tensor(1.1794e+09, device='cuda:0')
c= tensor(1.1796e+09, device='cuda:0')
c= tensor(1.1796e+09, device='cuda:0')
c= tensor(1.1820e+09, device='cuda:0')
c= tensor(1.3180e+09, device='cuda:0')
c= tensor(1.3221e+09, device='cuda:0')
c= tensor(1.3254e+09, device='cuda:0')
c= tensor(1.3258e+09, device='cuda:0')
c= tensor(1.3265e+09, device='cuda:0')
c= tensor(1.3269e+09, device='cuda:0')
c= tensor(1.3270e+09, device='cuda:0')
c= tensor(1.3270e+09, device='cuda:0')
c= tensor(1.3387e+09, device='cuda:0')
c= tensor(1.3397e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3620e+09, device='cuda:0')
c= tensor(1.3625e+09, device='cuda:0')
c= tensor(1.3626e+09, device='cuda:0')
c= tensor(1.3636e+09, device='cuda:0')
c= tensor(1.3647e+09, device='cuda:0')
c= tensor(1.3647e+09, device='cuda:0')
c= tensor(1.3787e+09, device='cuda:0')
c= tensor(1.3790e+09, device='cuda:0')
c= tensor(1.3794e+09, device='cuda:0')
c= tensor(1.3794e+09, device='cuda:0')
c= tensor(1.3794e+09, device='cuda:0')
c= tensor(1.3794e+09, device='cuda:0')
c= tensor(1.3794e+09, device='cuda:0')
c= tensor(1.3797e+09, device='cuda:0')
c= tensor(1.3798e+09, device='cuda:0')
c= tensor(2.8750e+09, device='cuda:0')
c= tensor(2.8752e+09, device='cuda:0')
c= tensor(2.8787e+09, device='cuda:0')
c= tensor(2.8787e+09, device='cuda:0')
c= tensor(2.8788e+09, device='cuda:0')
c= tensor(2.8788e+09, device='cuda:0')
c= tensor(2.8912e+09, device='cuda:0')
c= tensor(2.8912e+09, device='cuda:0')
c= tensor(2.9561e+09, device='cuda:0')
c= tensor(2.9561e+09, device='cuda:0')
c= tensor(2.9596e+09, device='cuda:0')
c= tensor(2.9599e+09, device='cuda:0')
c= tensor(2.9621e+09, device='cuda:0')
c= tensor(2.9680e+09, device='cuda:0')
c= tensor(2.9681e+09, device='cuda:0')
c= tensor(2.9681e+09, device='cuda:0')
c= tensor(2.9687e+09, device='cuda:0')
c= tensor(2.9687e+09, device='cuda:0')
c= tensor(2.9687e+09, device='cuda:0')
c= tensor(2.9699e+09, device='cuda:0')
c= tensor(2.9700e+09, device='cuda:0')
c= tensor(2.9709e+09, device='cuda:0')
c= tensor(2.9710e+09, device='cuda:0')
c= tensor(2.9742e+09, device='cuda:0')
c= tensor(2.9912e+09, device='cuda:0')
c= tensor(2.9916e+09, device='cuda:0')
c= tensor(2.9917e+09, device='cuda:0')
c= tensor(2.9929e+09, device='cuda:0')
c= tensor(2.9935e+09, device='cuda:0')
c= tensor(2.9936e+09, device='cuda:0')
c= tensor(2.9938e+09, device='cuda:0')
c= tensor(2.9939e+09, device='cuda:0')
c= tensor(2.9946e+09, device='cuda:0')
c= tensor(2.9986e+09, device='cuda:0')
c= tensor(3.0000e+09, device='cuda:0')
c= tensor(3.0000e+09, device='cuda:0')
c= tensor(3.0000e+09, device='cuda:0')
c= tensor(3.0001e+09, device='cuda:0')
c= tensor(3.0015e+09, device='cuda:0')
c= tensor(3.0020e+09, device='cuda:0')
c= tensor(3.0024e+09, device='cuda:0')
c= tensor(3.0024e+09, device='cuda:0')
c= tensor(3.0025e+09, device='cuda:0')
c= tensor(3.0027e+09, device='cuda:0')
c= tensor(3.0030e+09, device='cuda:0')
c= tensor(3.0031e+09, device='cuda:0')
c= tensor(3.0034e+09, device='cuda:0')
c= tensor(3.0037e+09, device='cuda:0')
c= tensor(3.0039e+09, device='cuda:0')
c= tensor(3.0039e+09, device='cuda:0')
c= tensor(3.0039e+09, device='cuda:0')
c= tensor(3.0055e+09, device='cuda:0')
c= tensor(3.0057e+09, device='cuda:0')
c= tensor(3.0061e+09, device='cuda:0')
c= tensor(3.0061e+09, device='cuda:0')
c= tensor(3.0061e+09, device='cuda:0')
c= tensor(3.0062e+09, device='cuda:0')
c= tensor(3.0068e+09, device='cuda:0')
c= tensor(3.0070e+09, device='cuda:0')
c= tensor(3.0070e+09, device='cuda:0')
c= tensor(3.0070e+09, device='cuda:0')
c= tensor(3.0075e+09, device='cuda:0')
c= tensor(3.0075e+09, device='cuda:0')
c= tensor(3.0083e+09, device='cuda:0')
c= tensor(3.0083e+09, device='cuda:0')
c= tensor(3.0083e+09, device='cuda:0')
c= tensor(3.0088e+09, device='cuda:0')
c= tensor(3.0088e+09, device='cuda:0')
c= tensor(3.0156e+09, device='cuda:0')
c= tensor(3.0156e+09, device='cuda:0')
c= tensor(3.0166e+09, device='cuda:0')
c= tensor(3.0177e+09, device='cuda:0')
c= tensor(3.0178e+09, device='cuda:0')
c= tensor(3.0200e+09, device='cuda:0')
c= tensor(3.0210e+09, device='cuda:0')
c= tensor(3.0210e+09, device='cuda:0')
c= tensor(3.0212e+09, device='cuda:0')
c= tensor(3.0213e+09, device='cuda:0')
c= tensor(3.0215e+09, device='cuda:0')
c= tensor(3.0231e+09, device='cuda:0')
c= tensor(3.0233e+09, device='cuda:0')
c= tensor(3.0236e+09, device='cuda:0')
c= tensor(3.0237e+09, device='cuda:0')
c= tensor(3.0258e+09, device='cuda:0')
c= tensor(3.0258e+09, device='cuda:0')
c= tensor(3.0258e+09, device='cuda:0')
c= tensor(3.0275e+09, device='cuda:0')
c= tensor(3.0373e+09, device='cuda:0')
c= tensor(3.0374e+09, device='cuda:0')
c= tensor(3.0374e+09, device='cuda:0')
c= tensor(3.0375e+09, device='cuda:0')
c= tensor(3.0383e+09, device='cuda:0')
c= tensor(3.0385e+09, device='cuda:0')
c= tensor(3.0386e+09, device='cuda:0')
c= tensor(3.0388e+09, device='cuda:0')
c= tensor(3.0404e+09, device='cuda:0')
c= tensor(3.0404e+09, device='cuda:0')
c= tensor(3.0411e+09, device='cuda:0')
c= tensor(3.0411e+09, device='cuda:0')
memory (bytes)
3881144320
time for making loss 2 is 15.476810932159424
p0 True
it  0 : 991520768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 28% |
shape of L is 
torch.Size([])
memory (bytes)
3881279488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3882254336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  29041700000.0
relative error loss 9.549609
shape of L is 
torch.Size([])
memory (bytes)
4091342848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  7% |
memory (bytes)
4091482112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  29041435000.0
relative error loss 9.549521
shape of L is 
torch.Size([])
memory (bytes)
4093292544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4093292544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 77% |  7% |
error is  29040220000.0
relative error loss 9.549123
shape of L is 
torch.Size([])
memory (bytes)
4094357504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4094357504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  29032489000.0
relative error loss 9.54658
shape of L is 
torch.Size([])
memory (bytes)
4096446464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4096446464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 79% |  7% |
error is  28949733000.0
relative error loss 9.519368
shape of L is 
torch.Size([])
memory (bytes)
4098560000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4098560000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  28133417000.0
relative error loss 9.250944
shape of L is 
torch.Size([])
memory (bytes)
4100681728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4100681728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  24343357000.0
relative error loss 8.004681
shape of L is 
torch.Size([])
memory (bytes)
4102791168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4102791168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  7579388400.0
relative error loss 2.492285
shape of L is 
torch.Size([])
memory (bytes)
4104908800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4104941568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 81% |  7% |
error is  4765820400.0
relative error loss 1.5671163
shape of L is 
torch.Size([])
memory (bytes)
4106948608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4106948608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  3585897500.0
relative error loss 1.1791292
time to take a step is 360.6362588405609
it  1 : 1234923520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4109176832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4109176832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 74% |  7% |
error is  3585897500.0
relative error loss 1.1791292
shape of L is 
torch.Size([])
memory (bytes)
4111298560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4111298560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  3238297900.0
relative error loss 1.0648302
shape of L is 
torch.Size([])
memory (bytes)
4113387520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4113387520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  3100219600.0
relative error loss 1.0194267
shape of L is 
torch.Size([])
memory (bytes)
4115484672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4115484672
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 80% |  7% |
error is  2922465000.0
relative error loss 0.9609767
shape of L is 
torch.Size([])
memory (bytes)
4117569536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4117569536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  9443860000.0
relative error loss 3.1053684
shape of L is 
torch.Size([])
memory (bytes)
4119699456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4119699456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  2755115500.0
relative error loss 0.9059482
shape of L is 
torch.Size([])
memory (bytes)
4121817088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4121817088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  2729556500.0
relative error loss 0.8975438
shape of L is 
torch.Size([])
memory (bytes)
4123983872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4124004352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  2988634400.0
relative error loss 0.9827348
shape of L is 
torch.Size([])
memory (bytes)
4126011392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4126011392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  2618173000.0
relative error loss 0.86091816
shape of L is 
torch.Size([])
memory (bytes)
4128284672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4128284672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  7% |
error is  6126338000.0
relative error loss 2.0144873
shape of L is 
torch.Size([])
memory (bytes)
4130254848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4130254848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 78% |  7% |
error is  2384456700.0
relative error loss 0.7840667
time to take a step is 354.67935037612915
it  2 : 1234924032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4132360192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4132360192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 78% |  7% |
error is  2384456700.0
relative error loss 0.7840667
shape of L is 
torch.Size([])
memory (bytes)
4134637568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4134637568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 80% |  7% |
error is  2047364100.0
relative error loss 0.67322254
shape of L is 
torch.Size([])
memory (bytes)
4136497152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4136755200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 77% |  7% |
error is  1930483600.0
relative error loss 0.6347894
shape of L is 
torch.Size([])
memory (bytes)
4138876928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4138876928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  1727180000.0
relative error loss 0.5679383
shape of L is 
torch.Size([])
memory (bytes)
4140994560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4140994560
| ID | GPU | MEM |
------------------
|  0 |  3% |  0% |
|  1 | 93% |  7% |
error is  1604243700.0
relative error loss 0.52751386
shape of L is 
torch.Size([])
memory (bytes)
4143042560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4143042560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  1510930400.0
relative error loss 0.49683025
shape of L is 
torch.Size([])
memory (bytes)
4145238016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4145238016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 81% |  7% |
error is  1429455900.0
relative error loss 0.47003946
shape of L is 
torch.Size([])
memory (bytes)
4147179520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4147179520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  1359182000.0
relative error loss 0.4469317
shape of L is 
torch.Size([])
memory (bytes)
4149514240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4149514240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 78% |  7% |
error is  1257511300.0
relative error loss 0.41349995
shape of L is 
torch.Size([])
memory (bytes)
4151484416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4151484416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  1145956900.0
relative error loss 0.37681815
time to take a step is 322.6245267391205
it  3 : 1234923520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4153729024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4153729024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  1145956900.0
relative error loss 0.37681815
shape of L is 
torch.Size([])
memory (bytes)
4155920384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4155920384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 73% |  7% |
error is  1036369400.0
relative error loss 0.34078318
shape of L is 
torch.Size([])
memory (bytes)
4157825024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4158066688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  975934200.0
relative error loss 0.32091063
shape of L is 
torch.Size([])
memory (bytes)
4160090112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4160090112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 81% |  7% |
error is  878965760.0
relative error loss 0.28902507
shape of L is 
torch.Size([])
memory (bytes)
4162355200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4162355200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  788159200.0
relative error loss 0.2591657
shape of L is 
torch.Size([])
memory (bytes)
4164337664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4164337664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  740696600.0
relative error loss 0.24355884
shape of L is 
torch.Size([])
memory (bytes)
4166660096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4166660096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 74% |  7% |
error is  674023940.0
relative error loss 0.22163527
shape of L is 
torch.Size([])
memory (bytes)
4168798208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4168798208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  613422340.0
relative error loss 0.201708
shape of L is 
torch.Size([])
memory (bytes)
4170817536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4170817536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  561988600.0
relative error loss 0.18479536
shape of L is 
torch.Size([])
memory (bytes)
4173094912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4173094912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 82% |  7% |
error is  498122500.0
relative error loss 0.16379465
time to take a step is 321.0836601257324
c= tensor(1530.1538, device='cuda:0')
c= tensor(77121.5312, device='cuda:0')
c= tensor(80017.5547, device='cuda:0')
c= tensor(153384.0781, device='cuda:0')
c= tensor(178951.9062, device='cuda:0')
c= tensor(523188.1562, device='cuda:0')
c= tensor(895564., device='cuda:0')
c= tensor(1076414., device='cuda:0')
c= tensor(1122592.3750, device='cuda:0')
c= tensor(1453876.2500, device='cuda:0')
c= tensor(1470151., device='cuda:0')
c= tensor(2798312., device='cuda:0')
c= tensor(2808487.2500, device='cuda:0')
c= tensor(26804028., device='cuda:0')
c= tensor(26881100., device='cuda:0')
c= tensor(27257470., device='cuda:0')
c= tensor(28032448., device='cuda:0')
c= tensor(28224020., device='cuda:0')
c= tensor(30576476., device='cuda:0')
c= tensor(33109908., device='cuda:0')
c= tensor(34138128., device='cuda:0')
c= tensor(37371892., device='cuda:0')
c= tensor(37388680., device='cuda:0')
c= tensor(38306972., device='cuda:0')
c= tensor(38311936., device='cuda:0')
c= tensor(38659580., device='cuda:0')
c= tensor(39093800., device='cuda:0')
c= tensor(39099348., device='cuda:0')
c= tensor(39313784., device='cuda:0')
c= tensor(1.3081e+08, device='cuda:0')
c= tensor(1.3083e+08, device='cuda:0')
c= tensor(2.0718e+08, device='cuda:0')
c= tensor(2.0727e+08, device='cuda:0')
c= tensor(2.0729e+08, device='cuda:0')
c= tensor(2.0739e+08, device='cuda:0')
c= tensor(2.1086e+08, device='cuda:0')
c= tensor(2.1187e+08, device='cuda:0')
c= tensor(2.1187e+08, device='cuda:0')
c= tensor(2.1188e+08, device='cuda:0')
c= tensor(2.1188e+08, device='cuda:0')
c= tensor(2.1188e+08, device='cuda:0')
c= tensor(2.1189e+08, device='cuda:0')
c= tensor(2.1189e+08, device='cuda:0')
c= tensor(2.1190e+08, device='cuda:0')
c= tensor(2.1190e+08, device='cuda:0')
c= tensor(2.1190e+08, device='cuda:0')
c= tensor(2.1190e+08, device='cuda:0')
c= tensor(2.1191e+08, device='cuda:0')
c= tensor(2.1191e+08, device='cuda:0')
c= tensor(2.1196e+08, device='cuda:0')
c= tensor(2.1203e+08, device='cuda:0')
c= tensor(2.1203e+08, device='cuda:0')
c= tensor(2.1206e+08, device='cuda:0')
c= tensor(2.1206e+08, device='cuda:0')
c= tensor(2.1206e+08, device='cuda:0')
c= tensor(2.1209e+08, device='cuda:0')
c= tensor(2.1209e+08, device='cuda:0')
c= tensor(2.1209e+08, device='cuda:0')
c= tensor(2.1209e+08, device='cuda:0')
c= tensor(2.1210e+08, device='cuda:0')
c= tensor(2.1210e+08, device='cuda:0')
c= tensor(2.1210e+08, device='cuda:0')
c= tensor(2.1211e+08, device='cuda:0')
c= tensor(2.1213e+08, device='cuda:0')
c= tensor(2.1213e+08, device='cuda:0')
c= tensor(2.1214e+08, device='cuda:0')
c= tensor(2.1214e+08, device='cuda:0')
c= tensor(2.1215e+08, device='cuda:0')
c= tensor(2.1216e+08, device='cuda:0')
c= tensor(2.1216e+08, device='cuda:0')
c= tensor(2.1217e+08, device='cuda:0')
c= tensor(2.1217e+08, device='cuda:0')
c= tensor(2.1217e+08, device='cuda:0')
c= tensor(2.1218e+08, device='cuda:0')
c= tensor(2.1218e+08, device='cuda:0')
c= tensor(2.1219e+08, device='cuda:0')
c= tensor(2.1219e+08, device='cuda:0')
c= tensor(2.1219e+08, device='cuda:0')
c= tensor(2.1219e+08, device='cuda:0')
c= tensor(2.1222e+08, device='cuda:0')
c= tensor(2.1222e+08, device='cuda:0')
c= tensor(2.1222e+08, device='cuda:0')
c= tensor(2.1223e+08, device='cuda:0')
c= tensor(2.1223e+08, device='cuda:0')
c= tensor(2.1223e+08, device='cuda:0')
c= tensor(2.1223e+08, device='cuda:0')
c= tensor(2.1224e+08, device='cuda:0')
c= tensor(2.1224e+08, device='cuda:0')
c= tensor(2.1224e+08, device='cuda:0')
c= tensor(2.1224e+08, device='cuda:0')
c= tensor(2.1225e+08, device='cuda:0')
c= tensor(2.1225e+08, device='cuda:0')
c= tensor(2.1225e+08, device='cuda:0')
c= tensor(2.1225e+08, device='cuda:0')
c= tensor(2.1225e+08, device='cuda:0')
c= tensor(2.1226e+08, device='cuda:0')
c= tensor(2.1227e+08, device='cuda:0')
c= tensor(2.1227e+08, device='cuda:0')
c= tensor(2.1228e+08, device='cuda:0')
c= tensor(2.1228e+08, device='cuda:0')
c= tensor(2.1230e+08, device='cuda:0')
c= tensor(2.1230e+08, device='cuda:0')
c= tensor(2.1230e+08, device='cuda:0')
c= tensor(2.1231e+08, device='cuda:0')
c= tensor(2.1231e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1232e+08, device='cuda:0')
c= tensor(2.1233e+08, device='cuda:0')
c= tensor(2.1233e+08, device='cuda:0')
c= tensor(2.1233e+08, device='cuda:0')
c= tensor(2.1234e+08, device='cuda:0')
c= tensor(2.1234e+08, device='cuda:0')
c= tensor(2.1234e+08, device='cuda:0')
c= tensor(2.1236e+08, device='cuda:0')
c= tensor(2.1236e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1238e+08, device='cuda:0')
c= tensor(2.1239e+08, device='cuda:0')
c= tensor(2.1240e+08, device='cuda:0')
c= tensor(2.1240e+08, device='cuda:0')
c= tensor(2.1242e+08, device='cuda:0')
c= tensor(2.1242e+08, device='cuda:0')
c= tensor(2.1242e+08, device='cuda:0')
c= tensor(2.1242e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1243e+08, device='cuda:0')
c= tensor(2.1244e+08, device='cuda:0')
c= tensor(2.1244e+08, device='cuda:0')
c= tensor(2.1246e+08, device='cuda:0')
c= tensor(2.1247e+08, device='cuda:0')
c= tensor(2.1247e+08, device='cuda:0')
c= tensor(2.1247e+08, device='cuda:0')
c= tensor(2.1247e+08, device='cuda:0')
c= tensor(2.1248e+08, device='cuda:0')
c= tensor(2.1248e+08, device='cuda:0')
c= tensor(2.1248e+08, device='cuda:0')
c= tensor(2.1248e+08, device='cuda:0')
c= tensor(2.1249e+08, device='cuda:0')
c= tensor(2.1249e+08, device='cuda:0')
c= tensor(2.1251e+08, device='cuda:0')
c= tensor(2.1251e+08, device='cuda:0')
c= tensor(2.1257e+08, device='cuda:0')
c= tensor(2.1258e+08, device='cuda:0')
c= tensor(2.1258e+08, device='cuda:0')
c= tensor(2.1258e+08, device='cuda:0')
c= tensor(2.1258e+08, device='cuda:0')
c= tensor(2.1260e+08, device='cuda:0')
c= tensor(2.1260e+08, device='cuda:0')
c= tensor(2.1261e+08, device='cuda:0')
c= tensor(2.1261e+08, device='cuda:0')
c= tensor(2.1262e+08, device='cuda:0')
c= tensor(2.1262e+08, device='cuda:0')
c= tensor(2.1262e+08, device='cuda:0')
c= tensor(2.1263e+08, device='cuda:0')
c= tensor(2.1263e+08, device='cuda:0')
c= tensor(2.1263e+08, device='cuda:0')
c= tensor(2.1263e+08, device='cuda:0')
c= tensor(2.1263e+08, device='cuda:0')
c= tensor(2.1264e+08, device='cuda:0')
c= tensor(2.1264e+08, device='cuda:0')
c= tensor(2.1265e+08, device='cuda:0')
c= tensor(2.1265e+08, device='cuda:0')
c= tensor(2.1266e+08, device='cuda:0')
c= tensor(2.1267e+08, device='cuda:0')
c= tensor(2.1267e+08, device='cuda:0')
c= tensor(2.1268e+08, device='cuda:0')
c= tensor(2.1268e+08, device='cuda:0')
c= tensor(2.1269e+08, device='cuda:0')
c= tensor(2.1269e+08, device='cuda:0')
c= tensor(2.1270e+08, device='cuda:0')
c= tensor(2.1270e+08, device='cuda:0')
c= tensor(2.1270e+08, device='cuda:0')
c= tensor(2.1270e+08, device='cuda:0')
c= tensor(2.1271e+08, device='cuda:0')
c= tensor(2.1272e+08, device='cuda:0')
c= tensor(2.1272e+08, device='cuda:0')
c= tensor(2.1272e+08, device='cuda:0')
c= tensor(2.1272e+08, device='cuda:0')
c= tensor(2.1273e+08, device='cuda:0')
c= tensor(2.1274e+08, device='cuda:0')
c= tensor(2.1275e+08, device='cuda:0')
c= tensor(2.1275e+08, device='cuda:0')
c= tensor(2.1276e+08, device='cuda:0')
c= tensor(2.1276e+08, device='cuda:0')
c= tensor(2.1276e+08, device='cuda:0')
c= tensor(2.1276e+08, device='cuda:0')
c= tensor(2.1277e+08, device='cuda:0')
c= tensor(2.1278e+08, device='cuda:0')
c= tensor(2.1278e+08, device='cuda:0')
c= tensor(2.1279e+08, device='cuda:0')
c= tensor(2.1279e+08, device='cuda:0')
c= tensor(2.1279e+08, device='cuda:0')
c= tensor(2.1280e+08, device='cuda:0')
c= tensor(2.1280e+08, device='cuda:0')
c= tensor(2.1280e+08, device='cuda:0')
c= tensor(2.1281e+08, device='cuda:0')
c= tensor(2.1282e+08, device='cuda:0')
c= tensor(2.1284e+08, device='cuda:0')
c= tensor(2.1284e+08, device='cuda:0')
c= tensor(2.1284e+08, device='cuda:0')
c= tensor(2.1285e+08, device='cuda:0')
c= tensor(2.1285e+08, device='cuda:0')
c= tensor(2.1285e+08, device='cuda:0')
c= tensor(2.1285e+08, device='cuda:0')
c= tensor(2.1286e+08, device='cuda:0')
c= tensor(2.1286e+08, device='cuda:0')
c= tensor(2.1286e+08, device='cuda:0')
c= tensor(2.1287e+08, device='cuda:0')
c= tensor(2.1287e+08, device='cuda:0')
c= tensor(2.1288e+08, device='cuda:0')
c= tensor(2.1288e+08, device='cuda:0')
c= tensor(2.1288e+08, device='cuda:0')
c= tensor(2.1288e+08, device='cuda:0')
c= tensor(2.1289e+08, device='cuda:0')
c= tensor(2.1289e+08, device='cuda:0')
c= tensor(2.1289e+08, device='cuda:0')
c= tensor(2.1289e+08, device='cuda:0')
c= tensor(2.1290e+08, device='cuda:0')
c= tensor(2.1290e+08, device='cuda:0')
c= tensor(2.1291e+08, device='cuda:0')
c= tensor(2.1291e+08, device='cuda:0')
c= tensor(2.1291e+08, device='cuda:0')
c= tensor(2.1294e+08, device='cuda:0')
c= tensor(2.1295e+08, device='cuda:0')
c= tensor(2.1295e+08, device='cuda:0')
c= tensor(2.1310e+08, device='cuda:0')
c= tensor(2.1478e+08, device='cuda:0')
c= tensor(2.1479e+08, device='cuda:0')
c= tensor(2.1480e+08, device='cuda:0')
c= tensor(2.1480e+08, device='cuda:0')
c= tensor(2.1480e+08, device='cuda:0')
c= tensor(2.1482e+08, device='cuda:0')
c= tensor(2.3189e+08, device='cuda:0')
c= tensor(2.3190e+08, device='cuda:0')
c= tensor(2.3819e+08, device='cuda:0')
c= tensor(2.3826e+08, device='cuda:0')
c= tensor(2.3844e+08, device='cuda:0')
c= tensor(2.3878e+08, device='cuda:0')
c= tensor(2.3878e+08, device='cuda:0')
c= tensor(2.3879e+08, device='cuda:0')
c= tensor(2.4173e+08, device='cuda:0')
c= tensor(2.6353e+08, device='cuda:0')
c= tensor(2.6354e+08, device='cuda:0')
c= tensor(2.6368e+08, device='cuda:0')
c= tensor(2.6373e+08, device='cuda:0')
c= tensor(2.6382e+08, device='cuda:0')
c= tensor(2.6741e+08, device='cuda:0')
c= tensor(2.6778e+08, device='cuda:0')
c= tensor(2.6790e+08, device='cuda:0')
c= tensor(2.6800e+08, device='cuda:0')
c= tensor(2.6801e+08, device='cuda:0')
c= tensor(2.7718e+08, device='cuda:0')
c= tensor(2.7719e+08, device='cuda:0')
c= tensor(2.7720e+08, device='cuda:0')
c= tensor(2.7732e+08, device='cuda:0')
c= tensor(2.7842e+08, device='cuda:0')
c= tensor(2.8602e+08, device='cuda:0')
c= tensor(2.8670e+08, device='cuda:0')
c= tensor(2.8671e+08, device='cuda:0')
c= tensor(2.8701e+08, device='cuda:0')
c= tensor(2.8701e+08, device='cuda:0')
c= tensor(2.8745e+08, device='cuda:0')
c= tensor(2.8775e+08, device='cuda:0')
c= tensor(2.8776e+08, device='cuda:0')
c= tensor(2.8831e+08, device='cuda:0')
c= tensor(2.8831e+08, device='cuda:0')
c= tensor(2.8833e+08, device='cuda:0')
c= tensor(2.9068e+08, device='cuda:0')
c= tensor(2.9106e+08, device='cuda:0')
c= tensor(2.9340e+08, device='cuda:0')
c= tensor(2.9341e+08, device='cuda:0')
c= tensor(3.0406e+08, device='cuda:0')
c= tensor(3.0409e+08, device='cuda:0')
c= tensor(3.0426e+08, device='cuda:0')
c= tensor(3.0506e+08, device='cuda:0')
c= tensor(3.0506e+08, device='cuda:0')
c= tensor(3.0642e+08, device='cuda:0')
c= tensor(3.1345e+08, device='cuda:0')
c= tensor(3.3653e+08, device='cuda:0')
c= tensor(3.3721e+08, device='cuda:0')
c= tensor(3.3738e+08, device='cuda:0')
c= tensor(3.3746e+08, device='cuda:0')
c= tensor(3.3746e+08, device='cuda:0')
c= tensor(3.3935e+08, device='cuda:0')
c= tensor(3.3937e+08, device='cuda:0')
c= tensor(3.3959e+08, device='cuda:0')
c= tensor(3.4126e+08, device='cuda:0')
c= tensor(3.4131e+08, device='cuda:0')
c= tensor(3.4137e+08, device='cuda:0')
c= tensor(3.4137e+08, device='cuda:0')
c= tensor(3.4601e+08, device='cuda:0')
c= tensor(3.4606e+08, device='cuda:0')
c= tensor(3.4651e+08, device='cuda:0')
c= tensor(3.4652e+08, device='cuda:0')
c= tensor(3.4773e+08, device='cuda:0')
c= tensor(3.4781e+08, device='cuda:0')
c= tensor(3.5244e+08, device='cuda:0')
c= tensor(3.5249e+08, device='cuda:0')
c= tensor(3.5422e+08, device='cuda:0')
c= tensor(3.5424e+08, device='cuda:0')
c= tensor(3.6021e+08, device='cuda:0')
c= tensor(3.6059e+08, device='cuda:0')
c= tensor(3.6063e+08, device='cuda:0')
c= tensor(3.6258e+08, device='cuda:0')
c= tensor(3.6428e+08, device='cuda:0')
c= tensor(3.6428e+08, device='cuda:0')
c= tensor(3.7010e+08, device='cuda:0')
c= tensor(3.7380e+08, device='cuda:0')
c= tensor(3.8543e+08, device='cuda:0')
c= tensor(3.8555e+08, device='cuda:0')
c= tensor(3.8555e+08, device='cuda:0')
c= tensor(3.8556e+08, device='cuda:0')
c= tensor(3.8563e+08, device='cuda:0')
c= tensor(3.8571e+08, device='cuda:0')
c= tensor(3.8598e+08, device='cuda:0')
c= tensor(3.8598e+08, device='cuda:0')
c= tensor(3.8738e+08, device='cuda:0')
c= tensor(3.8916e+08, device='cuda:0')
c= tensor(3.8917e+08, device='cuda:0')
c= tensor(3.8918e+08, device='cuda:0')
c= tensor(3.8952e+08, device='cuda:0')
c= tensor(3.8954e+08, device='cuda:0')
c= tensor(3.8955e+08, device='cuda:0')
c= tensor(3.8957e+08, device='cuda:0')
c= tensor(3.8957e+08, device='cuda:0')
c= tensor(3.8962e+08, device='cuda:0')
c= tensor(3.8986e+08, device='cuda:0')
c= tensor(3.8996e+08, device='cuda:0')
c= tensor(3.9045e+08, device='cuda:0')
c= tensor(3.9046e+08, device='cuda:0')
c= tensor(4.4177e+08, device='cuda:0')
c= tensor(4.4179e+08, device='cuda:0')
c= tensor(4.4461e+08, device='cuda:0')
c= tensor(4.4461e+08, device='cuda:0')
c= tensor(4.4462e+08, device='cuda:0')
c= tensor(4.4462e+08, device='cuda:0')
c= tensor(4.4464e+08, device='cuda:0')
c= tensor(4.4465e+08, device='cuda:0')
c= tensor(4.4877e+08, device='cuda:0')
c= tensor(4.4877e+08, device='cuda:0')
c= tensor(4.4877e+08, device='cuda:0')
c= tensor(4.5315e+08, device='cuda:0')
c= tensor(4.5365e+08, device='cuda:0')
c= tensor(4.5399e+08, device='cuda:0')
c= tensor(4.5541e+08, device='cuda:0')
c= tensor(4.6180e+08, device='cuda:0')
c= tensor(4.6181e+08, device='cuda:0')
c= tensor(4.6181e+08, device='cuda:0')
c= tensor(4.6189e+08, device='cuda:0')
c= tensor(4.6189e+08, device='cuda:0')
c= tensor(4.6190e+08, device='cuda:0')
c= tensor(4.6198e+08, device='cuda:0')
c= tensor(4.6198e+08, device='cuda:0')
c= tensor(4.6199e+08, device='cuda:0')
c= tensor(4.6200e+08, device='cuda:0')
c= tensor(4.6201e+08, device='cuda:0')
c= tensor(4.6863e+08, device='cuda:0')
c= tensor(4.6866e+08, device='cuda:0')
c= tensor(4.6990e+08, device='cuda:0')
c= tensor(4.6991e+08, device='cuda:0')
c= tensor(4.6991e+08, device='cuda:0')
c= tensor(4.7265e+08, device='cuda:0')
c= tensor(4.8683e+08, device='cuda:0')
c= tensor(5.0001e+08, device='cuda:0')
c= tensor(5.0004e+08, device='cuda:0')
c= tensor(5.0028e+08, device='cuda:0')
c= tensor(5.0029e+08, device='cuda:0')
c= tensor(5.0029e+08, device='cuda:0')
c= tensor(5.0114e+08, device='cuda:0')
c= tensor(5.0120e+08, device='cuda:0')
c= tensor(5.0121e+08, device='cuda:0')
c= tensor(5.0191e+08, device='cuda:0')
c= tensor(5.7307e+08, device='cuda:0')
c= tensor(5.7326e+08, device='cuda:0')
c= tensor(5.7331e+08, device='cuda:0')
c= tensor(5.7338e+08, device='cuda:0')
c= tensor(5.7339e+08, device='cuda:0')
c= tensor(5.7339e+08, device='cuda:0')
c= tensor(5.7652e+08, device='cuda:0')
c= tensor(5.7663e+08, device='cuda:0')
c= tensor(5.7663e+08, device='cuda:0')
c= tensor(5.7720e+08, device='cuda:0')
c= tensor(5.7723e+08, device='cuda:0')
c= tensor(5.7723e+08, device='cuda:0')
c= tensor(5.7800e+08, device='cuda:0')
c= tensor(5.7905e+08, device='cuda:0')
c= tensor(5.8844e+08, device='cuda:0')
c= tensor(5.9531e+08, device='cuda:0')
c= tensor(5.9891e+08, device='cuda:0')
c= tensor(5.9909e+08, device='cuda:0')
c= tensor(5.9913e+08, device='cuda:0')
c= tensor(6.0063e+08, device='cuda:0')
c= tensor(6.0164e+08, device='cuda:0')
c= tensor(6.0172e+08, device='cuda:0')
c= tensor(6.0444e+08, device='cuda:0')
c= tensor(6.0640e+08, device='cuda:0')
c= tensor(6.1145e+08, device='cuda:0')
c= tensor(6.1344e+08, device='cuda:0')
c= tensor(6.1484e+08, device='cuda:0')
c= tensor(6.1495e+08, device='cuda:0')
c= tensor(6.1496e+08, device='cuda:0')
c= tensor(6.1497e+08, device='cuda:0')
c= tensor(6.1596e+08, device='cuda:0')
c= tensor(6.1820e+08, device='cuda:0')
c= tensor(6.3713e+08, device='cuda:0')
c= tensor(6.3990e+08, device='cuda:0')
c= tensor(6.4068e+08, device='cuda:0')
c= tensor(6.4081e+08, device='cuda:0')
c= tensor(6.4240e+08, device='cuda:0')
c= tensor(6.4241e+08, device='cuda:0')
c= tensor(6.4243e+08, device='cuda:0')
c= tensor(6.4431e+08, device='cuda:0')
c= tensor(6.4461e+08, device='cuda:0')
c= tensor(6.4461e+08, device='cuda:0')
c= tensor(6.4470e+08, device='cuda:0')
c= tensor(7.2208e+08, device='cuda:0')
c= tensor(7.2216e+08, device='cuda:0')
c= tensor(7.2526e+08, device='cuda:0')
c= tensor(7.2526e+08, device='cuda:0')
c= tensor(7.2527e+08, device='cuda:0')
c= tensor(7.2528e+08, device='cuda:0')
c= tensor(7.2528e+08, device='cuda:0')
c= tensor(7.2535e+08, device='cuda:0')
c= tensor(7.2568e+08, device='cuda:0')
c= tensor(7.2569e+08, device='cuda:0')
c= tensor(7.2779e+08, device='cuda:0')
c= tensor(7.2780e+08, device='cuda:0')
c= tensor(7.2812e+08, device='cuda:0')
c= tensor(7.2816e+08, device='cuda:0')
c= tensor(7.2966e+08, device='cuda:0')
c= tensor(7.2968e+08, device='cuda:0')
c= tensor(7.2981e+08, device='cuda:0')
c= tensor(7.2998e+08, device='cuda:0')
c= tensor(7.3012e+08, device='cuda:0')
c= tensor(7.3085e+08, device='cuda:0')
c= tensor(7.7392e+08, device='cuda:0')
c= tensor(7.7394e+08, device='cuda:0')
c= tensor(7.7395e+08, device='cuda:0')
c= tensor(7.7445e+08, device='cuda:0')
c= tensor(7.7452e+08, device='cuda:0')
c= tensor(7.9729e+08, device='cuda:0')
c= tensor(7.9731e+08, device='cuda:0')
c= tensor(7.9881e+08, device='cuda:0')
c= tensor(8.0475e+08, device='cuda:0')
c= tensor(8.0475e+08, device='cuda:0')
c= tensor(8.1152e+08, device='cuda:0')
c= tensor(8.1173e+08, device='cuda:0')
c= tensor(8.5724e+08, device='cuda:0')
c= tensor(8.5726e+08, device='cuda:0')
c= tensor(8.5730e+08, device='cuda:0')
c= tensor(8.5738e+08, device='cuda:0')
c= tensor(8.5739e+08, device='cuda:0')
c= tensor(8.5741e+08, device='cuda:0')
c= tensor(8.5771e+08, device='cuda:0')
c= tensor(8.5772e+08, device='cuda:0')
c= tensor(8.6011e+08, device='cuda:0')
c= tensor(8.6014e+08, device='cuda:0')
c= tensor(8.6015e+08, device='cuda:0')
c= tensor(8.6018e+08, device='cuda:0')
c= tensor(8.6603e+08, device='cuda:0')
c= tensor(8.6915e+08, device='cuda:0')
c= tensor(8.7878e+08, device='cuda:0')
c= tensor(8.7881e+08, device='cuda:0')
c= tensor(8.7883e+08, device='cuda:0')
c= tensor(8.7883e+08, device='cuda:0')
c= tensor(8.7885e+08, device='cuda:0')
c= tensor(8.8196e+08, device='cuda:0')
c= tensor(8.8197e+08, device='cuda:0')
c= tensor(8.8202e+08, device='cuda:0')
c= tensor(8.8250e+08, device='cuda:0')
c= tensor(8.8300e+08, device='cuda:0')
c= tensor(8.8311e+08, device='cuda:0')
c= tensor(8.8312e+08, device='cuda:0')
c= tensor(9.0070e+08, device='cuda:0')
c= tensor(9.0118e+08, device='cuda:0')
c= tensor(9.0125e+08, device='cuda:0')
c= tensor(9.0127e+08, device='cuda:0')
c= tensor(9.0165e+08, device='cuda:0')
c= tensor(9.0308e+08, device='cuda:0')
c= tensor(9.2926e+08, device='cuda:0')
c= tensor(9.3201e+08, device='cuda:0')
c= tensor(9.3201e+08, device='cuda:0')
c= tensor(9.3217e+08, device='cuda:0')
c= tensor(9.3256e+08, device='cuda:0')
c= tensor(9.3256e+08, device='cuda:0')
c= tensor(9.3261e+08, device='cuda:0')
c= tensor(9.3261e+08, device='cuda:0')
c= tensor(9.3302e+08, device='cuda:0')
c= tensor(9.3343e+08, device='cuda:0')
c= tensor(9.3343e+08, device='cuda:0')
c= tensor(9.3347e+08, device='cuda:0')
c= tensor(9.3350e+08, device='cuda:0')
c= tensor(9.3385e+08, device='cuda:0')
c= tensor(9.3386e+08, device='cuda:0')
c= tensor(9.3394e+08, device='cuda:0')
c= tensor(9.3394e+08, device='cuda:0')
c= tensor(9.3399e+08, device='cuda:0')
c= tensor(9.3399e+08, device='cuda:0')
c= tensor(9.3401e+08, device='cuda:0')
c= tensor(9.3404e+08, device='cuda:0')
c= tensor(9.3779e+08, device='cuda:0')
c= tensor(9.3780e+08, device='cuda:0')
c= tensor(9.3780e+08, device='cuda:0')
c= tensor(9.3781e+08, device='cuda:0')
c= tensor(9.3992e+08, device='cuda:0')
c= tensor(9.4307e+08, device='cuda:0')
c= tensor(9.4443e+08, device='cuda:0')
c= tensor(9.4444e+08, device='cuda:0')
c= tensor(9.5562e+08, device='cuda:0')
c= tensor(9.5709e+08, device='cuda:0')
c= tensor(9.5710e+08, device='cuda:0')
c= tensor(9.5711e+08, device='cuda:0')
c= tensor(9.5719e+08, device='cuda:0')
c= tensor(9.5951e+08, device='cuda:0')
c= tensor(9.5993e+08, device='cuda:0')
c= tensor(9.6346e+08, device='cuda:0')
c= tensor(9.6348e+08, device='cuda:0')
c= tensor(9.6349e+08, device='cuda:0')
c= tensor(9.6349e+08, device='cuda:0')
c= tensor(9.6363e+08, device='cuda:0')
c= tensor(9.6365e+08, device='cuda:0')
c= tensor(9.6373e+08, device='cuda:0')
c= tensor(9.6510e+08, device='cuda:0')
c= tensor(9.7902e+08, device='cuda:0')
c= tensor(9.7903e+08, device='cuda:0')
c= tensor(9.7903e+08, device='cuda:0')
c= tensor(9.7919e+08, device='cuda:0')
c= tensor(9.7932e+08, device='cuda:0')
c= tensor(9.7945e+08, device='cuda:0')
c= tensor(9.7945e+08, device='cuda:0')
c= tensor(9.7946e+08, device='cuda:0')
c= tensor(9.7956e+08, device='cuda:0')
c= tensor(9.7956e+08, device='cuda:0')
c= tensor(9.8125e+08, device='cuda:0')
c= tensor(9.8126e+08, device='cuda:0')
c= tensor(9.8128e+08, device='cuda:0')
c= tensor(9.8131e+08, device='cuda:0')
c= tensor(9.8133e+08, device='cuda:0')
c= tensor(9.8134e+08, device='cuda:0')
c= tensor(9.8417e+08, device='cuda:0')
c= tensor(9.9736e+08, device='cuda:0')
c= tensor(9.9980e+08, device='cuda:0')
c= tensor(1.0017e+09, device='cuda:0')
c= tensor(1.0018e+09, device='cuda:0')
c= tensor(1.0018e+09, device='cuda:0')
c= tensor(1.0019e+09, device='cuda:0')
c= tensor(1.0029e+09, device='cuda:0')
c= tensor(1.0033e+09, device='cuda:0')
c= tensor(1.0034e+09, device='cuda:0')
c= tensor(1.0034e+09, device='cuda:0')
c= tensor(1.1212e+09, device='cuda:0')
c= tensor(1.1212e+09, device='cuda:0')
c= tensor(1.1216e+09, device='cuda:0')
c= tensor(1.1254e+09, device='cuda:0')
c= tensor(1.1255e+09, device='cuda:0')
c= tensor(1.1257e+09, device='cuda:0')
c= tensor(1.1750e+09, device='cuda:0')
c= tensor(1.1785e+09, device='cuda:0')
c= tensor(1.1794e+09, device='cuda:0')
c= tensor(1.1794e+09, device='cuda:0')
c= tensor(1.1796e+09, device='cuda:0')
c= tensor(1.1796e+09, device='cuda:0')
c= tensor(1.1820e+09, device='cuda:0')
c= tensor(1.3180e+09, device='cuda:0')
c= tensor(1.3221e+09, device='cuda:0')
c= tensor(1.3254e+09, device='cuda:0')
c= tensor(1.3258e+09, device='cuda:0')
c= tensor(1.3265e+09, device='cuda:0')
c= tensor(1.3269e+09, device='cuda:0')
c= tensor(1.3270e+09, device='cuda:0')
c= tensor(1.3270e+09, device='cuda:0')
c= tensor(1.3387e+09, device='cuda:0')
c= tensor(1.3397e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3620e+09, device='cuda:0')
c= tensor(1.3625e+09, device='cuda:0')
c= tensor(1.3626e+09, device='cuda:0')
c= tensor(1.3636e+09, device='cuda:0')
c= tensor(1.3647e+09, device='cuda:0')
c= tensor(1.3647e+09, device='cuda:0')
c= tensor(1.3787e+09, device='cuda:0')
c= tensor(1.3790e+09, device='cuda:0')
c= tensor(1.3794e+09, device='cuda:0')
c= tensor(1.3794e+09, device='cuda:0')
c= tensor(1.3794e+09, device='cuda:0')
c= tensor(1.3794e+09, device='cuda:0')
c= tensor(1.3794e+09, device='cuda:0')
c= tensor(1.3797e+09, device='cuda:0')
c= tensor(1.3798e+09, device='cuda:0')
c= tensor(2.8750e+09, device='cuda:0')
c= tensor(2.8752e+09, device='cuda:0')
c= tensor(2.8787e+09, device='cuda:0')
c= tensor(2.8787e+09, device='cuda:0')
c= tensor(2.8788e+09, device='cuda:0')
c= tensor(2.8788e+09, device='cuda:0')
c= tensor(2.8912e+09, device='cuda:0')
c= tensor(2.8912e+09, device='cuda:0')
c= tensor(2.9561e+09, device='cuda:0')
c= tensor(2.9561e+09, device='cuda:0')
c= tensor(2.9596e+09, device='cuda:0')
c= tensor(2.9599e+09, device='cuda:0')
c= tensor(2.9621e+09, device='cuda:0')
c= tensor(2.9680e+09, device='cuda:0')
c= tensor(2.9681e+09, device='cuda:0')
c= tensor(2.9681e+09, device='cuda:0')
c= tensor(2.9687e+09, device='cuda:0')
c= tensor(2.9687e+09, device='cuda:0')
c= tensor(2.9687e+09, device='cuda:0')
c= tensor(2.9699e+09, device='cuda:0')
c= tensor(2.9700e+09, device='cuda:0')
c= tensor(2.9709e+09, device='cuda:0')
c= tensor(2.9710e+09, device='cuda:0')
c= tensor(2.9742e+09, device='cuda:0')
c= tensor(2.9912e+09, device='cuda:0')
c= tensor(2.9916e+09, device='cuda:0')
c= tensor(2.9917e+09, device='cuda:0')
c= tensor(2.9929e+09, device='cuda:0')
c= tensor(2.9935e+09, device='cuda:0')
c= tensor(2.9936e+09, device='cuda:0')
c= tensor(2.9938e+09, device='cuda:0')
c= tensor(2.9939e+09, device='cuda:0')
c= tensor(2.9946e+09, device='cuda:0')
c= tensor(2.9986e+09, device='cuda:0')
c= tensor(3.0000e+09, device='cuda:0')
c= tensor(3.0000e+09, device='cuda:0')
c= tensor(3.0000e+09, device='cuda:0')
c= tensor(3.0001e+09, device='cuda:0')
c= tensor(3.0015e+09, device='cuda:0')
c= tensor(3.0020e+09, device='cuda:0')
c= tensor(3.0024e+09, device='cuda:0')
c= tensor(3.0024e+09, device='cuda:0')
c= tensor(3.0025e+09, device='cuda:0')
c= tensor(3.0027e+09, device='cuda:0')
c= tensor(3.0030e+09, device='cuda:0')
c= tensor(3.0031e+09, device='cuda:0')
c= tensor(3.0034e+09, device='cuda:0')
c= tensor(3.0037e+09, device='cuda:0')
c= tensor(3.0039e+09, device='cuda:0')
c= tensor(3.0039e+09, device='cuda:0')
c= tensor(3.0039e+09, device='cuda:0')
c= tensor(3.0055e+09, device='cuda:0')
c= tensor(3.0057e+09, device='cuda:0')
c= tensor(3.0061e+09, device='cuda:0')
c= tensor(3.0061e+09, device='cuda:0')
c= tensor(3.0061e+09, device='cuda:0')
c= tensor(3.0062e+09, device='cuda:0')
c= tensor(3.0068e+09, device='cuda:0')
c= tensor(3.0070e+09, device='cuda:0')
c= tensor(3.0070e+09, device='cuda:0')
c= tensor(3.0070e+09, device='cuda:0')
c= tensor(3.0075e+09, device='cuda:0')
c= tensor(3.0075e+09, device='cuda:0')
c= tensor(3.0083e+09, device='cuda:0')
c= tensor(3.0083e+09, device='cuda:0')
c= tensor(3.0083e+09, device='cuda:0')
c= tensor(3.0088e+09, device='cuda:0')
c= tensor(3.0088e+09, device='cuda:0')
c= tensor(3.0156e+09, device='cuda:0')
c= tensor(3.0156e+09, device='cuda:0')
c= tensor(3.0166e+09, device='cuda:0')
c= tensor(3.0177e+09, device='cuda:0')
c= tensor(3.0178e+09, device='cuda:0')
c= tensor(3.0200e+09, device='cuda:0')
c= tensor(3.0210e+09, device='cuda:0')
c= tensor(3.0210e+09, device='cuda:0')
c= tensor(3.0212e+09, device='cuda:0')
c= tensor(3.0213e+09, device='cuda:0')
c= tensor(3.0215e+09, device='cuda:0')
c= tensor(3.0231e+09, device='cuda:0')
c= tensor(3.0233e+09, device='cuda:0')
c= tensor(3.0236e+09, device='cuda:0')
c= tensor(3.0237e+09, device='cuda:0')
c= tensor(3.0258e+09, device='cuda:0')
c= tensor(3.0258e+09, device='cuda:0')
c= tensor(3.0258e+09, device='cuda:0')
c= tensor(3.0275e+09, device='cuda:0')
c= tensor(3.0373e+09, device='cuda:0')
c= tensor(3.0374e+09, device='cuda:0')
c= tensor(3.0374e+09, device='cuda:0')
c= tensor(3.0375e+09, device='cuda:0')
c= tensor(3.0383e+09, device='cuda:0')
c= tensor(3.0385e+09, device='cuda:0')
c= tensor(3.0386e+09, device='cuda:0')
c= tensor(3.0388e+09, device='cuda:0')
c= tensor(3.0404e+09, device='cuda:0')
c= tensor(3.0404e+09, device='cuda:0')
c= tensor(3.0411e+09, device='cuda:0')
c= tensor(3.0411e+09, device='cuda:0')
time to make c is 12.35159420967102
time for making loss is 12.351608276367188
p0 True
it  0 : 991636480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4175261696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4175396864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  498122500.0
relative error loss 0.16379465
shape of L is 
torch.Size([])
memory (bytes)
4206104576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4206178304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  491990270.0
relative error loss 0.16177823
shape of L is 
torch.Size([])
memory (bytes)
4209762304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4209762304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  462129920.0
relative error loss 0.15195942
shape of L is 
torch.Size([])
memory (bytes)
4212957184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4212957184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  449836800.0
relative error loss 0.14791715
shape of L is 
torch.Size([])
memory (bytes)
4216188928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4216188928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  442299900.0
relative error loss 0.14543884
shape of L is 
torch.Size([])
memory (bytes)
4219187200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4219400192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  437475070.0
relative error loss 0.14385232
shape of L is 
torch.Size([])
memory (bytes)
4222615552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4222615552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  434213380.0
relative error loss 0.1427798
shape of L is 
torch.Size([])
memory (bytes)
4225839104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4225839104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  431887360.0
relative error loss 0.14201495
shape of L is 
torch.Size([])
memory (bytes)
4228841472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4229062656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  430124030.0
relative error loss 0.14143512
shape of L is 
torch.Size([])
memory (bytes)
4232286208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4232286208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  428652800.0
relative error loss 0.14095135
time to take a step is 432.41361260414124
it  1 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4235526144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4235526144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  428652800.0
relative error loss 0.14095135
shape of L is 
torch.Size([])
memory (bytes)
4238614528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4238741504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  427478000.0
relative error loss 0.14056505
shape of L is 
torch.Size([])
memory (bytes)
4241973248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4241973248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  426545400.0
relative error loss 0.14025839
shape of L is 
torch.Size([])
memory (bytes)
4245168128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4245168128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  425727500.0
relative error loss 0.13998944
shape of L is 
torch.Size([])
memory (bytes)
4248444928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4248444928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  425025540.0
relative error loss 0.13975862
shape of L is 
torch.Size([])
memory (bytes)
4251672576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4251672576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  424415230.0
relative error loss 0.13955793
shape of L is 
torch.Size([])
memory (bytes)
4254892032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4254892032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  423927550.0
relative error loss 0.13939756
shape of L is 
torch.Size([])
memory (bytes)
4258123776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4258123776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  423435780.0
relative error loss 0.13923585
shape of L is 
torch.Size([])
memory (bytes)
4261289984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4261289984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  422953730.0
relative error loss 0.13907735
shape of L is 
torch.Size([])
memory (bytes)
4264591360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4264591360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  422598660.0
relative error loss 0.1389606
time to take a step is 446.7120478153229
it  2 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4267819008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4267819008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  422598660.0
relative error loss 0.1389606
shape of L is 
torch.Size([])
memory (bytes)
4271042560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4271042560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  422108160.0
relative error loss 0.13879931
shape of L is 
torch.Size([])
memory (bytes)
4274262016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4274262016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  421931000.0
relative error loss 0.13874106
shape of L is 
torch.Size([])
memory (bytes)
4277407744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4277506048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  421470460.0
relative error loss 0.13858962
shape of L is 
torch.Size([])
memory (bytes)
4280717312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4280717312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  421261300.0
relative error loss 0.13852085
shape of L is 
torch.Size([])
memory (bytes)
4283940864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4283940864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  421028600.0
relative error loss 0.13844432
shape of L is 
torch.Size([])
memory (bytes)
4287160320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4287160320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  420674800.0
relative error loss 0.13832799
shape of L is 
torch.Size([])
memory (bytes)
4290387968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4290387968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  420566270.0
relative error loss 0.1382923
shape of L is 
torch.Size([])
memory (bytes)
4293615616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4293615616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  420305400.0
relative error loss 0.13820653
shape of L is 
torch.Size([])
memory (bytes)
4296835072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4296835072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  420175600.0
relative error loss 0.13816383
time to take a step is 452.7975277900696
it  3 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4300058624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4300058624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  420175600.0
relative error loss 0.13816383
shape of L is 
torch.Size([])
memory (bytes)
4303282176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4303282176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  419939840.0
relative error loss 0.13808632
shape of L is 
torch.Size([])
memory (bytes)
4306399232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4306505728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  419838980.0
relative error loss 0.13805315
shape of L is 
torch.Size([])
memory (bytes)
4309729280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4309729280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  419630340.0
relative error loss 0.13798454
shape of L is 
torch.Size([])
memory (bytes)
4312948736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4312948736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  419522300.0
relative error loss 0.13794902
shape of L is 
torch.Size([])
memory (bytes)
4316094464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4316180480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  419422720.0
relative error loss 0.13791627
shape of L is 
torch.Size([])
memory (bytes)
4319395840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4319395840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  419279360.0
relative error loss 0.13786913
shape of L is 
torch.Size([])
memory (bytes)
4322631680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4322631680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  419177200.0
relative error loss 0.13783555
shape of L is 
torch.Size([])
memory (bytes)
4325859328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4325859328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  419042800.0
relative error loss 0.13779135
shape of L is 
torch.Size([])
memory (bytes)
4329078784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4329078784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  418956800.0
relative error loss 0.13776307
time to take a step is 422.4338285923004
it  4 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4332306432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4332306432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  418956800.0
relative error loss 0.13776307
shape of L is 
torch.Size([])
memory (bytes)
4335390720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4335521792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  418818050.0
relative error loss 0.13771744
shape of L is 
torch.Size([])
memory (bytes)
4338745344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4338745344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  418654200.0
relative error loss 0.13766357
shape of L is 
torch.Size([])
memory (bytes)
4341968896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4341968896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  418534660.0
relative error loss 0.13762425
shape of L is 
torch.Size([])
memory (bytes)
4345106432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4345106432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  418460160.0
relative error loss 0.13759975
shape of L is 
torch.Size([])
memory (bytes)
4348420096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4348420096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  418366200.0
relative error loss 0.13756886
shape of L is 
torch.Size([])
memory (bytes)
4351639552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4351639552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  418276100.0
relative error loss 0.13753924
shape of L is 
torch.Size([])
memory (bytes)
4354842624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4354871296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  418180350.0
relative error loss 0.13750775
shape of L is 
torch.Size([])
memory (bytes)
4358094848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4358094848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  418124540.0
relative error loss 0.1374894
shape of L is 
torch.Size([])
memory (bytes)
4361318400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4361318400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  418070800.0
relative error loss 0.13747172
time to take a step is 423.55983805656433
it  5 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4364546048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4364546048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  418070800.0
relative error loss 0.13747172
shape of L is 
torch.Size([])
memory (bytes)
4367765504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4367765504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  418016770.0
relative error loss 0.13745396
shape of L is 
torch.Size([])
memory (bytes)
4371001344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4371001344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  7% |
error is  417980400.0
relative error loss 0.13744201
shape of L is 
torch.Size([])
memory (bytes)
4374167552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4374167552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  417931260.0
relative error loss 0.13742584
shape of L is 
torch.Size([])
memory (bytes)
4377456640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4377456640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  417871360.0
relative error loss 0.13740614
shape of L is 
torch.Size([])
memory (bytes)
4380672000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4380672000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  417828600.0
relative error loss 0.13739209
shape of L is 
torch.Size([])
memory (bytes)
4383866880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4383866880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  417763600.0
relative error loss 0.1373707
shape of L is 
torch.Size([])
memory (bytes)
4387127296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4387127296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  417709060.0
relative error loss 0.13735278
shape of L is 
torch.Size([])
memory (bytes)
4390346752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4390346752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  7% |
error is  417672450.0
relative error loss 0.13734074
shape of L is 
torch.Size([])
memory (bytes)
4393517056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4393570304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  417639680.0
relative error loss 0.13732997
time to take a step is 425.0679740905762
it  6 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4396806144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4396806144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  417639680.0
relative error loss 0.13732997
shape of L is 
torch.Size([])
memory (bytes)
4400033792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4400033792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  417613570.0
relative error loss 0.13732138
shape of L is 
torch.Size([])
memory (bytes)
4403175424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4403175424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  417581300.0
relative error loss 0.13731077
shape of L is 
torch.Size([])
memory (bytes)
4406489088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4406489088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  417528060.0
relative error loss 0.13729326
shape of L is 
torch.Size([])
memory (bytes)
4409716736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4409716736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  417479420.0
relative error loss 0.13727728
shape of L is 
torch.Size([])
memory (bytes)
4412891136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4412891136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  417422080.0
relative error loss 0.13725841
shape of L is 
torch.Size([])
memory (bytes)
4416151552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4416151552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  417403650.0
relative error loss 0.13725235
shape of L is 
torch.Size([])
memory (bytes)
4419268608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4419268608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  417361660.0
relative error loss 0.13723855
shape of L is 
torch.Size([])
memory (bytes)
4422598656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4422598656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  7% |
error is  417340400.0
relative error loss 0.13723156
shape of L is 
torch.Size([])
memory (bytes)
4425814016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4425814016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  417318660.0
relative error loss 0.1372244
time to take a step is 425.0677058696747
it  7 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4428959744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4428959744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  417318660.0
relative error loss 0.1372244
shape of L is 
torch.Size([])
memory (bytes)
4432285696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4432285696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  417283070.0
relative error loss 0.13721271
shape of L is 
torch.Size([])
memory (bytes)
4435496960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  7% |
memory (bytes)
4435496960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  417243650.0
relative error loss 0.13719974
shape of L is 
torch.Size([])
memory (bytes)
4438736896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4438736896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  417207300.0
relative error loss 0.1371878
shape of L is 
torch.Size([])
memory (bytes)
4441960448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4441960448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  417175040.0
relative error loss 0.13717718
shape of L is 
torch.Size([])
memory (bytes)
4445188096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4445188096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  417131000.0
relative error loss 0.1371627
shape of L is 
torch.Size([])
memory (bytes)
4448399360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4448399360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  417082620.0
relative error loss 0.13714679
shape of L is 
torch.Size([])
memory (bytes)
4451622912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4451622912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  417059840.0
relative error loss 0.1371393
shape of L is 
torch.Size([])
memory (bytes)
4454723584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4454723584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  417041150.0
relative error loss 0.13713315
shape of L is 
torch.Size([])
memory (bytes)
4458061824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4458061824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  417004540.0
relative error loss 0.13712111
time to take a step is 432.3617784976959
it  8 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4461285376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4461285376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  7% |
error is  417004540.0
relative error loss 0.13712111
shape of L is 
torch.Size([])
memory (bytes)
4464496640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4464500736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  7% |
error is  416967680.0
relative error loss 0.137109
shape of L is 
torch.Size([])
memory (bytes)
4467720192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4467720192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  416937730.0
relative error loss 0.13709915
shape of L is 
torch.Size([])
memory (bytes)
4470910976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4470910976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416905200.0
relative error loss 0.13708846
shape of L is 
torch.Size([])
memory (bytes)
4474175488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4474175488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416850940.0
relative error loss 0.13707061
shape of L is 
torch.Size([])
memory (bytes)
4477390848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4477390848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  416840200.0
relative error loss 0.13706708
shape of L is 
torch.Size([])
memory (bytes)
4480593920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4480593920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416794620.0
relative error loss 0.13705209
shape of L is 
torch.Size([])
memory (bytes)
4483854336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4483854336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416780300.0
relative error loss 0.13704738
shape of L is 
torch.Size([])
memory (bytes)
4486995968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4486995968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416742660.0
relative error loss 0.137035
shape of L is 
torch.Size([])
memory (bytes)
4490301440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4490301440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 84% |  7% |
error is  416736000.0
relative error loss 0.1370328
time to take a step is 424.89480352401733
it  9 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4493524992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4493524992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416736000.0
relative error loss 0.1370328
shape of L is 
torch.Size([])
memory (bytes)
4496580608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4496744448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416698620.0
relative error loss 0.13702053
shape of L is 
torch.Size([])
memory (bytes)
4499976192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4499976192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416680200.0
relative error loss 0.13701446
shape of L is 
torch.Size([])
memory (bytes)
4503195648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4503195648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 84% |  7% |
error is  416663550.0
relative error loss 0.137009
shape of L is 
torch.Size([])
memory (bytes)
4506419200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4506419200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416638980.0
relative error loss 0.1370009
shape of L is 
torch.Size([])
memory (bytes)
4509646848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4509646848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416638460.0
relative error loss 0.13700074
shape of L is 
torch.Size([])
memory (bytes)
4512841728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4512870400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  416619000.0
relative error loss 0.13699435
shape of L is 
torch.Size([])
memory (bytes)
4516093952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4516093952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416597250.0
relative error loss 0.1369872
shape of L is 
torch.Size([])
memory (bytes)
4519317504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4519317504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  416582660.0
relative error loss 0.1369824
shape of L is 
torch.Size([])
memory (bytes)
4522545152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4522545152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416555260.0
relative error loss 0.13697338
time to take a step is 424.39573860168457
it  10 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4525768704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4525768704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  416555260.0
relative error loss 0.13697338
shape of L is 
torch.Size([])
memory (bytes)
4528996352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4528996352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416537340.0
relative error loss 0.1369675
shape of L is 
torch.Size([])
memory (bytes)
4532215808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4532215808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416520450.0
relative error loss 0.13696194
shape of L is 
torch.Size([])
memory (bytes)
4535431168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4535431168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416505860.0
relative error loss 0.13695714
shape of L is 
torch.Size([])
memory (bytes)
4538527744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4538527744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  7% |
error is  416491520.0
relative error loss 0.13695242
shape of L is 
torch.Size([])
memory (bytes)
4541890560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4541890560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  416476930.0
relative error loss 0.13694762
shape of L is 
torch.Size([])
memory (bytes)
4545118208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4545118208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416453900.0
relative error loss 0.13694005
shape of L is 
torch.Size([])
memory (bytes)
4548317184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4548317184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416446200.0
relative error loss 0.13693753
shape of L is 
torch.Size([])
memory (bytes)
4551565312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4551565312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  416434700.0
relative error loss 0.13693373
shape of L is 
torch.Size([])
memory (bytes)
4554694656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4554694656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  416422140.0
relative error loss 0.13692962
time to take a step is 423.36846947669983
it  11 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4558008320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4558008320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416422140.0
relative error loss 0.13692962
shape of L is 
torch.Size([])
memory (bytes)
4561235968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4561235968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  416407800.0
relative error loss 0.1369249
shape of L is 
torch.Size([])
memory (bytes)
4564348928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4564348928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416392700.0
relative error loss 0.13691993
shape of L is 
torch.Size([])
memory (bytes)
4567678976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4567678976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416383500.0
relative error loss 0.1369169
shape of L is 
torch.Size([])
memory (bytes)
4570902528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4570902528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  7% |
error is  416384500.0
relative error loss 0.13691723
shape of L is 
torch.Size([])
memory (bytes)
4574130176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4574130176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  416372740.0
relative error loss 0.13691336
shape of L is 
torch.Size([])
memory (bytes)
4577366016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4577366016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416356350.0
relative error loss 0.13690798
shape of L is 
torch.Size([])
memory (bytes)
4580589568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4580589568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  416346370.0
relative error loss 0.13690469
shape of L is 
torch.Size([])
memory (bytes)
4583813120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4583813120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416336400.0
relative error loss 0.13690141
shape of L is 
torch.Size([])
memory (bytes)
4587044864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4587044864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416323070.0
relative error loss 0.13689703
time to take a step is 425.09842014312744
it  12 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4590256128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4590256128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416323070.0
relative error loss 0.13689703
shape of L is 
torch.Size([])
memory (bytes)
4593487872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4593487872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416307970.0
relative error loss 0.13689207
shape of L is 
torch.Size([])
memory (bytes)
4596715520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4596715520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  416298500.0
relative error loss 0.13688895
shape of L is 
torch.Size([])
memory (bytes)
4599934976
| ID | GPU | MEM |
------------------
|  0 | 10% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4599934976
| ID | GPU | MEM |
------------------
|  0 |  8% |  0% |
|  1 | 93% |  7% |
error is  416288770.0
relative error loss 0.13688575
shape of L is 
torch.Size([])
memory (bytes)
4603162624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4603162624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416276480.0
relative error loss 0.13688171
shape of L is 
torch.Size([])
memory (bytes)
4606377984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4606377984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416272400.0
relative error loss 0.13688037
shape of L is 
torch.Size([])
memory (bytes)
4609548288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4609548288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416263420.0
relative error loss 0.13687742
shape of L is 
torch.Size([])
memory (bytes)
4612837376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4612837376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416259330.0
relative error loss 0.13687608
shape of L is 
torch.Size([])
memory (bytes)
4616056832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4616056832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416251900.0
relative error loss 0.13687363
shape of L is 
torch.Size([])
memory (bytes)
4619284480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4619284480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416242700.0
relative error loss 0.1368706
time to take a step is 424.5723216533661
it  13 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4622503936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4622503936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  416242700.0
relative error loss 0.1368706
shape of L is 
torch.Size([])
memory (bytes)
4625719296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4625719296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416234240.0
relative error loss 0.13686782
shape of L is 
torch.Size([])
memory (bytes)
4628918272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4628918272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416222720.0
relative error loss 0.13686404
shape of L is 
torch.Size([])
memory (bytes)
4632170496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4632170496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  416217600.0
relative error loss 0.13686235
shape of L is 
torch.Size([])
memory (bytes)
4635402240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4635402240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416204800.0
relative error loss 0.13685814
shape of L is 
torch.Size([])
memory (bytes)
4638482432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4638629888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  416207870.0
relative error loss 0.13685915
shape of L is 
torch.Size([])
memory (bytes)
4641861632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4641861632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  416197380.0
relative error loss 0.1368557
shape of L is 
torch.Size([])
memory (bytes)
4645072896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4645072896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  7% |
error is  416187900.0
relative error loss 0.13685259
shape of L is 
torch.Size([])
memory (bytes)
4648312832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4648312832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416176900.0
relative error loss 0.13684897
shape of L is 
torch.Size([])
memory (bytes)
4651532288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4651532288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416169730.0
relative error loss 0.1368466
time to take a step is 425.84464836120605
it  14 : 1236076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4654751744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4654751744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416169730.0
relative error loss 0.1368466
shape of L is 
torch.Size([])
memory (bytes)
4657905664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4657905664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416161540.0
relative error loss 0.13684392
shape of L is 
torch.Size([])
memory (bytes)
4661198848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4661198848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416155900.0
relative error loss 0.13684206
shape of L is 
torch.Size([])
memory (bytes)
4664422400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4664422400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416149760.0
relative error loss 0.13684005
shape of L is 
torch.Size([])
memory (bytes)
4667637760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4667637760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  416138240.0
relative error loss 0.13683626
shape of L is 
torch.Size([])
memory (bytes)
4670861312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4670861312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  416126720.0
relative error loss 0.13683246
shape of L is 
torch.Size([])
memory (bytes)
4674093056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4674093056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416119800.0
relative error loss 0.1368302
shape of L is 
torch.Size([])
memory (bytes)
4677312512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4677312512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  416113400.0
relative error loss 0.1368281
shape of L is 
torch.Size([])
memory (bytes)
4680536064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4680536064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416104200.0
relative error loss 0.13682505
shape of L is 
torch.Size([])
memory (bytes)
4683763712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4683763712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  7% |
error is  416097280.0
relative error loss 0.13682279
time to take a step is 425.1752030849457
sum tnnu_Z after tensor(7525548., device='cuda:0')
shape of features
(2408,)
shape of features
(2408,)
number of orig particles 9631
number of new particles after remove low mass 8773
tnuZ shape should be parts x labs
torch.Size([9631, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  498064900.0
relative error without small mass is  0.16377571
nnu_Z shape should be number of particles by maxV
(9631, 702)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
shape of features
(9631,)
Tue Jan 31 07:17:28 EST 2023
