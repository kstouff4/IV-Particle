Wed Feb 1 20:59:24 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 18683669
numbers of Z: 24171
shape of features
(24171,)
shape of features
(24171,)
ZX	Vol	Parts	Cubes	Eps
Z	0.023121952020425617	24171	24.171	0.09853184744455051
X	0.01995545435296932	248	0.248	0.4317211380197264
X	0.021583119747036493	5800	5.8	0.15496325931250463
X	0.0213214607964789	1226	1.226	0.25908493570366614
X	0.021357159533074772	4894	4.894	0.1634154730924013
X	0.021783476016889423	15816	15.816	0.11126114077911214
X	0.021797040494143067	6403	6.403	0.1504312311105296
X	0.021805961492149244	31784	31.784	0.0881973553852059
X	0.021809536729424445	14649	14.649	0.11418598971243497
X	0.0212404905212816	2366	2.366	0.20783400209922787
X	0.021732853105165297	4689	4.689	0.1667293935914208
X	0.020649534284027337	2133	2.133	0.2131276527608523
X	0.021522281074496285	52152	52.152	0.07445132712064328
X	0.021063330396900996	4109	4.109	0.17242370238137672
X	0.02168904070219635	154754	154.754	0.05194369265138054
X	0.021686584787876613	10666	10.666	0.12668636108378525
X	0.021568877679432004	8549	8.549	0.13613529709573163
X	0.02182407605388014	31844	31.844	0.0881663273836395
X	0.02165233693272462	16314	16.314	0.10989588721400378
X	0.021805459350246883	92917	92.917	0.061681750009964134
X	0.021621878677087505	63043	63.043	0.06999798175023941
X	0.021178401545520704	2885	2.885	0.19434906410597189
X	0.022059802051256402	106942	106.942	0.05908597908313548
X	0.021385121760717985	4966	4.966	0.1626928014223747
X	0.02173331217045654	20491	20.491	0.10198138871328653
X	0.0203397638054075	790	0.79	0.2952837888164099
X	0.02180543581563932	52434	52.434	0.07464211335024132
X	0.021923213459932017	31404	31.404	0.08871010254336056
X	0.021568961236128063	2800	2.8	0.19749543709139325
X	0.021769896600764158	20999	20.999	0.10120902729002801
X	0.021826554599005038	584775	584.775	0.033419411367390475
X	0.02172573432006715	2905	2.905	0.19555823513129755
X	0.022118285328733736	464000	464.0	0.03625861062347232
X	0.020838064242575536	6840	6.84	0.14496631195597523
X	0.02156390269512174	2435	2.435	0.20689157191935859
X	0.02112127255241431	2218	2.218	0.21195948045940585
X	0.021730108784560127	49129	49.129	0.07619175760031506
X	0.02197482827952414	69243	69.243	0.06821028614662616
X	0.02111557327984327	791	0.791	0.2988652797687599
X	0.021606048893606577	4381	4.381	0.1702161253740476
X	0.021356793492453535	1347	1.347	0.2512211219667249
X	0.02159387629530695	1720	1.72	0.23241673044712097
X	0.02060802305012486	682	0.682	0.3114707860804492
X	0.019343493742606833	246	0.246	0.42841690892973355
X	0.020569647150652324	1199	1.199	0.2579106165159522
X	0.020649701414429827	279	0.279	0.41985873313725286
X	0.020508577635650827	427	0.427	0.3634984433362631
X	0.0214718320106242	1733	1.733	0.23139653211613762
X	0.021259404139469074	1185	1.185	0.26178469109610963
X	0.021039846342319188	1003	1.003	0.27579128831685823
X	0.021295430253299694	6798	6.798	0.14631934653838005
X	0.021553515873007002	5457	5.457	0.1580719181328165
X	0.021437910480260967	2184	2.184	0.21411311658204032
X	0.021437126493110404	2765	2.765	0.19792036277251282
X	0.020705307799820414	876	0.876	0.2869849980969607
X	0.02130456968030339	3538	3.538	0.18193027639961276
X	0.021289852642342542	3794	3.794	0.17770177928356742
X	0.020691735699994426	569	0.569	0.3313051742738187
X	0.021500966512056695	1483	1.483	0.2438403710829842
X	0.020951298938106773	937	0.937	0.28172396207609507
X	0.020491510432217742	1408	1.408	0.24415021190752584
X	0.021720156247090307	7855	7.855	0.14035856472565877
X	0.02082145489717247	562	0.562	0.33336877484438693
X	0.021506075953796156	8328	8.328	0.13719559039368992
X	0.02108886198473086	1768	1.768	0.22848461453405175
X	0.021149786370488664	749	0.749	0.3045145551164079
X	0.02091501167837943	986	0.986	0.27681762113037733
X	0.02127734054632102	567	0.567	0.3347943307355447
X	0.020886889860627665	998	0.998	0.2755800312958406
X	0.020668879048881702	1075	1.075	0.26789800934365127
X	0.021141668825570413	841	0.841	0.2929416274069384
X	0.019247971873058153	1175	1.175	0.25396997112255354
X	0.02075735541029473	1498	1.498	0.24018915965035814
X	0.020404206842189226	644	0.644	0.31643021769625623
X	0.020312014538654232	1149	1.149	0.26050195685610694
X	0.021282721567573663	1580	1.58	0.237934102118621
X	0.021662374014429202	4749	4.749	0.1658445840579851
X	0.021336987066970313	638	0.638	0.3221841256146791
X	0.019671604363975918	163	0.163	0.4941787413826715
X	0.020896825888302085	878	0.878	0.2876483823997319
X	0.021118574492274438	2762	2.762	0.1970063880053682
X	0.021296710739133484	1195	1.195	0.26120503387503125
X	0.01967071375724748	337	0.337	0.38790919354453735
X	0.02159507408538261	4001	4.001	0.1754131101990034
X	0.019924900932588807	616	0.616	0.31861910952493006
X	0.021147658323276444	1196	1.196	0.2605215790328567
X	0.020406443154177417	318	0.318	0.4003563592745345
X	0.021365032714882378	2970	2.97	0.19304076839926554
X	0.020574202315711693	311	0.311	0.40444009683918136
X	0.021163925049668825	1621	1.621	0.2354711654553044
X	0.02138610743904388	1565	1.565	0.23907773578194594
X	0.02073297986737461	764	0.764	0.30050805837498906
X	0.020064782017789128	351	0.351	0.3852202509540907
X	0.02021062417019804	445	0.445	0.35678498710271705
X	0.021557724793248804	2274	2.274	0.21164311959167115
X	0.020940731957053565	1743	1.743	0.22903305018114342
X	0.02136093792725114	1426	1.426	0.24650946406469879
X	0.02098007642094722	2680	2.68	0.1985595396455253
X	0.021363409254548958	5429	5.429	0.15787628782742008
X	0.02151531069815156	3618	3.618	0.1811728098003269
X	0.020843615158624266	880	0.88	0.2871861277991608
X	0.021693448804897	7978	7.978	0.13957624908393843
X	0.021559996740114985	2040	2.04	0.21945198212126216
X	0.021031602693306155	1874	1.874	0.22388977546696992
X	0.02111907218536182	697	0.697	0.3117554244723767
X	0.021296124693770353	1273	1.273	0.2557549565509408
X	0.021023483150871335	821	0.821	0.294750016454323
X	0.021483194865253585	2563	2.563	0.2031343742085303
X	0.020418767609279853	179	0.179	0.4849818640983243
X	0.020727507454640706	1443	1.443	0.24308616920795553
X	0.02115655375278655	1044	1.044	0.272634916691088
X	0.021221576846685696	524	0.524	0.34341199026272384
X	0.020302593941377584	725	0.725	0.30367183051812024
X	0.020729190901395446	665	0.665	0.31471702357389475
X	0.02103537106858813	1491	1.491	0.2416337040542659
X	0.02096928165714975	1627	1.627	0.23445814975716447
X	0.02085289541670102	643	0.643	0.318898001371808
X	0.02070858587551991	515	0.515	0.3425950402708674
X	0.020956322133372996	484	0.484	0.3511477591708989
X	0.020991957933179383	3914	3.914	0.17504271548099018
X	0.020012480908158665	473	0.473	0.3484549165628393
X	0.021631836833328283	3311	3.311	0.18694394616620763
X	0.020489425947542588	842	0.842	0.2897827746721763
X	0.020291040598837993	576	0.576	0.3278138352149369
X	0.021150727205971283	805	0.805	0.29728738064371557
X	0.020945353479589323	855	0.855	0.29042936576235834
X	0.021211218960748953	1579	1.579	0.23771750626372937
X	0.021150298190350993	781	0.781	0.3002998765132953
X	0.02054349159774583	766	0.766	0.29932879432801063
X	0.021566230966807237	9708	9.708	0.13048126589395503
X	0.021210678283831344	1621	1.621	0.23564443107522895
X	0.02061236122961434	1756	1.756	0.22726597010797586
X	0.020078897067497846	593	0.593	0.32351547041200474
X	0.020963902551612172	1935	1.935	0.2212740004646376
X	0.02036384509324658	838	0.838	0.2896489256858317
X	0.021054692917072196	2218	2.218	0.21173652925692094
X	0.020856860808331976	613	0.613	0.3240381590040198
X	0.020779207038733104	337	0.337	0.39506299922887866
X	0.02050006525722496	682	0.682	0.3109259402284748
X	0.021092501452501827	590	0.59	0.3294266212882082
X	0.021026523255405986	628	0.628	0.3223066563126235
X	0.02056086712906179	1037	1.037	0.2706580135440001
X	0.020341675117758078	538	0.538	0.33563569811257404
X	0.02065921723469002	2908	2.908	0.19223825696906244
X	0.02180345083922486	13631	13.631	0.11694971623792988
X	0.021654760679268973	5358	5.358	0.15928818352054439
X	0.020220032986194868	432	0.432	0.3603844450458723
X	0.021262383859441517	883	0.883	0.28876889895141594
X	0.02080239577938067	840	0.84	0.2914817419361441
X	0.020485095412331004	642	0.642	0.31717648922814257
X	0.020920657343003236	772	0.772	0.3003673170268577
X	0.02057476634906563	837	0.837	0.29076122148981226
X	0.022013137220216767	5913	5.913	0.15498560178251172
X	0.02144789919584353	2203	2.203	0.2135289449561804
X	0.021407501208287508	4537	4.537	0.1677254887120055
X	0.020853627137798227	1346	1.346	0.24929420398461846
X	0.021568187480619717	9881	9.881	0.1297191965886886
X	0.020368042458917	682	0.682	0.31025703585182773
X	0.021180903487577583	1223	1.223	0.25872556345550163
X	0.021534314222182403	2359	2.359	0.20899426475928365
X	0.02098999175383773	138	0.138	0.5337989647579899
X	0.02137137100398259	4093	4.093	0.17348564549941545
X	0.020075828583763382	643	0.643	0.31488659203008973
X	0.02127810668620631	2333	2.333	0.20893253858629868
X	0.0200364633196164	413	0.413	0.3647178379285966
X	0.020904271453069542	1040	1.04	0.27189458958185286
X	0.02091792120644008	587	0.587	0.3290739269678337
X	0.020752413733171948	864	0.864	0.28852600220532143
X	0.020533353778857323	703	0.703	0.30796511315371944
X	0.021040589861476895	1303	1.303	0.2527577602887033
X	0.01903049773004636	144	0.144	0.5093651388283877
X	0.02010369454606722	451	0.451	0.35456819652013666
X	0.020906912056527078	712	0.712	0.3085105118230812
X	0.021058464738448524	2418	2.418	0.20574224280472075
X	0.02067596448069211	654	0.654	0.3162005903032695
X	0.021436104047309983	2669	2.669	0.20026225134773748
X	0.021334285442095507	1282	1.282	0.25530737406552273
X	0.02029662680572176	949	0.949	0.27757853841450214
X	0.02130093392173592	2681	2.681	0.19954182673716134
X	0.020902171545130944	2690	2.69	0.1980674190919095
X	0.02025848095332479	631	0.631	0.31782825690523453
X	0.020978854179439725	1553	1.553	0.2381607025931889
X	0.02063590393490225	1655	1.655	0.2318863402944329
X	0.02158534344130515	1834	1.834	0.22746777379455896
X	0.021145644152588985	587	0.587	0.33026377401962914
X	0.021168987011266096	3412	3.412	0.18375127081069229
X	0.02133038499504284	2159	2.159	0.21457643089114786
X	0.02158052693700875	2065	2.065	0.21863213522809313
X	0.0203808937156902	2206	2.206	0.20983241161464208
X	0.02151666079758428	2324	2.324	0.20998079866532046
X	0.021350149071898104	8428	8.428	0.1363197543842927
X	0.0207001131149961	745	0.745	0.30288108238730665
X	0.01998403956274308	196	0.196	0.46717085016412646
X	0.02036877175206222	758	0.758	0.29952417363590367
X	0.020967658983412796	536	0.536	0.3394650374989088
X	0.020572214302075446	1463	1.463	0.24136756244256996
X	0.021087800307642315	1043	1.043	0.27242627405786074
X	0.02104335226032846	426	0.426	0.36691751374544274
X	0.020480285141933268	734	0.734	0.3033052862889263
X	0.02095160634990729	1669	1.669	0.2324093858334318
X	0.021568185420769527	311	0.311	0.4108510549764732
X	0.02169712820312745	2448	2.448	0.20694913423675448
X	0.019838015319677525	405	0.405	0.3658876347802779
X	0.02163826854879283	5512	5.512	0.15775062885362737
X	0.02193227810906876	3878	3.878	0.17816619831046213
X	0.021146128566818485	838	0.838	0.2933114047782079
X	0.02014855169103504	174	0.174	0.4874142198737715
X	0.02151715239678733	3124	3.124	0.19026456982233764
X	0.021440377718938777	2218	2.218	0.21302159400098591
X	0.021175657346966754	1149	1.149	0.26414290959272696
X	0.02160135921186532	7021	7.021	0.14544340565453748
X	0.02159215535024686	5912	5.912	0.1539999300621021
X	0.02019370218014589	526	0.526	0.3373469082603489
X	0.02043975834310895	320	0.32	0.39973783826187753
X	0.019123249483099083	309	0.309	0.39554971004539896
X	0.020515254058859046	343	0.343	0.3910757857106159
X	0.01929042437863068	417	0.417	0.3589786133661185
X	0.02083391231385031	303	0.303	0.40967808662503463
X	0.019859436522820805	312	0.312	0.3992737658772341
X	0.021130202981485675	1958	1.958	0.22098544005684564
X	0.020412577960064734	579	0.579	0.32789875238030697
X	0.020454902941858064	1318	1.318	0.24943688304769873
X	0.021442525570184702	1059	1.059	0.27255866424747227
X	0.020757222217368934	1525	1.525	0.23876269026655836
X	0.020361870826157494	594	0.594	0.32484566657331615
X	0.02119668351682404	4062	4.062	0.1734506768097787
X	0.020981231622153606	1760	1.76	0.22844032540177553
X	0.019279245528448	388	0.388	0.3676371885363261
X	0.020247909648406705	400	0.4	0.3699190816983691
X	0.02077770112030328	2478	2.478	0.20315712781685835
X	0.01963110192381338	163	0.163	0.493839348671215
X	0.020499622174918995	364	0.364	0.3833082420138426
X	0.020351698375948864	957	0.957	0.27705303151958377
X	0.021045712182817082	1503	1.503	0.24102839864865008
X	0.02170502085103957	3429	3.429	0.18498261196762877
X	0.021515687017337762	1150	1.15	0.26547224708425293
X	0.01962420749827649	409	0.409	0.3633760277308657
X	0.02121619113648411	1337	1.337	0.2512920045344707
X	0.021528896788574288	2726	2.726	0.1991431204959945
X	0.020351942991159545	464	0.464	0.3526652206753586
X	0.021426932862096806	5316	5.316	0.1591445164014212
X	0.02174257808284719	22052	22.052	0.09953007963042107
X	0.021752735320535392	1242	1.242	0.2596953020641179
X	0.02149866263515037	3301	3.301	0.18674771937543563
X	0.020125994528629895	154	0.154	0.5074721460247145
X	0.02095987953456956	2240	2.24	0.2107237993265448
X	0.021409150864746113	30314	30.314	0.08905371993005348
X	0.02161737674451093	80099	80.099	0.06462372173142523
X	0.02130427224221445	788	0.788	0.3001328244426849
X	0.021533246616714564	20610	20.61	0.10147144403707348
X	0.02123115005327441	16507	16.507	0.10875146668775927
X	0.02080292916094509	2242	2.242	0.21013398373380934
X	0.022032030717752962	65684	65.684	0.06948082694279148
X	0.020421335381112963	575	0.575	0.32870433453870773
X	0.021729516262690967	2928	2.928	0.19505615631478393
X	0.021401449145547643	38489	38.489	0.0822309898262486
X	0.021786521332913037	85102	85.102	0.06349643974578445
X	0.02020763153042553	1223	1.223	0.2547004148979937
X	0.021580236216413362	12233	12.233	0.12082989723945518
X	0.021068763389656554	6839	6.839	0.14550641725099894
X	0.0214703187788497	23978	23.978	0.09638479150506071
X	0.02134843396515525	26744	26.744	0.09276408951030657
X	0.021199686776341847	16758	16.758	0.10815229826397355
X	0.021819055010244857	11824	11.824	0.12265648233379826
X	0.02171073119728189	11450	11.45	0.12377216507959424
X	0.02081871294678334	398	0.398	0.37398736930780896
X	0.02183073725518704	87283	87.283	0.06300566202831356
X	0.020293048774962356	875	0.875	0.2851760715352091
X	0.02164585159200922	1092	1.092	0.2706350320816231
X	0.021467455321751674	7692	7.692	0.14079279504585898
X	0.02153021924257743	17861	17.861	0.10642595613869422
X	0.021772284283521588	99405	99.405	0.060278891265431324
X	0.021769090148934016	38866	38.866	0.08243091940510164
X	0.020009311021503325	350	0.35	0.3852311201506879
X	0.02134935336250346	3543	3.543	0.18197198159438763
X	0.021490554816166765	8282	8.282	0.13741605206093566
X	0.0217374769920139	12614	12.614	0.11989068352669824
X	0.021830575124637312	37601	37.601	0.08342353601779201
X	0.0208514463361517	8216	8.216	0.13640341748066798
X	0.021590681762680582	7652	7.652	0.14130703969658043
X	0.019560199108478588	204	0.204	0.4577001287736596
X	0.020904494192429877	1623	1.623	0.23440869369064002
X	0.021493924741693142	21759	21.759	0.09959226218378028
X	0.021524325900832465	20196	20.196	0.10214600812292987
X	0.021308887875844	14415	14.415	0.1139152967477931
X	0.021291113677026605	2951	2.951	0.19323081683163723
X	0.021783776915921284	161622	161.622	0.05127168236260246
X	0.021592011246178804	3330	3.33	0.18647314178670044
X	0.02176600265243029	13586	13.586	0.1170116281913432
X	0.021826927575789037	40726	40.726	0.08122822257428595
X	0.02119958150035494	1249	1.249	0.25699319070978616
X	0.021573088696037343	23626	23.626	0.09701540696214292
X	0.021708297237824938	101805	101.805	0.05974279688181835
X	0.02168331685750347	76283	76.283	0.06575053898772036
X	0.021300387735875396	4204	4.204	0.17175455645936255
X	0.021036398692123284	1645	1.645	0.23384880967748173
X	0.020055073801034804	1036	1.036	0.2685065202819204
X	0.02115271940873555	1412	1.412	0.2465152763180285
X	0.021741581232417848	14700	14.7	0.11393520303149016
X	0.021370851101055757	4327	4.327	0.17029882091290555
X	0.02159297908012209	13914	13.914	0.11577643919360503
X	0.02183799307777706	47017	47.017	0.07744367191062017
X	0.02182951968937536	7283	7.283	0.1441820811956465
X	0.021106264702359576	3597	3.597	0.1803669580822293
X	0.02166052444533031	2958	2.958	0.19418854124359225
X	0.02182875644576251	39597	39.597	0.08199528956100914
X	0.021818703356949125	9377	9.377	0.13251193849480855
X	0.021683018842086597	4279	4.279	0.17176149845104807
X	0.02182055109722108	17864	17.864	0.10689621673118128
X	0.021957638690271206	149111	149.111	0.05280705937222488
X	0.021355864370543164	3405	3.405	0.18441660072594496
X	0.02166773915577607	58541	58.541	0.07179890495895201
X	0.021227654483881628	1197	1.197	0.2607770010300943
X	0.02098185669829786	16983	16.983	0.10730249879118665
X	0.021759536675497272	5684	5.684	0.1564342549686292
X	0.021453304633351634	98748	98.748	0.06011579993439992
X	0.02166109410711271	19930	19.93	0.10281528810551206
X	0.021210580434605077	1296	1.296	0.25389209775517746
X	0.02168396533311805	51152	51.152	0.07512053895319358
X	0.02181511162787955	67305	67.305	0.06869155856578747
X	0.021763319650491768	1993	1.993	0.22185673332942282
X	0.021564857085983147	21542	21.542	0.10003535574937886
X	0.021824092207940897	94760	94.76	0.06129669308844657
X	0.022228570587101006	199083	199.083	0.04815327021620757
X	0.02178990411984858	6497	6.497	0.14968587400373926
X	0.02035988716650544	823	0.823	0.29137917581715866
X	0.021401605472194014	1780	1.78	0.2290913987211642
X	0.021523580953530765	2149	2.149	0.2155556301936378
X	0.021357280904293438	7203	7.203	0.1436627520547909
X	0.021394957209659143	4540	4.54	0.16765577715229257
X	0.020935097986379027	1462	1.462	0.24283384050752665
X	0.021811431017203124	36292	36.292	0.08439001924378256
X	0.021696294628804434	55176	55.176	0.0732619463320035
X	0.021002608625628298	543	0.543	0.3381876962705942
X	0.02142156167142048	2131	2.131	0.21581877656057147
X	0.02172486251849752	3759	3.759	0.17945748223223318
X	0.021760926442452606	7712	7.712	0.1413090966284936
X	0.021364026461894142	5018	5.018	0.16207554230031285
X	0.021636332703472084	7694	7.694	0.14114878897283806
X	0.020973063010157226	900	0.9	0.28563173028178473
X	0.020754381543104116	5147	5.147	0.159166576823756
X	0.021788602035257742	12871	12.871	0.11918063419765575
X	0.021701563045968986	7408	7.408	0.143085847568659
X	0.021443460692546067	22453	22.453	0.09847821466770561
X	0.020740992727915446	2952	2.952	0.19153040574492625
X	0.021750222167896276	169807	169.807	0.05040837561065287
X	0.02147505726959135	2925	2.925	0.19435817842366046
X	0.02175611543069023	24526	24.526	0.09608410374611827
X	0.021202573812505645	462	0.462	0.35802679935814874
X	0.021463507979831688	1529	1.529	0.24122986452138415
X	0.02145967237829083	2500	2.5	0.20475178326796367
X	0.021671618791521234	7613	7.613	0.1417245785921823
X	0.021283154123391892	1680	1.68	0.23311787168492565
X	0.021490879682068604	54862	54.862	0.07316906729083132
X	0.02100984815241986	1912	1.912	0.22231990176923236
X	0.020518616576149656	2295	2.295	0.2075495496632155
X	0.021798961231948164	133248	133.248	0.0546921546659211
X	0.02167727647462071	22209	22.209	0.09919548477596951
X	0.021553091902158483	28141	28.141	0.09149345296572683
X	0.021748087686840948	97933	97.933	0.06055695169650439
X	0.02183162253040021	116419	116.419	0.05723838652633733
X	0.021302569253705887	3483	3.483	0.18287717477708323
X	0.02111849343101152	1675	1.675	0.23274625793380668
X	0.02100780281174586	5250	5.25	0.15875976338381836
X	0.020205230904508306	209	0.209	0.4589575416108136
X	0.02041079846747367	1587	1.587	0.23429392212756955
X	0.02177787753239967	6042	6.042	0.15332451870932975
X	0.021191533760064076	682	0.682	0.3143832060822876
X	0.020898947784628294	1432	1.432	0.24437707798462618
X	0.02041964120705887	1009	1.009	0.27251200990299057
X	0.021416643415016336	1305	1.305	0.254124750175811
X	0.02177243847367002	53615	53.615	0.07405259021206347
X	0.02079892459494231	5568	5.568	0.1551602787928396
X	0.021597160294905115	36470	36.47	0.0839756993709983
X	0.02078145737005217	2469	2.469	0.2034159345240706
X	0.020474621200409656	1181	1.181	0.2588145330499823
X	0.021473450956879504	2481	2.481	0.20531705525699112
X	0.021856125515343706	350551	350.551	0.03965281178186445
X	0.02185823477913737	200901	200.901	0.04773947763380939
X	0.021800857527320524	4023	4.023	0.1756471799032296
X	0.021797986666049804	18785	18.785	0.10508362865887318
X	0.020940774633977532	775	0.775	0.3000753676064667
X	0.020861476164125602	1451	1.451	0.24316022758068204
X	0.02165753458616712	11679	11.679	0.12285734318948537
X	0.021708629169883868	10507	10.507	0.12736532923891858
X	0.02158134727882806	13849	13.849	0.11593646280483587
X	0.02171731426153955	25356	25.356	0.09496752598186872
X	0.021912460407782563	255321	255.321	0.04410977771546586
X	0.02085789792672936	5977	5.977	0.15168027032263026
X	0.020493591571290656	1800	1.8	0.22496512151187442
X	0.023037040191461442	11261	11.261	0.12694528467626232
X	0.021318199367178645	8867	8.867	0.13396497590577322
X	0.02098841527387514	912	0.912	0.28444279451420146
X	0.021647262079892787	49522	49.522	0.07589297889433197
X	0.02143662666089037	8129	8.129	0.13815706767401448
X	0.02032521085379247	240	0.24	0.4391449969397281
X	0.02162606497545313	18798	18.798	0.10478246703401427
X	0.021788845549808172	13793	13.793	0.11646403379639331
X	0.020476112586199816	347	0.347	0.3893193694775443
X	0.021545934472757693	39238	39.238	0.08188785413655869
X	0.02180582572104444	63367	63.367	0.07007607950468425
X	0.02176562075369844	66922	66.922	0.0687702677724127
X	0.021576410251578724	55990	55.99	0.07277064783632277
X	0.02156023384207514	78214	78.214	0.06508131556949635
X	0.02101067016142666	4679	4.679	0.1649791088019141
X	0.021116080476058605	5303	5.303	0.15850047222253413
X	0.021790636373858318	18005	18.005	0.10656770079669804
X	0.021675616379902136	43271	43.271	0.07941907809303914
X	0.02123641018459358	1272	1.272	0.25558262793558667
X	0.021862217085433178	22164	22.164	0.09954406086016447
X	0.021841702199694647	104991	104.991	0.059253153627776695
X	0.021854695834422094	64966	64.966	0.06954825101377443
X	0.02182356616486973	37588	37.588	0.08342422237654896
X	0.02137924095729134	44061	44.061	0.07858011212417534
X	0.02166216478124277	4167	4.167	0.1732312017562602
X	0.0202745683608055	704	0.704	0.3065205898496568
X	0.020735644295765455	1173	1.173	0.2604992680645318
X	0.02199446024354964	44955	44.955	0.07879737843259896
X	0.021657421796101504	27747	27.747	0.09207256460450496
X	0.02175299452849304	209283	209.283	0.04701773452328245
X	0.021732864187674464	58474	58.474	0.0718982060301284
X	0.021714968451812974	27921	27.921	0.0919622125095481
X	0.0214509953586711	9080	9.08	0.13318455586689784
X	0.021815972201450855	43968	43.968	0.07916732185955952
X	0.02033040669379268	679	0.679	0.3105218017280692
X	0.02048245845922661	3010	3.01	0.18949824653664987
X	0.021649495973508393	83124	83.124	0.06386169658655938
X	0.020992768529242152	1848	1.848	0.22479634728109016
X	0.020495040549218094	1604	1.604	0.2337840112650538
X	0.021722375285172287	11007	11.007	0.12543328088422245
X	0.0217739657613306	107024	107.024	0.05881464125610373
X	0.02159588810088705	5685	5.685	0.1560319489449917
X	0.022343322219659702	62992	62.992	0.07078709943213876
X	0.020713231467134767	824	0.824	0.29293654086250276
X	0.02090764869939266	1645	1.645	0.23337075468759885
X	0.02089697234770632	1066	1.066	0.2696344552695525
X	0.021711576793263108	2656	2.656	0.20144404405124772
X	0.02167320750397556	23780	23.78	0.09695505258731539
X	0.021032716586956456	9419	9.419	0.1307064013352255
X	0.020541806176504945	1598	1.598	0.23425414489445218
X	0.021765454381485708	45575	45.575	0.07816524381793986
X	0.02055641466795972	621	0.621	0.32108394467922485
X	0.021958076320292785	35685	35.685	0.08505559030934508
X	0.021140651241738664	2637	2.637	0.2001410054865992
X	0.02148364594639173	9841	9.841	0.1297247726493383
X	0.021155754819948122	2334	2.334	0.20850151957926502
X	0.02122883039273777	5278	5.278	0.15903238074377968
X	0.02130742710748922	3884	3.884	0.1763669739117003
X	0.02177187043542459	12317	12.317	0.12091038385068885
X	0.021793661449289695	49120	49.12	0.07627062068540696
X	0.021307885220742385	74024	74.024	0.0660271573104278
X	0.021511241324742462	2473	2.473	0.20565871537140612
X	0.020794158123920856	2901	2.901	0.19281073245464475
X	0.021757922293064575	58038	58.038	0.07210548921166433
X	0.020897796332231725	2871	2.871	0.19380088975191684
X	0.021798620061211228	296837	296.837	0.04187649480755925
X	0.01970769341363547	330	0.33	0.39087745252295353
X	0.021657097896744894	24600	24.6	0.09584181970156538
X	0.021760670568383468	47828	47.828	0.07691245687214236
X	0.021001730133391773	1097	1.097	0.2675158274888853
X	0.021422254922668468	47819	47.819	0.07651646499797247
X	0.021295757787744042	8969	8.969	0.1334083536970068
X	0.022092503513602424	437112	437.112	0.03697294696416713
X	0.021389142086658014	2664	2.664	0.20024101976183475
X	0.020791079729729052	2696	2.696	0.19756911388153872
X	0.020612579250457796	2776	2.776	0.19509117550709526
X	0.02113941179266851	1153	1.153	0.26368648064776173
X	0.021568026331508422	6685	6.685	0.14776434890314866
X	0.021796518693276687	9391	9.391	0.1324011522251439
X	0.020975251519200135	5240	5.24	0.15877859821177193
X	0.021545091869938804	16119	16.119	0.11015479797996025
X	0.021242867216432742	7639	7.639	0.1406238146838875
X	0.020490272223129154	681	0.681	0.31102851400265613
X	0.0215312200946598	5188	5.188	0.16070259546337812
X	0.022017146838392423	149943	149.943	0.052756780051111965
X	0.021888776158128242	29045	29.045	0.09100181056072915
X	0.02180581259984481	117003	117.003	0.05712046830311087
X	0.021712953338839668	8124	8.124	0.13877662714781658
X	0.02153900280966031	5189	5.189	0.16071163055779988
X	0.021090204730419615	970	0.97	0.2791063305928666
X	0.020435705704355475	2773	2.773	0.19460168216832757
X	0.021847488485424466	77916	77.916	0.06545230669627256
X	0.021777060875804888	7159	7.159	0.1448935214704416
X	0.02163321416308449	5493	5.493	0.15792000541849197
X	0.021430700222547096	6509	6.509	0.1487672596665706
X	0.021945504977024348	17599	17.599	0.10763475562150736
X	0.021286525094319695	1273	1.273	0.2557165221069915
X	0.021304815112281526	1717	1.717	0.23150968186016724
X	0.021801888360523693	214375	214.375	0.04667740008781984
X	0.02159383732585376	6200	6.2	0.15158144586561478
X	0.021002855681691527	7663	7.663	0.13994579089968542
X	0.021749468150261755	3863	3.863	0.17789946598289125
X	0.021729116174319243	38511	38.511	0.082632791052525
X	0.021751684038564242	114698	114.698	0.05745295184699098
X	0.021502502446944897	84895	84.895	0.0632706502283207
X	0.021831711704848246	48153	48.153	0.07682244869614444
X	0.021402624238245932	3831	3.831	0.17744003926774968
X	0.021796549356027203	21366	21.366	0.10066724278921726
X	0.02119938489530143	11268	11.268	0.12345029741038047
X	0.021730202672700056	2125	2.125	0.21705424347297775
X	0.02156603118926055	7087	7.087	0.14491141551895897
X	0.02165340580257256	1795	1.795	0.2293439320043228
X	0.02151214883038693	8239	8.239	0.13770078952135753
X	0.02086853179746047	3307	3.307	0.1847931634535547
X	0.020692151358948304	331	0.331	0.39688000324898776
X	0.021570579104324836	6871	6.871	0.14642456822759042
X	0.02178187627859347	2789	2.789	0.19840331498881641
X	0.021171167118436713	10529	10.529	0.12621739859138423
X	0.020661621898087658	2505	2.505	0.20204691631810404
X	0.021542897304262357	15665	15.665	0.11120506560466446
X	0.020234528284577755	6256	6.256	0.14788772046006485
X	0.021767441095507847	7050	7.05	0.14561499075243986
X	0.02111786644530598	1006	1.006	0.2758570067051396
X	0.021222749748145017	3429	3.429	0.18360227405725876
X	0.021757071276679665	9118	9.118	0.13362886578273556
X	0.021783819240216815	87699	87.699	0.06286078417427957
X	0.021271445367435124	3349	3.349	0.18519415874548945
X	0.021149878423568875	434	0.434	0.36526330850785654
X	0.02126535204352744	1526	1.526	0.240642694096547
X	0.021689130955381282	102555	102.555	0.059579259892067936
X	0.023059924679962897	150758	150.758	0.053480136332147155
X	0.02128068319748946	10434	10.434	0.1268172246867632
X	0.02117090932492421	3148	3.148	0.18875636102712667
X	0.021096280232480637	18158	18.158	0.10512660144357729
X	0.022872979070204854	56236	56.236	0.0740916790481363
X	0.02079053046607323	1061	1.061	0.2695980389552464
X	0.02170947855984811	4072	4.072	0.17469502650540447
X	0.021610957506401574	4105	4.105	0.17396168915375027
X	0.02140995146999739	20996	20.996	0.10065291869832915
X	0.021017427313719077	2987	2.987	0.19162320977135874
X	0.021815890042319014	116900	116.9	0.05714604010060227
X	0.02134402054239437	2278	2.278	0.2108179099854627
X	0.021231472537746588	2006	2.006	0.21955823414927764
X	0.020894576839102907	451	0.451	0.359158111589922
X	0.02122246271442288	7641	7.641	0.14056650922201697
X	0.021759039547259322	4005	4.005	0.17579738084862986
X	0.021638826145451987	8957	8.957	0.13418079926384793
X	0.02174170153097357	151500	151.5	0.05235525737294368
X	0.022084896237023355	361030	361.03	0.03940191749192647
X	0.021334323316787477	2465	2.465	0.20531498590609357
X	0.0205988100515424	1480	1.48	0.24054341506521149
X	0.02174583794561383	8261	8.261	0.13807482719570435
X	0.021856189507535463	98338	98.338	0.06057373576786115
X	0.021712798053656255	14870	14.87	0.11344924452277133
X	0.02103053285468779	375	0.375	0.3827715666042836
X	0.020490185939524654	1473	1.473	0.24049961429469363
X	0.02158624849058135	17064	17.064	0.10815139109155718
X	0.020598541840183546	581	0.581	0.3285136719238181
X	0.021746865399546345	39576	39.576	0.08190710756443421
X	0.02067848395888105	1067	1.067	0.26860747352278064
X	0.02182489336812639	9091	9.091	0.13389987068299072
X	0.021312375104177963	3315	3.315	0.18594425845007417
X	0.020710956246801127	1660	1.66	0.231933787998871
X	0.020245461462724136	779	0.779	0.29620797072184973
X	0.021818179595445963	80847	80.847	0.0646226655408022
X	0.02194598651888901	148182	148.182	0.052907822189917256
X	0.021794769580351515	69345	69.345	0.0679901046761677
X	0.02114007153004241	10222	10.222	0.1274060912362363
X	0.021779267247715704	14114	14.114	0.11555738807209265
X	0.02162707420978889	3720	3.72	0.17981183656369154
X	0.021512231831072617	3380	3.38	0.18532026948995242
X	0.021770536105249566	32707	32.707	0.0873124425325715
X	0.023037834839569372	17103	17.103	0.11043916682521764
X	0.02177166365173192	54709	54.709	0.07355478774347254
X	0.021612354896257274	6211	6.211	0.15153519787369574
X	0.022060364999720906	332963	332.963	0.04046431932892438
X	0.021275418750416403	4725	4.725	0.16512979229991326
X	0.02178707104129933	23698	23.698	0.09723644028976994
X	0.02188495525624752	69656	69.656	0.06798227758978513
X	0.021825658621310324	25031	25.031	0.09553511981487406
X	0.02164581505958087	20212	20.212	0.10231081667023555
X	0.022038571374610662	63602	63.602	0.07023779815359617
X	0.021564401438458627	14003	14.003	0.11547964489043469
X	0.021445060585829193	9245	9.245	0.13237524818120944
X	0.02100977874956348	2520	2.52	0.20277153059354985
X	0.021715226992784303	15156	15.156	0.11273530019475965
X	0.021684842907508165	6464	6.464	0.14969879718713391
X	0.021811616226225326	41818	41.818	0.08049610438085963
X	0.02208191361062866	466366	466.366	0.03617733808550187
X	0.022003961837594843	47219	47.219	0.07752849722416223
X	0.021544800249814543	85497	85.497	0.06316316233020453
X	0.02180370755347335	22025	22.025	0.09966396103807683
X	0.021765047552114142	10393	10.393	0.12793996601450186
X	0.021883388356141186	23361	23.361	0.09784554263033327
X	0.02102191912610875	3955	3.955	0.1745186953669351
X	0.021351701217408873	3172	3.172	0.1888135256049328
X	0.02183366981918312	89883	89.883	0.062394990992400194
X	0.021675903347146364	11245	11.245	0.1244531562922216
X	0.02157412769542234	77039	77.039	0.06542456886082235
X	0.021759839808695235	135638	135.638	0.05433647225234413
X	0.021733214317230982	8161	8.161	0.13860966959342008
X	0.02115583997593363	1132	1.132	0.26537577907883003
X	0.02166838874149532	26400	26.4	0.09362839180637537
X	0.021747818569651963	25127	25.127	0.09529973372070895
X	0.020745297403441488	1504	1.504	0.23982287757070303
X	0.022100267814916242	177739	177.739	0.04991199564454873
X	0.021821842924046268	15067	15.067	0.11314139930728546
X	0.021458021489391743	13079	13.079	0.11794287554659502
X	0.021689740200910412	2337	2.337	0.2101512024112144
X	0.020739502754918517	738	0.738	0.30402828704344664
X	0.021273844111036375	828	0.828	0.2950791645943132
X	0.02028844857968126	708	0.708	0.3060120490889592
X	0.021529035666668465	4179	4.179	0.1727099673341548
X	0.021394373705420848	6635	6.635	0.14773595938483844
X	0.022153689515978906	1886315	1886.315	0.022730585132021874
X	0.021175182972499023	3265	3.265	0.18648675644957743
X	0.021423710082802763	5891	5.891	0.1537807188264917
X	0.02085289273941188	2877	2.877	0.19352726059441264
X	0.021433144638903827	3329	3.329	0.1860333042343498
X	0.02149705720133544	6829	6.829	0.14655724746376922
X	0.021813457022247688	77678	77.678	0.06548504864592387
X	0.021391759715919938	2279	2.279	0.2109441061294309
X	0.02182564481542304	308894	308.894	0.04134146574017539
X	0.02166172684143957	1992	1.992	0.2215480407065158
X	0.02181001281601842	102483	102.483	0.059703716111078345
X	0.021804802397930437	12733	12.733	0.1196392840406808
X	0.021737653395166673	70891	70.891	0.06743321289032649
X	0.021697672345116437	183871	183.871	0.049049355237821175
X	0.021648621457530895	5130	5.13	0.16159861234270634
X	0.02144994941144495	1482	1.482	0.24370214750420677
X	0.021811953753794133	48190	48.19	0.07677960625876432
X	0.021205766921335217	3410	3.41	0.18389356613291666
X	0.021149865002767502	7294	7.294	0.1425981682796489
X	0.02176323142769863	10126	10.126	0.12905113220346484
X	0.021832107938824407	3076	3.076	0.19217775376410232
X	0.022650172396148326	23481	23.481	0.09880637687044963
X	0.021118508684527408	6637	6.637	0.1470834505038457
X	0.021796571299977502	80472	80.472	0.06470151687087944
X	0.02161404564069042	100890	100.89	0.059836012934796734
X	0.020997043995643512	4836	4.836	0.16313880947919385
X	0.021498327732317805	2905	2.905	0.19487352790311505
X	0.021652915008426022	44523	44.523	0.07864008673137764
X	0.021417311061276725	8866	8.866	0.1341773074081543
X	0.021556523543819702	15029	15.029	0.11277587692375864
X	0.020866153108909217	3579	3.579	0.1799810980675647
X	0.020812559568992733	24161	24.161	0.09514884690664546
X	0.021461760029448527	12415	12.415	0.12001610743763674
X	0.021305345854430332	27873	27.873	0.0914327168344973
X	0.02149624958607391	34078	34.078	0.08576225646860007
X	0.021300611200795103	3785	3.785	0.17787246642711085
X	0.020860124212877227	2002	2.002	0.2184159109911663
X	0.022228007824573685	7636	7.636	0.14278356037361808
X	0.021058743019342876	13037	13.037	0.11733248258085206
X	0.021305889265678807	24675	24.675	0.09522423697769661
X	0.021847329801164918	63313	63.313	0.07014044077735473
X	0.02120735967271791	5025	5.025	0.16160328275233563
X	0.021830124502378416	13737	13.737	0.11669567796168157
X	0.021470576522924507	14005	14.005	0.11530643128458601
X	0.021755740399938883	54554	54.554	0.07360643029808782
X	0.021725197607867786	19987	19.987	0.10281868401024188
X	0.021826643434063972	34737	34.737	0.08565082457868015
X	0.021805214593128855	9020	9.02	0.13420991743274463
X	0.02182936898900085	13420	13.42	0.11760602066616002
X	0.020275238474461846	916	0.916	0.28077396334813165
X	0.0214833466757793	4419	4.419	0.16940490402814007
X	0.021721965961672023	14682	14.682	0.11394745674260258
X	0.02172514656559999	9669	9.669	0.13097660175159428
X	0.02181821197709718	14492	14.492	0.11461204910116388
X	0.02083322307999249	1021	1.021	0.2732605306669358
X	0.021551737273067275	2283	2.283	0.2113450712767193
X	0.021300891411216494	12646	12.646	0.11898207364831322
X	0.021244652531922707	7240	7.24	0.14316505421076145
X	0.021672747249339597	7184	7.184	0.14449380052613567
X	0.02071948422641532	908	0.908	0.2836379670243613
X	0.020982038039357827	1618	1.618	0.2349396861517228
X	0.021656339608826427	8476	8.476	0.13670929229469367
X	0.02176743558723884	8681	8.681	0.13585610856594588
X	0.021617115350167755	29418	29.418	0.09023912290350984
X	0.021478463166235796	753	0.753	0.30554093905002455
X	0.02124399201903667	3697	3.697	0.17911373616476
X	0.020643654064429594	3229	3.229	0.18559780795823058
X	0.02099987209278291	3250	3.25	0.1862563535136017
X	0.021852066898875543	107979	107.979	0.05871076511262876
X	0.02108333522922638	9882	9.882	0.12873544983079546
X	0.02154924832121546	23707	23.707	0.09686908005334498
X	0.021807755949751303	62836	62.836	0.07027499376851816
X	0.021618224602498883	42979	42.979	0.07952821582530295
X	0.02211739055240949	102773	102.773	0.05992641317391944
X	0.021815036991566245	68360	68.36	0.06833627470415977
X	0.020967966934717298	1114	1.114	0.266005353675718
X	0.02174657881226737	10529	10.529	0.12735068123890964
X	0.02181581834357954	5425	5.425	0.15902201616769743
X	0.021589012854609624	21099	21.099	0.10076823171133224
X	0.021444720098198887	75230	75.23	0.06581270986577424
X	0.021278139444869622	4596	4.596	0.1666676109618174
X	0.02178650216741522	23009	23.009	0.09819663112824442
X	0.0213389172177776	2172	2.172	0.214176024284964
X	0.021223283883164416	6120	6.12	0.1513632301021102
X	0.021284657610593404	2215	2.215	0.21260051519747886
X	0.02146265313139801	3468	3.468	0.18359806765199493
X	0.02179324899708765	103549	103.549	0.059482884956348354
X	0.02206000207857291	353690	353.69	0.03965770604566909
X	0.021173184427773873	11957	11.957	0.12098215525947928
X	0.021795282723193903	7773	7.773	0.14101260850720315
X	0.021635232777421156	11167	11.167	0.12466418018066866
X	0.021336880314004086	18481	18.481	0.10490635993165921
X	0.02149003413956554	5062	5.062	0.16192167516945125
X	0.021343064397047865	2727	2.727	0.19854420346173074
X	0.021778604873470397	13988	13.988	0.11590214601588386
X	0.021784952407448932	59993	59.993	0.07134310583885746
X	0.021508485751552734	2018	2.018	0.22007103775960904
X	0.021927077277924076	36099	36.099	0.08468929649797033
X	0.021335788600341358	1019	1.019	0.2756204930012765
time for making epsilon is 1.1011967658996582
epsilons are
[0.4317211380197264, 0.15496325931250463, 0.25908493570366614, 0.1634154730924013, 0.11126114077911214, 0.1504312311105296, 0.0881973553852059, 0.11418598971243497, 0.20783400209922787, 0.1667293935914208, 0.2131276527608523, 0.07445132712064328, 0.17242370238137672, 0.05194369265138054, 0.12668636108378525, 0.13613529709573163, 0.0881663273836395, 0.10989588721400378, 0.061681750009964134, 0.06999798175023941, 0.19434906410597189, 0.05908597908313548, 0.1626928014223747, 0.10198138871328653, 0.2952837888164099, 0.07464211335024132, 0.08871010254336056, 0.19749543709139325, 0.10120902729002801, 0.033419411367390475, 0.19555823513129755, 0.03625861062347232, 0.14496631195597523, 0.20689157191935859, 0.21195948045940585, 0.07619175760031506, 0.06821028614662616, 0.2988652797687599, 0.1702161253740476, 0.2512211219667249, 0.23241673044712097, 0.3114707860804492, 0.42841690892973355, 0.2579106165159522, 0.41985873313725286, 0.3634984433362631, 0.23139653211613762, 0.26178469109610963, 0.27579128831685823, 0.14631934653838005, 0.1580719181328165, 0.21411311658204032, 0.19792036277251282, 0.2869849980969607, 0.18193027639961276, 0.17770177928356742, 0.3313051742738187, 0.2438403710829842, 0.28172396207609507, 0.24415021190752584, 0.14035856472565877, 0.33336877484438693, 0.13719559039368992, 0.22848461453405175, 0.3045145551164079, 0.27681762113037733, 0.3347943307355447, 0.2755800312958406, 0.26789800934365127, 0.2929416274069384, 0.25396997112255354, 0.24018915965035814, 0.31643021769625623, 0.26050195685610694, 0.237934102118621, 0.1658445840579851, 0.3221841256146791, 0.4941787413826715, 0.2876483823997319, 0.1970063880053682, 0.26120503387503125, 0.38790919354453735, 0.1754131101990034, 0.31861910952493006, 0.2605215790328567, 0.4003563592745345, 0.19304076839926554, 0.40444009683918136, 0.2354711654553044, 0.23907773578194594, 0.30050805837498906, 0.3852202509540907, 0.35678498710271705, 0.21164311959167115, 0.22903305018114342, 0.24650946406469879, 0.1985595396455253, 0.15787628782742008, 0.1811728098003269, 0.2871861277991608, 0.13957624908393843, 0.21945198212126216, 0.22388977546696992, 0.3117554244723767, 0.2557549565509408, 0.294750016454323, 0.2031343742085303, 0.4849818640983243, 0.24308616920795553, 0.272634916691088, 0.34341199026272384, 0.30367183051812024, 0.31471702357389475, 0.2416337040542659, 0.23445814975716447, 0.318898001371808, 0.3425950402708674, 0.3511477591708989, 0.17504271548099018, 0.3484549165628393, 0.18694394616620763, 0.2897827746721763, 0.3278138352149369, 0.29728738064371557, 0.29042936576235834, 0.23771750626372937, 0.3002998765132953, 0.29932879432801063, 0.13048126589395503, 0.23564443107522895, 0.22726597010797586, 0.32351547041200474, 0.2212740004646376, 0.2896489256858317, 0.21173652925692094, 0.3240381590040198, 0.39506299922887866, 0.3109259402284748, 0.3294266212882082, 0.3223066563126235, 0.2706580135440001, 0.33563569811257404, 0.19223825696906244, 0.11694971623792988, 0.15928818352054439, 0.3603844450458723, 0.28876889895141594, 0.2914817419361441, 0.31717648922814257, 0.3003673170268577, 0.29076122148981226, 0.15498560178251172, 0.2135289449561804, 0.1677254887120055, 0.24929420398461846, 0.1297191965886886, 0.31025703585182773, 0.25872556345550163, 0.20899426475928365, 0.5337989647579899, 0.17348564549941545, 0.31488659203008973, 0.20893253858629868, 0.3647178379285966, 0.27189458958185286, 0.3290739269678337, 0.28852600220532143, 0.30796511315371944, 0.2527577602887033, 0.5093651388283877, 0.35456819652013666, 0.3085105118230812, 0.20574224280472075, 0.3162005903032695, 0.20026225134773748, 0.25530737406552273, 0.27757853841450214, 0.19954182673716134, 0.1980674190919095, 0.31782825690523453, 0.2381607025931889, 0.2318863402944329, 0.22746777379455896, 0.33026377401962914, 0.18375127081069229, 0.21457643089114786, 0.21863213522809313, 0.20983241161464208, 0.20998079866532046, 0.1363197543842927, 0.30288108238730665, 0.46717085016412646, 0.29952417363590367, 0.3394650374989088, 0.24136756244256996, 0.27242627405786074, 0.36691751374544274, 0.3033052862889263, 0.2324093858334318, 0.4108510549764732, 0.20694913423675448, 0.3658876347802779, 0.15775062885362737, 0.17816619831046213, 0.2933114047782079, 0.4874142198737715, 0.19026456982233764, 0.21302159400098591, 0.26414290959272696, 0.14544340565453748, 0.1539999300621021, 0.3373469082603489, 0.39973783826187753, 0.39554971004539896, 0.3910757857106159, 0.3589786133661185, 0.40967808662503463, 0.3992737658772341, 0.22098544005684564, 0.32789875238030697, 0.24943688304769873, 0.27255866424747227, 0.23876269026655836, 0.32484566657331615, 0.1734506768097787, 0.22844032540177553, 0.3676371885363261, 0.3699190816983691, 0.20315712781685835, 0.493839348671215, 0.3833082420138426, 0.27705303151958377, 0.24102839864865008, 0.18498261196762877, 0.26547224708425293, 0.3633760277308657, 0.2512920045344707, 0.1991431204959945, 0.3526652206753586, 0.1591445164014212, 0.09953007963042107, 0.2596953020641179, 0.18674771937543563, 0.5074721460247145, 0.2107237993265448, 0.08905371993005348, 0.06462372173142523, 0.3001328244426849, 0.10147144403707348, 0.10875146668775927, 0.21013398373380934, 0.06948082694279148, 0.32870433453870773, 0.19505615631478393, 0.0822309898262486, 0.06349643974578445, 0.2547004148979937, 0.12082989723945518, 0.14550641725099894, 0.09638479150506071, 0.09276408951030657, 0.10815229826397355, 0.12265648233379826, 0.12377216507959424, 0.37398736930780896, 0.06300566202831356, 0.2851760715352091, 0.2706350320816231, 0.14079279504585898, 0.10642595613869422, 0.060278891265431324, 0.08243091940510164, 0.3852311201506879, 0.18197198159438763, 0.13741605206093566, 0.11989068352669824, 0.08342353601779201, 0.13640341748066798, 0.14130703969658043, 0.4577001287736596, 0.23440869369064002, 0.09959226218378028, 0.10214600812292987, 0.1139152967477931, 0.19323081683163723, 0.05127168236260246, 0.18647314178670044, 0.1170116281913432, 0.08122822257428595, 0.25699319070978616, 0.09701540696214292, 0.05974279688181835, 0.06575053898772036, 0.17175455645936255, 0.23384880967748173, 0.2685065202819204, 0.2465152763180285, 0.11393520303149016, 0.17029882091290555, 0.11577643919360503, 0.07744367191062017, 0.1441820811956465, 0.1803669580822293, 0.19418854124359225, 0.08199528956100914, 0.13251193849480855, 0.17176149845104807, 0.10689621673118128, 0.05280705937222488, 0.18441660072594496, 0.07179890495895201, 0.2607770010300943, 0.10730249879118665, 0.1564342549686292, 0.06011579993439992, 0.10281528810551206, 0.25389209775517746, 0.07512053895319358, 0.06869155856578747, 0.22185673332942282, 0.10003535574937886, 0.06129669308844657, 0.04815327021620757, 0.14968587400373926, 0.29137917581715866, 0.2290913987211642, 0.2155556301936378, 0.1436627520547909, 0.16765577715229257, 0.24283384050752665, 0.08439001924378256, 0.0732619463320035, 0.3381876962705942, 0.21581877656057147, 0.17945748223223318, 0.1413090966284936, 0.16207554230031285, 0.14114878897283806, 0.28563173028178473, 0.159166576823756, 0.11918063419765575, 0.143085847568659, 0.09847821466770561, 0.19153040574492625, 0.05040837561065287, 0.19435817842366046, 0.09608410374611827, 0.35802679935814874, 0.24122986452138415, 0.20475178326796367, 0.1417245785921823, 0.23311787168492565, 0.07316906729083132, 0.22231990176923236, 0.2075495496632155, 0.0546921546659211, 0.09919548477596951, 0.09149345296572683, 0.06055695169650439, 0.05723838652633733, 0.18287717477708323, 0.23274625793380668, 0.15875976338381836, 0.4589575416108136, 0.23429392212756955, 0.15332451870932975, 0.3143832060822876, 0.24437707798462618, 0.27251200990299057, 0.254124750175811, 0.07405259021206347, 0.1551602787928396, 0.0839756993709983, 0.2034159345240706, 0.2588145330499823, 0.20531705525699112, 0.03965281178186445, 0.04773947763380939, 0.1756471799032296, 0.10508362865887318, 0.3000753676064667, 0.24316022758068204, 0.12285734318948537, 0.12736532923891858, 0.11593646280483587, 0.09496752598186872, 0.04410977771546586, 0.15168027032263026, 0.22496512151187442, 0.12694528467626232, 0.13396497590577322, 0.28444279451420146, 0.07589297889433197, 0.13815706767401448, 0.4391449969397281, 0.10478246703401427, 0.11646403379639331, 0.3893193694775443, 0.08188785413655869, 0.07007607950468425, 0.0687702677724127, 0.07277064783632277, 0.06508131556949635, 0.1649791088019141, 0.15850047222253413, 0.10656770079669804, 0.07941907809303914, 0.25558262793558667, 0.09954406086016447, 0.059253153627776695, 0.06954825101377443, 0.08342422237654896, 0.07858011212417534, 0.1732312017562602, 0.3065205898496568, 0.2604992680645318, 0.07879737843259896, 0.09207256460450496, 0.04701773452328245, 0.0718982060301284, 0.0919622125095481, 0.13318455586689784, 0.07916732185955952, 0.3105218017280692, 0.18949824653664987, 0.06386169658655938, 0.22479634728109016, 0.2337840112650538, 0.12543328088422245, 0.05881464125610373, 0.1560319489449917, 0.07078709943213876, 0.29293654086250276, 0.23337075468759885, 0.2696344552695525, 0.20144404405124772, 0.09695505258731539, 0.1307064013352255, 0.23425414489445218, 0.07816524381793986, 0.32108394467922485, 0.08505559030934508, 0.2001410054865992, 0.1297247726493383, 0.20850151957926502, 0.15903238074377968, 0.1763669739117003, 0.12091038385068885, 0.07627062068540696, 0.0660271573104278, 0.20565871537140612, 0.19281073245464475, 0.07210548921166433, 0.19380088975191684, 0.04187649480755925, 0.39087745252295353, 0.09584181970156538, 0.07691245687214236, 0.2675158274888853, 0.07651646499797247, 0.1334083536970068, 0.03697294696416713, 0.20024101976183475, 0.19756911388153872, 0.19509117550709526, 0.26368648064776173, 0.14776434890314866, 0.1324011522251439, 0.15877859821177193, 0.11015479797996025, 0.1406238146838875, 0.31102851400265613, 0.16070259546337812, 0.052756780051111965, 0.09100181056072915, 0.05712046830311087, 0.13877662714781658, 0.16071163055779988, 0.2791063305928666, 0.19460168216832757, 0.06545230669627256, 0.1448935214704416, 0.15792000541849197, 0.1487672596665706, 0.10763475562150736, 0.2557165221069915, 0.23150968186016724, 0.04667740008781984, 0.15158144586561478, 0.13994579089968542, 0.17789946598289125, 0.082632791052525, 0.05745295184699098, 0.0632706502283207, 0.07682244869614444, 0.17744003926774968, 0.10066724278921726, 0.12345029741038047, 0.21705424347297775, 0.14491141551895897, 0.2293439320043228, 0.13770078952135753, 0.1847931634535547, 0.39688000324898776, 0.14642456822759042, 0.19840331498881641, 0.12621739859138423, 0.20204691631810404, 0.11120506560466446, 0.14788772046006485, 0.14561499075243986, 0.2758570067051396, 0.18360227405725876, 0.13362886578273556, 0.06286078417427957, 0.18519415874548945, 0.36526330850785654, 0.240642694096547, 0.059579259892067936, 0.053480136332147155, 0.1268172246867632, 0.18875636102712667, 0.10512660144357729, 0.0740916790481363, 0.2695980389552464, 0.17469502650540447, 0.17396168915375027, 0.10065291869832915, 0.19162320977135874, 0.05714604010060227, 0.2108179099854627, 0.21955823414927764, 0.359158111589922, 0.14056650922201697, 0.17579738084862986, 0.13418079926384793, 0.05235525737294368, 0.03940191749192647, 0.20531498590609357, 0.24054341506521149, 0.13807482719570435, 0.06057373576786115, 0.11344924452277133, 0.3827715666042836, 0.24049961429469363, 0.10815139109155718, 0.3285136719238181, 0.08190710756443421, 0.26860747352278064, 0.13389987068299072, 0.18594425845007417, 0.231933787998871, 0.29620797072184973, 0.0646226655408022, 0.052907822189917256, 0.0679901046761677, 0.1274060912362363, 0.11555738807209265, 0.17981183656369154, 0.18532026948995242, 0.0873124425325715, 0.11043916682521764, 0.07355478774347254, 0.15153519787369574, 0.04046431932892438, 0.16512979229991326, 0.09723644028976994, 0.06798227758978513, 0.09553511981487406, 0.10231081667023555, 0.07023779815359617, 0.11547964489043469, 0.13237524818120944, 0.20277153059354985, 0.11273530019475965, 0.14969879718713391, 0.08049610438085963, 0.03617733808550187, 0.07752849722416223, 0.06316316233020453, 0.09966396103807683, 0.12793996601450186, 0.09784554263033327, 0.1745186953669351, 0.1888135256049328, 0.062394990992400194, 0.1244531562922216, 0.06542456886082235, 0.05433647225234413, 0.13860966959342008, 0.26537577907883003, 0.09362839180637537, 0.09529973372070895, 0.23982287757070303, 0.04991199564454873, 0.11314139930728546, 0.11794287554659502, 0.2101512024112144, 0.30402828704344664, 0.2950791645943132, 0.3060120490889592, 0.1727099673341548, 0.14773595938483844, 0.022730585132021874, 0.18648675644957743, 0.1537807188264917, 0.19352726059441264, 0.1860333042343498, 0.14655724746376922, 0.06548504864592387, 0.2109441061294309, 0.04134146574017539, 0.2215480407065158, 0.059703716111078345, 0.1196392840406808, 0.06743321289032649, 0.049049355237821175, 0.16159861234270634, 0.24370214750420677, 0.07677960625876432, 0.18389356613291666, 0.1425981682796489, 0.12905113220346484, 0.19217775376410232, 0.09880637687044963, 0.1470834505038457, 0.06470151687087944, 0.059836012934796734, 0.16313880947919385, 0.19487352790311505, 0.07864008673137764, 0.1341773074081543, 0.11277587692375864, 0.1799810980675647, 0.09514884690664546, 0.12001610743763674, 0.0914327168344973, 0.08576225646860007, 0.17787246642711085, 0.2184159109911663, 0.14278356037361808, 0.11733248258085206, 0.09522423697769661, 0.07014044077735473, 0.16160328275233563, 0.11669567796168157, 0.11530643128458601, 0.07360643029808782, 0.10281868401024188, 0.08565082457868015, 0.13420991743274463, 0.11760602066616002, 0.28077396334813165, 0.16940490402814007, 0.11394745674260258, 0.13097660175159428, 0.11461204910116388, 0.2732605306669358, 0.2113450712767193, 0.11898207364831322, 0.14316505421076145, 0.14449380052613567, 0.2836379670243613, 0.2349396861517228, 0.13670929229469367, 0.13585610856594588, 0.09023912290350984, 0.30554093905002455, 0.17911373616476, 0.18559780795823058, 0.1862563535136017, 0.05871076511262876, 0.12873544983079546, 0.09686908005334498, 0.07027499376851816, 0.07952821582530295, 0.05992641317391944, 0.06833627470415977, 0.266005353675718, 0.12735068123890964, 0.15902201616769743, 0.10076823171133224, 0.06581270986577424, 0.1666676109618174, 0.09819663112824442, 0.214176024284964, 0.1513632301021102, 0.21260051519747886, 0.18359806765199493, 0.059482884956348354, 0.03965770604566909, 0.12098215525947928, 0.14101260850720315, 0.12466418018066866, 0.10490635993165921, 0.16192167516945125, 0.19854420346173074, 0.11590214601588386, 0.07134310583885746, 0.22007103775960904, 0.08468929649797033, 0.2756204930012765]
0.09853184744455051
Making ranges
torch.Size([34332, 2])
We keep 7.52e+06/5.84e+08 =  1% of the original kernel matrix.

torch.Size([799, 2])
We keep 8.21e+03/6.15e+04 = 13% of the original kernel matrix.

torch.Size([7539, 2])
We keep 3.54e+05/5.99e+06 =  5% of the original kernel matrix.

torch.Size([9922, 2])
We keep 1.33e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([19171, 2])
We keep 2.81e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([2923, 2])
We keep 1.02e+05/1.50e+06 =  6% of the original kernel matrix.

torch.Size([11851, 2])
We keep 9.60e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([5979, 2])
We keep 1.79e+06/2.40e+07 =  7% of the original kernel matrix.

torch.Size([14212, 2])
We keep 2.45e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([24416, 2])
We keep 5.11e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([30551, 2])
We keep 6.03e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([11499, 2])
We keep 1.38e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([20611, 2])
We keep 3.05e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([44085, 2])
We keep 1.58e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([41312, 2])
We keep 1.06e+07/7.68e+08 =  1% of the original kernel matrix.

torch.Size([23784, 2])
We keep 4.22e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([30032, 2])
We keep 5.66e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([4712, 2])
We keep 3.48e+05/5.60e+06 =  6% of the original kernel matrix.

torch.Size([14012, 2])
We keep 1.45e+06/5.72e+07 =  2% of the original kernel matrix.

torch.Size([5422, 2])
We keep 1.56e+06/2.20e+07 =  7% of the original kernel matrix.

torch.Size([14274, 2])
We keep 2.36e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([4123, 2])
We keep 2.53e+05/4.55e+06 =  5% of the original kernel matrix.

torch.Size([13237, 2])
We keep 1.34e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([64696, 2])
We keep 6.56e+07/2.72e+09 =  2% of the original kernel matrix.

torch.Size([48360, 2])
We keep 1.60e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([8394, 2])
We keep 6.96e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([17900, 2])
We keep 2.20e+06/9.93e+07 =  2% of the original kernel matrix.

torch.Size([214178, 2])
We keep 2.62e+08/2.39e+10 =  1% of the original kernel matrix.

torch.Size([90984, 2])
We keep 4.01e+07/3.74e+09 =  1% of the original kernel matrix.

torch.Size([18060, 2])
We keep 2.61e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([25738, 2])
We keep 4.38e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([14653, 2])
We keep 2.25e+06/7.31e+07 =  3% of the original kernel matrix.

torch.Size([23119, 2])
We keep 3.73e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([45532, 2])
We keep 1.62e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([41988, 2])
We keep 1.05e+07/7.70e+08 =  1% of the original kernel matrix.

torch.Size([23165, 2])
We keep 5.37e+06/2.66e+08 =  2% of the original kernel matrix.

torch.Size([29403, 2])
We keep 6.12e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([126005, 2])
We keep 1.32e+08/8.63e+09 =  1% of the original kernel matrix.

torch.Size([68106, 2])
We keep 2.57e+07/2.25e+09 =  1% of the original kernel matrix.

torch.Size([92978, 2])
We keep 7.76e+07/3.97e+09 =  1% of the original kernel matrix.

torch.Size([58437, 2])
We keep 1.83e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([4434, 2])
We keep 4.55e+05/8.32e+06 =  5% of the original kernel matrix.

torch.Size([13187, 2])
We keep 1.72e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([153916, 2])
We keep 1.57e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([76349, 2])
We keep 2.83e+07/2.58e+09 =  1% of the original kernel matrix.

torch.Size([9639, 2])
We keep 9.38e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([18952, 2])
We keep 2.46e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([27978, 2])
We keep 1.90e+07/4.20e+08 =  4% of the original kernel matrix.

torch.Size([32505, 2])
We keep 7.44e+06/4.95e+08 =  1% of the original kernel matrix.

torch.Size([1872, 2])
We keep 6.30e+04/6.24e+05 = 10% of the original kernel matrix.

torch.Size([9642, 2])
We keep 6.88e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([75579, 2])
We keep 3.23e+07/2.75e+09 =  1% of the original kernel matrix.

torch.Size([53247, 2])
We keep 1.57e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([38179, 2])
We keep 1.85e+07/9.86e+08 =  1% of the original kernel matrix.

torch.Size([37400, 2])
We keep 1.05e+07/7.59e+08 =  1% of the original kernel matrix.

torch.Size([5952, 2])
We keep 3.40e+05/7.84e+06 =  4% of the original kernel matrix.

torch.Size([15476, 2])
We keep 1.64e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([27514, 2])
We keep 2.13e+07/4.41e+08 =  4% of the original kernel matrix.

torch.Size([31830, 2])
We keep 7.22e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([882437, 2])
We keep 2.02e+09/3.42e+11 =  0% of the original kernel matrix.

torch.Size([194516, 2])
We keep 1.30e+08/1.41e+10 =  0% of the original kernel matrix.

torch.Size([5178, 2])
We keep 4.38e+05/8.44e+06 =  5% of the original kernel matrix.

torch.Size([14548, 2])
We keep 1.70e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([583220, 2])
We keep 2.39e+09/2.15e+11 =  1% of the original kernel matrix.

torch.Size([152375, 2])
We keep 1.07e+08/1.12e+10 =  0% of the original kernel matrix.

torch.Size([9917, 2])
We keep 2.70e+06/4.68e+07 =  5% of the original kernel matrix.

torch.Size([18716, 2])
We keep 3.09e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([5318, 2])
We keep 3.05e+05/5.93e+06 =  5% of the original kernel matrix.

torch.Size([14844, 2])
We keep 1.48e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([4238, 2])
We keep 2.81e+05/4.92e+06 =  5% of the original kernel matrix.

torch.Size([13277, 2])
We keep 1.42e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([61060, 2])
We keep 5.60e+07/2.41e+09 =  2% of the original kernel matrix.

torch.Size([46704, 2])
We keep 1.51e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([100200, 2])
We keep 8.49e+07/4.79e+09 =  1% of the original kernel matrix.

torch.Size([60826, 2])
We keep 2.02e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([1913, 2])
We keep 4.59e+04/6.26e+05 =  7% of the original kernel matrix.

torch.Size([10055, 2])
We keep 7.27e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([8382, 2])
We keep 7.63e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([17752, 2])
We keep 2.30e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([2578, 2])
We keep 1.13e+05/1.81e+06 =  6% of the original kernel matrix.

torch.Size([10900, 2])
We keep 1.02e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([3858, 2])
We keep 1.52e+05/2.96e+06 =  5% of the original kernel matrix.

torch.Size([13180, 2])
We keep 1.19e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([1368, 2])
We keep 5.26e+04/4.65e+05 = 11% of the original kernel matrix.

torch.Size([8369, 2])
We keep 6.43e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([664, 2])
We keep 1.09e+04/6.05e+04 = 17% of the original kernel matrix.

torch.Size([6642, 2])
We keep 3.29e+05/5.95e+06 =  5% of the original kernel matrix.

torch.Size([2510, 2])
We keep 1.03e+05/1.44e+06 =  7% of the original kernel matrix.

torch.Size([10900, 2])
We keep 9.45e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([822, 2])
We keep 9.70e+03/7.78e+04 = 12% of the original kernel matrix.

torch.Size([7508, 2])
We keep 3.71e+05/6.74e+06 =  5% of the original kernel matrix.

torch.Size([1233, 2])
We keep 1.99e+04/1.82e+05 = 10% of the original kernel matrix.

torch.Size([8598, 2])
We keep 4.92e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([3594, 2])
We keep 1.80e+05/3.00e+06 =  5% of the original kernel matrix.

torch.Size([12597, 2])
We keep 1.18e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([2752, 2])
We keep 9.35e+04/1.40e+06 =  6% of the original kernel matrix.

torch.Size([11460, 2])
We keep 9.37e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([2300, 2])
We keep 7.13e+04/1.01e+06 =  7% of the original kernel matrix.

torch.Size([10751, 2])
We keep 8.35e+05/2.42e+07 =  3% of the original kernel matrix.

torch.Size([10729, 2])
We keep 1.64e+06/4.62e+07 =  3% of the original kernel matrix.

torch.Size([19822, 2])
We keep 3.18e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([9415, 2])
We keep 1.10e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([18694, 2])
We keep 2.69e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([4470, 2])
We keep 2.30e+05/4.77e+06 =  4% of the original kernel matrix.

torch.Size([13701, 2])
We keep 1.41e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([5263, 2])
We keep 3.66e+05/7.65e+06 =  4% of the original kernel matrix.

torch.Size([14641, 2])
We keep 1.66e+06/6.68e+07 =  2% of the original kernel matrix.

torch.Size([2089, 2])
We keep 5.91e+04/7.67e+05 =  7% of the original kernel matrix.

torch.Size([10243, 2])
We keep 7.71e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([7297, 2])
We keep 4.87e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([16834, 2])
We keep 1.95e+06/8.55e+07 =  2% of the original kernel matrix.

torch.Size([5629, 2])
We keep 7.33e+05/1.44e+07 =  5% of the original kernel matrix.

torch.Size([14369, 2])
We keep 2.08e+06/9.17e+07 =  2% of the original kernel matrix.

torch.Size([1528, 2])
We keep 2.69e+04/3.24e+05 =  8% of the original kernel matrix.

torch.Size([9343, 2])
We keep 5.66e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([3351, 2])
We keep 1.21e+05/2.20e+06 =  5% of the original kernel matrix.

torch.Size([12525, 2])
We keep 1.09e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([2052, 2])
We keep 6.26e+04/8.78e+05 =  7% of the original kernel matrix.

torch.Size([10128, 2])
We keep 7.93e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([3062, 2])
We keep 1.23e+05/1.98e+06 =  6% of the original kernel matrix.

torch.Size([11829, 2])
We keep 1.04e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([14020, 2])
We keep 1.77e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([22620, 2])
We keep 3.49e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([1350, 2])
We keep 2.84e+04/3.16e+05 =  8% of the original kernel matrix.

torch.Size([8759, 2])
We keep 5.73e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([14515, 2])
We keep 2.04e+06/6.94e+07 =  2% of the original kernel matrix.

torch.Size([22995, 2])
We keep 3.68e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([3919, 2])
We keep 1.63e+05/3.13e+06 =  5% of the original kernel matrix.

torch.Size([13165, 2])
We keep 1.21e+06/4.27e+07 =  2% of the original kernel matrix.

torch.Size([1824, 2])
We keep 4.24e+04/5.61e+05 =  7% of the original kernel matrix.

torch.Size([9951, 2])
We keep 6.95e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([2255, 2])
We keep 6.27e+04/9.72e+05 =  6% of the original kernel matrix.

torch.Size([10621, 2])
We keep 8.19e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([1311, 2])
We keep 3.01e+04/3.21e+05 =  9% of the original kernel matrix.

torch.Size([8549, 2])
We keep 5.85e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([2466, 2])
We keep 6.91e+04/9.96e+05 =  6% of the original kernel matrix.

torch.Size([11113, 2])
We keep 8.39e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([2520, 2])
We keep 7.74e+04/1.16e+06 =  6% of the original kernel matrix.

torch.Size([11143, 2])
We keep 8.69e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([2033, 2])
We keep 5.45e+04/7.07e+05 =  7% of the original kernel matrix.

torch.Size([10268, 2])
We keep 7.49e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([2507, 2])
We keep 1.75e+05/1.38e+06 = 12% of the original kernel matrix.

torch.Size([10571, 2])
We keep 9.38e+05/2.84e+07 =  3% of the original kernel matrix.

torch.Size([3251, 2])
We keep 1.62e+05/2.24e+06 =  7% of the original kernel matrix.

torch.Size([12048, 2])
We keep 1.09e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([1683, 2])
We keep 3.39e+04/4.15e+05 =  8% of the original kernel matrix.

torch.Size([9665, 2])
We keep 6.25e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([2530, 2])
We keep 8.64e+04/1.32e+06 =  6% of the original kernel matrix.

torch.Size([10939, 2])
We keep 9.11e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([3175, 2])
We keep 1.41e+05/2.50e+06 =  5% of the original kernel matrix.

torch.Size([11939, 2])
We keep 1.13e+06/3.82e+07 =  2% of the original kernel matrix.

torch.Size([8409, 2])
We keep 9.32e+05/2.26e+07 =  4% of the original kernel matrix.

torch.Size([17796, 2])
We keep 2.43e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([1661, 2])
We keep 4.30e+04/4.07e+05 = 10% of the original kernel matrix.

torch.Size([9573, 2])
We keep 6.24e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([550, 2])
We keep 4.26e+03/2.66e+04 = 16% of the original kernel matrix.

torch.Size([6585, 2])
We keep 2.73e+05/3.94e+06 =  6% of the original kernel matrix.

torch.Size([2043, 2])
We keep 5.28e+04/7.71e+05 =  6% of the original kernel matrix.

torch.Size([10143, 2])
We keep 7.46e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([5980, 2])
We keep 3.53e+05/7.63e+06 =  4% of the original kernel matrix.

torch.Size([15546, 2])
We keep 1.66e+06/6.68e+07 =  2% of the original kernel matrix.

torch.Size([2736, 2])
We keep 1.02e+05/1.43e+06 =  7% of the original kernel matrix.

torch.Size([11464, 2])
We keep 9.39e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([766, 2])
We keep 1.45e+04/1.14e+05 = 12% of the original kernel matrix.

torch.Size([6941, 2])
We keep 4.13e+05/8.15e+06 =  5% of the original kernel matrix.

torch.Size([7880, 2])
We keep 6.15e+05/1.60e+07 =  3% of the original kernel matrix.

torch.Size([17494, 2])
We keep 2.15e+06/9.67e+07 =  2% of the original kernel matrix.

torch.Size([1466, 2])
We keep 3.34e+04/3.79e+05 =  8% of the original kernel matrix.

torch.Size([8979, 2])
We keep 6.09e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([2765, 2])
We keep 1.02e+05/1.43e+06 =  7% of the original kernel matrix.

torch.Size([11464, 2])
We keep 9.22e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([946, 2])
We keep 1.20e+04/1.01e+05 = 11% of the original kernel matrix.

torch.Size([7979, 2])
We keep 4.12e+05/7.69e+06 =  5% of the original kernel matrix.

torch.Size([5176, 2])
We keep 4.47e+05/8.82e+06 =  5% of the original kernel matrix.

torch.Size([14281, 2])
We keep 1.77e+06/7.18e+07 =  2% of the original kernel matrix.

torch.Size([894, 2])
We keep 1.12e+04/9.67e+04 = 11% of the original kernel matrix.

torch.Size([7784, 2])
We keep 3.98e+05/7.52e+06 =  5% of the original kernel matrix.

torch.Size([3725, 2])
We keep 1.44e+05/2.63e+06 =  5% of the original kernel matrix.

torch.Size([12937, 2])
We keep 1.15e+06/3.92e+07 =  2% of the original kernel matrix.

torch.Size([3286, 2])
We keep 1.43e+05/2.45e+06 =  5% of the original kernel matrix.

torch.Size([12165, 2])
We keep 1.12e+06/3.78e+07 =  2% of the original kernel matrix.

torch.Size([2057, 2])
We keep 4.38e+04/5.84e+05 =  7% of the original kernel matrix.

torch.Size([10480, 2])
We keep 7.02e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([1034, 2])
We keep 1.32e+04/1.23e+05 = 10% of the original kernel matrix.

torch.Size([8133, 2])
We keep 4.22e+05/8.48e+06 =  4% of the original kernel matrix.

torch.Size([1186, 2])
We keep 1.93e+04/1.98e+05 =  9% of the original kernel matrix.

torch.Size([8502, 2])
We keep 4.99e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([4729, 2])
We keep 2.60e+05/5.17e+06 =  5% of the original kernel matrix.

torch.Size([14007, 2])
We keep 1.45e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([3533, 2])
We keep 1.63e+05/3.04e+06 =  5% of the original kernel matrix.

torch.Size([12454, 2])
We keep 1.21e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([3171, 2])
We keep 1.12e+05/2.03e+06 =  5% of the original kernel matrix.

torch.Size([12210, 2])
We keep 1.04e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([5692, 2])
We keep 3.37e+05/7.18e+06 =  4% of the original kernel matrix.

torch.Size([15217, 2])
We keep 1.62e+06/6.48e+07 =  2% of the original kernel matrix.

torch.Size([7989, 2])
We keep 1.32e+06/2.95e+07 =  4% of the original kernel matrix.

torch.Size([17021, 2])
We keep 2.68e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([7224, 2])
We keep 5.21e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([16815, 2])
We keep 2.00e+06/8.75e+07 =  2% of the original kernel matrix.

torch.Size([2226, 2])
We keep 5.39e+04/7.74e+05 =  6% of the original kernel matrix.

torch.Size([10715, 2])
We keep 7.57e+05/2.13e+07 =  3% of the original kernel matrix.

torch.Size([13653, 2])
We keep 1.85e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([22439, 2])
We keep 3.56e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([4361, 2])
We keep 2.23e+05/4.16e+06 =  5% of the original kernel matrix.

torch.Size([13676, 2])
We keep 1.35e+06/4.93e+07 =  2% of the original kernel matrix.

torch.Size([4120, 2])
We keep 1.80e+05/3.51e+06 =  5% of the original kernel matrix.

torch.Size([13409, 2])
We keep 1.26e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([1653, 2])
We keep 4.45e+04/4.86e+05 =  9% of the original kernel matrix.

torch.Size([9527, 2])
We keep 6.68e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([2675, 2])
We keep 1.13e+05/1.62e+06 =  6% of the original kernel matrix.

torch.Size([11221, 2])
We keep 9.53e+05/3.08e+07 =  3% of the original kernel matrix.

torch.Size([1973, 2])
We keep 4.99e+04/6.74e+05 =  7% of the original kernel matrix.

torch.Size([10124, 2])
We keep 7.33e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([5456, 2])
We keep 3.16e+05/6.57e+06 =  4% of the original kernel matrix.

torch.Size([14908, 2])
We keep 1.58e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([550, 2])
We keep 4.67e+03/3.20e+04 = 14% of the original kernel matrix.

torch.Size([6536, 2])
We keep 2.78e+05/4.33e+06 =  6% of the original kernel matrix.

torch.Size([2994, 2])
We keep 1.20e+05/2.08e+06 =  5% of the original kernel matrix.

torch.Size([11781, 2])
We keep 1.02e+06/3.49e+07 =  2% of the original kernel matrix.

torch.Size([2555, 2])
We keep 7.03e+04/1.09e+06 =  6% of the original kernel matrix.

torch.Size([11238, 2])
We keep 8.50e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([1287, 2])
We keep 2.57e+04/2.75e+05 =  9% of the original kernel matrix.

torch.Size([8692, 2])
We keep 5.53e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([1582, 2])
We keep 4.79e+04/5.26e+05 =  9% of the original kernel matrix.

torch.Size([8973, 2])
We keep 6.72e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([1386, 2])
We keep 3.95e+04/4.42e+05 =  8% of the original kernel matrix.

torch.Size([8649, 2])
We keep 6.51e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([3018, 2])
We keep 1.23e+05/2.22e+06 =  5% of the original kernel matrix.

torch.Size([11742, 2])
We keep 1.05e+06/3.60e+07 =  2% of the original kernel matrix.

torch.Size([3525, 2])
We keep 1.55e+05/2.65e+06 =  5% of the original kernel matrix.

torch.Size([12600, 2])
We keep 1.13e+06/3.93e+07 =  2% of the original kernel matrix.

torch.Size([1372, 2])
We keep 3.89e+04/4.13e+05 =  9% of the original kernel matrix.

torch.Size([8638, 2])
We keep 6.38e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([1264, 2])
We keep 2.50e+04/2.65e+05 =  9% of the original kernel matrix.

torch.Size([8552, 2])
We keep 5.34e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([1294, 2])
We keep 2.33e+04/2.34e+05 =  9% of the original kernel matrix.

torch.Size([8781, 2])
We keep 5.26e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([6898, 2])
We keep 6.13e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([16182, 2])
We keep 2.09e+06/9.46e+07 =  2% of the original kernel matrix.

torch.Size([1260, 2])
We keep 2.07e+04/2.24e+05 =  9% of the original kernel matrix.

torch.Size([8655, 2])
We keep 5.14e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([6709, 2])
We keep 4.42e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([16222, 2])
We keep 1.87e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([2066, 2])
We keep 5.25e+04/7.09e+05 =  7% of the original kernel matrix.

torch.Size([10244, 2])
We keep 7.27e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([1486, 2])
We keep 2.81e+04/3.32e+05 =  8% of the original kernel matrix.

torch.Size([9073, 2])
We keep 5.77e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([2008, 2])
We keep 5.34e+04/6.48e+05 =  8% of the original kernel matrix.

torch.Size([10215, 2])
We keep 7.15e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([2172, 2])
We keep 5.26e+04/7.31e+05 =  7% of the original kernel matrix.

torch.Size([10505, 2])
We keep 7.59e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([3390, 2])
We keep 1.40e+05/2.49e+06 =  5% of the original kernel matrix.

torch.Size([12314, 2])
We keep 1.13e+06/3.82e+07 =  2% of the original kernel matrix.

torch.Size([1964, 2])
We keep 4.75e+04/6.10e+05 =  7% of the original kernel matrix.

torch.Size([10252, 2])
We keep 7.13e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([1836, 2])
We keep 5.26e+04/5.87e+05 =  8% of the original kernel matrix.

torch.Size([9718, 2])
We keep 7.17e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([17454, 2])
We keep 2.29e+06/9.42e+07 =  2% of the original kernel matrix.

torch.Size([25306, 2])
We keep 4.10e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([3471, 2])
We keep 1.45e+05/2.63e+06 =  5% of the original kernel matrix.

torch.Size([12456, 2])
We keep 1.14e+06/3.92e+07 =  2% of the original kernel matrix.

torch.Size([3798, 2])
We keep 1.52e+05/3.08e+06 =  4% of the original kernel matrix.

torch.Size([13002, 2])
We keep 1.20e+06/4.24e+07 =  2% of the original kernel matrix.

torch.Size([1394, 2])
We keep 3.11e+04/3.52e+05 =  8% of the original kernel matrix.

torch.Size([8857, 2])
We keep 5.83e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([4003, 2])
We keep 2.18e+05/3.74e+06 =  5% of the original kernel matrix.

torch.Size([13151, 2])
We keep 1.29e+06/4.68e+07 =  2% of the original kernel matrix.

torch.Size([1694, 2])
We keep 5.47e+04/7.02e+05 =  7% of the original kernel matrix.

torch.Size([9211, 2])
We keep 7.40e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([4452, 2])
We keep 5.44e+05/4.92e+06 = 11% of the original kernel matrix.

torch.Size([13598, 2])
We keep 1.41e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([1518, 2])
We keep 3.75e+04/3.76e+05 =  9% of the original kernel matrix.

torch.Size([9136, 2])
We keep 5.88e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([869, 2])
We keep 1.25e+04/1.14e+05 = 11% of the original kernel matrix.

torch.Size([7533, 2])
We keep 4.21e+05/8.15e+06 =  5% of the original kernel matrix.

torch.Size([1541, 2])
We keep 3.45e+04/4.65e+05 =  7% of the original kernel matrix.

torch.Size([9152, 2])
We keep 6.52e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([1512, 2])
We keep 2.83e+04/3.48e+05 =  8% of the original kernel matrix.

torch.Size([9322, 2])
We keep 5.93e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([1624, 2])
We keep 3.77e+04/3.94e+05 =  9% of the original kernel matrix.

torch.Size([9534, 2])
We keep 6.12e+05/1.52e+07 =  4% of the original kernel matrix.

torch.Size([2350, 2])
We keep 7.61e+04/1.08e+06 =  7% of the original kernel matrix.

torch.Size([10682, 2])
We keep 8.52e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([1285, 2])
We keep 2.80e+04/2.89e+05 =  9% of the original kernel matrix.

torch.Size([8773, 2])
We keep 5.57e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([5514, 2])
We keep 3.99e+05/8.46e+06 =  4% of the original kernel matrix.

torch.Size([14824, 2])
We keep 1.71e+06/7.03e+07 =  2% of the original kernel matrix.

torch.Size([19882, 2])
We keep 4.18e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([27275, 2])
We keep 5.34e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([10049, 2])
We keep 1.05e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([19361, 2])
We keep 2.66e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([1095, 2])
We keep 1.99e+04/1.87e+05 = 10% of the original kernel matrix.

torch.Size([8147, 2])
We keep 4.85e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([2141, 2])
We keep 5.58e+04/7.80e+05 =  7% of the original kernel matrix.

torch.Size([10542, 2])
We keep 7.76e+05/2.13e+07 =  3% of the original kernel matrix.

torch.Size([2047, 2])
We keep 5.83e+04/7.06e+05 =  8% of the original kernel matrix.

torch.Size([10297, 2])
We keep 7.27e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([1410, 2])
We keep 5.49e+04/4.12e+05 = 13% of the original kernel matrix.

torch.Size([8611, 2])
We keep 6.30e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([1787, 2])
We keep 4.77e+04/5.96e+05 =  7% of the original kernel matrix.

torch.Size([9721, 2])
We keep 7.01e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([1972, 2])
We keep 5.23e+04/7.01e+05 =  7% of the original kernel matrix.

torch.Size([10059, 2])
We keep 7.32e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([10793, 2])
We keep 1.20e+06/3.50e+07 =  3% of the original kernel matrix.

torch.Size([20151, 2])
We keep 2.87e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([5039, 2])
We keep 2.24e+05/4.85e+06 =  4% of the original kernel matrix.

torch.Size([14625, 2])
We keep 1.41e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([7139, 2])
We keep 8.39e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([16317, 2])
We keep 2.36e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([2925, 2])
We keep 1.18e+05/1.81e+06 =  6% of the original kernel matrix.

torch.Size([11637, 2])
We keep 1.02e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([16065, 2])
We keep 2.79e+06/9.76e+07 =  2% of the original kernel matrix.

torch.Size([24216, 2])
We keep 4.21e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([1842, 2])
We keep 3.72e+04/4.65e+05 =  8% of the original kernel matrix.

torch.Size([9940, 2])
We keep 6.47e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([2499, 2])
We keep 9.95e+04/1.50e+06 =  6% of the original kernel matrix.

torch.Size([10872, 2])
We keep 9.62e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([4709, 2])
We keep 2.65e+05/5.56e+06 =  4% of the original kernel matrix.

torch.Size([14073, 2])
We keep 1.48e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([401, 2])
We keep 3.53e+03/1.90e+04 = 18% of the original kernel matrix.

torch.Size([5711, 2])
We keep 2.41e+05/3.34e+06 =  7% of the original kernel matrix.

torch.Size([8103, 2])
We keep 6.82e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([17516, 2])
We keep 2.18e+06/9.89e+07 =  2% of the original kernel matrix.

torch.Size([1554, 2])
We keep 3.55e+04/4.13e+05 =  8% of the original kernel matrix.

torch.Size([9133, 2])
We keep 6.26e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([4560, 2])
We keep 3.00e+05/5.44e+06 =  5% of the original kernel matrix.

torch.Size([13800, 2])
We keep 1.48e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([1165, 2])
We keep 1.87e+04/1.71e+05 = 10% of the original kernel matrix.

torch.Size([8350, 2])
We keep 4.79e+05/9.98e+06 =  4% of the original kernel matrix.

torch.Size([2314, 2])
We keep 7.53e+04/1.08e+06 =  6% of the original kernel matrix.

torch.Size([10629, 2])
We keep 8.47e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([1549, 2])
We keep 2.81e+04/3.45e+05 =  8% of the original kernel matrix.

torch.Size([9414, 2])
We keep 5.89e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([2148, 2])
We keep 5.12e+04/7.46e+05 =  6% of the original kernel matrix.

torch.Size([10478, 2])
We keep 7.40e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([1780, 2])
We keep 3.90e+04/4.94e+05 =  7% of the original kernel matrix.

torch.Size([9703, 2])
We keep 6.62e+05/1.70e+07 =  3% of the original kernel matrix.

torch.Size([3022, 2])
We keep 1.04e+05/1.70e+06 =  6% of the original kernel matrix.

torch.Size([11931, 2])
We keep 9.84e+05/3.15e+07 =  3% of the original kernel matrix.

torch.Size([485, 2])
We keep 3.75e+03/2.07e+04 = 18% of the original kernel matrix.

torch.Size([6397, 2])
We keep 2.58e+05/3.48e+06 =  7% of the original kernel matrix.

torch.Size([1203, 2])
We keep 2.02e+04/2.03e+05 =  9% of the original kernel matrix.

torch.Size([8350, 2])
We keep 4.98e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([1856, 2])
We keep 3.89e+04/5.07e+05 =  7% of the original kernel matrix.

torch.Size([10058, 2])
We keep 6.60e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([5174, 2])
We keep 2.94e+05/5.85e+06 =  5% of the original kernel matrix.

torch.Size([14680, 2])
We keep 1.50e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([1576, 2])
We keep 4.09e+04/4.28e+05 =  9% of the original kernel matrix.

torch.Size([9241, 2])
We keep 6.22e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([5588, 2])
We keep 3.10e+05/7.12e+06 =  4% of the original kernel matrix.

torch.Size([15168, 2])
We keep 1.58e+06/6.45e+07 =  2% of the original kernel matrix.

torch.Size([2871, 2])
We keep 1.59e+05/1.64e+06 =  9% of the original kernel matrix.

torch.Size([11680, 2])
We keep 9.71e+05/3.10e+07 =  3% of the original kernel matrix.

torch.Size([2259, 2])
We keep 6.68e+04/9.01e+05 =  7% of the original kernel matrix.

torch.Size([10535, 2])
We keep 7.99e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([5448, 2])
We keep 3.26e+05/7.19e+06 =  4% of the original kernel matrix.

torch.Size([15010, 2])
We keep 1.61e+06/6.48e+07 =  2% of the original kernel matrix.

torch.Size([4811, 2])
We keep 3.62e+05/7.24e+06 =  4% of the original kernel matrix.

torch.Size([13938, 2])
We keep 1.62e+06/6.50e+07 =  2% of the original kernel matrix.

torch.Size([1472, 2])
We keep 3.18e+04/3.98e+05 =  7% of the original kernel matrix.

torch.Size([8968, 2])
We keep 6.05e+05/1.53e+07 =  3% of the original kernel matrix.

torch.Size([3501, 2])
We keep 1.25e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([12736, 2])
We keep 1.09e+06/3.75e+07 =  2% of the original kernel matrix.

torch.Size([3527, 2])
We keep 1.47e+05/2.74e+06 =  5% of the original kernel matrix.

torch.Size([12587, 2])
We keep 1.16e+06/4.00e+07 =  2% of the original kernel matrix.

torch.Size([3445, 2])
We keep 1.83e+05/3.36e+06 =  5% of the original kernel matrix.

torch.Size([12259, 2])
We keep 1.27e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([1567, 2])
We keep 2.78e+04/3.45e+05 =  8% of the original kernel matrix.

torch.Size([9399, 2])
We keep 5.80e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([7227, 2])
We keep 4.47e+05/1.16e+07 =  3% of the original kernel matrix.

torch.Size([16846, 2])
We keep 1.89e+06/8.25e+07 =  2% of the original kernel matrix.

torch.Size([4496, 2])
We keep 2.30e+05/4.66e+06 =  4% of the original kernel matrix.

torch.Size([13808, 2])
We keep 1.38e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([4332, 2])
We keep 2.12e+05/4.26e+06 =  4% of the original kernel matrix.

torch.Size([13658, 2])
We keep 1.37e+06/4.99e+07 =  2% of the original kernel matrix.

torch.Size([3565, 2])
We keep 3.06e+05/4.87e+06 =  6% of the original kernel matrix.

torch.Size([12060, 2])
We keep 1.41e+06/5.33e+07 =  2% of the original kernel matrix.

torch.Size([4856, 2])
We keep 2.79e+05/5.40e+06 =  5% of the original kernel matrix.

torch.Size([14336, 2])
We keep 1.46e+06/5.62e+07 =  2% of the original kernel matrix.

torch.Size([13302, 2])
We keep 2.17e+06/7.10e+07 =  3% of the original kernel matrix.

torch.Size([22033, 2])
We keep 3.72e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([1500, 2])
We keep 5.96e+04/5.55e+05 = 10% of the original kernel matrix.

torch.Size([8718, 2])
We keep 6.91e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([653, 2])
We keep 5.65e+03/3.84e+04 = 14% of the original kernel matrix.

torch.Size([7052, 2])
We keep 3.02e+05/4.74e+06 =  6% of the original kernel matrix.

torch.Size([1806, 2])
We keep 4.49e+04/5.75e+05 =  7% of the original kernel matrix.

torch.Size([9751, 2])
We keep 6.88e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([1265, 2])
We keep 3.89e+04/2.87e+05 = 13% of the original kernel matrix.

torch.Size([8565, 2])
We keep 5.61e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([2797, 2])
We keep 1.32e+05/2.14e+06 =  6% of the original kernel matrix.

torch.Size([11235, 2])
We keep 1.07e+06/3.54e+07 =  3% of the original kernel matrix.

torch.Size([2324, 2])
We keep 7.38e+04/1.09e+06 =  6% of the original kernel matrix.

torch.Size([10689, 2])
We keep 8.51e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([1244, 2])
We keep 1.82e+04/1.81e+05 = 10% of the original kernel matrix.

torch.Size([8767, 2])
We keep 4.93e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([1583, 2])
We keep 6.08e+04/5.39e+05 = 11% of the original kernel matrix.

torch.Size([8929, 2])
We keep 6.93e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([3619, 2])
We keep 1.57e+05/2.79e+06 =  5% of the original kernel matrix.

torch.Size([12726, 2])
We keep 1.18e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([976, 2])
We keep 1.17e+04/9.67e+04 = 12% of the original kernel matrix.

torch.Size([8064, 2])
We keep 4.09e+05/7.52e+06 =  5% of the original kernel matrix.

torch.Size([5468, 2])
We keep 2.98e+05/5.99e+06 =  4% of the original kernel matrix.

torch.Size([15166, 2])
We keep 1.51e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([1208, 2])
We keep 1.68e+04/1.64e+05 = 10% of the original kernel matrix.

torch.Size([8542, 2])
We keep 4.65e+05/9.79e+06 =  4% of the original kernel matrix.

torch.Size([10711, 2])
We keep 1.00e+06/3.04e+07 =  3% of the original kernel matrix.

torch.Size([19879, 2])
We keep 2.71e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([7397, 2])
We keep 5.92e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([16897, 2])
We keep 2.12e+06/9.37e+07 =  2% of the original kernel matrix.

torch.Size([1964, 2])
We keep 5.09e+04/7.02e+05 =  7% of the original kernel matrix.

torch.Size([10203, 2])
We keep 7.49e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([526, 2])
We keep 5.00e+03/3.03e+04 = 16% of the original kernel matrix.

torch.Size([6395, 2])
We keep 2.91e+05/4.21e+06 =  6% of the original kernel matrix.

torch.Size([6146, 2])
We keep 4.42e+05/9.76e+06 =  4% of the original kernel matrix.

torch.Size([15589, 2])
We keep 1.81e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([4749, 2])
We keep 2.44e+05/4.92e+06 =  4% of the original kernel matrix.

torch.Size([14145, 2])
We keep 1.43e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([2535, 2])
We keep 8.23e+04/1.32e+06 =  6% of the original kernel matrix.

torch.Size([11037, 2])
We keep 9.05e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([12944, 2])
We keep 1.46e+06/4.93e+07 =  2% of the original kernel matrix.

torch.Size([21820, 2])
We keep 3.25e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([11121, 2])
We keep 1.12e+06/3.50e+07 =  3% of the original kernel matrix.

torch.Size([20347, 2])
We keep 2.86e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([1258, 2])
We keep 2.49e+04/2.77e+05 =  8% of the original kernel matrix.

torch.Size([8533, 2])
We keep 5.46e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([947, 2])
We keep 1.12e+04/1.02e+05 = 10% of the original kernel matrix.

torch.Size([7900, 2])
We keep 4.01e+05/7.73e+06 =  5% of the original kernel matrix.

torch.Size([933, 2])
We keep 1.18e+04/9.55e+04 = 12% of the original kernel matrix.

torch.Size([7745, 2])
We keep 3.99e+05/7.47e+06 =  5% of the original kernel matrix.

torch.Size([1024, 2])
We keep 1.31e+04/1.18e+05 = 11% of the original kernel matrix.

torch.Size([8138, 2])
We keep 4.19e+05/8.29e+06 =  5% of the original kernel matrix.

torch.Size([923, 2])
We keep 2.37e+04/1.74e+05 = 13% of the original kernel matrix.

torch.Size([7268, 2])
We keep 4.61e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([817, 2])
We keep 1.09e+04/9.18e+04 = 11% of the original kernel matrix.

torch.Size([7386, 2])
We keep 3.91e+05/7.32e+06 =  5% of the original kernel matrix.

torch.Size([882, 2])
We keep 1.29e+04/9.73e+04 = 13% of the original kernel matrix.

torch.Size([7576, 2])
We keep 3.97e+05/7.54e+06 =  5% of the original kernel matrix.

torch.Size([3731, 2])
We keep 2.32e+05/3.83e+06 =  6% of the original kernel matrix.

torch.Size([12551, 2])
We keep 1.32e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([1402, 2])
We keep 3.54e+04/3.35e+05 = 10% of the original kernel matrix.

torch.Size([8864, 2])
We keep 5.71e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([2206, 2])
We keep 1.11e+05/1.74e+06 =  6% of the original kernel matrix.

torch.Size([10105, 2])
We keep 9.96e+05/3.19e+07 =  3% of the original kernel matrix.

torch.Size([2323, 2])
We keep 7.55e+04/1.12e+06 =  6% of the original kernel matrix.

torch.Size([10723, 2])
We keep 8.58e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([3243, 2])
We keep 1.31e+05/2.33e+06 =  5% of the original kernel matrix.

torch.Size([12011, 2])
We keep 1.10e+06/3.69e+07 =  2% of the original kernel matrix.

torch.Size([1398, 2])
We keep 3.29e+04/3.53e+05 =  9% of the original kernel matrix.

torch.Size([8722, 2])
We keep 5.88e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([6787, 2])
We keep 7.06e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([16001, 2])
We keep 2.14e+06/9.82e+07 =  2% of the original kernel matrix.

torch.Size([3598, 2])
We keep 1.91e+05/3.10e+06 =  6% of the original kernel matrix.

torch.Size([12530, 2])
We keep 1.22e+06/4.25e+07 =  2% of the original kernel matrix.

torch.Size([1108, 2])
We keep 1.62e+04/1.51e+05 = 10% of the original kernel matrix.

torch.Size([8165, 2])
We keep 4.53e+05/9.38e+06 =  4% of the original kernel matrix.

torch.Size([940, 2])
We keep 1.82e+04/1.60e+05 = 11% of the original kernel matrix.

torch.Size([7364, 2])
We keep 4.61e+05/9.67e+06 =  4% of the original kernel matrix.

torch.Size([4852, 2])
We keep 2.84e+05/6.14e+06 =  4% of the original kernel matrix.

torch.Size([14112, 2])
We keep 1.51e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([517, 2])
We keep 4.40e+03/2.66e+04 = 16% of the original kernel matrix.

torch.Size([6516, 2])
We keep 2.78e+05/3.94e+06 =  7% of the original kernel matrix.

torch.Size([927, 2])
We keep 1.57e+04/1.32e+05 = 11% of the original kernel matrix.

torch.Size([7646, 2])
We keep 4.37e+05/8.80e+06 =  4% of the original kernel matrix.

torch.Size([1955, 2])
We keep 6.62e+04/9.16e+05 =  7% of the original kernel matrix.

torch.Size([9918, 2])
We keep 8.00e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([3164, 2])
We keep 1.27e+05/2.26e+06 =  5% of the original kernel matrix.

torch.Size([12005, 2])
We keep 1.08e+06/3.63e+07 =  2% of the original kernel matrix.

torch.Size([6464, 2])
We keep 4.77e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([15964, 2])
We keep 1.92e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([2399, 2])
We keep 8.71e+04/1.32e+06 =  6% of the original kernel matrix.

torch.Size([10748, 2])
We keep 9.31e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([958, 2])
We keep 1.81e+04/1.67e+05 = 10% of the original kernel matrix.

torch.Size([7554, 2])
We keep 4.72e+05/9.89e+06 =  4% of the original kernel matrix.

torch.Size([2848, 2])
We keep 1.00e+05/1.79e+06 =  5% of the original kernel matrix.

torch.Size([11617, 2])
We keep 1.00e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([5768, 2])
We keep 3.36e+05/7.43e+06 =  4% of the original kernel matrix.

torch.Size([15263, 2])
We keep 1.64e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([1105, 2])
We keep 3.51e+04/2.15e+05 = 16% of the original kernel matrix.

torch.Size([8016, 2])
We keep 5.14e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([8945, 2])
We keep 1.55e+06/2.83e+07 =  5% of the original kernel matrix.

torch.Size([18216, 2])
We keep 2.66e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([17921, 2])
We keep 2.13e+07/4.86e+08 =  4% of the original kernel matrix.

torch.Size([24066, 2])
We keep 7.91e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([2675, 2])
We keep 1.07e+05/1.54e+06 =  6% of the original kernel matrix.

torch.Size([11256, 2])
We keep 8.67e+05/3.00e+07 =  2% of the original kernel matrix.

torch.Size([6137, 2])
We keep 5.12e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([15512, 2])
We keep 1.86e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([382, 2])
We keep 3.53e+03/2.37e+04 = 14% of the original kernel matrix.

torch.Size([5749, 2])
We keep 2.60e+05/3.72e+06 =  6% of the original kernel matrix.

torch.Size([4735, 2])
We keep 2.47e+05/5.02e+06 =  4% of the original kernel matrix.

torch.Size([14088, 2])
We keep 1.42e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([34527, 2])
We keep 1.01e+08/9.19e+08 = 10% of the original kernel matrix.

torch.Size([35324, 2])
We keep 1.03e+07/7.33e+08 =  1% of the original kernel matrix.

torch.Size([80904, 2])
We keep 2.04e+08/6.42e+09 =  3% of the original kernel matrix.

torch.Size([52317, 2])
We keep 2.27e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([1869, 2])
We keep 4.68e+04/6.21e+05 =  7% of the original kernel matrix.

torch.Size([9931, 2])
We keep 7.26e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([22849, 2])
We keep 1.18e+07/4.25e+08 =  2% of the original kernel matrix.

torch.Size([28789, 2])
We keep 7.47e+06/4.98e+08 =  1% of the original kernel matrix.

torch.Size([21067, 2])
We keep 1.20e+07/2.72e+08 =  4% of the original kernel matrix.

torch.Size([27709, 2])
We keep 6.10e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([3936, 2])
We keep 3.34e+05/5.03e+06 =  6% of the original kernel matrix.

torch.Size([12891, 2])
We keep 1.39e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([57303, 2])
We keep 4.51e+08/4.31e+09 = 10% of the original kernel matrix.

torch.Size([45151, 2])
We keep 1.82e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([1551, 2])
We keep 2.90e+04/3.31e+05 =  8% of the original kernel matrix.

torch.Size([9303, 2])
We keep 5.76e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([5928, 2])
We keep 4.25e+05/8.57e+06 =  4% of the original kernel matrix.

torch.Size([15460, 2])
We keep 1.73e+06/7.08e+07 =  2% of the original kernel matrix.

torch.Size([45768, 2])
We keep 2.68e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([40479, 2])
We keep 1.23e+07/9.30e+08 =  1% of the original kernel matrix.

torch.Size([122971, 2])
We keep 1.19e+08/7.24e+09 =  1% of the original kernel matrix.

torch.Size([67406, 2])
We keep 2.39e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([2753, 2])
We keep 9.48e+04/1.50e+06 =  6% of the original kernel matrix.

torch.Size([11385, 2])
We keep 9.37e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([18994, 2])
We keep 4.48e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([26575, 2])
We keep 4.88e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([10580, 2])
We keep 2.67e+06/4.68e+07 =  5% of the original kernel matrix.

torch.Size([19703, 2])
We keep 3.18e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([27563, 2])
We keep 3.54e+07/5.75e+08 =  6% of the original kernel matrix.

torch.Size([31582, 2])
We keep 8.49e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([25467, 2])
We keep 2.98e+07/7.15e+08 =  4% of the original kernel matrix.

torch.Size([29524, 2])
We keep 9.07e+06/6.46e+08 =  1% of the original kernel matrix.

torch.Size([19309, 2])
We keep 1.28e+07/2.81e+08 =  4% of the original kernel matrix.

torch.Size([26056, 2])
We keep 6.17e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([19003, 2])
We keep 3.90e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([26701, 2])
We keep 4.74e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([18117, 2])
We keep 4.35e+06/1.31e+08 =  3% of the original kernel matrix.

torch.Size([25877, 2])
We keep 4.70e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([1088, 2])
We keep 1.71e+04/1.58e+05 = 10% of the original kernel matrix.

torch.Size([8077, 2])
We keep 4.63e+05/9.62e+06 =  4% of the original kernel matrix.

torch.Size([82504, 2])
We keep 1.57e+08/7.62e+09 =  2% of the original kernel matrix.

torch.Size([52657, 2])
We keep 2.44e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([2122, 2])
We keep 6.10e+04/7.66e+05 =  7% of the original kernel matrix.

torch.Size([10450, 2])
We keep 7.52e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([2430, 2])
We keep 8.73e+04/1.19e+06 =  7% of the original kernel matrix.

torch.Size([10892, 2])
We keep 8.97e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([12754, 2])
We keep 2.24e+06/5.92e+07 =  3% of the original kernel matrix.

torch.Size([21574, 2])
We keep 3.47e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([17646, 2])
We keep 9.46e+06/3.19e+08 =  2% of the original kernel matrix.

torch.Size([24559, 2])
We keep 6.68e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([132530, 2])
We keep 1.27e+08/9.88e+09 =  1% of the original kernel matrix.

torch.Size([70348, 2])
We keep 2.72e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([55699, 2])
We keep 2.23e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([46210, 2])
We keep 1.24e+07/9.39e+08 =  1% of the original kernel matrix.

torch.Size([1067, 2])
We keep 1.30e+04/1.22e+05 = 10% of the original kernel matrix.

torch.Size([8175, 2])
We keep 4.18e+05/8.46e+06 =  4% of the original kernel matrix.

torch.Size([6605, 2])
We keep 8.52e+05/1.26e+07 =  6% of the original kernel matrix.

torch.Size([16091, 2])
We keep 2.00e+06/8.56e+07 =  2% of the original kernel matrix.

torch.Size([12538, 2])
We keep 2.05e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([21275, 2])
We keep 3.67e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([16173, 2])
We keep 5.59e+06/1.59e+08 =  3% of the original kernel matrix.

torch.Size([24228, 2])
We keep 5.07e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([53212, 2])
We keep 3.75e+07/1.41e+09 =  2% of the original kernel matrix.

torch.Size([45028, 2])
We keep 1.17e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([8907, 2])
We keep 2.93e+07/6.75e+07 = 43% of the original kernel matrix.

torch.Size([17641, 2])
We keep 3.51e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([12838, 2])
We keep 2.70e+06/5.86e+07 =  4% of the original kernel matrix.

torch.Size([21598, 2])
We keep 3.50e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([613, 2])
We keep 5.14e+03/4.16e+04 = 12% of the original kernel matrix.

torch.Size([6851, 2])
We keep 2.99e+05/4.93e+06 =  6% of the original kernel matrix.

torch.Size([3812, 2])
We keep 1.36e+05/2.63e+06 =  5% of the original kernel matrix.

torch.Size([13090, 2])
We keep 1.14e+06/3.92e+07 =  2% of the original kernel matrix.

torch.Size([23330, 2])
We keep 1.08e+07/4.73e+08 =  2% of the original kernel matrix.

torch.Size([28583, 2])
We keep 7.89e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([22235, 2])
We keep 1.50e+07/4.08e+08 =  3% of the original kernel matrix.

torch.Size([28178, 2])
We keep 7.33e+06/4.88e+08 =  1% of the original kernel matrix.

torch.Size([17197, 2])
We keep 5.59e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([24849, 2])
We keep 5.63e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([5019, 2])
We keep 3.78e+05/8.71e+06 =  4% of the original kernel matrix.

torch.Size([14209, 2])
We keep 1.73e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([189655, 2])
We keep 2.25e+08/2.61e+10 =  0% of the original kernel matrix.

torch.Size([85730, 2])
We keep 4.06e+07/3.91e+09 =  1% of the original kernel matrix.

torch.Size([6828, 2])
We keep 4.38e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([16389, 2])
We keep 1.87e+06/8.05e+07 =  2% of the original kernel matrix.

torch.Size([21507, 2])
We keep 3.96e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([28387, 2])
We keep 5.29e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([51023, 2])
We keep 2.81e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([43488, 2])
We keep 1.28e+07/9.84e+08 =  1% of the original kernel matrix.

torch.Size([2952, 2])
We keep 9.12e+04/1.56e+06 =  5% of the original kernel matrix.

torch.Size([11852, 2])
We keep 9.58e+05/3.02e+07 =  3% of the original kernel matrix.

torch.Size([32840, 2])
We keep 1.18e+07/5.58e+08 =  2% of the original kernel matrix.

torch.Size([35726, 2])
We keep 8.43e+06/5.71e+08 =  1% of the original kernel matrix.

torch.Size([128837, 2])
We keep 2.00e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([69154, 2])
We keep 2.80e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([93256, 2])
We keep 8.94e+07/5.82e+09 =  1% of the original kernel matrix.

torch.Size([57419, 2])
We keep 2.16e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([7081, 2])
We keep 8.00e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([16269, 2])
We keep 2.20e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([3393, 2])
We keep 1.88e+05/2.71e+06 =  6% of the original kernel matrix.

torch.Size([12190, 2])
We keep 1.16e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([2003, 2])
We keep 1.20e+05/1.07e+06 = 11% of the original kernel matrix.

torch.Size([9711, 2])
We keep 8.63e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([3247, 2])
We keep 1.17e+05/1.99e+06 =  5% of the original kernel matrix.

torch.Size([12233, 2])
We keep 1.05e+06/3.41e+07 =  3% of the original kernel matrix.

torch.Size([15296, 2])
We keep 8.58e+06/2.16e+08 =  3% of the original kernel matrix.

torch.Size([23203, 2])
We keep 5.65e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([8487, 2])
We keep 6.86e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([18038, 2])
We keep 2.23e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([19280, 2])
We keep 5.49e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([26647, 2])
We keep 5.44e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([46779, 2])
We keep 8.99e+07/2.21e+09 =  4% of the original kernel matrix.

torch.Size([41455, 2])
We keep 1.32e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([11845, 2])
We keep 2.07e+06/5.30e+07 =  3% of the original kernel matrix.

torch.Size([21074, 2])
We keep 3.13e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([4913, 2])
We keep 7.90e+05/1.29e+07 =  6% of the original kernel matrix.

torch.Size([13522, 2])
We keep 2.02e+06/8.69e+07 =  2% of the original kernel matrix.

torch.Size([6164, 2])
We keep 3.75e+05/8.75e+06 =  4% of the original kernel matrix.

torch.Size([15933, 2])
We keep 1.70e+06/7.15e+07 =  2% of the original kernel matrix.

torch.Size([20749, 2])
We keep 6.43e+07/1.57e+09 =  4% of the original kernel matrix.

torch.Size([24488, 2])
We keep 1.24e+07/9.57e+08 =  1% of the original kernel matrix.

torch.Size([12139, 2])
We keep 1.29e+07/8.79e+07 = 14% of the original kernel matrix.

torch.Size([20897, 2])
We keep 3.89e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([5872, 2])
We keep 1.69e+06/1.83e+07 =  9% of the original kernel matrix.

torch.Size([14963, 2])
We keep 2.26e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([25699, 2])
We keep 6.14e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([31289, 2])
We keep 6.58e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([156265, 2])
We keep 4.40e+08/2.22e+10 =  1% of the original kernel matrix.

torch.Size([76176, 2])
We keep 3.80e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([6358, 2])
We keep 5.69e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([15817, 2])
We keep 1.91e+06/8.23e+07 =  2% of the original kernel matrix.

torch.Size([59650, 2])
We keep 1.50e+08/3.43e+09 =  4% of the original kernel matrix.

torch.Size([43710, 2])
We keep 1.72e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([2634, 2])
We keep 9.61e+04/1.43e+06 =  6% of the original kernel matrix.

torch.Size([11138, 2])
We keep 9.19e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([16134, 2])
We keep 1.05e+07/2.88e+08 =  3% of the original kernel matrix.

torch.Size([22146, 2])
We keep 6.43e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([9955, 2])
We keep 1.26e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([19559, 2])
We keep 2.65e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([95532, 2])
We keep 2.19e+08/9.75e+09 =  2% of the original kernel matrix.

torch.Size([55495, 2])
We keep 2.73e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([28293, 2])
We keep 6.66e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([32531, 2])
We keep 7.16e+06/4.82e+08 =  1% of the original kernel matrix.

torch.Size([2787, 2])
We keep 1.21e+05/1.68e+06 =  7% of the original kernel matrix.

torch.Size([11398, 2])
We keep 9.90e+05/3.13e+07 =  3% of the original kernel matrix.

torch.Size([74458, 2])
We keep 3.08e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([52790, 2])
We keep 1.54e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([99599, 2])
We keep 5.49e+07/4.53e+09 =  1% of the original kernel matrix.

torch.Size([60449, 2])
We keep 1.94e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([3877, 2])
We keep 2.77e+05/3.97e+06 =  6% of the original kernel matrix.

torch.Size([12928, 2])
We keep 1.33e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([28229, 2])
We keep 1.84e+07/4.64e+08 =  3% of the original kernel matrix.

torch.Size([33138, 2])
We keep 7.62e+06/5.21e+08 =  1% of the original kernel matrix.

torch.Size([134765, 2])
We keep 9.31e+07/8.98e+09 =  1% of the original kernel matrix.

torch.Size([71217, 2])
We keep 2.59e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([271671, 2])
We keep 4.16e+08/3.96e+10 =  1% of the original kernel matrix.

torch.Size([104702, 2])
We keep 4.95e+07/4.81e+09 =  1% of the original kernel matrix.

torch.Size([10125, 2])
We keep 2.32e+06/4.22e+07 =  5% of the original kernel matrix.

torch.Size([19475, 2])
We keep 3.00e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([1927, 2])
We keep 9.09e+04/6.77e+05 = 13% of the original kernel matrix.

torch.Size([9845, 2])
We keep 7.06e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([3890, 2])
We keep 1.87e+05/3.17e+06 =  5% of the original kernel matrix.

torch.Size([13137, 2])
We keep 1.21e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([3901, 2])
We keep 3.67e+05/4.62e+06 =  7% of the original kernel matrix.

torch.Size([13061, 2])
We keep 1.36e+06/5.19e+07 =  2% of the original kernel matrix.

torch.Size([12675, 2])
We keep 1.92e+06/5.19e+07 =  3% of the original kernel matrix.

torch.Size([21557, 2])
We keep 3.30e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([8023, 2])
We keep 1.23e+06/2.06e+07 =  5% of the original kernel matrix.

torch.Size([17415, 2])
We keep 2.35e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([3033, 2])
We keep 1.43e+05/2.14e+06 =  6% of the original kernel matrix.

torch.Size([11846, 2])
We keep 1.07e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([50129, 2])
We keep 2.55e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([43761, 2])
We keep 1.17e+07/8.77e+08 =  1% of the original kernel matrix.

torch.Size([76729, 2])
We keep 4.73e+07/3.04e+09 =  1% of the original kernel matrix.

torch.Size([53185, 2])
We keep 1.66e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([1108, 2])
We keep 3.57e+04/2.95e+05 = 12% of the original kernel matrix.

torch.Size([8021, 2])
We keep 5.40e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([4116, 2])
We keep 2.51e+05/4.54e+06 =  5% of the original kernel matrix.

torch.Size([13173, 2])
We keep 1.40e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([6619, 2])
We keep 5.56e+05/1.41e+07 =  3% of the original kernel matrix.

torch.Size([16013, 2])
We keep 2.04e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([13819, 2])
We keep 1.76e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([22531, 2])
We keep 3.46e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([9728, 2])
We keep 8.66e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([19082, 2])
We keep 2.51e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([13426, 2])
We keep 1.84e+06/5.92e+07 =  3% of the original kernel matrix.

torch.Size([22249, 2])
We keep 3.47e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([2260, 2])
We keep 5.61e+04/8.10e+05 =  6% of the original kernel matrix.

torch.Size([10733, 2])
We keep 7.74e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([8184, 2])
We keep 1.35e+06/2.65e+07 =  5% of the original kernel matrix.

torch.Size([17493, 2])
We keep 2.50e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([21114, 2])
We keep 3.71e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([28093, 2])
We keep 5.06e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([13224, 2])
We keep 1.80e+06/5.49e+07 =  3% of the original kernel matrix.

torch.Size([22065, 2])
We keep 3.35e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([25464, 2])
We keep 1.94e+07/5.04e+08 =  3% of the original kernel matrix.

torch.Size([29329, 2])
We keep 7.39e+06/5.43e+08 =  1% of the original kernel matrix.

torch.Size([4635, 2])
We keep 4.50e+05/8.71e+06 =  5% of the original kernel matrix.

torch.Size([13493, 2])
We keep 1.74e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([212000, 2])
We keep 1.54e+09/2.88e+10 =  5% of the original kernel matrix.

torch.Size([90810, 2])
We keep 4.23e+07/4.10e+09 =  1% of the original kernel matrix.

torch.Size([5655, 2])
We keep 3.98e+05/8.56e+06 =  4% of the original kernel matrix.

torch.Size([15070, 2])
We keep 1.73e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([34211, 2])
We keep 1.12e+07/6.02e+08 =  1% of the original kernel matrix.

torch.Size([35965, 2])
We keep 8.51e+06/5.93e+08 =  1% of the original kernel matrix.

torch.Size([1223, 2])
We keep 2.23e+04/2.13e+05 = 10% of the original kernel matrix.

torch.Size([8586, 2])
We keep 4.96e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([3153, 2])
We keep 1.24e+05/2.34e+06 =  5% of the original kernel matrix.

torch.Size([12031, 2])
We keep 1.09e+06/3.70e+07 =  2% of the original kernel matrix.

torch.Size([5357, 2])
We keep 2.94e+05/6.25e+06 =  4% of the original kernel matrix.

torch.Size([14740, 2])
We keep 1.54e+06/6.04e+07 =  2% of the original kernel matrix.

torch.Size([13234, 2])
We keep 2.41e+06/5.80e+07 =  4% of the original kernel matrix.

torch.Size([22101, 2])
We keep 3.31e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([3658, 2])
We keep 1.55e+05/2.82e+06 =  5% of the original kernel matrix.

torch.Size([12767, 2])
We keep 1.19e+06/4.06e+07 =  2% of the original kernel matrix.

torch.Size([62781, 2])
We keep 8.79e+07/3.01e+09 =  2% of the original kernel matrix.

torch.Size([46379, 2])
We keep 1.67e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([3897, 2])
We keep 1.92e+05/3.66e+06 =  5% of the original kernel matrix.

torch.Size([13004, 2])
We keep 1.28e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([4538, 2])
We keep 2.42e+05/5.27e+06 =  4% of the original kernel matrix.

torch.Size([13807, 2])
We keep 1.42e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([181350, 2])
We keep 1.99e+08/1.78e+10 =  1% of the original kernel matrix.

torch.Size([83647, 2])
We keep 3.50e+07/3.22e+09 =  1% of the original kernel matrix.

torch.Size([28040, 2])
We keep 1.21e+07/4.93e+08 =  2% of the original kernel matrix.

torch.Size([31954, 2])
We keep 7.76e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([36209, 2])
We keep 1.99e+07/7.92e+08 =  2% of the original kernel matrix.

torch.Size([36887, 2])
We keep 9.46e+06/6.80e+08 =  1% of the original kernel matrix.

torch.Size([128237, 2])
We keep 1.50e+08/9.59e+09 =  1% of the original kernel matrix.

torch.Size([69138, 2])
We keep 2.70e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([139062, 2])
We keep 1.69e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([69574, 2])
We keep 3.14e+07/2.81e+09 =  1% of the original kernel matrix.

torch.Size([7070, 2])
We keep 4.91e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([16601, 2])
We keep 1.95e+06/8.42e+07 =  2% of the original kernel matrix.

torch.Size([3803, 2])
We keep 1.59e+05/2.81e+06 =  5% of the original kernel matrix.

torch.Size([13018, 2])
We keep 1.19e+06/4.05e+07 =  2% of the original kernel matrix.

torch.Size([8114, 2])
We keep 1.45e+06/2.76e+07 =  5% of the original kernel matrix.

torch.Size([17248, 2])
We keep 2.59e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([594, 2])
We keep 6.45e+03/4.37e+04 = 14% of the original kernel matrix.

torch.Size([6582, 2])
We keep 3.10e+05/5.05e+06 =  6% of the original kernel matrix.

torch.Size([3374, 2])
We keep 1.98e+05/2.52e+06 =  7% of the original kernel matrix.

torch.Size([12225, 2])
We keep 1.11e+06/3.84e+07 =  2% of the original kernel matrix.

torch.Size([9660, 2])
We keep 1.53e+06/3.65e+07 =  4% of the original kernel matrix.

torch.Size([18931, 2])
We keep 2.91e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([1621, 2])
We keep 3.71e+04/4.65e+05 =  7% of the original kernel matrix.

torch.Size([9557, 2])
We keep 6.51e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([3109, 2])
We keep 1.21e+05/2.05e+06 =  5% of the original kernel matrix.

torch.Size([12028, 2])
We keep 1.05e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([2110, 2])
We keep 7.06e+04/1.02e+06 =  6% of the original kernel matrix.

torch.Size([10170, 2])
We keep 8.27e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([2952, 2])
We keep 9.97e+04/1.70e+06 =  5% of the original kernel matrix.

torch.Size([11825, 2])
We keep 9.82e+05/3.15e+07 =  3% of the original kernel matrix.

torch.Size([64710, 2])
We keep 4.35e+07/2.87e+09 =  1% of the original kernel matrix.

torch.Size([48212, 2])
We keep 1.61e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([8575, 2])
We keep 1.22e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([17596, 2])
We keep 2.74e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([46040, 2])
We keep 2.82e+07/1.33e+09 =  2% of the original kernel matrix.

torch.Size([41427, 2])
We keep 1.18e+07/8.82e+08 =  1% of the original kernel matrix.

torch.Size([3863, 2])
We keep 1.35e+06/6.10e+06 = 22% of the original kernel matrix.

torch.Size([12635, 2])
We keep 1.40e+06/5.97e+07 =  2% of the original kernel matrix.

torch.Size([2459, 2])
We keep 9.75e+04/1.39e+06 =  6% of the original kernel matrix.

torch.Size([10751, 2])
We keep 9.21e+05/2.85e+07 =  3% of the original kernel matrix.

torch.Size([4897, 2])
We keep 3.72e+05/6.16e+06 =  6% of the original kernel matrix.

torch.Size([14173, 2])
We keep 1.53e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([448062, 2])
We keep 1.42e+09/1.23e+11 =  1% of the original kernel matrix.

torch.Size([131069, 2])
We keep 8.04e+07/8.47e+09 =  0% of the original kernel matrix.

torch.Size([281058, 2])
We keep 3.22e+08/4.04e+10 =  0% of the original kernel matrix.

torch.Size([106332, 2])
We keep 4.93e+07/4.86e+09 =  1% of the original kernel matrix.

torch.Size([8005, 2])
We keep 7.36e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([17669, 2])
We keep 2.12e+06/9.72e+07 =  2% of the original kernel matrix.

torch.Size([25854, 2])
We keep 1.23e+07/3.53e+08 =  3% of the original kernel matrix.

torch.Size([31082, 2])
We keep 7.01e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([1801, 2])
We keep 4.73e+04/6.01e+05 =  7% of the original kernel matrix.

torch.Size([9824, 2])
We keep 6.99e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([2398, 2])
We keep 1.88e+05/2.11e+06 =  8% of the original kernel matrix.

torch.Size([10510, 2])
We keep 1.07e+06/3.51e+07 =  3% of the original kernel matrix.

torch.Size([13248, 2])
We keep 7.42e+06/1.36e+08 =  5% of the original kernel matrix.

torch.Size([21619, 2])
We keep 4.71e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([16863, 2])
We keep 8.36e+06/1.10e+08 =  7% of the original kernel matrix.

torch.Size([24929, 2])
We keep 4.13e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([16693, 2])
We keep 1.58e+07/1.92e+08 =  8% of the original kernel matrix.

torch.Size([24425, 2])
We keep 5.48e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([32731, 2])
We keep 1.39e+07/6.43e+08 =  2% of the original kernel matrix.

torch.Size([34892, 2])
We keep 8.71e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([299356, 2])
We keep 8.47e+08/6.52e+10 =  1% of the original kernel matrix.

torch.Size([108395, 2])
We keep 6.25e+07/6.17e+09 =  1% of the original kernel matrix.

torch.Size([9170, 2])
We keep 1.33e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([18077, 2])
We keep 2.83e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([3266, 2])
We keep 2.77e+05/3.24e+06 =  8% of the original kernel matrix.

torch.Size([11840, 2])
We keep 1.24e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([18489, 2])
We keep 3.36e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([26161, 2])
We keep 4.69e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([15159, 2])
We keep 2.10e+06/7.86e+07 =  2% of the original kernel matrix.

torch.Size([23542, 2])
We keep 3.85e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([2203, 2])
We keep 6.10e+04/8.32e+05 =  7% of the original kernel matrix.

torch.Size([10637, 2])
We keep 7.71e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([65281, 2])
We keep 7.24e+07/2.45e+09 =  2% of the original kernel matrix.

torch.Size([49065, 2])
We keep 1.50e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([10664, 2])
We keep 4.49e+06/6.61e+07 =  6% of the original kernel matrix.

torch.Size([19259, 2])
We keep 3.67e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([763, 2])
We keep 7.98e+03/5.76e+04 = 13% of the original kernel matrix.

torch.Size([7453, 2])
We keep 3.50e+05/5.80e+06 =  6% of the original kernel matrix.

torch.Size([27143, 2])
We keep 6.50e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([32142, 2])
We keep 6.79e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([21627, 2])
We keep 4.24e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([28549, 2])
We keep 5.40e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([850, 2])
We keep 1.39e+04/1.20e+05 = 11% of the original kernel matrix.

torch.Size([7451, 2])
We keep 4.32e+05/8.39e+06 =  5% of the original kernel matrix.

torch.Size([48931, 2])
We keep 3.55e+07/1.54e+09 =  2% of the original kernel matrix.

torch.Size([42321, 2])
We keep 1.25e+07/9.48e+08 =  1% of the original kernel matrix.

torch.Size([88537, 2])
We keep 5.11e+07/4.02e+09 =  1% of the original kernel matrix.

torch.Size([57196, 2])
We keep 1.85e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([90253, 2])
We keep 1.06e+08/4.48e+09 =  2% of the original kernel matrix.

torch.Size([57403, 2])
We keep 1.96e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([71902, 2])
We keep 8.03e+07/3.13e+09 =  2% of the original kernel matrix.

torch.Size([50943, 2])
We keep 1.69e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([96780, 2])
We keep 1.24e+08/6.12e+09 =  2% of the original kernel matrix.

torch.Size([58956, 2])
We keep 2.24e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([6375, 2])
We keep 1.35e+06/2.19e+07 =  6% of the original kernel matrix.

torch.Size([15120, 2])
We keep 2.42e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([7935, 2])
We keep 2.03e+06/2.81e+07 =  7% of the original kernel matrix.

torch.Size([17033, 2])
We keep 2.59e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([27079, 2])
We keep 6.28e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([32241, 2])
We keep 6.58e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([62317, 2])
We keep 2.44e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([48952, 2])
We keep 1.32e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([2924, 2])
We keep 9.98e+04/1.62e+06 =  6% of the original kernel matrix.

torch.Size([11716, 2])
We keep 9.84e+05/3.07e+07 =  3% of the original kernel matrix.

torch.Size([27866, 2])
We keep 1.65e+07/4.91e+08 =  3% of the original kernel matrix.

torch.Size([32320, 2])
We keep 7.34e+06/5.36e+08 =  1% of the original kernel matrix.

torch.Size([120907, 2])
We keep 3.27e+08/1.10e+10 =  2% of the original kernel matrix.

torch.Size([66270, 2])
We keep 2.71e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([80428, 2])
We keep 8.20e+07/4.22e+09 =  1% of the original kernel matrix.

torch.Size([53720, 2])
We keep 1.90e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([37693, 2])
We keep 3.48e+07/1.41e+09 =  2% of the original kernel matrix.

torch.Size([35958, 2])
We keep 1.20e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([33512, 2])
We keep 8.29e+07/1.94e+09 =  4% of the original kernel matrix.

torch.Size([31695, 2])
We keep 1.36e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([8205, 2])
We keep 8.17e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([17674, 2])
We keep 2.21e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([1793, 2])
We keep 3.91e+04/4.96e+05 =  7% of the original kernel matrix.

torch.Size([9795, 2])
We keep 6.64e+05/1.70e+07 =  3% of the original kernel matrix.

torch.Size([2277, 2])
We keep 9.73e+04/1.38e+06 =  7% of the original kernel matrix.

torch.Size([10443, 2])
We keep 9.31e+05/2.84e+07 =  3% of the original kernel matrix.

torch.Size([64913, 2])
We keep 2.53e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([49852, 2])
We keep 1.38e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([39871, 2])
We keep 1.16e+07/7.70e+08 =  1% of the original kernel matrix.

torch.Size([39200, 2])
We keep 9.27e+06/6.71e+08 =  1% of the original kernel matrix.

torch.Size([247509, 2])
We keep 4.54e+08/4.38e+10 =  1% of the original kernel matrix.

torch.Size([97424, 2])
We keep 5.13e+07/5.06e+09 =  1% of the original kernel matrix.

torch.Size([67847, 2])
We keep 9.08e+07/3.42e+09 =  2% of the original kernel matrix.

torch.Size([49042, 2])
We keep 1.75e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([35835, 2])
We keep 4.63e+07/7.80e+08 =  5% of the original kernel matrix.

torch.Size([36438, 2])
We keep 9.35e+06/6.75e+08 =  1% of the original kernel matrix.

torch.Size([12604, 2])
We keep 1.97e+06/8.24e+07 =  2% of the original kernel matrix.

torch.Size([21734, 2])
We keep 3.88e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([59501, 2])
We keep 2.89e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([47365, 2])
We keep 1.37e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([1636, 2])
We keep 4.00e+04/4.61e+05 =  8% of the original kernel matrix.

torch.Size([9374, 2])
We keep 6.39e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([4999, 2])
We keep 1.30e+06/9.06e+06 = 14% of the original kernel matrix.

torch.Size([14001, 2])
We keep 1.70e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([120626, 2])
We keep 1.07e+08/6.91e+09 =  1% of the original kernel matrix.

torch.Size([66769, 2])
We keep 2.34e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([4175, 2])
We keep 1.91e+05/3.42e+06 =  5% of the original kernel matrix.

torch.Size([13464, 2])
We keep 1.23e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([3298, 2])
We keep 1.64e+05/2.57e+06 =  6% of the original kernel matrix.

torch.Size([12096, 2])
We keep 1.13e+06/3.88e+07 =  2% of the original kernel matrix.

torch.Size([17200, 2])
We keep 2.87e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([25216, 2])
We keep 4.45e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([94145, 2])
We keep 2.66e+08/1.15e+10 =  2% of the original kernel matrix.

torch.Size([56377, 2])
We keep 2.92e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([9690, 2])
We keep 1.26e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([18870, 2])
We keep 2.81e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([78239, 2])
We keep 8.43e+07/3.97e+09 =  2% of the original kernel matrix.

torch.Size([53313, 2])
We keep 1.87e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([1853, 2])
We keep 7.44e+04/6.79e+05 = 10% of the original kernel matrix.

torch.Size([9661, 2])
We keep 7.31e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([3454, 2])
We keep 2.12e+05/2.71e+06 =  7% of the original kernel matrix.

torch.Size([12330, 2])
We keep 1.17e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([2306, 2])
We keep 8.77e+04/1.14e+06 =  7% of the original kernel matrix.

torch.Size([10529, 2])
We keep 8.76e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([5306, 2])
We keep 3.43e+05/7.05e+06 =  4% of the original kernel matrix.

torch.Size([14722, 2])
We keep 1.61e+06/6.42e+07 =  2% of the original kernel matrix.

torch.Size([33494, 2])
We keep 1.58e+07/5.65e+08 =  2% of the original kernel matrix.

torch.Size([36296, 2])
We keep 8.48e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([12866, 2])
We keep 3.49e+06/8.87e+07 =  3% of the original kernel matrix.

torch.Size([21067, 2])
We keep 4.09e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([3039, 2])
We keep 6.22e+05/2.55e+06 = 24% of the original kernel matrix.

torch.Size([11406, 2])
We keep 1.11e+06/3.86e+07 =  2% of the original kernel matrix.

torch.Size([59402, 2])
We keep 5.11e+07/2.08e+09 =  2% of the original kernel matrix.

torch.Size([47119, 2])
We keep 1.43e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([1535, 2])
We keep 3.52e+04/3.86e+05 =  9% of the original kernel matrix.

torch.Size([9360, 2])
We keep 5.93e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([48544, 2])
We keep 3.55e+07/1.27e+09 =  2% of the original kernel matrix.

torch.Size([43085, 2])
We keep 1.16e+07/8.63e+08 =  1% of the original kernel matrix.

torch.Size([4272, 2])
We keep 5.19e+05/6.95e+06 =  7% of the original kernel matrix.

torch.Size([13079, 2])
We keep 1.64e+06/6.37e+07 =  2% of the original kernel matrix.

torch.Size([16332, 2])
We keep 2.89e+06/9.68e+07 =  2% of the original kernel matrix.

torch.Size([24484, 2])
We keep 4.16e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([4339, 2])
We keep 3.26e+05/5.45e+06 =  5% of the original kernel matrix.

torch.Size([13390, 2])
We keep 1.49e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([9255, 2])
We keep 1.19e+06/2.79e+07 =  4% of the original kernel matrix.

torch.Size([18575, 2])
We keep 2.63e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([7521, 2])
We keep 6.96e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([17008, 2])
We keep 2.10e+06/9.39e+07 =  2% of the original kernel matrix.

torch.Size([19043, 2])
We keep 4.11e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([26662, 2])
We keep 4.97e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([67367, 2])
We keep 3.68e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([50118, 2])
We keep 1.50e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([81172, 2])
We keep 1.76e+08/5.48e+09 =  3% of the original kernel matrix.

torch.Size([53701, 2])
We keep 2.15e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([5083, 2])
We keep 2.95e+05/6.12e+06 =  4% of the original kernel matrix.

torch.Size([14527, 2])
We keep 1.53e+06/5.98e+07 =  2% of the original kernel matrix.

torch.Size([3576, 2])
We keep 6.03e+05/8.42e+06 =  7% of the original kernel matrix.

torch.Size([11637, 2])
We keep 1.73e+06/7.01e+07 =  2% of the original kernel matrix.

torch.Size([68925, 2])
We keep 8.97e+07/3.37e+09 =  2% of the original kernel matrix.

torch.Size([49277, 2])
We keep 1.70e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([5577, 2])
We keep 4.41e+05/8.24e+06 =  5% of the original kernel matrix.

torch.Size([14834, 2])
We keep 1.70e+06/6.94e+07 =  2% of the original kernel matrix.

torch.Size([427316, 2])
We keep 6.41e+08/8.81e+10 =  0% of the original kernel matrix.

torch.Size([131967, 2])
We keep 7.02e+07/7.17e+09 =  0% of the original kernel matrix.

torch.Size([910, 2])
We keep 1.33e+04/1.09e+05 = 12% of the original kernel matrix.

torch.Size([7570, 2])
We keep 4.14e+05/7.98e+06 =  5% of the original kernel matrix.

torch.Size([31102, 2])
We keep 1.26e+07/6.05e+08 =  2% of the original kernel matrix.

torch.Size([34293, 2])
We keep 8.60e+06/5.95e+08 =  1% of the original kernel matrix.

torch.Size([67566, 2])
We keep 3.03e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([50469, 2])
We keep 1.44e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([2632, 2])
We keep 7.92e+04/1.20e+06 =  6% of the original kernel matrix.

torch.Size([11342, 2])
We keep 8.79e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([51078, 2])
We keep 6.78e+07/2.29e+09 =  2% of the original kernel matrix.

torch.Size([42001, 2])
We keep 1.49e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([10012, 2])
We keep 3.14e+06/8.04e+07 =  3% of the original kernel matrix.

torch.Size([18606, 2])
We keep 3.89e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([532016, 2])
We keep 2.00e+09/1.91e+11 =  1% of the original kernel matrix.

torch.Size([141929, 2])
We keep 1.01e+08/1.06e+10 =  0% of the original kernel matrix.

torch.Size([5723, 2])
We keep 3.18e+05/7.10e+06 =  4% of the original kernel matrix.

torch.Size([15345, 2])
We keep 1.63e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([4981, 2])
We keep 3.83e+05/7.27e+06 =  5% of the original kernel matrix.

torch.Size([14125, 2])
We keep 1.60e+06/6.52e+07 =  2% of the original kernel matrix.

torch.Size([3936, 2])
We keep 5.19e+05/7.71e+06 =  6% of the original kernel matrix.

torch.Size([12189, 2])
We keep 1.67e+06/6.71e+07 =  2% of the original kernel matrix.

torch.Size([2625, 2])
We keep 8.87e+04/1.33e+06 =  6% of the original kernel matrix.

torch.Size([11226, 2])
We keep 9.17e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([9494, 2])
We keep 1.71e+06/4.47e+07 =  3% of the original kernel matrix.

torch.Size([18427, 2])
We keep 3.12e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([14512, 2])
We keep 2.64e+06/8.82e+07 =  2% of the original kernel matrix.

torch.Size([23312, 2])
We keep 3.94e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([6499, 2])
We keep 2.04e+06/2.75e+07 =  7% of the original kernel matrix.

torch.Size([15449, 2])
We keep 2.62e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([21897, 2])
We keep 7.66e+06/2.60e+08 =  2% of the original kernel matrix.

torch.Size([28525, 2])
We keep 6.09e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([12429, 2])
We keep 1.85e+06/5.84e+07 =  3% of the original kernel matrix.

torch.Size([21223, 2])
We keep 3.46e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([1580, 2])
We keep 3.95e+04/4.64e+05 =  8% of the original kernel matrix.

torch.Size([9263, 2])
We keep 6.53e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([7235, 2])
We keep 1.43e+06/2.69e+07 =  5% of the original kernel matrix.

torch.Size([15862, 2])
We keep 2.61e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([175356, 2])
We keep 3.64e+08/2.25e+10 =  1% of the original kernel matrix.

torch.Size([80361, 2])
We keep 3.92e+07/3.62e+09 =  1% of the original kernel matrix.

torch.Size([31010, 2])
We keep 2.16e+07/8.44e+08 =  2% of the original kernel matrix.

torch.Size([33143, 2])
We keep 9.86e+06/7.02e+08 =  1% of the original kernel matrix.

torch.Size([167220, 2])
We keep 1.65e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([80226, 2])
We keep 3.13e+07/2.83e+09 =  1% of the original kernel matrix.

torch.Size([12756, 2])
We keep 3.59e+06/6.60e+07 =  5% of the original kernel matrix.

torch.Size([21707, 2])
We keep 3.49e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([8856, 2])
We keep 9.70e+05/2.69e+07 =  3% of the original kernel matrix.

torch.Size([18190, 2])
We keep 2.57e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([2274, 2])
We keep 6.72e+04/9.41e+05 =  7% of the original kernel matrix.

torch.Size([10784, 2])
We keep 8.14e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([4051, 2])
We keep 1.53e+06/7.69e+06 = 19% of the original kernel matrix.

torch.Size([12457, 2])
We keep 1.54e+06/6.70e+07 =  2% of the original kernel matrix.

torch.Size([108594, 2])
We keep 2.15e+08/6.07e+09 =  3% of the original kernel matrix.

torch.Size([63474, 2])
We keep 2.15e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([12710, 2])
We keep 1.59e+06/5.13e+07 =  3% of the original kernel matrix.

torch.Size([21580, 2])
We keep 3.27e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([9590, 2])
We keep 1.25e+06/3.02e+07 =  4% of the original kernel matrix.

torch.Size([18846, 2])
We keep 2.70e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([10084, 2])
We keep 1.66e+06/4.24e+07 =  3% of the original kernel matrix.

torch.Size([19089, 2])
We keep 3.04e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([20694, 2])
We keep 7.73e+06/3.10e+08 =  2% of the original kernel matrix.

torch.Size([26684, 2])
We keep 6.60e+06/4.25e+08 =  1% of the original kernel matrix.

torch.Size([2660, 2])
We keep 1.25e+05/1.62e+06 =  7% of the original kernel matrix.

torch.Size([11201, 2])
We keep 9.62e+05/3.08e+07 =  3% of the original kernel matrix.

torch.Size([3313, 2])
We keep 1.64e+05/2.95e+06 =  5% of the original kernel matrix.

torch.Size([12171, 2])
We keep 1.19e+06/4.15e+07 =  2% of the original kernel matrix.

torch.Size([240684, 2])
We keep 8.02e+08/4.60e+10 =  1% of the original kernel matrix.

torch.Size([96107, 2])
We keep 5.27e+07/5.18e+09 =  1% of the original kernel matrix.

torch.Size([9923, 2])
We keep 1.68e+06/3.84e+07 =  4% of the original kernel matrix.

torch.Size([19127, 2])
We keep 2.97e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([8670, 2])
We keep 3.86e+06/5.87e+07 =  6% of the original kernel matrix.

torch.Size([17750, 2])
We keep 3.40e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([7594, 2])
We keep 5.89e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([17269, 2])
We keep 2.02e+06/9.34e+07 =  2% of the original kernel matrix.

torch.Size([49755, 2])
We keep 3.82e+07/1.48e+09 =  2% of the original kernel matrix.

torch.Size([43101, 2])
We keep 1.23e+07/9.31e+08 =  1% of the original kernel matrix.

torch.Size([146077, 2])
We keep 1.93e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([74113, 2])
We keep 3.08e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([113730, 2])
We keep 1.11e+08/7.21e+09 =  1% of the original kernel matrix.

torch.Size([64259, 2])
We keep 2.38e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([65890, 2])
We keep 4.33e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([49714, 2])
We keep 1.48e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([6216, 2])
We keep 1.54e+06/1.47e+07 = 10% of the original kernel matrix.

torch.Size([15685, 2])
We keep 2.08e+06/9.26e+07 =  2% of the original kernel matrix.

torch.Size([31941, 2])
We keep 7.36e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([34824, 2])
We keep 7.48e+06/5.16e+08 =  1% of the original kernel matrix.

torch.Size([13235, 2])
We keep 5.20e+06/1.27e+08 =  4% of the original kernel matrix.

torch.Size([21589, 2])
We keep 4.65e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([4523, 2])
We keep 2.40e+05/4.52e+06 =  5% of the original kernel matrix.

torch.Size([13957, 2])
We keep 1.39e+06/5.14e+07 =  2% of the original kernel matrix.

torch.Size([11424, 2])
We keep 1.81e+06/5.02e+07 =  3% of the original kernel matrix.

torch.Size([20615, 2])
We keep 3.22e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([3865, 2])
We keep 1.73e+05/3.22e+06 =  5% of the original kernel matrix.

torch.Size([13023, 2])
We keep 1.22e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([11560, 2])
We keep 5.71e+06/6.79e+07 =  8% of the original kernel matrix.

torch.Size([20730, 2])
We keep 3.67e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([6030, 2])
We keep 5.16e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([15410, 2])
We keep 1.88e+06/7.99e+07 =  2% of the original kernel matrix.

torch.Size([791, 2])
We keep 1.55e+04/1.10e+05 = 14% of the original kernel matrix.

torch.Size([7111, 2])
We keep 4.39e+05/8.00e+06 =  5% of the original kernel matrix.

torch.Size([11430, 2])
We keep 1.62e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([20448, 2])
We keep 3.20e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([4633, 2])
We keep 3.82e+05/7.78e+06 =  4% of the original kernel matrix.

torch.Size([13659, 2])
We keep 1.59e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([9692, 2])
We keep 9.67e+06/1.11e+08 =  8% of the original kernel matrix.

torch.Size([18368, 2])
We keep 4.24e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([4594, 2])
We keep 3.19e+05/6.28e+06 =  5% of the original kernel matrix.

torch.Size([13619, 2])
We keep 1.53e+06/6.05e+07 =  2% of the original kernel matrix.

torch.Size([24475, 2])
We keep 8.03e+06/2.45e+08 =  3% of the original kernel matrix.

torch.Size([30354, 2])
We keep 5.89e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([5116, 2])
We keep 1.04e+07/3.91e+07 = 26% of the original kernel matrix.

torch.Size([12690, 2])
We keep 2.77e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([11607, 2])
We keep 1.74e+06/4.97e+07 =  3% of the original kernel matrix.

torch.Size([20751, 2])
We keep 3.18e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([2085, 2])
We keep 8.44e+04/1.01e+06 =  8% of the original kernel matrix.

torch.Size([10121, 2])
We keep 8.48e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([6775, 2])
We keep 5.18e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([16190, 2])
We keep 1.93e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([16153, 2])
We keep 3.07e+06/8.31e+07 =  3% of the original kernel matrix.

torch.Size([24356, 2])
We keep 3.88e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([110440, 2])
We keep 1.56e+08/7.69e+09 =  2% of the original kernel matrix.

torch.Size([63928, 2])
We keep 2.47e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([6199, 2])
We keep 4.61e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([15595, 2])
We keep 1.88e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([1226, 2])
We keep 1.97e+04/1.88e+05 = 10% of the original kernel matrix.

torch.Size([8681, 2])
We keep 4.98e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([3034, 2])
We keep 1.37e+05/2.33e+06 =  5% of the original kernel matrix.

torch.Size([11650, 2])
We keep 1.11e+06/3.69e+07 =  2% of the original kernel matrix.

torch.Size([119714, 2])
We keep 3.18e+08/1.05e+10 =  3% of the original kernel matrix.

torch.Size([66358, 2])
We keep 2.79e+07/2.48e+09 =  1% of the original kernel matrix.

torch.Size([200202, 2])
We keep 2.66e+08/2.27e+10 =  1% of the original kernel matrix.

torch.Size([88752, 2])
We keep 3.83e+07/3.64e+09 =  1% of the original kernel matrix.

torch.Size([10944, 2])
We keep 4.67e+06/1.09e+08 =  4% of the original kernel matrix.

torch.Size([18940, 2])
We keep 4.39e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([5811, 2])
We keep 5.24e+05/9.91e+06 =  5% of the original kernel matrix.

torch.Size([14952, 2])
We keep 1.81e+06/7.61e+07 =  2% of the original kernel matrix.

torch.Size([20373, 2])
We keep 9.84e+06/3.30e+08 =  2% of the original kernel matrix.

torch.Size([26826, 2])
We keep 6.76e+06/4.39e+08 =  1% of the original kernel matrix.

torch.Size([80012, 2])
We keep 4.10e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([54757, 2])
We keep 1.68e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([2364, 2])
We keep 8.10e+04/1.13e+06 =  7% of the original kernel matrix.

torch.Size([10756, 2])
We keep 8.57e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([6827, 2])
We keep 7.60e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([16294, 2])
We keep 2.17e+06/9.84e+07 =  2% of the original kernel matrix.

torch.Size([8183, 2])
We keep 6.48e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([17548, 2])
We keep 2.16e+06/9.92e+07 =  2% of the original kernel matrix.

torch.Size([18961, 2])
We keep 1.96e+07/4.41e+08 =  4% of the original kernel matrix.

torch.Size([25182, 2])
We keep 7.59e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([5353, 2])
We keep 5.11e+05/8.92e+06 =  5% of the original kernel matrix.

torch.Size([14813, 2])
We keep 1.72e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([162577, 2])
We keep 1.69e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([78957, 2])
We keep 3.14e+07/2.83e+09 =  1% of the original kernel matrix.

torch.Size([4657, 2])
We keep 2.65e+05/5.19e+06 =  5% of the original kernel matrix.

torch.Size([13949, 2])
We keep 1.44e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([4203, 2])
We keep 2.18e+05/4.02e+06 =  5% of the original kernel matrix.

torch.Size([13541, 2])
We keep 1.31e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([1132, 2])
We keep 1.92e+04/2.03e+05 =  9% of the original kernel matrix.

torch.Size([8376, 2])
We keep 5.05e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([11860, 2])
We keep 6.16e+06/5.84e+07 = 10% of the original kernel matrix.

torch.Size([20683, 2])
We keep 3.40e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([7773, 2])
We keep 8.05e+05/1.60e+07 =  5% of the original kernel matrix.

torch.Size([17434, 2])
We keep 2.11e+06/9.68e+07 =  2% of the original kernel matrix.

torch.Size([13291, 2])
We keep 2.21e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([22044, 2])
We keep 3.85e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([185659, 2])
We keep 4.94e+08/2.30e+10 =  2% of the original kernel matrix.

torch.Size([83769, 2])
We keep 3.96e+07/3.66e+09 =  1% of the original kernel matrix.

torch.Size([534448, 2])
We keep 1.04e+09/1.30e+11 =  0% of the original kernel matrix.

torch.Size([146236, 2])
We keep 8.47e+07/8.73e+09 =  0% of the original kernel matrix.

torch.Size([4863, 2])
We keep 3.85e+05/6.08e+06 =  6% of the original kernel matrix.

torch.Size([14137, 2])
We keep 1.52e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([3161, 2])
We keep 1.31e+05/2.19e+06 =  5% of the original kernel matrix.

torch.Size([12025, 2])
We keep 1.07e+06/3.58e+07 =  2% of the original kernel matrix.

torch.Size([14712, 2])
We keep 1.97e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([23201, 2])
We keep 3.65e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([123662, 2])
We keep 6.23e+08/9.67e+09 =  6% of the original kernel matrix.

torch.Size([68502, 2])
We keep 2.54e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([21532, 2])
We keep 1.60e+07/2.21e+08 =  7% of the original kernel matrix.

torch.Size([28253, 2])
We keep 5.66e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([1014, 2])
We keep 1.70e+04/1.41e+05 = 12% of the original kernel matrix.

torch.Size([7925, 2])
We keep 4.52e+05/9.06e+06 =  4% of the original kernel matrix.

torch.Size([2908, 2])
We keep 1.23e+05/2.17e+06 =  5% of the original kernel matrix.

torch.Size([11392, 2])
We keep 1.06e+06/3.56e+07 =  2% of the original kernel matrix.

torch.Size([24009, 2])
We keep 2.19e+07/2.91e+08 =  7% of the original kernel matrix.

torch.Size([29819, 2])
We keep 6.50e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([1464, 2])
We keep 3.11e+04/3.38e+05 =  9% of the original kernel matrix.

torch.Size([9079, 2])
We keep 5.78e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([38834, 2])
We keep 3.46e+07/1.57e+09 =  2% of the original kernel matrix.

torch.Size([35879, 2])
We keep 1.28e+07/9.57e+08 =  1% of the original kernel matrix.

torch.Size([2461, 2])
We keep 7.58e+04/1.14e+06 =  6% of the original kernel matrix.

torch.Size([10888, 2])
We keep 8.55e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([16346, 2])
We keep 2.04e+06/8.26e+07 =  2% of the original kernel matrix.

torch.Size([24389, 2])
We keep 3.91e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([6064, 2])
We keep 4.75e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([15429, 2])
We keep 1.89e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([3793, 2])
We keep 1.55e+05/2.76e+06 =  5% of the original kernel matrix.

torch.Size([12820, 2])
We keep 1.16e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([1897, 2])
We keep 4.69e+04/6.07e+05 =  7% of the original kernel matrix.

torch.Size([9926, 2])
We keep 7.09e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([119116, 2])
We keep 7.81e+07/6.54e+09 =  1% of the original kernel matrix.

torch.Size([66471, 2])
We keep 2.24e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([205173, 2])
We keep 2.53e+08/2.20e+10 =  1% of the original kernel matrix.

torch.Size([89273, 2])
We keep 3.83e+07/3.58e+09 =  1% of the original kernel matrix.

torch.Size([91723, 2])
We keep 9.65e+07/4.81e+09 =  2% of the original kernel matrix.

torch.Size([58288, 2])
We keep 2.02e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([13657, 2])
We keep 4.92e+06/1.04e+08 =  4% of the original kernel matrix.

torch.Size([21814, 2])
We keep 4.33e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([23081, 2])
We keep 4.34e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([29348, 2])
We keep 5.47e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([6905, 2])
We keep 6.25e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([16357, 2])
We keep 2.04e+06/8.99e+07 =  2% of the original kernel matrix.

torch.Size([6420, 2])
We keep 5.55e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([15810, 2])
We keep 1.92e+06/8.17e+07 =  2% of the original kernel matrix.

torch.Size([46113, 2])
We keep 1.70e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([42285, 2])
We keep 1.07e+07/7.91e+08 =  1% of the original kernel matrix.

torch.Size([25606, 2])
We keep 6.70e+06/2.93e+08 =  2% of the original kernel matrix.

torch.Size([31314, 2])
We keep 6.43e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([61728, 2])
We keep 1.08e+08/2.99e+09 =  3% of the original kernel matrix.

torch.Size([47524, 2])
We keep 1.62e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([10268, 2])
We keep 1.57e+06/3.86e+07 =  4% of the original kernel matrix.

torch.Size([19375, 2])
We keep 2.98e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([441908, 2])
We keep 1.26e+09/1.11e+11 =  1% of the original kernel matrix.

torch.Size([133143, 2])
We keep 7.93e+07/8.05e+09 =  0% of the original kernel matrix.

torch.Size([9145, 2])
We keep 9.18e+05/2.23e+07 =  4% of the original kernel matrix.

torch.Size([18515, 2])
We keep 2.41e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([32311, 2])
We keep 1.14e+07/5.62e+08 =  2% of the original kernel matrix.

torch.Size([35829, 2])
We keep 8.47e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([95808, 2])
We keep 6.98e+07/4.85e+09 =  1% of the original kernel matrix.

torch.Size([59763, 2])
We keep 1.99e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([36271, 2])
We keep 1.02e+07/6.27e+08 =  1% of the original kernel matrix.

torch.Size([37882, 2])
We keep 8.52e+06/6.05e+08 =  1% of the original kernel matrix.

torch.Size([24981, 2])
We keep 1.95e+07/4.09e+08 =  4% of the original kernel matrix.

torch.Size([30437, 2])
We keep 7.10e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([77036, 2])
We keep 9.17e+07/4.05e+09 =  2% of the original kernel matrix.

torch.Size([52669, 2])
We keep 1.88e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([22403, 2])
We keep 4.37e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([28877, 2])
We keep 5.47e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([12813, 2])
We keep 3.65e+06/8.55e+07 =  4% of the original kernel matrix.

torch.Size([21361, 2])
We keep 3.91e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([4882, 2])
We keep 6.74e+05/6.35e+06 = 10% of the original kernel matrix.

torch.Size([14281, 2])
We keep 1.50e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([20253, 2])
We keep 8.52e+06/2.30e+08 =  3% of the original kernel matrix.

torch.Size([27337, 2])
We keep 5.79e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([9829, 2])
We keep 2.30e+06/4.18e+07 =  5% of the original kernel matrix.

torch.Size([19278, 2])
We keep 3.02e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([58431, 2])
We keep 3.20e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([47233, 2])
We keep 1.29e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([684268, 2])
We keep 2.00e+09/2.17e+11 =  0% of the original kernel matrix.

torch.Size([166287, 2])
We keep 1.07e+08/1.13e+10 =  0% of the original kernel matrix.

torch.Size([56785, 2])
We keep 8.68e+07/2.23e+09 =  3% of the original kernel matrix.

torch.Size([45658, 2])
We keep 1.46e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([95165, 2])
We keep 1.32e+08/7.31e+09 =  1% of the original kernel matrix.

torch.Size([56168, 2])
We keep 2.40e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([23515, 2])
We keep 2.05e+07/4.85e+08 =  4% of the original kernel matrix.

torch.Size([29143, 2])
We keep 8.01e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([11652, 2])
We keep 5.34e+06/1.08e+08 =  4% of the original kernel matrix.

torch.Size([19560, 2])
We keep 4.39e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([33962, 2])
We keep 9.68e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([36022, 2])
We keep 8.10e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([6036, 2])
We keep 9.28e+05/1.56e+07 =  5% of the original kernel matrix.

torch.Size([15008, 2])
We keep 2.10e+06/9.56e+07 =  2% of the original kernel matrix.

torch.Size([6144, 2])
We keep 5.41e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([15591, 2])
We keep 1.75e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([127542, 2])
We keep 9.17e+07/8.08e+09 =  1% of the original kernel matrix.

torch.Size([68702, 2])
We keep 2.47e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([10979, 2])
We keep 5.47e+06/1.26e+08 =  4% of the original kernel matrix.

torch.Size([18334, 2])
We keep 4.65e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([82101, 2])
We keep 1.37e+08/5.94e+09 =  2% of the original kernel matrix.

torch.Size([53455, 2])
We keep 2.22e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([115366, 2])
We keep 1.54e+09/1.84e+10 =  8% of the original kernel matrix.

torch.Size([63128, 2])
We keep 3.53e+07/3.28e+09 =  1% of the original kernel matrix.

torch.Size([14002, 2])
We keep 1.96e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([22814, 2])
We keep 3.63e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([2570, 2])
We keep 8.62e+04/1.28e+06 =  6% of the original kernel matrix.

torch.Size([11119, 2])
We keep 9.04e+05/2.74e+07 =  3% of the original kernel matrix.

torch.Size([35589, 2])
We keep 3.17e+07/6.97e+08 =  4% of the original kernel matrix.

torch.Size([36651, 2])
We keep 9.07e+06/6.38e+08 =  1% of the original kernel matrix.

torch.Size([35777, 2])
We keep 1.07e+07/6.31e+08 =  1% of the original kernel matrix.

torch.Size([37512, 2])
We keep 8.73e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([2843, 2])
We keep 1.44e+05/2.26e+06 =  6% of the original kernel matrix.

torch.Size([11386, 2])
We keep 1.06e+06/3.64e+07 =  2% of the original kernel matrix.

torch.Size([252061, 2])
We keep 2.79e+08/3.16e+10 =  0% of the original kernel matrix.

torch.Size([99521, 2])
We keep 4.47e+07/4.30e+09 =  1% of the original kernel matrix.

torch.Size([18901, 2])
We keep 8.42e+06/2.27e+08 =  3% of the original kernel matrix.

torch.Size([26401, 2])
We keep 5.49e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([19100, 2])
We keep 5.97e+06/1.71e+08 =  3% of the original kernel matrix.

torch.Size([26407, 2])
We keep 5.23e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([4945, 2])
We keep 2.54e+05/5.46e+06 =  4% of the original kernel matrix.

torch.Size([14480, 2])
We keep 1.42e+06/5.65e+07 =  2% of the original kernel matrix.

torch.Size([1637, 2])
We keep 4.36e+04/5.45e+05 =  7% of the original kernel matrix.

torch.Size([9338, 2])
We keep 6.87e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([2017, 2])
We keep 5.23e+04/6.86e+05 =  7% of the original kernel matrix.

torch.Size([10266, 2])
We keep 7.29e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([1757, 2])
We keep 3.79e+04/5.01e+05 =  7% of the original kernel matrix.

torch.Size([9613, 2])
We keep 6.60e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([7632, 2])
We keep 9.95e+05/1.75e+07 =  5% of the original kernel matrix.

torch.Size([16979, 2])
We keep 2.21e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([11044, 2])
We keep 2.02e+06/4.40e+07 =  4% of the original kernel matrix.

torch.Size([20088, 2])
We keep 3.06e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([2923618, 2])
We keep 2.41e+10/3.56e+12 =  0% of the original kernel matrix.

torch.Size([358747, 2])
We keep 3.88e+08/4.56e+10 =  0% of the original kernel matrix.

torch.Size([5561, 2])
We keep 4.55e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([14698, 2])
We keep 1.83e+06/7.89e+07 =  2% of the original kernel matrix.

torch.Size([10844, 2])
We keep 1.20e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([19899, 2])
We keep 2.80e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([5096, 2])
We keep 4.05e+05/8.28e+06 =  4% of the original kernel matrix.

torch.Size([14294, 2])
We keep 1.70e+06/6.95e+07 =  2% of the original kernel matrix.

torch.Size([6691, 2])
We keep 5.65e+05/1.11e+07 =  5% of the original kernel matrix.

torch.Size([16108, 2])
We keep 1.92e+06/8.05e+07 =  2% of the original kernel matrix.

torch.Size([11811, 2])
We keep 1.60e+06/4.66e+07 =  3% of the original kernel matrix.

torch.Size([20892, 2])
We keep 3.21e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([84784, 2])
We keep 1.22e+08/6.03e+09 =  2% of the original kernel matrix.

torch.Size([52752, 2])
We keep 2.22e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([4651, 2])
We keep 2.59e+05/5.19e+06 =  4% of the original kernel matrix.

torch.Size([13949, 2])
We keep 1.44e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([448637, 2])
We keep 7.39e+08/9.54e+10 =  0% of the original kernel matrix.

torch.Size([134996, 2])
We keep 7.30e+07/7.47e+09 =  0% of the original kernel matrix.

torch.Size([4264, 2])
We keep 1.98e+05/3.97e+06 =  4% of the original kernel matrix.

torch.Size([13590, 2])
We keep 1.32e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([144118, 2])
We keep 1.57e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([73781, 2])
We keep 2.80e+07/2.48e+09 =  1% of the original kernel matrix.

torch.Size([17319, 2])
We keep 1.32e+07/1.62e+08 =  8% of the original kernel matrix.

torch.Size([25223, 2])
We keep 5.09e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([87392, 2])
We keep 8.45e+07/5.03e+09 =  1% of the original kernel matrix.

torch.Size([56401, 2])
We keep 2.05e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([208794, 2])
We keep 4.87e+08/3.38e+10 =  1% of the original kernel matrix.

torch.Size([89894, 2])
We keep 4.66e+07/4.44e+09 =  1% of the original kernel matrix.

torch.Size([9560, 2])
We keep 1.24e+06/2.63e+07 =  4% of the original kernel matrix.

torch.Size([18824, 2])
We keep 2.60e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([2973, 2])
We keep 1.64e+05/2.20e+06 =  7% of the original kernel matrix.

torch.Size([11546, 2])
We keep 1.08e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([68963, 2])
We keep 3.19e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([51065, 2])
We keep 1.47e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([6547, 2])
We keep 5.02e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([16069, 2])
We keep 1.89e+06/8.24e+07 =  2% of the original kernel matrix.

torch.Size([11127, 2])
We keep 1.87e+06/5.32e+07 =  3% of the original kernel matrix.

torch.Size([19871, 2])
We keep 3.29e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([16110, 2])
We keep 3.03e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([24226, 2])
We keep 4.31e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([4938, 2])
We keep 4.26e+05/9.46e+06 =  4% of the original kernel matrix.

torch.Size([14148, 2])
We keep 1.69e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([32545, 2])
We keep 1.37e+07/5.51e+08 =  2% of the original kernel matrix.

torch.Size([33933, 2])
We keep 7.87e+06/5.68e+08 =  1% of the original kernel matrix.

torch.Size([10080, 2])
We keep 2.41e+06/4.40e+07 =  5% of the original kernel matrix.

torch.Size([19107, 2])
We keep 3.18e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([103053, 2])
We keep 1.19e+08/6.48e+09 =  1% of the original kernel matrix.

torch.Size([61187, 2])
We keep 2.29e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([128468, 2])
We keep 2.03e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([68684, 2])
We keep 2.74e+07/2.44e+09 =  1% of the original kernel matrix.

torch.Size([8518, 2])
We keep 1.09e+06/2.34e+07 =  4% of the original kernel matrix.

torch.Size([17671, 2])
We keep 2.46e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([5173, 2])
We keep 4.14e+05/8.44e+06 =  4% of the original kernel matrix.

torch.Size([14246, 2])
We keep 1.71e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([45424, 2])
We keep 7.18e+07/1.98e+09 =  3% of the original kernel matrix.

torch.Size([40089, 2])
We keep 1.40e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([11842, 2])
We keep 2.99e+06/7.86e+07 =  3% of the original kernel matrix.

torch.Size([20439, 2])
We keep 3.88e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([21426, 2])
We keep 1.27e+07/2.26e+08 =  5% of the original kernel matrix.

torch.Size([27956, 2])
We keep 5.62e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([6211, 2])
We keep 1.18e+06/1.28e+07 =  9% of the original kernel matrix.

torch.Size([15356, 2])
We keep 1.97e+06/8.65e+07 =  2% of the original kernel matrix.

torch.Size([25034, 2])
We keep 2.31e+07/5.84e+08 =  3% of the original kernel matrix.

torch.Size([29623, 2])
We keep 8.33e+06/5.84e+08 =  1% of the original kernel matrix.

torch.Size([17892, 2])
We keep 6.32e+06/1.54e+08 =  4% of the original kernel matrix.

torch.Size([25400, 2])
We keep 4.89e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([24755, 2])
We keep 3.44e+07/7.77e+08 =  4% of the original kernel matrix.

torch.Size([28648, 2])
We keep 9.56e+06/6.74e+08 =  1% of the original kernel matrix.

torch.Size([32933, 2])
We keep 1.27e+08/1.16e+09 = 10% of the original kernel matrix.

torch.Size([34072, 2])
We keep 1.09e+07/8.24e+08 =  1% of the original kernel matrix.

torch.Size([7662, 2])
We keep 6.19e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([17118, 2])
We keep 2.07e+06/9.15e+07 =  2% of the original kernel matrix.

torch.Size([4074, 2])
We keep 2.28e+05/4.01e+06 =  5% of the original kernel matrix.

torch.Size([13158, 2])
We keep 1.31e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([13399, 2])
We keep 1.82e+06/5.83e+07 =  3% of the original kernel matrix.

torch.Size([22077, 2])
We keep 3.46e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([12305, 2])
We keep 5.11e+06/1.70e+08 =  3% of the original kernel matrix.

torch.Size([20184, 2])
We keep 5.16e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([30727, 2])
We keep 2.11e+07/6.09e+08 =  3% of the original kernel matrix.

torch.Size([33890, 2])
We keep 8.62e+06/5.96e+08 =  1% of the original kernel matrix.

torch.Size([91611, 2])
We keep 9.97e+07/4.01e+09 =  2% of the original kernel matrix.

torch.Size([58142, 2])
We keep 1.86e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([9620, 2])
We keep 9.83e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([19027, 2])
We keep 2.52e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([21347, 2])
We keep 3.86e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([28290, 2])
We keep 5.31e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([20765, 2])
We keep 5.96e+06/1.96e+08 =  3% of the original kernel matrix.

torch.Size([27653, 2])
We keep 5.43e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([75940, 2])
We keep 1.15e+08/2.98e+09 =  3% of the original kernel matrix.

torch.Size([53382, 2])
We keep 1.59e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([26704, 2])
We keep 1.44e+07/3.99e+08 =  3% of the original kernel matrix.

torch.Size([31703, 2])
We keep 7.41e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([48735, 2])
We keep 1.71e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([43431, 2])
We keep 1.12e+07/8.40e+08 =  1% of the original kernel matrix.

torch.Size([11864, 2])
We keep 1.13e+07/8.14e+07 = 13% of the original kernel matrix.

torch.Size([20820, 2])
We keep 3.89e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([20945, 2])
We keep 4.34e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([27951, 2])
We keep 5.15e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([1662, 2])
We keep 6.75e+04/8.39e+05 =  8% of the original kernel matrix.

torch.Size([9156, 2])
We keep 7.65e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([8284, 2])
We keep 7.33e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([17752, 2])
We keep 2.31e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([23536, 2])
We keep 4.66e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([29924, 2])
We keep 5.57e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([13971, 2])
We keep 3.40e+06/9.35e+07 =  3% of the original kernel matrix.

torch.Size([22619, 2])
We keep 4.04e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([23150, 2])
We keep 4.75e+06/2.10e+08 =  2% of the original kernel matrix.

torch.Size([29548, 2])
We keep 5.53e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([2495, 2])
We keep 7.58e+04/1.04e+06 =  7% of the original kernel matrix.

torch.Size([11063, 2])
We keep 8.40e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([4868, 2])
We keep 2.71e+05/5.21e+06 =  5% of the original kernel matrix.

torch.Size([14344, 2])
We keep 1.43e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([18535, 2])
We keep 1.04e+07/1.60e+08 =  6% of the original kernel matrix.

torch.Size([26039, 2])
We keep 4.92e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([11810, 2])
We keep 2.01e+06/5.24e+07 =  3% of the original kernel matrix.

torch.Size([20643, 2])
We keep 3.29e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([12619, 2])
We keep 1.68e+06/5.16e+07 =  3% of the original kernel matrix.

torch.Size([21503, 2])
We keep 3.26e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([2025, 2])
We keep 5.97e+04/8.24e+05 =  7% of the original kernel matrix.

torch.Size([10131, 2])
We keep 7.86e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([2987, 2])
We keep 1.55e+05/2.62e+06 =  5% of the original kernel matrix.

torch.Size([11375, 2])
We keep 1.15e+06/3.91e+07 =  2% of the original kernel matrix.

torch.Size([12618, 2])
We keep 3.49e+06/7.18e+07 =  4% of the original kernel matrix.

torch.Size([21379, 2])
We keep 3.79e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([15342, 2])
We keep 2.19e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([23665, 2])
We keep 3.77e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([39773, 2])
We keep 2.13e+07/8.65e+08 =  2% of the original kernel matrix.

torch.Size([38876, 2])
We keep 9.81e+06/7.11e+08 =  1% of the original kernel matrix.

torch.Size([1814, 2])
We keep 5.53e+04/5.67e+05 =  9% of the original kernel matrix.

torch.Size([9803, 2])
We keep 6.92e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([6479, 2])
We keep 6.01e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([15793, 2])
We keep 2.04e+06/8.94e+07 =  2% of the original kernel matrix.

torch.Size([4820, 2])
We keep 7.14e+05/1.04e+07 =  6% of the original kernel matrix.

torch.Size([13295, 2])
We keep 1.86e+06/7.80e+07 =  2% of the original kernel matrix.

torch.Size([5923, 2])
We keep 4.78e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([15257, 2])
We keep 1.85e+06/7.86e+07 =  2% of the original kernel matrix.

torch.Size([137662, 2])
We keep 2.09e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([72068, 2])
We keep 2.87e+07/2.61e+09 =  1% of the original kernel matrix.

torch.Size([12223, 2])
We keep 7.17e+06/9.77e+07 =  7% of the original kernel matrix.

torch.Size([20805, 2])
We keep 4.05e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([29455, 2])
We keep 1.44e+07/5.62e+08 =  2% of the original kernel matrix.

torch.Size([33460, 2])
We keep 8.38e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([92838, 2])
We keep 5.58e+07/3.95e+09 =  1% of the original kernel matrix.

torch.Size([58270, 2])
We keep 1.84e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([49688, 2])
We keep 9.14e+07/1.85e+09 =  4% of the original kernel matrix.

torch.Size([42778, 2])
We keep 1.36e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([136787, 2])
We keep 1.75e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([71848, 2])
We keep 2.84e+07/2.48e+09 =  1% of the original kernel matrix.

torch.Size([101514, 2])
We keep 6.11e+07/4.67e+09 =  1% of the original kernel matrix.

torch.Size([60894, 2])
We keep 1.96e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([2370, 2])
We keep 1.28e+05/1.24e+06 = 10% of the original kernel matrix.

torch.Size([10687, 2])
We keep 8.75e+05/2.69e+07 =  3% of the original kernel matrix.

torch.Size([16907, 2])
We keep 2.95e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([24803, 2])
We keep 4.42e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([10676, 2])
We keep 1.18e+06/2.94e+07 =  4% of the original kernel matrix.

torch.Size([19945, 2])
We keep 2.63e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([30175, 2])
We keep 1.08e+07/4.45e+08 =  2% of the original kernel matrix.

torch.Size([33904, 2])
We keep 7.48e+06/5.10e+08 =  1% of the original kernel matrix.

torch.Size([96717, 2])
We keep 1.10e+08/5.66e+09 =  1% of the original kernel matrix.

torch.Size([59708, 2])
We keep 2.16e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([6482, 2])
We keep 1.19e+06/2.11e+07 =  5% of the original kernel matrix.

torch.Size([15328, 2])
We keep 2.35e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([32912, 2])
We keep 1.11e+07/5.29e+08 =  2% of the original kernel matrix.

torch.Size([34143, 2])
We keep 7.73e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([4799, 2])
We keep 2.21e+05/4.72e+06 =  4% of the original kernel matrix.

torch.Size([14263, 2])
We keep 1.38e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([6431, 2])
We keep 2.18e+06/3.75e+07 =  5% of the original kernel matrix.

torch.Size([14945, 2])
We keep 2.86e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([4395, 2])
We keep 5.22e+05/4.91e+06 = 10% of the original kernel matrix.

torch.Size([13703, 2])
We keep 1.34e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([6117, 2])
We keep 6.10e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([15449, 2])
We keep 1.92e+06/8.38e+07 =  2% of the original kernel matrix.

torch.Size([140164, 2])
We keep 2.03e+08/1.07e+10 =  1% of the original kernel matrix.

torch.Size([72761, 2])
We keep 2.82e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([467222, 2])
We keep 2.65e+09/1.25e+11 =  2% of the original kernel matrix.

torch.Size([137006, 2])
We keep 8.36e+07/8.55e+09 =  0% of the original kernel matrix.

torch.Size([13237, 2])
We keep 7.72e+06/1.43e+08 =  5% of the original kernel matrix.

torch.Size([21160, 2])
We keep 4.90e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([12716, 2])
We keep 1.99e+06/6.04e+07 =  3% of the original kernel matrix.

torch.Size([21605, 2])
We keep 3.40e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([18136, 2])
We keep 3.49e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([25917, 2])
We keep 4.63e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([25920, 2])
We keep 1.68e+07/3.42e+08 =  4% of the original kernel matrix.

torch.Size([31143, 2])
We keep 6.63e+06/4.47e+08 =  1% of the original kernel matrix.

torch.Size([9021, 2])
We keep 9.65e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([18281, 2])
We keep 2.57e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([5825, 2])
We keep 3.81e+05/7.44e+06 =  5% of the original kernel matrix.

torch.Size([15397, 2])
We keep 1.61e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([22565, 2])
We keep 4.96e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([29110, 2])
We keep 5.38e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([86023, 2])
We keep 5.29e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([56397, 2])
We keep 1.77e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([3477, 2])
We keep 3.20e+05/4.07e+06 =  7% of the original kernel matrix.

torch.Size([12068, 2])
We keep 1.36e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([44610, 2])
We keep 3.51e+07/1.30e+09 =  2% of the original kernel matrix.

torch.Size([40525, 2])
We keep 1.18e+07/8.73e+08 =  1% of the original kernel matrix.

torch.Size([2429, 2])
We keep 1.36e+05/1.04e+06 = 13% of the original kernel matrix.

torch.Size([10938, 2])
We keep 8.24e+05/2.46e+07 =  3% of the original kernel matrix.

time for making ranges is 2.6471381187438965
Sorting X and nu_X
time for sorting X is 0.06517696380615234
Sorting Z and nu_Z
time for sorting Z is 0.0002696514129638672
Starting Optim
sum tnu_Z before tensor(14787409., device='cuda:0')
c= tensor(268.0565, device='cuda:0')
c= tensor(15719.6914, device='cuda:0')
c= tensor(17351.7617, device='cuda:0')
c= tensor(36466.4258, device='cuda:0')
c= tensor(108254.3906, device='cuda:0')
c= tensor(126764.2188, device='cuda:0')
c= tensor(454526.1875, device='cuda:0')
c= tensor(510685.5000, device='cuda:0')
c= tensor(515116.2188, device='cuda:0')
c= tensor(686169.1250, device='cuda:0')
c= tensor(690298.3125, device='cuda:0')
c= tensor(2361367.2500, device='cuda:0')
c= tensor(2370067., device='cuda:0')
c= tensor(9386142., device='cuda:0')
c= tensor(9435209., device='cuda:0')
c= tensor(9462002., device='cuda:0')
c= tensor(9911540., device='cuda:0')
c= tensor(10073552., device='cuda:0')
c= tensor(13102500., device='cuda:0')
c= tensor(14932127., device='cuda:0')
c= tensor(14938178., device='cuda:0')
c= tensor(19169820., device='cuda:0')
c= tensor(19181360., device='cuda:0')
c= tensor(19485752., device='cuda:0')
c= tensor(19487242., device='cuda:0')
c= tensor(20153430., device='cuda:0')
c= tensor(20600554., device='cuda:0')
c= tensor(20606068., device='cuda:0')
c= tensor(21281184., device='cuda:0')
c= tensor(85397904., device='cuda:0')
c= tensor(85403952., device='cuda:0')
c= tensor(1.9298e+08, device='cuda:0')
c= tensor(1.9303e+08, device='cuda:0')
c= tensor(1.9304e+08, device='cuda:0')
c= tensor(1.9304e+08, device='cuda:0')
c= tensor(1.9438e+08, device='cuda:0')
c= tensor(1.9695e+08, device='cuda:0')
c= tensor(1.9695e+08, device='cuda:0')
c= tensor(1.9696e+08, device='cuda:0')
c= tensor(1.9696e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9698e+08, device='cuda:0')
c= tensor(1.9698e+08, device='cuda:0')
c= tensor(1.9700e+08, device='cuda:0')
c= tensor(1.9701e+08, device='cuda:0')
c= tensor(1.9702e+08, device='cuda:0')
c= tensor(1.9702e+08, device='cuda:0')
c= tensor(1.9703e+08, device='cuda:0')
c= tensor(1.9703e+08, device='cuda:0')
c= tensor(1.9704e+08, device='cuda:0')
c= tensor(1.9704e+08, device='cuda:0')
c= tensor(1.9704e+08, device='cuda:0')
c= tensor(1.9705e+08, device='cuda:0')
c= tensor(1.9705e+08, device='cuda:0')
c= tensor(1.9707e+08, device='cuda:0')
c= tensor(1.9707e+08, device='cuda:0')
c= tensor(1.9710e+08, device='cuda:0')
c= tensor(1.9710e+08, device='cuda:0')
c= tensor(1.9710e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9712e+08, device='cuda:0')
c= tensor(1.9712e+08, device='cuda:0')
c= tensor(1.9712e+08, device='cuda:0')
c= tensor(1.9713e+08, device='cuda:0')
c= tensor(1.9713e+08, device='cuda:0')
c= tensor(1.9713e+08, device='cuda:0')
c= tensor(1.9713e+08, device='cuda:0')
c= tensor(1.9714e+08, device='cuda:0')
c= tensor(1.9714e+08, device='cuda:0')
c= tensor(1.9714e+08, device='cuda:0')
c= tensor(1.9715e+08, device='cuda:0')
c= tensor(1.9715e+08, device='cuda:0')
c= tensor(1.9715e+08, device='cuda:0')
c= tensor(1.9715e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9717e+08, device='cuda:0')
c= tensor(1.9717e+08, device='cuda:0')
c= tensor(1.9717e+08, device='cuda:0')
c= tensor(1.9718e+08, device='cuda:0')
c= tensor(1.9719e+08, device='cuda:0')
c= tensor(1.9720e+08, device='cuda:0')
c= tensor(1.9720e+08, device='cuda:0')
c= tensor(1.9723e+08, device='cuda:0')
c= tensor(1.9723e+08, device='cuda:0')
c= tensor(1.9724e+08, device='cuda:0')
c= tensor(1.9724e+08, device='cuda:0')
c= tensor(1.9724e+08, device='cuda:0')
c= tensor(1.9724e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9726e+08, device='cuda:0')
c= tensor(1.9726e+08, device='cuda:0')
c= tensor(1.9726e+08, device='cuda:0')
c= tensor(1.9726e+08, device='cuda:0')
c= tensor(1.9727e+08, device='cuda:0')
c= tensor(1.9727e+08, device='cuda:0')
c= tensor(1.9727e+08, device='cuda:0')
c= tensor(1.9727e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9731e+08, device='cuda:0')
c= tensor(1.9731e+08, device='cuda:0')
c= tensor(1.9732e+08, device='cuda:0')
c= tensor(1.9732e+08, device='cuda:0')
c= tensor(1.9732e+08, device='cuda:0')
c= tensor(1.9732e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9735e+08, device='cuda:0')
c= tensor(1.9742e+08, device='cuda:0')
c= tensor(1.9743e+08, device='cuda:0')
c= tensor(1.9743e+08, device='cuda:0')
c= tensor(1.9743e+08, device='cuda:0')
c= tensor(1.9743e+08, device='cuda:0')
c= tensor(1.9744e+08, device='cuda:0')
c= tensor(1.9744e+08, device='cuda:0')
c= tensor(1.9744e+08, device='cuda:0')
c= tensor(1.9745e+08, device='cuda:0')
c= tensor(1.9746e+08, device='cuda:0')
c= tensor(1.9747e+08, device='cuda:0')
c= tensor(1.9747e+08, device='cuda:0')
c= tensor(1.9751e+08, device='cuda:0')
c= tensor(1.9751e+08, device='cuda:0')
c= tensor(1.9751e+08, device='cuda:0')
c= tensor(1.9752e+08, device='cuda:0')
c= tensor(1.9752e+08, device='cuda:0')
c= tensor(1.9752e+08, device='cuda:0')
c= tensor(1.9753e+08, device='cuda:0')
c= tensor(1.9753e+08, device='cuda:0')
c= tensor(1.9753e+08, device='cuda:0')
c= tensor(1.9753e+08, device='cuda:0')
c= tensor(1.9753e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9755e+08, device='cuda:0')
c= tensor(1.9755e+08, device='cuda:0')
c= tensor(1.9755e+08, device='cuda:0')
c= tensor(1.9756e+08, device='cuda:0')
c= tensor(1.9756e+08, device='cuda:0')
c= tensor(1.9757e+08, device='cuda:0')
c= tensor(1.9757e+08, device='cuda:0')
c= tensor(1.9757e+08, device='cuda:0')
c= tensor(1.9757e+08, device='cuda:0')
c= tensor(1.9757e+08, device='cuda:0')
c= tensor(1.9758e+08, device='cuda:0')
c= tensor(1.9758e+08, device='cuda:0')
c= tensor(1.9759e+08, device='cuda:0')
c= tensor(1.9759e+08, device='cuda:0')
c= tensor(1.9760e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9767e+08, device='cuda:0')
c= tensor(1.9767e+08, device='cuda:0')
c= tensor(1.9767e+08, device='cuda:0')
c= tensor(1.9767e+08, device='cuda:0')
c= tensor(1.9768e+08, device='cuda:0')
c= tensor(1.9768e+08, device='cuda:0')
c= tensor(1.9769e+08, device='cuda:0')
c= tensor(1.9771e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9773e+08, device='cuda:0')
c= tensor(1.9773e+08, device='cuda:0')
c= tensor(1.9773e+08, device='cuda:0')
c= tensor(1.9773e+08, device='cuda:0')
c= tensor(1.9773e+08, device='cuda:0')
c= tensor(1.9774e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9776e+08, device='cuda:0')
c= tensor(1.9776e+08, device='cuda:0')
c= tensor(1.9777e+08, device='cuda:0')
c= tensor(1.9777e+08, device='cuda:0')
c= tensor(1.9777e+08, device='cuda:0')
c= tensor(1.9777e+08, device='cuda:0')
c= tensor(1.9777e+08, device='cuda:0')
c= tensor(1.9778e+08, device='cuda:0')
c= tensor(1.9780e+08, device='cuda:0')
c= tensor(1.9825e+08, device='cuda:0')
c= tensor(1.9825e+08, device='cuda:0')
c= tensor(1.9826e+08, device='cuda:0')
c= tensor(1.9826e+08, device='cuda:0')
c= tensor(1.9826e+08, device='cuda:0')
c= tensor(2.0112e+08, device='cuda:0')
c= tensor(2.0782e+08, device='cuda:0')
c= tensor(2.0782e+08, device='cuda:0')
c= tensor(2.0801e+08, device='cuda:0')
c= tensor(2.0824e+08, device='cuda:0')
c= tensor(2.0824e+08, device='cuda:0')
c= tensor(2.1995e+08, device='cuda:0')
c= tensor(2.1995e+08, device='cuda:0')
c= tensor(2.1996e+08, device='cuda:0')
c= tensor(2.2046e+08, device='cuda:0')
c= tensor(2.2323e+08, device='cuda:0')
c= tensor(2.2323e+08, device='cuda:0')
c= tensor(2.2328e+08, device='cuda:0')
c= tensor(2.2336e+08, device='cuda:0')
c= tensor(2.2458e+08, device='cuda:0')
c= tensor(2.2513e+08, device='cuda:0')
c= tensor(2.2539e+08, device='cuda:0')
c= tensor(2.2548e+08, device='cuda:0')
c= tensor(2.2557e+08, device='cuda:0')
c= tensor(2.2557e+08, device='cuda:0')
c= tensor(2.3124e+08, device='cuda:0')
c= tensor(2.3124e+08, device='cuda:0')
c= tensor(2.3124e+08, device='cuda:0')
c= tensor(2.3127e+08, device='cuda:0')
c= tensor(2.3142e+08, device='cuda:0')
c= tensor(2.3573e+08, device='cuda:0')
c= tensor(2.3615e+08, device='cuda:0')
c= tensor(2.3615e+08, device='cuda:0')
c= tensor(2.3616e+08, device='cuda:0')
c= tensor(2.3620e+08, device='cuda:0')
c= tensor(2.3633e+08, device='cuda:0')
c= tensor(2.3754e+08, device='cuda:0')
c= tensor(2.3819e+08, device='cuda:0')
c= tensor(2.3823e+08, device='cuda:0')
c= tensor(2.3823e+08, device='cuda:0')
c= tensor(2.3823e+08, device='cuda:0')
c= tensor(2.3847e+08, device='cuda:0')
c= tensor(2.3888e+08, device='cuda:0')
c= tensor(2.3899e+08, device='cuda:0')
c= tensor(2.3900e+08, device='cuda:0')
c= tensor(2.5176e+08, device='cuda:0')
c= tensor(2.5176e+08, device='cuda:0')
c= tensor(2.5184e+08, device='cuda:0')
c= tensor(2.5268e+08, device='cuda:0')
c= tensor(2.5269e+08, device='cuda:0')
c= tensor(2.5291e+08, device='cuda:0')
c= tensor(2.5858e+08, device='cuda:0')
c= tensor(2.6432e+08, device='cuda:0')
c= tensor(2.6433e+08, device='cuda:0')
c= tensor(2.6433e+08, device='cuda:0')
c= tensor(2.6433e+08, device='cuda:0')
c= tensor(2.6433e+08, device='cuda:0')
c= tensor(2.6456e+08, device='cuda:0')
c= tensor(2.6458e+08, device='cuda:0')
c= tensor(2.6473e+08, device='cuda:0')
c= tensor(2.7149e+08, device='cuda:0')
c= tensor(2.7155e+08, device='cuda:0')
c= tensor(2.7156e+08, device='cuda:0')
c= tensor(2.7157e+08, device='cuda:0')
c= tensor(2.7336e+08, device='cuda:0')
c= tensor(2.7377e+08, device='cuda:0')
c= tensor(2.7382e+08, device='cuda:0')
c= tensor(2.7393e+08, device='cuda:0')
c= tensor(2.9245e+08, device='cuda:0')
c= tensor(2.9245e+08, device='cuda:0')
c= tensor(2.9616e+08, device='cuda:0')
c= tensor(2.9617e+08, device='cuda:0')
c= tensor(2.9690e+08, device='cuda:0')
c= tensor(2.9698e+08, device='cuda:0')
c= tensor(3.0481e+08, device='cuda:0')
c= tensor(3.0508e+08, device='cuda:0')
c= tensor(3.0509e+08, device='cuda:0')
c= tensor(3.0594e+08, device='cuda:0')
c= tensor(3.0701e+08, device='cuda:0')
c= tensor(3.0702e+08, device='cuda:0')
c= tensor(3.0729e+08, device='cuda:0')
c= tensor(3.1022e+08, device='cuda:0')
c= tensor(3.2271e+08, device='cuda:0')
c= tensor(3.2278e+08, device='cuda:0')
c= tensor(3.2278e+08, device='cuda:0')
c= tensor(3.2279e+08, device='cuda:0')
c= tensor(3.2280e+08, device='cuda:0')
c= tensor(3.2283e+08, device='cuda:0')
c= tensor(3.2285e+08, device='cuda:0')
c= tensor(3.2285e+08, device='cuda:0')
c= tensor(3.2334e+08, device='cuda:0')
c= tensor(3.2420e+08, device='cuda:0')
c= tensor(3.2420e+08, device='cuda:0')
c= tensor(3.2420e+08, device='cuda:0')
c= tensor(3.2422e+08, device='cuda:0')
c= tensor(3.2424e+08, device='cuda:0')
c= tensor(3.2425e+08, device='cuda:0')
c= tensor(3.2428e+08, device='cuda:0')
c= tensor(3.2428e+08, device='cuda:0')
c= tensor(3.2435e+08, device='cuda:0')
c= tensor(3.2441e+08, device='cuda:0')
c= tensor(3.2444e+08, device='cuda:0')
c= tensor(3.2492e+08, device='cuda:0')
c= tensor(3.2493e+08, device='cuda:0')
c= tensor(3.9804e+08, device='cuda:0')
c= tensor(3.9804e+08, device='cuda:0')
c= tensor(3.9831e+08, device='cuda:0')
c= tensor(3.9831e+08, device='cuda:0')
c= tensor(3.9832e+08, device='cuda:0')
c= tensor(3.9832e+08, device='cuda:0')
c= tensor(3.9837e+08, device='cuda:0')
c= tensor(3.9837e+08, device='cuda:0')
c= tensor(4.0005e+08, device='cuda:0')
c= tensor(4.0006e+08, device='cuda:0')
c= tensor(4.0006e+08, device='cuda:0')
c= tensor(4.0544e+08, device='cuda:0')
c= tensor(4.0567e+08, device='cuda:0')
c= tensor(4.0618e+08, device='cuda:0')
c= tensor(4.0973e+08, device='cuda:0')
c= tensor(4.1406e+08, device='cuda:0')
c= tensor(4.1406e+08, device='cuda:0')
c= tensor(4.1407e+08, device='cuda:0')
c= tensor(4.1409e+08, device='cuda:0')
c= tensor(4.1409e+08, device='cuda:0')
c= tensor(4.1409e+08, device='cuda:0')
c= tensor(4.1411e+08, device='cuda:0')
c= tensor(4.1411e+08, device='cuda:0')
c= tensor(4.1412e+08, device='cuda:0')
c= tensor(4.1412e+08, device='cuda:0')
c= tensor(4.1412e+08, device='cuda:0')
c= tensor(4.1598e+08, device='cuda:0')
c= tensor(4.1600e+08, device='cuda:0')
c= tensor(4.1658e+08, device='cuda:0')
c= tensor(4.1660e+08, device='cuda:0')
c= tensor(4.1660e+08, device='cuda:0')
c= tensor(4.1661e+08, device='cuda:0')
c= tensor(4.7022e+08, device='cuda:0')
c= tensor(4.7998e+08, device='cuda:0')
c= tensor(4.7999e+08, device='cuda:0')
c= tensor(4.8031e+08, device='cuda:0')
c= tensor(4.8031e+08, device='cuda:0')
c= tensor(4.8032e+08, device='cuda:0')
c= tensor(4.8042e+08, device='cuda:0')
c= tensor(4.8058e+08, device='cuda:0')
c= tensor(4.8121e+08, device='cuda:0')
c= tensor(4.8159e+08, device='cuda:0')
c= tensor(5.2337e+08, device='cuda:0')
c= tensor(5.2339e+08, device='cuda:0')
c= tensor(5.2339e+08, device='cuda:0')
c= tensor(5.2344e+08, device='cuda:0')
c= tensor(5.2348e+08, device='cuda:0')
c= tensor(5.2348e+08, device='cuda:0')
c= tensor(5.2507e+08, device='cuda:0')
c= tensor(5.2513e+08, device='cuda:0')
c= tensor(5.2513e+08, device='cuda:0')
c= tensor(5.2522e+08, device='cuda:0')
c= tensor(5.2529e+08, device='cuda:0')
c= tensor(5.2529e+08, device='cuda:0')
c= tensor(5.2598e+08, device='cuda:0')
c= tensor(5.2718e+08, device='cuda:0')
c= tensor(5.2906e+08, device='cuda:0')
c= tensor(5.3055e+08, device='cuda:0')
c= tensor(5.3390e+08, device='cuda:0')
c= tensor(5.3392e+08, device='cuda:0')
c= tensor(5.3395e+08, device='cuda:0')
c= tensor(5.3405e+08, device='cuda:0')
c= tensor(5.3453e+08, device='cuda:0')
c= tensor(5.3453e+08, device='cuda:0')
c= tensor(5.3546e+08, device='cuda:0')
c= tensor(5.5035e+08, device='cuda:0')
c= tensor(5.5261e+08, device='cuda:0')
c= tensor(5.5330e+08, device='cuda:0')
c= tensor(5.5533e+08, device='cuda:0')
c= tensor(5.5534e+08, device='cuda:0')
c= tensor(5.5534e+08, device='cuda:0')
c= tensor(5.5534e+08, device='cuda:0')
c= tensor(5.5578e+08, device='cuda:0')
c= tensor(5.5606e+08, device='cuda:0')
c= tensor(5.7317e+08, device='cuda:0')
c= tensor(5.7577e+08, device='cuda:0')
c= tensor(5.7683e+08, device='cuda:0')
c= tensor(5.7695e+08, device='cuda:0')
c= tensor(5.7746e+08, device='cuda:0')
c= tensor(5.7746e+08, device='cuda:0')
c= tensor(5.7748e+08, device='cuda:0')
c= tensor(5.8057e+08, device='cuda:0')
c= tensor(5.8057e+08, device='cuda:0')
c= tensor(5.8057e+08, device='cuda:0')
c= tensor(5.8065e+08, device='cuda:0')
c= tensor(5.8777e+08, device='cuda:0')
c= tensor(5.8779e+08, device='cuda:0')
c= tensor(5.8933e+08, device='cuda:0')
c= tensor(5.8933e+08, device='cuda:0')
c= tensor(5.8934e+08, device='cuda:0')
c= tensor(5.8934e+08, device='cuda:0')
c= tensor(5.8934e+08, device='cuda:0')
c= tensor(5.8971e+08, device='cuda:0')
c= tensor(5.8978e+08, device='cuda:0')
c= tensor(5.8979e+08, device='cuda:0')
c= tensor(5.9070e+08, device='cuda:0')
c= tensor(5.9070e+08, device='cuda:0')
c= tensor(5.9157e+08, device='cuda:0')
c= tensor(5.9158e+08, device='cuda:0')
c= tensor(5.9162e+08, device='cuda:0')
c= tensor(5.9162e+08, device='cuda:0')
c= tensor(5.9164e+08, device='cuda:0')
c= tensor(5.9164e+08, device='cuda:0')
c= tensor(5.9170e+08, device='cuda:0')
c= tensor(5.9267e+08, device='cuda:0')
c= tensor(5.9688e+08, device='cuda:0')
c= tensor(5.9689e+08, device='cuda:0')
c= tensor(5.9690e+08, device='cuda:0')
c= tensor(6.0003e+08, device='cuda:0')
c= tensor(6.0004e+08, device='cuda:0')
c= tensor(6.2152e+08, device='cuda:0')
c= tensor(6.2152e+08, device='cuda:0')
c= tensor(6.2172e+08, device='cuda:0')
c= tensor(6.2223e+08, device='cuda:0')
c= tensor(6.2223e+08, device='cuda:0')
c= tensor(6.2376e+08, device='cuda:0')
c= tensor(6.2391e+08, device='cuda:0')
c= tensor(6.8551e+08, device='cuda:0')
c= tensor(6.8552e+08, device='cuda:0')
c= tensor(6.8552e+08, device='cuda:0')
c= tensor(6.8554e+08, device='cuda:0')
c= tensor(6.8554e+08, device='cuda:0')
c= tensor(6.8556e+08, device='cuda:0')
c= tensor(6.8567e+08, device='cuda:0')
c= tensor(6.8573e+08, device='cuda:0')
c= tensor(6.8584e+08, device='cuda:0')
c= tensor(6.8587e+08, device='cuda:0')
c= tensor(6.8587e+08, device='cuda:0')
c= tensor(6.8588e+08, device='cuda:0')
c= tensor(6.9840e+08, device='cuda:0')
c= tensor(6.9911e+08, device='cuda:0')
c= tensor(7.0342e+08, device='cuda:0')
c= tensor(7.0349e+08, device='cuda:0')
c= tensor(7.0352e+08, device='cuda:0')
c= tensor(7.0352e+08, device='cuda:0')
c= tensor(7.0355e+08, device='cuda:0')
c= tensor(7.1062e+08, device='cuda:0')
c= tensor(7.1065e+08, device='cuda:0')
c= tensor(7.1066e+08, device='cuda:0')
c= tensor(7.1070e+08, device='cuda:0')
c= tensor(7.1114e+08, device='cuda:0')
c= tensor(7.1115e+08, device='cuda:0')
c= tensor(7.1115e+08, device='cuda:0')
c= tensor(7.4297e+08, device='cuda:0')
c= tensor(7.4300e+08, device='cuda:0')
c= tensor(7.4310e+08, device='cuda:0')
c= tensor(7.4311e+08, device='cuda:0')
c= tensor(7.4404e+08, device='cuda:0')
c= tensor(7.5029e+08, device='cuda:0')
c= tensor(7.5266e+08, device='cuda:0')
c= tensor(7.5379e+08, device='cuda:0')
c= tensor(7.5382e+08, device='cuda:0')
c= tensor(7.5402e+08, device='cuda:0')
c= tensor(7.5411e+08, device='cuda:0')
c= tensor(7.5411e+08, device='cuda:0')
c= tensor(7.5414e+08, device='cuda:0')
c= tensor(7.5414e+08, device='cuda:0')
c= tensor(7.5446e+08, device='cuda:0')
c= tensor(7.5447e+08, device='cuda:0')
c= tensor(7.5447e+08, device='cuda:0')
c= tensor(7.5449e+08, device='cuda:0')
c= tensor(7.5450e+08, device='cuda:0')
c= tensor(7.5473e+08, device='cuda:0')
c= tensor(7.5474e+08, device='cuda:0')
c= tensor(7.5488e+08, device='cuda:0')
c= tensor(7.5501e+08, device='cuda:0')
c= tensor(7.5505e+08, device='cuda:0')
c= tensor(7.5505e+08, device='cuda:0')
c= tensor(7.5506e+08, device='cuda:0')
c= tensor(7.5510e+08, device='cuda:0')
c= tensor(7.5957e+08, device='cuda:0')
c= tensor(7.5958e+08, device='cuda:0')
c= tensor(7.5958e+08, device='cuda:0')
c= tensor(7.5958e+08, device='cuda:0')
c= tensor(7.7140e+08, device='cuda:0')
c= tensor(7.7917e+08, device='cuda:0')
c= tensor(7.7922e+08, device='cuda:0')
c= tensor(7.7923e+08, device='cuda:0')
c= tensor(7.7938e+08, device='cuda:0')
c= tensor(7.8010e+08, device='cuda:0')
c= tensor(7.8010e+08, device='cuda:0')
c= tensor(7.8013e+08, device='cuda:0')
c= tensor(7.8014e+08, device='cuda:0')
c= tensor(7.8114e+08, device='cuda:0')
c= tensor(7.8115e+08, device='cuda:0')
c= tensor(7.8590e+08, device='cuda:0')
c= tensor(7.8591e+08, device='cuda:0')
c= tensor(7.8591e+08, device='cuda:0')
c= tensor(7.8591e+08, device='cuda:0')
c= tensor(7.8599e+08, device='cuda:0')
c= tensor(7.8600e+08, device='cuda:0')
c= tensor(7.8608e+08, device='cuda:0')
c= tensor(8.0213e+08, device='cuda:0')
c= tensor(8.2949e+08, device='cuda:0')
c= tensor(8.2950e+08, device='cuda:0')
c= tensor(8.2950e+08, device='cuda:0')
c= tensor(8.2953e+08, device='cuda:0')
c= tensor(8.4336e+08, device='cuda:0')
c= tensor(8.4374e+08, device='cuda:0')
c= tensor(8.4374e+08, device='cuda:0')
c= tensor(8.4374e+08, device='cuda:0')
c= tensor(8.4427e+08, device='cuda:0')
c= tensor(8.4427e+08, device='cuda:0')
c= tensor(8.4503e+08, device='cuda:0')
c= tensor(8.4503e+08, device='cuda:0')
c= tensor(8.4506e+08, device='cuda:0')
c= tensor(8.4507e+08, device='cuda:0')
c= tensor(8.4507e+08, device='cuda:0')
c= tensor(8.4507e+08, device='cuda:0')
c= tensor(8.4684e+08, device='cuda:0')
c= tensor(8.5308e+08, device='cuda:0')
c= tensor(8.5608e+08, device='cuda:0')
c= tensor(8.5615e+08, device='cuda:0')
c= tensor(8.5624e+08, device='cuda:0')
c= tensor(8.5625e+08, device='cuda:0')
c= tensor(8.5625e+08, device='cuda:0')
c= tensor(8.5654e+08, device='cuda:0')
c= tensor(8.5663e+08, device='cuda:0')
c= tensor(8.6160e+08, device='cuda:0')
c= tensor(8.6162e+08, device='cuda:0')
c= tensor(9.1038e+08, device='cuda:0')
c= tensor(9.1040e+08, device='cuda:0')
c= tensor(9.1064e+08, device='cuda:0')
c= tensor(9.1341e+08, device='cuda:0')
c= tensor(9.1361e+08, device='cuda:0')
c= tensor(9.1390e+08, device='cuda:0')
c= tensor(9.1561e+08, device='cuda:0')
c= tensor(9.1569e+08, device='cuda:0')
c= tensor(9.1573e+08, device='cuda:0')
c= tensor(9.1574e+08, device='cuda:0')
c= tensor(9.1590e+08, device='cuda:0')
c= tensor(9.1596e+08, device='cuda:0')
c= tensor(9.1653e+08, device='cuda:0')
c= tensor(9.6975e+08, device='cuda:0')
c= tensor(9.7170e+08, device='cuda:0')
c= tensor(9.7585e+08, device='cuda:0')
c= tensor(9.7624e+08, device='cuda:0')
c= tensor(9.7632e+08, device='cuda:0')
c= tensor(9.7648e+08, device='cuda:0')
c= tensor(9.7650e+08, device='cuda:0')
c= tensor(9.7651e+08, device='cuda:0')
c= tensor(9.7850e+08, device='cuda:0')
c= tensor(9.7857e+08, device='cuda:0')
c= tensor(9.8261e+08, device='cuda:0')
c= tensor(1.0300e+09, device='cuda:0')
c= tensor(1.0300e+09, device='cuda:0')
c= tensor(1.0300e+09, device='cuda:0')
c= tensor(1.0308e+09, device='cuda:0')
c= tensor(1.0310e+09, device='cuda:0')
c= tensor(1.0310e+09, device='cuda:0')
c= tensor(1.0370e+09, device='cuda:0')
c= tensor(1.0373e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(2.0860e+09, device='cuda:0')
c= tensor(2.0860e+09, device='cuda:0')
c= tensor(2.0860e+09, device='cuda:0')
c= tensor(2.0860e+09, device='cuda:0')
c= tensor(2.0860e+09, device='cuda:0')
c= tensor(2.0861e+09, device='cuda:0')
c= tensor(2.0893e+09, device='cuda:0')
c= tensor(2.0893e+09, device='cuda:0')
c= tensor(2.1138e+09, device='cuda:0')
c= tensor(2.1138e+09, device='cuda:0')
c= tensor(2.1183e+09, device='cuda:0')
c= tensor(2.1186e+09, device='cuda:0')
c= tensor(2.1215e+09, device='cuda:0')
c= tensor(2.1376e+09, device='cuda:0')
c= tensor(2.1376e+09, device='cuda:0')
c= tensor(2.1376e+09, device='cuda:0')
c= tensor(2.1382e+09, device='cuda:0')
c= tensor(2.1382e+09, device='cuda:0')
c= tensor(2.1383e+09, device='cuda:0')
c= tensor(2.1383e+09, device='cuda:0')
c= tensor(2.1383e+09, device='cuda:0')
c= tensor(2.1386e+09, device='cuda:0')
c= tensor(2.1386e+09, device='cuda:0')
c= tensor(2.1426e+09, device='cuda:0')
c= tensor(2.1478e+09, device='cuda:0')
c= tensor(2.1478e+09, device='cuda:0')
c= tensor(2.1478e+09, device='cuda:0')
c= tensor(2.1502e+09, device='cuda:0')
c= tensor(2.1502e+09, device='cuda:0')
c= tensor(2.1505e+09, device='cuda:0')
c= tensor(2.1505e+09, device='cuda:0')
c= tensor(2.1511e+09, device='cuda:0')
c= tensor(2.1512e+09, device='cuda:0')
c= tensor(2.1523e+09, device='cuda:0')
c= tensor(2.1554e+09, device='cuda:0')
c= tensor(2.1554e+09, device='cuda:0')
c= tensor(2.1554e+09, device='cuda:0')
c= tensor(2.1555e+09, device='cuda:0')
c= tensor(2.1556e+09, device='cuda:0')
c= tensor(2.1561e+09, device='cuda:0')
c= tensor(2.1592e+09, device='cuda:0')
c= tensor(2.1592e+09, device='cuda:0')
c= tensor(2.1593e+09, device='cuda:0')
c= tensor(2.1594e+09, device='cuda:0')
c= tensor(2.1616e+09, device='cuda:0')
c= tensor(2.1618e+09, device='cuda:0')
c= tensor(2.1622e+09, device='cuda:0')
c= tensor(2.1627e+09, device='cuda:0')
c= tensor(2.1627e+09, device='cuda:0')
c= tensor(2.1627e+09, device='cuda:0')
c= tensor(2.1628e+09, device='cuda:0')
c= tensor(2.1628e+09, device='cuda:0')
c= tensor(2.1629e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1632e+09, device='cuda:0')
c= tensor(2.1632e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1702e+09, device='cuda:0')
c= tensor(2.1705e+09, device='cuda:0')
c= tensor(2.1708e+09, device='cuda:0')
c= tensor(2.1717e+09, device='cuda:0')
c= tensor(2.1731e+09, device='cuda:0')
c= tensor(2.1769e+09, device='cuda:0')
c= tensor(2.1781e+09, device='cuda:0')
c= tensor(2.1781e+09, device='cuda:0')
c= tensor(2.1781e+09, device='cuda:0')
c= tensor(2.1781e+09, device='cuda:0')
c= tensor(2.1783e+09, device='cuda:0')
c= tensor(2.1810e+09, device='cuda:0')
c= tensor(2.1810e+09, device='cuda:0')
c= tensor(2.1813e+09, device='cuda:0')
c= tensor(2.1813e+09, device='cuda:0')
c= tensor(2.1814e+09, device='cuda:0')
c= tensor(2.1814e+09, device='cuda:0')
c= tensor(2.1815e+09, device='cuda:0')
c= tensor(2.1867e+09, device='cuda:0')
c= tensor(2.3004e+09, device='cuda:0')
c= tensor(2.3007e+09, device='cuda:0')
c= tensor(2.3007e+09, device='cuda:0')
c= tensor(2.3008e+09, device='cuda:0')
c= tensor(2.3011e+09, device='cuda:0')
c= tensor(2.3011e+09, device='cuda:0')
c= tensor(2.3011e+09, device='cuda:0')
c= tensor(2.3012e+09, device='cuda:0')
c= tensor(2.3022e+09, device='cuda:0')
c= tensor(2.3022e+09, device='cuda:0')
c= tensor(2.3028e+09, device='cuda:0')
c= tensor(2.3028e+09, device='cuda:0')
memory (bytes)
3926253568
time for making loss 2 is 17.182962894439697
p0 True
it  0 : 1173886976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 50% |
shape of L is 
torch.Size([])
memory (bytes)
3926515712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
3927130112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 13% |
error is  10832862000.0
relative error loss 4.7041135
shape of L is 
torch.Size([])
memory (bytes)
4159160320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 13% |
memory (bytes)
4159164416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  10832804000.0
relative error loss 4.704088
shape of L is 
torch.Size([])
memory (bytes)
4161253376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
4161253376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  10832542000.0
relative error loss 4.7039742
shape of L is 
torch.Size([])
memory (bytes)
4162752512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4162752512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  10831089000.0
relative error loss 4.7033434
shape of L is 
torch.Size([])
memory (bytes)
4164947968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4164947968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 13% |
error is  10823098000.0
relative error loss 4.699874
shape of L is 
torch.Size([])
memory (bytes)
4167069696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
4167069696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  10779172000.0
relative error loss 4.680799
shape of L is 
torch.Size([])
memory (bytes)
4169297920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4169297920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 13% |
error is  10540594000.0
relative error loss 4.5771976
shape of L is 
torch.Size([])
memory (bytes)
4171354112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
4171354112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  8252500000.0
relative error loss 3.5836048
shape of L is 
torch.Size([])
memory (bytes)
4173529088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 13% |
memory (bytes)
4173529088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  3650249700.0
relative error loss 1.5851018
shape of L is 
torch.Size([])
memory (bytes)
4175605760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
4175605760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  3199648300.0
relative error loss 1.3894305
shape of L is 
torch.Size([])
memory (bytes)
4177784832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
4177784832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  2963628500.0
relative error loss 1.2869401
time to take a step is 281.2371618747711
it  1 : 1648995840
| ID | GPU | MEM |
------------------
|  0 | 17% |  0% |
|  1 |  9% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4179902464
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 12% | 13% |
memory (bytes)
4179902464
| ID | GPU | MEM |
------------------
|  0 | 13% |  0% |
|  1 | 99% | 13% |
error is  2963628500.0
relative error loss 1.2869401
shape of L is 
torch.Size([])
memory (bytes)
4182044672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4182044672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2477987600.0
relative error loss 1.0760531
shape of L is 
torch.Size([])
memory (bytes)
4184199168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4184199168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 13% |
error is  2264864800.0
relative error loss 0.98350567
shape of L is 
torch.Size([])
memory (bytes)
4186304512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4186304512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2221255400.0
relative error loss 0.9645685
shape of L is 
torch.Size([])
memory (bytes)
4188450816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 13% |
memory (bytes)
4188450816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2089154200.0
relative error loss 0.9072043
shape of L is 
torch.Size([])
memory (bytes)
4190515200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
4190523392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1973377400.0
relative error loss 0.85692877
shape of L is 
torch.Size([])
memory (bytes)
4192706560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 13% |
memory (bytes)
4192706560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1804339000.0
relative error loss 0.78352475
shape of L is 
torch.Size([])
memory (bytes)
4194852864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 13% |
memory (bytes)
4194852864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1652664600.0
relative error loss 0.7176609
shape of L is 
torch.Size([])
memory (bytes)
4196958208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4196958208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1722877700.0
relative error loss 0.7481506
shape of L is 
torch.Size([])
memory (bytes)
4199051264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 13% |
memory (bytes)
4199051264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  1558198000.0
relative error loss 0.6766393
time to take a step is 250.101802110672
it  2 : 1784741376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4201164800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4201164800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1558198000.0
relative error loss 0.6766393
shape of L is 
torch.Size([])
memory (bytes)
4203237376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4203237376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1405277000.0
relative error loss 0.6102342
shape of L is 
torch.Size([])
memory (bytes)
4205408256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4205408256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1246421400.0
relative error loss 0.54125196
shape of L is 
torch.Size([])
memory (bytes)
4207443968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4207443968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1128504300.0
relative error loss 0.4900471
shape of L is 
torch.Size([])
memory (bytes)
4209651712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 13% |
memory (bytes)
4209651712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1014255900.0
relative error loss 0.4404353
shape of L is 
torch.Size([])
memory (bytes)
4211703808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 13% |
memory (bytes)
4211703808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  952917600.0
relative error loss 0.4137995
shape of L is 
torch.Size([])
memory (bytes)
4213698560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 13% |
memory (bytes)
4213698560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  893719300.0
relative error loss 0.3880929
shape of L is 
torch.Size([])
memory (bytes)
4216061952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4216061952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  841121900.0
relative error loss 0.3652528
shape of L is 
torch.Size([])
memory (bytes)
4218028032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4218028032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  801788160.0
relative error loss 0.3481723
shape of L is 
torch.Size([])
memory (bytes)
4220067840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 13% |
memory (bytes)
4220067840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  771435650.0
relative error loss 0.33499187
time to take a step is 250.5673213005066
it  3 : 1784740864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4222480384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
4222480384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  771435650.0
relative error loss 0.33499187
shape of L is 
torch.Size([])
memory (bytes)
4224491520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 13% |
memory (bytes)
4224491520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  726729700.0
relative error loss 0.31557858
shape of L is 
torch.Size([])
memory (bytes)
4226555904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4226555904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 13% |
error is  682195600.0
relative error loss 0.29623985
shape of L is 
torch.Size([])
memory (bytes)
4228923392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 13% |
memory (bytes)
4228923392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  626690400.0
relative error loss 0.27213705
shape of L is 
torch.Size([])
memory (bytes)
4231069696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4231069696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  578380300.0
relative error loss 0.2511586
shape of L is 
torch.Size([])
memory (bytes)
4232986624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 13% |
memory (bytes)
4232986624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  536939500.0
relative error loss 0.23316318
shape of L is 
torch.Size([])
memory (bytes)
4235362304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4235362304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  488581250.0
relative error loss 0.21216385
shape of L is 
torch.Size([])
memory (bytes)
4237500416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 13% |
memory (bytes)
4237500416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  451671800.0
relative error loss 0.19613612
shape of L is 
torch.Size([])
memory (bytes)
4239466496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 13% |
memory (bytes)
4239466496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 13% |
error is  420672500.0
relative error loss 0.18267483
shape of L is 
torch.Size([])
memory (bytes)
4241612800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 13% |
memory (bytes)
4241612800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  782008800.0
relative error loss 0.33958325
shape of L is 
torch.Size([])
memory (bytes)
4243939328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
4243939328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  396223740.0
relative error loss 0.17205808
time to take a step is 272.7868621349335
c= tensor(268.0565, device='cuda:0')
c= tensor(15719.6914, device='cuda:0')
c= tensor(17351.7617, device='cuda:0')
c= tensor(36466.4258, device='cuda:0')
c= tensor(108254.3906, device='cuda:0')
c= tensor(126764.2188, device='cuda:0')
c= tensor(454526.1875, device='cuda:0')
c= tensor(510685.5000, device='cuda:0')
c= tensor(515116.2188, device='cuda:0')
c= tensor(686169.1250, device='cuda:0')
c= tensor(690298.3125, device='cuda:0')
c= tensor(2361367.2500, device='cuda:0')
c= tensor(2370067., device='cuda:0')
c= tensor(9386142., device='cuda:0')
c= tensor(9435209., device='cuda:0')
c= tensor(9462002., device='cuda:0')
c= tensor(9911540., device='cuda:0')
c= tensor(10073552., device='cuda:0')
c= tensor(13102500., device='cuda:0')
c= tensor(14932127., device='cuda:0')
c= tensor(14938178., device='cuda:0')
c= tensor(19169820., device='cuda:0')
c= tensor(19181360., device='cuda:0')
c= tensor(19485752., device='cuda:0')
c= tensor(19487242., device='cuda:0')
c= tensor(20153430., device='cuda:0')
c= tensor(20600554., device='cuda:0')
c= tensor(20606068., device='cuda:0')
c= tensor(21281184., device='cuda:0')
c= tensor(85397904., device='cuda:0')
c= tensor(85403952., device='cuda:0')
c= tensor(1.9298e+08, device='cuda:0')
c= tensor(1.9303e+08, device='cuda:0')
c= tensor(1.9304e+08, device='cuda:0')
c= tensor(1.9304e+08, device='cuda:0')
c= tensor(1.9438e+08, device='cuda:0')
c= tensor(1.9695e+08, device='cuda:0')
c= tensor(1.9695e+08, device='cuda:0')
c= tensor(1.9696e+08, device='cuda:0')
c= tensor(1.9696e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9697e+08, device='cuda:0')
c= tensor(1.9698e+08, device='cuda:0')
c= tensor(1.9698e+08, device='cuda:0')
c= tensor(1.9700e+08, device='cuda:0')
c= tensor(1.9701e+08, device='cuda:0')
c= tensor(1.9702e+08, device='cuda:0')
c= tensor(1.9702e+08, device='cuda:0')
c= tensor(1.9703e+08, device='cuda:0')
c= tensor(1.9703e+08, device='cuda:0')
c= tensor(1.9704e+08, device='cuda:0')
c= tensor(1.9704e+08, device='cuda:0')
c= tensor(1.9704e+08, device='cuda:0')
c= tensor(1.9705e+08, device='cuda:0')
c= tensor(1.9705e+08, device='cuda:0')
c= tensor(1.9707e+08, device='cuda:0')
c= tensor(1.9707e+08, device='cuda:0')
c= tensor(1.9710e+08, device='cuda:0')
c= tensor(1.9710e+08, device='cuda:0')
c= tensor(1.9710e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9711e+08, device='cuda:0')
c= tensor(1.9712e+08, device='cuda:0')
c= tensor(1.9712e+08, device='cuda:0')
c= tensor(1.9712e+08, device='cuda:0')
c= tensor(1.9713e+08, device='cuda:0')
c= tensor(1.9713e+08, device='cuda:0')
c= tensor(1.9713e+08, device='cuda:0')
c= tensor(1.9713e+08, device='cuda:0')
c= tensor(1.9714e+08, device='cuda:0')
c= tensor(1.9714e+08, device='cuda:0')
c= tensor(1.9714e+08, device='cuda:0')
c= tensor(1.9715e+08, device='cuda:0')
c= tensor(1.9715e+08, device='cuda:0')
c= tensor(1.9715e+08, device='cuda:0')
c= tensor(1.9715e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9716e+08, device='cuda:0')
c= tensor(1.9717e+08, device='cuda:0')
c= tensor(1.9717e+08, device='cuda:0')
c= tensor(1.9717e+08, device='cuda:0')
c= tensor(1.9718e+08, device='cuda:0')
c= tensor(1.9719e+08, device='cuda:0')
c= tensor(1.9720e+08, device='cuda:0')
c= tensor(1.9720e+08, device='cuda:0')
c= tensor(1.9723e+08, device='cuda:0')
c= tensor(1.9723e+08, device='cuda:0')
c= tensor(1.9724e+08, device='cuda:0')
c= tensor(1.9724e+08, device='cuda:0')
c= tensor(1.9724e+08, device='cuda:0')
c= tensor(1.9724e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9725e+08, device='cuda:0')
c= tensor(1.9726e+08, device='cuda:0')
c= tensor(1.9726e+08, device='cuda:0')
c= tensor(1.9726e+08, device='cuda:0')
c= tensor(1.9726e+08, device='cuda:0')
c= tensor(1.9727e+08, device='cuda:0')
c= tensor(1.9727e+08, device='cuda:0')
c= tensor(1.9727e+08, device='cuda:0')
c= tensor(1.9727e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9731e+08, device='cuda:0')
c= tensor(1.9731e+08, device='cuda:0')
c= tensor(1.9732e+08, device='cuda:0')
c= tensor(1.9732e+08, device='cuda:0')
c= tensor(1.9732e+08, device='cuda:0')
c= tensor(1.9732e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9734e+08, device='cuda:0')
c= tensor(1.9735e+08, device='cuda:0')
c= tensor(1.9742e+08, device='cuda:0')
c= tensor(1.9743e+08, device='cuda:0')
c= tensor(1.9743e+08, device='cuda:0')
c= tensor(1.9743e+08, device='cuda:0')
c= tensor(1.9743e+08, device='cuda:0')
c= tensor(1.9744e+08, device='cuda:0')
c= tensor(1.9744e+08, device='cuda:0')
c= tensor(1.9744e+08, device='cuda:0')
c= tensor(1.9745e+08, device='cuda:0')
c= tensor(1.9746e+08, device='cuda:0')
c= tensor(1.9747e+08, device='cuda:0')
c= tensor(1.9747e+08, device='cuda:0')
c= tensor(1.9751e+08, device='cuda:0')
c= tensor(1.9751e+08, device='cuda:0')
c= tensor(1.9751e+08, device='cuda:0')
c= tensor(1.9752e+08, device='cuda:0')
c= tensor(1.9752e+08, device='cuda:0')
c= tensor(1.9752e+08, device='cuda:0')
c= tensor(1.9753e+08, device='cuda:0')
c= tensor(1.9753e+08, device='cuda:0')
c= tensor(1.9753e+08, device='cuda:0')
c= tensor(1.9753e+08, device='cuda:0')
c= tensor(1.9753e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9754e+08, device='cuda:0')
c= tensor(1.9755e+08, device='cuda:0')
c= tensor(1.9755e+08, device='cuda:0')
c= tensor(1.9755e+08, device='cuda:0')
c= tensor(1.9756e+08, device='cuda:0')
c= tensor(1.9756e+08, device='cuda:0')
c= tensor(1.9757e+08, device='cuda:0')
c= tensor(1.9757e+08, device='cuda:0')
c= tensor(1.9757e+08, device='cuda:0')
c= tensor(1.9757e+08, device='cuda:0')
c= tensor(1.9757e+08, device='cuda:0')
c= tensor(1.9758e+08, device='cuda:0')
c= tensor(1.9758e+08, device='cuda:0')
c= tensor(1.9759e+08, device='cuda:0')
c= tensor(1.9759e+08, device='cuda:0')
c= tensor(1.9760e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9764e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9765e+08, device='cuda:0')
c= tensor(1.9767e+08, device='cuda:0')
c= tensor(1.9767e+08, device='cuda:0')
c= tensor(1.9767e+08, device='cuda:0')
c= tensor(1.9767e+08, device='cuda:0')
c= tensor(1.9768e+08, device='cuda:0')
c= tensor(1.9768e+08, device='cuda:0')
c= tensor(1.9769e+08, device='cuda:0')
c= tensor(1.9771e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9772e+08, device='cuda:0')
c= tensor(1.9773e+08, device='cuda:0')
c= tensor(1.9773e+08, device='cuda:0')
c= tensor(1.9773e+08, device='cuda:0')
c= tensor(1.9773e+08, device='cuda:0')
c= tensor(1.9773e+08, device='cuda:0')
c= tensor(1.9774e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9775e+08, device='cuda:0')
c= tensor(1.9776e+08, device='cuda:0')
c= tensor(1.9776e+08, device='cuda:0')
c= tensor(1.9777e+08, device='cuda:0')
c= tensor(1.9777e+08, device='cuda:0')
c= tensor(1.9777e+08, device='cuda:0')
c= tensor(1.9777e+08, device='cuda:0')
c= tensor(1.9777e+08, device='cuda:0')
c= tensor(1.9778e+08, device='cuda:0')
c= tensor(1.9780e+08, device='cuda:0')
c= tensor(1.9825e+08, device='cuda:0')
c= tensor(1.9825e+08, device='cuda:0')
c= tensor(1.9826e+08, device='cuda:0')
c= tensor(1.9826e+08, device='cuda:0')
c= tensor(1.9826e+08, device='cuda:0')
c= tensor(2.0112e+08, device='cuda:0')
c= tensor(2.0782e+08, device='cuda:0')
c= tensor(2.0782e+08, device='cuda:0')
c= tensor(2.0801e+08, device='cuda:0')
c= tensor(2.0824e+08, device='cuda:0')
c= tensor(2.0824e+08, device='cuda:0')
c= tensor(2.1995e+08, device='cuda:0')
c= tensor(2.1995e+08, device='cuda:0')
c= tensor(2.1996e+08, device='cuda:0')
c= tensor(2.2046e+08, device='cuda:0')
c= tensor(2.2323e+08, device='cuda:0')
c= tensor(2.2323e+08, device='cuda:0')
c= tensor(2.2328e+08, device='cuda:0')
c= tensor(2.2336e+08, device='cuda:0')
c= tensor(2.2458e+08, device='cuda:0')
c= tensor(2.2513e+08, device='cuda:0')
c= tensor(2.2539e+08, device='cuda:0')
c= tensor(2.2548e+08, device='cuda:0')
c= tensor(2.2557e+08, device='cuda:0')
c= tensor(2.2557e+08, device='cuda:0')
c= tensor(2.3124e+08, device='cuda:0')
c= tensor(2.3124e+08, device='cuda:0')
c= tensor(2.3124e+08, device='cuda:0')
c= tensor(2.3127e+08, device='cuda:0')
c= tensor(2.3142e+08, device='cuda:0')
c= tensor(2.3573e+08, device='cuda:0')
c= tensor(2.3615e+08, device='cuda:0')
c= tensor(2.3615e+08, device='cuda:0')
c= tensor(2.3616e+08, device='cuda:0')
c= tensor(2.3620e+08, device='cuda:0')
c= tensor(2.3633e+08, device='cuda:0')
c= tensor(2.3754e+08, device='cuda:0')
c= tensor(2.3819e+08, device='cuda:0')
c= tensor(2.3823e+08, device='cuda:0')
c= tensor(2.3823e+08, device='cuda:0')
c= tensor(2.3823e+08, device='cuda:0')
c= tensor(2.3847e+08, device='cuda:0')
c= tensor(2.3888e+08, device='cuda:0')
c= tensor(2.3899e+08, device='cuda:0')
c= tensor(2.3900e+08, device='cuda:0')
c= tensor(2.5176e+08, device='cuda:0')
c= tensor(2.5176e+08, device='cuda:0')
c= tensor(2.5184e+08, device='cuda:0')
c= tensor(2.5268e+08, device='cuda:0')
c= tensor(2.5269e+08, device='cuda:0')
c= tensor(2.5291e+08, device='cuda:0')
c= tensor(2.5858e+08, device='cuda:0')
c= tensor(2.6432e+08, device='cuda:0')
c= tensor(2.6433e+08, device='cuda:0')
c= tensor(2.6433e+08, device='cuda:0')
c= tensor(2.6433e+08, device='cuda:0')
c= tensor(2.6433e+08, device='cuda:0')
c= tensor(2.6456e+08, device='cuda:0')
c= tensor(2.6458e+08, device='cuda:0')
c= tensor(2.6473e+08, device='cuda:0')
c= tensor(2.7149e+08, device='cuda:0')
c= tensor(2.7155e+08, device='cuda:0')
c= tensor(2.7156e+08, device='cuda:0')
c= tensor(2.7157e+08, device='cuda:0')
c= tensor(2.7336e+08, device='cuda:0')
c= tensor(2.7377e+08, device='cuda:0')
c= tensor(2.7382e+08, device='cuda:0')
c= tensor(2.7393e+08, device='cuda:0')
c= tensor(2.9245e+08, device='cuda:0')
c= tensor(2.9245e+08, device='cuda:0')
c= tensor(2.9616e+08, device='cuda:0')
c= tensor(2.9617e+08, device='cuda:0')
c= tensor(2.9690e+08, device='cuda:0')
c= tensor(2.9698e+08, device='cuda:0')
c= tensor(3.0481e+08, device='cuda:0')
c= tensor(3.0508e+08, device='cuda:0')
c= tensor(3.0509e+08, device='cuda:0')
c= tensor(3.0594e+08, device='cuda:0')
c= tensor(3.0701e+08, device='cuda:0')
c= tensor(3.0702e+08, device='cuda:0')
c= tensor(3.0729e+08, device='cuda:0')
c= tensor(3.1022e+08, device='cuda:0')
c= tensor(3.2271e+08, device='cuda:0')
c= tensor(3.2278e+08, device='cuda:0')
c= tensor(3.2278e+08, device='cuda:0')
c= tensor(3.2279e+08, device='cuda:0')
c= tensor(3.2280e+08, device='cuda:0')
c= tensor(3.2283e+08, device='cuda:0')
c= tensor(3.2285e+08, device='cuda:0')
c= tensor(3.2285e+08, device='cuda:0')
c= tensor(3.2334e+08, device='cuda:0')
c= tensor(3.2420e+08, device='cuda:0')
c= tensor(3.2420e+08, device='cuda:0')
c= tensor(3.2420e+08, device='cuda:0')
c= tensor(3.2422e+08, device='cuda:0')
c= tensor(3.2424e+08, device='cuda:0')
c= tensor(3.2425e+08, device='cuda:0')
c= tensor(3.2428e+08, device='cuda:0')
c= tensor(3.2428e+08, device='cuda:0')
c= tensor(3.2435e+08, device='cuda:0')
c= tensor(3.2441e+08, device='cuda:0')
c= tensor(3.2444e+08, device='cuda:0')
c= tensor(3.2492e+08, device='cuda:0')
c= tensor(3.2493e+08, device='cuda:0')
c= tensor(3.9804e+08, device='cuda:0')
c= tensor(3.9804e+08, device='cuda:0')
c= tensor(3.9831e+08, device='cuda:0')
c= tensor(3.9831e+08, device='cuda:0')
c= tensor(3.9832e+08, device='cuda:0')
c= tensor(3.9832e+08, device='cuda:0')
c= tensor(3.9837e+08, device='cuda:0')
c= tensor(3.9837e+08, device='cuda:0')
c= tensor(4.0005e+08, device='cuda:0')
c= tensor(4.0006e+08, device='cuda:0')
c= tensor(4.0006e+08, device='cuda:0')
c= tensor(4.0544e+08, device='cuda:0')
c= tensor(4.0567e+08, device='cuda:0')
c= tensor(4.0618e+08, device='cuda:0')
c= tensor(4.0973e+08, device='cuda:0')
c= tensor(4.1406e+08, device='cuda:0')
c= tensor(4.1406e+08, device='cuda:0')
c= tensor(4.1407e+08, device='cuda:0')
c= tensor(4.1409e+08, device='cuda:0')
c= tensor(4.1409e+08, device='cuda:0')
c= tensor(4.1409e+08, device='cuda:0')
c= tensor(4.1411e+08, device='cuda:0')
c= tensor(4.1411e+08, device='cuda:0')
c= tensor(4.1412e+08, device='cuda:0')
c= tensor(4.1412e+08, device='cuda:0')
c= tensor(4.1412e+08, device='cuda:0')
c= tensor(4.1598e+08, device='cuda:0')
c= tensor(4.1600e+08, device='cuda:0')
c= tensor(4.1658e+08, device='cuda:0')
c= tensor(4.1660e+08, device='cuda:0')
c= tensor(4.1660e+08, device='cuda:0')
c= tensor(4.1661e+08, device='cuda:0')
c= tensor(4.7022e+08, device='cuda:0')
c= tensor(4.7998e+08, device='cuda:0')
c= tensor(4.7999e+08, device='cuda:0')
c= tensor(4.8031e+08, device='cuda:0')
c= tensor(4.8031e+08, device='cuda:0')
c= tensor(4.8032e+08, device='cuda:0')
c= tensor(4.8042e+08, device='cuda:0')
c= tensor(4.8058e+08, device='cuda:0')
c= tensor(4.8121e+08, device='cuda:0')
c= tensor(4.8159e+08, device='cuda:0')
c= tensor(5.2337e+08, device='cuda:0')
c= tensor(5.2339e+08, device='cuda:0')
c= tensor(5.2339e+08, device='cuda:0')
c= tensor(5.2344e+08, device='cuda:0')
c= tensor(5.2348e+08, device='cuda:0')
c= tensor(5.2348e+08, device='cuda:0')
c= tensor(5.2507e+08, device='cuda:0')
c= tensor(5.2513e+08, device='cuda:0')
c= tensor(5.2513e+08, device='cuda:0')
c= tensor(5.2522e+08, device='cuda:0')
c= tensor(5.2529e+08, device='cuda:0')
c= tensor(5.2529e+08, device='cuda:0')
c= tensor(5.2598e+08, device='cuda:0')
c= tensor(5.2718e+08, device='cuda:0')
c= tensor(5.2906e+08, device='cuda:0')
c= tensor(5.3055e+08, device='cuda:0')
c= tensor(5.3390e+08, device='cuda:0')
c= tensor(5.3392e+08, device='cuda:0')
c= tensor(5.3395e+08, device='cuda:0')
c= tensor(5.3405e+08, device='cuda:0')
c= tensor(5.3453e+08, device='cuda:0')
c= tensor(5.3453e+08, device='cuda:0')
c= tensor(5.3546e+08, device='cuda:0')
c= tensor(5.5035e+08, device='cuda:0')
c= tensor(5.5261e+08, device='cuda:0')
c= tensor(5.5330e+08, device='cuda:0')
c= tensor(5.5533e+08, device='cuda:0')
c= tensor(5.5534e+08, device='cuda:0')
c= tensor(5.5534e+08, device='cuda:0')
c= tensor(5.5534e+08, device='cuda:0')
c= tensor(5.5578e+08, device='cuda:0')
c= tensor(5.5606e+08, device='cuda:0')
c= tensor(5.7317e+08, device='cuda:0')
c= tensor(5.7577e+08, device='cuda:0')
c= tensor(5.7683e+08, device='cuda:0')
c= tensor(5.7695e+08, device='cuda:0')
c= tensor(5.7746e+08, device='cuda:0')
c= tensor(5.7746e+08, device='cuda:0')
c= tensor(5.7748e+08, device='cuda:0')
c= tensor(5.8057e+08, device='cuda:0')
c= tensor(5.8057e+08, device='cuda:0')
c= tensor(5.8057e+08, device='cuda:0')
c= tensor(5.8065e+08, device='cuda:0')
c= tensor(5.8777e+08, device='cuda:0')
c= tensor(5.8779e+08, device='cuda:0')
c= tensor(5.8933e+08, device='cuda:0')
c= tensor(5.8933e+08, device='cuda:0')
c= tensor(5.8934e+08, device='cuda:0')
c= tensor(5.8934e+08, device='cuda:0')
c= tensor(5.8934e+08, device='cuda:0')
c= tensor(5.8971e+08, device='cuda:0')
c= tensor(5.8978e+08, device='cuda:0')
c= tensor(5.8979e+08, device='cuda:0')
c= tensor(5.9070e+08, device='cuda:0')
c= tensor(5.9070e+08, device='cuda:0')
c= tensor(5.9157e+08, device='cuda:0')
c= tensor(5.9158e+08, device='cuda:0')
c= tensor(5.9162e+08, device='cuda:0')
c= tensor(5.9162e+08, device='cuda:0')
c= tensor(5.9164e+08, device='cuda:0')
c= tensor(5.9164e+08, device='cuda:0')
c= tensor(5.9170e+08, device='cuda:0')
c= tensor(5.9267e+08, device='cuda:0')
c= tensor(5.9688e+08, device='cuda:0')
c= tensor(5.9689e+08, device='cuda:0')
c= tensor(5.9690e+08, device='cuda:0')
c= tensor(6.0003e+08, device='cuda:0')
c= tensor(6.0004e+08, device='cuda:0')
c= tensor(6.2152e+08, device='cuda:0')
c= tensor(6.2152e+08, device='cuda:0')
c= tensor(6.2172e+08, device='cuda:0')
c= tensor(6.2223e+08, device='cuda:0')
c= tensor(6.2223e+08, device='cuda:0')
c= tensor(6.2376e+08, device='cuda:0')
c= tensor(6.2391e+08, device='cuda:0')
c= tensor(6.8551e+08, device='cuda:0')
c= tensor(6.8552e+08, device='cuda:0')
c= tensor(6.8552e+08, device='cuda:0')
c= tensor(6.8554e+08, device='cuda:0')
c= tensor(6.8554e+08, device='cuda:0')
c= tensor(6.8556e+08, device='cuda:0')
c= tensor(6.8567e+08, device='cuda:0')
c= tensor(6.8573e+08, device='cuda:0')
c= tensor(6.8584e+08, device='cuda:0')
c= tensor(6.8587e+08, device='cuda:0')
c= tensor(6.8587e+08, device='cuda:0')
c= tensor(6.8588e+08, device='cuda:0')
c= tensor(6.9840e+08, device='cuda:0')
c= tensor(6.9911e+08, device='cuda:0')
c= tensor(7.0342e+08, device='cuda:0')
c= tensor(7.0349e+08, device='cuda:0')
c= tensor(7.0352e+08, device='cuda:0')
c= tensor(7.0352e+08, device='cuda:0')
c= tensor(7.0355e+08, device='cuda:0')
c= tensor(7.1062e+08, device='cuda:0')
c= tensor(7.1065e+08, device='cuda:0')
c= tensor(7.1066e+08, device='cuda:0')
c= tensor(7.1070e+08, device='cuda:0')
c= tensor(7.1114e+08, device='cuda:0')
c= tensor(7.1115e+08, device='cuda:0')
c= tensor(7.1115e+08, device='cuda:0')
c= tensor(7.4297e+08, device='cuda:0')
c= tensor(7.4300e+08, device='cuda:0')
c= tensor(7.4310e+08, device='cuda:0')
c= tensor(7.4311e+08, device='cuda:0')
c= tensor(7.4404e+08, device='cuda:0')
c= tensor(7.5029e+08, device='cuda:0')
c= tensor(7.5266e+08, device='cuda:0')
c= tensor(7.5379e+08, device='cuda:0')
c= tensor(7.5382e+08, device='cuda:0')
c= tensor(7.5402e+08, device='cuda:0')
c= tensor(7.5411e+08, device='cuda:0')
c= tensor(7.5411e+08, device='cuda:0')
c= tensor(7.5414e+08, device='cuda:0')
c= tensor(7.5414e+08, device='cuda:0')
c= tensor(7.5446e+08, device='cuda:0')
c= tensor(7.5447e+08, device='cuda:0')
c= tensor(7.5447e+08, device='cuda:0')
c= tensor(7.5449e+08, device='cuda:0')
c= tensor(7.5450e+08, device='cuda:0')
c= tensor(7.5473e+08, device='cuda:0')
c= tensor(7.5474e+08, device='cuda:0')
c= tensor(7.5488e+08, device='cuda:0')
c= tensor(7.5501e+08, device='cuda:0')
c= tensor(7.5505e+08, device='cuda:0')
c= tensor(7.5505e+08, device='cuda:0')
c= tensor(7.5506e+08, device='cuda:0')
c= tensor(7.5510e+08, device='cuda:0')
c= tensor(7.5957e+08, device='cuda:0')
c= tensor(7.5958e+08, device='cuda:0')
c= tensor(7.5958e+08, device='cuda:0')
c= tensor(7.5958e+08, device='cuda:0')
c= tensor(7.7140e+08, device='cuda:0')
c= tensor(7.7917e+08, device='cuda:0')
c= tensor(7.7922e+08, device='cuda:0')
c= tensor(7.7923e+08, device='cuda:0')
c= tensor(7.7938e+08, device='cuda:0')
c= tensor(7.8010e+08, device='cuda:0')
c= tensor(7.8010e+08, device='cuda:0')
c= tensor(7.8013e+08, device='cuda:0')
c= tensor(7.8014e+08, device='cuda:0')
c= tensor(7.8114e+08, device='cuda:0')
c= tensor(7.8115e+08, device='cuda:0')
c= tensor(7.8590e+08, device='cuda:0')
c= tensor(7.8591e+08, device='cuda:0')
c= tensor(7.8591e+08, device='cuda:0')
c= tensor(7.8591e+08, device='cuda:0')
c= tensor(7.8599e+08, device='cuda:0')
c= tensor(7.8600e+08, device='cuda:0')
c= tensor(7.8608e+08, device='cuda:0')
c= tensor(8.0213e+08, device='cuda:0')
c= tensor(8.2949e+08, device='cuda:0')
c= tensor(8.2950e+08, device='cuda:0')
c= tensor(8.2950e+08, device='cuda:0')
c= tensor(8.2953e+08, device='cuda:0')
c= tensor(8.4336e+08, device='cuda:0')
c= tensor(8.4374e+08, device='cuda:0')
c= tensor(8.4374e+08, device='cuda:0')
c= tensor(8.4374e+08, device='cuda:0')
c= tensor(8.4427e+08, device='cuda:0')
c= tensor(8.4427e+08, device='cuda:0')
c= tensor(8.4503e+08, device='cuda:0')
c= tensor(8.4503e+08, device='cuda:0')
c= tensor(8.4506e+08, device='cuda:0')
c= tensor(8.4507e+08, device='cuda:0')
c= tensor(8.4507e+08, device='cuda:0')
c= tensor(8.4507e+08, device='cuda:0')
c= tensor(8.4684e+08, device='cuda:0')
c= tensor(8.5308e+08, device='cuda:0')
c= tensor(8.5608e+08, device='cuda:0')
c= tensor(8.5615e+08, device='cuda:0')
c= tensor(8.5624e+08, device='cuda:0')
c= tensor(8.5625e+08, device='cuda:0')
c= tensor(8.5625e+08, device='cuda:0')
c= tensor(8.5654e+08, device='cuda:0')
c= tensor(8.5663e+08, device='cuda:0')
c= tensor(8.6160e+08, device='cuda:0')
c= tensor(8.6162e+08, device='cuda:0')
c= tensor(9.1038e+08, device='cuda:0')
c= tensor(9.1040e+08, device='cuda:0')
c= tensor(9.1064e+08, device='cuda:0')
c= tensor(9.1341e+08, device='cuda:0')
c= tensor(9.1361e+08, device='cuda:0')
c= tensor(9.1390e+08, device='cuda:0')
c= tensor(9.1561e+08, device='cuda:0')
c= tensor(9.1569e+08, device='cuda:0')
c= tensor(9.1573e+08, device='cuda:0')
c= tensor(9.1574e+08, device='cuda:0')
c= tensor(9.1590e+08, device='cuda:0')
c= tensor(9.1596e+08, device='cuda:0')
c= tensor(9.1653e+08, device='cuda:0')
c= tensor(9.6975e+08, device='cuda:0')
c= tensor(9.7170e+08, device='cuda:0')
c= tensor(9.7585e+08, device='cuda:0')
c= tensor(9.7624e+08, device='cuda:0')
c= tensor(9.7632e+08, device='cuda:0')
c= tensor(9.7648e+08, device='cuda:0')
c= tensor(9.7650e+08, device='cuda:0')
c= tensor(9.7651e+08, device='cuda:0')
c= tensor(9.7850e+08, device='cuda:0')
c= tensor(9.7857e+08, device='cuda:0')
c= tensor(9.8261e+08, device='cuda:0')
c= tensor(1.0300e+09, device='cuda:0')
c= tensor(1.0300e+09, device='cuda:0')
c= tensor(1.0300e+09, device='cuda:0')
c= tensor(1.0308e+09, device='cuda:0')
c= tensor(1.0310e+09, device='cuda:0')
c= tensor(1.0310e+09, device='cuda:0')
c= tensor(1.0370e+09, device='cuda:0')
c= tensor(1.0373e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(1.0375e+09, device='cuda:0')
c= tensor(2.0860e+09, device='cuda:0')
c= tensor(2.0860e+09, device='cuda:0')
c= tensor(2.0860e+09, device='cuda:0')
c= tensor(2.0860e+09, device='cuda:0')
c= tensor(2.0860e+09, device='cuda:0')
c= tensor(2.0861e+09, device='cuda:0')
c= tensor(2.0893e+09, device='cuda:0')
c= tensor(2.0893e+09, device='cuda:0')
c= tensor(2.1138e+09, device='cuda:0')
c= tensor(2.1138e+09, device='cuda:0')
c= tensor(2.1183e+09, device='cuda:0')
c= tensor(2.1186e+09, device='cuda:0')
c= tensor(2.1215e+09, device='cuda:0')
c= tensor(2.1376e+09, device='cuda:0')
c= tensor(2.1376e+09, device='cuda:0')
c= tensor(2.1376e+09, device='cuda:0')
c= tensor(2.1382e+09, device='cuda:0')
c= tensor(2.1382e+09, device='cuda:0')
c= tensor(2.1383e+09, device='cuda:0')
c= tensor(2.1383e+09, device='cuda:0')
c= tensor(2.1383e+09, device='cuda:0')
c= tensor(2.1386e+09, device='cuda:0')
c= tensor(2.1386e+09, device='cuda:0')
c= tensor(2.1426e+09, device='cuda:0')
c= tensor(2.1478e+09, device='cuda:0')
c= tensor(2.1478e+09, device='cuda:0')
c= tensor(2.1478e+09, device='cuda:0')
c= tensor(2.1502e+09, device='cuda:0')
c= tensor(2.1502e+09, device='cuda:0')
c= tensor(2.1505e+09, device='cuda:0')
c= tensor(2.1505e+09, device='cuda:0')
c= tensor(2.1511e+09, device='cuda:0')
c= tensor(2.1512e+09, device='cuda:0')
c= tensor(2.1523e+09, device='cuda:0')
c= tensor(2.1554e+09, device='cuda:0')
c= tensor(2.1554e+09, device='cuda:0')
c= tensor(2.1554e+09, device='cuda:0')
c= tensor(2.1555e+09, device='cuda:0')
c= tensor(2.1556e+09, device='cuda:0')
c= tensor(2.1561e+09, device='cuda:0')
c= tensor(2.1592e+09, device='cuda:0')
c= tensor(2.1592e+09, device='cuda:0')
c= tensor(2.1593e+09, device='cuda:0')
c= tensor(2.1594e+09, device='cuda:0')
c= tensor(2.1616e+09, device='cuda:0')
c= tensor(2.1618e+09, device='cuda:0')
c= tensor(2.1622e+09, device='cuda:0')
c= tensor(2.1627e+09, device='cuda:0')
c= tensor(2.1627e+09, device='cuda:0')
c= tensor(2.1627e+09, device='cuda:0')
c= tensor(2.1628e+09, device='cuda:0')
c= tensor(2.1628e+09, device='cuda:0')
c= tensor(2.1629e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1630e+09, device='cuda:0')
c= tensor(2.1632e+09, device='cuda:0')
c= tensor(2.1632e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1633e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1639e+09, device='cuda:0')
c= tensor(2.1702e+09, device='cuda:0')
c= tensor(2.1705e+09, device='cuda:0')
c= tensor(2.1708e+09, device='cuda:0')
c= tensor(2.1717e+09, device='cuda:0')
c= tensor(2.1731e+09, device='cuda:0')
c= tensor(2.1769e+09, device='cuda:0')
c= tensor(2.1781e+09, device='cuda:0')
c= tensor(2.1781e+09, device='cuda:0')
c= tensor(2.1781e+09, device='cuda:0')
c= tensor(2.1781e+09, device='cuda:0')
c= tensor(2.1783e+09, device='cuda:0')
c= tensor(2.1810e+09, device='cuda:0')
c= tensor(2.1810e+09, device='cuda:0')
c= tensor(2.1813e+09, device='cuda:0')
c= tensor(2.1813e+09, device='cuda:0')
c= tensor(2.1814e+09, device='cuda:0')
c= tensor(2.1814e+09, device='cuda:0')
c= tensor(2.1815e+09, device='cuda:0')
c= tensor(2.1867e+09, device='cuda:0')
c= tensor(2.3004e+09, device='cuda:0')
c= tensor(2.3007e+09, device='cuda:0')
c= tensor(2.3007e+09, device='cuda:0')
c= tensor(2.3008e+09, device='cuda:0')
c= tensor(2.3011e+09, device='cuda:0')
c= tensor(2.3011e+09, device='cuda:0')
c= tensor(2.3011e+09, device='cuda:0')
c= tensor(2.3012e+09, device='cuda:0')
c= tensor(2.3022e+09, device='cuda:0')
c= tensor(2.3022e+09, device='cuda:0')
c= tensor(2.3028e+09, device='cuda:0')
c= tensor(2.3028e+09, device='cuda:0')
time to make c is 11.71027398109436
time for making loss is 11.71028995513916
p0 True
it  0 : 1174177792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4245884928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
4246298624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  396223740.0
relative error loss 0.17205808
shape of L is 
torch.Size([])
memory (bytes)
4273135616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4273156096
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 13% |
error is  390436350.0
relative error loss 0.16954494
shape of L is 
torch.Size([])
memory (bytes)
4276781056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4276989952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  378844670.0
relative error loss 0.16451131
shape of L is 
torch.Size([])
memory (bytes)
4280188928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4280188928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  372049400.0
relative error loss 0.1615605
shape of L is 
torch.Size([])
memory (bytes)
4283265024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
4283265024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  366951420.0
relative error loss 0.15934673
shape of L is 
torch.Size([])
memory (bytes)
4286529536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4286529536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  363311870.0
relative error loss 0.15776627
shape of L is 
torch.Size([])
memory (bytes)
4289830912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4289830912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  360561920.0
relative error loss 0.15657212
shape of L is 
torch.Size([])
memory (bytes)
4292956160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4293062656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  358381060.0
relative error loss 0.15562509
shape of L is 
torch.Size([])
memory (bytes)
4296290304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4296290304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  356602100.0
relative error loss 0.1548526
shape of L is 
torch.Size([])
memory (bytes)
4299386880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4299386880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  354891780.0
relative error loss 0.1541099
time to take a step is 316.4975953102112
it  1 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4302749696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4302749696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  354891780.0
relative error loss 0.1541099
shape of L is 
torch.Size([])
memory (bytes)
4305899520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4305899520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  353348220.0
relative error loss 0.15343961
shape of L is 
torch.Size([])
memory (bytes)
4309180416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 13% |
memory (bytes)
4309180416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  352585600.0
relative error loss 0.15310845
shape of L is 
torch.Size([])
memory (bytes)
4312395776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 13% |
memory (bytes)
4312395776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  351239040.0
relative error loss 0.15252371
shape of L is 
torch.Size([])
memory (bytes)
4315578368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
4315578368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  350599040.0
relative error loss 0.15224579
shape of L is 
torch.Size([])
memory (bytes)
4318842880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 13% |
memory (bytes)
4318842880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  349640060.0
relative error loss 0.15182936
shape of L is 
torch.Size([])
memory (bytes)
4322058240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
4322058240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  348262800.0
relative error loss 0.15123129
shape of L is 
torch.Size([])
memory (bytes)
4325289984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
4325289984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  347994370.0
relative error loss 0.15111473
shape of L is 
torch.Size([])
memory (bytes)
4328493056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
4328493056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  347024770.0
relative error loss 0.15069368
shape of L is 
torch.Size([])
memory (bytes)
4331642880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
4331642880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  346612100.0
relative error loss 0.15051448
time to take a step is 229.37702751159668
it  2 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4334948352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 13% |
memory (bytes)
4334948352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  346612100.0
relative error loss 0.15051448
shape of L is 
torch.Size([])
memory (bytes)
4338139136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
4338139136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  345794700.0
relative error loss 0.15015952
shape of L is 
torch.Size([])
memory (bytes)
4341145600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
4341145600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  345032960.0
relative error loss 0.14982875
shape of L is 
torch.Size([])
memory (bytes)
4344610816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
4344610816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  344351600.0
relative error loss 0.14953288
shape of L is 
torch.Size([])
memory (bytes)
4347555840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
4347789312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  343896700.0
relative error loss 0.14933534
shape of L is 
torch.Size([])
memory (bytes)
4351041536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 13% |
memory (bytes)
4351049728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  343359500.0
relative error loss 0.14910206
shape of L is 
torch.Size([])
memory (bytes)
4354269184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
4354269184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  342783870.0
relative error loss 0.1488521
shape of L is 
torch.Size([])
memory (bytes)
4357316608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 13% |
memory (bytes)
4357316608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  342635900.0
relative error loss 0.14878784
shape of L is 
torch.Size([])
memory (bytes)
4360609792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4360609792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  342150140.0
relative error loss 0.1485769
shape of L is 
torch.Size([])
memory (bytes)
4363923456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4363923456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  341970700.0
relative error loss 0.14849898
time to take a step is 225.7151517868042
it  3 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4367052800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
4367142912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  341970700.0
relative error loss 0.14849898
shape of L is 
torch.Size([])
memory (bytes)
4370354176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4370354176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  341624830.0
relative error loss 0.1483488
shape of L is 
torch.Size([])
memory (bytes)
4373381120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4373471232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  341351940.0
relative error loss 0.14823028
shape of L is 
torch.Size([])
memory (bytes)
4376788992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4376788992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  341023870.0
relative error loss 0.14808783
shape of L is 
torch.Size([])
memory (bytes)
4379848704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4380020736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  340757380.0
relative error loss 0.1479721
shape of L is 
torch.Size([])
memory (bytes)
4383232000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
4383232000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  340469250.0
relative error loss 0.14784698
shape of L is 
torch.Size([])
memory (bytes)
4386410496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
4386410496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  340112000.0
relative error loss 0.14769185
shape of L is 
torch.Size([])
memory (bytes)
4389675008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
4389675008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  339729150.0
relative error loss 0.14752561
shape of L is 
torch.Size([])
memory (bytes)
4392894464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 13% |
memory (bytes)
4392894464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  339500300.0
relative error loss 0.14742622
shape of L is 
torch.Size([])
memory (bytes)
4395986944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4396077056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  339279360.0
relative error loss 0.14733028
time to take a step is 295.60157322883606
it  4 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4399341568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4399341568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  339279360.0
relative error loss 0.14733028
shape of L is 
torch.Size([])
memory (bytes)
4402511872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4402511872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  338912130.0
relative error loss 0.14717081
shape of L is 
torch.Size([])
memory (bytes)
4405772288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 13% |
memory (bytes)
4405772288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  338629500.0
relative error loss 0.14704809
shape of L is 
torch.Size([])
memory (bytes)
4408852480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4408991744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  338468600.0
relative error loss 0.14697821
shape of L is 
torch.Size([])
memory (bytes)
4412010496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4412211200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  338321540.0
relative error loss 0.14691435
shape of L is 
torch.Size([])
memory (bytes)
4415438848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
4415438848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  338140160.0
relative error loss 0.1468356
shape of L is 
torch.Size([])
memory (bytes)
4418580480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4418580480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  338037760.0
relative error loss 0.14679113
shape of L is 
torch.Size([])
memory (bytes)
4421885952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4421885952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  337951600.0
relative error loss 0.14675371
shape of L is 
torch.Size([])
memory (bytes)
4425031680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4425031680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  337879040.0
relative error loss 0.1467222
shape of L is 
torch.Size([])
memory (bytes)
4428328960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4428328960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  337801860.0
relative error loss 0.14668868
time to take a step is 297.6976261138916
it  5 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4431556608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4431556608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  337801860.0
relative error loss 0.14668868
shape of L is 
torch.Size([])
memory (bytes)
4434767872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
4434767872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  337869300.0
relative error loss 0.14671798
shape of L is 
torch.Size([])
memory (bytes)
4437819392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 13% |
memory (bytes)
4437987328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  337732480.0
relative error loss 0.14665855
shape of L is 
torch.Size([])
memory (bytes)
4441120768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4441120768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  337622530.0
relative error loss 0.14661081
shape of L is 
torch.Size([])
memory (bytes)
4444434432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4444442624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  337512320.0
relative error loss 0.14656295
shape of L is 
torch.Size([])
memory (bytes)
4447617024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4447617024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  337370620.0
relative error loss 0.14650142
shape of L is 
torch.Size([])
memory (bytes)
4450889728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4450889728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  337172480.0
relative error loss 0.14641538
shape of L is 
torch.Size([])
memory (bytes)
4454080512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 13% |
memory (bytes)
4454080512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  337054600.0
relative error loss 0.14636418
shape of L is 
torch.Size([])
memory (bytes)
4457320448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4457320448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336878720.0
relative error loss 0.14628781
shape of L is 
torch.Size([])
memory (bytes)
4460531712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4460531712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336785540.0
relative error loss 0.14624736
time to take a step is 296.09394454956055
it  6 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4463652864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4463771648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336785540.0
relative error loss 0.14624736
shape of L is 
torch.Size([])
memory (bytes)
4466982912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4466982912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336630660.0
relative error loss 0.1461801
shape of L is 
torch.Size([])
memory (bytes)
4470030336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4470030336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336734460.0
relative error loss 0.14622517
shape of L is 
torch.Size([])
memory (bytes)
4473421824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4473421824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  336542200.0
relative error loss 0.1461417
shape of L is 
torch.Size([])
memory (bytes)
4476563456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4476653568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  336425200.0
relative error loss 0.14609088
shape of L is 
torch.Size([])
memory (bytes)
4479778816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4479778816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336351360.0
relative error loss 0.14605881
shape of L is 
torch.Size([])
memory (bytes)
4483080192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 13% |
memory (bytes)
4483080192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336242050.0
relative error loss 0.14601135
shape of L is 
torch.Size([])
memory (bytes)
4486311936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4486311936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336159740.0
relative error loss 0.1459756
shape of L is 
torch.Size([])
memory (bytes)
4489539584
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4489539584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336106240.0
relative error loss 0.14595237
shape of L is 
torch.Size([])
memory (bytes)
4492713984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4492713984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336023800.0
relative error loss 0.14591658
time to take a step is 296.2843990325928
it  7 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4495896576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4495896576
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 13% |
error is  336023800.0
relative error loss 0.14591658
shape of L is 
torch.Size([])
memory (bytes)
4499206144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
4499206144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  336303600.0
relative error loss 0.14603809
shape of L is 
torch.Size([])
memory (bytes)
4502310912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4502310912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335960450.0
relative error loss 0.14588906
shape of L is 
torch.Size([])
memory (bytes)
4505665536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4505665536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335875600.0
relative error loss 0.14585221
shape of L is 
torch.Size([])
memory (bytes)
4508876800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4508876800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335811840.0
relative error loss 0.14582454
shape of L is 
torch.Size([])
memory (bytes)
4511973376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
4512100352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335771400.0
relative error loss 0.14580697
shape of L is 
torch.Size([])
memory (bytes)
4515311616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4515311616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  335729800.0
relative error loss 0.14578891
shape of L is 
torch.Size([])
memory (bytes)
4518486016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 13% |
memory (bytes)
4518486016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335665400.0
relative error loss 0.14576094
shape of L is 
torch.Size([])
memory (bytes)
4521754624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
4521754624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335643650.0
relative error loss 0.14575149
shape of L is 
torch.Size([])
memory (bytes)
4524806144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4524969984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  335599360.0
relative error loss 0.14573227
time to take a step is 295.80681109428406
it  8 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4528181248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
4528181248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335599360.0
relative error loss 0.14573227
shape of L is 
torch.Size([])
memory (bytes)
4531400704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4531400704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  335584000.0
relative error loss 0.1457256
shape of L is 
torch.Size([])
memory (bytes)
4534534144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4534624256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335558140.0
relative error loss 0.14571436
shape of L is 
torch.Size([])
memory (bytes)
4537847808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
4537847808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335511300.0
relative error loss 0.14569402
shape of L is 
torch.Size([])
memory (bytes)
4540878848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4541001728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 13% |
error is  335454340.0
relative error loss 0.14566928
shape of L is 
torch.Size([])
memory (bytes)
4544290816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4544290816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  335418500.0
relative error loss 0.14565372
shape of L is 
torch.Size([])
memory (bytes)
4547510272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4547510272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335346800.0
relative error loss 0.1456226
shape of L is 
torch.Size([])
memory (bytes)
4550631424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4550631424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335256830.0
relative error loss 0.14558353
shape of L is 
torch.Size([])
memory (bytes)
4553940992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4553940992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335192060.0
relative error loss 0.14555539
shape of L is 
torch.Size([])
memory (bytes)
4557058048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
4557058048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335061500.0
relative error loss 0.14549871
time to take a step is 296.4610252380371
it  9 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4560384000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4560384000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  335061500.0
relative error loss 0.14549871
shape of L is 
torch.Size([])
memory (bytes)
4563537920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4563537920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  334913020.0
relative error loss 0.14543423
shape of L is 
torch.Size([])
memory (bytes)
4566724608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4566814720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  334847230.0
relative error loss 0.14540565
shape of L is 
torch.Size([])
memory (bytes)
4570046464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 13% |
memory (bytes)
4570046464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  334771460.0
relative error loss 0.14537275
shape of L is 
torch.Size([])
memory (bytes)
4573216768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4573216768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  334640260.0
relative error loss 0.14531578
shape of L is 
torch.Size([])
memory (bytes)
4576481280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4576481280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  334521600.0
relative error loss 0.14526425
shape of L is 
torch.Size([])
memory (bytes)
4579700736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
4579700736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  334467460.0
relative error loss 0.14524074
shape of L is 
torch.Size([])
memory (bytes)
4582916096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4582916096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  334415230.0
relative error loss 0.14521806
shape of L is 
torch.Size([])
memory (bytes)
4586041344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 13% |
memory (bytes)
4586041344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  334304640.0
relative error loss 0.14517003
shape of L is 
torch.Size([])
memory (bytes)
4589281280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4589281280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  334230400.0
relative error loss 0.1451378
time to take a step is 299.51620650291443
it  10 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4592562176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4592562176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  334230400.0
relative error loss 0.1451378
shape of L is 
torch.Size([])
memory (bytes)
4595748864
| ID | GPU | MEM |
------------------
|  0 | 22% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4595748864
| ID | GPU | MEM |
------------------
|  0 | 18% |  0% |
|  1 | 99% | 13% |
error is  334138620.0
relative error loss 0.14509794
shape of L is 
torch.Size([])
memory (bytes)
4599013376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4599013376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  334044540.0
relative error loss 0.1450571
shape of L is 
torch.Size([])
memory (bytes)
4602232832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
4602232832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333974800.0
relative error loss 0.1450268
shape of L is 
torch.Size([])
memory (bytes)
4605452288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4605452288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  333931000.0
relative error loss 0.14500779
shape of L is 
torch.Size([])
memory (bytes)
4608606208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4608606208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 13% |
error is  333903870.0
relative error loss 0.144996
shape of L is 
torch.Size([])
memory (bytes)
4611911680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 13% |
memory (bytes)
4611911680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333872000.0
relative error loss 0.14498216
shape of L is 
torch.Size([])
memory (bytes)
4615122944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4615127040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333843070.0
relative error loss 0.1449696
shape of L is 
torch.Size([])
memory (bytes)
4618313728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 13% |
memory (bytes)
4618313728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333808130.0
relative error loss 0.14495443
shape of L is 
torch.Size([])
memory (bytes)
4621545472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4621545472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333735680.0
relative error loss 0.14492297
time to take a step is 298.4122772216797
it  11 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4624756736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4624756736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333735680.0
relative error loss 0.14492297
shape of L is 
torch.Size([])
memory (bytes)
4627980288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 13% |
memory (bytes)
4627980288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333663870.0
relative error loss 0.14489178
shape of L is 
torch.Size([])
memory (bytes)
4631097344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4631097344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333614200.0
relative error loss 0.14487022
shape of L is 
torch.Size([])
memory (bytes)
4634427392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
4634427392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333565200.0
relative error loss 0.14484893
shape of L is 
torch.Size([])
memory (bytes)
4637642752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 13% |
memory (bytes)
4637642752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333499780.0
relative error loss 0.14482053
shape of L is 
torch.Size([])
memory (bytes)
4640858112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4640858112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333448700.0
relative error loss 0.14479835
shape of L is 
torch.Size([])
memory (bytes)
4644085760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4644085760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  333419780.0
relative error loss 0.14478579
shape of L is 
torch.Size([])
memory (bytes)
4647219200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4647219200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333410800.0
relative error loss 0.1447819
shape of L is 
torch.Size([])
memory (bytes)
4650532864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 13% |
memory (bytes)
4650532864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 13% |
error is  333347200.0
relative error loss 0.14475428
shape of L is 
torch.Size([])
memory (bytes)
4653662208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 13% |
memory (bytes)
4653662208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  333327100.0
relative error loss 0.14474554
time to take a step is 298.469749212265
it  12 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4656967680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4656967680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333327100.0
relative error loss 0.14474554
shape of L is 
torch.Size([])
memory (bytes)
4660191232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4660191232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 13% |
error is  333299600.0
relative error loss 0.1447336
shape of L is 
torch.Size([])
memory (bytes)
4663320576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
4663320576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333262460.0
relative error loss 0.14471747
shape of L is 
torch.Size([])
memory (bytes)
4666630144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4666630144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333222660.0
relative error loss 0.14470018
shape of L is 
torch.Size([])
memory (bytes)
4669788160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 13% |
memory (bytes)
4669857792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333188100.0
relative error loss 0.14468518
shape of L is 
torch.Size([])
memory (bytes)
4673081344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4673081344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333147260.0
relative error loss 0.14466745
shape of L is 
torch.Size([])
memory (bytes)
4676153344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4676292608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333107200.0
relative error loss 0.14465006
shape of L is 
torch.Size([])
memory (bytes)
4679507968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
4679507968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333066240.0
relative error loss 0.14463226
shape of L is 
torch.Size([])
memory (bytes)
4682649600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4682649600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  333047940.0
relative error loss 0.14462432
shape of L is 
torch.Size([])
memory (bytes)
4685959168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4685967360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  333028740.0
relative error loss 0.14461598
time to take a step is 299.1196870803833
it  13 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4689170432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4689170432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  333028740.0
relative error loss 0.14461598
shape of L is 
torch.Size([])
memory (bytes)
4692393984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4692393984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332988300.0
relative error loss 0.14459842
shape of L is 
torch.Size([])
memory (bytes)
4695617536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 13% |
memory (bytes)
4695617536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332945660.0
relative error loss 0.1445799
shape of L is 
torch.Size([])
memory (bytes)
4698796032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4698796032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332890880.0
relative error loss 0.14455612
shape of L is 
torch.Size([])
memory (bytes)
4702060544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4702060544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332869900.0
relative error loss 0.144547
shape of L is 
torch.Size([])
memory (bytes)
4705259520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4705259520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332839040.0
relative error loss 0.1445336
shape of L is 
torch.Size([])
memory (bytes)
4708491264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4708491264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  332813200.0
relative error loss 0.14452238
shape of L is 
torch.Size([])
memory (bytes)
4711710720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 13% |
memory (bytes)
4711710720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332766340.0
relative error loss 0.14450203
shape of L is 
torch.Size([])
memory (bytes)
4714823680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 13% |
memory (bytes)
4714823680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332728450.0
relative error loss 0.14448558
shape of L is 
torch.Size([])
memory (bytes)
4718161920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4718161920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332701200.0
relative error loss 0.14447375
time to take a step is 298.63720536231995
it  14 : 1787644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
4721229824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
4721377280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332701200.0
relative error loss 0.14447375
shape of L is 
torch.Size([])
memory (bytes)
4724604928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4724604928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332715900.0
relative error loss 0.14448014
shape of L is 
torch.Size([])
memory (bytes)
4727771136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4727771136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332674430.0
relative error loss 0.14446212
shape of L is 
torch.Size([])
memory (bytes)
4731043840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4731043840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332633340.0
relative error loss 0.14444429
shape of L is 
torch.Size([])
memory (bytes)
4734271488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4734271488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332599040.0
relative error loss 0.14442939
shape of L is 
torch.Size([])
memory (bytes)
4737429504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
4737429504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332563700.0
relative error loss 0.14441405
shape of L is 
torch.Size([])
memory (bytes)
4740694016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
4740694016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332516860.0
relative error loss 0.1443937
shape of L is 
torch.Size([])
memory (bytes)
4743897088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4743897088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332516350.0
relative error loss 0.14439349
shape of L is 
torch.Size([])
memory (bytes)
4747141120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 13% |
memory (bytes)
4747141120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 13% |
error is  332486800.0
relative error loss 0.14438064
shape of L is 
torch.Size([])
memory (bytes)
4750360576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
4750360576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  332454900.0
relative error loss 0.1443668
time to take a step is 298.114848613739
sum tnnu_Z after tensor(7000037., device='cuda:0')
shape of features
(6043,)
shape of features
(6043,)
number of orig particles 24171
number of new particles after remove low mass 24171
tnuZ shape should be parts x labs
torch.Size([24171, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  396162020.0
relative error without small mass is  0.17203128
nnu_Z shape should be number of particles by maxV
(24171, 702)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
shape of features
(24171,)
Wed Feb 1 22:31:04 EST 2023
