Fri Feb 3 02:38:31 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 8194456
numbers of Z: 6610
shape of features
(6610,)
shape of features
(6610,)
ZX	Vol	Parts	Cubes	Eps
Z	0.009328180682780018	6610	6.61	0.11216701521424567
X	0.004861285218388949	780	0.78	0.1840286752986606
X	0.00601040047550988	2510	2.51	0.13378576189209454
X	0.0060087723391440175	338	0.338	0.2609894886527214
X	0.00511531633046706	681	0.681	0.19584318399292494
X	0.0059279893720479045	7775	7.775	0.09135570488148125
X	0.0050878379065004385	9470	9.47	0.08129448719695621
X	0.005040115119436439	8836	8.836	0.08293316688066381
X	0.00586173355869416	13138	13.138	0.07641271055234268
X	0.0049916625044621995	1230	1.23	0.15950710553277586
X	0.007236562529569098	6616	6.616	0.10303361577381853
X	0.005073592459645758	1224	1.224	0.16063668030004602
X	0.005466224550359399	34369	34.369	0.05418014846520203
X	0.00735190076468384	4481	4.481	0.11794374063157284
X	0.0050956828085288	15925	15.925	0.06839761871207051
X	0.005165265735232001	4965	4.965	0.1013268337759837
X	0.00511341458419787	14866	14.866	0.07006572589908987
X	0.005130631544784441	8671	8.671	0.08395254771825705
X	0.007065292797182798	6076	6.076	0.1051568253467908
X	0.0056104566748027196	134086	134.086	0.03471668582612242
X	0.005448640287313999	11386	11.386	0.07821781009057768
X	0.005026537143461999	8035	8.035	0.08552524724023892
X	0.0063436238881861996	119941	119.941	0.03753674299109687
X	0.005145194603840759	5875	5.875	0.09567489145502693
X	0.0052578879185316	2137	2.137	0.1350005695222557
X	0.0052412442904148605	3967	3.967	0.1097296685270406
X	0.005221199030328899	12187	12.187	0.07538607736601051
X	0.0053739948248501404	536	0.536	0.21563081187025562
X	0.005212501075604159	1672	1.672	0.14608421623337414
X	0.00495775556808762	2588	2.588	0.12419581058600201
X	0.00656008406219296	500238	500.238	0.023581833846241084
X	0.0048263786551828785	756	0.756	0.18550966110678235
X	0.00563773225392128	32082	32.082	0.05601199018490664
X	0.006574655948825761	1357	1.357	0.16921196202331443
X	0.0050973141754252806	2222	2.222	0.13188612732571217
X	0.0055414987615312	3770	3.77	0.11370039237856659
X	0.00564791359064163	5009	5.009	0.10408280887756355
X	0.005720088509385381	11436	11.436	0.07937971477177291
X	0.00498495034653366	280	0.28	0.2611164715300945
X	0.005924843108656379	680	0.68	0.2057734381269717
X	0.004686339015052799	495	0.495	0.21154828128383812
X	0.00584333427796365	1622	1.622	0.15329804883874867
X	0.006868024207896985	129	0.129	0.37619588060125936
X	0.004732147495669279	374	0.374	0.23302126566119802
X	0.0061917366487162185	624	0.624	0.21488658078236508
X	0.004595167543502399	305	0.305	0.2469830619573312
X	0.00456068818711086	252	0.252	0.26254874497450625
X	0.00615449927974552	391	0.391	0.2506139967445571
X	0.00496547075168176	1124	1.124	0.1640834223199367
X	0.00473069968692141	456	0.456	0.21809891857030522
X	0.007064184431422485	1459	1.459	0.16917476392593217
X	0.00565761151088605	1734	1.734	0.14831827864566918
X	0.0054899269452565	351	0.351	0.25008433159141624
X	0.00489061629876914	844	0.844	0.17961408135719886
X	0.006776859772725051	582	0.582	0.22665667050963298
X	0.005936498405643781	870	0.87	0.18967220845853935
X	0.005547914873302829	1175	1.175	0.1677634340403589
X	0.00496300950743332	1115	1.115	0.16449653208604514
X	0.004996967286726549	664	0.664	0.195965434889895
X	0.004811801580639902	725	0.725	0.1879273117349205
X	0.005313250711459599	583	0.583	0.20888018883337434
X	0.005321199445482613	759	0.759	0.1913914739650974
X	0.004916781583725599	403	0.403	0.23021058583247944
X	0.007241558671182109	741	0.741	0.2137984809691723
X	0.00514463958194592	3219	3.219	0.11691708871463631
X	0.004783560072343	1249	1.249	0.15645745610351977
X	0.00656127511134423	419	0.419	0.25018316074914376
X	0.004901974721248799	1007	1.007	0.16947820890083917
X	0.0051365472439655275	2610	2.61	0.1253170779771009
X	0.004920827886683519	1436	1.436	0.1507629332221814
X	0.0047817881378063995	394	0.394	0.22980776701597153
X	0.004815076787644179	2922	2.922	0.11811571101157135
X	0.00677039027342112	769	0.769	0.20648846130733317
X	0.0062952925856195755	1319	1.319	0.1683670262577063
X	0.00463152735269298	497	0.497	0.2104372485677523
X	0.0049492259643766105	1045	1.045	0.16793465667094204
X	0.005672036962181499	517	0.517	0.22220278763755724
X	0.00486909839062038	277	0.277	0.2600097261120743
X	0.005021822847547441	190	0.19	0.2978762851708698
X	0.00514348438887808	1139	1.139	0.16528923411288687
X	0.0070591301211311245	2591	2.591	0.13966725299935262
X	0.004926287578329511	944	0.944	0.17345330877724183
X	0.004788419199003001	509	0.509	0.21110156859859022
X	0.006777640770604199	631	0.631	0.22063939726602164
X	0.00515512559855842	415	0.415	0.23159564296609786
X	0.0056712703265439915	471	0.471	0.22920274465099089
X	0.004812707151458339	403	0.403	0.22857468502345937
X	0.00561534393558494	301	0.301	0.26521817889627625
X	0.004265873574443601	155	0.155	0.30192014684950685
X	0.007304801179425579	728	0.728	0.21568784610351371
X	0.0048422946188703	532	0.532	0.20879168176637247
X	0.004865336064972599	1192	1.192	0.15981303385574094
X	0.0049813328587548	363	0.363	0.23941221935621992
X	0.00539398826349811	351	0.351	0.2486189875601887
X	0.0052489958887719295	555	0.555	0.21147599799935846
X	0.0058353890260644	552	0.552	0.21947088363004089
X	0.007215246473970279	1022	1.022	0.19183746423176798
X	0.005853818863709459	575	0.575	0.2167323857789773
X	0.006603461952004059	3162	3.162	0.12782131213030487
X	0.00536466602203625	758	0.758	0.1919955493708976
X	0.004963725215021	1426	1.426	0.15155237105442312
X	0.00542888194657598	1312	1.312	0.1605433006856163
X	0.004810017909198719	627	0.627	0.19722397948205428
X	0.0052418526697093765	719	0.719	0.19390334746972854
X	0.005555612644939561	285	0.285	0.2691301577181301
X	0.005137321894171328	1635	1.635	0.14646696209941312
X	0.0048158755602966405	215	0.215	0.28188952134428713
X	0.005163292513889891	1292	1.292	0.1586918922293168
X	0.00619236104752706	293	0.293	0.2764794667976792
X	0.005501837576447872	418	0.418	0.23610788692582055
X	0.005701172646909601	316	0.316	0.26227629299413907
X	0.0050268399404974	366	0.366	0.2394811438879257
X	0.004702582839662799	289	0.289	0.25340365264378767
X	0.006175497140458199	462	0.462	0.23732490541894288
X	0.005306468714907751	711	0.711	0.19542434685225762
X	0.008048801075152356	714	0.714	0.22422171020760504
X	0.004971495426221751	953	0.953	0.1734328581177299
X	0.00549540404082504	713	0.713	0.19753165372001175
X	0.00486303577240922	350	0.35	0.24040664810644327
X	0.005049064279909599	1446	1.446	0.15170999787400877
X	0.0051543314326571105	272	0.272	0.2666047067302377
X	0.006678745455783324	4905	4.905	0.1108371355195682
X	0.005254784565246253	382	0.382	0.2396057214732979
X	0.00815721470524175	386	0.386	0.2764722389651679
X	0.006834418002332188	515	0.515	0.23675442387389165
X	0.00490939392981168	509	0.509	0.21286456240933402
X	0.006090887723613719	1091	1.091	0.17739931878589488
X	0.0046571126900224	221	0.221	0.27621133126349073
X	0.005190469451417918	537	0.537	0.21301529334290925
X	0.00607811102618863	2224	2.224	0.13981205376804354
X	0.0052647183919992185	644	0.644	0.20144806160945988
X	0.006830637822333068	3311	3.311	0.12730164800842367
X	0.004742787655479819	249	0.249	0.26706159876134516
X	0.005152595657803283	881	0.881	0.18017037601854513
X	0.0049141942268166	305	0.305	0.25257139291059866
X	0.006229978214545592	652	0.652	0.21220045498633738
X	0.004923578015460318	270	0.27	0.2632120008515635
X	0.0047854609831754985	334	0.334	0.24288038219903252
X	0.006122589063422699	290	0.29	0.27638366477544357
X	0.006661068181217912	570	0.57	0.22692883023446775
X	0.005067334043818899	420	0.42	0.22935598704767907
X	0.006534521446800844	522	0.522	0.23219227223468897
X	0.004571542139307999	255	0.255	0.2617223727810981
X	0.006558675188006825	1053	1.053	0.18399071936596367
X	0.00478829622896214	1145	1.145	0.16111099304595144
X	0.006416057273191126	614	0.614	0.21862506601943327
X	0.0049549282690508996	513	0.513	0.21296423219856656
X	0.0057098075337749	556	0.556	0.21736130152455482
X	0.00776795988210628	488	0.488	0.2515527391218618
X	0.0058508046829718685	586	0.586	0.21533072110138923
X	0.004638855437160399	257	0.257	0.2623166788072317
X	0.0050733578149806	440	0.44	0.22591632027723205
X	0.00662299828657152	436	0.436	0.2476601041022942
X	0.006745414236589148	599	0.599	0.22414407718915416
X	0.005026499089371201	1126	1.126	0.1646553093608196
X	0.006000932674641449	1190	1.19	0.1714841286349159
X	0.007867486407212639	1782	1.782	0.16404994930503505
X	0.004892636086213499	1238	1.238	0.15810323508730717
X	0.004897182266605501	470	0.47	0.21841517224440282
X	0.005155191277470849	1083	1.083	0.16821786206944217
X	0.00478941505551	279	0.279	0.25796419092243283
X	0.00819178508211784	1470	1.47	0.17729115595245884
X	0.0046897711660901995	416	0.416	0.2242260710553016
X	0.006125018464266957	467	0.467	0.23582879969569456
X	0.004659369240757759	212	0.212	0.28011116672576103
X	0.005916749035117201	640	0.64	0.20987839041699144
X	0.006594551985460321	488	0.488	0.2381890678027595
X	0.004927181865183	731	0.731	0.1888978747928778
X	0.0049641034720662395	851	0.851	0.1800129525052657
X	0.00516070952372634	858	0.858	0.1818614749635591
X	0.004805970701779658	379	0.379	0.23319217312742332
X	0.007001381688481339	354	0.354	0.27043329774974584
X	0.00484552874944224	535	0.535	0.2084470712612455
X	0.006617830750331591	1861	1.861	0.1526358071505111
X	0.004952091298134119	463	0.463	0.22032775905425156
X	0.007635247406231032	582	0.582	0.23584864012357812
X	0.006951089145460371	448	0.448	0.24941636647003532
X	0.004985986575150679	1124	1.124	0.16430909283003794
X	0.005972047881062549	682	0.682	0.20611659945755698
X	0.00489519656402805	983	0.983	0.17076762530946335
X	0.004961466071647798	1117	1.117	0.1643812521277419
X	0.00503724688597398	568	0.568	0.20698949284173335
X	0.00457014765339632	377	0.377	0.22971867032103863
X	0.0050891110503196784	889	0.889	0.17888752507578942
X	0.00671074939378793	505	0.505	0.23686081224694377
X	0.00591896105084119	932	0.932	0.18518672725028998
X	0.0055702086163260086	1001	1.001	0.17720628775323008
X	0.0049461332846799985	656	0.656	0.19608934324198082
X	0.004765876813844249	504	0.504	0.21146447837831667
X	0.0058622331976869125	855	0.855	0.18997611053943553
X	0.007462560261575617	659	0.659	0.2245609027496434
X	0.0053782645855953316	638	0.638	0.20352003171863997
X	0.004334365884061118	388	0.388	0.2235448219437031
X	0.0050769233383342405	630	0.63	0.20048721608486073
X	0.004819773276842329	712	0.712	0.18916852255391745
X	0.005130773254430999	1615	1.615	0.1470065769582467
X	0.005754463227201424	393	0.393	0.24464560767565438
X	0.006150937198984501	455	0.455	0.23821912882503507
X	0.005709350679887908	251	0.251	0.2833376793468467
X	0.00459779039587673	866	0.866	0.17445245771856216
X	0.005556492043857135	255	0.255	0.2793102276612091
X	0.004823940141606449	1158	1.158	0.16090311439498625
X	0.004901204516859259	648	0.648	0.19629501609931188
X	0.004786522066149599	1688	1.688	0.14154109171740278
X	0.0056153542753056	455	0.455	0.23109395656743342
X	0.004781768072752	1027	1.027	0.16698316488700332
X	0.004965062291391999	892	0.892	0.17722294494996396
X	0.008734338840921754	269	0.269	0.31902588246824914
X	0.006225179733460186	436	0.436	0.24259869957420535
X	0.005066872210323199	312	0.312	0.25323775362743844
X	0.007556404146182424	1353	1.353	0.17742131495301502
X	0.006253933639130101	1505	1.505	0.1607702138923019
X	0.004858473820878541	295	0.295	0.2544247073556042
X	0.0049651658452149	742	0.742	0.1884415283856144
X	0.004942803053559001	2071	2.071	0.13363833900375421
X	0.00470012446385082	175	0.175	0.2994726070252725
X	0.004818653885962001	420	0.42	0.2255409891862288
X	0.00486490897772611	327	0.327	0.24594744887010375
X	0.004639882242315001	232	0.232	0.2714394653503043
X	0.004866167093371249	1320	1.32	0.1544795617787253
X	0.00496354772826198	1389	1.389	0.15288445128043202
X	0.004937934841358399	1553	1.553	0.14704760870621392
X	0.0047460902701482195	372	0.372	0.2336671624817902
X	0.006780171650447748	1641	1.641	0.16046390958528903
X	0.004840281756883461	859	0.859	0.1779477467140767
X	0.007129415448438911	453	0.453	0.2506024083069394
X	0.007014926499214172	1055	1.055	0.18804287756102173
X	0.004889961650100451	585	0.585	0.2029472616006365
X	0.004997095082354699	231	0.231	0.27863470666304974
X	0.00664949330501418	833	0.833	0.199854769566274
X	0.005320954582241231	370	0.37	0.24318079614934748
X	0.0055666650577342195	416	0.416	0.23741086303022182
X	0.004697519081072509	431	0.431	0.22171603031988757
X	0.006004767359232168	420	0.42	0.24270699803792353
X	0.007369480073503311	750	0.75	0.21418638543456786
X	0.005168276400789019	261	0.261	0.27054223582534176
X	0.0059617578496767845	504	0.504	0.2278494223673327
X	0.005759932217276301	790	0.79	0.19390846143408275
X	0.00685877147784277	910	0.91	0.19606571025042557
X	0.00478710532119132	644	0.644	0.19516219996454245
X	0.00518921027633853	1298	1.298	0.15871166911786716
X	0.004969486067603639	1290	1.29	0.15676192541324693
X	0.0064367119111091095	4344	4.344	0.11400522386832852
X	0.005661501792375239	1939	1.939	0.14292820734996325
X	0.005243596197664418	305	0.305	0.2580931432660889
X	0.006973724842399704	1540	1.54	0.16544285994622343
X	0.006291499098381916	3549	3.549	0.12102712398984655
X	0.004936538913382161	11595	11.595	0.0752287658711641
X	0.00663320383515798	727	0.727	0.20895991235400826
X	0.00502901785772088	4534	4.534	0.10351434871289707
X	0.005642279387313473	4305	4.305	0.10943606712587481
X	0.007042651152081804	2707	2.707	0.13753586831642423
X	0.006429426084982121	97981	97.981	0.04033450951467022
X	0.0049306190428964	641	0.641	0.19740029910793197
X	0.005352663565234561	1185	1.185	0.1653041914314745
X	0.00514876313719482	52513	52.513	0.046111794937565245
X	0.005874345115462328	201652	201.652	0.03076939624340047
X	0.00494603994491226	798	0.798	0.1836897441992867
X	0.005805685281343037	2167	2.167	0.13888804328500204
X	0.00570944983896138	2089	2.089	0.1398144120058741
X	0.0056584618249550574	4606	4.606	0.1071004968419882
X	0.004851843462969799	3673	3.673	0.10972239635147857
X	0.006584359404666126	8862	8.862	0.09057199369398682
X	0.0056998961458398735	5100	5.1	0.10377647676553223
X	0.0050952117315292205	2569	2.569	0.1256413836543214
X	0.005443617999458399	648	0.648	0.2032844747023285
X	0.006148086321429495	9828	9.828	0.08552470855589793
X	0.0057893284912706585	2272	2.272	0.13658613686108229
X	0.00494175939015888	538	0.538	0.20942720988602886
X	0.0051267888374648	15452	15.452	0.06922881697719124
X	0.006457046901783375	2446	2.446	0.1382056600521141
X	0.00655590796229686	17298	17.298	0.07236784691672242
X	0.0051276761119156	31049	31.049	0.054864444875356
X	0.005330206767139839	1687	1.687	0.14673813372620828
X	0.0048025379876165	1677	1.677	0.14200792514248978
X	0.006627324334113519	460	0.46	0.24332877203326952
X	0.00499865276745384	5580	5.58	0.09639908762135029
X	0.0052068423093485396	26848	26.848	0.05788341717697529
X	0.004880702043966781	869	0.869	0.17775459647401987
X	0.006807129256013998	25000	25.0	0.06481487127571776
X	0.004848906882437729	296	0.296	0.25397095050008617
X	0.005786567970282603	1639	1.639	0.15226989037764652
X	0.00507777527206575	1223	1.223	0.16072459488101556
X	0.004956486722989241	37964	37.964	0.050730287956820094
X	0.0066419819758524005	8674	8.674	0.09148700807478075
X	0.004856254337449999	312	0.312	0.24967914972048363
X	0.005721258724809401	50644	50.644	0.04834161952102541
X	0.005809350296381896	5427	5.427	0.10229535512477449
X	0.004987489677896	5473	5.473	0.09695097639985735
X	0.005020804222286731	16557	16.557	0.06718369403420091
X	0.006654322102432589	1262	1.262	0.17405298990510745
X	0.00556295884881885	8711	8.711	0.08611509357921088
X	0.006460182999850998	4014	4.014	0.11718962129418067
X	0.006331552167042901	27482	27.482	0.061303784337229945
X	0.005614515990016182	6913	6.913	0.09330005978832102
X	0.004830104988678481	6858	6.858	0.08897193379488494
X	0.0047834388204652505	9856	9.856	0.07858623989618453
X	0.0052709764926817	1879	1.879	0.1410332237258834
X	0.00491616751715208	2516	2.516	0.12501784603413688
X	0.008813648876916363	6403	6.403	0.11123908664856623
X	0.0051370417655769605	8878	8.878	0.08332960413250155
X	0.005906385116700131	29197	29.197	0.05870311905046039
X	0.005801179398823709	8711	8.711	0.08732717827810137
X	0.005037617818844801	2569	2.569	0.12516619135065193
X	0.007520851510838189	1039	1.039	0.19344187312878314
X	0.005288330178888735	8082	8.082	0.08681602667433709
X	0.005129588073095639	3276	3.276	0.11612156115953846
X	0.0047708488608632195	758	0.758	0.18463278371216724
X	0.0065577004977396	569	0.569	0.22588103573970036
X	0.005068961174811369	1848	1.848	0.13998204461332767
X	0.005326240566280499	1922	1.922	0.14046101365100258
X	0.00679913388322686	139269	139.269	0.03654823246757406
X	0.006227732579744228	1121	1.121	0.17710933307197083
X	0.004930172519371159	2011	2.011	0.13483935983988932
X	0.006084233765108639	9694	9.694	0.08561850209063245
X	0.004937224080167179	565	0.565	0.2059735544287858
X	0.0051280467794139716	7061	7.061	0.08988666255417159
X	0.0056266995760655275	3365	3.365	0.11869239312991485
X	0.00504165867381288	17473	17.473	0.06607979003900402
X	0.005698984835896799	30428	30.428	0.05721475350941807
X	0.0054209866294067394	1380	1.38	0.15778525519368233
X	0.0048580651258668	10158	10.158	0.07820209264233886
X	0.0069914477629596985	26334	26.334	0.06427125228839593
X	0.005489870810265041	57096	57.096	0.04581265699621861
X	0.00518346788758306	22933	22.933	0.06091438264793409
X	0.004855255142063119	745	0.745	0.18678927138648393
X	0.004893579172989182	4195	4.195	0.10526844749732646
X	0.0055875344640305994	31881	31.881	0.055962369312496776
X	0.005429038815916716	8842	8.842	0.08499449778024484
X	0.006836476196392709	4525	4.525	0.11474609309332494
X	0.0049604351032928005	585	0.585	0.20391756445105955
X	0.00536148589990912	34923	34.923	0.05354569272093199
X	0.0053793087064683415	28961	28.961	0.057056391943928905
X	0.004570797875241798	4186	4.186	0.10297479378966504
X	0.0055481789936986485	1973	1.973	0.14114791247753725
X	0.005067186508214199	7450	7.45	0.08794345811916558
X	0.005292645226585261	1670	1.67	0.14688770232683213
X	0.006391844588932191	2028	2.028	0.1466175929908073
X	0.005124442739104399	3164	3.164	0.1174365785818279
X	0.005500334266541332	1750	1.75	0.14648180342679504
X	0.004979898114496801	6804	6.804	0.09011948837907995
X	0.005283757904961511	8318	8.318	0.08596229431183153
X	0.006295219285927519	6401	6.401	0.0994460822686492
X	0.007018683114385301	8193	8.193	0.09497389004887392
X	0.005022074745133791	2440	2.44	0.12720312534684333
X	0.00591066949695378	176458	176.458	0.03223530431078261
X	0.005382189360630921	3016	3.016	0.12129490129069488
X	0.005893079539124805	27460	27.46	0.05987064312297497
X	0.0048452716742759	421	0.421	0.22577647023247843
X	0.00512292809221	1488	1.488	0.15099820208900228
X	0.005024469545699999	814	0.814	0.18343770338367812
X	0.005558084592155049	5940	5.94	0.0978091690025375
X	0.006729972604950144	1098	1.098	0.18300810277364118
X	0.007008507441994549	316	0.316	0.28096107251930313
X	0.00848459357962044	2496	2.496	0.1503587896178366
X	0.0066378251348068185	1130	1.13	0.18043292433547073
X	0.005492491752600542	80426	80.426	0.04087498892315292
X	0.00547591941244512	16704	16.704	0.06895178638810374
X	0.00668728169328936	1786	1.786	0.15528257387133218
X	0.006006374467451862	9217	9.217	0.08669770589355372
X	0.0048776704004088	1085	1.085	0.16504190862171916
X	0.005831996430897361	1088	1.088	0.17500996772853497
X	0.005697002993006509	2329	2.329	0.1347386897195167
X	0.005157139552914911	5716	5.716	0.09662857044822992
X	0.004527222063887999	220	0.22	0.2740331095930879
X	0.007162618603372	2600	2.6	0.140184337262421
X	0.005359855280200749	2182	2.182	0.13492717859736558
X	0.00487114150006	876	0.876	0.17716402823525124
X	0.005011698946022361	1550	1.55	0.14787148203307623
X	0.004896723051557389	6633	6.633	0.09037851090767234
X	0.004807816402789199	898	0.898	0.17494056012361023
X	0.005493299497615949	36991	36.991	0.052955662041273305
X	0.00488016450230981	2522	2.522	0.12461293540355221
X	0.006407761018786442	4451	4.451	0.11291446226228016
X	0.005541517081772492	957	0.957	0.17957212121779123
X	0.00508358319226464	4860	4.86	0.10151055960644118
X	0.005367833765098966	927	0.927	0.17957247752375805
X	0.0055668998075087605	159152	159.152	0.032703966917484166
X	0.005084894658176431	113240	113.24	0.03554354165023445
X	0.006586991021376961	8990	8.99	0.09015208871279516
X	0.00547262446930818	4158	4.158	0.10958986391723584
X	0.00491743269671433	1063	1.063	0.1666230398245402
X	0.0057822614086921985	1812	1.812	0.147224416852609
X	0.0050207994798270395	1043	1.043	0.16884810446698817
X	0.005692713429816001	9861	9.861	0.08326569686426416
X	0.006622194026816321	1169	1.169	0.17826351403819296
X	0.004982540952321059	26044	26.044	0.05762103613067821
X	0.008079030648401103	22673	22.673	0.0708951875702225
X	0.005075541117120449	6364	6.364	0.09273650543425084
X	0.005047062193800268	3273	3.273	0.11553073874613119
X	0.007789953147802835	1235	1.235	0.18476624355599516
X	0.00492166731067992	1502	1.502	0.14852997464486425
X	0.005005433500591289	1009	1.009	0.17054939241784664
X	0.006610529361030599	22869	22.869	0.06611951937468588
X	0.00502043429378946	4540	4.54	0.1034098269948376
X	0.004782613197089459	3378	3.378	0.11228848031102674
X	0.007124054824454275	26585	26.585	0.06447086979596424
X	0.00660738479316488	1597	1.597	0.16053690814697988
X	0.004572911842756878	308	0.308	0.24578047026962496
X	0.0050030077315449	2001	2.001	0.1357254723633975
X	0.00571496434421682	19188	19.188	0.06678228456113013
X	0.0073148007532136	15230	15.23	0.07831306095647904
X	0.005755718469666131	40539	40.539	0.052168558566402436
X	0.006659954341888101	17716	17.716	0.07217196448468736
X	0.004668229643916551	1038	1.038	0.16506371660411326
X	0.005229189065120281	11019	11.019	0.07800050008493215
X	0.0051821756244471604	36918	36.918	0.051970630689753904
X	0.00559459651510351	14975	14.975	0.07202235420905607
X	0.004982542739569871	2943	2.943	0.11918463117693225
X	0.005599702100847439	32329	32.329	0.055743070087981676
X	0.00538509082940346	33432	33.432	0.05440977018904547
X	0.005786455457293176	12128	12.128	0.07814019224458217
X	0.00521977188878545	12264	12.264	0.07522111975184628
X	0.006928383712469142	1123	1.123	0.18340762704421054
X	0.004937686850425119	5772	5.772	0.09492903665546353
X	0.00497988127897339	2383	2.383	0.12784928855188402
X	0.005420060116341038	625	0.625	0.2054508908459812
X	0.005976318809712301	51057	51.057	0.04891696321533986
X	0.005116506687013002	28103	28.103	0.056677013038633414
X	0.0056504139327036	20750	20.75	0.06481677038177402
X	0.00571188042561468	4921	4.921	0.105093352752891
X	0.007056963381782566	10453	10.453	0.0877254567001654
X	0.0051631013607202495	3223	3.223	0.11700833025111515
X	0.005228875375248939	6283	6.283	0.09406189929541882
X	0.00872129427923355	494	0.494	0.2603862201858029
X	0.004935634504704279	2616	2.616	0.1235667859870692
X	0.005177233290892677	24503	24.503	0.059560658644673685
X	0.0051349607220054	9169	9.169	0.08242742212994288
X	0.005661386657986049	797	0.797	0.19223014722569678
X	0.005662567240855901	1906	1.906	0.14375738315142494
X	0.006899411208027688	62934	62.934	0.04786031632546782
X	0.004910584208848498	4679	4.679	0.10162331848184251
X	0.007989373181709315	5098	5.098	0.11615492411763158
X	0.0049433918349795205	6062	6.062	0.09342641867184906
X	0.00569144465544334	3701	3.701	0.11542533508098503
X	0.004920944927322001	581	0.581	0.20384064776413788
X	0.00515826998817264	600	0.6	0.20485706194434136
X	0.005520169917273719	9426	9.426	0.08366458673923606
X	0.004903192541228579	7085	7.085	0.08845309447701616
X	0.004916178433659802	645	0.645	0.1967988883360048
X	0.006944348841413611	49685	49.685	0.051896182432467866
X	0.0049449159816725196	1577	1.577	0.14636674266286245
X	0.006314793636844841	4363	4.363	0.11311615905281702
X	0.00516050417197224	1299	1.299	0.1583778078457013
X	0.0057209344358916305	27014	27.014	0.059606372386817416
X	0.005899701922863	897	0.897	0.18736100571534145
X	0.005022950551428479	2958	2.958	0.11930360795114661
X	0.0050152873629303995	4167	4.167	0.10637121188703409
X	0.006662933332728945	6252	6.252	0.10214462056685446
X	0.004921027818861918	4831	4.831	0.10061736218607575
X	0.006510613946768314	181668	181.668	0.032969662623328455
X	0.005581129680995198	3929	3.929	0.11241217780225261
X	0.004956465942497	796	0.796	0.18397254639394378
X	0.005670285070341579	17866	17.866	0.06821174940695789
X	0.005410797921639749	4067	4.067	0.10998388272619261
X	0.00526591500711386	18269	18.269	0.0660571283182391
X	0.004894273650007981	2360	2.36	0.12752404553698382
X	0.007027422445452931	20579	20.579	0.06989679538578525
X	0.007058128660610545	59743	59.743	0.04906828147856328
X	0.0059302928199339985	4451	4.451	0.11003719588521525
X	0.005492968450358629	90099	90.099	0.03935763960400628
X	0.00567797700942461	800	0.8	0.1921769135955003
X	0.005013887497956718	7768	7.768	0.0864214978837375
X	0.005038261494156522	3104	3.104	0.11752213265432254
X	0.006274496371070706	55947	55.947	0.04822445811122629
X	0.005952636689158391	582	0.582	0.21706782968323562
X	0.007411571953025578	1068	1.068	0.19074201175767552
X	0.0048682419499715395	1165	1.165	0.16107027852568323
X	0.006081129723644003	5837	5.837	0.10137515351441873
X	0.00489336088670766	20094	20.094	0.06244729286180155
X	0.0063678531659261345	5295	5.295	0.106343028778034
X	0.0073344686492975205	2507	2.507	0.14302263471239907
X	0.0047840289763221005	1202	1.202	0.15847585782190102
X	0.0051092790054691	2398	2.398	0.12867775642744594
X	0.007823262995892552	21391	21.391	0.07151310158450694
X	0.0073122211691715976	7913	7.913	0.09740233861867066
X	0.005777423311314158	56503	56.503	0.046761405992449316
X	0.006491402848276302	3788	3.788	0.11966749420027535
X	0.004922965470471999	3171	3.171	0.1157915451262348
X	0.005998841450017858	469	0.469	0.23386479342168204
X	0.00474856235677935	562	0.562	0.20367697830265474
X	0.005927151882868606	77847	77.847	0.04238394640617122
X	0.005831044453767399	1134	1.134	0.172601447119965
X	0.004792161951149791	1629	1.629	0.1432859038058591
X	0.005311548582351361	20014	20.014	0.06426321085390359
X	0.0046576258041599984	6812	6.812	0.08809744603440718
X	0.005202921304372559	1090	1.09	0.1683734405559627
X	0.004925791564848021	663	0.663	0.19512855753778008
X	0.0056871985031968	2384	2.384	0.13361771644098397
X	0.005451393933891782	3351	3.351	0.1176100350979265
X	0.005105051292718101	1529	1.529	0.1494620294126579
X	0.0050942669709824	7737	7.737	0.08699675120591922
X	0.00756544217276176	4532	4.532	0.11862643297270407
X	0.005117290695274229	2738	2.738	0.12317868067693227
X	0.005651075938173227	68832	68.832	0.043462388032640874
X	0.005311015712630759	13282	13.282	0.0736723805790288
X	0.00481231119382155	882	0.882	0.17604689629684725
X	0.005312119220792999	10259	10.259	0.08030099523355923
X	0.005029939375665262	1569	1.569	0.1474506197787182
X	0.0049385199676394405	1198	1.198	0.16034176701235925
X	0.0049197812185861195	3874	3.874	0.10829175481060656
X	0.00485288142129072	1573	1.573	0.1455761799745235
X	0.005688231048461698	19984	19.984	0.06578066644924951
X	0.005069058235922239	7379	7.379	0.08823548084876247
X	0.004787657443817711	1239	1.239	0.15692203116974268
X	0.00521961415904579	4957	4.957	0.10173564914846107
X	0.0049735471463424	2534	2.534	0.1252044821209792
X	0.0045714302939646794	3047	3.047	0.11447919035318432
X	0.00548090842386675	11743	11.743	0.07756953029110455
X	0.004817011399638579	275	0.275	0.25970580673722876
X	0.0050588691757871	755	0.755	0.18852494914278292
X	0.005772514978675887	930	0.93	0.1837782196495192
X	0.00586519446946524	705	0.705	0.20262721086332725
X	0.005204918825086919	1399	1.399	0.15495255752046638
X	0.00521036182224978	4732	4.732	0.10326212285798132
X	0.00508975433127616	19374	19.374	0.06404607335053232
X	0.00671599550205378	1480	1.48	0.16555776710876344
X	0.00483385978993275	413	0.413	0.22704642385711185
X	0.006311109558104782	866	0.866	0.19387833249494743
X	0.00553364236801932	8916	8.916	0.08529960289316195
X	0.006508299920741921	117997	117.997	0.03806555773885693
X	0.005001308507749549	7321	7.321	0.08807197715241155
X	0.005522679728774375	678	0.678	0.20120559578064673
X	0.00559127159115448	3217	3.217	0.12023193660434886
X	0.005950215918875521	47245	47.245	0.05012552766255778
X	0.007474262541370829	579	0.579	0.234583031874638
X	0.0055289718115392	1075	1.075	0.17261504156517568
X	0.0057887005019900415	10418	10.418	0.08221143756952139
X	0.006063253387858589	908	0.908	0.1883099594541882
X	0.0060175242102934404	7217	7.217	0.09412111144698737
X	0.0066734145318825995	10120	10.12	0.0870407310677488
X	0.00520110555622744	6965	6.965	0.09072455549067451
X	0.0059239034032474	2845	2.845	0.1276953940096915
X	0.0049618975473847	1045	1.045	0.168077856464221
X	0.006185180461031961	8390	8.39	0.09033654394175844
X	0.005359914394021025	13353	13.353	0.07376658325594558
X	0.005477969081736838	5251	5.251	0.10142052464546808
X	0.00571361590764944	9406	9.406	0.08469062850567466
X	0.005029243720081	41468	41.468	0.04949901203484197
X	0.0049295489019178995	806	0.806	0.1828762801088707
X	0.00476266892318608	316	0.316	0.24701373545372257
X	0.00579249104284655	2430	2.43	0.1335835659080851
X	0.006643785535526521	18343	18.343	0.071282306894622
X	0.005615927181773609	8206	8.206	0.0881244971301643
X	0.005030669755739701	1929	1.929	0.13764629528072436
X	0.0047171894106906006	548	0.548	0.20494354879126253
X	0.00519333151051407	12246	12.246	0.07513067032656356
X	0.004970075594809189	1541	1.541	0.1477474923582282
X	0.00489186698737373	5688	5.688	0.0950980658263884
X	0.0055585666505150386	912	0.912	0.18266527416680914
X	0.005171358536230049	3860	3.86	0.11023997720451091
X	0.005405009843116001	1205	1.205	0.16491866633627994
X	0.004842571930542419	2220	2.22	0.12969036184579072
X	0.00482903280505685	1993	1.993	0.13431286684095625
X	0.0052802182896129	21643	21.643	0.06248534027782256
X	0.005091528637049438	14387	14.387	0.07073363587152134
X	0.005201567988271771	35262	35.262	0.05283752253991109
X	0.00560637611500864	86241	86.241	0.04020895769791324
X	0.005141075796509029	7864	7.864	0.08679009571929075
X	0.004769365427729139	1089	1.089	0.16361040003020505
X	0.00906934871724876	1483	1.483	0.18287106043211032
X	0.0060388088102618495	26611	26.611	0.06099534540904183
X	0.006511546490734149	1158	1.158	0.17782460885321538
X	0.00515139814804368	5582	5.582	0.0973595209802294
X	0.005211378619044401	2439	2.439	0.12879933147234057
X	0.007367910361528387	49494	49.494	0.05299854252293623
X	0.005005995585719609	24931	24.931	0.05855764303029002
X	0.006478544974430523	1364	1.364	0.16809483500890912
X	0.006100091943287125	29339	29.339	0.05924208813878904
X	0.005896961816820703	6993	6.993	0.09447602139098342
X	0.004988193971221198	5533	5.533	0.09660380241276767
X	0.006699801505458825	63433	63.433	0.04726963275601501
X	0.007131171382150498	42654	42.654	0.05508927240342558
X	0.005063986281543859	35243	35.243	0.052376911217504686
X	0.004820982380980609	3127	3.127	0.11552320282992361
X	0.006858873201178616	31878	31.878	0.05992212910525464
X	0.005045241125764	2596	2.596	0.1247936286641136
X	0.004917611645251149	10521	10.521	0.0776066736607389
X	0.00649219527994404	106417	106.417	0.03936650454283078
X	0.0049498486150568	3954	3.954	0.1077750957262423
X	0.004923212941622429	583	0.583	0.2036385621557814
X	0.00501956725136745	1952	1.952	0.13700260382710255
X	0.004960135960879921	1921	1.921	0.13718987867802107
X	0.008031630407656051	2662	2.662	0.14449912636727244
X	0.005006766284071519	1883	1.883	0.138538027431996
X	0.006316927494212291	3320	3.32	0.12391463523425239
X	0.008089988305250142	62855	62.855	0.05048968808171107
X	0.006292920353681255	659	0.659	0.21215614081504705
X	0.004985795438623449	162956	162.956	0.03127673185204805
X	0.0072925095966626195	5018	5.018	0.11327014351712215
X	0.00507228769245435	9039	9.039	0.08248237620732812
X	0.00492783538265957	18145	18.145	0.06475894122693869
X	0.009116203944565518	18311	18.311	0.07925633578409805
X	0.006784167373684929	60831	60.831	0.048134581401149464
X	0.0048461208853784995	647	0.647	0.19565756945057544
X	0.005789290682271299	126936	126.936	0.03572839571289072
X	0.005157396372920919	8200	8.2	0.08567857197006012
X	0.007352675033452547	2608	2.608	0.1412686933654233
X	0.004857768815886059	976	0.976	0.1707375354941962
X	0.004969752139543141	4241	4.241	0.10542788292538621
X	0.004883485095382859	458	0.458	0.22010071980886944
X	0.0052455700313912505	379	0.379	0.24009576139514968
X	0.006625265243453473	1782	1.782	0.15491680211211584
X	0.00784094517433224	46014	46.014	0.0554403729342126
X	0.006269134754860724	223783	223.783	0.03037107548118093
X	0.00607838179159698	2244	2.244	0.13939751792052119
X	0.005081834497488511	17998	17.998	0.06560426736318442
X	0.007108741002521894	2468	2.468	0.14228174363443336
X	0.006228677392752361	10402	10.402	0.0842868187963505
X	0.004829850597492061	723	0.723	0.18833533107987233
X	0.007139001220531153	7581	7.581	0.09801750718486625
X	0.004977710354541959	1961	1.961	0.13641140313782252
X	0.005572571262175999	130611	130.611	0.03494287426268451
X	0.0049043121577076396	819	0.819	0.18159256695493764
X	0.0052029851110028495	9252	9.252	0.08254149292295804
X	0.006243682204721361	4354	4.354	0.11276754395901202
X	0.0052577573751729605	5390	5.39	0.0991753919121761
X	0.006389740455441941	88044	88.044	0.04171201316895025
X	0.005195420153154611	7498	7.498	0.08848950102231469
X	0.004913394170445001	1603	1.603	0.14526111094612632
X	0.007463452420849347	11521	11.521	0.08652664487041066
X	0.005772939670944119	1349	1.349	0.16235351279994184
X	0.004700461259454099	417	0.417	0.22421679525998683
X	0.00507241661956362	36493	36.493	0.05180063893339588
X	0.00529874646516025	161113	161.113	0.03203914011980216
X	0.008021690701434065	3109	3.109	0.1371560797679741
X	0.00493112547764144	10946	10.946	0.07665906830547786
X	0.00490009295741976	15913	15.913	0.06752803080471845
X	0.006210307623347	3438	3.438	0.12178741532680618
X	0.005281021092241393	18488	18.488	0.0658581191058858
X	0.007388464232197998	1728	1.728	0.16230683623948228
X	0.00546796385989854	40732	40.732	0.05120314457828941
X	0.005479319285688599	18038	18.038	0.06722220311859015
X	0.006121131751482579	15326	15.326	0.07364347987811112
X	0.00539365145586124	4940	4.94	0.10297188029675694
X	0.00521542998440706	939	0.939	0.17709576499303778
X	0.005064027448390669	2214	2.214	0.13175673112744196
X	0.007205510509860179	171358	171.358	0.03477390450186174
X	0.00582328695935295	11457	11.457	0.07980542710735394
X	0.006362142630059231	3411	3.411	0.12309504192286357
X	0.004730927665447839	494	0.494	0.21236019704606807
X	0.00746219452429152	1347	1.347	0.17694283858170987
X	0.004902383758633769	14413	14.413	0.06980464231353929
X	0.005534733377027024	16225	16.225	0.0698721327517879
X	0.00604892594503184	55952	55.952	0.047638076505828165
X	0.0056093180559816	857	0.857	0.18705805513647927
X	0.004920141656617478	5372	5.372	0.09711370745767936
X	0.0060931880695860195	5555	5.555	0.1031304327061643
X	0.007179978377123876	10368	10.368	0.08847272065303075
X	0.004986761892458939	9406	9.406	0.08093528230341474
X	0.005269339867132882	19610	19.61	0.06452968410956854
X	0.0048680029532095805	539	0.539	0.20825112487786174
X	0.005081417212951801	9175	9.175	0.08212201440591402
X	0.0049840435039131	411	0.411	0.2297453168929879
X	0.007394017942879892	1378	1.378	0.17506947424987324
X	0.00606041774380224	62123	62.123	0.04603449468166362
X	0.004946899277913599	7969	7.969	0.08530537838739072
X	0.005953453916898659	18406	18.406	0.06864409645176964
X	0.00547933528267963	1563	1.563	0.15191114341506254
X	0.005134217087746275	879	0.879	0.18009228879935715
X	0.004868195200436641	12537	12.537	0.07295551362194809
X	0.005036375511435569	23367	23.367	0.059956791156050016
X	0.00562235184699419	10891	10.891	0.08022014095537852
X	0.004874045018224109	944	0.944	0.17283797946451968
X	0.0050748269646514	2339	2.339	0.12945843094229165
X	0.005212155258551802	3110	3.11	0.11878246081053266
X	0.004911874425103859	3538	3.538	0.11155690981388018
X	0.005452165490174459	11051	11.051	0.07901735416000565
X	0.0050879977845354	3294	3.294	0.11559554748596079
X	0.00571278956956481	553	0.553	0.21779155310862414
X	0.005043072656238851	2566	2.566	0.12526012988693955
X	0.0047923876472832005	824	0.824	0.17983528580421385
X	0.005899055727695198	17664	17.664	0.06937949711695036
X	0.005196028620252598	1441	1.441	0.15334484630956854
X	0.0051056086804787985	6332	6.332	0.0930755346676887
X	0.005338327512340624	23729	23.729	0.06081937125390623
X	0.005219969039824001	4567	4.567	0.10455520033990533
X	0.0055697189017958485	16527	16.527	0.0695899327398381
X	0.006150771603429091	25645	25.645	0.062131088999682275
X	0.00535846067549572	722	0.722	0.1950597291171784
X	0.008031690608639083	26058	26.058	0.06754947275925803
X	0.0052708206583259995	10831	10.831	0.07865685201564153
X	0.0065299593870033115	64829	64.829	0.046527928687233655
X	0.005087362742751459	32748	32.748	0.053757117625532265
X	0.005469128658441999	4279	4.279	0.1085238730135274
X	0.006559035760046578	15645	15.645	0.07484362436394429
X	0.0049913529963312	3436	3.436	0.11325438947814398
X	0.004940914345485199	427	0.427	0.22618292532851686
X	0.005322871543650151	1234	1.234	0.16278345999956592
X	0.006633550788193577	6547	6.547	0.10043873614805986
X	0.005068232418536409	18676	18.676	0.06474271173009834
X	0.005329002154136659	10542	10.542	0.07966012091904315
X	0.007411567299376109	586	0.586	0.23299010664184458
X	0.005489482284115209	9259	9.259	0.08400834607849283
X	0.005180637354668621	2720	2.72	0.12395710635458798
X	0.00808609749169319	44459	44.459	0.056657807047323865
X	0.005036324995773298	11224	11.224	0.076557668077531
X	0.005232265346379232	1454	1.454	0.1532410496095102
X	0.005811867396361655	12883	12.883	0.0766948548806865
X	0.005292951356673681	22844	22.844	0.06141982628059845
X	0.004854241483980001	1430	1.43	0.1502894332275327
X	0.0050927234414447995	15122	15.122	0.06957399681275148
X	0.00513493943366052	15780	15.78	0.06878220732402844
time for making epsilon is 0.46163463592529297
epsilons are
[0.1840286752986606, 0.13378576189209454, 0.2609894886527214, 0.19584318399292494, 0.09135570488148125, 0.08129448719695621, 0.08293316688066381, 0.07641271055234268, 0.15950710553277586, 0.10303361577381853, 0.16063668030004602, 0.05418014846520203, 0.11794374063157284, 0.06839761871207051, 0.1013268337759837, 0.07006572589908987, 0.08395254771825705, 0.1051568253467908, 0.03471668582612242, 0.07821781009057768, 0.08552524724023892, 0.03753674299109687, 0.09567489145502693, 0.1350005695222557, 0.1097296685270406, 0.07538607736601051, 0.21563081187025562, 0.14608421623337414, 0.12419581058600201, 0.023581833846241084, 0.18550966110678235, 0.05601199018490664, 0.16921196202331443, 0.13188612732571217, 0.11370039237856659, 0.10408280887756355, 0.07937971477177291, 0.2611164715300945, 0.2057734381269717, 0.21154828128383812, 0.15329804883874867, 0.37619588060125936, 0.23302126566119802, 0.21488658078236508, 0.2469830619573312, 0.26254874497450625, 0.2506139967445571, 0.1640834223199367, 0.21809891857030522, 0.16917476392593217, 0.14831827864566918, 0.25008433159141624, 0.17961408135719886, 0.22665667050963298, 0.18967220845853935, 0.1677634340403589, 0.16449653208604514, 0.195965434889895, 0.1879273117349205, 0.20888018883337434, 0.1913914739650974, 0.23021058583247944, 0.2137984809691723, 0.11691708871463631, 0.15645745610351977, 0.25018316074914376, 0.16947820890083917, 0.1253170779771009, 0.1507629332221814, 0.22980776701597153, 0.11811571101157135, 0.20648846130733317, 0.1683670262577063, 0.2104372485677523, 0.16793465667094204, 0.22220278763755724, 0.2600097261120743, 0.2978762851708698, 0.16528923411288687, 0.13966725299935262, 0.17345330877724183, 0.21110156859859022, 0.22063939726602164, 0.23159564296609786, 0.22920274465099089, 0.22857468502345937, 0.26521817889627625, 0.30192014684950685, 0.21568784610351371, 0.20879168176637247, 0.15981303385574094, 0.23941221935621992, 0.2486189875601887, 0.21147599799935846, 0.21947088363004089, 0.19183746423176798, 0.2167323857789773, 0.12782131213030487, 0.1919955493708976, 0.15155237105442312, 0.1605433006856163, 0.19722397948205428, 0.19390334746972854, 0.2691301577181301, 0.14646696209941312, 0.28188952134428713, 0.1586918922293168, 0.2764794667976792, 0.23610788692582055, 0.26227629299413907, 0.2394811438879257, 0.25340365264378767, 0.23732490541894288, 0.19542434685225762, 0.22422171020760504, 0.1734328581177299, 0.19753165372001175, 0.24040664810644327, 0.15170999787400877, 0.2666047067302377, 0.1108371355195682, 0.2396057214732979, 0.2764722389651679, 0.23675442387389165, 0.21286456240933402, 0.17739931878589488, 0.27621133126349073, 0.21301529334290925, 0.13981205376804354, 0.20144806160945988, 0.12730164800842367, 0.26706159876134516, 0.18017037601854513, 0.25257139291059866, 0.21220045498633738, 0.2632120008515635, 0.24288038219903252, 0.27638366477544357, 0.22692883023446775, 0.22935598704767907, 0.23219227223468897, 0.2617223727810981, 0.18399071936596367, 0.16111099304595144, 0.21862506601943327, 0.21296423219856656, 0.21736130152455482, 0.2515527391218618, 0.21533072110138923, 0.2623166788072317, 0.22591632027723205, 0.2476601041022942, 0.22414407718915416, 0.1646553093608196, 0.1714841286349159, 0.16404994930503505, 0.15810323508730717, 0.21841517224440282, 0.16821786206944217, 0.25796419092243283, 0.17729115595245884, 0.2242260710553016, 0.23582879969569456, 0.28011116672576103, 0.20987839041699144, 0.2381890678027595, 0.1888978747928778, 0.1800129525052657, 0.1818614749635591, 0.23319217312742332, 0.27043329774974584, 0.2084470712612455, 0.1526358071505111, 0.22032775905425156, 0.23584864012357812, 0.24941636647003532, 0.16430909283003794, 0.20611659945755698, 0.17076762530946335, 0.1643812521277419, 0.20698949284173335, 0.22971867032103863, 0.17888752507578942, 0.23686081224694377, 0.18518672725028998, 0.17720628775323008, 0.19608934324198082, 0.21146447837831667, 0.18997611053943553, 0.2245609027496434, 0.20352003171863997, 0.2235448219437031, 0.20048721608486073, 0.18916852255391745, 0.1470065769582467, 0.24464560767565438, 0.23821912882503507, 0.2833376793468467, 0.17445245771856216, 0.2793102276612091, 0.16090311439498625, 0.19629501609931188, 0.14154109171740278, 0.23109395656743342, 0.16698316488700332, 0.17722294494996396, 0.31902588246824914, 0.24259869957420535, 0.25323775362743844, 0.17742131495301502, 0.1607702138923019, 0.2544247073556042, 0.1884415283856144, 0.13363833900375421, 0.2994726070252725, 0.2255409891862288, 0.24594744887010375, 0.2714394653503043, 0.1544795617787253, 0.15288445128043202, 0.14704760870621392, 0.2336671624817902, 0.16046390958528903, 0.1779477467140767, 0.2506024083069394, 0.18804287756102173, 0.2029472616006365, 0.27863470666304974, 0.199854769566274, 0.24318079614934748, 0.23741086303022182, 0.22171603031988757, 0.24270699803792353, 0.21418638543456786, 0.27054223582534176, 0.2278494223673327, 0.19390846143408275, 0.19606571025042557, 0.19516219996454245, 0.15871166911786716, 0.15676192541324693, 0.11400522386832852, 0.14292820734996325, 0.2580931432660889, 0.16544285994622343, 0.12102712398984655, 0.0752287658711641, 0.20895991235400826, 0.10351434871289707, 0.10943606712587481, 0.13753586831642423, 0.04033450951467022, 0.19740029910793197, 0.1653041914314745, 0.046111794937565245, 0.03076939624340047, 0.1836897441992867, 0.13888804328500204, 0.1398144120058741, 0.1071004968419882, 0.10972239635147857, 0.09057199369398682, 0.10377647676553223, 0.1256413836543214, 0.2032844747023285, 0.08552470855589793, 0.13658613686108229, 0.20942720988602886, 0.06922881697719124, 0.1382056600521141, 0.07236784691672242, 0.054864444875356, 0.14673813372620828, 0.14200792514248978, 0.24332877203326952, 0.09639908762135029, 0.05788341717697529, 0.17775459647401987, 0.06481487127571776, 0.25397095050008617, 0.15226989037764652, 0.16072459488101556, 0.050730287956820094, 0.09148700807478075, 0.24967914972048363, 0.04834161952102541, 0.10229535512477449, 0.09695097639985735, 0.06718369403420091, 0.17405298990510745, 0.08611509357921088, 0.11718962129418067, 0.061303784337229945, 0.09330005978832102, 0.08897193379488494, 0.07858623989618453, 0.1410332237258834, 0.12501784603413688, 0.11123908664856623, 0.08332960413250155, 0.05870311905046039, 0.08732717827810137, 0.12516619135065193, 0.19344187312878314, 0.08681602667433709, 0.11612156115953846, 0.18463278371216724, 0.22588103573970036, 0.13998204461332767, 0.14046101365100258, 0.03654823246757406, 0.17710933307197083, 0.13483935983988932, 0.08561850209063245, 0.2059735544287858, 0.08988666255417159, 0.11869239312991485, 0.06607979003900402, 0.05721475350941807, 0.15778525519368233, 0.07820209264233886, 0.06427125228839593, 0.04581265699621861, 0.06091438264793409, 0.18678927138648393, 0.10526844749732646, 0.055962369312496776, 0.08499449778024484, 0.11474609309332494, 0.20391756445105955, 0.05354569272093199, 0.057056391943928905, 0.10297479378966504, 0.14114791247753725, 0.08794345811916558, 0.14688770232683213, 0.1466175929908073, 0.1174365785818279, 0.14648180342679504, 0.09011948837907995, 0.08596229431183153, 0.0994460822686492, 0.09497389004887392, 0.12720312534684333, 0.03223530431078261, 0.12129490129069488, 0.05987064312297497, 0.22577647023247843, 0.15099820208900228, 0.18343770338367812, 0.0978091690025375, 0.18300810277364118, 0.28096107251930313, 0.1503587896178366, 0.18043292433547073, 0.04087498892315292, 0.06895178638810374, 0.15528257387133218, 0.08669770589355372, 0.16504190862171916, 0.17500996772853497, 0.1347386897195167, 0.09662857044822992, 0.2740331095930879, 0.140184337262421, 0.13492717859736558, 0.17716402823525124, 0.14787148203307623, 0.09037851090767234, 0.17494056012361023, 0.052955662041273305, 0.12461293540355221, 0.11291446226228016, 0.17957212121779123, 0.10151055960644118, 0.17957247752375805, 0.032703966917484166, 0.03554354165023445, 0.09015208871279516, 0.10958986391723584, 0.1666230398245402, 0.147224416852609, 0.16884810446698817, 0.08326569686426416, 0.17826351403819296, 0.05762103613067821, 0.0708951875702225, 0.09273650543425084, 0.11553073874613119, 0.18476624355599516, 0.14852997464486425, 0.17054939241784664, 0.06611951937468588, 0.1034098269948376, 0.11228848031102674, 0.06447086979596424, 0.16053690814697988, 0.24578047026962496, 0.1357254723633975, 0.06678228456113013, 0.07831306095647904, 0.052168558566402436, 0.07217196448468736, 0.16506371660411326, 0.07800050008493215, 0.051970630689753904, 0.07202235420905607, 0.11918463117693225, 0.055743070087981676, 0.05440977018904547, 0.07814019224458217, 0.07522111975184628, 0.18340762704421054, 0.09492903665546353, 0.12784928855188402, 0.2054508908459812, 0.04891696321533986, 0.056677013038633414, 0.06481677038177402, 0.105093352752891, 0.0877254567001654, 0.11700833025111515, 0.09406189929541882, 0.2603862201858029, 0.1235667859870692, 0.059560658644673685, 0.08242742212994288, 0.19223014722569678, 0.14375738315142494, 0.04786031632546782, 0.10162331848184251, 0.11615492411763158, 0.09342641867184906, 0.11542533508098503, 0.20384064776413788, 0.20485706194434136, 0.08366458673923606, 0.08845309447701616, 0.1967988883360048, 0.051896182432467866, 0.14636674266286245, 0.11311615905281702, 0.1583778078457013, 0.059606372386817416, 0.18736100571534145, 0.11930360795114661, 0.10637121188703409, 0.10214462056685446, 0.10061736218607575, 0.032969662623328455, 0.11241217780225261, 0.18397254639394378, 0.06821174940695789, 0.10998388272619261, 0.0660571283182391, 0.12752404553698382, 0.06989679538578525, 0.04906828147856328, 0.11003719588521525, 0.03935763960400628, 0.1921769135955003, 0.0864214978837375, 0.11752213265432254, 0.04822445811122629, 0.21706782968323562, 0.19074201175767552, 0.16107027852568323, 0.10137515351441873, 0.06244729286180155, 0.106343028778034, 0.14302263471239907, 0.15847585782190102, 0.12867775642744594, 0.07151310158450694, 0.09740233861867066, 0.046761405992449316, 0.11966749420027535, 0.1157915451262348, 0.23386479342168204, 0.20367697830265474, 0.04238394640617122, 0.172601447119965, 0.1432859038058591, 0.06426321085390359, 0.08809744603440718, 0.1683734405559627, 0.19512855753778008, 0.13361771644098397, 0.1176100350979265, 0.1494620294126579, 0.08699675120591922, 0.11862643297270407, 0.12317868067693227, 0.043462388032640874, 0.0736723805790288, 0.17604689629684725, 0.08030099523355923, 0.1474506197787182, 0.16034176701235925, 0.10829175481060656, 0.1455761799745235, 0.06578066644924951, 0.08823548084876247, 0.15692203116974268, 0.10173564914846107, 0.1252044821209792, 0.11447919035318432, 0.07756953029110455, 0.25970580673722876, 0.18852494914278292, 0.1837782196495192, 0.20262721086332725, 0.15495255752046638, 0.10326212285798132, 0.06404607335053232, 0.16555776710876344, 0.22704642385711185, 0.19387833249494743, 0.08529960289316195, 0.03806555773885693, 0.08807197715241155, 0.20120559578064673, 0.12023193660434886, 0.05012552766255778, 0.234583031874638, 0.17261504156517568, 0.08221143756952139, 0.1883099594541882, 0.09412111144698737, 0.0870407310677488, 0.09072455549067451, 0.1276953940096915, 0.168077856464221, 0.09033654394175844, 0.07376658325594558, 0.10142052464546808, 0.08469062850567466, 0.04949901203484197, 0.1828762801088707, 0.24701373545372257, 0.1335835659080851, 0.071282306894622, 0.0881244971301643, 0.13764629528072436, 0.20494354879126253, 0.07513067032656356, 0.1477474923582282, 0.0950980658263884, 0.18266527416680914, 0.11023997720451091, 0.16491866633627994, 0.12969036184579072, 0.13431286684095625, 0.06248534027782256, 0.07073363587152134, 0.05283752253991109, 0.04020895769791324, 0.08679009571929075, 0.16361040003020505, 0.18287106043211032, 0.06099534540904183, 0.17782460885321538, 0.0973595209802294, 0.12879933147234057, 0.05299854252293623, 0.05855764303029002, 0.16809483500890912, 0.05924208813878904, 0.09447602139098342, 0.09660380241276767, 0.04726963275601501, 0.05508927240342558, 0.052376911217504686, 0.11552320282992361, 0.05992212910525464, 0.1247936286641136, 0.0776066736607389, 0.03936650454283078, 0.1077750957262423, 0.2036385621557814, 0.13700260382710255, 0.13718987867802107, 0.14449912636727244, 0.138538027431996, 0.12391463523425239, 0.05048968808171107, 0.21215614081504705, 0.03127673185204805, 0.11327014351712215, 0.08248237620732812, 0.06475894122693869, 0.07925633578409805, 0.048134581401149464, 0.19565756945057544, 0.03572839571289072, 0.08567857197006012, 0.1412686933654233, 0.1707375354941962, 0.10542788292538621, 0.22010071980886944, 0.24009576139514968, 0.15491680211211584, 0.0554403729342126, 0.03037107548118093, 0.13939751792052119, 0.06560426736318442, 0.14228174363443336, 0.0842868187963505, 0.18833533107987233, 0.09801750718486625, 0.13641140313782252, 0.03494287426268451, 0.18159256695493764, 0.08254149292295804, 0.11276754395901202, 0.0991753919121761, 0.04171201316895025, 0.08848950102231469, 0.14526111094612632, 0.08652664487041066, 0.16235351279994184, 0.22421679525998683, 0.05180063893339588, 0.03203914011980216, 0.1371560797679741, 0.07665906830547786, 0.06752803080471845, 0.12178741532680618, 0.0658581191058858, 0.16230683623948228, 0.05120314457828941, 0.06722220311859015, 0.07364347987811112, 0.10297188029675694, 0.17709576499303778, 0.13175673112744196, 0.03477390450186174, 0.07980542710735394, 0.12309504192286357, 0.21236019704606807, 0.17694283858170987, 0.06980464231353929, 0.0698721327517879, 0.047638076505828165, 0.18705805513647927, 0.09711370745767936, 0.1031304327061643, 0.08847272065303075, 0.08093528230341474, 0.06452968410956854, 0.20825112487786174, 0.08212201440591402, 0.2297453168929879, 0.17506947424987324, 0.04603449468166362, 0.08530537838739072, 0.06864409645176964, 0.15191114341506254, 0.18009228879935715, 0.07295551362194809, 0.059956791156050016, 0.08022014095537852, 0.17283797946451968, 0.12945843094229165, 0.11878246081053266, 0.11155690981388018, 0.07901735416000565, 0.11559554748596079, 0.21779155310862414, 0.12526012988693955, 0.17983528580421385, 0.06937949711695036, 0.15334484630956854, 0.0930755346676887, 0.06081937125390623, 0.10455520033990533, 0.0695899327398381, 0.062131088999682275, 0.1950597291171784, 0.06754947275925803, 0.07865685201564153, 0.046527928687233655, 0.053757117625532265, 0.1085238730135274, 0.07484362436394429, 0.11325438947814398, 0.22618292532851686, 0.16278345999956592, 0.10043873614805986, 0.06474271173009834, 0.07966012091904315, 0.23299010664184458, 0.08400834607849283, 0.12395710635458798, 0.056657807047323865, 0.076557668077531, 0.1532410496095102, 0.0766948548806865, 0.06141982628059845, 0.1502894332275327, 0.06957399681275148, 0.06878220732402844]
0.11216701521424567
Making ranges
torch.Size([11161, 2])
We keep 1.83e+06/4.37e+07 =  4% of the original kernel matrix.

torch.Size([1142, 2])
We keep 1.22e+05/6.08e+05 = 20% of the original kernel matrix.

torch.Size([2527, 2])
We keep 4.08e+05/5.16e+06 =  7% of the original kernel matrix.

torch.Size([2242, 2])
We keep 1.22e+06/6.30e+06 = 19% of the original kernel matrix.

torch.Size([3392, 2])
We keep 1.05e+06/1.66e+07 =  6% of the original kernel matrix.

torch.Size([555, 2])
We keep 2.60e+04/1.14e+05 = 22% of the original kernel matrix.

torch.Size([2135, 2])
We keep 2.28e+05/2.23e+06 = 10% of the original kernel matrix.

torch.Size([998, 2])
We keep 9.68e+04/4.64e+05 = 20% of the original kernel matrix.

torch.Size([2502, 2])
We keep 3.73e+05/4.50e+06 =  8% of the original kernel matrix.

torch.Size([5382, 2])
We keep 9.00e+06/6.05e+07 = 14% of the original kernel matrix.

torch.Size([5202, 2])
We keep 2.62e+06/5.14e+07 =  5% of the original kernel matrix.

torch.Size([8889, 2])
We keep 6.04e+06/8.97e+07 =  6% of the original kernel matrix.

torch.Size([6771, 2])
We keep 2.92e+06/6.26e+07 =  4% of the original kernel matrix.

torch.Size([8498, 2])
We keep 6.80e+06/7.81e+07 =  8% of the original kernel matrix.

torch.Size([6591, 2])
We keep 2.80e+06/5.84e+07 =  4% of the original kernel matrix.

torch.Size([11042, 2])
We keep 1.31e+07/1.73e+08 =  7% of the original kernel matrix.

torch.Size([7473, 2])
We keep 3.98e+06/8.68e+07 =  4% of the original kernel matrix.

torch.Size([1706, 2])
We keep 2.18e+05/1.51e+06 = 14% of the original kernel matrix.

torch.Size([3164, 2])
We keep 5.51e+05/8.13e+06 =  6% of the original kernel matrix.

torch.Size([4527, 2])
We keep 5.79e+06/4.38e+07 = 13% of the original kernel matrix.

torch.Size([4970, 2])
We keep 2.20e+06/4.37e+07 =  5% of the original kernel matrix.

torch.Size([1410, 2])
We keep 2.59e+05/1.50e+06 = 17% of the original kernel matrix.

torch.Size([2787, 2])
We keep 5.71e+05/8.09e+06 =  7% of the original kernel matrix.

torch.Size([26479, 2])
We keep 6.64e+07/1.18e+09 =  5% of the original kernel matrix.

torch.Size([11182, 2])
We keep 9.07e+06/2.27e+08 =  3% of the original kernel matrix.

torch.Size([3538, 2])
We keep 2.07e+06/2.01e+07 = 10% of the original kernel matrix.

torch.Size([4810, 2])
We keep 1.65e+06/2.96e+07 =  5% of the original kernel matrix.

torch.Size([14419, 2])
We keep 2.59e+07/2.54e+08 = 10% of the original kernel matrix.

torch.Size([8147, 2])
We keep 4.65e+06/1.05e+08 =  4% of the original kernel matrix.

torch.Size([4205, 2])
We keep 2.46e+06/2.47e+07 =  9% of the original kernel matrix.

torch.Size([4561, 2])
We keep 1.74e+06/3.28e+07 =  5% of the original kernel matrix.

torch.Size([12538, 2])
We keep 2.18e+07/2.21e+08 =  9% of the original kernel matrix.

torch.Size([7310, 2])
We keep 4.39e+06/9.83e+07 =  4% of the original kernel matrix.

torch.Size([8023, 2])
We keep 4.85e+06/7.52e+07 =  6% of the original kernel matrix.

torch.Size([6579, 2])
We keep 2.65e+06/5.73e+07 =  4% of the original kernel matrix.

torch.Size([4233, 2])
We keep 4.67e+06/3.69e+07 = 12% of the original kernel matrix.

torch.Size([4610, 2])
We keep 2.21e+06/4.02e+07 =  5% of the original kernel matrix.

torch.Size([94972, 2])
We keep 1.09e+09/1.80e+10 =  6% of the original kernel matrix.

torch.Size([20610, 2])
We keep 3.11e+07/8.86e+08 =  3% of the original kernel matrix.

torch.Size([9234, 2])
We keep 1.14e+07/1.30e+08 =  8% of the original kernel matrix.

torch.Size([6528, 2])
We keep 3.50e+06/7.53e+07 =  4% of the original kernel matrix.

torch.Size([6020, 2])
We keep 7.99e+06/6.46e+07 = 12% of the original kernel matrix.

torch.Size([5059, 2])
We keep 2.61e+06/5.31e+07 =  4% of the original kernel matrix.

torch.Size([82782, 2])
We keep 6.60e+08/1.44e+10 =  4% of the original kernel matrix.

torch.Size([19908, 2])
We keep 2.81e+07/7.93e+08 =  3% of the original kernel matrix.

torch.Size([5208, 2])
We keep 3.30e+06/3.45e+07 =  9% of the original kernel matrix.

torch.Size([5228, 2])
We keep 1.98e+06/3.88e+07 =  5% of the original kernel matrix.

torch.Size([2483, 2])
We keep 5.57e+05/4.57e+06 = 12% of the original kernel matrix.

torch.Size([3649, 2])
We keep 8.69e+05/1.41e+07 =  6% of the original kernel matrix.

torch.Size([3003, 2])
We keep 3.31e+06/1.57e+07 = 21% of the original kernel matrix.

torch.Size([3462, 2])
We keep 1.39e+06/2.62e+07 =  5% of the original kernel matrix.

torch.Size([10744, 2])
We keep 1.09e+07/1.49e+08 =  7% of the original kernel matrix.

torch.Size([7223, 2])
We keep 3.63e+06/8.06e+07 =  4% of the original kernel matrix.

torch.Size([698, 2])
We keep 6.68e+04/2.87e+05 = 23% of the original kernel matrix.

torch.Size([2214, 2])
We keep 3.18e+05/3.54e+06 =  8% of the original kernel matrix.

torch.Size([1968, 2])
We keep 4.46e+05/2.80e+06 = 15% of the original kernel matrix.

torch.Size([3143, 2])
We keep 7.24e+05/1.11e+07 =  6% of the original kernel matrix.

torch.Size([2664, 2])
We keep 1.16e+06/6.70e+06 = 17% of the original kernel matrix.

torch.Size([3512, 2])
We keep 1.02e+06/1.71e+07 =  5% of the original kernel matrix.

torch.Size([406172, 2])
We keep 8.39e+09/2.50e+11 =  3% of the original kernel matrix.

torch.Size([48050, 2])
We keep 1.07e+08/3.31e+09 =  3% of the original kernel matrix.

torch.Size([996, 2])
We keep 1.31e+05/5.72e+05 = 22% of the original kernel matrix.

torch.Size([2212, 2])
We keep 4.00e+05/5.00e+06 =  8% of the original kernel matrix.

torch.Size([25926, 2])
We keep 7.28e+07/1.03e+09 =  7% of the original kernel matrix.

torch.Size([11332, 2])
We keep 8.34e+06/2.12e+08 =  3% of the original kernel matrix.

torch.Size([1371, 2])
We keep 3.56e+05/1.84e+06 = 19% of the original kernel matrix.

torch.Size([2905, 2])
We keep 6.69e+05/8.97e+06 =  7% of the original kernel matrix.

torch.Size([2580, 2])
We keep 5.32e+05/4.94e+06 = 10% of the original kernel matrix.

torch.Size([3664, 2])
We keep 8.71e+05/1.47e+07 =  5% of the original kernel matrix.

torch.Size([3398, 2])
We keep 2.57e+06/1.42e+07 = 18% of the original kernel matrix.

torch.Size([4179, 2])
We keep 1.39e+06/2.49e+07 =  5% of the original kernel matrix.

torch.Size([3928, 2])
We keep 4.09e+06/2.51e+07 = 16% of the original kernel matrix.

torch.Size([4376, 2])
We keep 1.79e+06/3.31e+07 =  5% of the original kernel matrix.

torch.Size([9600, 2])
We keep 9.48e+06/1.31e+08 =  7% of the original kernel matrix.

torch.Size([7200, 2])
We keep 3.49e+06/7.56e+07 =  4% of the original kernel matrix.

torch.Size([486, 2])
We keep 2.44e+04/7.84e+04 = 31% of the original kernel matrix.

torch.Size([1772, 2])
We keep 1.93e+05/1.85e+06 = 10% of the original kernel matrix.

torch.Size([839, 2])
We keep 1.08e+05/4.62e+05 = 23% of the original kernel matrix.

torch.Size([2350, 2])
We keep 3.85e+05/4.49e+06 =  8% of the original kernel matrix.

torch.Size([603, 2])
We keep 7.43e+04/2.45e+05 = 30% of the original kernel matrix.

torch.Size([1814, 2])
We keep 2.93e+05/3.27e+06 =  8% of the original kernel matrix.

torch.Size([1694, 2])
We keep 5.97e+05/2.63e+06 = 22% of the original kernel matrix.

torch.Size([3008, 2])
We keep 7.27e+05/1.07e+07 =  6% of the original kernel matrix.

torch.Size([157, 2])
We keep 8.55e+03/1.66e+04 = 51% of the original kernel matrix.

torch.Size([1313, 2])
We keep 1.25e+05/8.53e+05 = 14% of the original kernel matrix.

torch.Size([608, 2])
We keep 3.73e+04/1.40e+05 = 26% of the original kernel matrix.

torch.Size([1982, 2])
We keep 2.38e+05/2.47e+06 =  9% of the original kernel matrix.

torch.Size([728, 2])
We keep 1.06e+05/3.89e+05 = 27% of the original kernel matrix.

torch.Size([2263, 2])
We keep 3.69e+05/4.12e+06 =  8% of the original kernel matrix.

torch.Size([551, 2])
We keep 2.58e+04/9.30e+04 = 27% of the original kernel matrix.

torch.Size([1968, 2])
We keep 2.06e+05/2.02e+06 = 10% of the original kernel matrix.

torch.Size([437, 2])
We keep 1.99e+04/6.35e+04 = 31% of the original kernel matrix.

torch.Size([1771, 2])
We keep 1.81e+05/1.67e+06 = 10% of the original kernel matrix.

torch.Size([441, 2])
We keep 4.53e+04/1.53e+05 = 29% of the original kernel matrix.

torch.Size([1841, 2])
We keep 2.61e+05/2.58e+06 = 10% of the original kernel matrix.

torch.Size([1579, 2])
We keep 2.52e+05/1.26e+06 = 19% of the original kernel matrix.

torch.Size([2910, 2])
We keep 5.39e+05/7.43e+06 =  7% of the original kernel matrix.

torch.Size([713, 2])
We keep 4.90e+04/2.08e+05 = 23% of the original kernel matrix.

torch.Size([2105, 2])
We keep 2.75e+05/3.01e+06 =  9% of the original kernel matrix.

torch.Size([1408, 2])
We keep 4.21e+05/2.13e+06 = 19% of the original kernel matrix.

torch.Size([2993, 2])
We keep 7.15e+05/9.64e+06 =  7% of the original kernel matrix.

torch.Size([2083, 2])
We keep 4.40e+05/3.01e+06 = 14% of the original kernel matrix.

torch.Size([3358, 2])
We keep 7.62e+05/1.15e+07 =  6% of the original kernel matrix.

torch.Size([464, 2])
We keep 3.43e+04/1.23e+05 = 27% of the original kernel matrix.

torch.Size([1920, 2])
We keep 2.41e+05/2.32e+06 = 10% of the original kernel matrix.

torch.Size([1189, 2])
We keep 1.32e+05/7.12e+05 = 18% of the original kernel matrix.

torch.Size([2625, 2])
We keep 4.26e+05/5.58e+06 =  7% of the original kernel matrix.

torch.Size([760, 2])
We keep 6.81e+04/3.39e+05 = 20% of the original kernel matrix.

torch.Size([2491, 2])
We keep 3.49e+05/3.85e+06 =  9% of the original kernel matrix.

torch.Size([1024, 2])
We keep 1.76e+05/7.57e+05 = 23% of the original kernel matrix.

torch.Size([2404, 2])
We keep 4.69e+05/5.75e+06 =  8% of the original kernel matrix.

torch.Size([1277, 2])
We keep 2.80e+05/1.38e+06 = 20% of the original kernel matrix.

torch.Size([2650, 2])
We keep 5.73e+05/7.77e+06 =  7% of the original kernel matrix.

torch.Size([1077, 2])
We keep 4.09e+05/1.24e+06 = 32% of the original kernel matrix.

torch.Size([2249, 2])
We keep 5.29e+05/7.37e+06 =  7% of the original kernel matrix.

torch.Size([1007, 2])
We keep 9.97e+04/4.41e+05 = 22% of the original kernel matrix.

torch.Size([2363, 2])
We keep 3.67e+05/4.39e+06 =  8% of the original kernel matrix.

torch.Size([1038, 2])
We keep 1.10e+05/5.26e+05 = 20% of the original kernel matrix.

torch.Size([2466, 2])
We keep 3.84e+05/4.79e+06 =  8% of the original kernel matrix.

torch.Size([797, 2])
We keep 9.80e+04/3.40e+05 = 28% of the original kernel matrix.

torch.Size([2183, 2])
We keep 3.41e+05/3.85e+06 =  8% of the original kernel matrix.

torch.Size([1037, 2])
We keep 1.15e+05/5.76e+05 = 19% of the original kernel matrix.

torch.Size([2579, 2])
We keep 3.99e+05/5.02e+06 =  7% of the original kernel matrix.

torch.Size([638, 2])
We keep 4.36e+04/1.62e+05 = 26% of the original kernel matrix.

torch.Size([2050, 2])
We keep 2.55e+05/2.66e+06 =  9% of the original kernel matrix.

torch.Size([808, 2])
We keep 1.28e+05/5.49e+05 = 23% of the original kernel matrix.

torch.Size([2442, 2])
We keep 4.33e+05/4.90e+06 =  8% of the original kernel matrix.

torch.Size([3580, 2])
We keep 1.13e+06/1.04e+07 = 10% of the original kernel matrix.

torch.Size([4358, 2])
We keep 1.22e+06/2.13e+07 =  5% of the original kernel matrix.

torch.Size([1630, 2])
We keep 2.92e+05/1.56e+06 = 18% of the original kernel matrix.

torch.Size([2934, 2])
We keep 5.71e+05/8.26e+06 =  6% of the original kernel matrix.

torch.Size([533, 2])
We keep 4.62e+04/1.76e+05 = 26% of the original kernel matrix.

torch.Size([1992, 2])
We keep 2.76e+05/2.77e+06 =  9% of the original kernel matrix.

torch.Size([1415, 2])
We keep 1.81e+05/1.01e+06 = 17% of the original kernel matrix.

torch.Size([2863, 2])
We keep 4.88e+05/6.66e+06 =  7% of the original kernel matrix.

torch.Size([2589, 2])
We keep 9.12e+05/6.81e+06 = 13% of the original kernel matrix.

torch.Size([3512, 2])
We keep 1.03e+06/1.73e+07 =  5% of the original kernel matrix.

torch.Size([1792, 2])
We keep 3.25e+05/2.06e+06 = 15% of the original kernel matrix.

torch.Size([3080, 2])
We keep 6.42e+05/9.49e+06 =  6% of the original kernel matrix.

torch.Size([642, 2])
We keep 4.46e+04/1.55e+05 = 28% of the original kernel matrix.

torch.Size([2030, 2])
We keep 2.52e+05/2.60e+06 =  9% of the original kernel matrix.

torch.Size([2903, 2])
We keep 1.12e+06/8.54e+06 = 13% of the original kernel matrix.

torch.Size([3694, 2])
We keep 1.14e+06/1.93e+07 =  5% of the original kernel matrix.

torch.Size([963, 2])
We keep 1.39e+05/5.91e+05 = 23% of the original kernel matrix.

torch.Size([2488, 2])
We keep 4.35e+05/5.08e+06 =  8% of the original kernel matrix.

torch.Size([1413, 2])
We keep 3.36e+05/1.74e+06 = 19% of the original kernel matrix.

torch.Size([2839, 2])
We keep 6.35e+05/8.72e+06 =  7% of the original kernel matrix.

torch.Size([761, 2])
We keep 6.09e+04/2.47e+05 = 24% of the original kernel matrix.

torch.Size([2211, 2])
We keep 2.89e+05/3.29e+06 =  8% of the original kernel matrix.

torch.Size([1212, 2])
We keep 2.20e+05/1.09e+06 = 20% of the original kernel matrix.

torch.Size([2547, 2])
We keep 5.04e+05/6.91e+06 =  7% of the original kernel matrix.

torch.Size([758, 2])
We keep 6.48e+04/2.67e+05 = 24% of the original kernel matrix.

torch.Size([2301, 2])
We keep 3.13e+05/3.42e+06 =  9% of the original kernel matrix.

torch.Size([487, 2])
We keep 2.33e+04/7.67e+04 = 30% of the original kernel matrix.

torch.Size([1930, 2])
We keep 1.87e+05/1.83e+06 = 10% of the original kernel matrix.

torch.Size([395, 2])
We keep 1.03e+04/3.61e+04 = 28% of the original kernel matrix.

torch.Size([1864, 2])
We keep 1.48e+05/1.26e+06 = 11% of the original kernel matrix.

torch.Size([1480, 2])
We keep 2.54e+05/1.30e+06 = 19% of the original kernel matrix.

torch.Size([2638, 2])
We keep 5.51e+05/7.53e+06 =  7% of the original kernel matrix.

torch.Size([2190, 2])
We keep 1.04e+06/6.71e+06 = 15% of the original kernel matrix.

torch.Size([3471, 2])
We keep 1.10e+06/1.71e+07 =  6% of the original kernel matrix.

torch.Size([1143, 2])
We keep 2.39e+05/8.91e+05 = 26% of the original kernel matrix.

torch.Size([2298, 2])
We keep 4.66e+05/6.24e+06 =  7% of the original kernel matrix.

torch.Size([628, 2])
We keep 7.31e+04/2.59e+05 = 28% of the original kernel matrix.

torch.Size([1885, 2])
We keep 2.97e+05/3.36e+06 =  8% of the original kernel matrix.

torch.Size([708, 2])
We keep 9.37e+04/3.98e+05 = 23% of the original kernel matrix.

torch.Size([2232, 2])
We keep 3.76e+05/4.17e+06 =  9% of the original kernel matrix.

torch.Size([616, 2])
We keep 4.57e+04/1.72e+05 = 26% of the original kernel matrix.

torch.Size([2091, 2])
We keep 2.58e+05/2.74e+06 =  9% of the original kernel matrix.

torch.Size([530, 2])
We keep 7.82e+04/2.22e+05 = 35% of the original kernel matrix.

torch.Size([1791, 2])
We keep 2.92e+05/3.11e+06 =  9% of the original kernel matrix.

torch.Size([601, 2])
We keep 4.53e+04/1.62e+05 = 27% of the original kernel matrix.

torch.Size([1864, 2])
We keep 2.49e+05/2.66e+06 =  9% of the original kernel matrix.

torch.Size([581, 2])
We keep 2.44e+04/9.06e+04 = 26% of the original kernel matrix.

torch.Size([2137, 2])
We keep 2.13e+05/1.99e+06 = 10% of the original kernel matrix.

torch.Size([289, 2])
We keep 8.56e+03/2.40e+04 = 35% of the original kernel matrix.

torch.Size([1475, 2])
We keep 1.27e+05/1.02e+06 = 12% of the original kernel matrix.

torch.Size([906, 2])
We keep 1.21e+05/5.30e+05 = 22% of the original kernel matrix.

torch.Size([2878, 2])
We keep 4.19e+05/4.81e+06 =  8% of the original kernel matrix.

torch.Size([909, 2])
We keep 6.00e+04/2.83e+05 = 21% of the original kernel matrix.

torch.Size([2445, 2])
We keep 3.04e+05/3.52e+06 =  8% of the original kernel matrix.

torch.Size([1696, 2])
We keep 2.60e+05/1.42e+06 = 18% of the original kernel matrix.

torch.Size([3000, 2])
We keep 5.64e+05/7.88e+06 =  7% of the original kernel matrix.

torch.Size([573, 2])
We keep 3.62e+04/1.32e+05 = 27% of the original kernel matrix.

torch.Size([1938, 2])
We keep 2.35e+05/2.40e+06 =  9% of the original kernel matrix.

torch.Size([591, 2])
We keep 3.41e+04/1.23e+05 = 27% of the original kernel matrix.

torch.Size([2077, 2])
We keep 2.31e+05/2.32e+06 =  9% of the original kernel matrix.

torch.Size([711, 2])
We keep 8.67e+04/3.08e+05 = 28% of the original kernel matrix.

torch.Size([2192, 2])
We keep 3.25e+05/3.67e+06 =  8% of the original kernel matrix.

torch.Size([735, 2])
We keep 8.96e+04/3.05e+05 = 29% of the original kernel matrix.

torch.Size([2149, 2])
We keep 3.34e+05/3.65e+06 =  9% of the original kernel matrix.

torch.Size([1059, 2])
We keep 2.23e+05/1.04e+06 = 21% of the original kernel matrix.

torch.Size([2787, 2])
We keep 5.47e+05/6.76e+06 =  8% of the original kernel matrix.

torch.Size([758, 2])
We keep 7.93e+04/3.31e+05 = 23% of the original kernel matrix.

torch.Size([2237, 2])
We keep 3.36e+05/3.80e+06 =  8% of the original kernel matrix.

torch.Size([2408, 2])
We keep 2.27e+06/1.00e+07 = 22% of the original kernel matrix.

torch.Size([3292, 2])
We keep 1.27e+06/2.09e+07 =  6% of the original kernel matrix.

torch.Size([926, 2])
We keep 1.38e+05/5.75e+05 = 23% of the original kernel matrix.

torch.Size([2325, 2])
We keep 4.09e+05/5.01e+06 =  8% of the original kernel matrix.

torch.Size([1710, 2])
We keep 3.69e+05/2.03e+06 = 18% of the original kernel matrix.

torch.Size([3016, 2])
We keep 6.47e+05/9.43e+06 =  6% of the original kernel matrix.

torch.Size([1388, 2])
We keep 3.16e+05/1.72e+06 = 18% of the original kernel matrix.

torch.Size([2711, 2])
We keep 6.19e+05/8.67e+06 =  7% of the original kernel matrix.

torch.Size([1030, 2])
We keep 8.63e+04/3.93e+05 = 21% of the original kernel matrix.

torch.Size([2455, 2])
We keep 3.49e+05/4.14e+06 =  8% of the original kernel matrix.

torch.Size([998, 2])
We keep 1.16e+05/5.17e+05 = 22% of the original kernel matrix.

torch.Size([2445, 2])
We keep 3.93e+05/4.75e+06 =  8% of the original kernel matrix.

torch.Size([357, 2])
We keep 2.97e+04/8.12e+04 = 36% of the original kernel matrix.

torch.Size([1582, 2])
We keep 2.06e+05/1.88e+06 = 10% of the original kernel matrix.

torch.Size([1910, 2])
We keep 4.64e+05/2.67e+06 = 17% of the original kernel matrix.

torch.Size([3104, 2])
We keep 7.21e+05/1.08e+07 =  6% of the original kernel matrix.

torch.Size([453, 2])
We keep 1.29e+04/4.62e+04 = 27% of the original kernel matrix.

torch.Size([1963, 2])
We keep 1.57e+05/1.42e+06 = 11% of the original kernel matrix.

torch.Size([1582, 2])
We keep 3.04e+05/1.67e+06 = 18% of the original kernel matrix.

torch.Size([2932, 2])
We keep 6.16e+05/8.54e+06 =  7% of the original kernel matrix.

torch.Size([441, 2])
We keep 2.17e+04/8.58e+04 = 25% of the original kernel matrix.

torch.Size([1986, 2])
We keep 2.09e+05/1.94e+06 = 10% of the original kernel matrix.

torch.Size([582, 2])
We keep 5.35e+04/1.75e+05 = 30% of the original kernel matrix.

torch.Size([2070, 2])
We keep 2.73e+05/2.76e+06 =  9% of the original kernel matrix.

torch.Size([575, 2])
We keep 2.53e+04/9.99e+04 = 25% of the original kernel matrix.

torch.Size([2151, 2])
We keep 2.16e+05/2.09e+06 = 10% of the original kernel matrix.

torch.Size([528, 2])
We keep 3.85e+04/1.34e+05 = 28% of the original kernel matrix.

torch.Size([1882, 2])
We keep 2.37e+05/2.42e+06 =  9% of the original kernel matrix.

torch.Size([455, 2])
We keep 2.69e+04/8.35e+04 = 32% of the original kernel matrix.

torch.Size([1852, 2])
We keep 1.96e+05/1.91e+06 = 10% of the original kernel matrix.

torch.Size([564, 2])
We keep 5.96e+04/2.13e+05 = 27% of the original kernel matrix.

torch.Size([1976, 2])
We keep 3.02e+05/3.05e+06 =  9% of the original kernel matrix.

torch.Size([1187, 2])
We keep 9.02e+04/5.06e+05 = 17% of the original kernel matrix.

torch.Size([2930, 2])
We keep 3.82e+05/4.70e+06 =  8% of the original kernel matrix.

torch.Size([730, 2])
We keep 1.35e+05/5.10e+05 = 26% of the original kernel matrix.

torch.Size([2436, 2])
We keep 4.18e+05/4.72e+06 =  8% of the original kernel matrix.

torch.Size([1188, 2])
We keep 2.35e+05/9.08e+05 = 25% of the original kernel matrix.

torch.Size([2544, 2])
We keep 4.89e+05/6.30e+06 =  7% of the original kernel matrix.

torch.Size([889, 2])
We keep 1.51e+05/5.08e+05 = 29% of the original kernel matrix.

torch.Size([2240, 2])
We keep 3.99e+05/4.71e+06 =  8% of the original kernel matrix.

torch.Size([607, 2])
We keep 3.25e+04/1.22e+05 = 26% of the original kernel matrix.

torch.Size([2059, 2])
We keep 2.26e+05/2.31e+06 =  9% of the original kernel matrix.

torch.Size([1883, 2])
We keep 3.43e+05/2.09e+06 = 16% of the original kernel matrix.

torch.Size([3299, 2])
We keep 6.42e+05/9.56e+06 =  6% of the original kernel matrix.

torch.Size([486, 2])
We keep 1.93e+04/7.40e+04 = 26% of the original kernel matrix.

torch.Size([2009, 2])
We keep 1.95e+05/1.80e+06 = 10% of the original kernel matrix.

torch.Size([3766, 2])
We keep 2.86e+06/2.41e+07 = 11% of the original kernel matrix.

torch.Size([4255, 2])
We keep 1.77e+06/3.24e+07 =  5% of the original kernel matrix.

torch.Size([536, 2])
We keep 3.81e+04/1.46e+05 = 26% of the original kernel matrix.

torch.Size([1877, 2])
We keep 2.41e+05/2.53e+06 =  9% of the original kernel matrix.

torch.Size([506, 2])
We keep 5.03e+04/1.49e+05 = 33% of the original kernel matrix.

torch.Size([2213, 2])
We keep 2.86e+05/2.55e+06 = 11% of the original kernel matrix.

torch.Size([593, 2])
We keep 6.14e+04/2.65e+05 = 23% of the original kernel matrix.

torch.Size([2166, 2])
We keep 3.29e+05/3.40e+06 =  9% of the original kernel matrix.

torch.Size([743, 2])
We keep 6.39e+04/2.59e+05 = 24% of the original kernel matrix.

torch.Size([2104, 2])
We keep 3.03e+05/3.36e+06 =  9% of the original kernel matrix.

torch.Size([1132, 2])
We keep 2.98e+05/1.19e+06 = 25% of the original kernel matrix.

torch.Size([2462, 2])
We keep 5.57e+05/7.21e+06 =  7% of the original kernel matrix.

torch.Size([353, 2])
We keep 1.71e+04/4.88e+04 = 34% of the original kernel matrix.

torch.Size([1547, 2])
We keep 1.71e+05/1.46e+06 = 11% of the original kernel matrix.

torch.Size([783, 2])
We keep 7.43e+04/2.88e+05 = 25% of the original kernel matrix.

torch.Size([2267, 2])
We keep 3.23e+05/3.55e+06 =  9% of the original kernel matrix.

torch.Size([2209, 2])
We keep 8.02e+05/4.95e+06 = 16% of the original kernel matrix.

torch.Size([3631, 2])
We keep 9.33e+05/1.47e+07 =  6% of the original kernel matrix.

torch.Size([720, 2])
We keep 9.71e+04/4.15e+05 = 23% of the original kernel matrix.

torch.Size([2009, 2])
We keep 3.59e+05/4.26e+06 =  8% of the original kernel matrix.

torch.Size([2936, 2])
We keep 1.37e+06/1.10e+07 = 12% of the original kernel matrix.

torch.Size([4090, 2])
We keep 1.33e+06/2.19e+07 =  6% of the original kernel matrix.

torch.Size([396, 2])
We keep 1.91e+04/6.20e+04 = 30% of the original kernel matrix.

torch.Size([1681, 2])
We keep 1.78e+05/1.65e+06 = 10% of the original kernel matrix.

torch.Size([1101, 2])
We keep 1.87e+05/7.76e+05 = 24% of the original kernel matrix.

torch.Size([2408, 2])
We keep 4.47e+05/5.82e+06 =  7% of the original kernel matrix.

torch.Size([394, 2])
We keep 2.79e+04/9.30e+04 = 30% of the original kernel matrix.

torch.Size([1707, 2])
We keep 2.07e+05/2.02e+06 = 10% of the original kernel matrix.

torch.Size([788, 2])
We keep 9.66e+04/4.25e+05 = 22% of the original kernel matrix.

torch.Size([2340, 2])
We keep 3.77e+05/4.31e+06 =  8% of the original kernel matrix.

torch.Size([517, 2])
We keep 2.34e+04/7.29e+04 = 32% of the original kernel matrix.

torch.Size([1899, 2])
We keep 1.91e+05/1.78e+06 = 10% of the original kernel matrix.

torch.Size([584, 2])
We keep 3.29e+04/1.12e+05 = 29% of the original kernel matrix.

torch.Size([1944, 2])
We keep 2.20e+05/2.21e+06 =  9% of the original kernel matrix.

torch.Size([378, 2])
We keep 2.72e+04/8.41e+04 = 32% of the original kernel matrix.

torch.Size([1766, 2])
We keep 2.20e+05/1.92e+06 = 11% of the original kernel matrix.

torch.Size([721, 2])
We keep 8.24e+04/3.25e+05 = 25% of the original kernel matrix.

torch.Size([2324, 2])
We keep 3.52e+05/3.77e+06 =  9% of the original kernel matrix.

torch.Size([658, 2])
We keep 4.70e+04/1.76e+05 = 26% of the original kernel matrix.

torch.Size([2073, 2])
We keep 2.68e+05/2.78e+06 =  9% of the original kernel matrix.

torch.Size([621, 2])
We keep 7.73e+04/2.72e+05 = 28% of the original kernel matrix.

torch.Size([2094, 2])
We keep 3.37e+05/3.45e+06 =  9% of the original kernel matrix.

torch.Size([429, 2])
We keep 2.28e+04/6.50e+04 = 35% of the original kernel matrix.

torch.Size([1669, 2])
We keep 1.79e+05/1.69e+06 = 10% of the original kernel matrix.

torch.Size([1049, 2])
We keep 2.48e+05/1.11e+06 = 22% of the original kernel matrix.

torch.Size([2536, 2])
We keep 5.55e+05/6.96e+06 =  7% of the original kernel matrix.

torch.Size([1371, 2])
We keep 2.51e+05/1.31e+06 = 19% of the original kernel matrix.

torch.Size([2682, 2])
We keep 5.41e+05/7.57e+06 =  7% of the original kernel matrix.

torch.Size([827, 2])
We keep 8.22e+04/3.77e+05 = 21% of the original kernel matrix.

torch.Size([2552, 2])
We keep 3.56e+05/4.06e+06 =  8% of the original kernel matrix.

torch.Size([720, 2])
We keep 6.13e+04/2.63e+05 = 23% of the original kernel matrix.

torch.Size([2104, 2])
We keep 3.02e+05/3.39e+06 =  8% of the original kernel matrix.

torch.Size([737, 2])
We keep 8.52e+04/3.09e+05 = 27% of the original kernel matrix.

torch.Size([2355, 2])
We keep 3.20e+05/3.68e+06 =  8% of the original kernel matrix.

torch.Size([598, 2])
We keep 6.24e+04/2.38e+05 = 26% of the original kernel matrix.

torch.Size([2563, 2])
We keep 3.29e+05/3.23e+06 = 10% of the original kernel matrix.

torch.Size([778, 2])
We keep 7.75e+04/3.43e+05 = 22% of the original kernel matrix.

torch.Size([2380, 2])
We keep 3.33e+05/3.87e+06 =  8% of the original kernel matrix.

torch.Size([434, 2])
We keep 2.32e+04/6.60e+04 = 35% of the original kernel matrix.

torch.Size([1692, 2])
We keep 1.84e+05/1.70e+06 = 10% of the original kernel matrix.

torch.Size([563, 2])
We keep 5.92e+04/1.94e+05 = 30% of the original kernel matrix.

torch.Size([1864, 2])
We keep 2.72e+05/2.91e+06 =  9% of the original kernel matrix.

torch.Size([573, 2])
We keep 5.26e+04/1.90e+05 = 27% of the original kernel matrix.

torch.Size([1945, 2])
We keep 2.86e+05/2.88e+06 =  9% of the original kernel matrix.

torch.Size([733, 2])
We keep 8.20e+04/3.59e+05 = 22% of the original kernel matrix.

torch.Size([2364, 2])
We keep 3.49e+05/3.96e+06 =  8% of the original kernel matrix.

torch.Size([1390, 2])
We keep 3.39e+05/1.27e+06 = 26% of the original kernel matrix.

torch.Size([2691, 2])
We keep 5.36e+05/7.44e+06 =  7% of the original kernel matrix.

torch.Size([1352, 2])
We keep 3.07e+05/1.42e+06 = 21% of the original kernel matrix.

torch.Size([2765, 2])
We keep 5.87e+05/7.87e+06 =  7% of the original kernel matrix.

torch.Size([1583, 2])
We keep 6.32e+05/3.18e+06 = 19% of the original kernel matrix.

torch.Size([3319, 2])
We keep 8.46e+05/1.18e+07 =  7% of the original kernel matrix.

torch.Size([1679, 2])
We keep 2.58e+05/1.53e+06 = 16% of the original kernel matrix.

torch.Size([2965, 2])
We keep 5.72e+05/8.18e+06 =  6% of the original kernel matrix.

torch.Size([768, 2])
We keep 5.07e+04/2.21e+05 = 22% of the original kernel matrix.

torch.Size([2213, 2])
We keep 2.80e+05/3.11e+06 =  9% of the original kernel matrix.

torch.Size([1259, 2])
We keep 2.65e+05/1.17e+06 = 22% of the original kernel matrix.

torch.Size([2679, 2])
We keep 5.14e+05/7.16e+06 =  7% of the original kernel matrix.

torch.Size([442, 2])
We keep 2.85e+04/7.78e+04 = 36% of the original kernel matrix.

torch.Size([1639, 2])
We keep 1.97e+05/1.84e+06 = 10% of the original kernel matrix.

torch.Size([2366, 2])
We keep 3.30e+05/2.16e+06 = 15% of the original kernel matrix.

torch.Size([5130, 2])
We keep 6.87e+05/9.72e+06 =  7% of the original kernel matrix.

torch.Size([637, 2])
We keep 4.71e+04/1.73e+05 = 27% of the original kernel matrix.

torch.Size([2016, 2])
We keep 2.49e+05/2.75e+06 =  9% of the original kernel matrix.

torch.Size([489, 2])
We keep 5.60e+04/2.18e+05 = 25% of the original kernel matrix.

torch.Size([1845, 2])
We keep 2.97e+05/3.09e+06 =  9% of the original kernel matrix.

torch.Size([390, 2])
We keep 1.51e+04/4.49e+04 = 33% of the original kernel matrix.

torch.Size([1674, 2])
We keep 1.63e+05/1.40e+06 = 11% of the original kernel matrix.

torch.Size([838, 2])
We keep 9.88e+04/4.10e+05 = 24% of the original kernel matrix.

torch.Size([2361, 2])
We keep 3.69e+05/4.23e+06 =  8% of the original kernel matrix.

torch.Size([550, 2])
We keep 7.30e+04/2.38e+05 = 30% of the original kernel matrix.

torch.Size([1890, 2])
We keep 3.19e+05/3.23e+06 =  9% of the original kernel matrix.

torch.Size([1040, 2])
We keep 1.28e+05/5.34e+05 = 23% of the original kernel matrix.

torch.Size([2426, 2])
We keep 3.79e+05/4.83e+06 =  7% of the original kernel matrix.

torch.Size([1308, 2])
We keep 1.43e+05/7.24e+05 = 19% of the original kernel matrix.

torch.Size([2719, 2])
We keep 4.38e+05/5.63e+06 =  7% of the original kernel matrix.

torch.Size([1062, 2])
We keep 1.67e+05/7.36e+05 = 22% of the original kernel matrix.

torch.Size([2497, 2])
We keep 4.46e+05/5.67e+06 =  7% of the original kernel matrix.

torch.Size([619, 2])
We keep 5.54e+04/1.44e+05 = 38% of the original kernel matrix.

torch.Size([2042, 2])
We keep 2.37e+05/2.51e+06 =  9% of the original kernel matrix.

torch.Size([423, 2])
We keep 4.74e+04/1.25e+05 = 37% of the original kernel matrix.

torch.Size([1815, 2])
We keep 2.64e+05/2.34e+06 = 11% of the original kernel matrix.

torch.Size([801, 2])
We keep 6.85e+04/2.86e+05 = 23% of the original kernel matrix.

torch.Size([2119, 2])
We keep 3.11e+05/3.54e+06 =  8% of the original kernel matrix.

torch.Size([1878, 2])
We keep 6.06e+05/3.46e+06 = 17% of the original kernel matrix.

torch.Size([3358, 2])
We keep 8.25e+05/1.23e+07 =  6% of the original kernel matrix.

torch.Size([629, 2])
We keep 5.96e+04/2.14e+05 = 27% of the original kernel matrix.

torch.Size([1883, 2])
We keep 2.79e+05/3.06e+06 =  9% of the original kernel matrix.

torch.Size([600, 2])
We keep 8.41e+04/3.39e+05 = 24% of the original kernel matrix.

torch.Size([2336, 2])
We keep 3.45e+05/3.85e+06 =  8% of the original kernel matrix.

torch.Size([629, 2])
We keep 4.11e+04/2.01e+05 = 20% of the original kernel matrix.

torch.Size([2446, 2])
We keep 2.85e+05/2.96e+06 =  9% of the original kernel matrix.

torch.Size([1496, 2])
We keep 2.80e+05/1.26e+06 = 22% of the original kernel matrix.

torch.Size([2794, 2])
We keep 5.35e+05/7.43e+06 =  7% of the original kernel matrix.

torch.Size([854, 2])
We keep 1.15e+05/4.65e+05 = 24% of the original kernel matrix.

torch.Size([2338, 2])
We keep 3.93e+05/4.51e+06 =  8% of the original kernel matrix.

torch.Size([1281, 2])
We keep 2.37e+05/9.66e+05 = 24% of the original kernel matrix.

torch.Size([2619, 2])
We keep 4.79e+05/6.50e+06 =  7% of the original kernel matrix.

torch.Size([1660, 2])
We keep 2.35e+05/1.25e+06 = 18% of the original kernel matrix.

torch.Size([3041, 2])
We keep 5.33e+05/7.38e+06 =  7% of the original kernel matrix.

torch.Size([904, 2])
We keep 6.51e+04/3.23e+05 = 20% of the original kernel matrix.

torch.Size([2490, 2])
We keep 3.26e+05/3.75e+06 =  8% of the original kernel matrix.

torch.Size([593, 2])
We keep 4.16e+04/1.42e+05 = 29% of the original kernel matrix.

torch.Size([1883, 2])
We keep 2.44e+05/2.49e+06 =  9% of the original kernel matrix.

torch.Size([1163, 2])
We keep 1.74e+05/7.90e+05 = 22% of the original kernel matrix.

torch.Size([2536, 2])
We keep 4.50e+05/5.88e+06 =  7% of the original kernel matrix.

torch.Size([596, 2])
We keep 6.38e+04/2.55e+05 = 25% of the original kernel matrix.

torch.Size([2212, 2])
We keep 3.19e+05/3.34e+06 =  9% of the original kernel matrix.

torch.Size([1170, 2])
We keep 1.97e+05/8.69e+05 = 22% of the original kernel matrix.

torch.Size([2673, 2])
We keep 4.91e+05/6.16e+06 =  7% of the original kernel matrix.

torch.Size([1290, 2])
We keep 1.88e+05/1.00e+06 = 18% of the original kernel matrix.

torch.Size([2802, 2])
We keep 5.06e+05/6.62e+06 =  7% of the original kernel matrix.

torch.Size([1063, 2])
We keep 8.75e+04/4.30e+05 = 20% of the original kernel matrix.

torch.Size([2560, 2])
We keep 3.53e+05/4.34e+06 =  8% of the original kernel matrix.

torch.Size([760, 2])
We keep 6.30e+04/2.54e+05 = 24% of the original kernel matrix.

torch.Size([2180, 2])
We keep 2.99e+05/3.33e+06 =  8% of the original kernel matrix.

torch.Size([1222, 2])
We keep 1.43e+05/7.31e+05 = 19% of the original kernel matrix.

torch.Size([2985, 2])
We keep 4.55e+05/5.65e+06 =  8% of the original kernel matrix.

torch.Size([689, 2])
We keep 1.10e+05/4.34e+05 = 25% of the original kernel matrix.

torch.Size([2186, 2])
We keep 3.98e+05/4.36e+06 =  9% of the original kernel matrix.

torch.Size([891, 2])
We keep 9.24e+04/4.07e+05 = 22% of the original kernel matrix.

torch.Size([2352, 2])
We keep 3.61e+05/4.22e+06 =  8% of the original kernel matrix.

torch.Size([561, 2])
We keep 5.70e+04/1.51e+05 = 37% of the original kernel matrix.

torch.Size([1832, 2])
We keep 2.35e+05/2.56e+06 =  9% of the original kernel matrix.

torch.Size([800, 2])
We keep 8.60e+04/3.97e+05 = 21% of the original kernel matrix.

torch.Size([2281, 2])
We keep 3.47e+05/4.16e+06 =  8% of the original kernel matrix.

torch.Size([1022, 2])
We keep 1.17e+05/5.07e+05 = 23% of the original kernel matrix.

torch.Size([2394, 2])
We keep 3.65e+05/4.71e+06 =  7% of the original kernel matrix.

torch.Size([2041, 2])
We keep 4.47e+05/2.61e+06 = 17% of the original kernel matrix.

torch.Size([3170, 2])
We keep 7.23e+05/1.07e+07 =  6% of the original kernel matrix.

torch.Size([593, 2])
We keep 4.53e+04/1.54e+05 = 29% of the original kernel matrix.

torch.Size([1972, 2])
We keep 2.62e+05/2.60e+06 = 10% of the original kernel matrix.

torch.Size([585, 2])
We keep 5.59e+04/2.07e+05 = 26% of the original kernel matrix.

torch.Size([2089, 2])
We keep 2.91e+05/3.01e+06 =  9% of the original kernel matrix.

torch.Size([370, 2])
We keep 2.14e+04/6.30e+04 = 33% of the original kernel matrix.

torch.Size([1573, 2])
We keep 1.89e+05/1.66e+06 = 11% of the original kernel matrix.

torch.Size([1073, 2])
We keep 1.64e+05/7.50e+05 = 21% of the original kernel matrix.

torch.Size([2274, 2])
We keep 4.37e+05/5.72e+06 =  7% of the original kernel matrix.

torch.Size([483, 2])
We keep 2.06e+04/6.50e+04 = 31% of the original kernel matrix.

torch.Size([1974, 2])
We keep 1.88e+05/1.69e+06 = 11% of the original kernel matrix.

torch.Size([1443, 2])
We keep 2.72e+05/1.34e+06 = 20% of the original kernel matrix.

torch.Size([2810, 2])
We keep 5.49e+05/7.65e+06 =  7% of the original kernel matrix.

torch.Size([921, 2])
We keep 1.01e+05/4.20e+05 = 24% of the original kernel matrix.

torch.Size([2224, 2])
We keep 3.61e+05/4.28e+06 =  8% of the original kernel matrix.

torch.Size([2153, 2])
We keep 4.66e+05/2.85e+06 = 16% of the original kernel matrix.

torch.Size([3219, 2])
We keep 7.32e+05/1.12e+07 =  6% of the original kernel matrix.

torch.Size([659, 2])
We keep 6.30e+04/2.07e+05 = 30% of the original kernel matrix.

torch.Size([2128, 2])
We keep 2.90e+05/3.01e+06 =  9% of the original kernel matrix.

torch.Size([1285, 2])
We keep 2.09e+05/1.05e+06 = 19% of the original kernel matrix.

torch.Size([2493, 2])
We keep 4.93e+05/6.79e+06 =  7% of the original kernel matrix.

torch.Size([1215, 2])
We keep 1.45e+05/7.96e+05 = 18% of the original kernel matrix.

torch.Size([2658, 2])
We keep 4.54e+05/5.90e+06 =  7% of the original kernel matrix.

torch.Size([446, 2])
We keep 2.39e+04/7.24e+04 = 32% of the original kernel matrix.

torch.Size([2313, 2])
We keep 2.21e+05/1.78e+06 = 12% of the original kernel matrix.

torch.Size([539, 2])
We keep 5.24e+04/1.90e+05 = 27% of the original kernel matrix.

torch.Size([2002, 2])
We keep 2.84e+05/2.88e+06 =  9% of the original kernel matrix.

torch.Size([453, 2])
We keep 2.65e+04/9.73e+04 = 27% of the original kernel matrix.

torch.Size([1833, 2])
We keep 2.14e+05/2.06e+06 = 10% of the original kernel matrix.

torch.Size([1445, 2])
We keep 3.33e+05/1.83e+06 = 18% of the original kernel matrix.

torch.Size([3486, 2])
We keep 6.76e+05/8.94e+06 =  7% of the original kernel matrix.

torch.Size([1451, 2])
We keep 4.61e+05/2.27e+06 = 20% of the original kernel matrix.

torch.Size([2995, 2])
We keep 7.19e+05/9.95e+06 =  7% of the original kernel matrix.

torch.Size([348, 2])
We keep 2.47e+04/8.70e+04 = 28% of the original kernel matrix.

torch.Size([1615, 2])
We keep 1.94e+05/1.95e+06 =  9% of the original kernel matrix.

torch.Size([1057, 2])
We keep 1.17e+05/5.51e+05 = 21% of the original kernel matrix.

torch.Size([2440, 2])
We keep 4.05e+05/4.90e+06 =  8% of the original kernel matrix.

torch.Size([2342, 2])
We keep 6.23e+05/4.29e+06 = 14% of the original kernel matrix.

torch.Size([3424, 2])
We keep 8.65e+05/1.37e+07 =  6% of the original kernel matrix.

torch.Size([332, 2])
We keep 9.32e+03/3.06e+04 = 30% of the original kernel matrix.

torch.Size([1783, 2])
We keep 1.40e+05/1.16e+06 = 12% of the original kernel matrix.

torch.Size([580, 2])
We keep 4.93e+04/1.76e+05 = 27% of the original kernel matrix.

torch.Size([1960, 2])
We keep 2.57e+05/2.78e+06 =  9% of the original kernel matrix.

torch.Size([584, 2])
We keep 2.98e+04/1.07e+05 = 27% of the original kernel matrix.

torch.Size([1958, 2])
We keep 2.12e+05/2.16e+06 =  9% of the original kernel matrix.

torch.Size([392, 2])
We keep 1.91e+04/5.38e+04 = 35% of the original kernel matrix.

torch.Size([1671, 2])
We keep 1.76e+05/1.53e+06 = 11% of the original kernel matrix.

torch.Size([1405, 2])
We keep 4.28e+05/1.74e+06 = 24% of the original kernel matrix.

torch.Size([2600, 2])
We keep 5.90e+05/8.73e+06 =  6% of the original kernel matrix.

torch.Size([1891, 2])
We keep 3.06e+05/1.93e+06 = 15% of the original kernel matrix.

torch.Size([3154, 2])
We keep 6.25e+05/9.18e+06 =  6% of the original kernel matrix.

torch.Size([1887, 2])
We keep 5.52e+05/2.41e+06 = 22% of the original kernel matrix.

torch.Size([3067, 2])
We keep 6.78e+05/1.03e+07 =  6% of the original kernel matrix.

torch.Size([600, 2])
We keep 3.45e+04/1.38e+05 = 24% of the original kernel matrix.

torch.Size([2049, 2])
We keep 2.30e+05/2.46e+06 =  9% of the original kernel matrix.

torch.Size([1358, 2])
We keep 5.58e+05/2.69e+06 = 20% of the original kernel matrix.

torch.Size([2702, 2])
We keep 7.68e+05/1.08e+07 =  7% of the original kernel matrix.

torch.Size([1085, 2])
We keep 1.68e+05/7.38e+05 = 22% of the original kernel matrix.

torch.Size([2402, 2])
We keep 4.39e+05/5.68e+06 =  7% of the original kernel matrix.

torch.Size([502, 2])
We keep 6.37e+04/2.05e+05 = 31% of the original kernel matrix.

torch.Size([2123, 2])
We keep 3.01e+05/2.99e+06 = 10% of the original kernel matrix.

torch.Size([1138, 2])
We keep 2.43e+05/1.11e+06 = 21% of the original kernel matrix.

torch.Size([2753, 2])
We keep 5.54e+05/6.97e+06 =  7% of the original kernel matrix.

torch.Size([771, 2])
We keep 9.40e+04/3.42e+05 = 27% of the original kernel matrix.

torch.Size([2036, 2])
We keep 3.34e+05/3.87e+06 =  8% of the original kernel matrix.

torch.Size([423, 2])
We keep 1.53e+04/5.34e+04 = 28% of the original kernel matrix.

torch.Size([1887, 2])
We keep 1.68e+05/1.53e+06 = 11% of the original kernel matrix.

torch.Size([979, 2])
We keep 1.58e+05/6.94e+05 = 22% of the original kernel matrix.

torch.Size([2597, 2])
We keep 4.66e+05/5.51e+06 =  8% of the original kernel matrix.

torch.Size([572, 2])
We keep 3.89e+04/1.37e+05 = 28% of the original kernel matrix.

torch.Size([2045, 2])
We keep 2.47e+05/2.45e+06 = 10% of the original kernel matrix.

torch.Size([590, 2])
We keep 4.33e+04/1.73e+05 = 24% of the original kernel matrix.

torch.Size([2115, 2])
We keep 2.63e+05/2.75e+06 =  9% of the original kernel matrix.

torch.Size([699, 2])
We keep 4.96e+04/1.86e+05 = 26% of the original kernel matrix.

torch.Size([2121, 2])
We keep 2.64e+05/2.85e+06 =  9% of the original kernel matrix.

torch.Size([683, 2])
We keep 4.72e+04/1.76e+05 = 26% of the original kernel matrix.

torch.Size([2219, 2])
We keep 2.71e+05/2.78e+06 =  9% of the original kernel matrix.

torch.Size([770, 2])
We keep 1.47e+05/5.62e+05 = 26% of the original kernel matrix.

torch.Size([2557, 2])
We keep 4.33e+05/4.96e+06 =  8% of the original kernel matrix.

torch.Size([480, 2])
We keep 2.05e+04/6.81e+04 = 30% of the original kernel matrix.

torch.Size([1938, 2])
We keep 1.89e+05/1.73e+06 = 10% of the original kernel matrix.

torch.Size([710, 2])
We keep 6.78e+04/2.54e+05 = 26% of the original kernel matrix.

torch.Size([2155, 2])
We keep 3.03e+05/3.33e+06 =  9% of the original kernel matrix.

torch.Size([1171, 2])
We keep 1.02e+05/6.24e+05 = 16% of the original kernel matrix.

torch.Size([2969, 2])
We keep 4.12e+05/5.22e+06 =  7% of the original kernel matrix.

torch.Size([1023, 2])
We keep 2.04e+05/8.28e+05 = 24% of the original kernel matrix.

torch.Size([2546, 2])
We keep 4.87e+05/6.02e+06 =  8% of the original kernel matrix.

torch.Size([1002, 2])
We keep 9.15e+04/4.15e+05 = 22% of the original kernel matrix.

torch.Size([2425, 2])
We keep 3.54e+05/4.26e+06 =  8% of the original kernel matrix.

torch.Size([1618, 2])
We keep 2.60e+05/1.68e+06 = 15% of the original kernel matrix.

torch.Size([3129, 2])
We keep 6.05e+05/8.58e+06 =  7% of the original kernel matrix.

torch.Size([1505, 2])
We keep 4.72e+05/1.66e+06 = 28% of the original kernel matrix.

torch.Size([2862, 2])
We keep 6.18e+05/8.53e+06 =  7% of the original kernel matrix.

torch.Size([3846, 2])
We keep 2.59e+06/1.89e+07 = 13% of the original kernel matrix.

torch.Size([4633, 2])
We keep 1.57e+06/2.87e+07 =  5% of the original kernel matrix.

torch.Size([1943, 2])
We keep 7.14e+05/3.76e+06 = 18% of the original kernel matrix.

torch.Size([3055, 2])
We keep 8.37e+05/1.28e+07 =  6% of the original kernel matrix.

torch.Size([425, 2])
We keep 3.02e+04/9.30e+04 = 32% of the original kernel matrix.

torch.Size([1680, 2])
We keep 2.12e+05/2.02e+06 = 10% of the original kernel matrix.

torch.Size([1646, 2])
We keep 4.57e+05/2.37e+06 = 19% of the original kernel matrix.

torch.Size([3194, 2])
We keep 7.36e+05/1.02e+07 =  7% of the original kernel matrix.

torch.Size([2840, 2])
We keep 4.00e+06/1.26e+07 = 31% of the original kernel matrix.

torch.Size([3857, 2])
We keep 1.44e+06/2.35e+07 =  6% of the original kernel matrix.

torch.Size([10036, 2])
We keep 3.72e+07/1.34e+08 = 27% of the original kernel matrix.

torch.Size([6841, 2])
We keep 3.60e+06/7.66e+07 =  4% of the original kernel matrix.

torch.Size([934, 2])
We keep 1.26e+05/5.29e+05 = 23% of the original kernel matrix.

torch.Size([2592, 2])
We keep 4.13e+05/4.81e+06 =  8% of the original kernel matrix.

torch.Size([4120, 2])
We keep 2.98e+06/2.06e+07 = 14% of the original kernel matrix.

torch.Size([4452, 2])
We keep 1.60e+06/3.00e+07 =  5% of the original kernel matrix.

torch.Size([3101, 2])
We keep 7.95e+06/1.85e+07 = 42% of the original kernel matrix.

torch.Size([3655, 2])
We keep 1.62e+06/2.85e+07 =  5% of the original kernel matrix.

torch.Size([2036, 2])
We keep 2.32e+06/7.33e+06 = 31% of the original kernel matrix.

torch.Size([3187, 2])
We keep 1.14e+06/1.79e+07 =  6% of the original kernel matrix.

torch.Size([61502, 2])
We keep 7.74e+08/9.60e+09 =  8% of the original kernel matrix.

torch.Size([16723, 2])
We keep 2.37e+07/6.48e+08 =  3% of the original kernel matrix.

torch.Size([1004, 2])
We keep 9.03e+04/4.11e+05 = 21% of the original kernel matrix.

torch.Size([2462, 2])
We keep 3.53e+05/4.24e+06 =  8% of the original kernel matrix.

torch.Size([1574, 2])
We keep 2.83e+05/1.40e+06 = 20% of the original kernel matrix.

torch.Size([2956, 2])
We keep 5.72e+05/7.83e+06 =  7% of the original kernel matrix.

torch.Size([37336, 2])
We keep 2.18e+08/2.76e+09 =  7% of the original kernel matrix.

torch.Size([12663, 2])
We keep 1.34e+07/3.47e+08 =  3% of the original kernel matrix.

torch.Size([149128, 2])
We keep 1.48e+09/4.07e+10 =  3% of the original kernel matrix.

torch.Size([27078, 2])
We keep 4.54e+07/1.33e+09 =  3% of the original kernel matrix.

torch.Size([1102, 2])
We keep 1.27e+05/6.37e+05 = 19% of the original kernel matrix.

torch.Size([2547, 2])
We keep 3.97e+05/5.27e+06 =  7% of the original kernel matrix.

torch.Size([2085, 2])
We keep 8.41e+05/4.70e+06 = 17% of the original kernel matrix.

torch.Size([3291, 2])
We keep 9.43e+05/1.43e+07 =  6% of the original kernel matrix.

torch.Size([2191, 2])
We keep 7.76e+05/4.36e+06 = 17% of the original kernel matrix.

torch.Size([3452, 2])
We keep 8.82e+05/1.38e+07 =  6% of the original kernel matrix.

torch.Size([4004, 2])
We keep 3.07e+06/2.12e+07 = 14% of the original kernel matrix.

torch.Size([4453, 2])
We keep 1.69e+06/3.04e+07 =  5% of the original kernel matrix.

torch.Size([3229, 2])
We keep 2.58e+06/1.35e+07 = 19% of the original kernel matrix.

torch.Size([3810, 2])
We keep 1.35e+06/2.43e+07 =  5% of the original kernel matrix.

torch.Size([5613, 2])
We keep 8.61e+06/7.85e+07 = 10% of the original kernel matrix.

torch.Size([5075, 2])
We keep 2.90e+06/5.86e+07 =  4% of the original kernel matrix.

torch.Size([4300, 2])
We keep 4.98e+06/2.60e+07 = 19% of the original kernel matrix.

torch.Size([4826, 2])
We keep 1.68e+06/3.37e+07 =  4% of the original kernel matrix.

torch.Size([2615, 2])
We keep 1.13e+06/6.60e+06 = 17% of the original kernel matrix.

torch.Size([3696, 2])
We keep 1.00e+06/1.70e+07 =  5% of the original kernel matrix.

torch.Size([800, 2])
We keep 1.15e+05/4.20e+05 = 27% of the original kernel matrix.

torch.Size([2255, 2])
We keep 3.59e+05/4.28e+06 =  8% of the original kernel matrix.

torch.Size([6963, 2])
We keep 1.75e+07/9.66e+07 = 18% of the original kernel matrix.

torch.Size([6349, 2])
We keep 2.56e+06/6.50e+07 =  3% of the original kernel matrix.

torch.Size([2282, 2])
We keep 1.42e+06/5.16e+06 = 27% of the original kernel matrix.

torch.Size([3508, 2])
We keep 9.83e+05/1.50e+07 =  6% of the original kernel matrix.

torch.Size([873, 2])
We keep 5.61e+04/2.89e+05 = 19% of the original kernel matrix.

torch.Size([2380, 2])
We keep 3.03e+05/3.56e+06 =  8% of the original kernel matrix.

torch.Size([12524, 2])
We keep 2.31e+07/2.39e+08 =  9% of the original kernel matrix.

torch.Size([7562, 2])
We keep 4.50e+06/1.02e+08 =  4% of the original kernel matrix.

torch.Size([1712, 2])
We keep 1.78e+06/5.98e+06 = 29% of the original kernel matrix.

torch.Size([2881, 2])
We keep 1.04e+06/1.62e+07 =  6% of the original kernel matrix.

torch.Size([11324, 2])
We keep 3.85e+07/2.99e+08 = 12% of the original kernel matrix.

torch.Size([7390, 2])
We keep 5.31e+06/1.14e+08 =  4% of the original kernel matrix.

torch.Size([25220, 2])
We keep 5.42e+07/9.64e+08 =  5% of the original kernel matrix.

torch.Size([10647, 2])
We keep 8.17e+06/2.05e+08 =  3% of the original kernel matrix.

torch.Size([1797, 2])
We keep 5.95e+05/2.85e+06 = 20% of the original kernel matrix.

torch.Size([2811, 2])
We keep 7.47e+05/1.12e+07 =  6% of the original kernel matrix.

torch.Size([2034, 2])
We keep 6.04e+05/2.81e+06 = 21% of the original kernel matrix.

torch.Size([3025, 2])
We keep 7.30e+05/1.11e+07 =  6% of the original kernel matrix.

torch.Size([618, 2])
We keep 5.72e+04/2.12e+05 = 27% of the original kernel matrix.

torch.Size([2242, 2])
We keep 3.03e+05/3.04e+06 =  9% of the original kernel matrix.

torch.Size([5579, 2])
We keep 5.62e+06/3.11e+07 = 18% of the original kernel matrix.

torch.Size([5474, 2])
We keep 1.77e+06/3.69e+07 =  4% of the original kernel matrix.

torch.Size([24367, 2])
We keep 4.44e+07/7.21e+08 =  6% of the original kernel matrix.

torch.Size([11055, 2])
We keep 7.09e+06/1.77e+08 =  3% of the original kernel matrix.

torch.Size([1111, 2])
We keep 1.78e+05/7.55e+05 = 23% of the original kernel matrix.

torch.Size([2449, 2])
We keep 4.44e+05/5.74e+06 =  7% of the original kernel matrix.

torch.Size([16891, 2])
We keep 4.21e+07/6.25e+08 =  6% of the original kernel matrix.

torch.Size([8998, 2])
We keep 7.09e+06/1.65e+08 =  4% of the original kernel matrix.

torch.Size([399, 2])
We keep 3.07e+04/8.76e+04 = 35% of the original kernel matrix.

torch.Size([1593, 2])
We keep 2.04e+05/1.96e+06 = 10% of the original kernel matrix.

torch.Size([1924, 2])
We keep 4.26e+05/2.69e+06 = 15% of the original kernel matrix.

torch.Size([3367, 2])
We keep 7.18e+05/1.08e+07 =  6% of the original kernel matrix.

torch.Size([1604, 2])
We keep 2.50e+05/1.50e+06 = 16% of the original kernel matrix.

torch.Size([2971, 2])
We keep 5.73e+05/8.08e+06 =  7% of the original kernel matrix.

torch.Size([27307, 2])
We keep 1.32e+08/1.44e+09 =  9% of the original kernel matrix.

torch.Size([10472, 2])
We keep 9.63e+06/2.51e+08 =  3% of the original kernel matrix.

torch.Size([4839, 2])
We keep 8.52e+06/7.52e+07 = 11% of the original kernel matrix.

torch.Size([4741, 2])
We keep 2.87e+06/5.73e+07 =  4% of the original kernel matrix.

torch.Size([485, 2])
We keep 2.67e+04/9.73e+04 = 27% of the original kernel matrix.

torch.Size([1854, 2])
We keep 2.12e+05/2.06e+06 = 10% of the original kernel matrix.

torch.Size([35948, 2])
We keep 1.30e+08/2.56e+09 =  5% of the original kernel matrix.

torch.Size([13144, 2])
We keep 1.28e+07/3.35e+08 =  3% of the original kernel matrix.

torch.Size([5101, 2])
We keep 2.68e+06/2.95e+07 =  9% of the original kernel matrix.

torch.Size([5368, 2])
We keep 1.80e+06/3.59e+07 =  5% of the original kernel matrix.

torch.Size([5177, 2])
We keep 2.98e+06/3.00e+07 =  9% of the original kernel matrix.

torch.Size([5032, 2])
We keep 1.88e+06/3.62e+07 =  5% of the original kernel matrix.

torch.Size([14842, 2])
We keep 2.24e+07/2.74e+08 =  8% of the original kernel matrix.

torch.Size([8538, 2])
We keep 4.68e+06/1.09e+08 =  4% of the original kernel matrix.

torch.Size([1175, 2])
We keep 4.62e+05/1.59e+06 = 29% of the original kernel matrix.

torch.Size([2728, 2])
We keep 6.39e+05/8.34e+06 =  7% of the original kernel matrix.

torch.Size([7057, 2])
We keep 8.22e+06/7.59e+07 = 10% of the original kernel matrix.

torch.Size([6023, 2])
We keep 2.79e+06/5.76e+07 =  4% of the original kernel matrix.

torch.Size([3413, 2])
We keep 3.57e+06/1.61e+07 = 22% of the original kernel matrix.

torch.Size([4239, 2])
We keep 1.52e+06/2.65e+07 =  5% of the original kernel matrix.

torch.Size([18977, 2])
We keep 6.36e+07/7.55e+08 =  8% of the original kernel matrix.

torch.Size([9386, 2])
We keep 7.39e+06/1.82e+08 =  4% of the original kernel matrix.

torch.Size([5339, 2])
We keep 5.42e+06/4.78e+07 = 11% of the original kernel matrix.

torch.Size([5139, 2])
We keep 2.30e+06/4.57e+07 =  5% of the original kernel matrix.

torch.Size([5490, 2])
We keep 4.95e+06/4.70e+07 = 10% of the original kernel matrix.

torch.Size([4798, 2])
We keep 2.23e+06/4.53e+07 =  4% of the original kernel matrix.

torch.Size([7458, 2])
We keep 1.11e+07/9.71e+07 = 11% of the original kernel matrix.

torch.Size([5122, 2])
We keep 3.07e+06/6.51e+07 =  4% of the original kernel matrix.

torch.Size([2399, 2])
We keep 4.78e+05/3.53e+06 = 13% of the original kernel matrix.

torch.Size([3542, 2])
We keep 7.93e+05/1.24e+07 =  6% of the original kernel matrix.

torch.Size([2656, 2])
We keep 9.86e+05/6.33e+06 = 15% of the original kernel matrix.

torch.Size([3772, 2])
We keep 1.00e+06/1.66e+07 =  6% of the original kernel matrix.

torch.Size([4413, 2])
We keep 5.96e+06/4.10e+07 = 14% of the original kernel matrix.

torch.Size([5590, 2])
We keep 2.19e+06/4.23e+07 =  5% of the original kernel matrix.

torch.Size([7702, 2])
We keep 1.10e+07/7.88e+07 = 13% of the original kernel matrix.

torch.Size([6230, 2])
We keep 2.62e+06/5.87e+07 =  4% of the original kernel matrix.

torch.Size([21484, 2])
We keep 9.92e+07/8.52e+08 = 11% of the original kernel matrix.

torch.Size([10918, 2])
We keep 6.99e+06/1.93e+08 =  3% of the original kernel matrix.

torch.Size([7143, 2])
We keep 1.16e+07/7.59e+07 = 15% of the original kernel matrix.

torch.Size([6209, 2])
We keep 2.56e+06/5.76e+07 =  4% of the original kernel matrix.

torch.Size([2456, 2])
We keep 8.63e+05/6.60e+06 = 13% of the original kernel matrix.

torch.Size([3492, 2])
We keep 9.62e+05/1.70e+07 =  5% of the original kernel matrix.

torch.Size([1042, 2])
We keep 2.02e+05/1.08e+06 = 18% of the original kernel matrix.

torch.Size([2772, 2])
We keep 5.42e+05/6.87e+06 =  7% of the original kernel matrix.

torch.Size([6011, 2])
We keep 7.87e+06/6.53e+07 = 12% of the original kernel matrix.

torch.Size([5753, 2])
We keep 2.26e+06/5.34e+07 =  4% of the original kernel matrix.

torch.Size([3509, 2])
We keep 1.05e+06/1.07e+07 =  9% of the original kernel matrix.

torch.Size([4445, 2])
We keep 1.15e+06/2.17e+07 =  5% of the original kernel matrix.

torch.Size([896, 2])
We keep 1.82e+05/5.75e+05 = 31% of the original kernel matrix.

torch.Size([2200, 2])
We keep 3.92e+05/5.01e+06 =  7% of the original kernel matrix.

torch.Size([716, 2])
We keep 8.19e+04/3.24e+05 = 25% of the original kernel matrix.

torch.Size([2332, 2])
We keep 3.49e+05/3.76e+06 =  9% of the original kernel matrix.

torch.Size([2128, 2])
We keep 5.25e+05/3.42e+06 = 15% of the original kernel matrix.

torch.Size([3340, 2])
We keep 7.87e+05/1.22e+07 =  6% of the original kernel matrix.

torch.Size([2267, 2])
We keep 4.67e+05/3.69e+06 = 12% of the original kernel matrix.

torch.Size([3476, 2])
We keep 8.06e+05/1.27e+07 =  6% of the original kernel matrix.

torch.Size([79398, 2])
We keep 1.88e+09/1.94e+10 =  9% of the original kernel matrix.

torch.Size([18559, 2])
We keep 3.26e+07/9.21e+08 =  3% of the original kernel matrix.

torch.Size([1018, 2])
We keep 2.98e+05/1.26e+06 = 23% of the original kernel matrix.

torch.Size([2153, 2])
We keep 5.86e+05/7.41e+06 =  7% of the original kernel matrix.

torch.Size([1820, 2])
We keep 2.27e+06/4.04e+06 = 56% of the original kernel matrix.

torch.Size([2899, 2])
We keep 8.63e+05/1.33e+07 =  6% of the original kernel matrix.

torch.Size([7554, 2])
We keep 1.55e+07/9.40e+07 = 16% of the original kernel matrix.

torch.Size([6472, 2])
We keep 2.87e+06/6.41e+07 =  4% of the original kernel matrix.

torch.Size([965, 2])
We keep 6.09e+04/3.19e+05 = 19% of the original kernel matrix.

torch.Size([2668, 2])
We keep 3.18e+05/3.73e+06 =  8% of the original kernel matrix.

torch.Size([6450, 2])
We keep 3.85e+06/4.99e+07 =  7% of the original kernel matrix.

torch.Size([5710, 2])
We keep 2.30e+06/4.67e+07 =  4% of the original kernel matrix.

torch.Size([3476, 2])
We keep 1.06e+06/1.13e+07 =  9% of the original kernel matrix.

torch.Size([4394, 2])
We keep 1.26e+06/2.22e+07 =  5% of the original kernel matrix.

torch.Size([15575, 2])
We keep 1.92e+07/3.05e+08 =  6% of the original kernel matrix.

torch.Size([8681, 2])
We keep 4.94e+06/1.15e+08 =  4% of the original kernel matrix.

torch.Size([26122, 2])
We keep 4.64e+07/9.26e+08 =  5% of the original kernel matrix.

torch.Size([11632, 2])
We keep 7.98e+06/2.01e+08 =  3% of the original kernel matrix.

torch.Size([1816, 2])
We keep 2.46e+05/1.90e+06 = 12% of the original kernel matrix.

torch.Size([3317, 2])
We keep 5.97e+05/9.12e+06 =  6% of the original kernel matrix.

torch.Size([9264, 2])
We keep 9.65e+06/1.03e+08 =  9% of the original kernel matrix.

torch.Size([6468, 2])
We keep 3.12e+06/6.71e+07 =  4% of the original kernel matrix.

torch.Size([16953, 2])
We keep 4.73e+07/6.93e+08 =  6% of the original kernel matrix.

torch.Size([9151, 2])
We keep 7.48e+06/1.74e+08 =  4% of the original kernel matrix.

torch.Size([40860, 2])
We keep 1.78e+08/3.26e+09 =  5% of the original kernel matrix.

torch.Size([13952, 2])
We keep 1.42e+07/3.77e+08 =  3% of the original kernel matrix.

torch.Size([18962, 2])
We keep 3.52e+07/5.26e+08 =  6% of the original kernel matrix.

torch.Size([9375, 2])
We keep 6.30e+06/1.52e+08 =  4% of the original kernel matrix.

torch.Size([1050, 2])
We keep 1.44e+05/5.55e+05 = 26% of the original kernel matrix.

torch.Size([2335, 2])
We keep 3.84e+05/4.92e+06 =  7% of the original kernel matrix.

torch.Size([3935, 2])
We keep 1.96e+06/1.76e+07 = 11% of the original kernel matrix.

torch.Size([4116, 2])
We keep 1.51e+06/2.77e+07 =  5% of the original kernel matrix.

torch.Size([13839, 2])
We keep 1.99e+08/1.02e+09 = 19% of the original kernel matrix.

torch.Size([8670, 2])
We keep 7.66e+06/2.11e+08 =  3% of the original kernel matrix.

torch.Size([7275, 2])
We keep 7.62e+06/7.82e+07 =  9% of the original kernel matrix.

torch.Size([5942, 2])
We keep 2.84e+06/5.84e+07 =  4% of the original kernel matrix.

torch.Size([3394, 2])
We keep 2.47e+06/2.05e+07 = 12% of the original kernel matrix.

torch.Size([4160, 2])
We keep 1.71e+06/2.99e+07 =  5% of the original kernel matrix.

torch.Size([786, 2])
We keep 8.14e+04/3.42e+05 = 23% of the original kernel matrix.

torch.Size([2133, 2])
We keep 3.18e+05/3.87e+06 =  8% of the original kernel matrix.

torch.Size([30453, 2])
We keep 9.24e+07/1.22e+09 =  7% of the original kernel matrix.

torch.Size([12559, 2])
We keep 9.02e+06/2.31e+08 =  3% of the original kernel matrix.

torch.Size([22955, 2])
We keep 5.85e+07/8.39e+08 =  6% of the original kernel matrix.

torch.Size([10368, 2])
We keep 7.72e+06/1.91e+08 =  4% of the original kernel matrix.

torch.Size([2497, 2])
We keep 5.46e+06/1.75e+07 = 31% of the original kernel matrix.

torch.Size([3024, 2])
We keep 1.36e+06/2.77e+07 =  4% of the original kernel matrix.

torch.Size([2075, 2])
We keep 9.54e+05/3.89e+06 = 24% of the original kernel matrix.

torch.Size([3277, 2])
We keep 8.29e+05/1.30e+07 =  6% of the original kernel matrix.

torch.Size([6928, 2])
We keep 4.96e+06/5.55e+07 =  8% of the original kernel matrix.

torch.Size([5912, 2])
We keep 2.39e+06/4.92e+07 =  4% of the original kernel matrix.

torch.Size([2090, 2])
We keep 4.61e+05/2.79e+06 = 16% of the original kernel matrix.

torch.Size([3251, 2])
We keep 7.29e+05/1.10e+07 =  6% of the original kernel matrix.

torch.Size([2123, 2])
We keep 6.62e+05/4.11e+06 = 16% of the original kernel matrix.

torch.Size([3431, 2])
We keep 8.89e+05/1.34e+07 =  6% of the original kernel matrix.

torch.Size([3337, 2])
We keep 1.14e+06/1.00e+07 = 11% of the original kernel matrix.

torch.Size([4037, 2])
We keep 1.21e+06/2.09e+07 =  5% of the original kernel matrix.

torch.Size([2163, 2])
We keep 4.82e+05/3.06e+06 = 15% of the original kernel matrix.

torch.Size([3508, 2])
We keep 7.56e+05/1.16e+07 =  6% of the original kernel matrix.

torch.Size([6080, 2])
We keep 5.67e+06/4.63e+07 = 12% of the original kernel matrix.

torch.Size([5249, 2])
We keep 2.23e+06/4.50e+07 =  4% of the original kernel matrix.

torch.Size([7558, 2])
We keep 4.90e+06/6.92e+07 =  7% of the original kernel matrix.

torch.Size([6428, 2])
We keep 2.64e+06/5.50e+07 =  4% of the original kernel matrix.

torch.Size([5004, 2])
We keep 4.65e+06/4.10e+07 = 11% of the original kernel matrix.

torch.Size([5239, 2])
We keep 2.16e+06/4.23e+07 =  5% of the original kernel matrix.

torch.Size([5805, 2])
We keep 1.40e+07/6.71e+07 = 20% of the original kernel matrix.

torch.Size([5797, 2])
We keep 2.53e+06/5.42e+07 =  4% of the original kernel matrix.

torch.Size([2900, 2])
We keep 8.08e+05/5.95e+06 = 13% of the original kernel matrix.

torch.Size([3944, 2])
We keep 9.79e+05/1.61e+07 =  6% of the original kernel matrix.

torch.Size([141465, 2])
We keep 1.12e+09/3.11e+10 =  3% of the original kernel matrix.

torch.Size([27498, 2])
We keep 4.00e+07/1.17e+09 =  3% of the original kernel matrix.

torch.Size([2832, 2])
We keep 1.26e+06/9.10e+06 = 13% of the original kernel matrix.

torch.Size([3733, 2])
We keep 1.17e+06/1.99e+07 =  5% of the original kernel matrix.

torch.Size([20215, 2])
We keep 5.19e+07/7.54e+08 =  6% of the original kernel matrix.

torch.Size([9705, 2])
We keep 7.55e+06/1.82e+08 =  4% of the original kernel matrix.

torch.Size([650, 2])
We keep 3.81e+04/1.77e+05 = 21% of the original kernel matrix.

torch.Size([2072, 2])
We keep 2.51e+05/2.78e+06 =  9% of the original kernel matrix.

torch.Size([1756, 2])
We keep 3.96e+05/2.21e+06 = 17% of the original kernel matrix.

torch.Size([2978, 2])
We keep 6.83e+05/9.84e+06 =  6% of the original kernel matrix.

torch.Size([1115, 2])
We keep 1.35e+05/6.63e+05 = 20% of the original kernel matrix.

torch.Size([2577, 2])
We keep 4.23e+05/5.38e+06 =  7% of the original kernel matrix.

torch.Size([5171, 2])
We keep 2.70e+06/3.53e+07 =  7% of the original kernel matrix.

torch.Size([5416, 2])
We keep 1.93e+06/3.93e+07 =  4% of the original kernel matrix.

torch.Size([1502, 2])
We keep 2.17e+05/1.21e+06 = 18% of the original kernel matrix.

torch.Size([3726, 2])
We keep 5.58e+05/7.26e+06 =  7% of the original kernel matrix.

torch.Size([472, 2])
We keep 3.28e+04/9.99e+04 = 32% of the original kernel matrix.

torch.Size([1920, 2])
We keep 2.41e+05/2.09e+06 = 11% of the original kernel matrix.

torch.Size([2173, 2])
We keep 8.81e+05/6.23e+06 = 14% of the original kernel matrix.

torch.Size([3941, 2])
We keep 1.10e+06/1.65e+07 =  6% of the original kernel matrix.

torch.Size([1370, 2])
We keep 2.82e+05/1.28e+06 = 22% of the original kernel matrix.

torch.Size([2952, 2])
We keep 5.78e+05/7.47e+06 =  7% of the original kernel matrix.

torch.Size([63654, 2])
We keep 3.48e+08/6.47e+09 =  5% of the original kernel matrix.

torch.Size([17638, 2])
We keep 1.93e+07/5.32e+08 =  3% of the original kernel matrix.

torch.Size([13305, 2])
We keep 3.89e+07/2.79e+08 = 13% of the original kernel matrix.

torch.Size([7904, 2])
We keep 4.83e+06/1.10e+08 =  4% of the original kernel matrix.

torch.Size([1719, 2])
We keep 6.96e+05/3.19e+06 = 21% of the original kernel matrix.

torch.Size([3212, 2])
We keep 8.04e+05/1.18e+07 =  6% of the original kernel matrix.

torch.Size([7278, 2])
We keep 9.08e+06/8.50e+07 = 10% of the original kernel matrix.

torch.Size([6263, 2])
We keep 2.91e+06/6.09e+07 =  4% of the original kernel matrix.

torch.Size([1545, 2])
We keep 2.40e+05/1.18e+06 = 20% of the original kernel matrix.

torch.Size([2858, 2])
We keep 5.23e+05/7.17e+06 =  7% of the original kernel matrix.

torch.Size([1403, 2])
We keep 2.10e+05/1.18e+06 = 17% of the original kernel matrix.

torch.Size([3032, 2])
We keep 5.32e+05/7.19e+06 =  7% of the original kernel matrix.

torch.Size([2287, 2])
We keep 8.58e+05/5.42e+06 = 15% of the original kernel matrix.

torch.Size([3300, 2])
We keep 9.77e+05/1.54e+07 =  6% of the original kernel matrix.

torch.Size([5095, 2])
We keep 5.80e+06/3.27e+07 = 17% of the original kernel matrix.

torch.Size([5113, 2])
We keep 1.78e+06/3.78e+07 =  4% of the original kernel matrix.

torch.Size([386, 2])
We keep 1.35e+04/4.84e+04 = 27% of the original kernel matrix.

torch.Size([1794, 2])
We keep 1.57e+05/1.45e+06 = 10% of the original kernel matrix.

torch.Size([2224, 2])
We keep 1.46e+06/6.76e+06 = 21% of the original kernel matrix.

torch.Size([3603, 2])
We keep 1.11e+06/1.72e+07 =  6% of the original kernel matrix.

torch.Size([2291, 2])
We keep 7.35e+05/4.76e+06 = 15% of the original kernel matrix.

torch.Size([3513, 2])
We keep 8.72e+05/1.44e+07 =  6% of the original kernel matrix.

torch.Size([970, 2])
We keep 2.55e+05/7.67e+05 = 33% of the original kernel matrix.

torch.Size([2062, 2])
We keep 4.67e+05/5.79e+06 =  8% of the original kernel matrix.

torch.Size([2108, 2])
We keep 4.14e+05/2.40e+06 = 17% of the original kernel matrix.

torch.Size([3292, 2])
We keep 6.90e+05/1.02e+07 =  6% of the original kernel matrix.

torch.Size([5607, 2])
We keep 4.45e+06/4.40e+07 = 10% of the original kernel matrix.

torch.Size([4833, 2])
We keep 2.20e+06/4.38e+07 =  5% of the original kernel matrix.

torch.Size([1104, 2])
We keep 1.65e+05/8.06e+05 = 20% of the original kernel matrix.

torch.Size([2385, 2])
We keep 4.49e+05/5.94e+06 =  7% of the original kernel matrix.

torch.Size([22581, 2])
We keep 1.42e+08/1.37e+09 = 10% of the original kernel matrix.

torch.Size([9713, 2])
We keep 9.52e+06/2.45e+08 =  3% of the original kernel matrix.

torch.Size([2617, 2])
We keep 1.34e+06/6.36e+06 = 21% of the original kernel matrix.

torch.Size([3506, 2])
We keep 9.87e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([3590, 2])
We keep 3.04e+06/1.98e+07 = 15% of the original kernel matrix.

torch.Size([4421, 2])
We keep 1.65e+06/2.94e+07 =  5% of the original kernel matrix.

torch.Size([1125, 2])
We keep 2.42e+05/9.16e+05 = 26% of the original kernel matrix.

torch.Size([2634, 2])
We keep 5.07e+05/6.33e+06 =  8% of the original kernel matrix.

torch.Size([3440, 2])
We keep 1.08e+07/2.36e+07 = 45% of the original kernel matrix.

torch.Size([3777, 2])
We keep 1.77e+06/3.21e+07 =  5% of the original kernel matrix.

torch.Size([1215, 2])
We keep 2.66e+05/8.59e+05 = 31% of the original kernel matrix.

torch.Size([2673, 2])
We keep 4.94e+05/6.13e+06 =  8% of the original kernel matrix.

torch.Size([105641, 2])
We keep 2.26e+09/2.53e+10 =  8% of the original kernel matrix.

torch.Size([22872, 2])
We keep 3.42e+07/1.05e+09 =  3% of the original kernel matrix.

torch.Size([98743, 2])
We keep 5.24e+08/1.28e+10 =  4% of the original kernel matrix.

torch.Size([22362, 2])
We keep 2.62e+07/7.49e+08 =  3% of the original kernel matrix.

torch.Size([7067, 2])
We keep 8.16e+06/8.08e+07 = 10% of the original kernel matrix.

torch.Size([6250, 2])
We keep 2.69e+06/5.94e+07 =  4% of the original kernel matrix.

torch.Size([4158, 2])
We keep 1.43e+06/1.73e+07 =  8% of the original kernel matrix.

torch.Size([4822, 2])
We keep 1.45e+06/2.75e+07 =  5% of the original kernel matrix.

torch.Size([1611, 2])
We keep 1.89e+05/1.13e+06 = 16% of the original kernel matrix.

torch.Size([3078, 2])
We keep 5.03e+05/7.03e+06 =  7% of the original kernel matrix.

torch.Size([1720, 2])
We keep 8.27e+05/3.28e+06 = 25% of the original kernel matrix.

torch.Size([2932, 2])
We keep 7.66e+05/1.20e+07 =  6% of the original kernel matrix.

torch.Size([1302, 2])
We keep 1.65e+05/1.09e+06 = 15% of the original kernel matrix.

torch.Size([2830, 2])
We keep 4.83e+05/6.89e+06 =  7% of the original kernel matrix.

torch.Size([8942, 2])
We keep 6.81e+06/9.72e+07 =  7% of the original kernel matrix.

torch.Size([6986, 2])
We keep 3.01e+06/6.52e+07 =  4% of the original kernel matrix.

torch.Size([1127, 2])
We keep 3.10e+05/1.37e+06 = 22% of the original kernel matrix.

torch.Size([2476, 2])
We keep 6.03e+05/7.73e+06 =  7% of the original kernel matrix.

torch.Size([21219, 2])
We keep 6.82e+07/6.78e+08 = 10% of the original kernel matrix.

torch.Size([9308, 2])
We keep 6.94e+06/1.72e+08 =  4% of the original kernel matrix.

torch.Size([11515, 2])
We keep 4.62e+07/5.14e+08 =  8% of the original kernel matrix.

torch.Size([7570, 2])
We keep 6.67e+06/1.50e+08 =  4% of the original kernel matrix.

torch.Size([4511, 2])
We keep 6.39e+06/4.05e+07 = 15% of the original kernel matrix.

torch.Size([4099, 2])
We keep 2.10e+06/4.21e+07 =  5% of the original kernel matrix.

torch.Size([2527, 2])
We keep 1.76e+06/1.07e+07 = 16% of the original kernel matrix.

torch.Size([3226, 2])
We keep 1.26e+06/2.16e+07 =  5% of the original kernel matrix.

torch.Size([1842, 2])
We keep 2.51e+05/1.53e+06 = 16% of the original kernel matrix.

torch.Size([4372, 2])
We keep 6.15e+05/8.16e+06 =  7% of the original kernel matrix.

torch.Size([1793, 2])
We keep 3.86e+05/2.26e+06 = 17% of the original kernel matrix.

torch.Size([3046, 2])
We keep 6.69e+05/9.93e+06 =  6% of the original kernel matrix.

torch.Size([1432, 2])
We keep 1.83e+05/1.02e+06 = 17% of the original kernel matrix.

torch.Size([2852, 2])
We keep 4.94e+05/6.67e+06 =  7% of the original kernel matrix.

torch.Size([14783, 2])
We keep 4.24e+07/5.23e+08 =  8% of the original kernel matrix.

torch.Size([8124, 2])
We keep 6.48e+06/1.51e+08 =  4% of the original kernel matrix.

torch.Size([3907, 2])
We keep 2.53e+06/2.06e+07 = 12% of the original kernel matrix.

torch.Size([4169, 2])
We keep 1.59e+06/3.00e+07 =  5% of the original kernel matrix.

torch.Size([2804, 2])
We keep 2.06e+06/1.14e+07 = 18% of the original kernel matrix.

torch.Size([3291, 2])
We keep 1.23e+06/2.23e+07 =  5% of the original kernel matrix.

torch.Size([18439, 2])
We keep 5.59e+07/7.07e+08 =  7% of the original kernel matrix.

torch.Size([9602, 2])
We keep 7.42e+06/1.76e+08 =  4% of the original kernel matrix.

torch.Size([1659, 2])
We keep 4.15e+05/2.55e+06 = 16% of the original kernel matrix.

torch.Size([3349, 2])
We keep 7.32e+05/1.06e+07 =  6% of the original kernel matrix.

torch.Size([511, 2])
We keep 2.29e+04/9.49e+04 = 24% of the original kernel matrix.

torch.Size([1896, 2])
We keep 1.97e+05/2.04e+06 =  9% of the original kernel matrix.

torch.Size([2395, 2])
We keep 5.74e+05/4.00e+06 = 14% of the original kernel matrix.

torch.Size([3488, 2])
We keep 8.14e+05/1.32e+07 =  6% of the original kernel matrix.

torch.Size([15953, 2])
We keep 2.63e+07/3.68e+08 =  7% of the original kernel matrix.

torch.Size([8668, 2])
We keep 5.48e+06/1.27e+08 =  4% of the original kernel matrix.

torch.Size([9106, 2])
We keep 2.79e+07/2.32e+08 = 12% of the original kernel matrix.

torch.Size([6594, 2])
We keep 4.60e+06/1.01e+08 =  4% of the original kernel matrix.

torch.Size([25607, 2])
We keep 1.79e+08/1.64e+09 = 10% of the original kernel matrix.

torch.Size([10366, 2])
We keep 1.06e+07/2.68e+08 =  3% of the original kernel matrix.

torch.Size([10032, 2])
We keep 3.03e+07/3.14e+08 =  9% of the original kernel matrix.

torch.Size([6291, 2])
We keep 5.21e+06/1.17e+08 =  4% of the original kernel matrix.

torch.Size([1357, 2])
We keep 2.58e+05/1.08e+06 = 23% of the original kernel matrix.

torch.Size([2562, 2])
We keep 4.94e+05/6.86e+06 =  7% of the original kernel matrix.

torch.Size([9008, 2])
We keep 1.20e+07/1.21e+08 =  9% of the original kernel matrix.

torch.Size([6452, 2])
We keep 3.33e+06/7.28e+07 =  4% of the original kernel matrix.

torch.Size([32397, 2])
We keep 7.43e+07/1.36e+09 =  5% of the original kernel matrix.

torch.Size([12746, 2])
We keep 9.51e+06/2.44e+08 =  3% of the original kernel matrix.

torch.Size([12455, 2])
We keep 1.53e+07/2.24e+08 =  6% of the original kernel matrix.

torch.Size([7918, 2])
We keep 4.38e+06/9.90e+07 =  4% of the original kernel matrix.

torch.Size([3312, 2])
We keep 8.96e+05/8.66e+06 = 10% of the original kernel matrix.

torch.Size([4219, 2])
We keep 1.09e+06/1.95e+07 =  5% of the original kernel matrix.

torch.Size([24656, 2])
We keep 1.16e+08/1.05e+09 = 11% of the original kernel matrix.

torch.Size([11701, 2])
We keep 7.42e+06/2.14e+08 =  3% of the original kernel matrix.

torch.Size([28275, 2])
We keep 6.94e+07/1.12e+09 =  6% of the original kernel matrix.

torch.Size([12186, 2])
We keep 8.12e+06/2.21e+08 =  3% of the original kernel matrix.

torch.Size([10861, 2])
We keep 9.76e+06/1.47e+08 =  6% of the original kernel matrix.

torch.Size([7683, 2])
We keep 3.57e+06/8.02e+07 =  4% of the original kernel matrix.

torch.Size([9804, 2])
We keep 1.29e+07/1.50e+08 =  8% of the original kernel matrix.

torch.Size([7156, 2])
We keep 3.31e+06/8.11e+07 =  4% of the original kernel matrix.

torch.Size([1040, 2])
We keep 3.86e+05/1.26e+06 = 30% of the original kernel matrix.

torch.Size([2491, 2])
We keep 5.81e+05/7.42e+06 =  7% of the original kernel matrix.

torch.Size([4799, 2])
We keep 5.81e+06/3.33e+07 = 17% of the original kernel matrix.

torch.Size([4541, 2])
We keep 1.93e+06/3.82e+07 =  5% of the original kernel matrix.

torch.Size([2564, 2])
We keep 8.24e+05/5.68e+06 = 14% of the original kernel matrix.

torch.Size([3516, 2])
We keep 9.38e+05/1.58e+07 =  5% of the original kernel matrix.

torch.Size([841, 2])
We keep 7.58e+04/3.91e+05 = 19% of the original kernel matrix.

torch.Size([2368, 2])
We keep 3.49e+05/4.13e+06 =  8% of the original kernel matrix.

torch.Size([38704, 2])
We keep 1.18e+08/2.61e+09 =  4% of the original kernel matrix.

torch.Size([14296, 2])
We keep 1.30e+07/3.37e+08 =  3% of the original kernel matrix.

torch.Size([25683, 2])
We keep 4.02e+07/7.90e+08 =  5% of the original kernel matrix.

torch.Size([11618, 2])
We keep 7.46e+06/1.86e+08 =  4% of the original kernel matrix.

torch.Size([15569, 2])
We keep 4.19e+07/4.31e+08 =  9% of the original kernel matrix.

torch.Size([8533, 2])
We keep 5.69e+06/1.37e+08 =  4% of the original kernel matrix.

torch.Size([4384, 2])
We keep 3.81e+06/2.42e+07 = 15% of the original kernel matrix.

torch.Size([4732, 2])
We keep 1.77e+06/3.25e+07 =  5% of the original kernel matrix.

torch.Size([6755, 2])
We keep 1.47e+07/1.09e+08 = 13% of the original kernel matrix.

torch.Size([5867, 2])
We keep 3.45e+06/6.91e+07 =  4% of the original kernel matrix.

torch.Size([2964, 2])
We keep 1.37e+06/1.04e+07 = 13% of the original kernel matrix.

torch.Size([3907, 2])
We keep 1.23e+06/2.13e+07 =  5% of the original kernel matrix.

torch.Size([5948, 2])
We keep 3.01e+06/3.95e+07 =  7% of the original kernel matrix.

torch.Size([5758, 2])
We keep 2.03e+06/4.15e+07 =  4% of the original kernel matrix.

torch.Size([496, 2])
We keep 7.85e+04/2.44e+05 = 32% of the original kernel matrix.

torch.Size([1865, 2])
We keep 3.46e+05/3.27e+06 = 10% of the original kernel matrix.

torch.Size([2667, 2])
We keep 1.03e+06/6.84e+06 = 15% of the original kernel matrix.

torch.Size([3573, 2])
We keep 1.04e+06/1.73e+07 =  6% of the original kernel matrix.

torch.Size([19098, 2])
We keep 5.01e+07/6.00e+08 =  8% of the original kernel matrix.

torch.Size([8936, 2])
We keep 6.72e+06/1.62e+08 =  4% of the original kernel matrix.

torch.Size([8301, 2])
We keep 8.01e+06/8.41e+07 =  9% of the original kernel matrix.

torch.Size([6391, 2])
We keep 2.86e+06/6.06e+07 =  4% of the original kernel matrix.

torch.Size([1010, 2])
We keep 1.29e+05/6.35e+05 = 20% of the original kernel matrix.

torch.Size([2617, 2])
We keep 4.33e+05/5.27e+06 =  8% of the original kernel matrix.

torch.Size([1648, 2])
We keep 6.08e+05/3.63e+06 = 16% of the original kernel matrix.

torch.Size([2857, 2])
We keep 8.19e+05/1.26e+07 =  6% of the original kernel matrix.

torch.Size([31557, 2])
We keep 6.42e+08/3.96e+09 = 16% of the original kernel matrix.

torch.Size([11688, 2])
We keep 1.63e+07/4.16e+08 =  3% of the original kernel matrix.

torch.Size([4277, 2])
We keep 3.65e+06/2.19e+07 = 16% of the original kernel matrix.

torch.Size([4460, 2])
We keep 1.67e+06/3.09e+07 =  5% of the original kernel matrix.

torch.Size([3668, 2])
We keep 2.85e+06/2.60e+07 = 10% of the original kernel matrix.

torch.Size([4846, 2])
We keep 1.87e+06/3.37e+07 =  5% of the original kernel matrix.

torch.Size([5413, 2])
We keep 3.86e+06/3.67e+07 = 10% of the original kernel matrix.

torch.Size([5149, 2])
We keep 2.01e+06/4.01e+07 =  5% of the original kernel matrix.

torch.Size([2833, 2])
We keep 3.12e+06/1.37e+07 = 22% of the original kernel matrix.

torch.Size([3543, 2])
We keep 1.39e+06/2.45e+07 =  5% of the original kernel matrix.

torch.Size([713, 2])
We keep 8.25e+04/3.38e+05 = 24% of the original kernel matrix.

torch.Size([2030, 2])
We keep 3.24e+05/3.84e+06 =  8% of the original kernel matrix.

torch.Size([767, 2])
We keep 8.78e+04/3.60e+05 = 24% of the original kernel matrix.

torch.Size([2093, 2])
We keep 3.38e+05/3.97e+06 =  8% of the original kernel matrix.

torch.Size([7669, 2])
We keep 6.41e+06/8.88e+07 =  7% of the original kernel matrix.

torch.Size([6441, 2])
We keep 2.90e+06/6.23e+07 =  4% of the original kernel matrix.

torch.Size([6247, 2])
We keep 5.56e+06/5.02e+07 = 11% of the original kernel matrix.

torch.Size([5305, 2])
We keep 2.31e+06/4.68e+07 =  4% of the original kernel matrix.

torch.Size([954, 2])
We keep 1.09e+05/4.16e+05 = 26% of the original kernel matrix.

torch.Size([2345, 2])
We keep 3.52e+05/4.26e+06 =  8% of the original kernel matrix.

torch.Size([33729, 2])
We keep 1.29e+08/2.47e+09 =  5% of the original kernel matrix.

torch.Size([13031, 2])
We keep 1.29e+07/3.28e+08 =  3% of the original kernel matrix.

torch.Size([1904, 2])
We keep 4.80e+05/2.49e+06 = 19% of the original kernel matrix.

torch.Size([3196, 2])
We keep 6.72e+05/1.04e+07 =  6% of the original kernel matrix.

torch.Size([3721, 2])
We keep 2.15e+06/1.90e+07 = 11% of the original kernel matrix.

torch.Size([4690, 2])
We keep 1.62e+06/2.88e+07 =  5% of the original kernel matrix.

torch.Size([1653, 2])
We keep 2.73e+05/1.69e+06 = 16% of the original kernel matrix.

torch.Size([3113, 2])
We keep 5.74e+05/8.59e+06 =  6% of the original kernel matrix.

torch.Size([21226, 2])
We keep 4.55e+07/7.30e+08 =  6% of the original kernel matrix.

torch.Size([9954, 2])
We keep 7.39e+06/1.79e+08 =  4% of the original kernel matrix.

torch.Size([1168, 2])
We keep 1.72e+05/8.05e+05 = 21% of the original kernel matrix.

torch.Size([2820, 2])
We keep 4.71e+05/5.93e+06 =  7% of the original kernel matrix.

torch.Size([2780, 2])
We keep 1.83e+06/8.75e+06 = 20% of the original kernel matrix.

torch.Size([3567, 2])
We keep 1.13e+06/1.96e+07 =  5% of the original kernel matrix.

torch.Size([3750, 2])
We keep 5.02e+06/1.74e+07 = 28% of the original kernel matrix.

torch.Size([4115, 2])
We keep 1.52e+06/2.75e+07 =  5% of the original kernel matrix.

torch.Size([4195, 2])
We keep 7.81e+06/3.91e+07 = 19% of the original kernel matrix.

torch.Size([4551, 2])
We keep 2.21e+06/4.13e+07 =  5% of the original kernel matrix.

torch.Size([4716, 2])
We keep 2.21e+06/2.33e+07 =  9% of the original kernel matrix.

torch.Size([5047, 2])
We keep 1.66e+06/3.19e+07 =  5% of the original kernel matrix.

torch.Size([135002, 2])
We keep 1.02e+09/3.30e+10 =  3% of the original kernel matrix.

torch.Size([27103, 2])
We keep 4.06e+07/1.20e+09 =  3% of the original kernel matrix.

torch.Size([3383, 2])
We keep 1.78e+06/1.54e+07 = 11% of the original kernel matrix.

torch.Size([4174, 2])
We keep 1.44e+06/2.60e+07 =  5% of the original kernel matrix.

torch.Size([1043, 2])
We keep 1.63e+05/6.34e+05 = 25% of the original kernel matrix.

torch.Size([2380, 2])
We keep 4.11e+05/5.26e+06 =  7% of the original kernel matrix.

torch.Size([11950, 2])
We keep 4.12e+07/3.19e+08 = 12% of the original kernel matrix.

torch.Size([7087, 2])
We keep 5.03e+06/1.18e+08 =  4% of the original kernel matrix.

torch.Size([3586, 2])
We keep 2.44e+06/1.65e+07 = 14% of the original kernel matrix.

torch.Size([4120, 2])
We keep 1.50e+06/2.69e+07 =  5% of the original kernel matrix.

torch.Size([16480, 2])
We keep 1.99e+07/3.34e+08 =  5% of the original kernel matrix.

torch.Size([9067, 2])
We keep 5.12e+06/1.21e+08 =  4% of the original kernel matrix.

torch.Size([2376, 2])
We keep 1.20e+06/5.57e+06 = 21% of the original kernel matrix.

torch.Size([3230, 2])
We keep 9.57e+05/1.56e+07 =  6% of the original kernel matrix.

torch.Size([11276, 2])
We keep 3.82e+07/4.23e+08 =  9% of the original kernel matrix.

torch.Size([6341, 2])
We keep 6.05e+06/1.36e+08 =  4% of the original kernel matrix.

torch.Size([37091, 2])
We keep 1.84e+08/3.57e+09 =  5% of the original kernel matrix.

torch.Size([13885, 2])
We keep 1.52e+07/3.95e+08 =  3% of the original kernel matrix.

torch.Size([3613, 2])
We keep 2.82e+06/1.98e+07 = 14% of the original kernel matrix.

torch.Size([4272, 2])
We keep 1.54e+06/2.94e+07 =  5% of the original kernel matrix.

torch.Size([66524, 2])
We keep 3.61e+08/8.12e+09 =  4% of the original kernel matrix.

torch.Size([17047, 2])
We keep 2.14e+07/5.96e+08 =  3% of the original kernel matrix.

torch.Size([987, 2])
We keep 1.02e+05/6.40e+05 = 15% of the original kernel matrix.

torch.Size([2615, 2])
We keep 3.69e+05/5.29e+06 =  6% of the original kernel matrix.

torch.Size([6204, 2])
We keep 7.88e+06/6.03e+07 = 13% of the original kernel matrix.

torch.Size([5324, 2])
We keep 2.51e+06/5.13e+07 =  4% of the original kernel matrix.

torch.Size([3265, 2])
We keep 1.24e+06/9.63e+06 = 12% of the original kernel matrix.

torch.Size([4150, 2])
We keep 1.20e+06/2.05e+07 =  5% of the original kernel matrix.

torch.Size([34956, 2])
We keep 2.95e+08/3.13e+09 =  9% of the original kernel matrix.

torch.Size([12480, 2])
We keep 1.42e+07/3.70e+08 =  3% of the original kernel matrix.

torch.Size([742, 2])
We keep 7.79e+04/3.39e+05 = 23% of the original kernel matrix.

torch.Size([2329, 2])
We keep 3.36e+05/3.85e+06 =  8% of the original kernel matrix.

torch.Size([999, 2])
We keep 2.62e+05/1.14e+06 = 23% of the original kernel matrix.

torch.Size([2468, 2])
We keep 5.76e+05/7.06e+06 =  8% of the original kernel matrix.

torch.Size([1316, 2])
We keep 2.87e+05/1.36e+06 = 21% of the original kernel matrix.

torch.Size([2669, 2])
We keep 5.54e+05/7.70e+06 =  7% of the original kernel matrix.

torch.Size([4783, 2])
We keep 3.14e+06/3.41e+07 =  9% of the original kernel matrix.

torch.Size([5102, 2])
We keep 2.03e+06/3.86e+07 =  5% of the original kernel matrix.

torch.Size([16782, 2])
We keep 4.85e+07/4.04e+08 = 12% of the original kernel matrix.

torch.Size([8213, 2])
We keep 5.67e+06/1.33e+08 =  4% of the original kernel matrix.

torch.Size([4198, 2])
We keep 3.25e+06/2.80e+07 = 11% of the original kernel matrix.

torch.Size([4845, 2])
We keep 1.85e+06/3.50e+07 =  5% of the original kernel matrix.

torch.Size([2186, 2])
We keep 1.01e+06/6.29e+06 = 16% of the original kernel matrix.

torch.Size([3399, 2])
We keep 1.08e+06/1.66e+07 =  6% of the original kernel matrix.

torch.Size([1557, 2])
We keep 2.69e+05/1.44e+06 = 18% of the original kernel matrix.

torch.Size([2818, 2])
We keep 5.61e+05/7.95e+06 =  7% of the original kernel matrix.

torch.Size([2430, 2])
We keep 1.42e+06/5.75e+06 = 24% of the original kernel matrix.

torch.Size([3494, 2])
We keep 9.32e+05/1.59e+07 =  5% of the original kernel matrix.

torch.Size([11303, 2])
We keep 6.94e+07/4.58e+08 = 15% of the original kernel matrix.

torch.Size([7301, 2])
We keep 6.25e+06/1.41e+08 =  4% of the original kernel matrix.

torch.Size([4549, 2])
We keep 1.17e+07/6.26e+07 = 18% of the original kernel matrix.

torch.Size([4826, 2])
We keep 2.73e+06/5.23e+07 =  5% of the original kernel matrix.

torch.Size([44831, 2])
We keep 1.72e+08/3.19e+09 =  5% of the original kernel matrix.

torch.Size([15609, 2])
We keep 1.41e+07/3.73e+08 =  3% of the original kernel matrix.

torch.Size([3386, 2])
We keep 2.02e+06/1.43e+07 = 14% of the original kernel matrix.

torch.Size([4325, 2])
We keep 1.49e+06/2.50e+07 =  5% of the original kernel matrix.

torch.Size([2917, 2])
We keep 1.99e+06/1.01e+07 = 19% of the original kernel matrix.

torch.Size([3777, 2])
We keep 1.29e+06/2.10e+07 =  6% of the original kernel matrix.

torch.Size([598, 2])
We keep 5.65e+04/2.20e+05 = 25% of the original kernel matrix.

torch.Size([2105, 2])
We keep 2.96e+05/3.10e+06 =  9% of the original kernel matrix.

torch.Size([775, 2])
We keep 7.63e+04/3.16e+05 = 24% of the original kernel matrix.

torch.Size([2098, 2])
We keep 3.25e+05/3.71e+06 =  8% of the original kernel matrix.

torch.Size([62229, 2])
We keep 3.34e+08/6.06e+09 =  5% of the original kernel matrix.

torch.Size([18691, 2])
We keep 1.82e+07/5.15e+08 =  3% of the original kernel matrix.

torch.Size([1487, 2])
We keep 2.28e+05/1.29e+06 = 17% of the original kernel matrix.

torch.Size([3180, 2])
We keep 5.43e+05/7.50e+06 =  7% of the original kernel matrix.

torch.Size([1846, 2])
We keep 4.67e+05/2.65e+06 = 17% of the original kernel matrix.

torch.Size([3080, 2])
We keep 7.12e+05/1.08e+07 =  6% of the original kernel matrix.

torch.Size([15322, 2])
We keep 4.08e+07/4.01e+08 = 10% of the original kernel matrix.

torch.Size([7993, 2])
We keep 5.60e+06/1.32e+08 =  4% of the original kernel matrix.

torch.Size([4300, 2])
We keep 1.23e+07/4.64e+07 = 26% of the original kernel matrix.

torch.Size([3829, 2])
We keep 2.06e+06/4.50e+07 =  4% of the original kernel matrix.

torch.Size([1395, 2])
We keep 2.19e+05/1.19e+06 = 18% of the original kernel matrix.

torch.Size([2826, 2])
We keep 5.27e+05/7.20e+06 =  7% of the original kernel matrix.

torch.Size([927, 2])
We keep 1.07e+05/4.40e+05 = 24% of the original kernel matrix.

torch.Size([2321, 2])
We keep 3.63e+05/4.38e+06 =  8% of the original kernel matrix.

torch.Size([2264, 2])
We keep 1.28e+06/5.68e+06 = 22% of the original kernel matrix.

torch.Size([3403, 2])
We keep 9.93e+05/1.58e+07 =  6% of the original kernel matrix.

torch.Size([3591, 2])
We keep 1.25e+06/1.12e+07 = 11% of the original kernel matrix.

torch.Size([4410, 2])
We keep 1.24e+06/2.22e+07 =  5% of the original kernel matrix.

torch.Size([1540, 2])
We keep 8.31e+05/2.34e+06 = 35% of the original kernel matrix.

torch.Size([2753, 2])
We keep 7.04e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([7663, 2])
We keep 4.76e+06/5.99e+07 =  7% of the original kernel matrix.

torch.Size([6513, 2])
We keep 2.37e+06/5.11e+07 =  4% of the original kernel matrix.

torch.Size([3538, 2])
We keep 2.71e+06/2.05e+07 = 13% of the original kernel matrix.

torch.Size([5000, 2])
We keep 1.65e+06/3.00e+07 =  5% of the original kernel matrix.

torch.Size([2898, 2])
We keep 1.33e+06/7.50e+06 = 17% of the original kernel matrix.

torch.Size([3886, 2])
We keep 1.08e+06/1.81e+07 =  5% of the original kernel matrix.

torch.Size([49560, 2])
We keep 3.93e+08/4.74e+09 =  8% of the original kernel matrix.

torch.Size([15331, 2])
We keep 1.69e+07/4.55e+08 =  3% of the original kernel matrix.

torch.Size([12155, 2])
We keep 1.31e+07/1.76e+08 =  7% of the original kernel matrix.

torch.Size([7914, 2])
We keep 3.93e+06/8.78e+07 =  4% of the original kernel matrix.

torch.Size([1102, 2])
We keep 1.55e+05/7.78e+05 = 19% of the original kernel matrix.

torch.Size([2545, 2])
We keep 4.44e+05/5.83e+06 =  7% of the original kernel matrix.

torch.Size([8973, 2])
We keep 7.80e+06/1.05e+08 =  7% of the original kernel matrix.

torch.Size([6841, 2])
We keep 3.15e+06/6.78e+07 =  4% of the original kernel matrix.

torch.Size([1977, 2])
We keep 4.49e+05/2.46e+06 = 18% of the original kernel matrix.

torch.Size([3275, 2])
We keep 6.42e+05/1.04e+07 =  6% of the original kernel matrix.

torch.Size([1414, 2])
We keep 2.57e+05/1.44e+06 = 17% of the original kernel matrix.

torch.Size([2797, 2])
We keep 5.60e+05/7.92e+06 =  7% of the original kernel matrix.

torch.Size([3293, 2])
We keep 1.93e+06/1.50e+07 = 12% of the original kernel matrix.

torch.Size([3750, 2])
We keep 1.42e+06/2.56e+07 =  5% of the original kernel matrix.

torch.Size([1798, 2])
We keep 4.28e+05/2.47e+06 = 17% of the original kernel matrix.

torch.Size([2838, 2])
We keep 6.89e+05/1.04e+07 =  6% of the original kernel matrix.

torch.Size([14153, 2])
We keep 3.72e+07/3.99e+08 =  9% of the original kernel matrix.

torch.Size([8106, 2])
We keep 5.64e+06/1.32e+08 =  4% of the original kernel matrix.

torch.Size([5349, 2])
We keep 5.73e+06/5.44e+07 = 10% of the original kernel matrix.

torch.Size([4647, 2])
We keep 2.36e+06/4.88e+07 =  4% of the original kernel matrix.

torch.Size([1553, 2])
We keep 3.52e+05/1.54e+06 = 22% of the original kernel matrix.

torch.Size([2719, 2])
We keep 5.81e+05/8.19e+06 =  7% of the original kernel matrix.

torch.Size([4596, 2])
We keep 2.90e+06/2.46e+07 = 11% of the original kernel matrix.

torch.Size([4789, 2])
We keep 1.74e+06/3.28e+07 =  5% of the original kernel matrix.

torch.Size([2573, 2])
We keep 1.11e+06/6.42e+06 = 17% of the original kernel matrix.

torch.Size([3801, 2])
We keep 8.77e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([2910, 2])
We keep 1.98e+06/9.28e+06 = 21% of the original kernel matrix.

torch.Size([3343, 2])
We keep 1.12e+06/2.01e+07 =  5% of the original kernel matrix.

torch.Size([7661, 2])
We keep 2.76e+07/1.38e+08 = 20% of the original kernel matrix.

torch.Size([5619, 2])
We keep 3.59e+06/7.76e+07 =  4% of the original kernel matrix.

torch.Size([477, 2])
We keep 2.09e+04/7.56e+04 = 27% of the original kernel matrix.

torch.Size([1804, 2])
We keep 1.76e+05/1.82e+06 =  9% of the original kernel matrix.

torch.Size([1010, 2])
We keep 1.43e+05/5.70e+05 = 25% of the original kernel matrix.

torch.Size([2290, 2])
We keep 4.07e+05/4.99e+06 =  8% of the original kernel matrix.

torch.Size([1043, 2])
We keep 2.11e+05/8.65e+05 = 24% of the original kernel matrix.

torch.Size([2626, 2])
We keep 4.97e+05/6.15e+06 =  8% of the original kernel matrix.

torch.Size([783, 2])
We keep 1.24e+05/4.97e+05 = 25% of the original kernel matrix.

torch.Size([2166, 2])
We keep 3.92e+05/4.66e+06 =  8% of the original kernel matrix.

torch.Size([1555, 2])
We keep 4.19e+05/1.96e+06 = 21% of the original kernel matrix.

torch.Size([2652, 2])
We keep 6.45e+05/9.25e+06 =  6% of the original kernel matrix.

torch.Size([4828, 2])
We keep 1.79e+06/2.24e+07 =  7% of the original kernel matrix.

torch.Size([5129, 2])
We keep 1.61e+06/3.13e+07 =  5% of the original kernel matrix.

torch.Size([17896, 2])
We keep 2.65e+07/3.75e+08 =  7% of the original kernel matrix.

torch.Size([9326, 2])
We keep 5.44e+06/1.28e+08 =  4% of the original kernel matrix.

torch.Size([1571, 2])
We keep 5.03e+05/2.19e+06 = 22% of the original kernel matrix.

torch.Size([3128, 2])
We keep 6.91e+05/9.78e+06 =  7% of the original kernel matrix.

torch.Size([635, 2])
We keep 4.51e+04/1.71e+05 = 26% of the original kernel matrix.

torch.Size([1952, 2])
We keep 2.52e+05/2.73e+06 =  9% of the original kernel matrix.

torch.Size([979, 2])
We keep 1.99e+05/7.50e+05 = 26% of the original kernel matrix.

torch.Size([2441, 2])
We keep 4.85e+05/5.72e+06 =  8% of the original kernel matrix.

torch.Size([6955, 2])
We keep 8.99e+06/7.95e+07 = 11% of the original kernel matrix.

torch.Size([5816, 2])
We keep 2.90e+06/5.89e+07 =  4% of the original kernel matrix.

torch.Size([84910, 2])
We keep 7.84e+08/1.39e+10 =  5% of the original kernel matrix.

torch.Size([20726, 2])
We keep 2.78e+07/7.80e+08 =  3% of the original kernel matrix.

torch.Size([5348, 2])
We keep 7.27e+06/5.36e+07 = 13% of the original kernel matrix.

torch.Size([4729, 2])
We keep 2.35e+06/4.84e+07 =  4% of the original kernel matrix.

torch.Size([828, 2])
We keep 9.57e+04/4.60e+05 = 20% of the original kernel matrix.

torch.Size([2424, 2])
We keep 3.82e+05/4.48e+06 =  8% of the original kernel matrix.

torch.Size([3436, 2])
We keep 1.19e+06/1.03e+07 = 11% of the original kernel matrix.

torch.Size([4479, 2])
We keep 1.17e+06/2.13e+07 =  5% of the original kernel matrix.

torch.Size([37395, 2])
We keep 1.14e+08/2.23e+09 =  5% of the original kernel matrix.

torch.Size([13993, 2])
We keep 1.21e+07/3.12e+08 =  3% of the original kernel matrix.

torch.Size([867, 2])
We keep 7.92e+04/3.35e+05 = 23% of the original kernel matrix.

torch.Size([3017, 2])
We keep 3.61e+05/3.83e+06 =  9% of the original kernel matrix.

torch.Size([1396, 2])
We keep 2.02e+05/1.16e+06 = 17% of the original kernel matrix.

torch.Size([2818, 2])
We keep 5.15e+05/7.11e+06 =  7% of the original kernel matrix.

torch.Size([8484, 2])
We keep 6.69e+06/1.09e+08 =  6% of the original kernel matrix.

torch.Size([6854, 2])
We keep 3.19e+06/6.89e+07 =  4% of the original kernel matrix.

torch.Size([1025, 2])
We keep 3.23e+05/8.24e+05 = 39% of the original kernel matrix.

torch.Size([2486, 2])
We keep 5.04e+05/6.00e+06 =  8% of the original kernel matrix.

torch.Size([5196, 2])
We keep 7.38e+06/5.21e+07 = 14% of the original kernel matrix.

torch.Size([5148, 2])
We keep 2.36e+06/4.77e+07 =  4% of the original kernel matrix.

torch.Size([7637, 2])
We keep 8.39e+06/1.02e+08 =  8% of the original kernel matrix.

torch.Size([6368, 2])
We keep 3.24e+06/6.69e+07 =  4% of the original kernel matrix.

torch.Size([6973, 2])
We keep 3.39e+06/4.85e+07 =  6% of the original kernel matrix.

torch.Size([6161, 2])
We keep 2.25e+06/4.60e+07 =  4% of the original kernel matrix.

torch.Size([2869, 2])
We keep 1.19e+06/8.09e+06 = 14% of the original kernel matrix.

torch.Size([4087, 2])
We keep 1.07e+06/1.88e+07 =  5% of the original kernel matrix.

torch.Size([1209, 2])
We keep 2.63e+05/1.09e+06 = 24% of the original kernel matrix.

torch.Size([2506, 2])
We keep 5.09e+05/6.91e+06 =  7% of the original kernel matrix.

torch.Size([6116, 2])
We keep 8.82e+06/7.04e+07 = 12% of the original kernel matrix.

torch.Size([5301, 2])
We keep 2.73e+06/5.55e+07 =  4% of the original kernel matrix.

torch.Size([10155, 2])
We keep 2.48e+07/1.78e+08 = 13% of the original kernel matrix.

torch.Size([6655, 2])
We keep 4.09e+06/8.83e+07 =  4% of the original kernel matrix.

torch.Size([3792, 2])
We keep 4.29e+06/2.76e+07 = 15% of the original kernel matrix.

torch.Size([4052, 2])
We keep 1.85e+06/3.47e+07 =  5% of the original kernel matrix.

torch.Size([8090, 2])
We keep 8.26e+06/8.85e+07 =  9% of the original kernel matrix.

torch.Size([6401, 2])
We keep 2.88e+06/6.22e+07 =  4% of the original kernel matrix.

torch.Size([33868, 2])
We keep 9.26e+07/1.72e+09 =  5% of the original kernel matrix.

torch.Size([12539, 2])
We keep 1.06e+07/2.74e+08 =  3% of the original kernel matrix.

torch.Size([1126, 2])
We keep 1.26e+05/6.50e+05 = 19% of the original kernel matrix.

torch.Size([2639, 2])
We keep 4.10e+05/5.33e+06 =  7% of the original kernel matrix.

torch.Size([541, 2])
We keep 2.68e+04/9.99e+04 = 26% of the original kernel matrix.

torch.Size([1957, 2])
We keep 2.12e+05/2.09e+06 = 10% of the original kernel matrix.

torch.Size([2459, 2])
We keep 7.57e+05/5.90e+06 = 12% of the original kernel matrix.

torch.Size([3579, 2])
We keep 9.85e+05/1.61e+07 =  6% of the original kernel matrix.

torch.Size([13871, 2])
We keep 2.82e+07/3.36e+08 =  8% of the original kernel matrix.

torch.Size([8474, 2])
We keep 5.31e+06/1.21e+08 =  4% of the original kernel matrix.

torch.Size([7036, 2])
We keep 4.98e+06/6.73e+07 =  7% of the original kernel matrix.

torch.Size([6027, 2])
We keep 2.65e+06/5.42e+07 =  4% of the original kernel matrix.

torch.Size([2058, 2])
We keep 6.78e+05/3.72e+06 = 18% of the original kernel matrix.

torch.Size([3086, 2])
We keep 8.04e+05/1.28e+07 =  6% of the original kernel matrix.

torch.Size([863, 2])
We keep 6.60e+04/3.00e+05 = 21% of the original kernel matrix.

torch.Size([2303, 2])
We keep 3.06e+05/3.62e+06 =  8% of the original kernel matrix.

torch.Size([10060, 2])
We keep 1.14e+07/1.50e+08 =  7% of the original kernel matrix.

torch.Size([6576, 2])
We keep 3.71e+06/8.09e+07 =  4% of the original kernel matrix.

torch.Size([2010, 2])
We keep 4.28e+05/2.37e+06 = 18% of the original kernel matrix.

torch.Size([3135, 2])
We keep 6.81e+05/1.02e+07 =  6% of the original kernel matrix.

torch.Size([4846, 2])
We keep 7.27e+06/3.24e+07 = 22% of the original kernel matrix.

torch.Size([4936, 2])
We keep 1.75e+06/3.76e+07 =  4% of the original kernel matrix.

torch.Size([1110, 2])
We keep 1.67e+05/8.32e+05 = 20% of the original kernel matrix.

torch.Size([2613, 2])
We keep 4.67e+05/6.03e+06 =  7% of the original kernel matrix.

torch.Size([4197, 2])
We keep 1.52e+06/1.49e+07 = 10% of the original kernel matrix.

torch.Size([4824, 2])
We keep 1.39e+06/2.55e+07 =  5% of the original kernel matrix.

torch.Size([1400, 2])
We keep 2.96e+05/1.45e+06 = 20% of the original kernel matrix.

torch.Size([2693, 2])
We keep 5.91e+05/7.97e+06 =  7% of the original kernel matrix.

torch.Size([2574, 2])
We keep 6.18e+05/4.93e+06 = 12% of the original kernel matrix.

torch.Size([3655, 2])
We keep 9.01e+05/1.47e+07 =  6% of the original kernel matrix.

torch.Size([2273, 2])
We keep 5.87e+05/3.97e+06 = 14% of the original kernel matrix.

torch.Size([3326, 2])
We keep 8.40e+05/1.32e+07 =  6% of the original kernel matrix.

torch.Size([20054, 2])
We keep 2.75e+07/4.68e+08 =  5% of the original kernel matrix.

torch.Size([9898, 2])
We keep 5.96e+06/1.43e+08 =  4% of the original kernel matrix.

torch.Size([13393, 2])
We keep 1.53e+07/2.07e+08 =  7% of the original kernel matrix.

torch.Size([8132, 2])
We keep 4.21e+06/9.51e+07 =  4% of the original kernel matrix.

torch.Size([27216, 2])
We keep 1.15e+08/1.24e+09 =  9% of the original kernel matrix.

torch.Size([11516, 2])
We keep 9.20e+06/2.33e+08 =  3% of the original kernel matrix.

torch.Size([66315, 2])
We keep 5.35e+08/7.44e+09 =  7% of the original kernel matrix.

torch.Size([18047, 2])
We keep 2.09e+07/5.70e+08 =  3% of the original kernel matrix.

torch.Size([7560, 2])
We keep 4.47e+06/6.18e+07 =  7% of the original kernel matrix.

torch.Size([6257, 2])
We keep 2.47e+06/5.20e+07 =  4% of the original kernel matrix.

torch.Size([1474, 2])
We keep 2.58e+05/1.19e+06 = 21% of the original kernel matrix.

torch.Size([2749, 2])
We keep 5.25e+05/7.20e+06 =  7% of the original kernel matrix.

torch.Size([2142, 2])
We keep 4.10e+05/2.20e+06 = 18% of the original kernel matrix.

torch.Size([4836, 2])
We keep 7.42e+05/9.80e+06 =  7% of the original kernel matrix.

torch.Size([19021, 2])
We keep 4.75e+07/7.08e+08 =  6% of the original kernel matrix.

torch.Size([9337, 2])
We keep 7.31e+06/1.76e+08 =  4% of the original kernel matrix.

torch.Size([1253, 2])
We keep 2.60e+05/1.34e+06 = 19% of the original kernel matrix.

torch.Size([2932, 2])
We keep 5.82e+05/7.65e+06 =  7% of the original kernel matrix.

torch.Size([5148, 2])
We keep 3.18e+06/3.12e+07 = 10% of the original kernel matrix.

torch.Size([5088, 2])
We keep 1.89e+06/3.69e+07 =  5% of the original kernel matrix.

torch.Size([2914, 2])
We keep 7.20e+05/5.95e+06 = 12% of the original kernel matrix.

torch.Size([3898, 2])
We keep 9.63e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([24131, 2])
We keep 3.34e+08/2.45e+09 = 13% of the original kernel matrix.

torch.Size([10887, 2])
We keep 1.26e+07/3.27e+08 =  3% of the original kernel matrix.

torch.Size([22817, 2])
We keep 4.82e+07/6.22e+08 =  7% of the original kernel matrix.

torch.Size([10498, 2])
We keep 6.83e+06/1.65e+08 =  4% of the original kernel matrix.

torch.Size([1303, 2])
We keep 3.82e+05/1.86e+06 = 20% of the original kernel matrix.

torch.Size([2869, 2])
We keep 6.54e+05/9.02e+06 =  7% of the original kernel matrix.

torch.Size([22181, 2])
We keep 5.60e+07/8.61e+08 =  6% of the original kernel matrix.

torch.Size([10535, 2])
We keep 7.83e+06/1.94e+08 =  4% of the original kernel matrix.

torch.Size([5819, 2])
We keep 3.94e+06/4.89e+07 =  8% of the original kernel matrix.

torch.Size([5779, 2])
We keep 2.33e+06/4.62e+07 =  5% of the original kernel matrix.

torch.Size([5322, 2])
We keep 3.48e+06/3.06e+07 = 11% of the original kernel matrix.

torch.Size([5286, 2])
We keep 1.82e+06/3.66e+07 =  4% of the original kernel matrix.

torch.Size([35666, 2])
We keep 3.96e+08/4.02e+09 =  9% of the original kernel matrix.

torch.Size([12872, 2])
We keep 1.60e+07/4.19e+08 =  3% of the original kernel matrix.

torch.Size([26731, 2])
We keep 1.03e+08/1.82e+09 =  5% of the original kernel matrix.

torch.Size([11700, 2])
We keep 1.13e+07/2.82e+08 =  3% of the original kernel matrix.

torch.Size([26697, 2])
We keep 1.20e+08/1.24e+09 =  9% of the original kernel matrix.

torch.Size([10625, 2])
We keep 9.26e+06/2.33e+08 =  3% of the original kernel matrix.

torch.Size([2863, 2])
We keep 1.36e+06/9.78e+06 = 13% of the original kernel matrix.

torch.Size([3325, 2])
We keep 1.18e+06/2.07e+07 =  5% of the original kernel matrix.

torch.Size([21677, 2])
We keep 1.01e+08/1.02e+09 =  9% of the original kernel matrix.

torch.Size([10384, 2])
We keep 8.57e+06/2.11e+08 =  4% of the original kernel matrix.

torch.Size([3144, 2])
We keep 7.56e+05/6.74e+06 = 11% of the original kernel matrix.

torch.Size([4169, 2])
We keep 9.86e+05/1.72e+07 =  5% of the original kernel matrix.

torch.Size([8415, 2])
We keep 9.07e+06/1.11e+08 =  8% of the original kernel matrix.

torch.Size([5840, 2])
We keep 3.19e+06/6.95e+07 =  4% of the original kernel matrix.

torch.Size([66310, 2])
We keep 9.29e+08/1.13e+10 =  8% of the original kernel matrix.

torch.Size([17485, 2])
We keep 2.52e+07/7.03e+08 =  3% of the original kernel matrix.

torch.Size([3758, 2])
We keep 1.77e+06/1.56e+07 = 11% of the original kernel matrix.

torch.Size([4349, 2])
We keep 1.44e+06/2.61e+07 =  5% of the original kernel matrix.

torch.Size([826, 2])
We keep 8.12e+04/3.40e+05 = 23% of the original kernel matrix.

torch.Size([2288, 2])
We keep 3.29e+05/3.85e+06 =  8% of the original kernel matrix.

torch.Size([2294, 2])
We keep 4.61e+05/3.81e+06 = 12% of the original kernel matrix.

torch.Size([3497, 2])
We keep 7.92e+05/1.29e+07 =  6% of the original kernel matrix.

torch.Size([2130, 2])
We keep 6.62e+05/3.69e+06 = 17% of the original kernel matrix.

torch.Size([3177, 2])
We keep 8.28e+05/1.27e+07 =  6% of the original kernel matrix.

torch.Size([2259, 2])
We keep 1.61e+06/7.09e+06 = 22% of the original kernel matrix.

torch.Size([4040, 2])
We keep 1.15e+06/1.76e+07 =  6% of the original kernel matrix.

torch.Size([2117, 2])
We keep 5.09e+05/3.55e+06 = 14% of the original kernel matrix.

torch.Size([3286, 2])
We keep 7.88e+05/1.24e+07 =  6% of the original kernel matrix.

torch.Size([3072, 2])
We keep 1.59e+06/1.10e+07 = 14% of the original kernel matrix.

torch.Size([3969, 2])
We keep 1.30e+06/2.19e+07 =  5% of the original kernel matrix.

torch.Size([33719, 2])
We keep 2.14e+08/3.95e+09 =  5% of the original kernel matrix.

torch.Size([13053, 2])
We keep 1.61e+07/4.15e+08 =  3% of the original kernel matrix.

torch.Size([758, 2])
We keep 8.76e+04/4.34e+05 = 20% of the original kernel matrix.

torch.Size([2378, 2])
We keep 3.79e+05/4.36e+06 =  8% of the original kernel matrix.

torch.Size([127469, 2])
We keep 1.52e+09/2.66e+10 =  5% of the original kernel matrix.

torch.Size([24364, 2])
We keep 3.70e+07/1.08e+09 =  3% of the original kernel matrix.

torch.Size([3922, 2])
We keep 2.96e+06/2.52e+07 = 11% of the original kernel matrix.

torch.Size([4834, 2])
We keep 1.72e+06/3.32e+07 =  5% of the original kernel matrix.

torch.Size([8035, 2])
We keep 6.89e+06/8.17e+07 =  8% of the original kernel matrix.

torch.Size([6271, 2])
We keep 2.86e+06/5.97e+07 =  4% of the original kernel matrix.

torch.Size([11923, 2])
We keep 4.64e+07/3.29e+08 = 14% of the original kernel matrix.

torch.Size([6918, 2])
We keep 5.08e+06/1.20e+08 =  4% of the original kernel matrix.

torch.Size([12438, 2])
We keep 2.35e+07/3.35e+08 =  7% of the original kernel matrix.

torch.Size([10665, 2])
We keep 5.42e+06/1.21e+08 =  4% of the original kernel matrix.

torch.Size([40960, 2])
We keep 1.84e+08/3.70e+09 =  4% of the original kernel matrix.

torch.Size([14671, 2])
We keep 1.53e+07/4.02e+08 =  3% of the original kernel matrix.

torch.Size([992, 2])
We keep 9.49e+04/4.19e+05 = 22% of the original kernel matrix.

torch.Size([2422, 2])
We keep 3.57e+05/4.28e+06 =  8% of the original kernel matrix.

torch.Size([107766, 2])
We keep 6.64e+08/1.61e+10 =  4% of the original kernel matrix.

torch.Size([23540, 2])
We keep 2.93e+07/8.39e+08 =  3% of the original kernel matrix.

torch.Size([7669, 2])
We keep 7.49e+06/6.72e+07 = 11% of the original kernel matrix.

torch.Size([6358, 2])
We keep 2.57e+06/5.42e+07 =  4% of the original kernel matrix.

torch.Size([2112, 2])
We keep 1.51e+06/6.80e+06 = 22% of the original kernel matrix.

torch.Size([3369, 2])
We keep 1.11e+06/1.72e+07 =  6% of the original kernel matrix.

torch.Size([1493, 2])
We keep 1.39e+05/9.53e+05 = 14% of the original kernel matrix.

torch.Size([3086, 2])
We keep 4.45e+05/6.45e+06 =  6% of the original kernel matrix.

torch.Size([4018, 2])
We keep 2.15e+06/1.80e+07 = 11% of the original kernel matrix.

torch.Size([4323, 2])
We keep 1.49e+06/2.80e+07 =  5% of the original kernel matrix.

torch.Size([715, 2])
We keep 6.02e+04/2.10e+05 = 28% of the original kernel matrix.

torch.Size([2090, 2])
We keep 2.82e+05/3.03e+06 =  9% of the original kernel matrix.

torch.Size([563, 2])
We keep 3.85e+04/1.44e+05 = 26% of the original kernel matrix.

torch.Size([1940, 2])
We keep 2.45e+05/2.51e+06 =  9% of the original kernel matrix.

torch.Size([1771, 2])
We keep 4.92e+05/3.18e+06 = 15% of the original kernel matrix.

torch.Size([3251, 2])
We keep 8.14e+05/1.18e+07 =  6% of the original kernel matrix.

torch.Size([24566, 2])
We keep 1.69e+08/2.12e+09 =  7% of the original kernel matrix.

torch.Size([11123, 2])
We keep 1.26e+07/3.04e+08 =  4% of the original kernel matrix.

torch.Size([97349, 2])
We keep 4.39e+09/5.01e+10 =  8% of the original kernel matrix.

torch.Size([20433, 2])
We keep 5.00e+07/1.48e+09 =  3% of the original kernel matrix.

torch.Size([1997, 2])
We keep 9.03e+05/5.04e+06 = 17% of the original kernel matrix.

torch.Size([3226, 2])
We keep 9.30e+05/1.48e+07 =  6% of the original kernel matrix.

torch.Size([16458, 2])
We keep 2.43e+07/3.24e+08 =  7% of the original kernel matrix.

torch.Size([8749, 2])
We keep 5.17e+06/1.19e+08 =  4% of the original kernel matrix.

torch.Size([2316, 2])
We keep 1.23e+06/6.09e+06 = 20% of the original kernel matrix.

torch.Size([3586, 2])
We keep 1.04e+06/1.63e+07 =  6% of the original kernel matrix.

torch.Size([7757, 2])
We keep 8.08e+06/1.08e+08 =  7% of the original kernel matrix.

torch.Size([6319, 2])
We keep 3.27e+06/6.88e+07 =  4% of the original kernel matrix.

torch.Size([1055, 2])
We keep 1.13e+05/5.23e+05 = 21% of the original kernel matrix.

torch.Size([2499, 2])
We keep 3.87e+05/4.78e+06 =  8% of the original kernel matrix.

torch.Size([4145, 2])
We keep 1.07e+07/5.75e+07 = 18% of the original kernel matrix.

torch.Size([4123, 2])
We keep 2.59e+06/5.01e+07 =  5% of the original kernel matrix.

torch.Size([2354, 2])
We keep 6.74e+05/3.85e+06 = 17% of the original kernel matrix.

torch.Size([3411, 2])
We keep 8.22e+05/1.30e+07 =  6% of the original kernel matrix.

torch.Size([111944, 2])
We keep 7.12e+08/1.71e+10 =  4% of the original kernel matrix.

torch.Size([24416, 2])
We keep 3.01e+07/8.63e+08 =  3% of the original kernel matrix.

torch.Size([1054, 2])
We keep 1.37e+05/6.71e+05 = 20% of the original kernel matrix.

torch.Size([2491, 2])
We keep 4.19e+05/5.41e+06 =  7% of the original kernel matrix.

torch.Size([8731, 2])
We keep 5.98e+06/8.56e+07 =  6% of the original kernel matrix.

torch.Size([6817, 2])
We keep 2.85e+06/6.12e+07 =  4% of the original kernel matrix.

torch.Size([3798, 2])
We keep 3.07e+06/1.90e+07 = 16% of the original kernel matrix.

torch.Size([4718, 2])
We keep 1.43e+06/2.88e+07 =  4% of the original kernel matrix.

torch.Size([4669, 2])
We keep 3.57e+06/2.91e+07 = 12% of the original kernel matrix.

torch.Size([4788, 2])
We keep 1.87e+06/3.56e+07 =  5% of the original kernel matrix.

torch.Size([64578, 2])
We keep 2.89e+08/7.75e+09 =  3% of the original kernel matrix.

torch.Size([18252, 2])
We keep 2.08e+07/5.82e+08 =  3% of the original kernel matrix.

torch.Size([6735, 2])
We keep 4.50e+06/5.62e+07 =  8% of the original kernel matrix.

torch.Size([5810, 2])
We keep 2.45e+06/4.96e+07 =  4% of the original kernel matrix.

torch.Size([1991, 2])
We keep 4.35e+05/2.57e+06 = 16% of the original kernel matrix.

torch.Size([3235, 2])
We keep 6.93e+05/1.06e+07 =  6% of the original kernel matrix.

torch.Size([7394, 2])
We keep 1.05e+07/1.33e+08 =  7% of the original kernel matrix.

torch.Size([6422, 2])
We keep 3.73e+06/7.62e+07 =  4% of the original kernel matrix.

torch.Size([1600, 2])
We keep 3.35e+05/1.82e+06 = 18% of the original kernel matrix.

torch.Size([2976, 2])
We keep 6.38e+05/8.92e+06 =  7% of the original kernel matrix.

torch.Size([594, 2])
We keep 5.00e+04/1.74e+05 = 28% of the original kernel matrix.

torch.Size([1850, 2])
We keep 2.58e+05/2.76e+06 =  9% of the original kernel matrix.

torch.Size([29711, 2])
We keep 7.44e+07/1.33e+09 =  5% of the original kernel matrix.

torch.Size([11487, 2])
We keep 9.49e+06/2.41e+08 =  3% of the original kernel matrix.

torch.Size([121661, 2])
We keep 1.99e+09/2.60e+10 =  7% of the original kernel matrix.

torch.Size([25215, 2])
We keep 3.61e+07/1.06e+09 =  3% of the original kernel matrix.

torch.Size([3002, 2])
We keep 1.76e+06/9.67e+06 = 18% of the original kernel matrix.

torch.Size([5022, 2])
We keep 1.28e+06/2.06e+07 =  6% of the original kernel matrix.

torch.Size([9293, 2])
We keep 9.35e+06/1.20e+08 =  7% of the original kernel matrix.

torch.Size([6011, 2])
We keep 3.35e+06/7.24e+07 =  4% of the original kernel matrix.

torch.Size([12523, 2])
We keep 2.87e+07/2.53e+08 = 11% of the original kernel matrix.

torch.Size([7291, 2])
We keep 4.56e+06/1.05e+08 =  4% of the original kernel matrix.

torch.Size([2461, 2])
We keep 3.26e+06/1.18e+07 = 27% of the original kernel matrix.

torch.Size([3406, 2])
We keep 1.36e+06/2.27e+07 =  5% of the original kernel matrix.

torch.Size([16619, 2])
We keep 2.19e+07/3.42e+08 =  6% of the original kernel matrix.

torch.Size([9011, 2])
We keep 5.23e+06/1.22e+08 =  4% of the original kernel matrix.

torch.Size([1961, 2])
We keep 4.24e+05/2.99e+06 = 14% of the original kernel matrix.

torch.Size([4003, 2])
We keep 7.94e+05/1.14e+07 =  6% of the original kernel matrix.

torch.Size([27878, 2])
We keep 1.63e+08/1.66e+09 =  9% of the original kernel matrix.

torch.Size([11397, 2])
We keep 1.06e+07/2.69e+08 =  3% of the original kernel matrix.

torch.Size([12521, 2])
We keep 3.05e+07/3.25e+08 =  9% of the original kernel matrix.

torch.Size([6845, 2])
We keep 5.19e+06/1.19e+08 =  4% of the original kernel matrix.

torch.Size([9990, 2])
We keep 2.81e+07/2.35e+08 = 11% of the original kernel matrix.

torch.Size([6454, 2])
We keep 4.68e+06/1.01e+08 =  4% of the original kernel matrix.

torch.Size([4275, 2])
We keep 3.48e+06/2.44e+07 = 14% of the original kernel matrix.

torch.Size([4635, 2])
We keep 1.78e+06/3.27e+07 =  5% of the original kernel matrix.

torch.Size([1100, 2])
We keep 2.21e+05/8.82e+05 = 25% of the original kernel matrix.

torch.Size([2383, 2])
We keep 4.97e+05/6.21e+06 =  8% of the original kernel matrix.

torch.Size([2150, 2])
We keep 1.06e+06/4.90e+06 = 21% of the original kernel matrix.

torch.Size([3170, 2])
We keep 9.04e+05/1.46e+07 =  6% of the original kernel matrix.

torch.Size([86949, 2])
We keep 1.66e+09/2.94e+10 =  5% of the original kernel matrix.

torch.Size([18834, 2])
We keep 3.99e+07/1.13e+09 =  3% of the original kernel matrix.

torch.Size([8241, 2])
We keep 1.86e+07/1.31e+08 = 14% of the original kernel matrix.

torch.Size([6201, 2])
We keep 3.44e+06/7.57e+07 =  4% of the original kernel matrix.

torch.Size([2643, 2])
We keep 1.70e+06/1.16e+07 = 14% of the original kernel matrix.

torch.Size([3505, 2])
We keep 1.33e+06/2.25e+07 =  5% of the original kernel matrix.

torch.Size([681, 2])
We keep 6.44e+04/2.44e+05 = 26% of the original kernel matrix.

torch.Size([2056, 2])
We keep 2.95e+05/3.27e+06 =  9% of the original kernel matrix.

torch.Size([1231, 2])
We keep 4.08e+05/1.81e+06 = 22% of the original kernel matrix.

torch.Size([2890, 2])
We keep 6.80e+05/8.90e+06 =  7% of the original kernel matrix.

torch.Size([11175, 2])
We keep 2.83e+07/2.08e+08 = 13% of the original kernel matrix.

torch.Size([6579, 2])
We keep 4.21e+06/9.53e+07 =  4% of the original kernel matrix.

torch.Size([11734, 2])
We keep 2.37e+07/2.63e+08 =  9% of the original kernel matrix.

torch.Size([6862, 2])
We keep 4.78e+06/1.07e+08 =  4% of the original kernel matrix.

torch.Size([41497, 2])
We keep 2.45e+08/3.13e+09 =  7% of the original kernel matrix.

torch.Size([14578, 2])
We keep 1.40e+07/3.70e+08 =  3% of the original kernel matrix.

torch.Size([1096, 2])
We keep 1.60e+05/7.34e+05 = 21% of the original kernel matrix.

torch.Size([2755, 2])
We keep 4.61e+05/5.66e+06 =  8% of the original kernel matrix.

torch.Size([5120, 2])
We keep 2.74e+06/2.89e+07 =  9% of the original kernel matrix.

torch.Size([5062, 2])
We keep 1.85e+06/3.55e+07 =  5% of the original kernel matrix.

torch.Size([4120, 2])
We keep 3.65e+06/3.09e+07 = 11% of the original kernel matrix.

torch.Size([4498, 2])
We keep 1.98e+06/3.67e+07 =  5% of the original kernel matrix.

torch.Size([7493, 2])
We keep 7.12e+06/1.07e+08 =  6% of the original kernel matrix.

torch.Size([6497, 2])
We keep 3.28e+06/6.85e+07 =  4% of the original kernel matrix.

torch.Size([8176, 2])
We keep 8.55e+06/8.85e+07 =  9% of the original kernel matrix.

torch.Size([6061, 2])
We keep 2.90e+06/6.22e+07 =  4% of the original kernel matrix.

torch.Size([18452, 2])
We keep 2.35e+07/3.85e+08 =  6% of the original kernel matrix.

torch.Size([9355, 2])
We keep 5.44e+06/1.30e+08 =  4% of the original kernel matrix.

torch.Size([882, 2])
We keep 7.32e+04/2.91e+05 = 25% of the original kernel matrix.

torch.Size([2360, 2])
We keep 3.16e+05/3.56e+06 =  8% of the original kernel matrix.

torch.Size([8358, 2])
We keep 5.47e+06/8.42e+07 =  6% of the original kernel matrix.

torch.Size([6707, 2])
We keep 2.77e+06/6.06e+07 =  4% of the original kernel matrix.

torch.Size([703, 2])
We keep 3.35e+04/1.69e+05 = 19% of the original kernel matrix.

torch.Size([2283, 2])
We keep 2.51e+05/2.72e+06 =  9% of the original kernel matrix.

torch.Size([1337, 2])
We keep 3.66e+05/1.90e+06 = 19% of the original kernel matrix.

torch.Size([2971, 2])
We keep 6.87e+05/9.11e+06 =  7% of the original kernel matrix.

torch.Size([48278, 2])
We keep 1.76e+08/3.86e+09 =  4% of the original kernel matrix.

torch.Size([16207, 2])
We keep 1.49e+07/4.11e+08 =  3% of the original kernel matrix.

torch.Size([6323, 2])
We keep 6.94e+06/6.35e+07 = 10% of the original kernel matrix.

torch.Size([5243, 2])
We keep 2.57e+06/5.27e+07 =  4% of the original kernel matrix.

torch.Size([15255, 2])
We keep 1.86e+07/3.39e+08 =  5% of the original kernel matrix.

torch.Size([8793, 2])
We keep 5.16e+06/1.22e+08 =  4% of the original kernel matrix.

torch.Size([1716, 2])
We keep 4.36e+05/2.44e+06 = 17% of the original kernel matrix.

torch.Size([2918, 2])
We keep 7.10e+05/1.03e+07 =  6% of the original kernel matrix.

torch.Size([1074, 2])
We keep 3.87e+05/7.73e+05 = 50% of the original kernel matrix.

torch.Size([2506, 2])
We keep 4.61e+05/5.81e+06 =  7% of the original kernel matrix.

torch.Size([9855, 2])
We keep 2.14e+07/1.57e+08 = 13% of the original kernel matrix.

torch.Size([6327, 2])
We keep 3.80e+06/8.29e+07 =  4% of the original kernel matrix.

torch.Size([18467, 2])
We keep 4.13e+07/5.46e+08 =  7% of the original kernel matrix.

torch.Size([8777, 2])
We keep 6.43e+06/1.54e+08 =  4% of the original kernel matrix.

torch.Size([8487, 2])
We keep 1.06e+07/1.19e+08 =  8% of the original kernel matrix.

torch.Size([6553, 2])
We keep 3.32e+06/7.20e+07 =  4% of the original kernel matrix.

torch.Size([1326, 2])
We keep 1.72e+05/8.91e+05 = 19% of the original kernel matrix.

torch.Size([2707, 2])
We keep 4.63e+05/6.24e+06 =  7% of the original kernel matrix.

torch.Size([2785, 2])
We keep 7.14e+05/5.47e+06 = 13% of the original kernel matrix.

torch.Size([3986, 2])
We keep 9.47e+05/1.55e+07 =  6% of the original kernel matrix.

torch.Size([3308, 2])
We keep 1.16e+06/9.67e+06 = 12% of the original kernel matrix.

torch.Size([4203, 2])
We keep 1.19e+06/2.06e+07 =  5% of the original kernel matrix.

torch.Size([3072, 2])
We keep 1.84e+06/1.25e+07 = 14% of the original kernel matrix.

torch.Size([3741, 2])
We keep 1.32e+06/2.34e+07 =  5% of the original kernel matrix.

torch.Size([8240, 2])
We keep 2.25e+07/1.22e+08 = 18% of the original kernel matrix.

torch.Size([6268, 2])
We keep 3.41e+06/7.30e+07 =  4% of the original kernel matrix.

torch.Size([3086, 2])
We keep 1.63e+06/1.09e+07 = 15% of the original kernel matrix.

torch.Size([3919, 2])
We keep 1.22e+06/2.18e+07 =  5% of the original kernel matrix.

torch.Size([730, 2])
We keep 7.67e+04/3.06e+05 = 25% of the original kernel matrix.

torch.Size([2146, 2])
We keep 3.28e+05/3.66e+06 =  8% of the original kernel matrix.

torch.Size([2245, 2])
We keep 1.75e+06/6.58e+06 = 26% of the original kernel matrix.

torch.Size([3176, 2])
We keep 1.05e+06/1.70e+07 =  6% of the original kernel matrix.

torch.Size([910, 2])
We keep 2.01e+05/6.79e+05 = 29% of the original kernel matrix.

torch.Size([2098, 2])
We keep 4.20e+05/5.45e+06 =  7% of the original kernel matrix.

torch.Size([14667, 2])
We keep 2.27e+07/3.12e+08 =  7% of the original kernel matrix.

torch.Size([8783, 2])
We keep 4.88e+06/1.17e+08 =  4% of the original kernel matrix.

torch.Size([1891, 2])
We keep 3.23e+05/2.08e+06 = 15% of the original kernel matrix.

torch.Size([3206, 2])
We keep 6.45e+05/9.53e+06 =  6% of the original kernel matrix.

torch.Size([5964, 2])
We keep 3.80e+06/4.01e+07 =  9% of the original kernel matrix.

torch.Size([5419, 2])
We keep 2.10e+06/4.19e+07 =  5% of the original kernel matrix.

torch.Size([20683, 2])
We keep 4.25e+07/5.63e+08 =  7% of the original kernel matrix.

torch.Size([10116, 2])
We keep 6.51e+06/1.57e+08 =  4% of the original kernel matrix.

torch.Size([4405, 2])
We keep 2.08e+06/2.09e+07 =  9% of the original kernel matrix.

torch.Size([4879, 2])
We keep 1.61e+06/3.02e+07 =  5% of the original kernel matrix.

torch.Size([14246, 2])
We keep 2.88e+07/2.73e+08 = 10% of the original kernel matrix.

torch.Size([8454, 2])
We keep 4.78e+06/1.09e+08 =  4% of the original kernel matrix.

torch.Size([19269, 2])
We keep 4.85e+07/6.58e+08 =  7% of the original kernel matrix.

torch.Size([9651, 2])
We keep 7.14e+06/1.70e+08 =  4% of the original kernel matrix.

torch.Size([1068, 2])
We keep 1.15e+05/5.21e+05 = 22% of the original kernel matrix.

torch.Size([2647, 2])
We keep 3.89e+05/4.77e+06 =  8% of the original kernel matrix.

torch.Size([14641, 2])
We keep 7.44e+07/6.79e+08 = 10% of the original kernel matrix.

torch.Size([8162, 2])
We keep 7.47e+06/1.72e+08 =  4% of the original kernel matrix.

torch.Size([9933, 2])
We keep 9.06e+06/1.17e+08 =  7% of the original kernel matrix.

torch.Size([7257, 2])
We keep 3.18e+06/7.16e+07 =  4% of the original kernel matrix.

torch.Size([41946, 2])
We keep 2.94e+08/4.20e+09 =  7% of the original kernel matrix.

torch.Size([14547, 2])
We keep 1.59e+07/4.29e+08 =  3% of the original kernel matrix.

torch.Size([25948, 2])
We keep 5.93e+07/1.07e+09 =  5% of the original kernel matrix.

torch.Size([10674, 2])
We keep 8.63e+06/2.16e+08 =  3% of the original kernel matrix.

torch.Size([3233, 2])
We keep 4.87e+06/1.83e+07 = 26% of the original kernel matrix.

torch.Size([3865, 2])
We keep 1.59e+06/2.83e+07 =  5% of the original kernel matrix.

torch.Size([11903, 2])
We keep 1.75e+07/2.45e+08 =  7% of the original kernel matrix.

torch.Size([7955, 2])
We keep 4.61e+06/1.03e+08 =  4% of the original kernel matrix.

torch.Size([3622, 2])
We keep 1.25e+06/1.18e+07 = 10% of the original kernel matrix.

torch.Size([4323, 2])
We keep 1.27e+06/2.27e+07 =  5% of the original kernel matrix.

torch.Size([568, 2])
We keep 5.78e+04/1.82e+05 = 31% of the original kernel matrix.

torch.Size([1860, 2])
We keep 2.50e+05/2.82e+06 =  8% of the original kernel matrix.

torch.Size([1475, 2])
We keep 2.94e+05/1.52e+06 = 19% of the original kernel matrix.

torch.Size([2853, 2])
We keep 5.93e+05/8.16e+06 =  7% of the original kernel matrix.

torch.Size([3636, 2])
We keep 6.30e+06/4.29e+07 = 14% of the original kernel matrix.

torch.Size([3923, 2])
We keep 2.30e+06/4.33e+07 =  5% of the original kernel matrix.

torch.Size([16058, 2])
We keep 2.72e+07/3.49e+08 =  7% of the original kernel matrix.

torch.Size([8706, 2])
We keep 5.41e+06/1.23e+08 =  4% of the original kernel matrix.

torch.Size([8516, 2])
We keep 1.21e+07/1.11e+08 = 10% of the original kernel matrix.

torch.Size([6440, 2])
We keep 3.24e+06/6.97e+07 =  4% of the original kernel matrix.

torch.Size([740, 2])
We keep 8.22e+04/3.43e+05 = 23% of the original kernel matrix.

torch.Size([2657, 2])
We keep 3.67e+05/3.87e+06 =  9% of the original kernel matrix.

torch.Size([7729, 2])
We keep 7.92e+06/8.57e+07 =  9% of the original kernel matrix.

torch.Size([6403, 2])
We keep 2.86e+06/6.12e+07 =  4% of the original kernel matrix.

torch.Size([3182, 2])
We keep 7.85e+05/7.40e+06 = 10% of the original kernel matrix.

torch.Size([4158, 2])
We keep 1.04e+06/1.80e+07 =  5% of the original kernel matrix.

torch.Size([21984, 2])
We keep 1.50e+08/1.98e+09 =  7% of the original kernel matrix.

torch.Size([10336, 2])
We keep 1.21e+07/2.94e+08 =  4% of the original kernel matrix.

torch.Size([10997, 2])
We keep 7.91e+06/1.26e+08 =  6% of the original kernel matrix.

torch.Size([7491, 2])
We keep 3.38e+06/7.42e+07 =  4% of the original kernel matrix.

torch.Size([1732, 2])
We keep 4.15e+05/2.11e+06 = 19% of the original kernel matrix.

torch.Size([2928, 2])
We keep 6.68e+05/9.61e+06 =  6% of the original kernel matrix.

torch.Size([11130, 2])
We keep 1.25e+07/1.66e+08 =  7% of the original kernel matrix.

torch.Size([7635, 2])
We keep 3.87e+06/8.52e+07 =  4% of the original kernel matrix.

torch.Size([21044, 2])
We keep 3.23e+07/5.22e+08 =  6% of the original kernel matrix.

torch.Size([10140, 2])
We keep 6.33e+06/1.51e+08 =  4% of the original kernel matrix.

torch.Size([1653, 2])
We keep 5.17e+05/2.04e+06 = 25% of the original kernel matrix.

torch.Size([2913, 2])
We keep 6.62e+05/9.45e+06 =  7% of the original kernel matrix.

torch.Size([12092, 2])
We keep 2.60e+07/2.29e+08 = 11% of the original kernel matrix.

torch.Size([7166, 2])
We keep 4.38e+06/1.00e+08 =  4% of the original kernel matrix.

torch.Size([13320, 2])
We keep 3.26e+07/2.49e+08 = 13% of the original kernel matrix.

torch.Size([7975, 2])
We keep 4.54e+06/1.04e+08 =  4% of the original kernel matrix.

time for making ranges is 1.4907832145690918
Sorting X and nu_X
time for sorting X is 0.0529019832611084
Sorting Z and nu_Z
time for sorting Z is 0.0002799034118652344
Starting Optim
sum tnu_Z before tensor(12447836., device='cuda:0')
c= tensor(1616.1284, device='cuda:0')
c= tensor(16197.9922, device='cuda:0')
c= tensor(16684.3867, device='cuda:0')
c= tensor(18169.0352, device='cuda:0')
c= tensor(245313.7969, device='cuda:0')
c= tensor(376486.6562, device='cuda:0')
c= tensor(513099.7500, device='cuda:0')
c= tensor(756525.3750, device='cuda:0')
c= tensor(762042.1250, device='cuda:0')
c= tensor(1779731.3750, device='cuda:0')
c= tensor(1783141.2500, device='cuda:0')
c= tensor(3527667.5000, device='cuda:0')
c= tensor(3566064., device='cuda:0')
c= tensor(4100457.5000, device='cuda:0')
c= tensor(4155134., device='cuda:0')
c= tensor(4593210.5000, device='cuda:0')
c= tensor(4704108., device='cuda:0')
c= tensor(4823494., device='cuda:0')
c= tensor(45332376., device='cuda:0')
c= tensor(45556624., device='cuda:0')
c= tensor(45728924., device='cuda:0')
c= tensor(67804984., device='cuda:0')
c= tensor(67863096., device='cuda:0')
c= tensor(67872040., device='cuda:0')
c= tensor(68109160., device='cuda:0')
c= tensor(68326096., device='cuda:0')
c= tensor(68326992., device='cuda:0')
c= tensor(68332632., device='cuda:0')
c= tensor(68368040., device='cuda:0')
c= tensor(4.1275e+08, device='cuda:0')
c= tensor(4.1275e+08, device='cuda:0')
c= tensor(4.1482e+08, device='cuda:0')
c= tensor(4.1482e+08, device='cuda:0')
c= tensor(4.1483e+08, device='cuda:0')
c= tensor(4.1487e+08, device='cuda:0')
c= tensor(4.1494e+08, device='cuda:0')
c= tensor(4.1512e+08, device='cuda:0')
c= tensor(4.1512e+08, device='cuda:0')
c= tensor(4.1512e+08, device='cuda:0')
c= tensor(4.1513e+08, device='cuda:0')
c= tensor(4.1513e+08, device='cuda:0')
c= tensor(4.1513e+08, device='cuda:0')
c= tensor(4.1513e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1515e+08, device='cuda:0')
c= tensor(4.1515e+08, device='cuda:0')
c= tensor(4.1515e+08, device='cuda:0')
c= tensor(4.1515e+08, device='cuda:0')
c= tensor(4.1515e+08, device='cuda:0')
c= tensor(4.1516e+08, device='cuda:0')
c= tensor(4.1516e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1519e+08, device='cuda:0')
c= tensor(4.1519e+08, device='cuda:0')
c= tensor(4.1519e+08, device='cuda:0')
c= tensor(4.1520e+08, device='cuda:0')
c= tensor(4.1521e+08, device='cuda:0')
c= tensor(4.1521e+08, device='cuda:0')
c= tensor(4.1521e+08, device='cuda:0')
c= tensor(4.1523e+08, device='cuda:0')
c= tensor(4.1523e+08, device='cuda:0')
c= tensor(4.1523e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1525e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1531e+08, device='cuda:0')
c= tensor(4.1531e+08, device='cuda:0')
c= tensor(4.1532e+08, device='cuda:0')
c= tensor(4.1532e+08, device='cuda:0')
c= tensor(4.1532e+08, device='cuda:0')
c= tensor(4.1532e+08, device='cuda:0')
c= tensor(4.1532e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1535e+08, device='cuda:0')
c= tensor(4.1535e+08, device='cuda:0')
c= tensor(4.1539e+08, device='cuda:0')
c= tensor(4.1539e+08, device='cuda:0')
c= tensor(4.1539e+08, device='cuda:0')
c= tensor(4.1539e+08, device='cuda:0')
c= tensor(4.1539e+08, device='cuda:0')
c= tensor(4.1540e+08, device='cuda:0')
c= tensor(4.1540e+08, device='cuda:0')
c= tensor(4.1540e+08, device='cuda:0')
c= tensor(4.1541e+08, device='cuda:0')
c= tensor(4.1541e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1546e+08, device='cuda:0')
c= tensor(4.1546e+08, device='cuda:0')
c= tensor(4.1547e+08, device='cuda:0')
c= tensor(4.1547e+08, device='cuda:0')
c= tensor(4.1547e+08, device='cuda:0')
c= tensor(4.1547e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1549e+08, device='cuda:0')
c= tensor(4.1549e+08, device='cuda:0')
c= tensor(4.1549e+08, device='cuda:0')
c= tensor(4.1549e+08, device='cuda:0')
c= tensor(4.1549e+08, device='cuda:0')
c= tensor(4.1550e+08, device='cuda:0')
c= tensor(4.1550e+08, device='cuda:0')
c= tensor(4.1550e+08, device='cuda:0')
c= tensor(4.1550e+08, device='cuda:0')
c= tensor(4.1550e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1553e+08, device='cuda:0')
c= tensor(4.1553e+08, device='cuda:0')
c= tensor(4.1553e+08, device='cuda:0')
c= tensor(4.1553e+08, device='cuda:0')
c= tensor(4.1553e+08, device='cuda:0')
c= tensor(4.1554e+08, device='cuda:0')
c= tensor(4.1554e+08, device='cuda:0')
c= tensor(4.1554e+08, device='cuda:0')
c= tensor(4.1554e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1556e+08, device='cuda:0')
c= tensor(4.1556e+08, device='cuda:0')
c= tensor(4.1556e+08, device='cuda:0')
c= tensor(4.1556e+08, device='cuda:0')
c= tensor(4.1557e+08, device='cuda:0')
c= tensor(4.1557e+08, device='cuda:0')
c= tensor(4.1557e+08, device='cuda:0')
c= tensor(4.1557e+08, device='cuda:0')
c= tensor(4.1557e+08, device='cuda:0')
c= tensor(4.1558e+08, device='cuda:0')
c= tensor(4.1558e+08, device='cuda:0')
c= tensor(4.1559e+08, device='cuda:0')
c= tensor(4.1559e+08, device='cuda:0')
c= tensor(4.1560e+08, device='cuda:0')
c= tensor(4.1560e+08, device='cuda:0')
c= tensor(4.1560e+08, device='cuda:0')
c= tensor(4.1560e+08, device='cuda:0')
c= tensor(4.1560e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1562e+08, device='cuda:0')
c= tensor(4.1562e+08, device='cuda:0')
c= tensor(4.1562e+08, device='cuda:0')
c= tensor(4.1563e+08, device='cuda:0')
c= tensor(4.1573e+08, device='cuda:0')
c= tensor(4.1574e+08, device='cuda:0')
c= tensor(4.1574e+08, device='cuda:0')
c= tensor(4.1575e+08, device='cuda:0')
c= tensor(4.1585e+08, device='cuda:0')
c= tensor(4.1732e+08, device='cuda:0')
c= tensor(4.1732e+08, device='cuda:0')
c= tensor(4.1738e+08, device='cuda:0')
c= tensor(4.1758e+08, device='cuda:0')
c= tensor(4.1764e+08, device='cuda:0')
c= tensor(4.4533e+08, device='cuda:0')
c= tensor(4.4533e+08, device='cuda:0')
c= tensor(4.4534e+08, device='cuda:0')
c= tensor(4.5180e+08, device='cuda:0')
c= tensor(5.0712e+08, device='cuda:0')
c= tensor(5.0712e+08, device='cuda:0')
c= tensor(5.0713e+08, device='cuda:0')
c= tensor(5.0714e+08, device='cuda:0')
c= tensor(5.0725e+08, device='cuda:0')
c= tensor(5.0732e+08, device='cuda:0')
c= tensor(5.0753e+08, device='cuda:0')
c= tensor(5.0767e+08, device='cuda:0')
c= tensor(5.0770e+08, device='cuda:0')
c= tensor(5.0770e+08, device='cuda:0')
c= tensor(5.1117e+08, device='cuda:0')
c= tensor(5.1121e+08, device='cuda:0')
c= tensor(5.1122e+08, device='cuda:0')
c= tensor(5.1173e+08, device='cuda:0')
c= tensor(5.1175e+08, device='cuda:0')
c= tensor(5.1274e+08, device='cuda:0')
c= tensor(5.1399e+08, device='cuda:0')
c= tensor(5.1400e+08, device='cuda:0')
c= tensor(5.1401e+08, device='cuda:0')
c= tensor(5.1401e+08, device='cuda:0')
c= tensor(5.1419e+08, device='cuda:0')
c= tensor(5.1548e+08, device='cuda:0')
c= tensor(5.1549e+08, device='cuda:0')
c= tensor(5.1642e+08, device='cuda:0')
c= tensor(5.1642e+08, device='cuda:0')
c= tensor(5.1642e+08, device='cuda:0')
c= tensor(5.1643e+08, device='cuda:0')
c= tensor(5.2067e+08, device='cuda:0')
c= tensor(5.2086e+08, device='cuda:0')
c= tensor(5.2086e+08, device='cuda:0')
c= tensor(5.2715e+08, device='cuda:0')
c= tensor(5.2720e+08, device='cuda:0')
c= tensor(5.2727e+08, device='cuda:0')
c= tensor(5.2793e+08, device='cuda:0')
c= tensor(5.2793e+08, device='cuda:0')
c= tensor(5.2810e+08, device='cuda:0')
c= tensor(5.2821e+08, device='cuda:0')
c= tensor(5.3018e+08, device='cuda:0')
c= tensor(5.3030e+08, device='cuda:0')
c= tensor(5.3041e+08, device='cuda:0')
c= tensor(5.3065e+08, device='cuda:0')
c= tensor(5.3066e+08, device='cuda:0')
c= tensor(5.3069e+08, device='cuda:0')
c= tensor(5.3082e+08, device='cuda:0')
c= tensor(5.3113e+08, device='cuda:0')
c= tensor(5.3510e+08, device='cuda:0')
c= tensor(5.3546e+08, device='cuda:0')
c= tensor(5.3549e+08, device='cuda:0')
c= tensor(5.3549e+08, device='cuda:0')
c= tensor(5.3626e+08, device='cuda:0')
c= tensor(5.3631e+08, device='cuda:0')
c= tensor(5.3631e+08, device='cuda:0')
c= tensor(5.3631e+08, device='cuda:0')
c= tensor(5.3633e+08, device='cuda:0')
c= tensor(5.3634e+08, device='cuda:0')
c= tensor(6.0569e+08, device='cuda:0')
c= tensor(6.0570e+08, device='cuda:0')
c= tensor(6.0582e+08, device='cuda:0')
c= tensor(6.0632e+08, device='cuda:0')
c= tensor(6.0632e+08, device='cuda:0')
c= tensor(6.0650e+08, device='cuda:0')
c= tensor(6.0652e+08, device='cuda:0')
c= tensor(6.0717e+08, device='cuda:0')
c= tensor(6.0834e+08, device='cuda:0')
c= tensor(6.0834e+08, device='cuda:0')
c= tensor(6.0852e+08, device='cuda:0')
c= tensor(6.1036e+08, device='cuda:0')
c= tensor(6.1509e+08, device='cuda:0')
c= tensor(6.1593e+08, device='cuda:0')
c= tensor(6.1593e+08, device='cuda:0')
c= tensor(6.1596e+08, device='cuda:0')
c= tensor(6.2273e+08, device='cuda:0')
c= tensor(6.2341e+08, device='cuda:0')
c= tensor(6.2345e+08, device='cuda:0')
c= tensor(6.2345e+08, device='cuda:0')
c= tensor(6.2595e+08, device='cuda:0')
c= tensor(6.2751e+08, device='cuda:0')
c= tensor(6.2778e+08, device='cuda:0')
c= tensor(6.2779e+08, device='cuda:0')
c= tensor(6.2788e+08, device='cuda:0')
c= tensor(6.2788e+08, device='cuda:0')
c= tensor(6.2789e+08, device='cuda:0')
c= tensor(6.2791e+08, device='cuda:0')
c= tensor(6.2792e+08, device='cuda:0')
c= tensor(6.2815e+08, device='cuda:0')
c= tensor(6.2826e+08, device='cuda:0')
c= tensor(6.2835e+08, device='cuda:0')
c= tensor(6.2870e+08, device='cuda:0')
c= tensor(6.2871e+08, device='cuda:0')
c= tensor(6.7436e+08, device='cuda:0')
c= tensor(6.7438e+08, device='cuda:0')
c= tensor(6.7550e+08, device='cuda:0')
c= tensor(6.7550e+08, device='cuda:0')
c= tensor(6.7551e+08, device='cuda:0')
c= tensor(6.7551e+08, device='cuda:0')
c= tensor(6.7567e+08, device='cuda:0')
c= tensor(6.7567e+08, device='cuda:0')
c= tensor(6.7567e+08, device='cuda:0')
c= tensor(6.7568e+08, device='cuda:0')
c= tensor(6.7568e+08, device='cuda:0')
c= tensor(6.8647e+08, device='cuda:0')
c= tensor(6.8752e+08, device='cuda:0')
c= tensor(6.8753e+08, device='cuda:0')
c= tensor(6.8772e+08, device='cuda:0')
c= tensor(6.8772e+08, device='cuda:0')
c= tensor(6.8773e+08, device='cuda:0')
c= tensor(6.8774e+08, device='cuda:0')
c= tensor(6.8790e+08, device='cuda:0')
c= tensor(6.8790e+08, device='cuda:0')
c= tensor(6.8792e+08, device='cuda:0')
c= tensor(6.8793e+08, device='cuda:0')
c= tensor(6.8794e+08, device='cuda:0')
c= tensor(6.8794e+08, device='cuda:0')
c= tensor(6.8802e+08, device='cuda:0')
c= tensor(6.8802e+08, device='cuda:0')
c= tensor(6.9293e+08, device='cuda:0')
c= tensor(6.9295e+08, device='cuda:0')
c= tensor(6.9303e+08, device='cuda:0')
c= tensor(6.9303e+08, device='cuda:0')
c= tensor(6.9329e+08, device='cuda:0')
c= tensor(6.9330e+08, device='cuda:0')
c= tensor(7.9707e+08, device='cuda:0')
c= tensor(8.1618e+08, device='cuda:0')
c= tensor(8.1661e+08, device='cuda:0')
c= tensor(8.1664e+08, device='cuda:0')
c= tensor(8.1664e+08, device='cuda:0')
c= tensor(8.1669e+08, device='cuda:0')
c= tensor(8.1669e+08, device='cuda:0')
c= tensor(8.1690e+08, device='cuda:0')
c= tensor(8.1690e+08, device='cuda:0')
c= tensor(8.1911e+08, device='cuda:0')
c= tensor(8.2027e+08, device='cuda:0')
c= tensor(8.2042e+08, device='cuda:0')
c= tensor(8.2045e+08, device='cuda:0')
c= tensor(8.2046e+08, device='cuda:0')
c= tensor(8.2046e+08, device='cuda:0')
c= tensor(8.2046e+08, device='cuda:0')
c= tensor(8.2142e+08, device='cuda:0')
c= tensor(8.2149e+08, device='cuda:0')
c= tensor(8.2153e+08, device='cuda:0')
c= tensor(8.2267e+08, device='cuda:0')
c= tensor(8.2268e+08, device='cuda:0')
c= tensor(8.2268e+08, device='cuda:0')
c= tensor(8.2269e+08, device='cuda:0')
c= tensor(8.2327e+08, device='cuda:0')
c= tensor(8.2401e+08, device='cuda:0')
c= tensor(8.2884e+08, device='cuda:0')
c= tensor(8.2959e+08, device='cuda:0')
c= tensor(8.2960e+08, device='cuda:0')
c= tensor(8.2995e+08, device='cuda:0')
c= tensor(8.3183e+08, device='cuda:0')
c= tensor(8.3217e+08, device='cuda:0')
c= tensor(8.3219e+08, device='cuda:0')
c= tensor(8.4468e+08, device='cuda:0')
c= tensor(8.4807e+08, device='cuda:0')
c= tensor(8.4833e+08, device='cuda:0')
c= tensor(8.4914e+08, device='cuda:0')
c= tensor(8.4914e+08, device='cuda:0')
c= tensor(8.4927e+08, device='cuda:0')
c= tensor(8.4928e+08, device='cuda:0')
c= tensor(8.4928e+08, device='cuda:0')
c= tensor(8.5209e+08, device='cuda:0')
c= tensor(8.5345e+08, device='cuda:0')
c= tensor(8.5499e+08, device='cuda:0')
c= tensor(8.5506e+08, device='cuda:0')
c= tensor(8.5539e+08, device='cuda:0')
c= tensor(8.5547e+08, device='cuda:0')
c= tensor(8.5554e+08, device='cuda:0')
c= tensor(8.5554e+08, device='cuda:0')
c= tensor(8.5556e+08, device='cuda:0')
c= tensor(8.5684e+08, device='cuda:0')
c= tensor(8.5702e+08, device='cuda:0')
c= tensor(8.5702e+08, device='cuda:0')
c= tensor(8.5703e+08, device='cuda:0')
c= tensor(8.7980e+08, device='cuda:0')
c= tensor(8.7987e+08, device='cuda:0')
c= tensor(8.7992e+08, device='cuda:0')
c= tensor(8.8000e+08, device='cuda:0')
c= tensor(8.8004e+08, device='cuda:0')
c= tensor(8.8004e+08, device='cuda:0')
c= tensor(8.8005e+08, device='cuda:0')
c= tensor(8.8019e+08, device='cuda:0')
c= tensor(8.8031e+08, device='cuda:0')
c= tensor(8.8031e+08, device='cuda:0')
c= tensor(8.8329e+08, device='cuda:0')
c= tensor(8.8330e+08, device='cuda:0')
c= tensor(8.8333e+08, device='cuda:0')
c= tensor(8.8334e+08, device='cuda:0')
c= tensor(8.8421e+08, device='cuda:0')
c= tensor(8.8421e+08, device='cuda:0')
c= tensor(8.8424e+08, device='cuda:0')
c= tensor(8.8435e+08, device='cuda:0')
c= tensor(8.8450e+08, device='cuda:0')
c= tensor(8.8456e+08, device='cuda:0')
c= tensor(9.2724e+08, device='cuda:0')
c= tensor(9.2726e+08, device='cuda:0')
c= tensor(9.2726e+08, device='cuda:0')
c= tensor(9.2833e+08, device='cuda:0')
c= tensor(9.2837e+08, device='cuda:0')
c= tensor(9.2895e+08, device='cuda:0')
c= tensor(9.2897e+08, device='cuda:0')
c= tensor(9.2990e+08, device='cuda:0')
c= tensor(9.3438e+08, device='cuda:0')
c= tensor(9.3444e+08, device='cuda:0')
c= tensor(9.4767e+08, device='cuda:0')
c= tensor(9.4768e+08, device='cuda:0')
c= tensor(9.4781e+08, device='cuda:0')
c= tensor(9.4783e+08, device='cuda:0')
c= tensor(9.5745e+08, device='cuda:0')
c= tensor(9.5745e+08, device='cuda:0')
c= tensor(9.5745e+08, device='cuda:0')
c= tensor(9.5745e+08, device='cuda:0')
c= tensor(9.5755e+08, device='cuda:0')
c= tensor(9.5873e+08, device='cuda:0')
c= tensor(9.5882e+08, device='cuda:0')
c= tensor(9.5883e+08, device='cuda:0')
c= tensor(9.5884e+08, device='cuda:0')
c= tensor(9.5886e+08, device='cuda:0')
c= tensor(9.6041e+08, device='cuda:0')
c= tensor(9.6063e+08, device='cuda:0')
c= tensor(9.6533e+08, device='cuda:0')
c= tensor(9.6536e+08, device='cuda:0')
c= tensor(9.6540e+08, device='cuda:0')
c= tensor(9.6540e+08, device='cuda:0')
c= tensor(9.6540e+08, device='cuda:0')
c= tensor(9.8558e+08, device='cuda:0')
c= tensor(9.8559e+08, device='cuda:0')
c= tensor(9.8559e+08, device='cuda:0')
c= tensor(9.8679e+08, device='cuda:0')
c= tensor(9.8720e+08, device='cuda:0')
c= tensor(9.8720e+08, device='cuda:0')
c= tensor(9.8720e+08, device='cuda:0')
c= tensor(9.8725e+08, device='cuda:0')
c= tensor(9.8728e+08, device='cuda:0')
c= tensor(9.8730e+08, device='cuda:0')
c= tensor(9.8741e+08, device='cuda:0')
c= tensor(9.8747e+08, device='cuda:0')
c= tensor(9.8749e+08, device='cuda:0')
c= tensor(1.0009e+09, device='cuda:0')
c= tensor(1.0012e+09, device='cuda:0')
c= tensor(1.0012e+09, device='cuda:0')
c= tensor(1.0013e+09, device='cuda:0')
c= tensor(1.0013e+09, device='cuda:0')
c= tensor(1.0014e+09, device='cuda:0')
c= tensor(1.0014e+09, device='cuda:0')
c= tensor(1.0014e+09, device='cuda:0')
c= tensor(1.0028e+09, device='cuda:0')
c= tensor(1.0030e+09, device='cuda:0')
c= tensor(1.0030e+09, device='cuda:0')
c= tensor(1.0030e+09, device='cuda:0')
c= tensor(1.0031e+09, device='cuda:0')
c= tensor(1.0032e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0045e+09, device='cuda:0')
c= tensor(1.0045e+09, device='cuda:0')
c= tensor(1.0045e+09, device='cuda:0')
c= tensor(1.0045e+09, device='cuda:0')
c= tensor(1.0047e+09, device='cuda:0')
c= tensor(1.0309e+09, device='cuda:0')
c= tensor(1.0311e+09, device='cuda:0')
c= tensor(1.0311e+09, device='cuda:0')
c= tensor(1.0311e+09, device='cuda:0')
c= tensor(1.0338e+09, device='cuda:0')
c= tensor(1.0338e+09, device='cuda:0')
c= tensor(1.0338e+09, device='cuda:0')
c= tensor(1.0339e+09, device='cuda:0')
c= tensor(1.0340e+09, device='cuda:0')
c= tensor(1.0343e+09, device='cuda:0')
c= tensor(1.0344e+09, device='cuda:0')
c= tensor(1.0345e+09, device='cuda:0')
c= tensor(1.0345e+09, device='cuda:0')
c= tensor(1.0345e+09, device='cuda:0')
c= tensor(1.0347e+09, device='cuda:0')
c= tensor(1.0352e+09, device='cuda:0')
c= tensor(1.0354e+09, device='cuda:0')
c= tensor(1.0356e+09, device='cuda:0')
c= tensor(1.0377e+09, device='cuda:0')
c= tensor(1.0377e+09, device='cuda:0')
c= tensor(1.0377e+09, device='cuda:0')
c= tensor(1.0378e+09, device='cuda:0')
c= tensor(1.0386e+09, device='cuda:0')
c= tensor(1.0387e+09, device='cuda:0')
c= tensor(1.0387e+09, device='cuda:0')
c= tensor(1.0387e+09, device='cuda:0')
c= tensor(1.0389e+09, device='cuda:0')
c= tensor(1.0389e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0397e+09, device='cuda:0')
c= tensor(1.0401e+09, device='cuda:0')
c= tensor(1.0436e+09, device='cuda:0')
c= tensor(1.0611e+09, device='cuda:0')
c= tensor(1.0612e+09, device='cuda:0')
c= tensor(1.0612e+09, device='cuda:0')
c= tensor(1.0612e+09, device='cuda:0')
c= tensor(1.0620e+09, device='cuda:0')
c= tensor(1.0620e+09, device='cuda:0')
c= tensor(1.0621e+09, device='cuda:0')
c= tensor(1.0621e+09, device='cuda:0')
c= tensor(1.0722e+09, device='cuda:0')
c= tensor(1.0735e+09, device='cuda:0')
c= tensor(1.0735e+09, device='cuda:0')
c= tensor(1.0754e+09, device='cuda:0')
c= tensor(1.0755e+09, device='cuda:0')
c= tensor(1.0756e+09, device='cuda:0')
c= tensor(1.0882e+09, device='cuda:0')
c= tensor(1.0917e+09, device='cuda:0')
c= tensor(1.0951e+09, device='cuda:0')
c= tensor(1.0951e+09, device='cuda:0')
c= tensor(1.0978e+09, device='cuda:0')
c= tensor(1.0978e+09, device='cuda:0')
c= tensor(1.0979e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1246e+09, device='cuda:0')
c= tensor(1.1246e+09, device='cuda:0')
c= tensor(1.1246e+09, device='cuda:0')
c= tensor(1.1303e+09, device='cuda:0')
c= tensor(1.1303e+09, device='cuda:0')
c= tensor(1.2030e+09, device='cuda:0')
c= tensor(1.2031e+09, device='cuda:0')
c= tensor(1.2032e+09, device='cuda:0')
c= tensor(1.2044e+09, device='cuda:0')
c= tensor(1.2050e+09, device='cuda:0')
c= tensor(1.2099e+09, device='cuda:0')
c= tensor(1.2099e+09, device='cuda:0')
c= tensor(1.2288e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2344e+09, device='cuda:0')
c= tensor(1.4211e+09, device='cuda:0')
c= tensor(1.4211e+09, device='cuda:0')
c= tensor(1.4216e+09, device='cuda:0')
c= tensor(1.4216e+09, device='cuda:0')
c= tensor(1.4218e+09, device='cuda:0')
c= tensor(1.4218e+09, device='cuda:0')
c= tensor(1.4221e+09, device='cuda:0')
c= tensor(1.4221e+09, device='cuda:0')
c= tensor(1.4455e+09, device='cuda:0')
c= tensor(1.4455e+09, device='cuda:0')
c= tensor(1.4456e+09, device='cuda:0')
c= tensor(1.4457e+09, device='cuda:0')
c= tensor(1.4458e+09, device='cuda:0')
c= tensor(1.4564e+09, device='cuda:0')
c= tensor(1.4565e+09, device='cuda:0')
c= tensor(1.4565e+09, device='cuda:0')
c= tensor(1.4567e+09, device='cuda:0')
c= tensor(1.4567e+09, device='cuda:0')
c= tensor(1.4567e+09, device='cuda:0')
c= tensor(1.4587e+09, device='cuda:0')
c= tensor(1.5560e+09, device='cuda:0')
c= tensor(1.5561e+09, device='cuda:0')
c= tensor(1.5563e+09, device='cuda:0')
c= tensor(1.5570e+09, device='cuda:0')
c= tensor(1.5570e+09, device='cuda:0')
c= tensor(1.5575e+09, device='cuda:0')
c= tensor(1.5575e+09, device='cuda:0')
c= tensor(1.5636e+09, device='cuda:0')
c= tensor(1.5643e+09, device='cuda:0')
c= tensor(1.5649e+09, device='cuda:0')
c= tensor(1.5650e+09, device='cuda:0')
c= tensor(1.5650e+09, device='cuda:0')
c= tensor(1.5650e+09, device='cuda:0')
c= tensor(1.6374e+09, device='cuda:0')
c= tensor(1.6379e+09, device='cuda:0')
c= tensor(1.6379e+09, device='cuda:0')
c= tensor(1.6379e+09, device='cuda:0')
c= tensor(1.6379e+09, device='cuda:0')
c= tensor(1.6385e+09, device='cuda:0')
c= tensor(1.6391e+09, device='cuda:0')
c= tensor(1.6463e+09, device='cuda:0')
c= tensor(1.6464e+09, device='cuda:0')
c= tensor(1.6464e+09, device='cuda:0')
c= tensor(1.6465e+09, device='cuda:0')
c= tensor(1.6467e+09, device='cuda:0')
c= tensor(1.6468e+09, device='cuda:0')
c= tensor(1.6473e+09, device='cuda:0')
c= tensor(1.6473e+09, device='cuda:0')
c= tensor(1.6474e+09, device='cuda:0')
c= tensor(1.6474e+09, device='cuda:0')
c= tensor(1.6474e+09, device='cuda:0')
c= tensor(1.6536e+09, device='cuda:0')
c= tensor(1.6538e+09, device='cuda:0')
c= tensor(1.6542e+09, device='cuda:0')
c= tensor(1.6542e+09, device='cuda:0')
c= tensor(1.6542e+09, device='cuda:0')
c= tensor(1.6548e+09, device='cuda:0')
c= tensor(1.6558e+09, device='cuda:0')
c= tensor(1.6560e+09, device='cuda:0')
c= tensor(1.6560e+09, device='cuda:0')
c= tensor(1.6560e+09, device='cuda:0')
c= tensor(1.6560e+09, device='cuda:0')
c= tensor(1.6561e+09, device='cuda:0')
c= tensor(1.6572e+09, device='cuda:0')
c= tensor(1.6572e+09, device='cuda:0')
c= tensor(1.6572e+09, device='cuda:0')
c= tensor(1.6572e+09, device='cuda:0')
c= tensor(1.6572e+09, device='cuda:0')
c= tensor(1.6588e+09, device='cuda:0')
c= tensor(1.6588e+09, device='cuda:0')
c= tensor(1.6589e+09, device='cuda:0')
c= tensor(1.6599e+09, device='cuda:0')
c= tensor(1.6599e+09, device='cuda:0')
c= tensor(1.6605e+09, device='cuda:0')
c= tensor(1.6616e+09, device='cuda:0')
c= tensor(1.6616e+09, device='cuda:0')
c= tensor(1.6632e+09, device='cuda:0')
c= tensor(1.6634e+09, device='cuda:0')
c= tensor(1.6722e+09, device='cuda:0')
c= tensor(1.6738e+09, device='cuda:0')
c= tensor(1.6739e+09, device='cuda:0')
c= tensor(1.6744e+09, device='cuda:0')
c= tensor(1.6744e+09, device='cuda:0')
c= tensor(1.6744e+09, device='cuda:0')
c= tensor(1.6744e+09, device='cuda:0')
c= tensor(1.6745e+09, device='cuda:0')
c= tensor(1.6752e+09, device='cuda:0')
c= tensor(1.6754e+09, device='cuda:0')
c= tensor(1.6754e+09, device='cuda:0')
c= tensor(1.6756e+09, device='cuda:0')
c= tensor(1.6757e+09, device='cuda:0')
c= tensor(1.6808e+09, device='cuda:0')
c= tensor(1.6809e+09, device='cuda:0')
c= tensor(1.6809e+09, device='cuda:0')
c= tensor(1.6812e+09, device='cuda:0')
c= tensor(1.6819e+09, device='cuda:0')
c= tensor(1.6819e+09, device='cuda:0')
c= tensor(1.6825e+09, device='cuda:0')
c= tensor(1.6833e+09, device='cuda:0')
memory (bytes)
3290185728
time for making loss 2 is 14.5181884765625
p0 True
it  0 : 334253568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3290451968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  3% |
memory (bytes)
3291185152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  4% |
error is  25114968000.0
relative error loss 14.920371
shape of L is 
torch.Size([])
memory (bytes)
3557064704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  3% |  4% |
memory (bytes)
3557146624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  4% |
error is  25114636000.0
relative error loss 14.920175
shape of L is 
torch.Size([])
memory (bytes)
3559030784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  4% |
memory (bytes)
3559030784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  4% |
error is  25113130000.0
relative error loss 14.91928
shape of L is 
torch.Size([])
memory (bytes)
3561095168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  4% |
memory (bytes)
3561095168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 55% |  4% |
error is  25103470000.0
relative error loss 14.913541
shape of L is 
torch.Size([])
memory (bytes)
3563196416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  4% |
memory (bytes)
3563200512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 58% |  4% |
error is  25051722000.0
relative error loss 14.882798
shape of L is 
torch.Size([])
memory (bytes)
3565387776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  4% |
memory (bytes)
3565387776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 71% |  4% |
error is  24768380000.0
relative error loss 14.71447
shape of L is 
torch.Size([])
memory (bytes)
3567542272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  4% |
memory (bytes)
3567542272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 76% |  4% |
error is  21825208000.0
relative error loss 12.9659815
shape of L is 
torch.Size([])
memory (bytes)
3569651712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  4% |
memory (bytes)
3569651712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  4% |
error is  5869472000.0
relative error loss 3.4869523
shape of L is 
torch.Size([])
memory (bytes)
3571744768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  4% |
memory (bytes)
3571769344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  3223527000.0
relative error loss 1.915042
shape of L is 
torch.Size([])
memory (bytes)
3573899264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3573899264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  2106225500.0
relative error loss 1.2512724
time to take a step is 322.21982979774475
it  1 : 503813120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3576020992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  4% |
memory (bytes)
3576020992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  4% |
error is  2106225500.0
relative error loss 1.2512724
shape of L is 
torch.Size([])
memory (bytes)
3577991168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  4% |
memory (bytes)
3577991168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 64% |  5% |
error is  1797700000.0
relative error loss 1.0679827
shape of L is 
torch.Size([])
memory (bytes)
3580243968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3580243968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 53% |  5% |
error is  1539225500.0
relative error loss 0.91442746
shape of L is 
torch.Size([])
memory (bytes)
3582361600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3582361600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  1704494800.0
relative error loss 1.0126112
shape of L is 
torch.Size([])
memory (bytes)
3584446464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3584450560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 54% |  5% |
error is  1428972000.0
relative error loss 0.84892774
shape of L is 
torch.Size([])
memory (bytes)
3586367488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3586609152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  1250073100.0
relative error loss 0.742647
shape of L is 
torch.Size([])
memory (bytes)
3588730880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3588730880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 64% |  5% |
error is  1117872400.0
relative error loss 0.6641088
shape of L is 
torch.Size([])
memory (bytes)
3590750208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3590852608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  1167986000.0
relative error loss 0.69388044
shape of L is 
torch.Size([])
memory (bytes)
3592785920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3592888320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 65% |  5% |
error is  1030814500.0
relative error loss 0.6123892
shape of L is 
torch.Size([])
memory (bytes)
3595161600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3595161600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 53% |  5% |
error is  922044540.0
relative error loss 0.54777086
time to take a step is 307.7289996147156
it  2 : 503813120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3597168640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  4% |
memory (bytes)
3597168640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 74% |  4% |
error is  922044540.0
relative error loss 0.54777086
shape of L is 
torch.Size([])
memory (bytes)
3599237120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  4% |
memory (bytes)
3599237120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  816088500.0
relative error loss 0.48482418
shape of L is 
torch.Size([])
memory (bytes)
3601510400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3601510400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  5% |
error is  726319900.0
relative error loss 0.43149418
shape of L is 
torch.Size([])
memory (bytes)
3603390464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3603390464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 75% |  5% |
error is  640246400.0
relative error loss 0.3803594
shape of L is 
torch.Size([])
memory (bytes)
3605729280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3605729280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 55% |  5% |
error is  684544500.0
relative error loss 0.40667614
shape of L is 
torch.Size([])
memory (bytes)
3607801856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3607801856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 60% |  5% |
error is  561147140.0
relative error loss 0.33336788
shape of L is 
torch.Size([])
memory (bytes)
3609927680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3609927680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 54% |  5% |
error is  493447420.0
relative error loss 0.29314864
shape of L is 
torch.Size([])
memory (bytes)
3612114944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  5% |
memory (bytes)
3612114944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 73% |  5% |
error is  433539070.0
relative error loss 0.2575581
shape of L is 
torch.Size([])
memory (bytes)
3614208000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
3614208000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 79% |  5% |
error is  385847420.0
relative error loss 0.22922534
shape of L is 
torch.Size([])
memory (bytes)
3616260096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3616260096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 69% |  5% |
error is  336306560.0
relative error loss 0.19979395
time to take a step is 305.46984124183655
it  3 : 503813120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3618533376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  4% |
memory (bytes)
3618537472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 64% |  4% |
error is  336306560.0
relative error loss 0.19979395
shape of L is 
torch.Size([])
memory (bytes)
3620671488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  4% |
memory (bytes)
3620671488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 53% |  5% |
error is  300036100.0
relative error loss 0.17824629
shape of L is 
torch.Size([])
memory (bytes)
3622846464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3622846464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  270471940.0
relative error loss 0.16068274
shape of L is 
torch.Size([])
memory (bytes)
3624992768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
3624996864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 72% |  5% |
error is  247546110.0
relative error loss 0.1470629
shape of L is 
torch.Size([])
memory (bytes)
3626991616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3626991616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  221574530.0
relative error loss 0.13163362
shape of L is 
torch.Size([])
memory (bytes)
3629223936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3629223936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  200773890.0
relative error loss 0.11927632
shape of L is 
torch.Size([])
memory (bytes)
3631443968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
3631443968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 74% |  5% |
error is  189023870.0
relative error loss 0.112295836
shape of L is 
torch.Size([])
memory (bytes)
3633573888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3633573888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  178499200.0
relative error loss 0.10604331
shape of L is 
torch.Size([])
memory (bytes)
3635744768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3635744768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  165274620.0
relative error loss 0.09818682
shape of L is 
torch.Size([])
memory (bytes)
3637788672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
3637788672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 80% |  5% |
error is  154963970.0
relative error loss 0.09206143
time to take a step is 305.6482195854187
c= tensor(1616.1284, device='cuda:0')
c= tensor(16197.9922, device='cuda:0')
c= tensor(16684.3867, device='cuda:0')
c= tensor(18169.0352, device='cuda:0')
c= tensor(245313.7969, device='cuda:0')
c= tensor(376486.6562, device='cuda:0')
c= tensor(513099.7500, device='cuda:0')
c= tensor(756525.3750, device='cuda:0')
c= tensor(762042.1250, device='cuda:0')
c= tensor(1779731.3750, device='cuda:0')
c= tensor(1783141.2500, device='cuda:0')
c= tensor(3527667.5000, device='cuda:0')
c= tensor(3566064., device='cuda:0')
c= tensor(4100457.5000, device='cuda:0')
c= tensor(4155134., device='cuda:0')
c= tensor(4593210.5000, device='cuda:0')
c= tensor(4704108., device='cuda:0')
c= tensor(4823494., device='cuda:0')
c= tensor(45332376., device='cuda:0')
c= tensor(45556624., device='cuda:0')
c= tensor(45728924., device='cuda:0')
c= tensor(67804984., device='cuda:0')
c= tensor(67863096., device='cuda:0')
c= tensor(67872040., device='cuda:0')
c= tensor(68109160., device='cuda:0')
c= tensor(68326096., device='cuda:0')
c= tensor(68326992., device='cuda:0')
c= tensor(68332632., device='cuda:0')
c= tensor(68368040., device='cuda:0')
c= tensor(4.1275e+08, device='cuda:0')
c= tensor(4.1275e+08, device='cuda:0')
c= tensor(4.1482e+08, device='cuda:0')
c= tensor(4.1482e+08, device='cuda:0')
c= tensor(4.1483e+08, device='cuda:0')
c= tensor(4.1487e+08, device='cuda:0')
c= tensor(4.1494e+08, device='cuda:0')
c= tensor(4.1512e+08, device='cuda:0')
c= tensor(4.1512e+08, device='cuda:0')
c= tensor(4.1512e+08, device='cuda:0')
c= tensor(4.1513e+08, device='cuda:0')
c= tensor(4.1513e+08, device='cuda:0')
c= tensor(4.1513e+08, device='cuda:0')
c= tensor(4.1513e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1515e+08, device='cuda:0')
c= tensor(4.1515e+08, device='cuda:0')
c= tensor(4.1515e+08, device='cuda:0')
c= tensor(4.1515e+08, device='cuda:0')
c= tensor(4.1515e+08, device='cuda:0')
c= tensor(4.1516e+08, device='cuda:0')
c= tensor(4.1516e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1517e+08, device='cuda:0')
c= tensor(4.1519e+08, device='cuda:0')
c= tensor(4.1519e+08, device='cuda:0')
c= tensor(4.1519e+08, device='cuda:0')
c= tensor(4.1520e+08, device='cuda:0')
c= tensor(4.1521e+08, device='cuda:0')
c= tensor(4.1521e+08, device='cuda:0')
c= tensor(4.1521e+08, device='cuda:0')
c= tensor(4.1523e+08, device='cuda:0')
c= tensor(4.1523e+08, device='cuda:0')
c= tensor(4.1523e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1524e+08, device='cuda:0')
c= tensor(4.1525e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1526e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1527e+08, device='cuda:0')
c= tensor(4.1531e+08, device='cuda:0')
c= tensor(4.1531e+08, device='cuda:0')
c= tensor(4.1532e+08, device='cuda:0')
c= tensor(4.1532e+08, device='cuda:0')
c= tensor(4.1532e+08, device='cuda:0')
c= tensor(4.1532e+08, device='cuda:0')
c= tensor(4.1532e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1533e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1534e+08, device='cuda:0')
c= tensor(4.1535e+08, device='cuda:0')
c= tensor(4.1535e+08, device='cuda:0')
c= tensor(4.1539e+08, device='cuda:0')
c= tensor(4.1539e+08, device='cuda:0')
c= tensor(4.1539e+08, device='cuda:0')
c= tensor(4.1539e+08, device='cuda:0')
c= tensor(4.1539e+08, device='cuda:0')
c= tensor(4.1540e+08, device='cuda:0')
c= tensor(4.1540e+08, device='cuda:0')
c= tensor(4.1540e+08, device='cuda:0')
c= tensor(4.1541e+08, device='cuda:0')
c= tensor(4.1541e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1544e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1546e+08, device='cuda:0')
c= tensor(4.1546e+08, device='cuda:0')
c= tensor(4.1547e+08, device='cuda:0')
c= tensor(4.1547e+08, device='cuda:0')
c= tensor(4.1547e+08, device='cuda:0')
c= tensor(4.1547e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1548e+08, device='cuda:0')
c= tensor(4.1549e+08, device='cuda:0')
c= tensor(4.1549e+08, device='cuda:0')
c= tensor(4.1549e+08, device='cuda:0')
c= tensor(4.1549e+08, device='cuda:0')
c= tensor(4.1549e+08, device='cuda:0')
c= tensor(4.1550e+08, device='cuda:0')
c= tensor(4.1550e+08, device='cuda:0')
c= tensor(4.1550e+08, device='cuda:0')
c= tensor(4.1550e+08, device='cuda:0')
c= tensor(4.1550e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1551e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1552e+08, device='cuda:0')
c= tensor(4.1553e+08, device='cuda:0')
c= tensor(4.1553e+08, device='cuda:0')
c= tensor(4.1553e+08, device='cuda:0')
c= tensor(4.1553e+08, device='cuda:0')
c= tensor(4.1553e+08, device='cuda:0')
c= tensor(4.1554e+08, device='cuda:0')
c= tensor(4.1554e+08, device='cuda:0')
c= tensor(4.1554e+08, device='cuda:0')
c= tensor(4.1554e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1555e+08, device='cuda:0')
c= tensor(4.1556e+08, device='cuda:0')
c= tensor(4.1556e+08, device='cuda:0')
c= tensor(4.1556e+08, device='cuda:0')
c= tensor(4.1556e+08, device='cuda:0')
c= tensor(4.1557e+08, device='cuda:0')
c= tensor(4.1557e+08, device='cuda:0')
c= tensor(4.1557e+08, device='cuda:0')
c= tensor(4.1557e+08, device='cuda:0')
c= tensor(4.1557e+08, device='cuda:0')
c= tensor(4.1558e+08, device='cuda:0')
c= tensor(4.1558e+08, device='cuda:0')
c= tensor(4.1559e+08, device='cuda:0')
c= tensor(4.1559e+08, device='cuda:0')
c= tensor(4.1560e+08, device='cuda:0')
c= tensor(4.1560e+08, device='cuda:0')
c= tensor(4.1560e+08, device='cuda:0')
c= tensor(4.1560e+08, device='cuda:0')
c= tensor(4.1560e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1561e+08, device='cuda:0')
c= tensor(4.1562e+08, device='cuda:0')
c= tensor(4.1562e+08, device='cuda:0')
c= tensor(4.1562e+08, device='cuda:0')
c= tensor(4.1563e+08, device='cuda:0')
c= tensor(4.1573e+08, device='cuda:0')
c= tensor(4.1574e+08, device='cuda:0')
c= tensor(4.1574e+08, device='cuda:0')
c= tensor(4.1575e+08, device='cuda:0')
c= tensor(4.1585e+08, device='cuda:0')
c= tensor(4.1732e+08, device='cuda:0')
c= tensor(4.1732e+08, device='cuda:0')
c= tensor(4.1738e+08, device='cuda:0')
c= tensor(4.1758e+08, device='cuda:0')
c= tensor(4.1764e+08, device='cuda:0')
c= tensor(4.4533e+08, device='cuda:0')
c= tensor(4.4533e+08, device='cuda:0')
c= tensor(4.4534e+08, device='cuda:0')
c= tensor(4.5180e+08, device='cuda:0')
c= tensor(5.0712e+08, device='cuda:0')
c= tensor(5.0712e+08, device='cuda:0')
c= tensor(5.0713e+08, device='cuda:0')
c= tensor(5.0714e+08, device='cuda:0')
c= tensor(5.0725e+08, device='cuda:0')
c= tensor(5.0732e+08, device='cuda:0')
c= tensor(5.0753e+08, device='cuda:0')
c= tensor(5.0767e+08, device='cuda:0')
c= tensor(5.0770e+08, device='cuda:0')
c= tensor(5.0770e+08, device='cuda:0')
c= tensor(5.1117e+08, device='cuda:0')
c= tensor(5.1121e+08, device='cuda:0')
c= tensor(5.1122e+08, device='cuda:0')
c= tensor(5.1173e+08, device='cuda:0')
c= tensor(5.1175e+08, device='cuda:0')
c= tensor(5.1274e+08, device='cuda:0')
c= tensor(5.1399e+08, device='cuda:0')
c= tensor(5.1400e+08, device='cuda:0')
c= tensor(5.1401e+08, device='cuda:0')
c= tensor(5.1401e+08, device='cuda:0')
c= tensor(5.1419e+08, device='cuda:0')
c= tensor(5.1548e+08, device='cuda:0')
c= tensor(5.1549e+08, device='cuda:0')
c= tensor(5.1642e+08, device='cuda:0')
c= tensor(5.1642e+08, device='cuda:0')
c= tensor(5.1642e+08, device='cuda:0')
c= tensor(5.1643e+08, device='cuda:0')
c= tensor(5.2067e+08, device='cuda:0')
c= tensor(5.2086e+08, device='cuda:0')
c= tensor(5.2086e+08, device='cuda:0')
c= tensor(5.2715e+08, device='cuda:0')
c= tensor(5.2720e+08, device='cuda:0')
c= tensor(5.2727e+08, device='cuda:0')
c= tensor(5.2793e+08, device='cuda:0')
c= tensor(5.2793e+08, device='cuda:0')
c= tensor(5.2810e+08, device='cuda:0')
c= tensor(5.2821e+08, device='cuda:0')
c= tensor(5.3018e+08, device='cuda:0')
c= tensor(5.3030e+08, device='cuda:0')
c= tensor(5.3041e+08, device='cuda:0')
c= tensor(5.3065e+08, device='cuda:0')
c= tensor(5.3066e+08, device='cuda:0')
c= tensor(5.3069e+08, device='cuda:0')
c= tensor(5.3082e+08, device='cuda:0')
c= tensor(5.3113e+08, device='cuda:0')
c= tensor(5.3510e+08, device='cuda:0')
c= tensor(5.3546e+08, device='cuda:0')
c= tensor(5.3549e+08, device='cuda:0')
c= tensor(5.3549e+08, device='cuda:0')
c= tensor(5.3626e+08, device='cuda:0')
c= tensor(5.3631e+08, device='cuda:0')
c= tensor(5.3631e+08, device='cuda:0')
c= tensor(5.3631e+08, device='cuda:0')
c= tensor(5.3633e+08, device='cuda:0')
c= tensor(5.3634e+08, device='cuda:0')
c= tensor(6.0569e+08, device='cuda:0')
c= tensor(6.0570e+08, device='cuda:0')
c= tensor(6.0582e+08, device='cuda:0')
c= tensor(6.0632e+08, device='cuda:0')
c= tensor(6.0632e+08, device='cuda:0')
c= tensor(6.0650e+08, device='cuda:0')
c= tensor(6.0652e+08, device='cuda:0')
c= tensor(6.0717e+08, device='cuda:0')
c= tensor(6.0834e+08, device='cuda:0')
c= tensor(6.0834e+08, device='cuda:0')
c= tensor(6.0852e+08, device='cuda:0')
c= tensor(6.1036e+08, device='cuda:0')
c= tensor(6.1509e+08, device='cuda:0')
c= tensor(6.1593e+08, device='cuda:0')
c= tensor(6.1593e+08, device='cuda:0')
c= tensor(6.1596e+08, device='cuda:0')
c= tensor(6.2273e+08, device='cuda:0')
c= tensor(6.2341e+08, device='cuda:0')
c= tensor(6.2345e+08, device='cuda:0')
c= tensor(6.2345e+08, device='cuda:0')
c= tensor(6.2595e+08, device='cuda:0')
c= tensor(6.2751e+08, device='cuda:0')
c= tensor(6.2778e+08, device='cuda:0')
c= tensor(6.2779e+08, device='cuda:0')
c= tensor(6.2788e+08, device='cuda:0')
c= tensor(6.2788e+08, device='cuda:0')
c= tensor(6.2789e+08, device='cuda:0')
c= tensor(6.2791e+08, device='cuda:0')
c= tensor(6.2792e+08, device='cuda:0')
c= tensor(6.2815e+08, device='cuda:0')
c= tensor(6.2826e+08, device='cuda:0')
c= tensor(6.2835e+08, device='cuda:0')
c= tensor(6.2870e+08, device='cuda:0')
c= tensor(6.2871e+08, device='cuda:0')
c= tensor(6.7436e+08, device='cuda:0')
c= tensor(6.7438e+08, device='cuda:0')
c= tensor(6.7550e+08, device='cuda:0')
c= tensor(6.7550e+08, device='cuda:0')
c= tensor(6.7551e+08, device='cuda:0')
c= tensor(6.7551e+08, device='cuda:0')
c= tensor(6.7567e+08, device='cuda:0')
c= tensor(6.7567e+08, device='cuda:0')
c= tensor(6.7567e+08, device='cuda:0')
c= tensor(6.7568e+08, device='cuda:0')
c= tensor(6.7568e+08, device='cuda:0')
c= tensor(6.8647e+08, device='cuda:0')
c= tensor(6.8752e+08, device='cuda:0')
c= tensor(6.8753e+08, device='cuda:0')
c= tensor(6.8772e+08, device='cuda:0')
c= tensor(6.8772e+08, device='cuda:0')
c= tensor(6.8773e+08, device='cuda:0')
c= tensor(6.8774e+08, device='cuda:0')
c= tensor(6.8790e+08, device='cuda:0')
c= tensor(6.8790e+08, device='cuda:0')
c= tensor(6.8792e+08, device='cuda:0')
c= tensor(6.8793e+08, device='cuda:0')
c= tensor(6.8794e+08, device='cuda:0')
c= tensor(6.8794e+08, device='cuda:0')
c= tensor(6.8802e+08, device='cuda:0')
c= tensor(6.8802e+08, device='cuda:0')
c= tensor(6.9293e+08, device='cuda:0')
c= tensor(6.9295e+08, device='cuda:0')
c= tensor(6.9303e+08, device='cuda:0')
c= tensor(6.9303e+08, device='cuda:0')
c= tensor(6.9329e+08, device='cuda:0')
c= tensor(6.9330e+08, device='cuda:0')
c= tensor(7.9707e+08, device='cuda:0')
c= tensor(8.1618e+08, device='cuda:0')
c= tensor(8.1661e+08, device='cuda:0')
c= tensor(8.1664e+08, device='cuda:0')
c= tensor(8.1664e+08, device='cuda:0')
c= tensor(8.1669e+08, device='cuda:0')
c= tensor(8.1669e+08, device='cuda:0')
c= tensor(8.1690e+08, device='cuda:0')
c= tensor(8.1690e+08, device='cuda:0')
c= tensor(8.1911e+08, device='cuda:0')
c= tensor(8.2027e+08, device='cuda:0')
c= tensor(8.2042e+08, device='cuda:0')
c= tensor(8.2045e+08, device='cuda:0')
c= tensor(8.2046e+08, device='cuda:0')
c= tensor(8.2046e+08, device='cuda:0')
c= tensor(8.2046e+08, device='cuda:0')
c= tensor(8.2142e+08, device='cuda:0')
c= tensor(8.2149e+08, device='cuda:0')
c= tensor(8.2153e+08, device='cuda:0')
c= tensor(8.2267e+08, device='cuda:0')
c= tensor(8.2268e+08, device='cuda:0')
c= tensor(8.2268e+08, device='cuda:0')
c= tensor(8.2269e+08, device='cuda:0')
c= tensor(8.2327e+08, device='cuda:0')
c= tensor(8.2401e+08, device='cuda:0')
c= tensor(8.2884e+08, device='cuda:0')
c= tensor(8.2959e+08, device='cuda:0')
c= tensor(8.2960e+08, device='cuda:0')
c= tensor(8.2995e+08, device='cuda:0')
c= tensor(8.3183e+08, device='cuda:0')
c= tensor(8.3217e+08, device='cuda:0')
c= tensor(8.3219e+08, device='cuda:0')
c= tensor(8.4468e+08, device='cuda:0')
c= tensor(8.4807e+08, device='cuda:0')
c= tensor(8.4833e+08, device='cuda:0')
c= tensor(8.4914e+08, device='cuda:0')
c= tensor(8.4914e+08, device='cuda:0')
c= tensor(8.4927e+08, device='cuda:0')
c= tensor(8.4928e+08, device='cuda:0')
c= tensor(8.4928e+08, device='cuda:0')
c= tensor(8.5209e+08, device='cuda:0')
c= tensor(8.5345e+08, device='cuda:0')
c= tensor(8.5499e+08, device='cuda:0')
c= tensor(8.5506e+08, device='cuda:0')
c= tensor(8.5539e+08, device='cuda:0')
c= tensor(8.5547e+08, device='cuda:0')
c= tensor(8.5554e+08, device='cuda:0')
c= tensor(8.5554e+08, device='cuda:0')
c= tensor(8.5556e+08, device='cuda:0')
c= tensor(8.5684e+08, device='cuda:0')
c= tensor(8.5702e+08, device='cuda:0')
c= tensor(8.5702e+08, device='cuda:0')
c= tensor(8.5703e+08, device='cuda:0')
c= tensor(8.7980e+08, device='cuda:0')
c= tensor(8.7987e+08, device='cuda:0')
c= tensor(8.7992e+08, device='cuda:0')
c= tensor(8.8000e+08, device='cuda:0')
c= tensor(8.8004e+08, device='cuda:0')
c= tensor(8.8004e+08, device='cuda:0')
c= tensor(8.8005e+08, device='cuda:0')
c= tensor(8.8019e+08, device='cuda:0')
c= tensor(8.8031e+08, device='cuda:0')
c= tensor(8.8031e+08, device='cuda:0')
c= tensor(8.8329e+08, device='cuda:0')
c= tensor(8.8330e+08, device='cuda:0')
c= tensor(8.8333e+08, device='cuda:0')
c= tensor(8.8334e+08, device='cuda:0')
c= tensor(8.8421e+08, device='cuda:0')
c= tensor(8.8421e+08, device='cuda:0')
c= tensor(8.8424e+08, device='cuda:0')
c= tensor(8.8435e+08, device='cuda:0')
c= tensor(8.8450e+08, device='cuda:0')
c= tensor(8.8456e+08, device='cuda:0')
c= tensor(9.2724e+08, device='cuda:0')
c= tensor(9.2726e+08, device='cuda:0')
c= tensor(9.2726e+08, device='cuda:0')
c= tensor(9.2833e+08, device='cuda:0')
c= tensor(9.2837e+08, device='cuda:0')
c= tensor(9.2895e+08, device='cuda:0')
c= tensor(9.2897e+08, device='cuda:0')
c= tensor(9.2990e+08, device='cuda:0')
c= tensor(9.3438e+08, device='cuda:0')
c= tensor(9.3444e+08, device='cuda:0')
c= tensor(9.4767e+08, device='cuda:0')
c= tensor(9.4768e+08, device='cuda:0')
c= tensor(9.4781e+08, device='cuda:0')
c= tensor(9.4783e+08, device='cuda:0')
c= tensor(9.5745e+08, device='cuda:0')
c= tensor(9.5745e+08, device='cuda:0')
c= tensor(9.5745e+08, device='cuda:0')
c= tensor(9.5745e+08, device='cuda:0')
c= tensor(9.5755e+08, device='cuda:0')
c= tensor(9.5873e+08, device='cuda:0')
c= tensor(9.5882e+08, device='cuda:0')
c= tensor(9.5883e+08, device='cuda:0')
c= tensor(9.5884e+08, device='cuda:0')
c= tensor(9.5886e+08, device='cuda:0')
c= tensor(9.6041e+08, device='cuda:0')
c= tensor(9.6063e+08, device='cuda:0')
c= tensor(9.6533e+08, device='cuda:0')
c= tensor(9.6536e+08, device='cuda:0')
c= tensor(9.6540e+08, device='cuda:0')
c= tensor(9.6540e+08, device='cuda:0')
c= tensor(9.6540e+08, device='cuda:0')
c= tensor(9.8558e+08, device='cuda:0')
c= tensor(9.8559e+08, device='cuda:0')
c= tensor(9.8559e+08, device='cuda:0')
c= tensor(9.8679e+08, device='cuda:0')
c= tensor(9.8720e+08, device='cuda:0')
c= tensor(9.8720e+08, device='cuda:0')
c= tensor(9.8720e+08, device='cuda:0')
c= tensor(9.8725e+08, device='cuda:0')
c= tensor(9.8728e+08, device='cuda:0')
c= tensor(9.8730e+08, device='cuda:0')
c= tensor(9.8741e+08, device='cuda:0')
c= tensor(9.8747e+08, device='cuda:0')
c= tensor(9.8749e+08, device='cuda:0')
c= tensor(1.0009e+09, device='cuda:0')
c= tensor(1.0012e+09, device='cuda:0')
c= tensor(1.0012e+09, device='cuda:0')
c= tensor(1.0013e+09, device='cuda:0')
c= tensor(1.0013e+09, device='cuda:0')
c= tensor(1.0014e+09, device='cuda:0')
c= tensor(1.0014e+09, device='cuda:0')
c= tensor(1.0014e+09, device='cuda:0')
c= tensor(1.0028e+09, device='cuda:0')
c= tensor(1.0030e+09, device='cuda:0')
c= tensor(1.0030e+09, device='cuda:0')
c= tensor(1.0030e+09, device='cuda:0')
c= tensor(1.0031e+09, device='cuda:0')
c= tensor(1.0032e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0038e+09, device='cuda:0')
c= tensor(1.0045e+09, device='cuda:0')
c= tensor(1.0045e+09, device='cuda:0')
c= tensor(1.0045e+09, device='cuda:0')
c= tensor(1.0045e+09, device='cuda:0')
c= tensor(1.0047e+09, device='cuda:0')
c= tensor(1.0309e+09, device='cuda:0')
c= tensor(1.0311e+09, device='cuda:0')
c= tensor(1.0311e+09, device='cuda:0')
c= tensor(1.0311e+09, device='cuda:0')
c= tensor(1.0338e+09, device='cuda:0')
c= tensor(1.0338e+09, device='cuda:0')
c= tensor(1.0338e+09, device='cuda:0')
c= tensor(1.0339e+09, device='cuda:0')
c= tensor(1.0340e+09, device='cuda:0')
c= tensor(1.0343e+09, device='cuda:0')
c= tensor(1.0344e+09, device='cuda:0')
c= tensor(1.0345e+09, device='cuda:0')
c= tensor(1.0345e+09, device='cuda:0')
c= tensor(1.0345e+09, device='cuda:0')
c= tensor(1.0347e+09, device='cuda:0')
c= tensor(1.0352e+09, device='cuda:0')
c= tensor(1.0354e+09, device='cuda:0')
c= tensor(1.0356e+09, device='cuda:0')
c= tensor(1.0377e+09, device='cuda:0')
c= tensor(1.0377e+09, device='cuda:0')
c= tensor(1.0377e+09, device='cuda:0')
c= tensor(1.0378e+09, device='cuda:0')
c= tensor(1.0386e+09, device='cuda:0')
c= tensor(1.0387e+09, device='cuda:0')
c= tensor(1.0387e+09, device='cuda:0')
c= tensor(1.0387e+09, device='cuda:0')
c= tensor(1.0389e+09, device='cuda:0')
c= tensor(1.0389e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0391e+09, device='cuda:0')
c= tensor(1.0397e+09, device='cuda:0')
c= tensor(1.0401e+09, device='cuda:0')
c= tensor(1.0436e+09, device='cuda:0')
c= tensor(1.0611e+09, device='cuda:0')
c= tensor(1.0612e+09, device='cuda:0')
c= tensor(1.0612e+09, device='cuda:0')
c= tensor(1.0612e+09, device='cuda:0')
c= tensor(1.0620e+09, device='cuda:0')
c= tensor(1.0620e+09, device='cuda:0')
c= tensor(1.0621e+09, device='cuda:0')
c= tensor(1.0621e+09, device='cuda:0')
c= tensor(1.0722e+09, device='cuda:0')
c= tensor(1.0735e+09, device='cuda:0')
c= tensor(1.0735e+09, device='cuda:0')
c= tensor(1.0754e+09, device='cuda:0')
c= tensor(1.0755e+09, device='cuda:0')
c= tensor(1.0756e+09, device='cuda:0')
c= tensor(1.0882e+09, device='cuda:0')
c= tensor(1.0917e+09, device='cuda:0')
c= tensor(1.0951e+09, device='cuda:0')
c= tensor(1.0951e+09, device='cuda:0')
c= tensor(1.0978e+09, device='cuda:0')
c= tensor(1.0978e+09, device='cuda:0')
c= tensor(1.0979e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1246e+09, device='cuda:0')
c= tensor(1.1246e+09, device='cuda:0')
c= tensor(1.1246e+09, device='cuda:0')
c= tensor(1.1303e+09, device='cuda:0')
c= tensor(1.1303e+09, device='cuda:0')
c= tensor(1.2030e+09, device='cuda:0')
c= tensor(1.2031e+09, device='cuda:0')
c= tensor(1.2032e+09, device='cuda:0')
c= tensor(1.2044e+09, device='cuda:0')
c= tensor(1.2050e+09, device='cuda:0')
c= tensor(1.2099e+09, device='cuda:0')
c= tensor(1.2099e+09, device='cuda:0')
c= tensor(1.2288e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2291e+09, device='cuda:0')
c= tensor(1.2344e+09, device='cuda:0')
c= tensor(1.4211e+09, device='cuda:0')
c= tensor(1.4211e+09, device='cuda:0')
c= tensor(1.4216e+09, device='cuda:0')
c= tensor(1.4216e+09, device='cuda:0')
c= tensor(1.4218e+09, device='cuda:0')
c= tensor(1.4218e+09, device='cuda:0')
c= tensor(1.4221e+09, device='cuda:0')
c= tensor(1.4221e+09, device='cuda:0')
c= tensor(1.4455e+09, device='cuda:0')
c= tensor(1.4455e+09, device='cuda:0')
c= tensor(1.4456e+09, device='cuda:0')
c= tensor(1.4457e+09, device='cuda:0')
c= tensor(1.4458e+09, device='cuda:0')
c= tensor(1.4564e+09, device='cuda:0')
c= tensor(1.4565e+09, device='cuda:0')
c= tensor(1.4565e+09, device='cuda:0')
c= tensor(1.4567e+09, device='cuda:0')
c= tensor(1.4567e+09, device='cuda:0')
c= tensor(1.4567e+09, device='cuda:0')
c= tensor(1.4587e+09, device='cuda:0')
c= tensor(1.5560e+09, device='cuda:0')
c= tensor(1.5561e+09, device='cuda:0')
c= tensor(1.5563e+09, device='cuda:0')
c= tensor(1.5570e+09, device='cuda:0')
c= tensor(1.5570e+09, device='cuda:0')
c= tensor(1.5575e+09, device='cuda:0')
c= tensor(1.5575e+09, device='cuda:0')
c= tensor(1.5636e+09, device='cuda:0')
c= tensor(1.5643e+09, device='cuda:0')
c= tensor(1.5649e+09, device='cuda:0')
c= tensor(1.5650e+09, device='cuda:0')
c= tensor(1.5650e+09, device='cuda:0')
c= tensor(1.5650e+09, device='cuda:0')
c= tensor(1.6374e+09, device='cuda:0')
c= tensor(1.6379e+09, device='cuda:0')
c= tensor(1.6379e+09, device='cuda:0')
c= tensor(1.6379e+09, device='cuda:0')
c= tensor(1.6379e+09, device='cuda:0')
c= tensor(1.6385e+09, device='cuda:0')
c= tensor(1.6391e+09, device='cuda:0')
c= tensor(1.6463e+09, device='cuda:0')
c= tensor(1.6464e+09, device='cuda:0')
c= tensor(1.6464e+09, device='cuda:0')
c= tensor(1.6465e+09, device='cuda:0')
c= tensor(1.6467e+09, device='cuda:0')
c= tensor(1.6468e+09, device='cuda:0')
c= tensor(1.6473e+09, device='cuda:0')
c= tensor(1.6473e+09, device='cuda:0')
c= tensor(1.6474e+09, device='cuda:0')
c= tensor(1.6474e+09, device='cuda:0')
c= tensor(1.6474e+09, device='cuda:0')
c= tensor(1.6536e+09, device='cuda:0')
c= tensor(1.6538e+09, device='cuda:0')
c= tensor(1.6542e+09, device='cuda:0')
c= tensor(1.6542e+09, device='cuda:0')
c= tensor(1.6542e+09, device='cuda:0')
c= tensor(1.6548e+09, device='cuda:0')
c= tensor(1.6558e+09, device='cuda:0')
c= tensor(1.6560e+09, device='cuda:0')
c= tensor(1.6560e+09, device='cuda:0')
c= tensor(1.6560e+09, device='cuda:0')
c= tensor(1.6560e+09, device='cuda:0')
c= tensor(1.6561e+09, device='cuda:0')
c= tensor(1.6572e+09, device='cuda:0')
c= tensor(1.6572e+09, device='cuda:0')
c= tensor(1.6572e+09, device='cuda:0')
c= tensor(1.6572e+09, device='cuda:0')
c= tensor(1.6572e+09, device='cuda:0')
c= tensor(1.6588e+09, device='cuda:0')
c= tensor(1.6588e+09, device='cuda:0')
c= tensor(1.6589e+09, device='cuda:0')
c= tensor(1.6599e+09, device='cuda:0')
c= tensor(1.6599e+09, device='cuda:0')
c= tensor(1.6605e+09, device='cuda:0')
c= tensor(1.6616e+09, device='cuda:0')
c= tensor(1.6616e+09, device='cuda:0')
c= tensor(1.6632e+09, device='cuda:0')
c= tensor(1.6634e+09, device='cuda:0')
c= tensor(1.6722e+09, device='cuda:0')
c= tensor(1.6738e+09, device='cuda:0')
c= tensor(1.6739e+09, device='cuda:0')
c= tensor(1.6744e+09, device='cuda:0')
c= tensor(1.6744e+09, device='cuda:0')
c= tensor(1.6744e+09, device='cuda:0')
c= tensor(1.6744e+09, device='cuda:0')
c= tensor(1.6745e+09, device='cuda:0')
c= tensor(1.6752e+09, device='cuda:0')
c= tensor(1.6754e+09, device='cuda:0')
c= tensor(1.6754e+09, device='cuda:0')
c= tensor(1.6756e+09, device='cuda:0')
c= tensor(1.6757e+09, device='cuda:0')
c= tensor(1.6808e+09, device='cuda:0')
c= tensor(1.6809e+09, device='cuda:0')
c= tensor(1.6809e+09, device='cuda:0')
c= tensor(1.6812e+09, device='cuda:0')
c= tensor(1.6819e+09, device='cuda:0')
c= tensor(1.6819e+09, device='cuda:0')
c= tensor(1.6825e+09, device='cuda:0')
c= tensor(1.6833e+09, device='cuda:0')
time to make c is 11.643246412277222
time for making loss is 11.643264293670654
p0 True
it  0 : 334646784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  4% |
shape of L is 
torch.Size([])
memory (bytes)
3639873536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  4% |
memory (bytes)
3640295424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 62% |  4% |
error is  154963970.0
relative error loss 0.09206143
shape of L is 
torch.Size([])
memory (bytes)
3669647360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  4% |
memory (bytes)
3669647360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  4% |
error is  144464640.0
relative error loss 0.08582396
shape of L is 
torch.Size([])
memory (bytes)
3673333760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  4% |
memory (bytes)
3673337856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 68% |  4% |
error is  132791680.0
relative error loss 0.07888926
shape of L is 
torch.Size([])
memory (bytes)
3676319744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3676545024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  5% |
error is  128388100.0
relative error loss 0.076273166
shape of L is 
torch.Size([])
memory (bytes)
3679776768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3679776768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  5% |
error is  124764420.0
relative error loss 0.074120395
shape of L is 
torch.Size([])
memory (bytes)
3683069952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  5% |
memory (bytes)
3683069952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 66% |  5% |
error is  122475650.0
relative error loss 0.07276068
shape of L is 
torch.Size([])
memory (bytes)
3686301696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3686301696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  120911360.0
relative error loss 0.07183136
shape of L is 
torch.Size([])
memory (bytes)
3689324544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3689541632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  119161730.0
relative error loss 0.07079194
shape of L is 
torch.Size([])
memory (bytes)
3692769280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
3692773376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 69% |  5% |
error is  117864580.0
relative error loss 0.070021324
shape of L is 
torch.Size([])
memory (bytes)
3695910912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3695988736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 84% |  5% |
error is  117017220.0
relative error loss 0.06951792
time to take a step is 386.544873714447
it  1 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3699224576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  4% |
memory (bytes)
3699224576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  5% |
error is  117017220.0
relative error loss 0.06951792
shape of L is 
torch.Size([])
memory (bytes)
3702353920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
3702456320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 78% |  5% |
error is  116035200.0
relative error loss 0.06893452
shape of L is 
torch.Size([])
memory (bytes)
3705708544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3705708544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  115534590.0
relative error loss 0.06863712
shape of L is 
torch.Size([])
memory (bytes)
3708940288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3708940288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  115021310.0
relative error loss 0.06833219
shape of L is 
torch.Size([])
memory (bytes)
3712143360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3712163840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  114704900.0
relative error loss 0.06814421
shape of L is 
torch.Size([])
memory (bytes)
3715403776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3715403776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 84% |  5% |
error is  114263550.0
relative error loss 0.06788202
shape of L is 
torch.Size([])
memory (bytes)
3718447104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3718635520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  5% |
error is  113967620.0
relative error loss 0.067706205
shape of L is 
torch.Size([])
memory (bytes)
3721867264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3721867264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  113511040.0
relative error loss 0.06743496
shape of L is 
torch.Size([])
memory (bytes)
3724955648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3725103104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  5% |
error is  113270660.0
relative error loss 0.067292154
shape of L is 
torch.Size([])
memory (bytes)
3728330752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3728330752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  113024900.0
relative error loss 0.06714615
time to take a step is 380.08772921562195
it  2 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3731570688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  4% |
memory (bytes)
3731570688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  113024900.0
relative error loss 0.06714615
shape of L is 
torch.Size([])
memory (bytes)
3734798336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3734798336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  112720900.0
relative error loss 0.06696555
shape of L is 
torch.Size([])
memory (bytes)
3737866240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3738058752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  5% |
error is  112497150.0
relative error loss 0.066832624
shape of L is 
torch.Size([])
memory (bytes)
3741294592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
3741294592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  5% |
error is  112369790.0
relative error loss 0.066756964
shape of L is 
torch.Size([])
memory (bytes)
3744485376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3744485376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  112213890.0
relative error loss 0.066664346
shape of L is 
torch.Size([])
memory (bytes)
3747766272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3747766272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  112059780.0
relative error loss 0.06657279
shape of L is 
torch.Size([])
memory (bytes)
3750993920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  5% |
memory (bytes)
3750993920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 71% |  5% |
error is  111889540.0
relative error loss 0.06647165
shape of L is 
torch.Size([])
memory (bytes)
3754254336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3754254336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  5% |
error is  111762940.0
relative error loss 0.066396445
shape of L is 
torch.Size([])
memory (bytes)
3757297664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  5% |
memory (bytes)
3757481984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 71% |  5% |
error is  111628540.0
relative error loss 0.066316605
shape of L is 
torch.Size([])
memory (bytes)
3760721920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3760721920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  111552000.0
relative error loss 0.066271126
time to take a step is 382.1942389011383
it  3 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3763769344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  4% |
memory (bytes)
3763957760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 70% |  5% |
error is  111552000.0
relative error loss 0.066271126
shape of L is 
torch.Size([])
memory (bytes)
3767189504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3767189504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  111475200.0
relative error loss 0.06622551
shape of L is 
torch.Size([])
memory (bytes)
3770417152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3770437632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  111427200.0
relative error loss 0.066196986
shape of L is 
torch.Size([])
memory (bytes)
3773669376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3773669376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  111373700.0
relative error loss 0.0661652
shape of L is 
torch.Size([])
memory (bytes)
3776753664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  5% |
memory (bytes)
3776905216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 71% |  5% |
error is  111277310.0
relative error loss 0.06610794
shape of L is 
torch.Size([])
memory (bytes)
3780128768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3780128768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  111229570.0
relative error loss 0.06607958
shape of L is 
torch.Size([])
memory (bytes)
3783274496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3783364608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  111147900.0
relative error loss 0.06603106
shape of L is 
torch.Size([])
memory (bytes)
3786600448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3786600448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  111103870.0
relative error loss 0.0660049
shape of L is 
torch.Size([])
memory (bytes)
3789737984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3789828096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  111012480.0
relative error loss 0.06595061
shape of L is 
torch.Size([])
memory (bytes)
3793063936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  5% |
memory (bytes)
3793063936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 84% |  5% |
error is  111012100.0
relative error loss 0.06595038
shape of L is 
torch.Size([])
memory (bytes)
3796271104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3796312064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  5% |
error is  110966660.0
relative error loss 0.065923385
time to take a step is 421.6268980503082
it  4 : 505070080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3799539712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3799543808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  110966660.0
relative error loss 0.065923385
shape of L is 
torch.Size([])
memory (bytes)
3802759168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3802759168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110894590.0
relative error loss 0.065880574
shape of L is 
torch.Size([])
memory (bytes)
3806035968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3806035968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110828670.0
relative error loss 0.065841414
shape of L is 
torch.Size([])
memory (bytes)
3809263616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3809263616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  110781570.0
relative error loss 0.06581343
shape of L is 
torch.Size([])
memory (bytes)
3812495360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3812495360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 82% |  5% |
error is  110738690.0
relative error loss 0.065787956
shape of L is 
torch.Size([])
memory (bytes)
3815727104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  5% |
memory (bytes)
3815731200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 63% |  5% |
error is  110700290.0
relative error loss 0.06576514
shape of L is 
torch.Size([])
memory (bytes)
3818967040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3818967040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110668160.0
relative error loss 0.065746054
shape of L is 
torch.Size([])
memory (bytes)
3822194688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3822194688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  5% |
error is  110622720.0
relative error loss 0.06571906
shape of L is 
torch.Size([])
memory (bytes)
3825393664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3825393664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 78% |  5% |
error is  110581890.0
relative error loss 0.0656948
shape of L is 
torch.Size([])
memory (bytes)
3828662272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3828662272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 78% |  5% |
error is  110553860.0
relative error loss 0.06567815
time to take a step is 384.6006088256836
it  5 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3831808000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3831906304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  110553860.0
relative error loss 0.06567815
shape of L is 
torch.Size([])
memory (bytes)
3835125760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3835125760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110528260.0
relative error loss 0.06566294
shape of L is 
torch.Size([])
memory (bytes)
3838353408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3838373888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110492290.0
relative error loss 0.065641575
shape of L is 
torch.Size([])
memory (bytes)
3841613824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3841613824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110458370.0
relative error loss 0.06562142
shape of L is 
torch.Size([])
memory (bytes)
3844747264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3844747264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110439940.0
relative error loss 0.06561047
shape of L is 
torch.Size([])
memory (bytes)
3848056832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3848056832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  110413700.0
relative error loss 0.06559488
shape of L is 
torch.Size([])
memory (bytes)
3851202560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3851202560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  110395010.0
relative error loss 0.06558378
shape of L is 
torch.Size([])
memory (bytes)
3854520320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3854520320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  110376450.0
relative error loss 0.06557275
shape of L is 
torch.Size([])
memory (bytes)
3857756160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3857756160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110339970.0
relative error loss 0.06555108
shape of L is 
torch.Size([])
memory (bytes)
3860983808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  5% |
memory (bytes)
3860983808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 68% |  5% |
error is  110320770.0
relative error loss 0.06553967
time to take a step is 384.5273685455322
it  6 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3864231936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3864231936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  5% |
error is  110320770.0
relative error loss 0.06553967
shape of L is 
torch.Size([])
memory (bytes)
3867443200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3867443200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  110320380.0
relative error loss 0.06553945
shape of L is 
torch.Size([])
memory (bytes)
3870703616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3870703616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110295550.0
relative error loss 0.0655247
shape of L is 
torch.Size([])
memory (bytes)
3873939456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
3873939456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 84% |  5% |
error is  110264960.0
relative error loss 0.06550652
shape of L is 
torch.Size([])
memory (bytes)
3877175296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
3877175296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 73% |  5% |
error is  110247170.0
relative error loss 0.06549595
shape of L is 
torch.Size([])
memory (bytes)
3880394752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3880394752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  5% |
error is  110225020.0
relative error loss 0.065482795
shape of L is 
torch.Size([])
memory (bytes)
3883536384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3883630592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  5% |
error is  110205700.0
relative error loss 0.065471314
shape of L is 
torch.Size([])
memory (bytes)
3886866432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  5% |
memory (bytes)
3886866432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 78% |  5% |
error is  110189820.0
relative error loss 0.06546188
shape of L is 
torch.Size([])
memory (bytes)
3890094080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3890094080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110173700.0
relative error loss 0.0654523
shape of L is 
torch.Size([])
memory (bytes)
3893256192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
3893256192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 66% |  5% |
error is  110162690.0
relative error loss 0.06544576
time to take a step is 384.6225199699402
it  7 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3896565760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3896565760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  110162690.0
relative error loss 0.06544576
shape of L is 
torch.Size([])
memory (bytes)
3899723776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
3899785216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 68% |  5% |
error is  110150270.0
relative error loss 0.06543838
shape of L is 
torch.Size([])
memory (bytes)
3903037440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3903037440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  110141440.0
relative error loss 0.06543314
shape of L is 
torch.Size([])
memory (bytes)
3906220032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
3906220032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 68% |  5% |
error is  110131200.0
relative error loss 0.06542706
shape of L is 
torch.Size([])
memory (bytes)
3909505024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3909505024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  110128000.0
relative error loss 0.06542516
shape of L is 
torch.Size([])
memory (bytes)
3912732672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  5% |
memory (bytes)
3912732672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 68% |  5% |
error is  110119810.0
relative error loss 0.065420285
shape of L is 
torch.Size([])
memory (bytes)
3915968512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3915968512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 84% |  5% |
error is  110116480.0
relative error loss 0.06541831
shape of L is 
torch.Size([])
memory (bytes)
3919110144
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3919110144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  110111870.0
relative error loss 0.065415576
shape of L is 
torch.Size([])
memory (bytes)
3922444288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3922444288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110103550.0
relative error loss 0.06541063
shape of L is 
torch.Size([])
memory (bytes)
3925684224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3925684224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  5% |
error is  110103040.0
relative error loss 0.06541032
shape of L is 
torch.Size([])
memory (bytes)
3928924160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
3928924160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 66% |  5% |
error is  110097540.0
relative error loss 0.06540706
time to take a step is 421.88966631889343
it  8 : 505070080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3932139520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3932155904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  110097540.0
relative error loss 0.06540706
shape of L is 
torch.Size([])
memory (bytes)
3935379456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3935379456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110088830.0
relative error loss 0.06540188
shape of L is 
torch.Size([])
memory (bytes)
3938619392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3938635776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  5% |
error is  110080900.0
relative error loss 0.06539717
shape of L is 
torch.Size([])
memory (bytes)
3941859328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3941863424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110065660.0
relative error loss 0.06538812
shape of L is 
torch.Size([])
memory (bytes)
3945099264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3945099264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110052350.0
relative error loss 0.065380216
shape of L is 
torch.Size([])
memory (bytes)
3948298240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3948298240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  5% |
error is  110042750.0
relative error loss 0.06537451
shape of L is 
torch.Size([])
memory (bytes)
3951550464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  5% |
memory (bytes)
3951550464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 67% |  5% |
error is  110031100.0
relative error loss 0.06536759
shape of L is 
torch.Size([])
memory (bytes)
3954761728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3954761728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  5% |
error is  110015740.0
relative error loss 0.06535847
shape of L is 
torch.Size([])
memory (bytes)
3958013952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3958013952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  110044160.0
relative error loss 0.06537534
shape of L is 
torch.Size([])
memory (bytes)
3961221120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3961221120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110007550.0
relative error loss 0.0653536
time to take a step is 385.8010678291321
it  9 : 505070080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3964481536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3964481536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  110007550.0
relative error loss 0.0653536
shape of L is 
torch.Size([])
memory (bytes)
3967541248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
3967696896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 70% |  5% |
error is  109995140.0
relative error loss 0.06534622
shape of L is 
torch.Size([])
memory (bytes)
3970953216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
3970953216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  5% |
error is  109986050.0
relative error loss 0.065340824
shape of L is 
torch.Size([])
memory (bytes)
3974139904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3974139904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  109973380.0
relative error loss 0.06533329
shape of L is 
torch.Size([])
memory (bytes)
3977400320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3977400320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  109959680.0
relative error loss 0.065325156
shape of L is 
torch.Size([])
memory (bytes)
3980451840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  5% |
memory (bytes)
3980636160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 74% |  5% |
error is  109953020.0
relative error loss 0.06532121
shape of L is 
torch.Size([])
memory (bytes)
3983872000
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3983872000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  109942530.0
relative error loss 0.06531497
shape of L is 
torch.Size([])
memory (bytes)
3987030016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3987103744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 63% |  5% |
error is  109936640.0
relative error loss 0.06531147
shape of L is 
torch.Size([])
memory (bytes)
3990347776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
3990347776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  5% |
error is  109924220.0
relative error loss 0.06530409
shape of L is 
torch.Size([])
memory (bytes)
3993444352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3993575424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  109919870.0
relative error loss 0.06530151
time to take a step is 384.2095568180084
it  10 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3996803072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  5% |
memory (bytes)
3996803072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 77% |  5% |
error is  109919870.0
relative error loss 0.06530151
shape of L is 
torch.Size([])
memory (bytes)
4000030720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
4000030720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 64% |  5% |
error is  109915140.0
relative error loss 0.0652987
shape of L is 
torch.Size([])
memory (bytes)
4003303424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4003303424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109906940.0
relative error loss 0.065293826
shape of L is 
torch.Size([])
memory (bytes)
4006535168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4006535168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109898750.0
relative error loss 0.06528896
shape of L is 
torch.Size([])
memory (bytes)
4009762816
| ID | GPU | MEM |
------------------
|  0 | 25% |  0% |
|  1 | 10% |  5% |
memory (bytes)
4009766912
| ID | GPU | MEM |
------------------
|  0 | 23% |  0% |
|  1 | 68% |  5% |
error is  109888640.0
relative error loss 0.065282956
shape of L is 
torch.Size([])
memory (bytes)
4013006848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4013006848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109883140.0
relative error loss 0.065279685
shape of L is 
torch.Size([])
memory (bytes)
4016242688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4016242688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109867520.0
relative error loss 0.06527041
shape of L is 
torch.Size([])
memory (bytes)
4019490816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4019490816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  5% |
error is  109875970.0
relative error loss 0.06527542
shape of L is 
torch.Size([])
memory (bytes)
4022706176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4022706176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 84% |  5% |
error is  109860990.0
relative error loss 0.06526653
shape of L is 
torch.Size([])
memory (bytes)
4025933824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  5% |
memory (bytes)
4025933824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 74% |  5% |
error is  109852540.0
relative error loss 0.06526151
time to take a step is 384.1419382095337
it  11 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
4029181952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  5% |
memory (bytes)
4029181952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 73% |  5% |
error is  109852540.0
relative error loss 0.06526151
shape of L is 
torch.Size([])
memory (bytes)
4032393216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
4032409600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 65% |  5% |
error is  109831810.0
relative error loss 0.06524919
shape of L is 
torch.Size([])
memory (bytes)
4035653632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  5% |
memory (bytes)
4035653632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 76% |  5% |
error is  109831680.0
relative error loss 0.065249115
shape of L is 
torch.Size([])
memory (bytes)
4038881280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
4038881280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 87% |  5% |
error is  109824130.0
relative error loss 0.06524463
shape of L is 
torch.Size([])
memory (bytes)
4042129408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4042129408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 81% |  5% |
error is  109817860.0
relative error loss 0.065240905
shape of L is 
torch.Size([])
memory (bytes)
4045369344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4045369344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109804420.0
relative error loss 0.06523292
shape of L is 
torch.Size([])
memory (bytes)
4048605184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4048605184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109797380.0
relative error loss 0.06522874
shape of L is 
torch.Size([])
memory (bytes)
4051828736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
4051828736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 83% |  5% |
error is  109791620.0
relative error loss 0.06522532
shape of L is 
torch.Size([])
memory (bytes)
4055080960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4055080960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  109781630.0
relative error loss 0.06521938
shape of L is 
torch.Size([])
memory (bytes)
4058316800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
4058316800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109776770.0
relative error loss 0.0652165
time to take a step is 385.4084939956665
it  12 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
4061556736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4061556736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109776770.0
relative error loss 0.0652165
shape of L is 
torch.Size([])
memory (bytes)
4064784384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
4064784384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109770750.0
relative error loss 0.06521292
shape of L is 
torch.Size([])
memory (bytes)
4068028416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
4068032512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  109762820.0
relative error loss 0.065208204
shape of L is 
torch.Size([])
memory (bytes)
4071280640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4071280640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  109752190.0
relative error loss 0.06520189
shape of L is 
torch.Size([])
memory (bytes)
4074508288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4074508288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  109747200.0
relative error loss 0.06519893
shape of L is 
torch.Size([])
memory (bytes)
4077735936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4077735936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  5% |
error is  109743870.0
relative error loss 0.06519695
shape of L is 
torch.Size([])
memory (bytes)
4080967680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4080967680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 80% |  5% |
error is  109735420.0
relative error loss 0.06519193
shape of L is 
torch.Size([])
memory (bytes)
4084199424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4084199424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  5% |
error is  109728380.0
relative error loss 0.06518775
shape of L is 
torch.Size([])
memory (bytes)
4087402496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4087402496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  5% |
error is  109719550.0
relative error loss 0.0651825
shape of L is 
torch.Size([])
memory (bytes)
4090658816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4090658816
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 81% |  5% |
error is  109712770.0
relative error loss 0.06517847
time to take a step is 385.1849067211151
it  13 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
4093796352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  5% |
memory (bytes)
4093890560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 77% |  5% |
error is  109712770.0
relative error loss 0.06517847
shape of L is 
torch.Size([])
memory (bytes)
4097122304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4097122304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109709440.0
relative error loss 0.065176494
shape of L is 
torch.Size([])
memory (bytes)
4100194304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4100362240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109706370.0
relative error loss 0.06517467
shape of L is 
torch.Size([])
memory (bytes)
4103598080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4103598080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109704320.0
relative error loss 0.065173455
shape of L is 
torch.Size([])
memory (bytes)
4106817536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4106842112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109701120.0
relative error loss 0.065171555
shape of L is 
torch.Size([])
memory (bytes)
4110065664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4110065664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109697280.0
relative error loss 0.065169275
shape of L is 
torch.Size([])
memory (bytes)
4113178624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4113297408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  109692930.0
relative error loss 0.06516669
shape of L is 
torch.Size([])
memory (bytes)
4116541440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
4116541440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 71% |  5% |
error is  109691390.0
relative error loss 0.06516577
shape of L is 
torch.Size([])
memory (bytes)
4119683072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4119773184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 84% |  5% |
error is  109688830.0
relative error loss 0.06516425
shape of L is 
torch.Size([])
memory (bytes)
4123000832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4123000832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 88% |  5% |
error is  109684740.0
relative error loss 0.06516182
time to take a step is 385.1679892539978
it  14 : 505069568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
4126232576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
4126232576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109684740.0
relative error loss 0.06516182
shape of L is 
torch.Size([])
memory (bytes)
4129456128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4129456128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  109682180.0
relative error loss 0.0651603
shape of L is 
torch.Size([])
memory (bytes)
4132610048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4132700160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109679620.0
relative error loss 0.06515878
shape of L is 
torch.Size([])
memory (bytes)
4135936000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  5% |
memory (bytes)
4135936000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  5% |
error is  109678460.0
relative error loss 0.06515809
shape of L is 
torch.Size([])
memory (bytes)
4139167744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
4139167744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 63% |  5% |
error is  109676290.0
relative error loss 0.0651568
shape of L is 
torch.Size([])
memory (bytes)
4142407680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  5% |
memory (bytes)
4142407680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 73% |  5% |
error is  109674240.0
relative error loss 0.06515558
shape of L is 
torch.Size([])
memory (bytes)
4145508352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
4145639424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 62% |  5% |
error is  109671040.0
relative error loss 0.06515368
shape of L is 
torch.Size([])
memory (bytes)
4148875264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4148875264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 85% |  5% |
error is  109668220.0
relative error loss 0.06515201
shape of L is 
torch.Size([])
memory (bytes)
4152102912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4152102912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 62% |  5% |
error is  109666560.0
relative error loss 0.06515102
shape of L is 
torch.Size([])
memory (bytes)
4155351040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4155351040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  5% |
error is  109666690.0
relative error loss 0.065151095
shape of L is 
torch.Size([])
memory (bytes)
4158558208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
4158578688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  5% |
error is  109665280.0
relative error loss 0.06515026
time to take a step is 426.77971172332764
sum tnnu_Z after tensor(4778353., device='cuda:0')
shape of features
(1653,)
shape of features
(1653,)
number of orig particles 6610
number of new particles after remove low mass 3663
tnuZ shape should be parts x labs
torch.Size([6610, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  154958200.0
relative error without small mass is  0.09205801
nnu_Z shape should be number of particles by maxV
(6610, 702)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
shape of features
(6610,)
Fri Feb 3 04:38:25 EST 2023
