Wed Feb 1 05:56:20 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 47546585
numbers of Z: 26113
shape of features
(26113,)
shape of features
(26113,)
ZX	Vol	Parts	Cubes	Eps
Z	0.019704496558165953	26113	26.113	0.09104082645258761
X	0.01733766432135311	1070	1.07	0.25304762808118597
X	0.017921729222281163	27472	27.472	0.08672877478523111
X	0.019247973662615863	4511	4.511	0.16219497541419234
X	0.017859743376340522	18038	18.038	0.09966949945857785
X	0.01851717404929385	37808	37.808	0.07882494350011197
X	0.017876968416421384	47856	47.856	0.07201989641443039
X	0.017798132297396616	54344	54.344	0.06892989384932575
X	0.017845454529957623	56487	56.487	0.06810720134126819
X	0.017756526667409935	13341	13.341	0.10999928897327053
X	0.01779729750333118	30806	30.806	0.08328617485677335
X	0.017713160310181567	8808	8.808	0.12622331515841576
X	0.017754819996925703	142118	142.118	0.04999068205871371
X	0.017843531475830354	12894	12.894	0.11143741993027789
X	0.018999304783624393	363578	363.578	0.037386376146589816
X	0.01782988487174682	27257	27.257	0.08680741590024389
X	0.017893295163150934	51851	51.851	0.07014193293824621
X	0.017853472889614454	78418	78.418	0.06106171280071698
X	0.017828935441307602	28639	28.639	0.08538649518178673
X	0.017829550674377113	149200	149.2	0.04925577897873781
X	0.017974628444502132	107799	107.799	0.05504041881576053
X	0.017771796140255692	46392	46.392	0.07262666061486289
X	0.018601657283398798	301016	301.016	0.0395355106626757
X	0.017863045496267466	12701	12.701	0.1120398527677893
X	0.018860507430317728	51889	51.889	0.07136621937686756
X	0.017466289613173633	2844	2.844	0.1831289568919604
X	0.017900275888987793	83171	83.171	0.05992792866921374
X	0.01792357101701988	68106	68.106	0.06408352445163934
X	0.017860467074577984	11662	11.662	0.11526742506694468
X	0.017949953834195235	87301	87.301	0.059022106557911805
X	0.018907313370138023	1615596	1615.596	0.0227038243239826
X	0.017819502406285114	13858	13.858	0.10874225594090364
X	0.018246545521089504	877871	877.871	0.027494763229379465
X	0.017792210702083505	33283	33.283	0.08115883718806853
X	0.01784646245992723	12086	12.086	0.11387363503991149
X	0.017828163922872363	21957	21.957	0.09329202906361586
X	0.018467562699167175	208828	208.828	0.04455265785955417
X	0.01807974918320272	94966	94.966	0.057527400753445895
X	0.017728745156690394	1528	1.528	0.22638703298458013
X	0.018777873595171687	5841	5.841	0.14758868673036063
X	0.017562992675103646	3761	3.761	0.16714692665531156
X	0.017861431752955757	4367	4.367	0.15992303586793769
X	0.017050867968141686	1803	1.803	0.2114707253459941
X	0.016583970547264563	698	0.698	0.2874822476752166
X	0.017633280744854322	5894	5.894	0.14409237406909364
X	0.017626197617227647	967	0.967	0.2631746497480311
X	0.017320476043167825	1067	1.067	0.25320083579453656
X	0.017767364689535877	3735	3.735	0.1681812090076909
X	0.017823550083340113	4014	4.014	0.16436358838988174
X	0.01776118480385929	4986	4.986	0.15272374076329895
X	0.017774554879318	14235	14.235	0.10768298539650613
X	0.01782968392808237	16540	16.54	0.10253434931464936
X	0.018573602219667543	2300	2.3	0.20062702568909466
X	0.017294951235289076	4956	4.956	0.15168035905115637
X	0.01742853371779818	2874	2.874	0.18235794762357052
X	0.017769481375308257	8140	8.14	0.1297229399194268
X	0.017740363656504013	8497	8.497	0.12781023497560187
X	0.017739087434677638	2666	2.666	0.18808624799341084
X	0.018640532369902152	5852	5.852	0.14713567908942196
X	0.017769890991113195	2340	2.34	0.196557507089223
X	0.017568858743780356	4183	4.183	0.16134365666438646
X	0.017774574762867597	8895	8.895	0.125955679655517
X	0.01774009601473384	1371	1.371	0.23476832626400956
X	0.018627160959067083	13319	13.319	0.11183000632988649
X	0.017780440243135753	9592	9.592	0.12284131209715134
X	0.017746742876774385	2842	2.842	0.18414709037928775
X	0.017751333598429453	2300	2.3	0.19762158729902446
X	0.017524730256056286	2367	2.367	0.19490262783771503
X	0.017570752302473257	4949	4.949	0.15255424640183965
X	0.017854920246673966	5014	5.014	0.15270661896916043
X	0.017799112448996086	3666	3.666	0.16933054079312862
X	0.017478099201673956	10928	10.928	0.11694576644908351
X	0.01764914464586372	5493	5.493	0.14756092007058527
X	0.017736022982197154	3860	3.86	0.16624795535849288
X	0.017229939764569477	3969	3.969	0.16313014420100957
X	0.01768910939936938	4772	4.772	0.15476348865331255
X	0.01776231757782104	11035	11.035	0.11719500387533374
X	0.01727945252454396	802	0.802	0.2782606978867221
X	0.017761509789465085	1147	1.147	0.2492520968585174
X	0.017759700670194993	3400	3.4	0.1735077620690183
X	0.018734478540204953	21033	21.033	0.0962158887566917
X	0.01768654854514284	3201	3.201	0.17678787357551326
X	0.017277816583872604	1786	1.786	0.21307662126382423
X	0.017967691959038242	5408	5.408	0.14921707989652652
X	0.01747618724876521	2255	2.255	0.19789434362649178
X	0.017365862149585975	2324	2.324	0.19550299210847985
X	0.017644046661374514	1523	1.523	0.22627301555837437
X	0.01774599755589479	3403	3.403	0.1734121366960488
X	0.01748593282681524	735	0.735	0.28760768663484626
X	0.017986028625338983	6102	6.102	0.14337970824212476
X	0.017796725141292703	4121	4.121	0.16284677397269962
X	0.01786158446945703	5373	5.373	0.14924543187604755
X	0.017507515609633793	1367	1.367	0.23396559722009322
X	0.017796004019128807	1474	1.474	0.22940788299996306
X	0.01776130149708611	7002	7.002	0.1363800650772266
X	0.01748896609112975	5273	5.273	0.14913129465726385
X	0.017776034352049774	5386	5.386	0.14888679364688162
X	0.018928806197633347	5044	5.044	0.15539941596898765
X	0.01751509399171108	9724	9.724	0.12167158736189083
X	0.018791289004306027	12936	12.936	0.1132536449804983
X	0.0177461365652684	3722	3.722	0.1683097016576429
X	0.01784133887736028	11976	11.976	0.1142102857984947
X	0.017710327152501214	5991	5.991	0.1435188970200634
X	0.017681243882916756	5965	5.965	0.14364839861030276
X	0.017597137134900173	2012	2.012	0.20603358247989284
X	0.01752357842822964	6926	6.926	0.1362636767434884
X	0.017770397187926508	2425	2.425	0.19423543563881024
X	0.017599580012258132	4669	4.669	0.15562981817987126
X	0.01738546712960295	840	0.84	0.27455900527106236
X	0.017766709836679617	3128	3.128	0.1784213228795893
X	0.017485677477471224	1294	1.294	0.23818590748032006
X	0.017713366217848917	3112	3.112	0.17854752636593416
X	0.01752885626886765	3065	3.065	0.1788302705976201
X	0.017757725426417053	2697	2.697	0.1874284282947764
X	0.017813198871127298	4104	4.104	0.1631216183447497
X	0.01757457042131271	5214	5.214	0.14993553160601547
X	0.01754434962830706	2450	2.45	0.19274822899854635
X	0.017635742225738875	2518	2.518	0.19132835556860878
X	0.017140550166425458	939	0.939	0.26330133136024975
X	0.017884738544168537	9900	9.9	0.12179116936862248
X	0.017364512794303726	1044	1.044	0.2552627055048028
X	0.01778207274931065	12290	12.29	0.11310370543866673
X	0.01738427879863701	2016	2.016	0.20506366585705008
X	0.017735977503552325	2213	2.213	0.20012034293543945
X	0.017723866625262118	3171	3.171	0.17746828144711987
X	0.017552385933003612	1883	1.883	0.21045630755527
X	0.01779701890921803	3485	3.485	0.172205952714373
X	0.01751093986009288	1028	1.028	0.25729940422637526
X	0.01766596672387542	4996	4.996	0.1523485503510259
X	0.018746256358230056	13702	13.702	0.11101377292165075
X	0.01772617341442689	2314	2.314	0.19712901249827797
X	0.01779239670733605	8117	8.117	0.12990114199598377
X	0.017481219620260748	2048	2.048	0.20436848503432284
X	0.018494533128827685	6123	6.123	0.14455275427929715
X	0.01707492259374611	1695	1.695	0.2159714514543939
X	0.017769113714849124	3736	3.736	0.16817172013879475
X	0.01765866740614763	2694	2.694	0.1871486843986362
X	0.017375956543479995	2347	2.347	0.19490001717410171
X	0.017757974225974867	1844	1.844	0.2127538861039367
X	0.017209364328276126	1746	1.746	0.21440761035699962
X	0.01749533360172843	1589	1.589	0.22246726224811095
X	0.017474226092109236	3241	3.241	0.17535021653464186
X	0.017478073333306556	1265	1.265	0.2399575084583419
X	0.018779506012781704	9342	9.342	0.12620641596489512
X	0.017743028855057047	14463	14.463	0.10705076887097675
X	0.018683399301228316	10088	10.088	0.12280525201649418
X	0.017592753062185586	2331	2.331	0.19615400407449807
X	0.0177140636767961	2752	2.752	0.18601869386806785
X	0.01781414499214963	1537	1.537	0.22630650544083478
X	0.017380937054850797	1386	1.386	0.23232894421108102
X	0.017685839571296103	2626	2.626	0.18884709990175133
X	0.01755384527016598	2537	2.537	0.19055364748030848
X	0.018499426471003264	7342	7.342	0.13607592485797979
X	0.017600759980595827	3712	3.712	0.16799943365380604
X	0.017788849676997318	10884	10.884	0.11779307991518394
X	0.017554642725921596	3888	3.888	0.16528061225148807
X	0.017844043814113173	25085	25.085	0.08926743552144739
X	0.017660807909943572	2175	2.175	0.200994312543646
X	0.0175649273988386	3480	3.48	0.17153616840611657
X	0.017689232946611712	4548	4.548	0.15726407144963694
X	0.017266640588617838	535	0.535	0.31838408872099905
X	0.018721877945437053	15799	15.799	0.10582134329683737
X	0.017431823522390467	1341	1.341	0.23512817927353216
X	0.017711850678673712	8733	8.733	0.12658050597943202
X	0.017510646461829378	1213	1.213	0.24348962433134325
X	0.017772132366630118	4223	4.223	0.16145053404998827
X	0.017788802140035795	3012	3.012	0.18075788273899673
X	0.017793429221533895	1816	1.816	0.21398398462597248
X	0.017255050446974927	3187	3.187	0.17559469456535148
X	0.017742954700027888	6869	6.869	0.13720738136086413
X	0.017095062771593877	508	0.508	0.3228511436617902
X	0.01727098977190886	1024	1.024	0.25645180302002407
X	0.017324758741808922	1466	1.466	0.22777763273298976
X	0.017784468889878644	7855	7.855	0.13131013169462513
X	0.017246687048900693	1471	1.471	0.22717698775185421
X	0.01767789748050704	5637	5.637	0.14637296113063605
X	0.018207936237475807	10384	10.384	0.12058649762275149
X	0.0176308668106115	5504	5.504	0.1474116293392373
X	0.017598456434189357	3292	3.292	0.1748523943083405
X	0.018443002968802604	7574	7.574	0.13453505020142542
X	0.01776795255984572	1812	1.812	0.21403907439498046
X	0.01781736381328488	10096	10.096	0.12084574646331354
X	0.01747417100729482	2483	2.483	0.19163431882740609
X	0.017689226780648076	8149	8.149	0.12947964817158436
X	0.01770993085478838	2185	2.185	0.20087311210041725
X	0.0177262701913744	5853	5.853	0.144681481398768
X	0.017802915747147617	6061	6.061	0.14321294783758362
X	0.017703563082156797	2719	2.719	0.18673131620331482
X	0.0175971164756395	8018	8.018	0.129954648502563
X	0.018293684310081926	6301	6.301	0.1426583621331236
X	0.018638904161500765	12171	12.171	0.1152651547328921
X	0.017765703744179233	3036	3.036	0.18020225285521638
X	0.017610355791964433	815	0.815	0.27852884219962815
X	0.01762460592983916	2144	2.144	0.20182031426790792
X	0.017798721932168634	2172	2.172	0.20160888406649344
X	0.017389646607089682	5038	5.038	0.15112774343592694
X	0.017765718966507787	3681	3.681	0.1689944038495717
X	0.01732159481347904	1997	1.997	0.20546440910670452
X	0.017905709782274702	1845	1.845	0.21330370066427973
X	0.01765135638235618	5566	5.566	0.1469191116896662
X	0.0178419760640192	1012	1.012	0.26026802307117325
X	0.01861651926597246	11236	11.236	0.11833019617847618
X	0.017743535089584647	2661	2.661	0.188219706436557
X	0.01778284173371164	19454	19.454	0.09705041956518631
X	0.018115104077384946	5031	5.031	0.15327176977413162
X	0.01780876781433048	3610	3.61	0.1702324004870236
X	0.01731365087134563	1033	1.033	0.2559152176645987
X	0.018897969638538025	6548	6.548	0.1423755753345472
X	0.017808605327642803	5151	5.151	0.15120963083919226
X	0.017796063379680212	2412	2.412	0.19467740419296284
X	0.018807213831701752	12167	12.167	0.11562373246209455
X	0.01775322865652474	8685	8.685	0.1269119473358366
X	0.01745171829262021	2597	2.597	0.18870732053532013
X	0.01748608078682677	1847	1.847	0.21154780737199227
X	0.017604302149155975	3286	3.286	0.17497812222860942
X	0.01723655795229797	800	0.8	0.2782617543977368
X	0.017462342747585083	1383	1.383	0.23285922231476294
X	0.017411497099698883	570	0.57	0.3125986415508206
X	0.016991926652722877	1184	1.184	0.24301344798837493
X	0.017257858680089964	13504	13.504	0.10851959385128153
X	0.01767183291088136	1668	1.668	0.21963178106746356
X	0.01739762272883924	5633	5.633	0.14562973275343535
X	0.017476103709095796	1216	1.216	0.2431291393798906
X	0.017949489551687483	5624	5.624	0.14723201761065532
X	0.017432104580848793	2384	2.384	0.19409507867118259
X	0.0177382532497151	3358	3.358	0.17415798304290253
X	0.01782060470170403	4462	4.462	0.15865884568273939
X	0.017364894210773422	1447	1.447	0.22894677847395176
X	0.01733654480751807	1518	1.518	0.22519751382844094
X	0.017541542124240892	5833	5.833	0.14434173838195427
X	0.017019002848186082	1299	1.299	0.235744545601865
X	0.017595097952208434	1728	1.728	0.21674511439492794
X	0.01734809934726497	2605	2.605	0.188140108858559
X	0.017657179489405588	2741	2.741	0.18606760406980133
X	0.01862807810092867	5735	5.735	0.1480965276553346
X	0.01737783140306131	2388	2.388	0.19378511677597024
X	0.017630374486379267	2028	2.028	0.20561960894567663
X	0.018645781527621707	7425	7.425	0.13592355073621998
X	0.01803646120694204	7624	7.624	0.13324660368606203
X	0.017587646968966066	2544	2.544	0.19050083233671283
X	0.01785923378873179	23692	23.692	0.09100956380052012
X	0.018716356442904508	61711	61.711	0.06718715339431204
X	0.01809607560178022	4509	4.509	0.15891615751280164
X	0.017933485249052313	7724	7.724	0.13241610578406265
X	0.016458393547398024	620	0.62	0.298308240896705
X	0.017715515792277262	6060	6.06	0.142986068719122
X	0.017829769215471712	45071	45.071	0.07340903401921936
X	0.017834911220080536	137512	137.512	0.050618598647521024
X	0.017770305973770825	3245	3.245	0.17626255786664896
X	0.017889785636896378	147821	147.821	0.04946404936574983
X	0.017745365387813794	27205	27.205	0.08672521902064037
X	0.01778064746326872	22424	22.424	0.09255747438445475
X	0.018382104875478176	33857	33.857	0.08157972844482318
X	0.017631204704027195	2441	2.441	0.19330276643933172
X	0.017618195762945293	7162	7.162	0.13499231283418214
X	0.01787460294797622	212781	212.781	0.043796007281764096
X	0.018070409548562476	558154	558.154	0.0318716576189573
X	0.01783159864288981	6288	6.288	0.1415443643605111
X	0.017878927077190975	43591	43.591	0.07429875565484434
X	0.017711435421922037	22365	22.365	0.09251843670251846
X	0.017832788618954862	40545	40.545	0.07604908369143333
X	0.018688532556348714	77164	77.164	0.06233330528032903
X	0.017695002385512547	61564	61.564	0.06599452874709509
X	0.01788758582524319	53822	53.822	0.0692676785535294
X	0.01781364124570977	16211	16.211	0.1031923807010107
X	0.01734568675180203	3877	3.877	0.1647777491149577
X	0.017965455225081167	175166	175.166	0.04680901163419712
X	0.017525520957214977	7055	7.055	0.13543304389811214
X	0.01758893711289178	3004	3.004	0.18023808147367595
X	0.017782995929059013	23586	23.586	0.09101581834506528
X	0.017734755012743474	71106	71.106	0.06294670212364398
X	0.01886091176111517	267794	267.794	0.041297200636562455
X	0.01794129564614337	105073	105.073	0.05547801200300133
X	0.017583000856608806	1768	1.768	0.21504874489587825
X	0.018127407454073798	29066	29.066	0.08543781683930252
X	0.018667492201630383	12638	12.638	0.11388570357816143
X	0.017852728413876525	36134	36.134	0.07905503210132091
X	0.018026985720046674	61321	61.321	0.06649229077143805
X	0.017645683115812976	13330	13.33	0.10980011006862472
X	0.017843420629853904	90952	90.952	0.05810613025447649
X	0.016853288213946663	708	0.708	0.28766286182022427
X	0.01781061135565591	7257	7.257	0.13488818198617697
X	0.017774110152714136	105719	105.719	0.055192272174387
X	0.017972853806970484	51794	51.794	0.07027149553981192
X	0.018705049942712576	73675	73.675	0.06332077804121751
X	0.01770229453106158	6124	6.124	0.1424507674770024
X	0.017868784690013836	278589	278.589	0.04002920981533249
X	0.017837461952242593	14080	14.08	0.10820403801917765
X	0.01790350624306472	30792	30.792	0.08346416725232705
X	0.018805748902939262	100568	100.568	0.05718431720474165
X	0.017607803814006145	2424	2.424	0.19366784723004568
X	0.017830311359304272	111619	111.619	0.0542592266975351
X	0.018885796948379153	215306	215.306	0.04443171340778467
X	0.0180200393868082	439691	439.691	0.03447752271257093
X	0.017808818723357142	42463	42.463	0.07485281654277186
X	0.017842069966104022	30966	30.966	0.083212143537837
X	0.01766732068034181	6080	6.08	0.142699473551114
X	0.017794882356384902	6473	6.473	0.14008657095015253
X	0.018110436371047808	110718	110.718	0.054689450640908015
X	0.01798151095940804	10126	10.126	0.12109589452989557
X	0.017828117222080407	25580	25.58	0.08866148075531892
X	0.018174516256944388	41639	41.639	0.0758555782067369
X	0.017949636895681017	21472	21.472	0.09420220237479798
X	0.01750304776505373	20907	20.907	0.09424840001377495
X	0.017872077611352618	5822	5.822	0.1453341294491407
X	0.01803587122725069	119239	119.239	0.053281069477297154
X	0.01781084325462781	33476	33.476	0.08103083374390174
X	0.01796246519733107	30007	30.007	0.08427804521167254
X	0.018645033534032542	27625	27.625	0.08771764444728875
X	0.017877666394316188	120938	120.938	0.052874874415868744
X	0.017860684562443857	17051	17.051	0.10155845421767502
X	0.017846280845212573	216921	216.921	0.04349260044175182
X	0.017827989788312237	5806	5.806	0.14534779523367772
X	0.017609511140566855	42619	42.619	0.07448143146019416
X	0.0186697671514498	13796	13.796	0.11061021553561852
X	0.017824274459194718	116545	116.545	0.05347770216823176
X	0.017866356939589014	48588	48.588	0.0716422133122611
X	0.018708191268354143	11082	11.082	0.1190706026870954
X	0.017897392026752747	124818	124.818	0.0523404570306256
X	0.017945230665902905	161877	161.877	0.04803833716108495
X	0.01764785294325175	4221	4.221	0.16109875149703454
X	0.017936006178109053	144417	144.417	0.0498925623341633
X	0.01946962419881712	208826	208.826	0.04534447030674765
X	0.018784719655859476	365251	365.251	0.03718813807670316
X	0.017763775197823534	51098	51.098	0.07031432125063476
X	0.017273330616023447	2493	2.493	0.1906417147639805
X	0.017884647626162944	14469	14.469	0.107319990049809
X	0.018393142881100766	17203	17.203	0.10225485056982651
X	0.017879990217943436	26694	26.694	0.08749526286283414
X	0.017903212868968725	55859	55.859	0.06843515779172507
X	0.017486283619283456	2059	2.059	0.2040235925228791
X	0.017918430002065572	69149	69.149	0.06375359563062057
X	0.01787285347990865	152959	152.959	0.048888452051350846
X	0.01744850353153995	1539	1.539	0.22465005122209308
X	0.01781435511836309	8139	8.139	0.12983736278604321
X	0.017833342325261025	31874	31.874	0.08240091976037475
X	0.017996931491353316	14268	14.268	0.10804676979522385
X	0.01867460010187727	17098	17.098	0.10298374145980764
X	0.01835776896642728	15126	15.126	0.10666745532637607
X	0.01770486076267887	2963	2.963	0.18146252711348662
X	0.017634930903925147	25606	25.606	0.08831016043165309
X	0.018029112263606362	37586	37.586	0.07827974661122859
X	0.01793188395224265	21581	21.581	0.09401232320257594
X	0.017867626795946456	40990	40.99	0.07582218894810606
X	0.01770069501469082	10638	10.638	0.11849775188254554
X	0.019201789610559447	892394	892.394	0.027813887161380568
X	0.017832840168711155	9980	9.98	0.12134726487801475
X	0.017855650082980196	114801	114.801	0.05377866269074998
X	0.017741061453632058	2218	2.218	0.19998895886459891
X	0.01789477107405667	3971	3.971	0.16517415229496257
X	0.017619543599492346	4246	4.246	0.16069593403144314
X	0.017893126274979024	16906	16.906	0.10190960557673966
X	0.018720239080790044	6636	6.636	0.14129767134536844
X	0.01792831008251528	78944	78.944	0.06101080319106621
X	0.017612013255505665	7352	7.352	0.13380361132689322
X	0.017692201595558534	4135	4.135	0.16234370573204274
X	0.018003215095295892	295260	295.26	0.039359401413180145
X	0.01792591575210758	48289	48.289	0.07186945966511082
X	0.017980911575952937	35183	35.183	0.07995143793834292
X	0.018020207446331638	116565	116.565	0.053669869513857674
X	0.01898511381374452	315998	315.998	0.03916603644558703
X	0.017795343875787344	8501	8.501	0.12792206344384835
X	0.01782897773326488	10713	10.713	0.11850548332349631
X	0.017850408068291235	23191	23.191	0.09164516502996803
X	0.017324649065888657	1784	1.784	0.21334863341499488
X	0.017821173210846905	5567	5.567	0.14737993364805166
X	0.017738401021963925	17541	17.541	0.10037372464574745
X	0.017568489472181825	2598	2.598	0.18910300288383994
X	0.01865165506955685	5542	5.542	0.14985927180921557
X	0.017710340437153093	5609	5.609	0.146705756937747
X	0.017828117619061817	4897	4.897	0.15383617117001602
X	0.01788275881374365	262644	262.644	0.0408340395502789
X	0.01780134720049444	22004	22.004	0.0931787925080881
X	0.017855207664426675	80756	80.756	0.060468616944833305
X	0.017736208754835465	3925	3.925	0.16532570254521647
X	0.017756306351323598	5075	5.075	0.15181179928418112
X	0.017833957059810897	73748	73.748	0.06230159374398563
X	0.01887008692883002	558181	558.181	0.03233450998636147
X	0.018406700699606503	444228	444.228	0.03460376392823022
X	0.01867392576622098	13758	13.758	0.11072017774480175
X	0.017855685689277714	44681	44.681	0.07365765404250434
X	0.01782221555564151	2571	2.571	0.19067183161580578
X	0.017846525744096033	4906	4.906	0.1537949397561498
X	0.01814600464376006	442969	442.969	0.034472218018991904
X	0.017936950060165907	17556	17.556	0.1007181350626862
X	0.017692419356051994	10980	10.98	0.1172361716772816
X	0.018279357454363487	45263	45.263	0.07391612594127012
X	0.018397497315692256	712583	712.583	0.029555689609554795
X	0.017815297880929202	31302	31.302	0.082871843596918
X	0.01777683659458696	11143	11.143	0.11684696863429742
X	0.01927771689903679	19414	19.414	0.09976545577848876
X	0.01777768411136054	33008	33.008	0.08136144593089588
X	0.01773927367671228	3595	3.595	0.17024680869692432
X	0.019021797367689462	197356	197.356	0.04584940867410221
X	0.01774648362230804	14568	14.568	0.10679988702243104
X	0.01728534980552026	1795	1.795	0.2127508186885127
X	0.01882226539430125	62592	62.592	0.06699633126831017
X	0.01947817368079185	32057	32.057	0.08469838295130915
X	0.017235901742480752	984	0.984	0.25970459698265086
X	0.017883108419407948	73299	73.299	0.062485845875831454
X	0.01785975103945726	126425	126.425	0.05208117967897537
X	0.018937730142286207	237574	237.574	0.04303710052781699
X	0.018015385967923625	245316	245.316	0.041876711060252265
X	0.01783874651539324	187681	187.681	0.04563674647029248
X	0.017524744739159895	20957	20.957	0.09421228325831488
X	0.017864229038959546	17008	17.008	0.10165069297940443
X	0.017892782919611407	97732	97.732	0.05678239444111176
X	0.017945035552381867	199841	199.841	0.0447802564388981
X	0.017823202768050737	10211	10.211	0.12040351266131766
X	0.018291845669530533	104438	104.438	0.055949944581680344
X	0.018043511425094774	203157	203.157	0.0446165949417
X	0.018311665052412773	168544	168.544	0.04771682218135163
X	0.017935881373978016	90797	90.797	0.05823942446240144
X	0.01773336028475231	52719	52.719	0.06954639528145432
X	0.017552495325249988	12428	12.428	0.1121964736373669
X	0.017748945480136447	5372	5.372	0.14894028611613042
X	0.017837480858354467	15169	15.169	0.10555015034417446
X	0.01843791831683426	143151	143.151	0.050501680905906225
X	0.01787867745896203	101149	101.149	0.05612089632575729
X	0.018047469835523806	370154	370.154	0.036532379397160336
X	0.018772517423506753	114470	114.47	0.054736489970758764
X	0.017836094444774072	68861	68.861	0.0637444159093263
X	0.017788579324703475	14879	14.879	0.10613431965685638
X	0.018945712929546228	171234	171.234	0.04800726708483387
X	0.017649920593582898	6299	6.299	0.14097985162846538
X	0.017769187633134675	11159	11.159	0.11677434283422698
X	0.01788788111105932	182511	182.511	0.04610591214215052
X	0.017769153660762455	42868	42.868	0.0745609055017732
X	0.01773970296733809	6916	6.916	0.13688749823923108
X	0.017805329647332592	18475	18.475	0.09877685412649648
X	0.018453067378532407	418982	418.982	0.03531487992986808
X	0.017529209302013515	24834	24.834	0.08903727979728482
X	0.018706657953157128	89205	89.205	0.05941129277177751
X	0.017613384575656317	1703	1.703	0.21787599823556741
X	0.01764544388858884	3948	3.948	0.16472207887104148
X	0.017788453408259932	5290	5.29	0.1498169114134604
X	0.017661622832093958	7907	7.907	0.13071927257099156
X	0.017967658886413	26392	26.392	0.08797104085415045
X	0.017827718198882857	47586	47.586	0.07208952878972293
X	0.01747372301683852	4248	4.248	0.16022624187912024
X	0.018090224669317097	170769	170.769	0.047316394249948705
X	0.01761970034758894	7608	7.608	0.13230492312198505
X	0.01876689096354539	62763	62.763	0.06686972628335931
X	0.017809899880195313	10411	10.411	0.1195977503315191
X	0.017884758565953562	75779	75.779	0.06179851946557639
X	0.01756583223697852	7579	7.579	0.13233831719256894
X	0.017816126478059396	22291	22.291	0.09280283533345877
X	0.01803396287824752	30171	30.171	0.08423653396036987
X	0.017858640584558116	26943	26.943	0.08719016146885965
X	0.01795740033530722	77881	77.881	0.06132025918352084
X	0.01783940566757621	601381	601.381	0.030955908872204247
X	0.01789171352882146	8982	8.982	0.12582288194779018
X	0.01776271587069217	4018	4.018	0.16412187838436385
X	0.01785325396986637	65036	65.036	0.06499119466484404
X	0.01784038975380113	20032	20.032	0.09621143129559453
X	0.019108628009828047	416497	416.497	0.03579913580274884
X	0.017493401959335582	2234	2.234	0.19857765090789625
X	0.018211147950314685	127759	127.759	0.05223744653060355
X	0.01920031896827995	330368	330.368	0.03873499371051557
X	0.017711605363167196	3898	3.898	0.1656298838903254
X	0.017746315095356558	276349	276.349	0.04004516588104458
X	0.017749384904645053	19073	19.073	0.0976310813532259
X	0.017986904457304224	529661	529.661	0.032383178759495895
X	0.017985451612226314	9255	9.255	0.12479053350773224
X	0.017795782826160712	16636	16.636	0.10227183818270455
X	0.017754104216396654	9458	9.458	0.12335776296686349
X	0.017666171170849322	4001	4.001	0.16405568866501208
X	0.017786708683413092	11146	11.146	0.11685810815457057
X	0.017892357493696092	22411	22.411	0.09276883664771547
X	0.01782950556545814	5978	5.978	0.14394429700273237
X	0.01913738619876277	118173	118.173	0.05450731466214277
X	0.017728150628363595	16501	16.501	0.10241990993514186
X	0.017776677246309505	6370	6.37	0.14078953839930522
X	0.01871095666002061	20159	20.159	0.09754589608904214
X	0.018754450456116244	261180	261.18	0.04156439612221249
X	0.01903379804153718	125278	125.278	0.053360130348951366
X	0.01879168931298987	322181	322.181	0.03878126455085047
X	0.017843755376665698	15224	15.224	0.10543524911712164
X	0.017852574237038878	9589	9.589	0.12302003408638201
X	0.01851653447193098	4827	4.827	0.1565407714476265
X	0.017436445399754683	4349	4.349	0.15886303264619037
X	0.018125195363500237	198521	198.521	0.04502897702608338
X	0.01871723806678143	8957	8.957	0.12784774237032873
X	0.017869441984559396	22535	22.535	0.09255882003621191
X	0.01780941822907818	29236	29.236	0.08477034029133795
X	0.017947292959910908	30076	30.076	0.08418982858732149
X	0.017788141251970566	10541	10.541	0.11905553346810338
X	0.01783573483419555	11602	11.602	0.11541246458342679
X	0.017851957225402688	246910	246.91	0.041659659825323915
X	0.018001539276653298	42475	42.475	0.07511478323025787
X	0.017818260695873202	28036	28.036	0.08597715810283461
X	0.017845934608572384	24882	24.882	0.08951270276805136
X	0.019010432183343043	136285	136.285	0.051861846717551306
X	0.018791417662823294	121435	121.435	0.05368734031275952
X	0.018705006863725452	357480	357.48	0.037402621369793394
X	0.018234817912305638	101157	101.157	0.056489599622457566
X	0.017727010707832294	5948	5.948	0.14390907523215174
X	0.017874515643145024	42178	42.178	0.07511317156434828
X	0.017939630604868403	35188	35.188	0.07988642215117453
X	0.017688669800296448	7032	7.032	0.1359999569654801
X	0.017811080543895314	14991	14.991	0.10591396366614661
X	0.01782001828414757	5195	5.195	0.15081372416741998
X	0.018143143117956704	28687	28.687	0.0858372504231114
X	0.01779007912101825	26394	26.394	0.08767805136106849
X	0.0168784815588302	4731	4.731	0.15280121533498156
X	0.017826708911033808	19534	19.534	0.09699737840739203
X	0.017795970792348167	7529	7.529	0.13320733172831783
X	0.017715630354196443	38899	38.899	0.07693773398108615
X	0.01775356658501655	7899	7.899	0.130989906807515
X	0.01799296496078063	26868	26.868	0.08748947253000884
X	0.017628447778761644	9113	9.113	0.12459986383764576
X	0.018381818296252832	17932	17.932	0.10082925992628367
X	0.017831113717061812	6324	6.324	0.14127398730171803
X	0.01777084935821019	10584	10.584	0.11885554599002017
X	0.01786278479834219	23714	23.714	0.09098744087780161
X	0.018014769532165697	112297	112.297	0.054335898385963574
X	0.01780503951850469	7801	7.801	0.13166314396789391
X	0.017278939004626376	1967	1.967	0.20633405053573164
X	0.017572145203044367	8551	8.551	0.12713621923742524
X	0.0178761457106045	111381	111.381	0.05434433237123286
X	0.019129758439748838	508137	508.137	0.03351528284933188
X	0.017682332437982014	43561	43.561	0.0740424140873781
X	0.017763451109223907	7356	7.356	0.13416169653444524
X	0.01784433256167747	144847	144.847	0.04975807853834957
X	0.01957074594029107	181163	181.163	0.04762620296438485
X	0.01769312681072547	2500	2.5	0.19199423742371044
X	0.017721692078809016	9242	9.242	0.12423569886029855
X	0.017868125475341733	34375	34.375	0.08040420500952561
X	0.017853070056857667	95991	95.991	0.05708133622957446
X	0.01775417076644787	153346	153.346	0.048738928842256196
X	0.017927522439468706	185999	185.999	0.04584970599476594
X	0.017765893936745897	17050	17.05	0.10138045294720609
X	0.017413937421816825	4823	4.823	0.1534122027866109
X	0.017646681194156938	3241	3.241	0.17592517953860726
X	0.018765250571554246	55267	55.267	0.0697636978294826
X	0.017876997461562252	12314	12.314	0.113230947925359
X	0.017844960519426085	16387	16.387	0.10288183757170609
X	0.018265565174916815	102303	102.303	0.056309488425004606
X	0.019251682795461172	495522	495.522	0.033868964496505165
X	0.017724077253666416	10606	10.606	0.11866901489474146
X	0.01761293680485481	2307	2.307	0.19690710819192392
X	0.01896347150765851	13364	13.364	0.11237260762002692
X	0.017890630851225182	74668	74.668	0.06211031216076436
X	0.017847032704625003	40233	40.233	0.07626545531824859
X	0.017512140461860337	3564	3.564	0.1700071468530574
X	0.01726455137548407	3939	3.939	0.16365267390002874
X	0.01795455864040206	49197	49.197	0.07146258213688327
X	0.017586832918477666	3019	3.019	0.17993190370364656
X	0.01793563653320124	41417	41.417	0.07565646618692719
X	0.017830547748890587	5556	5.556	0.14750298728722433
X	0.017935299070502692	16621	16.621	0.1025692447800975
X	0.017858523003571627	18026	18.026	0.09968934065872241
X	0.01772114905430017	7265	7.265	0.13461250929285828
X	0.017622934733343885	7687	7.687	0.1318581922172536
X	0.01802308768794093	163822	163.822	0.04791656281117417
X	0.018004635451680594	336169	336.169	0.03769428906776305
X	0.0187894219441701	101588	101.588	0.05697578746782225
X	0.01781956534343032	147224	147.224	0.049465928390895794
X	0.01788625552315095	21971	21.971	0.09337340627861113
X	0.01773834403589874	16557	16.557	0.102323913446818
X	0.01875772933828787	15983	15.983	0.1054809393025367
X	0.018073664953957142	139163	139.163	0.05064160734475738
X	0.01900520674459633	46504	46.504	0.074209714846964
X	0.01779556272423027	41068	41.068	0.07567214722911689
X	0.017830061208899687	13979	13.979	0.10844900614858961
X	0.01923114019326089	1109456	1109.456	0.025880042402236597
X	0.018201637119560715	22312	22.312	0.09343809671077444
X	0.01783657005495655	66377	66.377	0.06453043231717011
X	0.018775634201020523	170303	170.303	0.04795023857719429
X	0.01803181606999457	48046	48.046	0.07213190642421846
X	0.018143396249770734	41147	41.147	0.07611322718789475
X	0.018792616793769718	586200	586.2	0.03176731294145934
X	0.017905513683088653	124009	124.009	0.052461961751031176
X	0.017880139240249723	91225	91.225	0.0580878998294404
X	0.01768847542698661	8189	8.189	0.12926665490138683
X	0.017849616727579787	37572	37.572	0.07802878788090872
X	0.01770050441579804	8698	8.698	0.12672299049346503
X	0.017899407355088263	193964	193.964	0.045189670362833084
X	0.019171979437659185	1064756	1064.756	0.026210317761680837
X	0.018217575840689136	98795	98.795	0.05691829386251709
X	0.01776071467812204	168700	168.7	0.047218818744772725
X	0.017860502536596722	65445	65.445	0.06486429976992233
X	0.017804360685949327	28433	28.433	0.08555286666226866
X	0.01944108781393062	37649	37.649	0.08022733614541326
X	0.01759550536524335	9123	9.123	0.12447668770453561
X	0.01929065651300467	109949	109.949	0.055982450932255894
X	0.018356890406567506	385536	385.536	0.03624471807374316
X	0.017848246880185697	78636	78.636	0.06099928111936043
X	0.01872756341163841	731161	731.161	0.029477416605052226
X	0.018801123110052186	157330	157.33	0.0492558045005846
X	0.01788338759404039	38336	38.336	0.07755580874244815
X	0.017818462291518067	8161	8.161	0.12973055662450164
X	0.01861862011454401	66832	66.832	0.06531120214750653
X	0.0178779635094939	135517	135.517	0.050906677825249484
X	0.01760324262097121	8571	8.571	0.12711214864448997
X	0.018548790968139474	519294	519.294	0.03293321282991917
X	0.0179671451672572	33281	33.281	0.08142558795785304
X	0.01779009211328953	34088	34.088	0.08051168053787491
X	0.017589661365359232	6007	6.007	0.14306496145817746
X	0.017833840543046604	7289	7.289	0.1347490285769369
X	0.0177298950394032	1898	1.898	0.21060563424327255
X	0.0175007713942628	7119	7.119	0.13496238146936734
X	0.018622091613094013	23313	23.313	0.09278478485505082
X	0.017782383566128383	43212	43.212	0.07438097532547028
X	0.019281541431290905	3102100	3102.1	0.018386341350895626
X	0.017791701207012116	12877	12.877	0.11137838772842186
X	0.01807883429355857	135657	135.657	0.051079041429426
X	0.018679977709256064	7982	7.982	0.13276640937771142
X	0.017830868020073335	23618	23.618	0.09105625595924309
X	0.018507389170462282	8472	8.472	0.12975369578616455
X	0.017977478535010426	191428	191.428	0.04545424400530471
X	0.017868813898986816	46874	46.874	0.07250833011840083
X	0.019057313708781172	842380	842.38	0.028282505395129873
X	0.017640085594144445	8291	8.291	0.1286168664070612
X	0.018269970770239147	177142	177.142	0.046896320546940005
X	0.01782930048379234	41647	41.647	0.07536739891954404
X	0.01794230606509291	153534	153.534	0.048890509744392345
X	0.017975538525024	322832	322.832	0.03818578982284875
X	0.017788737288645128	24377	24.377	0.0900302151493286
X	0.017843774287622953	11469	11.469	0.11587428199850427
X	0.01872257303227902	112851	112.851	0.05494818779318046
X	0.01872649900099526	13601	13.601	0.11124877829745872
X	0.017794902635541166	27434	27.434	0.08656363675371406
X	0.017822197312891825	189821	189.821	0.04545053470264105
X	0.01780371406283251	18758	18.758	0.09827461694778084
X	0.0193089384788038	78089	78.089	0.06276576780083373
X	0.01777915035585594	32981	32.981	0.08138587949960467
X	0.018842123931559904	131644	131.644	0.052308981260763616
X	0.018977425840100915	318848	318.848	0.039043721109019366
X	0.01786360495458397	34551	34.551	0.08026067874389682
X	0.019051424050953787	10263	10.263	0.12289976098818195
X	0.017871636502374753	111258	111.258	0.05435978010911929
X	0.01779763134226984	76385	76.385	0.06153441004746194
X	0.017911617423326802	313573	313.573	0.038512234248665095
X	0.017725000755095505	41242	41.242	0.07546556851950015
X	0.017779383758778078	32505	32.505	0.08178158272402235
X	0.01810774172400279	35102	35.102	0.08020057398878579
X	0.017672633596171525	159527	159.527	0.04802739432509539
X	0.017940260478153978	105464	105.464	0.0554083012072068
X	0.018083584283429998	28802	28.802	0.08562895083214955
X	0.01751112912032042	6439	6.439	0.13958280080189941
X	0.018701585657762816	12231	12.231	0.11520521927235904
X	0.017619972632829804	57660	57.66	0.06735608926536718
X	0.018111731248112252	95349	95.349	0.057484127089436676
X	0.01780494679097575	113341	113.341	0.053957424704016786
X	0.01772663006032712	13946	13.946	0.10832420411707096
X	0.01797372830636155	26001	26.001	0.08841976855053293
X	0.01784534859670446	32577	32.577	0.08182223082565641
X	0.018111656511523005	88936	88.936	0.058833794629021635
X	0.017883206881034397	22461	22.461	0.0926841427891331
X	0.017876975346880424	76501	76.501	0.06159455326275099
X	0.017683127928503548	11425	11.425	0.11567361103688187
X	0.017944108008673174	30547	30.547	0.08374992689001644
X	0.017747426849955963	2622	2.622	0.1891621475147038
X	0.017774325266966255	9063	9.063	0.12517195859939284
X	0.01840728155050486	146826	146.826	0.05004901821612323
X	0.01781947287000626	19456	19.456	0.09711368452555838
X	0.01792441644533342	73909	73.909	0.06236140621855415
X	0.01774583832514909	2650	2.65	0.18848793168503872
X	0.017765869654719397	6845	6.845	0.13742666466300227
X	0.017841014017071503	27010	27.01	0.08708933681520317
X	0.017844629345281564	56478	56.478	0.06810976902461299
X	0.018727403085684415	29332	29.332	0.08610835527054883
X	0.017609393498625996	4492	4.492	0.15767695227125894
X	0.017739179509599308	7129	7.129	0.13550905124218948
X	0.017831570689866415	26665	26.665	0.08744789044129088
X	0.017935991049783412	23154	23.154	0.09184026253383619
X	0.017796123220442633	111376	111.376	0.05426393234331068
X	0.017726693725220483	6798	6.798	0.1376413299520194
X	0.01725570254858704	11899	11.899	0.11318982638297483
X	0.017848399402432418	50933	50.933	0.0705017676884695
X	0.017723657652426682	9442	9.442	0.12335680682930127
X	0.018160793455408726	203588	203.588	0.044681479769736576
X	0.01777355972460183	13010	13.01	0.11095981069036907
X	0.017825495844948126	66602	66.602	0.06444434037809105
X	0.017931052327210817	152096	152.096	0.0490338501061153
X	0.01871869479916586	26928	26.928	0.088584381020693
X	0.017947747399124577	130355	130.355	0.05163697013070812
X	0.018042898216292205	140276	140.276	0.050478639574519195
X	0.01744521297853733	1722	1.722	0.21637863474146884
X	0.0178756581031189	52067	52.067	0.07002178239296815
X	0.017865692081053937	16462	16.462	0.10276512363539787
X	0.018080565172589644	122131	122.131	0.05290077439548998
X	0.017773143945934294	142824	142.824	0.0499253389574271
X	0.017823832119575538	26644	26.644	0.08745820360096686
X	0.018716999138248935	65101	65.101	0.06600084871179926
X	0.01784713020466451	17662	17.662	0.10034818085974734
X	0.0180573772354055	24584	24.584	0.0902264855347219
X	0.017798735735573327	4520	4.52	0.15791264456031937
X	0.017667483596184134	9539	9.539	0.12280723386045966
X	0.01789811122637248	172506	172.506	0.046989518971227
X	0.019184471971708035	479641	479.641	0.034198824893332425
X	0.01751329301927744	20571	20.571	0.09477725726621297
X	0.017940827992849635	19240	19.24	0.09769652990722798
X	0.017973675916833066	28271	28.271	0.08598682475857294
X	0.018117754497899125	74272	74.272	0.06248269461758166
X	0.017814887371069114	42183	42.183	0.0750265896616867
X	0.017733582691449454	7800	7.8	0.1314923924429017
X	0.018330715521386837	45169	45.169	0.07403657401748324
X	0.01795207921955305	144873	144.873	0.049855043652161944
X	0.017799811345622807	9141	9.141	0.1248745386737992
X	0.018010202686704974	41301	41.301	0.07583202924607958
X	0.017704394825956066	9890	9.89	0.12142131487377679
time for making epsilon is 2.5064196586608887
epsilons are
[0.25304762808118597, 0.08672877478523111, 0.16219497541419234, 0.09966949945857785, 0.07882494350011197, 0.07201989641443039, 0.06892989384932575, 0.06810720134126819, 0.10999928897327053, 0.08328617485677335, 0.12622331515841576, 0.04999068205871371, 0.11143741993027789, 0.037386376146589816, 0.08680741590024389, 0.07014193293824621, 0.06106171280071698, 0.08538649518178673, 0.04925577897873781, 0.05504041881576053, 0.07262666061486289, 0.0395355106626757, 0.1120398527677893, 0.07136621937686756, 0.1831289568919604, 0.05992792866921374, 0.06408352445163934, 0.11526742506694468, 0.059022106557911805, 0.0227038243239826, 0.10874225594090364, 0.027494763229379465, 0.08115883718806853, 0.11387363503991149, 0.09329202906361586, 0.04455265785955417, 0.057527400753445895, 0.22638703298458013, 0.14758868673036063, 0.16714692665531156, 0.15992303586793769, 0.2114707253459941, 0.2874822476752166, 0.14409237406909364, 0.2631746497480311, 0.25320083579453656, 0.1681812090076909, 0.16436358838988174, 0.15272374076329895, 0.10768298539650613, 0.10253434931464936, 0.20062702568909466, 0.15168035905115637, 0.18235794762357052, 0.1297229399194268, 0.12781023497560187, 0.18808624799341084, 0.14713567908942196, 0.196557507089223, 0.16134365666438646, 0.125955679655517, 0.23476832626400956, 0.11183000632988649, 0.12284131209715134, 0.18414709037928775, 0.19762158729902446, 0.19490262783771503, 0.15255424640183965, 0.15270661896916043, 0.16933054079312862, 0.11694576644908351, 0.14756092007058527, 0.16624795535849288, 0.16313014420100957, 0.15476348865331255, 0.11719500387533374, 0.2782606978867221, 0.2492520968585174, 0.1735077620690183, 0.0962158887566917, 0.17678787357551326, 0.21307662126382423, 0.14921707989652652, 0.19789434362649178, 0.19550299210847985, 0.22627301555837437, 0.1734121366960488, 0.28760768663484626, 0.14337970824212476, 0.16284677397269962, 0.14924543187604755, 0.23396559722009322, 0.22940788299996306, 0.1363800650772266, 0.14913129465726385, 0.14888679364688162, 0.15539941596898765, 0.12167158736189083, 0.1132536449804983, 0.1683097016576429, 0.1142102857984947, 0.1435188970200634, 0.14364839861030276, 0.20603358247989284, 0.1362636767434884, 0.19423543563881024, 0.15562981817987126, 0.27455900527106236, 0.1784213228795893, 0.23818590748032006, 0.17854752636593416, 0.1788302705976201, 0.1874284282947764, 0.1631216183447497, 0.14993553160601547, 0.19274822899854635, 0.19132835556860878, 0.26330133136024975, 0.12179116936862248, 0.2552627055048028, 0.11310370543866673, 0.20506366585705008, 0.20012034293543945, 0.17746828144711987, 0.21045630755527, 0.172205952714373, 0.25729940422637526, 0.1523485503510259, 0.11101377292165075, 0.19712901249827797, 0.12990114199598377, 0.20436848503432284, 0.14455275427929715, 0.2159714514543939, 0.16817172013879475, 0.1871486843986362, 0.19490001717410171, 0.2127538861039367, 0.21440761035699962, 0.22246726224811095, 0.17535021653464186, 0.2399575084583419, 0.12620641596489512, 0.10705076887097675, 0.12280525201649418, 0.19615400407449807, 0.18601869386806785, 0.22630650544083478, 0.23232894421108102, 0.18884709990175133, 0.19055364748030848, 0.13607592485797979, 0.16799943365380604, 0.11779307991518394, 0.16528061225148807, 0.08926743552144739, 0.200994312543646, 0.17153616840611657, 0.15726407144963694, 0.31838408872099905, 0.10582134329683737, 0.23512817927353216, 0.12658050597943202, 0.24348962433134325, 0.16145053404998827, 0.18075788273899673, 0.21398398462597248, 0.17559469456535148, 0.13720738136086413, 0.3228511436617902, 0.25645180302002407, 0.22777763273298976, 0.13131013169462513, 0.22717698775185421, 0.14637296113063605, 0.12058649762275149, 0.1474116293392373, 0.1748523943083405, 0.13453505020142542, 0.21403907439498046, 0.12084574646331354, 0.19163431882740609, 0.12947964817158436, 0.20087311210041725, 0.144681481398768, 0.14321294783758362, 0.18673131620331482, 0.129954648502563, 0.1426583621331236, 0.1152651547328921, 0.18020225285521638, 0.27852884219962815, 0.20182031426790792, 0.20160888406649344, 0.15112774343592694, 0.1689944038495717, 0.20546440910670452, 0.21330370066427973, 0.1469191116896662, 0.26026802307117325, 0.11833019617847618, 0.188219706436557, 0.09705041956518631, 0.15327176977413162, 0.1702324004870236, 0.2559152176645987, 0.1423755753345472, 0.15120963083919226, 0.19467740419296284, 0.11562373246209455, 0.1269119473358366, 0.18870732053532013, 0.21154780737199227, 0.17497812222860942, 0.2782617543977368, 0.23285922231476294, 0.3125986415508206, 0.24301344798837493, 0.10851959385128153, 0.21963178106746356, 0.14562973275343535, 0.2431291393798906, 0.14723201761065532, 0.19409507867118259, 0.17415798304290253, 0.15865884568273939, 0.22894677847395176, 0.22519751382844094, 0.14434173838195427, 0.235744545601865, 0.21674511439492794, 0.188140108858559, 0.18606760406980133, 0.1480965276553346, 0.19378511677597024, 0.20561960894567663, 0.13592355073621998, 0.13324660368606203, 0.19050083233671283, 0.09100956380052012, 0.06718715339431204, 0.15891615751280164, 0.13241610578406265, 0.298308240896705, 0.142986068719122, 0.07340903401921936, 0.050618598647521024, 0.17626255786664896, 0.04946404936574983, 0.08672521902064037, 0.09255747438445475, 0.08157972844482318, 0.19330276643933172, 0.13499231283418214, 0.043796007281764096, 0.0318716576189573, 0.1415443643605111, 0.07429875565484434, 0.09251843670251846, 0.07604908369143333, 0.06233330528032903, 0.06599452874709509, 0.0692676785535294, 0.1031923807010107, 0.1647777491149577, 0.04680901163419712, 0.13543304389811214, 0.18023808147367595, 0.09101581834506528, 0.06294670212364398, 0.041297200636562455, 0.05547801200300133, 0.21504874489587825, 0.08543781683930252, 0.11388570357816143, 0.07905503210132091, 0.06649229077143805, 0.10980011006862472, 0.05810613025447649, 0.28766286182022427, 0.13488818198617697, 0.055192272174387, 0.07027149553981192, 0.06332077804121751, 0.1424507674770024, 0.04002920981533249, 0.10820403801917765, 0.08346416725232705, 0.05718431720474165, 0.19366784723004568, 0.0542592266975351, 0.04443171340778467, 0.03447752271257093, 0.07485281654277186, 0.083212143537837, 0.142699473551114, 0.14008657095015253, 0.054689450640908015, 0.12109589452989557, 0.08866148075531892, 0.0758555782067369, 0.09420220237479798, 0.09424840001377495, 0.1453341294491407, 0.053281069477297154, 0.08103083374390174, 0.08427804521167254, 0.08771764444728875, 0.052874874415868744, 0.10155845421767502, 0.04349260044175182, 0.14534779523367772, 0.07448143146019416, 0.11061021553561852, 0.05347770216823176, 0.0716422133122611, 0.1190706026870954, 0.0523404570306256, 0.04803833716108495, 0.16109875149703454, 0.0498925623341633, 0.04534447030674765, 0.03718813807670316, 0.07031432125063476, 0.1906417147639805, 0.107319990049809, 0.10225485056982651, 0.08749526286283414, 0.06843515779172507, 0.2040235925228791, 0.06375359563062057, 0.048888452051350846, 0.22465005122209308, 0.12983736278604321, 0.08240091976037475, 0.10804676979522385, 0.10298374145980764, 0.10666745532637607, 0.18146252711348662, 0.08831016043165309, 0.07827974661122859, 0.09401232320257594, 0.07582218894810606, 0.11849775188254554, 0.027813887161380568, 0.12134726487801475, 0.05377866269074998, 0.19998895886459891, 0.16517415229496257, 0.16069593403144314, 0.10190960557673966, 0.14129767134536844, 0.06101080319106621, 0.13380361132689322, 0.16234370573204274, 0.039359401413180145, 0.07186945966511082, 0.07995143793834292, 0.053669869513857674, 0.03916603644558703, 0.12792206344384835, 0.11850548332349631, 0.09164516502996803, 0.21334863341499488, 0.14737993364805166, 0.10037372464574745, 0.18910300288383994, 0.14985927180921557, 0.146705756937747, 0.15383617117001602, 0.0408340395502789, 0.0931787925080881, 0.060468616944833305, 0.16532570254521647, 0.15181179928418112, 0.06230159374398563, 0.03233450998636147, 0.03460376392823022, 0.11072017774480175, 0.07365765404250434, 0.19067183161580578, 0.1537949397561498, 0.034472218018991904, 0.1007181350626862, 0.1172361716772816, 0.07391612594127012, 0.029555689609554795, 0.082871843596918, 0.11684696863429742, 0.09976545577848876, 0.08136144593089588, 0.17024680869692432, 0.04584940867410221, 0.10679988702243104, 0.2127508186885127, 0.06699633126831017, 0.08469838295130915, 0.25970459698265086, 0.062485845875831454, 0.05208117967897537, 0.04303710052781699, 0.041876711060252265, 0.04563674647029248, 0.09421228325831488, 0.10165069297940443, 0.05678239444111176, 0.0447802564388981, 0.12040351266131766, 0.055949944581680344, 0.0446165949417, 0.04771682218135163, 0.05823942446240144, 0.06954639528145432, 0.1121964736373669, 0.14894028611613042, 0.10555015034417446, 0.050501680905906225, 0.05612089632575729, 0.036532379397160336, 0.054736489970758764, 0.0637444159093263, 0.10613431965685638, 0.04800726708483387, 0.14097985162846538, 0.11677434283422698, 0.04610591214215052, 0.0745609055017732, 0.13688749823923108, 0.09877685412649648, 0.03531487992986808, 0.08903727979728482, 0.05941129277177751, 0.21787599823556741, 0.16472207887104148, 0.1498169114134604, 0.13071927257099156, 0.08797104085415045, 0.07208952878972293, 0.16022624187912024, 0.047316394249948705, 0.13230492312198505, 0.06686972628335931, 0.1195977503315191, 0.06179851946557639, 0.13233831719256894, 0.09280283533345877, 0.08423653396036987, 0.08719016146885965, 0.06132025918352084, 0.030955908872204247, 0.12582288194779018, 0.16412187838436385, 0.06499119466484404, 0.09621143129559453, 0.03579913580274884, 0.19857765090789625, 0.05223744653060355, 0.03873499371051557, 0.1656298838903254, 0.04004516588104458, 0.0976310813532259, 0.032383178759495895, 0.12479053350773224, 0.10227183818270455, 0.12335776296686349, 0.16405568866501208, 0.11685810815457057, 0.09276883664771547, 0.14394429700273237, 0.05450731466214277, 0.10241990993514186, 0.14078953839930522, 0.09754589608904214, 0.04156439612221249, 0.053360130348951366, 0.03878126455085047, 0.10543524911712164, 0.12302003408638201, 0.1565407714476265, 0.15886303264619037, 0.04502897702608338, 0.12784774237032873, 0.09255882003621191, 0.08477034029133795, 0.08418982858732149, 0.11905553346810338, 0.11541246458342679, 0.041659659825323915, 0.07511478323025787, 0.08597715810283461, 0.08951270276805136, 0.051861846717551306, 0.05368734031275952, 0.037402621369793394, 0.056489599622457566, 0.14390907523215174, 0.07511317156434828, 0.07988642215117453, 0.1359999569654801, 0.10591396366614661, 0.15081372416741998, 0.0858372504231114, 0.08767805136106849, 0.15280121533498156, 0.09699737840739203, 0.13320733172831783, 0.07693773398108615, 0.130989906807515, 0.08748947253000884, 0.12459986383764576, 0.10082925992628367, 0.14127398730171803, 0.11885554599002017, 0.09098744087780161, 0.054335898385963574, 0.13166314396789391, 0.20633405053573164, 0.12713621923742524, 0.05434433237123286, 0.03351528284933188, 0.0740424140873781, 0.13416169653444524, 0.04975807853834957, 0.04762620296438485, 0.19199423742371044, 0.12423569886029855, 0.08040420500952561, 0.05708133622957446, 0.048738928842256196, 0.04584970599476594, 0.10138045294720609, 0.1534122027866109, 0.17592517953860726, 0.0697636978294826, 0.113230947925359, 0.10288183757170609, 0.056309488425004606, 0.033868964496505165, 0.11866901489474146, 0.19690710819192392, 0.11237260762002692, 0.06211031216076436, 0.07626545531824859, 0.1700071468530574, 0.16365267390002874, 0.07146258213688327, 0.17993190370364656, 0.07565646618692719, 0.14750298728722433, 0.1025692447800975, 0.09968934065872241, 0.13461250929285828, 0.1318581922172536, 0.04791656281117417, 0.03769428906776305, 0.05697578746782225, 0.049465928390895794, 0.09337340627861113, 0.102323913446818, 0.1054809393025367, 0.05064160734475738, 0.074209714846964, 0.07567214722911689, 0.10844900614858961, 0.025880042402236597, 0.09343809671077444, 0.06453043231717011, 0.04795023857719429, 0.07213190642421846, 0.07611322718789475, 0.03176731294145934, 0.052461961751031176, 0.0580878998294404, 0.12926665490138683, 0.07802878788090872, 0.12672299049346503, 0.045189670362833084, 0.026210317761680837, 0.05691829386251709, 0.047218818744772725, 0.06486429976992233, 0.08555286666226866, 0.08022733614541326, 0.12447668770453561, 0.055982450932255894, 0.03624471807374316, 0.06099928111936043, 0.029477416605052226, 0.0492558045005846, 0.07755580874244815, 0.12973055662450164, 0.06531120214750653, 0.050906677825249484, 0.12711214864448997, 0.03293321282991917, 0.08142558795785304, 0.08051168053787491, 0.14306496145817746, 0.1347490285769369, 0.21060563424327255, 0.13496238146936734, 0.09278478485505082, 0.07438097532547028, 0.018386341350895626, 0.11137838772842186, 0.051079041429426, 0.13276640937771142, 0.09105625595924309, 0.12975369578616455, 0.04545424400530471, 0.07250833011840083, 0.028282505395129873, 0.1286168664070612, 0.046896320546940005, 0.07536739891954404, 0.048890509744392345, 0.03818578982284875, 0.0900302151493286, 0.11587428199850427, 0.05494818779318046, 0.11124877829745872, 0.08656363675371406, 0.04545053470264105, 0.09827461694778084, 0.06276576780083373, 0.08138587949960467, 0.052308981260763616, 0.039043721109019366, 0.08026067874389682, 0.12289976098818195, 0.05435978010911929, 0.06153441004746194, 0.038512234248665095, 0.07546556851950015, 0.08178158272402235, 0.08020057398878579, 0.04802739432509539, 0.0554083012072068, 0.08562895083214955, 0.13958280080189941, 0.11520521927235904, 0.06735608926536718, 0.057484127089436676, 0.053957424704016786, 0.10832420411707096, 0.08841976855053293, 0.08182223082565641, 0.058833794629021635, 0.0926841427891331, 0.06159455326275099, 0.11567361103688187, 0.08374992689001644, 0.1891621475147038, 0.12517195859939284, 0.05004901821612323, 0.09711368452555838, 0.06236140621855415, 0.18848793168503872, 0.13742666466300227, 0.08708933681520317, 0.06810976902461299, 0.08610835527054883, 0.15767695227125894, 0.13550905124218948, 0.08744789044129088, 0.09184026253383619, 0.05426393234331068, 0.1376413299520194, 0.11318982638297483, 0.0705017676884695, 0.12335680682930127, 0.044681479769736576, 0.11095981069036907, 0.06444434037809105, 0.0490338501061153, 0.088584381020693, 0.05163697013070812, 0.050478639574519195, 0.21637863474146884, 0.07002178239296815, 0.10276512363539787, 0.05290077439548998, 0.0499253389574271, 0.08745820360096686, 0.06600084871179926, 0.10034818085974734, 0.0902264855347219, 0.15791264456031937, 0.12280723386045966, 0.046989518971227, 0.034198824893332425, 0.09477725726621297, 0.09769652990722798, 0.08598682475857294, 0.06248269461758166, 0.0750265896616867, 0.1314923924429017, 0.07403657401748324, 0.049855043652161944, 0.1248745386737992, 0.07583202924607958, 0.12142131487377679]
0.09104082645258761
Making ranges
torch.Size([47891, 2])
We keep 7.51e+06/6.82e+08 =  1% of the original kernel matrix.

torch.Size([2900, 2])
We keep 7.25e+04/1.14e+06 =  6% of the original kernel matrix.

torch.Size([13547, 2])
We keep 8.07e+05/2.79e+07 =  2% of the original kernel matrix.

torch.Size([47891, 2])
We keep 1.20e+07/7.55e+08 =  1% of the original kernel matrix.

torch.Size([49341, 2])
We keep 8.72e+06/7.17e+08 =  1% of the original kernel matrix.

torch.Size([10753, 2])
We keep 7.00e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([23589, 2])
We keep 2.20e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([25394, 2])
We keep 1.20e+07/3.25e+08 =  3% of the original kernel matrix.

torch.Size([34918, 2])
We keep 6.28e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([68076, 2])
We keep 1.67e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([58314, 2])
We keep 1.12e+07/9.87e+08 =  1% of the original kernel matrix.

torch.Size([90734, 2])
We keep 2.61e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([66168, 2])
We keep 1.36e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([102723, 2])
We keep 3.48e+07/2.95e+09 =  1% of the original kernel matrix.

torch.Size([69936, 2])
We keep 1.51e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([109040, 2])
We keep 3.40e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([72022, 2])
We keep 1.55e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([24483, 2])
We keep 4.47e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([34851, 2])
We keep 4.92e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([42200, 2])
We keep 3.50e+07/9.49e+08 =  3% of the original kernel matrix.

torch.Size([44916, 2])
We keep 9.06e+06/8.04e+08 =  1% of the original kernel matrix.

torch.Size([16964, 2])
We keep 2.20e+06/7.76e+07 =  2% of the original kernel matrix.

torch.Size([28536, 2])
We keep 3.56e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([234659, 2])
We keep 2.78e+08/2.02e+10 =  1% of the original kernel matrix.

torch.Size([107309, 2])
We keep 3.44e+07/3.71e+09 =  0% of the original kernel matrix.

torch.Size([26337, 2])
We keep 2.93e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([36296, 2])
We keep 4.75e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([657420, 2])
We keep 1.13e+09/1.32e+11 =  0% of the original kernel matrix.

torch.Size([184839, 2])
We keep 7.99e+07/9.49e+09 =  0% of the original kernel matrix.

torch.Size([49722, 2])
We keep 9.44e+06/7.43e+08 =  1% of the original kernel matrix.

torch.Size([49942, 2])
We keep 8.53e+06/7.12e+08 =  1% of the original kernel matrix.

torch.Size([99460, 2])
We keep 3.88e+07/2.69e+09 =  1% of the original kernel matrix.

torch.Size([68928, 2])
We keep 1.44e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([145856, 2])
We keep 8.46e+07/6.15e+09 =  1% of the original kernel matrix.

torch.Size([84149, 2])
We keep 2.06e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([48701, 2])
We keep 2.44e+07/8.20e+08 =  2% of the original kernel matrix.

torch.Size([48970, 2])
We keep 9.03e+06/7.48e+08 =  1% of the original kernel matrix.

torch.Size([253864, 2])
We keep 2.60e+08/2.23e+10 =  1% of the original kernel matrix.

torch.Size([113480, 2])
We keep 3.59e+07/3.90e+09 =  0% of the original kernel matrix.

torch.Size([195000, 2])
We keep 1.51e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([98747, 2])
We keep 2.70e+07/2.81e+09 =  0% of the original kernel matrix.

torch.Size([69579, 2])
We keep 4.02e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([55936, 2])
We keep 1.34e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([560965, 2])
We keep 8.23e+08/9.06e+10 =  0% of the original kernel matrix.

torch.Size([171107, 2])
We keep 6.55e+07/7.86e+09 =  0% of the original kernel matrix.

torch.Size([25951, 2])
We keep 3.33e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([36046, 2])
We keep 4.65e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([84228, 2])
We keep 7.79e+07/2.69e+09 =  2% of the original kernel matrix.

torch.Size([63884, 2])
We keep 1.48e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([6200, 2])
We keep 9.36e+05/8.09e+06 = 11% of the original kernel matrix.

torch.Size([18127, 2])
We keep 1.52e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([156007, 2])
We keep 6.25e+07/6.92e+09 =  0% of the original kernel matrix.

torch.Size([87295, 2])
We keep 2.15e+07/2.17e+09 =  0% of the original kernel matrix.

torch.Size([119660, 2])
We keep 5.65e+07/4.64e+09 =  1% of the original kernel matrix.

torch.Size([75444, 2])
We keep 1.84e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([24048, 2])
We keep 2.67e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([34599, 2])
We keep 4.34e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([107224, 2])
We keep 2.30e+08/7.62e+09 =  3% of the original kernel matrix.

torch.Size([71193, 2])
We keep 2.25e+07/2.28e+09 =  0% of the original kernel matrix.

torch.Size([3441243, 2])
We keep 1.23e+10/2.61e+12 =  0% of the original kernel matrix.

torch.Size([446149, 2])
We keep 3.10e+08/4.22e+10 =  0% of the original kernel matrix.

torch.Size([22212, 2])
We keep 4.56e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([32501, 2])
We keep 5.06e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([1570104, 2])
We keep 5.11e+09/7.71e+11 =  0% of the original kernel matrix.

torch.Size([291058, 2])
We keep 1.78e+08/2.29e+10 =  0% of the original kernel matrix.

torch.Size([51964, 2])
We keep 2.92e+07/1.11e+09 =  2% of the original kernel matrix.

torch.Size([49999, 2])
We keep 1.02e+07/8.69e+08 =  1% of the original kernel matrix.

torch.Size([25056, 2])
We keep 3.08e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([35347, 2])
We keep 4.51e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([33866, 2])
We keep 9.21e+06/4.82e+08 =  1% of the original kernel matrix.

torch.Size([40479, 2])
We keep 7.33e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([314863, 2])
We keep 5.58e+08/4.36e+10 =  1% of the original kernel matrix.

torch.Size([126101, 2])
We keep 4.85e+07/5.45e+09 =  0% of the original kernel matrix.

torch.Size([176053, 2])
We keep 9.93e+07/9.02e+09 =  1% of the original kernel matrix.

torch.Size([93364, 2])
We keep 2.42e+07/2.48e+09 =  0% of the original kernel matrix.

torch.Size([4080, 2])
We keep 1.23e+05/2.33e+06 =  5% of the original kernel matrix.

torch.Size([15673, 2])
We keep 1.01e+06/3.99e+07 =  2% of the original kernel matrix.

torch.Size([13292, 2])
We keep 1.08e+06/3.41e+07 =  3% of the original kernel matrix.

torch.Size([25490, 2])
We keep 2.66e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([8962, 2])
We keep 5.67e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([21162, 2])
We keep 1.90e+06/9.82e+07 =  1% of the original kernel matrix.

torch.Size([10878, 2])
We keep 5.98e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([23281, 2])
We keep 2.10e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([3166, 2])
We keep 2.20e+05/3.25e+06 =  6% of the original kernel matrix.

torch.Size([12500, 2])
We keep 1.13e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([1969, 2])
We keep 3.76e+04/4.87e+05 =  7% of the original kernel matrix.

torch.Size([11666, 2])
We keep 6.11e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([13549, 2])
We keep 1.19e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([25506, 2])
We keep 2.64e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([2469, 2])
We keep 5.71e+04/9.35e+05 =  6% of the original kernel matrix.

torch.Size([12828, 2])
We keep 7.39e+05/2.53e+07 =  2% of the original kernel matrix.

torch.Size([2903, 2])
We keep 6.75e+04/1.14e+06 =  5% of the original kernel matrix.

torch.Size([13789, 2])
We keep 7.91e+05/2.79e+07 =  2% of the original kernel matrix.

torch.Size([9223, 2])
We keep 5.78e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([21738, 2])
We keep 1.89e+06/9.75e+07 =  1% of the original kernel matrix.

torch.Size([9676, 2])
We keep 5.38e+05/1.61e+07 =  3% of the original kernel matrix.

torch.Size([22061, 2])
We keep 1.99e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([10960, 2])
We keep 9.30e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([23027, 2])
We keep 2.34e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([25643, 2])
We keep 4.92e+06/2.03e+08 =  2% of the original kernel matrix.

torch.Size([35741, 2])
We keep 5.14e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([30648, 2])
We keep 5.02e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([39441, 2])
We keep 5.79e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([5536, 2])
We keep 2.56e+05/5.29e+06 =  4% of the original kernel matrix.

torch.Size([17469, 2])
We keep 1.37e+06/6.01e+07 =  2% of the original kernel matrix.

torch.Size([10733, 2])
We keep 8.25e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([22789, 2])
We keep 2.32e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([7317, 2])
We keep 3.52e+05/8.26e+06 =  4% of the original kernel matrix.

torch.Size([19441, 2])
We keep 1.59e+06/7.50e+07 =  2% of the original kernel matrix.

torch.Size([18514, 2])
We keep 1.58e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([30031, 2])
We keep 3.34e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([15235, 2])
We keep 2.38e+06/7.22e+07 =  3% of the original kernel matrix.

torch.Size([26516, 2])
We keep 3.48e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([6616, 2])
We keep 2.92e+05/7.11e+06 =  4% of the original kernel matrix.

torch.Size([18858, 2])
We keep 1.46e+06/6.96e+07 =  2% of the original kernel matrix.

torch.Size([13120, 2])
We keep 1.03e+06/3.42e+07 =  2% of the original kernel matrix.

torch.Size([25110, 2])
We keep 2.65e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([5756, 2])
We keep 2.49e+05/5.48e+06 =  4% of the original kernel matrix.

torch.Size([17664, 2])
We keep 1.36e+06/6.11e+07 =  2% of the original kernel matrix.

torch.Size([9372, 2])
We keep 6.77e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([21343, 2])
We keep 2.06e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([19941, 2])
We keep 1.81e+06/7.91e+07 =  2% of the original kernel matrix.

torch.Size([31426, 2])
We keep 3.54e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([3587, 2])
We keep 1.12e+05/1.88e+06 =  5% of the original kernel matrix.

torch.Size([14797, 2])
We keep 9.47e+05/3.58e+07 =  2% of the original kernel matrix.

torch.Size([25815, 2])
We keep 3.98e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([36012, 2])
We keep 4.94e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([20981, 2])
We keep 1.95e+06/9.20e+07 =  2% of the original kernel matrix.

torch.Size([32045, 2])
We keep 3.78e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([6837, 2])
We keep 3.21e+05/8.08e+06 =  3% of the original kernel matrix.

torch.Size([18890, 2])
We keep 1.55e+06/7.42e+07 =  2% of the original kernel matrix.

torch.Size([5671, 2])
We keep 2.55e+05/5.29e+06 =  4% of the original kernel matrix.

torch.Size([17768, 2])
We keep 1.35e+06/6.01e+07 =  2% of the original kernel matrix.

torch.Size([5977, 2])
We keep 2.64e+05/5.60e+06 =  4% of the original kernel matrix.

torch.Size([17973, 2])
We keep 1.38e+06/6.18e+07 =  2% of the original kernel matrix.

torch.Size([12101, 2])
We keep 7.45e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([24271, 2])
We keep 2.32e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([11922, 2])
We keep 8.18e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([24374, 2])
We keep 2.34e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([8906, 2])
We keep 5.05e+05/1.34e+07 =  3% of the original kernel matrix.

torch.Size([21191, 2])
We keep 1.87e+06/9.57e+07 =  1% of the original kernel matrix.

torch.Size([19585, 2])
We keep 4.25e+06/1.19e+08 =  3% of the original kernel matrix.

torch.Size([30451, 2])
We keep 4.16e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([13126, 2])
We keep 9.61e+05/3.02e+07 =  3% of the original kernel matrix.

torch.Size([25214, 2])
We keep 2.50e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([9741, 2])
We keep 4.96e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([22227, 2])
We keep 1.93e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([8764, 2])
We keep 6.32e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([20644, 2])
We keep 1.98e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([10303, 2])
We keep 1.01e+06/2.28e+07 =  4% of the original kernel matrix.

torch.Size([22386, 2])
We keep 2.25e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([21000, 2])
We keep 3.38e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([31957, 2])
We keep 4.23e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([2482, 2])
We keep 4.22e+04/6.43e+05 =  6% of the original kernel matrix.

torch.Size([13092, 2])
We keep 6.65e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([2922, 2])
We keep 8.00e+04/1.32e+06 =  6% of the original kernel matrix.

torch.Size([13583, 2])
We keep 8.40e+05/3.00e+07 =  2% of the original kernel matrix.

torch.Size([8439, 2])
We keep 5.01e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([20791, 2])
We keep 1.77e+06/8.88e+07 =  1% of the original kernel matrix.

torch.Size([36453, 2])
We keep 7.12e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([42950, 2])
We keep 7.06e+06/5.49e+08 =  1% of the original kernel matrix.

torch.Size([8068, 2])
We keep 3.95e+05/1.02e+07 =  3% of the original kernel matrix.

torch.Size([20342, 2])
We keep 1.70e+06/8.36e+07 =  2% of the original kernel matrix.

torch.Size([4035, 2])
We keep 1.91e+05/3.19e+06 =  5% of the original kernel matrix.

torch.Size([14743, 2])
We keep 1.13e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([11439, 2])
We keep 1.05e+06/2.92e+07 =  3% of the original kernel matrix.

torch.Size([23542, 2])
We keep 2.50e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([5906, 2])
We keep 2.24e+05/5.09e+06 =  4% of the original kernel matrix.

torch.Size([17953, 2])
We keep 1.33e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([5584, 2])
We keep 2.52e+05/5.40e+06 =  4% of the original kernel matrix.

torch.Size([17304, 2])
We keep 1.35e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([4204, 2])
We keep 1.27e+05/2.32e+06 =  5% of the original kernel matrix.

torch.Size([15928, 2])
We keep 1.02e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([7911, 2])
We keep 5.36e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([20077, 2])
We keep 1.78e+06/8.89e+07 =  2% of the original kernel matrix.

torch.Size([2152, 2])
We keep 3.77e+04/5.40e+05 =  6% of the original kernel matrix.

torch.Size([12471, 2])
We keep 6.27e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([14217, 2])
We keep 1.05e+06/3.72e+07 =  2% of the original kernel matrix.

torch.Size([26343, 2])
We keep 2.71e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([9904, 2])
We keep 6.37e+05/1.70e+07 =  3% of the original kernel matrix.

torch.Size([22304, 2])
We keep 2.03e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([12927, 2])
We keep 8.32e+05/2.89e+07 =  2% of the original kernel matrix.

torch.Size([25201, 2])
We keep 2.46e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([3692, 2])
We keep 9.97e+04/1.87e+06 =  5% of the original kernel matrix.

torch.Size([14960, 2])
We keep 9.43e+05/3.57e+07 =  2% of the original kernel matrix.

torch.Size([4073, 2])
We keep 1.11e+05/2.17e+06 =  5% of the original kernel matrix.

torch.Size([15690, 2])
We keep 9.94e+05/3.85e+07 =  2% of the original kernel matrix.

torch.Size([14577, 2])
We keep 1.55e+06/4.90e+07 =  3% of the original kernel matrix.

torch.Size([26422, 2])
We keep 3.01e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([12076, 2])
We keep 9.25e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([24301, 2])
We keep 2.42e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([12496, 2])
We keep 8.92e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([24798, 2])
We keep 2.45e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([11945, 2])
We keep 7.61e+05/2.54e+07 =  2% of the original kernel matrix.

torch.Size([24426, 2])
We keep 2.38e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([16479, 2])
We keep 2.84e+06/9.46e+07 =  3% of the original kernel matrix.

torch.Size([27620, 2])
We keep 3.82e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([24309, 2])
We keep 3.55e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([34815, 2])
We keep 4.83e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([9504, 2])
We keep 4.69e+05/1.39e+07 =  3% of the original kernel matrix.

torch.Size([21981, 2])
We keep 1.86e+06/9.72e+07 =  1% of the original kernel matrix.

torch.Size([23391, 2])
We keep 2.99e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([34000, 2])
We keep 4.49e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([14437, 2])
We keep 1.10e+06/3.59e+07 =  3% of the original kernel matrix.

torch.Size([26567, 2])
We keep 2.66e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([13905, 2])
We keep 1.07e+06/3.56e+07 =  2% of the original kernel matrix.

torch.Size([26068, 2])
We keep 2.65e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([5032, 2])
We keep 2.10e+05/4.05e+06 =  5% of the original kernel matrix.

torch.Size([16795, 2])
We keep 1.24e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([14040, 2])
We keep 1.50e+06/4.80e+07 =  3% of the original kernel matrix.

torch.Size([25910, 2])
We keep 2.96e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([6220, 2])
We keep 2.59e+05/5.88e+06 =  4% of the original kernel matrix.

torch.Size([18325, 2])
We keep 1.40e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([11697, 2])
We keep 6.64e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([24027, 2])
We keep 2.20e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([2235, 2])
We keep 5.19e+04/7.06e+05 =  7% of the original kernel matrix.

torch.Size([12259, 2])
We keep 6.89e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([7187, 2])
We keep 5.06e+05/9.78e+06 =  5% of the original kernel matrix.

torch.Size([19262, 2])
We keep 1.66e+06/8.17e+07 =  2% of the original kernel matrix.

torch.Size([3536, 2])
We keep 9.49e+04/1.67e+06 =  5% of the original kernel matrix.

torch.Size([14739, 2])
We keep 9.07e+05/3.38e+07 =  2% of the original kernel matrix.

torch.Size([7063, 2])
We keep 3.92e+05/9.68e+06 =  4% of the original kernel matrix.

torch.Size([19070, 2])
We keep 1.65e+06/8.13e+07 =  2% of the original kernel matrix.

torch.Size([6181, 2])
We keep 5.60e+05/9.39e+06 =  5% of the original kernel matrix.

torch.Size([17592, 2])
We keep 1.65e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([6121, 2])
We keep 3.51e+05/7.27e+06 =  4% of the original kernel matrix.

torch.Size([17767, 2])
We keep 1.51e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([9692, 2])
We keep 6.05e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([22320, 2])
We keep 2.01e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([11319, 2])
We keep 9.71e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([23309, 2])
We keep 2.40e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([5528, 2])
We keep 2.81e+05/6.00e+06 =  4% of the original kernel matrix.

torch.Size([17014, 2])
We keep 1.42e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([6216, 2])
We keep 2.74e+05/6.34e+06 =  4% of the original kernel matrix.

torch.Size([18249, 2])
We keep 1.41e+06/6.58e+07 =  2% of the original kernel matrix.

torch.Size([2702, 2])
We keep 5.39e+04/8.82e+05 =  6% of the original kernel matrix.

torch.Size([13355, 2])
We keep 7.42e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([19318, 2])
We keep 2.36e+06/9.80e+07 =  2% of the original kernel matrix.

torch.Size([30700, 2])
We keep 3.89e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([2917, 2])
We keep 6.62e+04/1.09e+06 =  6% of the original kernel matrix.

torch.Size([13654, 2])
We keep 7.90e+05/2.73e+07 =  2% of the original kernel matrix.

torch.Size([24810, 2])
We keep 2.76e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([35150, 2])
We keep 4.54e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([5201, 2])
We keep 2.03e+05/4.06e+06 =  4% of the original kernel matrix.

torch.Size([17092, 2])
We keep 1.21e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([5304, 2])
We keep 2.32e+05/4.90e+06 =  4% of the original kernel matrix.

torch.Size([16918, 2])
We keep 1.32e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([7883, 2])
We keep 3.73e+05/1.01e+07 =  3% of the original kernel matrix.

torch.Size([20351, 2])
We keep 1.66e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([4855, 2])
We keep 1.84e+05/3.55e+06 =  5% of the original kernel matrix.

torch.Size([16740, 2])
We keep 1.18e+06/4.92e+07 =  2% of the original kernel matrix.

torch.Size([8715, 2])
We keep 4.19e+05/1.21e+07 =  3% of the original kernel matrix.

torch.Size([20971, 2])
We keep 1.81e+06/9.10e+07 =  1% of the original kernel matrix.

torch.Size([3054, 2])
We keep 6.48e+04/1.06e+06 =  6% of the original kernel matrix.

torch.Size([14030, 2])
We keep 7.81e+05/2.68e+07 =  2% of the original kernel matrix.

torch.Size([12216, 2])
We keep 8.62e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([24549, 2])
We keep 2.34e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([26054, 2])
We keep 3.59e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([36315, 2])
We keep 5.04e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([5834, 2])
We keep 2.28e+05/5.35e+06 =  4% of the original kernel matrix.

torch.Size([17888, 2])
We keep 1.35e+06/6.04e+07 =  2% of the original kernel matrix.

torch.Size([17572, 2])
We keep 1.63e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([29303, 2])
We keep 3.33e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([5143, 2])
We keep 2.26e+05/4.19e+06 =  5% of the original kernel matrix.

torch.Size([16995, 2])
We keep 1.22e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([12802, 2])
We keep 2.19e+06/3.75e+07 =  5% of the original kernel matrix.

torch.Size([25058, 2])
We keep 2.71e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([3677, 2])
We keep 1.72e+05/2.87e+06 =  5% of the original kernel matrix.

torch.Size([13964, 2])
We keep 1.10e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([8924, 2])
We keep 6.30e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([21253, 2])
We keep 1.92e+06/9.76e+07 =  1% of the original kernel matrix.

torch.Size([6630, 2])
We keep 3.19e+05/7.26e+06 =  4% of the original kernel matrix.

torch.Size([18812, 2])
We keep 1.47e+06/7.03e+07 =  2% of the original kernel matrix.

torch.Size([5493, 2])
We keep 2.56e+05/5.51e+06 =  4% of the original kernel matrix.

torch.Size([17221, 2])
We keep 1.36e+06/6.13e+07 =  2% of the original kernel matrix.

torch.Size([4807, 2])
We keep 1.71e+05/3.40e+06 =  5% of the original kernel matrix.

torch.Size([16609, 2])
We keep 1.16e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([4817, 2])
We keep 1.45e+05/3.05e+06 =  4% of the original kernel matrix.

torch.Size([16525, 2])
We keep 1.11e+06/4.56e+07 =  2% of the original kernel matrix.

torch.Size([4417, 2])
We keep 1.17e+05/2.52e+06 =  4% of the original kernel matrix.

torch.Size([16254, 2])
We keep 1.04e+06/4.15e+07 =  2% of the original kernel matrix.

torch.Size([7364, 2])
We keep 4.56e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([19231, 2])
We keep 1.72e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([3361, 2])
We keep 1.06e+05/1.60e+06 =  6% of the original kernel matrix.

torch.Size([14316, 2])
We keep 9.07e+05/3.30e+07 =  2% of the original kernel matrix.

torch.Size([17604, 2])
We keep 2.23e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([29183, 2])
We keep 3.79e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([26907, 2])
We keep 3.97e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([36585, 2])
We keep 5.18e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([20121, 2])
We keep 2.58e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([31507, 2])
We keep 4.01e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([6350, 2])
We keep 2.14e+05/5.43e+06 =  3% of the original kernel matrix.

torch.Size([18769, 2])
We keep 1.34e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([6941, 2])
We keep 3.33e+05/7.57e+06 =  4% of the original kernel matrix.

torch.Size([19291, 2])
We keep 1.52e+06/7.19e+07 =  2% of the original kernel matrix.

torch.Size([3642, 2])
We keep 1.42e+05/2.36e+06 =  6% of the original kernel matrix.

torch.Size([14679, 2])
We keep 1.02e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([3607, 2])
We keep 1.11e+05/1.92e+06 =  5% of the original kernel matrix.

torch.Size([14804, 2])
We keep 9.62e+05/3.62e+07 =  2% of the original kernel matrix.

torch.Size([6298, 2])
We keep 3.35e+05/6.90e+06 =  4% of the original kernel matrix.

torch.Size([18224, 2])
We keep 1.47e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([6077, 2])
We keep 2.99e+05/6.44e+06 =  4% of the original kernel matrix.

torch.Size([17805, 2])
We keep 1.44e+06/6.62e+07 =  2% of the original kernel matrix.

torch.Size([15442, 2])
We keep 1.56e+06/5.39e+07 =  2% of the original kernel matrix.

torch.Size([27471, 2])
We keep 3.13e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([9006, 2])
We keep 5.25e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([21400, 2])
We keep 1.88e+06/9.69e+07 =  1% of the original kernel matrix.

torch.Size([17654, 2])
We keep 3.83e+06/1.18e+08 =  3% of the original kernel matrix.

torch.Size([29059, 2])
We keep 4.16e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([8811, 2])
We keep 6.04e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([20893, 2])
We keep 1.96e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([40713, 2])
We keep 1.40e+07/6.29e+08 =  2% of the original kernel matrix.

torch.Size([45050, 2])
We keep 8.19e+06/6.55e+08 =  1% of the original kernel matrix.

torch.Size([5954, 2])
We keep 2.02e+05/4.73e+06 =  4% of the original kernel matrix.

torch.Size([18284, 2])
We keep 1.29e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([8303, 2])
We keep 5.60e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([20631, 2])
We keep 1.81e+06/9.09e+07 =  1% of the original kernel matrix.

torch.Size([10752, 2])
We keep 8.56e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([23158, 2])
We keep 2.17e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([1489, 2])
We keep 2.44e+04/2.86e+05 =  8% of the original kernel matrix.

torch.Size([10838, 2])
We keep 5.23e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([29664, 2])
We keep 4.17e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([38920, 2])
We keep 5.60e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([3628, 2])
We keep 9.72e+04/1.80e+06 =  5% of the original kernel matrix.

torch.Size([14993, 2])
We keep 9.33e+05/3.50e+07 =  2% of the original kernel matrix.

torch.Size([16511, 2])
We keep 3.18e+06/7.63e+07 =  4% of the original kernel matrix.

torch.Size([28108, 2])
We keep 3.56e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([3251, 2])
We keep 9.02e+04/1.47e+06 =  6% of the original kernel matrix.

torch.Size([14206, 2])
We keep 8.89e+05/3.17e+07 =  2% of the original kernel matrix.

torch.Size([9008, 2])
We keep 6.46e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([20823, 2])
We keep 2.05e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([7587, 2])
We keep 3.49e+05/9.07e+06 =  3% of the original kernel matrix.

torch.Size([19860, 2])
We keep 1.61e+06/7.87e+07 =  2% of the original kernel matrix.

torch.Size([4842, 2])
We keep 1.63e+05/3.30e+06 =  4% of the original kernel matrix.

torch.Size([16791, 2])
We keep 1.14e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([7798, 2])
We keep 4.04e+05/1.02e+07 =  3% of the original kernel matrix.

torch.Size([19979, 2])
We keep 1.68e+06/8.32e+07 =  2% of the original kernel matrix.

torch.Size([15742, 2])
We keep 1.24e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([27671, 2])
We keep 2.96e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([1638, 2])
We keep 2.05e+04/2.58e+05 =  7% of the original kernel matrix.

torch.Size([11230, 2])
We keep 5.01e+05/1.33e+07 =  3% of the original kernel matrix.

torch.Size([2490, 2])
We keep 7.70e+04/1.05e+06 =  7% of the original kernel matrix.

torch.Size([12472, 2])
We keep 7.77e+05/2.67e+07 =  2% of the original kernel matrix.

torch.Size([3632, 2])
We keep 1.54e+05/2.15e+06 =  7% of the original kernel matrix.

torch.Size([14572, 2])
We keep 1.00e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([16881, 2])
We keep 1.72e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([28698, 2])
We keep 3.27e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([4171, 2])
We keep 1.08e+05/2.16e+06 =  5% of the original kernel matrix.

torch.Size([15822, 2])
We keep 9.87e+05/3.84e+07 =  2% of the original kernel matrix.

torch.Size([12457, 2])
We keep 1.12e+06/3.18e+07 =  3% of the original kernel matrix.

torch.Size([24704, 2])
We keep 2.53e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([15215, 2])
We keep 2.97e+07/1.08e+08 = 27% of the original kernel matrix.

torch.Size([26977, 2])
We keep 4.11e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([12039, 2])
We keep 1.04e+06/3.03e+07 =  3% of the original kernel matrix.

torch.Size([24129, 2])
We keep 2.51e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([8012, 2])
We keep 4.11e+05/1.08e+07 =  3% of the original kernel matrix.

torch.Size([20341, 2])
We keep 1.72e+06/8.60e+07 =  2% of the original kernel matrix.

torch.Size([14319, 2])
We keep 2.94e+06/5.74e+07 =  5% of the original kernel matrix.

torch.Size([26342, 2])
We keep 3.20e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([4687, 2])
We keep 1.61e+05/3.28e+06 =  4% of the original kernel matrix.

torch.Size([16417, 2])
We keep 1.15e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([20695, 2])
We keep 2.22e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([31886, 2])
We keep 3.91e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([6162, 2])
We keep 2.87e+05/6.17e+06 =  4% of the original kernel matrix.

torch.Size([18107, 2])
We keep 1.42e+06/6.48e+07 =  2% of the original kernel matrix.

torch.Size([15006, 2])
We keep 2.98e+06/6.64e+07 =  4% of the original kernel matrix.

torch.Size([26697, 2])
We keep 3.33e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([5828, 2])
We keep 2.29e+05/4.77e+06 =  4% of the original kernel matrix.

torch.Size([18125, 2])
We keep 1.30e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([13463, 2])
We keep 1.11e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([25607, 2])
We keep 2.60e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([14202, 2])
We keep 1.01e+06/3.67e+07 =  2% of the original kernel matrix.

torch.Size([26407, 2])
We keep 2.66e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([7025, 2])
We keep 3.07e+05/7.39e+06 =  4% of the original kernel matrix.

torch.Size([19228, 2])
We keep 1.52e+06/7.10e+07 =  2% of the original kernel matrix.

torch.Size([14693, 2])
We keep 2.12e+06/6.43e+07 =  3% of the original kernel matrix.

torch.Size([26108, 2])
We keep 3.33e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([13975, 2])
We keep 1.20e+06/3.97e+07 =  3% of the original kernel matrix.

torch.Size([26064, 2])
We keep 2.79e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([22354, 2])
We keep 3.81e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([33128, 2])
We keep 4.60e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([7013, 2])
We keep 4.22e+05/9.22e+06 =  4% of the original kernel matrix.

torch.Size([19032, 2])
We keep 1.65e+06/7.93e+07 =  2% of the original kernel matrix.

torch.Size([2323, 2])
We keep 4.54e+04/6.64e+05 =  6% of the original kernel matrix.

torch.Size([12577, 2])
We keep 6.70e+05/2.13e+07 =  3% of the original kernel matrix.

torch.Size([5609, 2])
We keep 1.94e+05/4.60e+06 =  4% of the original kernel matrix.

torch.Size([17713, 2])
We keep 1.27e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([5192, 2])
We keep 2.51e+05/4.72e+06 =  5% of the original kernel matrix.

torch.Size([16909, 2])
We keep 1.31e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([10416, 2])
We keep 1.04e+06/2.54e+07 =  4% of the original kernel matrix.

torch.Size([22323, 2])
We keep 2.35e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([8862, 2])
We keep 5.64e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([21265, 2])
We keep 1.87e+06/9.61e+07 =  1% of the original kernel matrix.

torch.Size([5460, 2])
We keep 1.90e+05/3.99e+06 =  4% of the original kernel matrix.

torch.Size([17587, 2])
We keep 1.21e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([5129, 2])
We keep 1.67e+05/3.40e+06 =  4% of the original kernel matrix.

torch.Size([17216, 2])
We keep 1.14e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([12528, 2])
We keep 1.36e+06/3.10e+07 =  4% of the original kernel matrix.

torch.Size([24685, 2])
We keep 2.52e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([2909, 2])
We keep 6.21e+04/1.02e+06 =  6% of the original kernel matrix.

torch.Size([13856, 2])
We keep 7.70e+05/2.64e+07 =  2% of the original kernel matrix.

torch.Size([23073, 2])
We keep 2.36e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([33905, 2])
We keep 4.26e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([6745, 2])
We keep 3.01e+05/7.08e+06 =  4% of the original kernel matrix.

torch.Size([18855, 2])
We keep 1.49e+06/6.95e+07 =  2% of the original kernel matrix.

torch.Size([36908, 2])
We keep 5.14e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([43784, 2])
We keep 6.53e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([11482, 2])
We keep 8.61e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([23782, 2])
We keep 2.36e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([8590, 2])
We keep 4.61e+05/1.30e+07 =  3% of the original kernel matrix.

torch.Size([20977, 2])
We keep 1.85e+06/9.43e+07 =  1% of the original kernel matrix.

torch.Size([3076, 2])
We keep 6.35e+04/1.07e+06 =  5% of the original kernel matrix.

torch.Size([13989, 2])
We keep 7.86e+05/2.70e+07 =  2% of the original kernel matrix.

torch.Size([13562, 2])
We keep 1.49e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([25839, 2])
We keep 2.91e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([12187, 2])
We keep 8.48e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([24497, 2])
We keep 2.38e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([6058, 2])
We keep 3.07e+05/5.82e+06 =  5% of the original kernel matrix.

torch.Size([18137, 2])
We keep 1.39e+06/6.30e+07 =  2% of the original kernel matrix.

torch.Size([23816, 2])
We keep 3.52e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([34376, 2])
We keep 4.61e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([18152, 2])
We keep 1.93e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([29661, 2])
We keep 3.52e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([6399, 2])
We keep 3.21e+05/6.74e+06 =  4% of the original kernel matrix.

torch.Size([18420, 2])
We keep 1.46e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([4915, 2])
We keep 1.66e+05/3.41e+06 =  4% of the original kernel matrix.

torch.Size([16728, 2])
We keep 1.16e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([7955, 2])
We keep 4.78e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([20159, 2])
We keep 1.73e+06/8.58e+07 =  2% of the original kernel matrix.

torch.Size([2270, 2])
We keep 4.55e+04/6.40e+05 =  7% of the original kernel matrix.

torch.Size([12509, 2])
We keep 6.65e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([3219, 2])
We keep 1.25e+05/1.91e+06 =  6% of the original kernel matrix.

torch.Size([13753, 2])
We keep 9.53e+05/3.61e+07 =  2% of the original kernel matrix.

torch.Size([1866, 2])
We keep 2.46e+04/3.25e+05 =  7% of the original kernel matrix.

torch.Size([11887, 2])
We keep 5.34e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([3168, 2])
We keep 8.29e+04/1.40e+06 =  5% of the original kernel matrix.

torch.Size([13970, 2])
We keep 8.52e+05/3.09e+07 =  2% of the original kernel matrix.

torch.Size([21095, 2])
We keep 6.21e+06/1.82e+08 =  3% of the original kernel matrix.

torch.Size([31032, 2])
We keep 4.92e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([4156, 2])
We keep 1.52e+05/2.78e+06 =  5% of the original kernel matrix.

torch.Size([15595, 2])
We keep 1.09e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([10190, 2])
We keep 1.77e+06/3.17e+07 =  5% of the original kernel matrix.

torch.Size([21925, 2])
We keep 2.52e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([3240, 2])
We keep 8.57e+04/1.48e+06 =  5% of the original kernel matrix.

torch.Size([14286, 2])
We keep 8.62e+05/3.18e+07 =  2% of the original kernel matrix.

torch.Size([12188, 2])
We keep 9.98e+05/3.16e+07 =  3% of the original kernel matrix.

torch.Size([24100, 2])
We keep 2.56e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([5335, 2])
We keep 3.01e+05/5.68e+06 =  5% of the original kernel matrix.

torch.Size([16760, 2])
We keep 1.38e+06/6.23e+07 =  2% of the original kernel matrix.

torch.Size([7569, 2])
We keep 4.50e+05/1.13e+07 =  3% of the original kernel matrix.

torch.Size([19360, 2])
We keep 1.76e+06/8.77e+07 =  2% of the original kernel matrix.

torch.Size([9784, 2])
We keep 8.79e+05/1.99e+07 =  4% of the original kernel matrix.

torch.Size([21793, 2])
We keep 2.16e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([4117, 2])
We keep 1.06e+05/2.09e+06 =  5% of the original kernel matrix.

torch.Size([15640, 2])
We keep 9.79e+05/3.78e+07 =  2% of the original kernel matrix.

torch.Size([3815, 2])
We keep 1.21e+05/2.30e+06 =  5% of the original kernel matrix.

torch.Size([15080, 2])
We keep 1.01e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([12862, 2])
We keep 1.09e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([24885, 2])
We keep 2.60e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([3610, 2])
We keep 9.24e+04/1.69e+06 =  5% of the original kernel matrix.

torch.Size([14804, 2])
We keep 9.08e+05/3.39e+07 =  2% of the original kernel matrix.

torch.Size([4322, 2])
We keep 1.64e+05/2.99e+06 =  5% of the original kernel matrix.

torch.Size([15785, 2])
We keep 1.11e+06/4.51e+07 =  2% of the original kernel matrix.

torch.Size([5499, 2])
We keep 3.02e+05/6.79e+06 =  4% of the original kernel matrix.

torch.Size([16904, 2])
We keep 1.45e+06/6.80e+07 =  2% of the original kernel matrix.

torch.Size([6578, 2])
We keep 3.60e+05/7.51e+06 =  4% of the original kernel matrix.

torch.Size([18548, 2])
We keep 1.52e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([12231, 2])
We keep 1.12e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([24554, 2])
We keep 2.61e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([6019, 2])
We keep 3.00e+05/5.70e+06 =  5% of the original kernel matrix.

torch.Size([17918, 2])
We keep 1.39e+06/6.24e+07 =  2% of the original kernel matrix.

torch.Size([4871, 2])
We keep 2.45e+05/4.11e+06 =  5% of the original kernel matrix.

torch.Size([16394, 2])
We keep 1.24e+06/5.30e+07 =  2% of the original kernel matrix.

torch.Size([15196, 2])
We keep 1.48e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([27201, 2])
We keep 3.15e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([16812, 2])
We keep 1.52e+06/5.81e+07 =  2% of the original kernel matrix.

torch.Size([28713, 2])
We keep 3.18e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([5868, 2])
We keep 9.79e+05/6.47e+06 = 15% of the original kernel matrix.

torch.Size([17644, 2])
We keep 1.45e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([40822, 2])
We keep 1.21e+07/5.61e+08 =  2% of the original kernel matrix.

torch.Size([42856, 2])
We keep 7.18e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([68063, 2])
We keep 9.78e+07/3.81e+09 =  2% of the original kernel matrix.

torch.Size([55040, 2])
We keep 1.70e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([9880, 2])
We keep 1.03e+06/2.03e+07 =  5% of the original kernel matrix.

torch.Size([22525, 2])
We keep 2.04e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([15817, 2])
We keep 2.01e+06/5.97e+07 =  3% of the original kernel matrix.

torch.Size([27704, 2])
We keep 3.26e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([1593, 2])
We keep 3.89e+04/3.84e+05 = 10% of the original kernel matrix.

torch.Size([10421, 2])
We keep 5.57e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([14237, 2])
We keep 1.12e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([26341, 2])
We keep 2.69e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([68291, 2])
We keep 5.28e+08/2.03e+09 = 26% of the original kernel matrix.

torch.Size([57060, 2])
We keep 1.31e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([182313, 2])
We keep 3.14e+08/1.89e+10 =  1% of the original kernel matrix.

torch.Size([93292, 2])
We keep 3.32e+07/3.59e+09 =  0% of the original kernel matrix.

torch.Size([7660, 2])
We keep 4.19e+05/1.05e+07 =  3% of the original kernel matrix.

torch.Size([20038, 2])
We keep 1.70e+06/8.47e+07 =  2% of the original kernel matrix.

torch.Size([226827, 2])
We keep 2.52e+08/2.19e+10 =  1% of the original kernel matrix.

torch.Size([105248, 2])
We keep 3.58e+07/3.86e+09 =  0% of the original kernel matrix.

torch.Size([42060, 2])
We keep 3.05e+07/7.40e+08 =  4% of the original kernel matrix.

torch.Size([44981, 2])
We keep 8.45e+06/7.10e+08 =  1% of the original kernel matrix.

torch.Size([35381, 2])
We keep 1.29e+07/5.03e+08 =  2% of the original kernel matrix.

torch.Size([42210, 2])
We keep 7.45e+06/5.86e+08 =  1% of the original kernel matrix.

torch.Size([50175, 2])
We keep 1.76e+08/1.15e+09 = 15% of the original kernel matrix.

torch.Size([49207, 2])
We keep 8.62e+06/8.84e+08 =  0% of the original kernel matrix.

torch.Size([6436, 2])
We keep 2.38e+05/5.96e+06 =  4% of the original kernel matrix.

torch.Size([18659, 2])
We keep 1.39e+06/6.37e+07 =  2% of the original kernel matrix.

torch.Size([16215, 2])
We keep 1.41e+06/5.13e+07 =  2% of the original kernel matrix.

torch.Size([28118, 2])
We keep 3.05e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([318737, 2])
We keep 4.39e+08/4.53e+10 =  0% of the original kernel matrix.

torch.Size([125641, 2])
We keep 4.92e+07/5.56e+09 =  0% of the original kernel matrix.

torch.Size([1077679, 2])
We keep 2.58e+09/3.12e+11 =  0% of the original kernel matrix.

torch.Size([244899, 2])
We keep 1.17e+08/1.46e+10 =  0% of the original kernel matrix.

torch.Size([13632, 2])
We keep 1.68e+06/3.95e+07 =  4% of the original kernel matrix.

torch.Size([25664, 2])
We keep 2.79e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([78753, 2])
We keep 2.99e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([61782, 2])
We keep 1.26e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([33973, 2])
We keep 1.77e+07/5.00e+08 =  3% of the original kernel matrix.

torch.Size([41152, 2])
We keep 7.55e+06/5.84e+08 =  1% of the original kernel matrix.

torch.Size([71914, 2])
We keep 7.11e+07/1.64e+09 =  4% of the original kernel matrix.

torch.Size([59207, 2])
We keep 1.17e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([96755, 2])
We keep 1.35e+08/5.95e+09 =  2% of the original kernel matrix.

torch.Size([66157, 2])
We keep 2.07e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([67590, 2])
We keep 1.42e+08/3.79e+09 =  3% of the original kernel matrix.

torch.Size([52899, 2])
We keep 1.71e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([93619, 2])
We keep 8.97e+07/2.90e+09 =  3% of the original kernel matrix.

torch.Size([67208, 2])
We keep 1.47e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([30629, 2])
We keep 5.97e+06/2.63e+08 =  2% of the original kernel matrix.

torch.Size([39128, 2])
We keep 5.60e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([7975, 2])
We keep 1.89e+06/1.50e+07 = 12% of the original kernel matrix.

torch.Size([20160, 2])
We keep 1.93e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([191308, 2])
We keep 5.43e+08/3.07e+10 =  1% of the original kernel matrix.

torch.Size([90587, 2])
We keep 4.11e+07/4.57e+09 =  0% of the original kernel matrix.

torch.Size([14128, 2])
We keep 3.41e+06/4.98e+07 =  6% of the original kernel matrix.

torch.Size([26190, 2])
We keep 3.00e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([7565, 2])
We keep 3.40e+05/9.02e+06 =  3% of the original kernel matrix.

torch.Size([19824, 2])
We keep 1.60e+06/7.84e+07 =  2% of the original kernel matrix.

torch.Size([41833, 2])
We keep 1.31e+07/5.56e+08 =  2% of the original kernel matrix.

torch.Size([43418, 2])
We keep 7.04e+06/6.16e+08 =  1% of the original kernel matrix.

torch.Size([95803, 2])
We keep 1.13e+08/5.06e+09 =  2% of the original kernel matrix.

torch.Size([63975, 2])
We keep 1.92e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([457241, 2])
We keep 7.34e+08/7.17e+10 =  1% of the original kernel matrix.

torch.Size([154562, 2])
We keep 6.03e+07/6.99e+09 =  0% of the original kernel matrix.

torch.Size([194053, 2])
We keep 9.83e+07/1.10e+10 =  0% of the original kernel matrix.

torch.Size([98426, 2])
We keep 2.65e+07/2.74e+09 =  0% of the original kernel matrix.

torch.Size([4614, 2])
We keep 1.61e+05/3.13e+06 =  5% of the original kernel matrix.

torch.Size([16374, 2])
We keep 1.13e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([48056, 2])
We keep 2.09e+07/8.45e+08 =  2% of the original kernel matrix.

torch.Size([48754, 2])
We keep 9.13e+06/7.59e+08 =  1% of the original kernel matrix.

torch.Size([21732, 2])
We keep 6.63e+06/1.60e+08 =  4% of the original kernel matrix.

torch.Size([32502, 2])
We keep 4.71e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([58141, 2])
We keep 2.27e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([52991, 2])
We keep 1.07e+07/9.44e+08 =  1% of the original kernel matrix.

torch.Size([111148, 2])
We keep 5.24e+07/3.76e+09 =  1% of the original kernel matrix.

torch.Size([73727, 2])
We keep 1.65e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([23727, 2])
We keep 7.77e+06/1.78e+08 =  4% of the original kernel matrix.

torch.Size([33985, 2])
We keep 4.75e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([163836, 2])
We keep 9.92e+07/8.27e+09 =  1% of the original kernel matrix.

torch.Size([89313, 2])
We keep 2.33e+07/2.38e+09 =  0% of the original kernel matrix.

torch.Size([2101, 2])
We keep 3.65e+04/5.01e+05 =  7% of the original kernel matrix.

torch.Size([12145, 2])
We keep 6.15e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([15829, 2])
We keep 1.57e+06/5.27e+07 =  2% of the original kernel matrix.

torch.Size([27726, 2])
We keep 3.06e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([153351, 2])
We keep 1.59e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([84165, 2])
We keep 2.67e+07/2.76e+09 =  0% of the original kernel matrix.

torch.Size([71765, 2])
We keep 5.44e+07/2.68e+09 =  2% of the original kernel matrix.

torch.Size([56857, 2])
We keep 1.46e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([102183, 2])
We keep 9.18e+07/5.43e+09 =  1% of the original kernel matrix.

torch.Size([66869, 2])
We keep 1.98e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([12743, 2])
We keep 1.71e+06/3.75e+07 =  4% of the original kernel matrix.

torch.Size([24881, 2])
We keep 2.67e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([436571, 2])
We keep 5.07e+08/7.76e+10 =  0% of the original kernel matrix.

torch.Size([150231, 2])
We keep 6.15e+07/7.27e+09 =  0% of the original kernel matrix.

torch.Size([28318, 2])
We keep 3.27e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([37861, 2])
We keep 5.03e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([55884, 2])
We keep 1.40e+07/9.48e+08 =  1% of the original kernel matrix.

torch.Size([52767, 2])
We keep 9.44e+06/8.04e+08 =  1% of the original kernel matrix.

torch.Size([151448, 2])
We keep 1.66e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([85452, 2])
We keep 2.56e+07/2.63e+09 =  0% of the original kernel matrix.

torch.Size([6398, 2])
We keep 2.55e+05/5.88e+06 =  4% of the original kernel matrix.

torch.Size([18533, 2])
We keep 1.39e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([196955, 2])
We keep 2.06e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([98917, 2])
We keep 2.80e+07/2.91e+09 =  0% of the original kernel matrix.

torch.Size([295515, 2])
We keep 8.13e+08/4.64e+10 =  1% of the original kernel matrix.

torch.Size([121732, 2])
We keep 5.02e+07/5.62e+09 =  0% of the original kernel matrix.

torch.Size([748055, 2])
We keep 2.08e+09/1.93e+11 =  1% of the original kernel matrix.

torch.Size([198277, 2])
We keep 9.37e+07/1.15e+10 =  0% of the original kernel matrix.

torch.Size([67635, 2])
We keep 2.74e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([56176, 2])
We keep 1.24e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([51573, 2])
We keep 1.95e+07/9.59e+08 =  2% of the original kernel matrix.

torch.Size([50182, 2])
We keep 9.57e+06/8.09e+08 =  1% of the original kernel matrix.

torch.Size([11293, 2])
We keep 1.60e+06/3.70e+07 =  4% of the original kernel matrix.

torch.Size([22932, 2])
We keep 2.70e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([15652, 2])
We keep 1.12e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([27616, 2])
We keep 2.81e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([157438, 2])
We keep 1.64e+08/1.23e+10 =  1% of the original kernel matrix.

torch.Size([86051, 2])
We keep 2.76e+07/2.89e+09 =  0% of the original kernel matrix.

torch.Size([20466, 2])
We keep 2.17e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([31780, 2])
We keep 3.91e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([41643, 2])
We keep 1.23e+07/6.54e+08 =  1% of the original kernel matrix.

torch.Size([45341, 2])
We keep 8.16e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([60947, 2])
We keep 6.29e+07/1.73e+09 =  3% of the original kernel matrix.

torch.Size([54373, 2])
We keep 1.08e+07/1.09e+09 =  0% of the original kernel matrix.

torch.Size([36538, 2])
We keep 1.65e+07/4.61e+08 =  3% of the original kernel matrix.

torch.Size([42884, 2])
We keep 6.94e+06/5.61e+08 =  1% of the original kernel matrix.

torch.Size([29744, 2])
We keep 1.52e+07/4.37e+08 =  3% of the original kernel matrix.

torch.Size([36845, 2])
We keep 7.09e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([14027, 2])
We keep 9.16e+05/3.39e+07 =  2% of the original kernel matrix.

torch.Size([26366, 2])
We keep 2.58e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([97732, 2])
We keep 2.77e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([62096, 2])
We keep 2.97e+07/3.11e+09 =  0% of the original kernel matrix.

torch.Size([47411, 2])
We keep 1.17e+08/1.12e+09 = 10% of the original kernel matrix.

torch.Size([47566, 2])
We keep 1.03e+07/8.74e+08 =  1% of the original kernel matrix.

torch.Size([39868, 2])
We keep 4.62e+07/9.00e+08 =  5% of the original kernel matrix.

torch.Size([43063, 2])
We keep 9.32e+06/7.84e+08 =  1% of the original kernel matrix.

torch.Size([47220, 2])
We keep 1.16e+07/7.63e+08 =  1% of the original kernel matrix.

torch.Size([49386, 2])
We keep 8.75e+06/7.21e+08 =  1% of the original kernel matrix.

torch.Size([147208, 2])
We keep 5.13e+08/1.46e+10 =  3% of the original kernel matrix.

torch.Size([83557, 2])
We keep 2.89e+07/3.16e+09 =  0% of the original kernel matrix.

torch.Size([31269, 2])
We keep 5.61e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([39937, 2])
We keep 5.94e+06/4.45e+08 =  1% of the original kernel matrix.

torch.Size([321758, 2])
We keep 6.66e+08/4.71e+10 =  1% of the original kernel matrix.

torch.Size([126156, 2])
We keep 5.01e+07/5.66e+09 =  0% of the original kernel matrix.

torch.Size([10613, 2])
We keep 1.35e+06/3.37e+07 =  4% of the original kernel matrix.

torch.Size([22930, 2])
We keep 2.57e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([48773, 2])
We keep 4.84e+07/1.82e+09 =  2% of the original kernel matrix.

torch.Size([44245, 2])
We keep 1.24e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([25630, 2])
We keep 5.04e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([35961, 2])
We keep 4.90e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([148849, 2])
We keep 3.45e+08/1.36e+10 =  2% of the original kernel matrix.

torch.Size([80943, 2])
We keep 2.88e+07/3.04e+09 =  0% of the original kernel matrix.

torch.Size([72782, 2])
We keep 2.44e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([59397, 2])
We keep 1.36e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([22222, 2])
We keep 2.49e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([33254, 2])
We keep 4.22e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([226081, 2])
We keep 1.20e+08/1.56e+10 =  0% of the original kernel matrix.

torch.Size([106868, 2])
We keep 3.05e+07/3.26e+09 =  0% of the original kernel matrix.

torch.Size([298320, 2])
We keep 2.01e+08/2.62e+10 =  0% of the original kernel matrix.

torch.Size([124830, 2])
We keep 3.84e+07/4.23e+09 =  0% of the original kernel matrix.

torch.Size([10327, 2])
We keep 5.94e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([22884, 2])
We keep 2.02e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([249684, 2])
We keep 4.04e+08/2.09e+10 =  1% of the original kernel matrix.

torch.Size([112709, 2])
We keep 3.49e+07/3.77e+09 =  0% of the original kernel matrix.

torch.Size([340291, 2])
We keep 2.98e+08/4.36e+10 =  0% of the original kernel matrix.

torch.Size([133883, 2])
We keep 4.87e+07/5.45e+09 =  0% of the original kernel matrix.

torch.Size([684353, 2])
We keep 9.50e+08/1.33e+11 =  0% of the original kernel matrix.

torch.Size([189104, 2])
We keep 7.92e+07/9.54e+09 =  0% of the original kernel matrix.

torch.Size([74672, 2])
We keep 2.44e+08/2.61e+09 =  9% of the original kernel matrix.

torch.Size([59607, 2])
We keep 1.40e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([5616, 2])
We keep 3.22e+05/6.22e+06 =  5% of the original kernel matrix.

torch.Size([17095, 2])
We keep 1.42e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([28281, 2])
We keep 3.45e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([37697, 2])
We keep 5.13e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([19712, 2])
We keep 3.09e+07/2.96e+08 = 10% of the original kernel matrix.

torch.Size([30461, 2])
We keep 5.48e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([47866, 2])
We keep 1.23e+07/7.13e+08 =  1% of the original kernel matrix.

torch.Size([48833, 2])
We keep 8.41e+06/6.97e+08 =  1% of the original kernel matrix.

torch.Size([99325, 2])
We keep 8.90e+07/3.12e+09 =  2% of the original kernel matrix.

torch.Size([69073, 2])
We keep 1.57e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([5504, 2])
We keep 1.94e+05/4.24e+06 =  4% of the original kernel matrix.

torch.Size([17534, 2])
We keep 1.25e+06/5.38e+07 =  2% of the original kernel matrix.

torch.Size([130630, 2])
We keep 7.18e+07/4.78e+09 =  1% of the original kernel matrix.

torch.Size([79192, 2])
We keep 1.85e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([263905, 2])
We keep 2.08e+08/2.34e+10 =  0% of the original kernel matrix.

torch.Size([116036, 2])
We keep 3.67e+07/3.99e+09 =  0% of the original kernel matrix.

torch.Size([3507, 2])
We keep 2.24e+05/2.37e+06 =  9% of the original kernel matrix.

torch.Size([14338, 2])
We keep 9.88e+05/4.02e+07 =  2% of the original kernel matrix.

torch.Size([16947, 2])
We keep 1.94e+06/6.62e+07 =  2% of the original kernel matrix.

torch.Size([28679, 2])
We keep 3.37e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([52188, 2])
We keep 1.69e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([50079, 2])
We keep 9.71e+06/8.32e+08 =  1% of the original kernel matrix.

torch.Size([27658, 2])
We keep 4.04e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([37514, 2])
We keep 5.16e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([30952, 2])
We keep 7.40e+06/2.92e+08 =  2% of the original kernel matrix.

torch.Size([39392, 2])
We keep 6.00e+06/4.46e+08 =  1% of the original kernel matrix.

torch.Size([28218, 2])
We keep 3.94e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([37649, 2])
We keep 5.37e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([7511, 2])
We keep 3.53e+05/8.78e+06 =  4% of the original kernel matrix.

torch.Size([19983, 2])
We keep 1.60e+06/7.74e+07 =  2% of the original kernel matrix.

torch.Size([38139, 2])
We keep 2.17e+07/6.56e+08 =  3% of the original kernel matrix.

torch.Size([41972, 2])
We keep 7.86e+06/6.69e+08 =  1% of the original kernel matrix.

torch.Size([69472, 2])
We keep 1.69e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([58801, 2])
We keep 1.11e+07/9.81e+08 =  1% of the original kernel matrix.

torch.Size([38550, 2])
We keep 9.27e+06/4.66e+08 =  1% of the original kernel matrix.

torch.Size([44383, 2])
We keep 7.18e+06/5.64e+08 =  1% of the original kernel matrix.

torch.Size([62007, 2])
We keep 3.25e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([54080, 2])
We keep 1.20e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([19623, 2])
We keep 3.23e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([30826, 2])
We keep 4.12e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([1638658, 2])
We keep 7.98e+09/7.96e+11 =  1% of the original kernel matrix.

torch.Size([300348, 2])
We keep 1.81e+08/2.33e+10 =  0% of the original kernel matrix.

torch.Size([20136, 2])
We keep 2.45e+06/9.96e+07 =  2% of the original kernel matrix.

torch.Size([31362, 2])
We keep 3.89e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([210974, 2])
We keep 1.16e+08/1.32e+10 =  0% of the original kernel matrix.

torch.Size([102735, 2])
We keep 2.85e+07/3.00e+09 =  0% of the original kernel matrix.

torch.Size([5273, 2])
We keep 2.19e+05/4.92e+06 =  4% of the original kernel matrix.

torch.Size([17000, 2])
We keep 1.32e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([9660, 2])
We keep 5.73e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([21958, 2])
We keep 1.98e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([10693, 2])
We keep 5.82e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([23024, 2])
We keep 2.07e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([32539, 2])
We keep 6.70e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([40741, 2])
We keep 5.62e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([14145, 2])
We keep 1.50e+06/4.40e+07 =  3% of the original kernel matrix.

torch.Size([26158, 2])
We keep 2.92e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([116143, 2])
We keep 1.51e+08/6.23e+09 =  2% of the original kernel matrix.

torch.Size([73182, 2])
We keep 2.09e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([16475, 2])
We keep 1.43e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([28148, 2])
We keep 3.09e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([9953, 2])
We keep 5.94e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([22200, 2])
We keep 2.02e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([509752, 2])
We keep 8.22e+08/8.72e+10 =  0% of the original kernel matrix.

torch.Size([160286, 2])
We keep 6.56e+07/7.71e+09 =  0% of the original kernel matrix.

torch.Size([81380, 2])
We keep 4.06e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([62138, 2])
We keep 1.38e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([59691, 2])
We keep 2.05e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([54066, 2])
We keep 1.06e+07/9.19e+08 =  1% of the original kernel matrix.

torch.Size([200030, 2])
We keep 1.51e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([100032, 2])
We keep 2.90e+07/3.04e+09 =  0% of the original kernel matrix.

torch.Size([539581, 2])
We keep 7.78e+08/9.99e+10 =  0% of the original kernel matrix.

torch.Size([164971, 2])
We keep 7.05e+07/8.25e+09 =  0% of the original kernel matrix.

torch.Size([18458, 2])
We keep 1.64e+06/7.23e+07 =  2% of the original kernel matrix.

torch.Size([29871, 2])
We keep 3.46e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([22599, 2])
We keep 2.36e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([33356, 2])
We keep 4.11e+06/2.80e+08 =  1% of the original kernel matrix.

torch.Size([33220, 2])
We keep 1.49e+07/5.38e+08 =  2% of the original kernel matrix.

torch.Size([39034, 2])
We keep 7.28e+06/6.06e+08 =  1% of the original kernel matrix.

torch.Size([4081, 2])
We keep 1.69e+05/3.18e+06 =  5% of the original kernel matrix.

torch.Size([15105, 2])
We keep 1.12e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([13003, 2])
We keep 9.87e+05/3.10e+07 =  3% of the original kernel matrix.

torch.Size([25196, 2])
We keep 2.53e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([31036, 2])
We keep 6.39e+06/3.08e+08 =  2% of the original kernel matrix.

torch.Size([39313, 2])
We keep 6.01e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([6241, 2])
We keep 3.19e+05/6.75e+06 =  4% of the original kernel matrix.

torch.Size([18311, 2])
We keep 1.47e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([12871, 2])
We keep 9.30e+05/3.07e+07 =  3% of the original kernel matrix.

torch.Size([25084, 2])
We keep 2.54e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([13177, 2])
We keep 9.15e+05/3.15e+07 =  2% of the original kernel matrix.

torch.Size([25377, 2])
We keep 2.51e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([10682, 2])
We keep 8.85e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([22794, 2])
We keep 2.30e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([425288, 2])
We keep 9.02e+08/6.90e+10 =  1% of the original kernel matrix.

torch.Size([146385, 2])
We keep 5.87e+07/6.86e+09 =  0% of the original kernel matrix.

torch.Size([28630, 2])
We keep 1.76e+07/4.84e+08 =  3% of the original kernel matrix.

torch.Size([36176, 2])
We keep 7.33e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([140808, 2])
We keep 9.01e+07/6.52e+09 =  1% of the original kernel matrix.

torch.Size([82540, 2])
We keep 2.11e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([7948, 2])
We keep 1.38e+06/1.54e+07 =  8% of the original kernel matrix.

torch.Size([20142, 2])
We keep 1.72e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([10516, 2])
We keep 9.65e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([22454, 2])
We keep 2.38e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([113450, 2])
We keep 8.13e+07/5.44e+09 =  1% of the original kernel matrix.

torch.Size([71478, 2])
We keep 1.97e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([948924, 2])
We keep 3.89e+09/3.12e+11 =  1% of the original kernel matrix.

torch.Size([230373, 2])
We keep 1.13e+08/1.46e+10 =  0% of the original kernel matrix.

torch.Size([838199, 2])
We keep 1.10e+09/1.97e+11 =  0% of the original kernel matrix.

torch.Size([212364, 2])
We keep 9.38e+07/1.16e+10 =  0% of the original kernel matrix.

torch.Size([26602, 2])
We keep 3.49e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([36642, 2])
We keep 4.97e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([81551, 2])
We keep 3.70e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([63103, 2])
We keep 1.28e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([6591, 2])
We keep 2.80e+05/6.61e+06 =  4% of the original kernel matrix.

torch.Size([18781, 2])
We keep 1.45e+06/6.71e+07 =  2% of the original kernel matrix.

torch.Size([10409, 2])
We keep 1.21e+06/2.41e+07 =  5% of the original kernel matrix.

torch.Size([22689, 2])
We keep 2.28e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([533831, 2])
We keep 4.26e+09/1.96e+11 =  2% of the original kernel matrix.

torch.Size([156131, 2])
We keep 9.52e+07/1.16e+10 =  0% of the original kernel matrix.

torch.Size([33257, 2])
We keep 6.43e+06/3.08e+08 =  2% of the original kernel matrix.

torch.Size([41005, 2])
We keep 5.84e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([22049, 2])
We keep 3.84e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([32749, 2])
We keep 4.15e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([74184, 2])
We keep 3.20e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([59663, 2])
We keep 1.29e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([1149826, 2])
We keep 4.33e+09/5.08e+11 =  0% of the original kernel matrix.

torch.Size([244484, 2])
We keep 1.47e+08/1.86e+10 =  0% of the original kernel matrix.

torch.Size([47750, 2])
We keep 1.57e+07/9.80e+08 =  1% of the original kernel matrix.

torch.Size([46954, 2])
We keep 9.56e+06/8.17e+08 =  1% of the original kernel matrix.

torch.Size([19254, 2])
We keep 3.29e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([30174, 2])
We keep 4.26e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([33602, 2])
We keep 7.93e+06/3.77e+08 =  2% of the original kernel matrix.

torch.Size([41978, 2])
We keep 6.68e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([60325, 2])
We keep 1.48e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([54740, 2])
We keep 9.98e+06/8.62e+08 =  1% of the original kernel matrix.

torch.Size([8831, 2])
We keep 5.79e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([21324, 2])
We keep 1.83e+06/9.39e+07 =  1% of the original kernel matrix.

torch.Size([321136, 2])
We keep 3.96e+08/3.89e+10 =  1% of the original kernel matrix.

torch.Size([128489, 2])
We keep 4.59e+07/5.15e+09 =  0% of the original kernel matrix.

torch.Size([24878, 2])
We keep 5.85e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([34688, 2])
We keep 5.24e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([4880, 2])
We keep 1.59e+05/3.22e+06 =  4% of the original kernel matrix.

torch.Size([16730, 2])
We keep 1.13e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([113345, 2])
We keep 4.69e+07/3.92e+09 =  1% of the original kernel matrix.

torch.Size([73301, 2])
We keep 1.69e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([54064, 2])
We keep 1.37e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([52656, 2])
We keep 9.89e+06/8.37e+08 =  1% of the original kernel matrix.

torch.Size([2532, 2])
We keep 6.55e+04/9.68e+05 =  6% of the original kernel matrix.

torch.Size([12805, 2])
We keep 7.57e+05/2.57e+07 =  2% of the original kernel matrix.

torch.Size([124299, 2])
We keep 8.88e+07/5.37e+09 =  1% of the original kernel matrix.

torch.Size([76893, 2])
We keep 1.92e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([223274, 2])
We keep 1.68e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([106079, 2])
We keep 3.11e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([379141, 2])
We keep 8.46e+08/5.64e+10 =  1% of the original kernel matrix.

torch.Size([140623, 2])
We keep 5.51e+07/6.20e+09 =  0% of the original kernel matrix.

torch.Size([387773, 2])
We keep 1.11e+09/6.02e+10 =  1% of the original kernel matrix.

torch.Size([139523, 2])
We keep 5.56e+07/6.41e+09 =  0% of the original kernel matrix.

torch.Size([275223, 2])
We keep 4.16e+08/3.52e+10 =  1% of the original kernel matrix.

torch.Size([116984, 2])
We keep 4.48e+07/4.90e+09 =  0% of the original kernel matrix.

torch.Size([29620, 2])
We keep 1.09e+07/4.39e+08 =  2% of the original kernel matrix.

torch.Size([37101, 2])
We keep 7.14e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([28828, 2])
We keep 1.24e+07/2.89e+08 =  4% of the original kernel matrix.

torch.Size([37794, 2])
We keep 5.86e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([181779, 2])
We keep 8.85e+07/9.55e+09 =  0% of the original kernel matrix.

torch.Size([95238, 2])
We keep 2.48e+07/2.55e+09 =  0% of the original kernel matrix.

torch.Size([366801, 2])
We keep 3.15e+08/3.99e+10 =  0% of the original kernel matrix.

torch.Size([139359, 2])
We keep 4.58e+07/5.22e+09 =  0% of the original kernel matrix.

torch.Size([20242, 2])
We keep 2.51e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([31520, 2])
We keep 3.96e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([127075, 2])
We keep 4.98e+08/1.09e+10 =  4% of the original kernel matrix.

torch.Size([79134, 2])
We keep 2.50e+07/2.73e+09 =  0% of the original kernel matrix.

torch.Size([290012, 2])
We keep 1.57e+09/4.13e+10 =  3% of the original kernel matrix.

torch.Size([121071, 2])
We keep 4.68e+07/5.31e+09 =  0% of the original kernel matrix.

torch.Size([260993, 2])
We keep 4.06e+08/2.84e+10 =  1% of the original kernel matrix.

torch.Size([115359, 2])
We keep 4.02e+07/4.40e+09 =  0% of the original kernel matrix.

torch.Size([134825, 2])
We keep 1.36e+08/8.24e+09 =  1% of the original kernel matrix.

torch.Size([79330, 2])
We keep 2.35e+07/2.37e+09 =  0% of the original kernel matrix.

torch.Size([60669, 2])
We keep 1.03e+08/2.78e+09 =  3% of the original kernel matrix.

torch.Size([49706, 2])
We keep 1.48e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([22759, 2])
We keep 3.99e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([33300, 2])
We keep 4.62e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([12870, 2])
We keep 7.96e+05/2.89e+07 =  2% of the original kernel matrix.

torch.Size([25327, 2])
We keep 2.42e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([23540, 2])
We keep 1.27e+07/2.30e+08 =  5% of the original kernel matrix.

torch.Size([33383, 2])
We keep 5.33e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([260754, 2])
We keep 1.56e+08/2.05e+10 =  0% of the original kernel matrix.

torch.Size([115312, 2])
We keep 3.45e+07/3.74e+09 =  0% of the original kernel matrix.

torch.Size([183425, 2])
We keep 8.86e+07/1.02e+10 =  0% of the original kernel matrix.

torch.Size([95626, 2])
We keep 2.54e+07/2.64e+09 =  0% of the original kernel matrix.

torch.Size([541210, 2])
We keep 1.44e+09/1.37e+11 =  1% of the original kernel matrix.

torch.Size([163177, 2])
We keep 8.03e+07/9.67e+09 =  0% of the original kernel matrix.

torch.Size([181863, 2])
We keep 2.16e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([94809, 2])
We keep 2.86e+07/2.99e+09 =  0% of the original kernel matrix.

torch.Size([116954, 2])
We keep 1.03e+08/4.74e+09 =  2% of the original kernel matrix.

torch.Size([75013, 2])
We keep 1.83e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([24391, 2])
We keep 3.70e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([34404, 2])
We keep 5.24e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([285638, 2])
We keep 3.02e+08/2.93e+10 =  1% of the original kernel matrix.

torch.Size([121592, 2])
We keep 4.10e+07/4.47e+09 =  0% of the original kernel matrix.

torch.Size([11320, 2])
We keep 1.76e+06/3.97e+07 =  4% of the original kernel matrix.

torch.Size([23014, 2])
We keep 2.80e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([21392, 2])
We keep 2.97e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([32244, 2])
We keep 4.24e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([327316, 2])
We keep 2.85e+08/3.33e+10 =  0% of the original kernel matrix.

torch.Size([130525, 2])
We keep 4.28e+07/4.77e+09 =  0% of the original kernel matrix.

torch.Size([76263, 2])
We keep 2.95e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([60765, 2])
We keep 1.24e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([14058, 2])
We keep 1.57e+06/4.78e+07 =  3% of the original kernel matrix.

torch.Size([25937, 2])
We keep 2.97e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([32879, 2])
We keep 5.65e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([40223, 2])
We keep 6.20e+06/4.82e+08 =  1% of the original kernel matrix.

torch.Size([562572, 2])
We keep 2.66e+09/1.76e+11 =  1% of the original kernel matrix.

torch.Size([162574, 2])
We keep 9.15e+07/1.09e+10 =  0% of the original kernel matrix.

torch.Size([41333, 2])
We keep 2.73e+07/6.17e+08 =  4% of the original kernel matrix.

torch.Size([44929, 2])
We keep 7.70e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([150185, 2])
We keep 1.13e+08/7.96e+09 =  1% of the original kernel matrix.

torch.Size([85546, 2])
We keep 2.31e+07/2.33e+09 =  0% of the original kernel matrix.

torch.Size([4436, 2])
We keep 1.45e+05/2.90e+06 =  4% of the original kernel matrix.

torch.Size([16022, 2])
We keep 1.10e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([8919, 2])
We keep 6.07e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([21012, 2])
We keep 1.98e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([11193, 2])
We keep 9.53e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([22999, 2])
We keep 2.42e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([15592, 2])
We keep 3.65e+06/6.25e+07 =  5% of the original kernel matrix.

torch.Size([27417, 2])
We keep 3.20e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([46872, 2])
We keep 1.12e+07/6.97e+08 =  1% of the original kernel matrix.

torch.Size([49062, 2])
We keep 8.35e+06/6.89e+08 =  1% of the original kernel matrix.

torch.Size([77901, 2])
We keep 3.61e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([59898, 2])
We keep 1.36e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([9076, 2])
We keep 1.57e+06/1.80e+07 =  8% of the original kernel matrix.

torch.Size([21078, 2])
We keep 2.06e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([292631, 2])
We keep 3.63e+08/2.92e+10 =  1% of the original kernel matrix.

torch.Size([123004, 2])
We keep 4.04e+07/4.46e+09 =  0% of the original kernel matrix.

torch.Size([14228, 2])
We keep 2.29e+06/5.79e+07 =  3% of the original kernel matrix.

torch.Size([26157, 2])
We keep 3.15e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([113116, 2])
We keep 6.70e+07/3.94e+09 =  1% of the original kernel matrix.

torch.Size([73564, 2])
We keep 1.72e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([18697, 2])
We keep 3.29e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([30219, 2])
We keep 4.07e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([138555, 2])
We keep 7.21e+07/5.74e+09 =  1% of the original kernel matrix.

torch.Size([81406, 2])
We keep 2.00e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([15580, 2])
We keep 2.10e+06/5.74e+07 =  3% of the original kernel matrix.

torch.Size([27462, 2])
We keep 3.18e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([40856, 2])
We keep 1.58e+07/4.97e+08 =  3% of the original kernel matrix.

torch.Size([45896, 2])
We keep 7.30e+06/5.82e+08 =  1% of the original kernel matrix.

torch.Size([51929, 2])
We keep 1.43e+07/9.10e+08 =  1% of the original kernel matrix.

torch.Size([50984, 2])
We keep 9.33e+06/7.88e+08 =  1% of the original kernel matrix.

torch.Size([43281, 2])
We keep 2.00e+07/7.26e+08 =  2% of the original kernel matrix.

torch.Size([46345, 2])
We keep 8.61e+06/7.04e+08 =  1% of the original kernel matrix.

torch.Size([135323, 2])
We keep 7.23e+07/6.07e+09 =  1% of the original kernel matrix.

torch.Size([81214, 2])
We keep 2.06e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([1013044, 2])
We keep 3.81e+09/3.62e+11 =  1% of the original kernel matrix.

torch.Size([231885, 2])
We keep 1.25e+08/1.57e+10 =  0% of the original kernel matrix.

torch.Size([20213, 2])
We keep 1.72e+06/8.07e+07 =  2% of the original kernel matrix.

torch.Size([31453, 2])
We keep 3.57e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([8747, 2])
We keep 7.11e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([21046, 2])
We keep 1.99e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([107911, 2])
We keep 1.76e+08/4.23e+09 =  4% of the original kernel matrix.

torch.Size([71871, 2])
We keep 1.74e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([35262, 2])
We keep 7.60e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([42186, 2])
We keep 6.77e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([762224, 2])
We keep 1.06e+09/1.73e+11 =  0% of the original kernel matrix.

torch.Size([201392, 2])
We keep 8.95e+07/1.09e+10 =  0% of the original kernel matrix.

torch.Size([5459, 2])
We keep 2.62e+05/4.99e+06 =  5% of the original kernel matrix.

torch.Size([17390, 2])
We keep 1.32e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([203728, 2])
We keep 1.61e+08/1.63e+10 =  0% of the original kernel matrix.

torch.Size([100047, 2])
We keep 3.15e+07/3.34e+09 =  0% of the original kernel matrix.

torch.Size([602769, 2])
We keep 6.56e+08/1.09e+11 =  0% of the original kernel matrix.

torch.Size([176611, 2])
We keep 7.23e+07/8.63e+09 =  0% of the original kernel matrix.

torch.Size([8740, 2])
We keep 6.07e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([20862, 2])
We keep 1.97e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([376762, 2])
We keep 1.00e+09/7.64e+10 =  1% of the original kernel matrix.

torch.Size([134536, 2])
We keep 6.17e+07/7.22e+09 =  0% of the original kernel matrix.

torch.Size([24084, 2])
We keep 8.93e+06/3.64e+08 =  2% of the original kernel matrix.

torch.Size([32980, 2])
We keep 6.50e+06/4.98e+08 =  1% of the original kernel matrix.

torch.Size([867562, 2])
We keep 2.39e+09/2.81e+11 =  0% of the original kernel matrix.

torch.Size([216613, 2])
We keep 1.12e+08/1.38e+10 =  0% of the original kernel matrix.

torch.Size([19620, 2])
We keep 1.96e+06/8.57e+07 =  2% of the original kernel matrix.

torch.Size([30883, 2])
We keep 3.68e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([29174, 2])
We keep 7.60e+06/2.77e+08 =  2% of the original kernel matrix.

torch.Size([37987, 2])
We keep 5.79e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([14288, 2])
We keep 3.71e+06/8.95e+07 =  4% of the original kernel matrix.

torch.Size([25548, 2])
We keep 3.83e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([9863, 2])
We keep 5.86e+05/1.60e+07 =  3% of the original kernel matrix.

torch.Size([22308, 2])
We keep 1.98e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([19452, 2])
We keep 3.30e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([30452, 2])
We keep 4.23e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([41292, 2])
We keep 7.31e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([45695, 2])
We keep 7.09e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([13985, 2])
We keep 1.19e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([26272, 2])
We keep 2.58e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([185058, 2])
We keep 1.61e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([95878, 2])
We keep 2.94e+07/3.09e+09 =  0% of the original kernel matrix.

torch.Size([28921, 2])
We keep 4.57e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([38186, 2])
We keep 5.78e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([14449, 2])
We keep 1.48e+06/4.06e+07 =  3% of the original kernel matrix.

torch.Size([26464, 2])
We keep 2.81e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([29841, 2])
We keep 8.13e+06/4.06e+08 =  2% of the original kernel matrix.

torch.Size([38172, 2])
We keep 6.94e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([337381, 2])
We keep 1.10e+09/6.82e+10 =  1% of the original kernel matrix.

torch.Size([128001, 2])
We keep 5.92e+07/6.82e+09 =  0% of the original kernel matrix.

torch.Size([148611, 2])
We keep 4.09e+08/1.57e+10 =  2% of the original kernel matrix.

torch.Size([82170, 2])
We keep 3.11e+07/3.27e+09 =  0% of the original kernel matrix.

torch.Size([603951, 2])
We keep 7.92e+08/1.04e+11 =  0% of the original kernel matrix.

torch.Size([176971, 2])
We keep 7.11e+07/8.41e+09 =  0% of the original kernel matrix.

torch.Size([29975, 2])
We keep 4.65e+06/2.32e+08 =  2% of the original kernel matrix.

torch.Size([38919, 2])
We keep 5.30e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([16285, 2])
We keep 4.49e+06/9.19e+07 =  4% of the original kernel matrix.

torch.Size([28118, 2])
We keep 3.69e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([11594, 2])
We keep 8.08e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([24042, 2])
We keep 2.28e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([8036, 2])
We keep 1.15e+06/1.89e+07 =  6% of the original kernel matrix.

torch.Size([20119, 2])
We keep 2.11e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([347809, 2])
We keep 6.11e+08/3.94e+10 =  1% of the original kernel matrix.

torch.Size([135518, 2])
We keep 4.48e+07/5.18e+09 =  0% of the original kernel matrix.

torch.Size([17955, 2])
We keep 2.13e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([29701, 2])
We keep 3.65e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([39925, 2])
We keep 9.33e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([45734, 2])
We keep 7.52e+06/5.88e+08 =  1% of the original kernel matrix.

torch.Size([47276, 2])
We keep 1.88e+07/8.55e+08 =  2% of the original kernel matrix.

torch.Size([47569, 2])
We keep 8.93e+06/7.63e+08 =  1% of the original kernel matrix.

torch.Size([38846, 2])
We keep 1.84e+07/9.05e+08 =  2% of the original kernel matrix.

torch.Size([41772, 2])
We keep 9.19e+06/7.85e+08 =  1% of the original kernel matrix.

torch.Size([18334, 2])
We keep 3.82e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([29793, 2])
We keep 4.13e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([21230, 2])
We keep 2.86e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([32066, 2])
We keep 4.37e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([362136, 2])
We keep 8.53e+08/6.10e+10 =  1% of the original kernel matrix.

torch.Size([132064, 2])
We keep 5.57e+07/6.45e+09 =  0% of the original kernel matrix.

torch.Size([70200, 2])
We keep 3.09e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([58247, 2])
We keep 1.24e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([32507, 2])
We keep 1.52e+08/7.86e+08 = 19% of the original kernel matrix.

torch.Size([38697, 2])
We keep 8.62e+06/7.32e+08 =  1% of the original kernel matrix.

torch.Size([44727, 2])
We keep 1.51e+07/6.19e+08 =  2% of the original kernel matrix.

torch.Size([48356, 2])
We keep 7.95e+06/6.50e+08 =  1% of the original kernel matrix.

torch.Size([183703, 2])
We keep 7.75e+08/1.86e+10 =  4% of the original kernel matrix.

torch.Size([94763, 2])
We keep 3.37e+07/3.56e+09 =  0% of the original kernel matrix.

torch.Size([187418, 2])
We keep 5.12e+08/1.47e+10 =  3% of the original kernel matrix.

torch.Size([96083, 2])
We keep 2.99e+07/3.17e+09 =  0% of the original kernel matrix.

torch.Size([491181, 2])
We keep 1.48e+09/1.28e+11 =  1% of the original kernel matrix.

torch.Size([151186, 2])
We keep 7.87e+07/9.33e+09 =  0% of the original kernel matrix.

torch.Size([178855, 2])
We keep 1.81e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([94293, 2])
We keep 2.55e+07/2.64e+09 =  0% of the original kernel matrix.

torch.Size([12735, 2])
We keep 3.26e+06/3.54e+07 =  9% of the original kernel matrix.

torch.Size([25285, 2])
We keep 2.52e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([75172, 2])
We keep 2.75e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([60850, 2])
We keep 1.20e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([43867, 2])
We keep 3.97e+07/1.24e+09 =  3% of the original kernel matrix.

torch.Size([44099, 2])
We keep 1.08e+07/9.19e+08 =  1% of the original kernel matrix.

torch.Size([15497, 2])
We keep 1.39e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([27442, 2])
We keep 2.98e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([26692, 2])
We keep 4.94e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([36067, 2])
We keep 5.28e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([12379, 2])
We keep 7.95e+05/2.70e+07 =  2% of the original kernel matrix.

torch.Size([24738, 2])
We keep 2.39e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([49282, 2])
We keep 3.13e+07/8.23e+08 =  3% of the original kernel matrix.

torch.Size([49542, 2])
We keep 8.90e+06/7.49e+08 =  1% of the original kernel matrix.

torch.Size([40529, 2])
We keep 1.47e+07/6.97e+08 =  2% of the original kernel matrix.

torch.Size([44325, 2])
We keep 8.48e+06/6.89e+08 =  1% of the original kernel matrix.

torch.Size([9427, 2])
We keep 1.26e+06/2.24e+07 =  5% of the original kernel matrix.

torch.Size([20887, 2])
We keep 2.21e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([35073, 2])
We keep 6.28e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([42133, 2])
We keep 6.58e+06/5.10e+08 =  1% of the original kernel matrix.

torch.Size([13285, 2])
We keep 1.88e+06/5.67e+07 =  3% of the original kernel matrix.

torch.Size([25086, 2])
We keep 3.03e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([41669, 2])
We keep 4.45e+07/1.51e+09 =  2% of the original kernel matrix.

torch.Size([42959, 2])
We keep 1.14e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([15289, 2])
We keep 2.09e+06/6.24e+07 =  3% of the original kernel matrix.

torch.Size([27206, 2])
We keep 3.27e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([46069, 2])
We keep 1.58e+07/7.22e+08 =  2% of the original kernel matrix.

torch.Size([48091, 2])
We keep 8.60e+06/7.02e+08 =  1% of the original kernel matrix.

torch.Size([12454, 2])
We keep 1.26e+07/8.30e+07 = 15% of the original kernel matrix.

torch.Size([23944, 2])
We keep 3.63e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([30386, 2])
We keep 7.64e+06/3.22e+08 =  2% of the original kernel matrix.

torch.Size([38712, 2])
We keep 6.12e+06/4.68e+08 =  1% of the original kernel matrix.

torch.Size([13299, 2])
We keep 1.47e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([25247, 2])
We keep 2.81e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([21868, 2])
We keep 2.43e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([32844, 2])
We keep 4.11e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([44406, 2])
We keep 8.13e+06/5.62e+08 =  1% of the original kernel matrix.

torch.Size([44827, 2])
We keep 7.05e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([196637, 2])
We keep 1.61e+08/1.26e+10 =  1% of the original kernel matrix.

torch.Size([99162, 2])
We keep 2.81e+07/2.93e+09 =  0% of the original kernel matrix.

torch.Size([16725, 2])
We keep 1.74e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([28629, 2])
We keep 3.22e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([5312, 2])
We keep 1.68e+05/3.87e+06 =  4% of the original kernel matrix.

torch.Size([17274, 2])
We keep 1.20e+06/5.14e+07 =  2% of the original kernel matrix.

torch.Size([15924, 2])
We keep 3.11e+06/7.31e+07 =  4% of the original kernel matrix.

torch.Size([27644, 2])
We keep 3.47e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([170785, 2])
We keep 3.28e+08/1.24e+10 =  2% of the original kernel matrix.

torch.Size([91721, 2])
We keep 2.76e+07/2.91e+09 =  0% of the original kernel matrix.

torch.Size([896023, 2])
We keep 2.43e+09/2.58e+11 =  0% of the original kernel matrix.

torch.Size([221062, 2])
We keep 1.08e+08/1.33e+10 =  0% of the original kernel matrix.

torch.Size([61703, 2])
We keep 3.54e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([52019, 2])
We keep 1.27e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([15808, 2])
We keep 1.54e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([27691, 2])
We keep 3.09e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([198725, 2])
We keep 3.06e+08/2.10e+10 =  1% of the original kernel matrix.

torch.Size([97968, 2])
We keep 3.51e+07/3.78e+09 =  0% of the original kernel matrix.

torch.Size([304095, 2])
We keep 2.33e+08/3.28e+10 =  0% of the original kernel matrix.

torch.Size([126739, 2])
We keep 4.29e+07/4.73e+09 =  0% of the original kernel matrix.

torch.Size([6654, 2])
We keep 2.65e+05/6.25e+06 =  4% of the original kernel matrix.

torch.Size([18767, 2])
We keep 1.43e+06/6.53e+07 =  2% of the original kernel matrix.

torch.Size([18119, 2])
We keep 2.34e+06/8.54e+07 =  2% of the original kernel matrix.

torch.Size([29787, 2])
We keep 3.65e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([63342, 2])
We keep 1.31e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([56442, 2])
We keep 1.02e+07/8.98e+08 =  1% of the original kernel matrix.

torch.Size([97894, 2])
We keep 2.24e+08/9.21e+09 =  2% of the original kernel matrix.

torch.Size([64688, 2])
We keep 2.47e+07/2.51e+09 =  0% of the original kernel matrix.

torch.Size([171346, 2])
We keep 7.26e+08/2.35e+10 =  3% of the original kernel matrix.

torch.Size([89948, 2])
We keep 3.68e+07/4.00e+09 =  0% of the original kernel matrix.

torch.Size([321189, 2])
We keep 3.03e+08/3.46e+10 =  0% of the original kernel matrix.

torch.Size([128656, 2])
We keep 4.35e+07/4.86e+09 =  0% of the original kernel matrix.

torch.Size([32493, 2])
We keep 5.09e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([40807, 2])
We keep 5.84e+06/4.45e+08 =  1% of the original kernel matrix.

torch.Size([11921, 2])
We keep 7.07e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([24322, 2])
We keep 2.25e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([8517, 2])
We keep 3.84e+05/1.05e+07 =  3% of the original kernel matrix.

torch.Size([20883, 2])
We keep 1.70e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([83745, 2])
We keep 6.06e+07/3.05e+09 =  1% of the original kernel matrix.

torch.Size([62637, 2])
We keep 1.57e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([24798, 2])
We keep 2.82e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([35205, 2])
We keep 4.53e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([21592, 2])
We keep 1.07e+07/2.69e+08 =  3% of the original kernel matrix.

torch.Size([31445, 2])
We keep 5.66e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([175703, 2])
We keep 1.23e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([93102, 2])
We keep 2.62e+07/2.67e+09 =  0% of the original kernel matrix.

torch.Size([904831, 2])
We keep 1.50e+09/2.46e+11 =  0% of the original kernel matrix.

torch.Size([221736, 2])
We keep 1.05e+08/1.29e+10 =  0% of the original kernel matrix.

torch.Size([20925, 2])
We keep 2.91e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([31941, 2])
We keep 4.12e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([5736, 2])
We keep 2.46e+05/5.32e+06 =  4% of the original kernel matrix.

torch.Size([17571, 2])
We keep 1.35e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([24151, 2])
We keep 3.53e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([34688, 2])
We keep 4.93e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([135024, 2])
We keep 3.28e+08/5.58e+09 =  5% of the original kernel matrix.

torch.Size([81410, 2])
We keep 1.92e+07/1.95e+09 =  0% of the original kernel matrix.

torch.Size([72151, 2])
We keep 2.78e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([59272, 2])
We keep 1.17e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([6310, 2])
We keep 2.89e+06/1.27e+07 = 22% of the original kernel matrix.

torch.Size([17853, 2])
We keep 1.86e+06/9.31e+07 =  1% of the original kernel matrix.

torch.Size([9031, 2])
We keep 8.45e+05/1.55e+07 =  5% of the original kernel matrix.

torch.Size([21235, 2])
We keep 1.95e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([84520, 2])
We keep 5.70e+07/2.42e+09 =  2% of the original kernel matrix.

torch.Size([63862, 2])
We keep 1.39e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([6792, 2])
We keep 4.65e+05/9.11e+06 =  5% of the original kernel matrix.

torch.Size([18794, 2])
We keep 1.62e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([53755, 2])
We keep 5.01e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([48949, 2])
We keep 1.20e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([12568, 2])
We keep 1.02e+06/3.09e+07 =  3% of the original kernel matrix.

torch.Size([24873, 2])
We keep 2.53e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([32538, 2])
We keep 4.22e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([40768, 2])
We keep 5.74e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([31898, 2])
We keep 6.13e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([40201, 2])
We keep 6.22e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([16919, 2])
We keep 1.24e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([28656, 2])
We keep 3.05e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([17026, 2])
We keep 1.46e+06/5.91e+07 =  2% of the original kernel matrix.

torch.Size([28605, 2])
We keep 3.20e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([296394, 2])
We keep 2.29e+08/2.68e+10 =  0% of the original kernel matrix.

torch.Size([123974, 2])
We keep 3.89e+07/4.28e+09 =  0% of the original kernel matrix.

torch.Size([640990, 2])
We keep 9.06e+08/1.13e+11 =  0% of the original kernel matrix.

torch.Size([182317, 2])
We keep 7.38e+07/8.78e+09 =  0% of the original kernel matrix.

torch.Size([153073, 2])
We keep 2.12e+08/1.03e+10 =  2% of the original kernel matrix.

torch.Size([86906, 2])
We keep 2.58e+07/2.65e+09 =  0% of the original kernel matrix.

torch.Size([223745, 2])
We keep 6.72e+08/2.17e+10 =  3% of the original kernel matrix.

torch.Size([104842, 2])
We keep 3.52e+07/3.84e+09 =  0% of the original kernel matrix.

torch.Size([41389, 2])
We keep 7.06e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([45937, 2])
We keep 7.18e+06/5.74e+08 =  1% of the original kernel matrix.

torch.Size([31209, 2])
We keep 6.50e+06/2.74e+08 =  2% of the original kernel matrix.

torch.Size([39614, 2])
We keep 5.76e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([27634, 2])
We keep 5.27e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([37115, 2])
We keep 5.69e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([253101, 2])
We keep 1.68e+08/1.94e+10 =  0% of the original kernel matrix.

torch.Size([113037, 2])
We keep 3.38e+07/3.63e+09 =  0% of the original kernel matrix.

torch.Size([69832, 2])
We keep 8.38e+07/2.16e+09 =  3% of the original kernel matrix.

torch.Size([57835, 2])
We keep 1.31e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([64396, 2])
We keep 1.06e+08/1.69e+09 =  6% of the original kernel matrix.

torch.Size([55395, 2])
We keep 1.17e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([26791, 2])
We keep 4.00e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([36587, 2])
We keep 5.07e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([1959415, 2])
We keep 9.86e+09/1.23e+12 =  0% of the original kernel matrix.

torch.Size([328851, 2])
We keep 2.21e+08/2.90e+10 =  0% of the original kernel matrix.

torch.Size([40608, 2])
We keep 1.01e+07/4.98e+08 =  2% of the original kernel matrix.

torch.Size([45323, 2])
We keep 7.24e+06/5.83e+08 =  1% of the original kernel matrix.

torch.Size([110743, 2])
We keep 5.80e+07/4.41e+09 =  1% of the original kernel matrix.

torch.Size([71988, 2])
We keep 1.80e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([278273, 2])
We keep 2.98e+08/2.90e+10 =  1% of the original kernel matrix.

torch.Size([119690, 2])
We keep 4.03e+07/4.45e+09 =  0% of the original kernel matrix.

torch.Size([91036, 2])
We keep 3.06e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([66575, 2])
We keep 1.35e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([65823, 2])
We keep 6.66e+07/1.69e+09 =  3% of the original kernel matrix.

torch.Size([56525, 2])
We keep 1.18e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([941010, 2])
We keep 3.25e+09/3.44e+11 =  0% of the original kernel matrix.

torch.Size([225927, 2])
We keep 1.24e+08/1.53e+10 =  0% of the original kernel matrix.

torch.Size([225578, 2])
We keep 1.20e+08/1.54e+10 =  0% of the original kernel matrix.

torch.Size([106831, 2])
We keep 3.04e+07/3.24e+09 =  0% of the original kernel matrix.

torch.Size([151089, 2])
We keep 1.19e+08/8.32e+09 =  1% of the original kernel matrix.

torch.Size([86005, 2])
We keep 2.35e+07/2.38e+09 =  0% of the original kernel matrix.

torch.Size([17649, 2])
We keep 2.53e+06/6.71e+07 =  3% of the original kernel matrix.

torch.Size([29382, 2])
We keep 3.29e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([62528, 2])
We keep 2.90e+07/1.41e+09 =  2% of the original kernel matrix.

torch.Size([55260, 2])
We keep 1.11e+07/9.81e+08 =  1% of the original kernel matrix.

torch.Size([17391, 2])
We keep 2.63e+06/7.57e+07 =  3% of the original kernel matrix.

torch.Size([29380, 2])
We keep 3.44e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([346838, 2])
We keep 6.80e+08/3.76e+10 =  1% of the original kernel matrix.

torch.Size([134351, 2])
We keep 4.53e+07/5.06e+09 =  0% of the original kernel matrix.

torch.Size([2066763, 2])
We keep 7.26e+09/1.13e+12 =  0% of the original kernel matrix.

torch.Size([339975, 2])
We keep 2.13e+08/2.78e+10 =  0% of the original kernel matrix.

torch.Size([150420, 2])
We keep 2.95e+08/9.76e+09 =  3% of the original kernel matrix.

torch.Size([85451, 2])
We keep 2.53e+07/2.58e+09 =  0% of the original kernel matrix.

torch.Size([242320, 2])
We keep 3.10e+08/2.85e+10 =  1% of the original kernel matrix.

torch.Size([106554, 2])
We keep 4.01e+07/4.41e+09 =  0% of the original kernel matrix.

torch.Size([98419, 2])
We keep 2.10e+08/4.28e+09 =  4% of the original kernel matrix.

torch.Size([67680, 2])
We keep 1.79e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([42769, 2])
We keep 2.25e+07/8.08e+08 =  2% of the original kernel matrix.

torch.Size([45236, 2])
We keep 8.91e+06/7.42e+08 =  1% of the original kernel matrix.

torch.Size([59632, 2])
We keep 2.68e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([54339, 2])
We keep 1.14e+07/9.83e+08 =  1% of the original kernel matrix.

torch.Size([15262, 2])
We keep 2.63e+06/8.32e+07 =  3% of the original kernel matrix.

torch.Size([26813, 2])
We keep 3.63e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([116480, 2])
We keep 4.44e+08/1.21e+10 =  3% of the original kernel matrix.

torch.Size([73653, 2])
We keep 2.80e+07/2.87e+09 =  0% of the original kernel matrix.

torch.Size([736206, 2])
We keep 8.53e+08/1.49e+11 =  0% of the original kernel matrix.

torch.Size([196964, 2])
We keep 8.30e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([89060, 2])
We keep 1.13e+08/6.18e+09 =  1% of the original kernel matrix.

torch.Size([60146, 2])
We keep 2.10e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([1001683, 2])
We keep 8.27e+09/5.35e+11 =  1% of the original kernel matrix.

torch.Size([220764, 2])
We keep 1.51e+08/1.91e+10 =  0% of the original kernel matrix.

torch.Size([193143, 2])
We keep 3.98e+08/2.48e+10 =  1% of the original kernel matrix.

torch.Size([96050, 2])
We keep 3.82e+07/4.11e+09 =  0% of the original kernel matrix.

torch.Size([68723, 2])
We keep 1.81e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([58101, 2])
We keep 1.12e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([16101, 2])
We keep 2.25e+06/6.66e+07 =  3% of the original kernel matrix.

torch.Size([28013, 2])
We keep 3.36e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([116609, 2])
We keep 9.75e+07/4.47e+09 =  2% of the original kernel matrix.

torch.Size([75100, 2])
We keep 1.80e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([251124, 2])
We keep 1.52e+08/1.84e+10 =  0% of the original kernel matrix.

torch.Size([113029, 2])
We keep 3.28e+07/3.54e+09 =  0% of the original kernel matrix.

torch.Size([13964, 2])
We keep 2.60e+06/7.35e+07 =  3% of the original kernel matrix.

torch.Size([25619, 2])
We keep 3.48e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([963860, 2])
We keep 1.44e+09/2.70e+11 =  0% of the original kernel matrix.

torch.Size([231924, 2])
We keep 1.09e+08/1.36e+10 =  0% of the original kernel matrix.

torch.Size([48580, 2])
We keep 6.32e+07/1.11e+09 =  5% of the original kernel matrix.

torch.Size([48930, 2])
We keep 9.60e+06/8.69e+08 =  1% of the original kernel matrix.

torch.Size([54581, 2])
We keep 2.14e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([51354, 2])
We keep 1.03e+07/8.90e+08 =  1% of the original kernel matrix.

torch.Size([12834, 2])
We keep 1.63e+06/3.61e+07 =  4% of the original kernel matrix.

torch.Size([25523, 2])
We keep 2.57e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([16371, 2])
We keep 1.38e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([28077, 2])
We keep 3.07e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([4658, 2])
We keep 1.86e+05/3.60e+06 =  5% of the original kernel matrix.

torch.Size([16282, 2])
We keep 1.19e+06/4.96e+07 =  2% of the original kernel matrix.

torch.Size([14249, 2])
We keep 4.27e+06/5.07e+07 =  8% of the original kernel matrix.

torch.Size([25999, 2])
We keep 2.97e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([40331, 2])
We keep 1.50e+07/5.43e+08 =  2% of the original kernel matrix.

torch.Size([45468, 2])
We keep 7.81e+06/6.09e+08 =  1% of the original kernel matrix.

torch.Size([70002, 2])
We keep 8.56e+07/1.87e+09 =  4% of the original kernel matrix.

torch.Size([57350, 2])
We keep 1.23e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([4842380, 2])
We keep 7.11e+10/9.62e+12 =  0% of the original kernel matrix.

torch.Size([490809, 2])
We keep 5.82e+08/8.10e+10 =  0% of the original kernel matrix.

torch.Size([19883, 2])
We keep 5.21e+06/1.66e+08 =  3% of the original kernel matrix.

torch.Size([30455, 2])
We keep 4.64e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([247411, 2])
We keep 1.43e+08/1.84e+10 =  0% of the original kernel matrix.

torch.Size([111934, 2])
We keep 3.27e+07/3.54e+09 =  0% of the original kernel matrix.

torch.Size([16065, 2])
We keep 1.74e+06/6.37e+07 =  2% of the original kernel matrix.

torch.Size([27932, 2])
We keep 3.34e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([43395, 2])
We keep 8.92e+06/5.58e+08 =  1% of the original kernel matrix.

torch.Size([44018, 2])
We keep 7.09e+06/6.17e+08 =  1% of the original kernel matrix.

torch.Size([15331, 2])
We keep 2.57e+06/7.18e+07 =  3% of the original kernel matrix.

torch.Size([26940, 2])
We keep 3.51e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([243331, 2])
We keep 5.30e+08/3.66e+10 =  1% of the original kernel matrix.

torch.Size([109253, 2])
We keep 4.55e+07/5.00e+09 =  0% of the original kernel matrix.

torch.Size([59159, 2])
We keep 9.24e+07/2.20e+09 =  4% of the original kernel matrix.

torch.Size([51768, 2])
We keep 1.35e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([1657381, 2])
We keep 3.78e+09/7.10e+11 =  0% of the original kernel matrix.

torch.Size([302005, 2])
We keep 1.71e+08/2.20e+10 =  0% of the original kernel matrix.

torch.Size([17168, 2])
We keep 2.47e+06/6.87e+07 =  3% of the original kernel matrix.

torch.Size([28984, 2])
We keep 3.41e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([308615, 2])
We keep 2.98e+08/3.14e+10 =  0% of the original kernel matrix.

torch.Size([127055, 2])
We keep 4.18e+07/4.63e+09 =  0% of the original kernel matrix.

torch.Size([70345, 2])
We keep 7.65e+07/1.73e+09 =  4% of the original kernel matrix.

torch.Size([58509, 2])
We keep 1.20e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([253627, 2])
We keep 2.60e+08/2.36e+10 =  1% of the original kernel matrix.

torch.Size([113282, 2])
We keep 3.70e+07/4.01e+09 =  0% of the original kernel matrix.

torch.Size([471256, 2])
We keep 1.13e+09/1.04e+11 =  1% of the original kernel matrix.

torch.Size([155091, 2])
We keep 7.10e+07/8.43e+09 =  0% of the original kernel matrix.

torch.Size([44222, 2])
We keep 1.07e+07/5.94e+08 =  1% of the original kernel matrix.

torch.Size([47657, 2])
We keep 7.98e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([21872, 2])
We keep 3.35e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([32857, 2])
We keep 4.37e+06/2.99e+08 =  1% of the original kernel matrix.

torch.Size([200382, 2])
We keep 1.08e+08/1.27e+10 =  0% of the original kernel matrix.

torch.Size([100260, 2])
We keep 2.82e+07/2.95e+09 =  0% of the original kernel matrix.

torch.Size([26043, 2])
We keep 5.11e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([36045, 2])
We keep 4.91e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([43798, 2])
We keep 1.83e+07/7.53e+08 =  2% of the original kernel matrix.

torch.Size([45961, 2])
We keep 8.66e+06/7.16e+08 =  1% of the original kernel matrix.

torch.Size([330471, 2])
We keep 3.87e+08/3.60e+10 =  1% of the original kernel matrix.

torch.Size([131931, 2])
We keep 4.49e+07/4.96e+09 =  0% of the original kernel matrix.

torch.Size([25919, 2])
We keep 2.77e+07/3.52e+08 =  7% of the original kernel matrix.

torch.Size([35172, 2])
We keep 6.18e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([127269, 2])
We keep 1.37e+08/6.10e+09 =  2% of the original kernel matrix.

torch.Size([79518, 2])
We keep 2.10e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([55946, 2])
We keep 2.28e+07/1.09e+09 =  2% of the original kernel matrix.

torch.Size([51833, 2])
We keep 1.00e+07/8.61e+08 =  1% of the original kernel matrix.

torch.Size([206141, 2])
We keep 2.74e+08/1.73e+10 =  1% of the original kernel matrix.

torch.Size([101081, 2])
We keep 3.22e+07/3.44e+09 =  0% of the original kernel matrix.

torch.Size([474173, 2])
We keep 1.29e+09/1.02e+11 =  1% of the original kernel matrix.

torch.Size([152279, 2])
We keep 7.00e+07/8.33e+09 =  0% of the original kernel matrix.

torch.Size([63326, 2])
We keep 1.76e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([56126, 2])
We keep 1.04e+07/9.02e+08 =  1% of the original kernel matrix.

torch.Size([18356, 2])
We keep 2.78e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([29760, 2])
We keep 4.08e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([161337, 2])
We keep 2.01e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([87059, 2])
We keep 2.79e+07/2.91e+09 =  0% of the original kernel matrix.

torch.Size([122793, 2])
We keep 8.61e+07/5.83e+09 =  1% of the original kernel matrix.

torch.Size([75198, 2])
We keep 2.03e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([390368, 2])
We keep 2.43e+09/9.83e+10 =  2% of the original kernel matrix.

torch.Size([135855, 2])
We keep 6.93e+07/8.19e+09 =  0% of the original kernel matrix.

torch.Size([65437, 2])
We keep 4.66e+07/1.70e+09 =  2% of the original kernel matrix.

torch.Size([55412, 2])
We keep 1.22e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([46565, 2])
We keep 2.19e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([46118, 2])
We keep 9.81e+06/8.49e+08 =  1% of the original kernel matrix.

torch.Size([56338, 2])
We keep 3.20e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([52354, 2])
We keep 1.04e+07/9.17e+08 =  1% of the original kernel matrix.

torch.Size([193209, 2])
We keep 4.00e+08/2.54e+10 =  1% of the original kernel matrix.

torch.Size([92093, 2])
We keep 3.80e+07/4.17e+09 =  0% of the original kernel matrix.

torch.Size([153916, 2])
We keep 2.15e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([85800, 2])
We keep 2.67e+07/2.75e+09 =  0% of the original kernel matrix.

torch.Size([51728, 2])
We keep 1.31e+07/8.30e+08 =  1% of the original kernel matrix.

torch.Size([51278, 2])
We keep 8.99e+06/7.52e+08 =  1% of the original kernel matrix.

torch.Size([14367, 2])
We keep 1.31e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([26402, 2])
We keep 2.81e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([23884, 2])
We keep 3.03e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([34511, 2])
We keep 4.61e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([80233, 2])
We keep 9.83e+07/3.32e+09 =  2% of the original kernel matrix.

torch.Size([59833, 2])
We keep 1.58e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([136573, 2])
We keep 1.98e+08/9.09e+09 =  2% of the original kernel matrix.

torch.Size([80118, 2])
We keep 2.45e+07/2.49e+09 =  0% of the original kernel matrix.

torch.Size([209100, 2])
We keep 2.63e+08/1.28e+10 =  2% of the original kernel matrix.

torch.Size([102625, 2])
We keep 2.81e+07/2.96e+09 =  0% of the original kernel matrix.

torch.Size([26644, 2])
We keep 3.92e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([36521, 2])
We keep 5.06e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([45717, 2])
We keep 8.75e+06/6.76e+08 =  1% of the original kernel matrix.

torch.Size([47710, 2])
We keep 8.17e+06/6.79e+08 =  1% of the original kernel matrix.

torch.Size([58714, 2])
We keep 1.98e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([53946, 2])
We keep 9.97e+06/8.51e+08 =  1% of the original kernel matrix.

torch.Size([164030, 2])
We keep 2.62e+08/7.91e+09 =  3% of the original kernel matrix.

torch.Size([89946, 2])
We keep 2.22e+07/2.32e+09 =  0% of the original kernel matrix.

torch.Size([39473, 2])
We keep 1.02e+07/5.04e+08 =  2% of the original kernel matrix.

torch.Size([45149, 2])
We keep 7.45e+06/5.87e+08 =  1% of the original kernel matrix.

torch.Size([146483, 2])
We keep 5.60e+07/5.85e+09 =  0% of the original kernel matrix.

torch.Size([84019, 2])
We keep 2.01e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([19930, 2])
We keep 4.85e+06/1.31e+08 =  3% of the original kernel matrix.

torch.Size([30804, 2])
We keep 4.32e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([54658, 2])
We keep 1.30e+07/9.33e+08 =  1% of the original kernel matrix.

torch.Size([52614, 2])
We keep 9.32e+06/7.98e+08 =  1% of the original kernel matrix.

torch.Size([6132, 2])
We keep 3.21e+05/6.87e+06 =  4% of the original kernel matrix.

torch.Size([17984, 2])
We keep 1.49e+06/6.85e+07 =  2% of the original kernel matrix.

torch.Size([19130, 2])
We keep 2.08e+06/8.21e+07 =  2% of the original kernel matrix.

torch.Size([30493, 2])
We keep 3.63e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([263103, 2])
We keep 1.72e+08/2.16e+10 =  0% of the original kernel matrix.

torch.Size([116152, 2])
We keep 3.49e+07/3.83e+09 =  0% of the original kernel matrix.

torch.Size([31725, 2])
We keep 9.31e+06/3.79e+08 =  2% of the original kernel matrix.

torch.Size([39360, 2])
We keep 6.57e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([143074, 2])
We keep 5.41e+07/5.46e+09 =  0% of the original kernel matrix.

torch.Size([83185, 2])
We keep 1.94e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([6823, 2])
We keep 2.68e+05/7.02e+06 =  3% of the original kernel matrix.

torch.Size([19337, 2])
We keep 1.47e+06/6.92e+07 =  2% of the original kernel matrix.

torch.Size([14006, 2])
We keep 2.49e+06/4.69e+07 =  5% of the original kernel matrix.

torch.Size([26168, 2])
We keep 2.97e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([47604, 2])
We keep 1.39e+07/7.30e+08 =  1% of the original kernel matrix.

torch.Size([48841, 2])
We keep 8.48e+06/7.05e+08 =  1% of the original kernel matrix.

torch.Size([99768, 2])
We keep 3.92e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([68648, 2])
We keep 1.56e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([50336, 2])
We keep 1.22e+07/8.60e+08 =  1% of the original kernel matrix.

torch.Size([50631, 2])
We keep 9.15e+06/7.66e+08 =  1% of the original kernel matrix.

torch.Size([10263, 2])
We keep 7.11e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([22571, 2])
We keep 2.17e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([14360, 2])
We keep 1.69e+06/5.08e+07 =  3% of the original kernel matrix.

torch.Size([26329, 2])
We keep 3.03e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([47059, 2])
We keep 1.51e+07/7.11e+08 =  2% of the original kernel matrix.

torch.Size([49037, 2])
We keep 8.47e+06/6.96e+08 =  1% of the original kernel matrix.

torch.Size([40935, 2])
We keep 9.64e+06/5.36e+08 =  1% of the original kernel matrix.

torch.Size([45163, 2])
We keep 7.32e+06/6.05e+08 =  1% of the original kernel matrix.

torch.Size([172831, 2])
We keep 2.15e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([91592, 2])
We keep 2.81e+07/2.91e+09 =  0% of the original kernel matrix.

torch.Size([13673, 2])
We keep 1.66e+06/4.62e+07 =  3% of the original kernel matrix.

torch.Size([25891, 2])
We keep 2.86e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([19818, 2])
We keep 7.41e+06/1.42e+08 =  5% of the original kernel matrix.

torch.Size([30361, 2])
We keep 4.42e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([62476, 2])
We keep 7.65e+07/2.59e+09 =  2% of the original kernel matrix.

torch.Size([51150, 2])
We keep 1.44e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([18306, 2])
We keep 3.04e+06/8.92e+07 =  3% of the original kernel matrix.

torch.Size([29692, 2])
We keep 3.72e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([314561, 2])
We keep 5.20e+08/4.14e+10 =  1% of the original kernel matrix.

torch.Size([126982, 2])
We keep 4.61e+07/5.32e+09 =  0% of the original kernel matrix.

torch.Size([21998, 2])
We keep 1.68e+07/1.69e+08 =  9% of the original kernel matrix.

torch.Size([32890, 2])
We keep 4.74e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([103897, 2])
We keep 7.16e+07/4.44e+09 =  1% of the original kernel matrix.

torch.Size([69884, 2])
We keep 1.80e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([281604, 2])
We keep 2.18e+08/2.31e+10 =  0% of the original kernel matrix.

torch.Size([120565, 2])
We keep 3.67e+07/3.97e+09 =  0% of the original kernel matrix.

torch.Size([47391, 2])
We keep 1.29e+07/7.25e+08 =  1% of the original kernel matrix.

torch.Size([49150, 2])
We keep 8.49e+06/7.03e+08 =  1% of the original kernel matrix.

torch.Size([223030, 2])
We keep 2.35e+08/1.70e+10 =  1% of the original kernel matrix.

torch.Size([105886, 2])
We keep 3.22e+07/3.40e+09 =  0% of the original kernel matrix.

torch.Size([259478, 2])
We keep 1.67e+08/1.97e+10 =  0% of the original kernel matrix.

torch.Size([115246, 2])
We keep 3.40e+07/3.66e+09 =  0% of the original kernel matrix.

torch.Size([4919, 2])
We keep 1.37e+05/2.97e+06 =  4% of the original kernel matrix.

torch.Size([16855, 2])
We keep 1.08e+06/4.50e+07 =  2% of the original kernel matrix.

torch.Size([97927, 2])
We keep 5.45e+07/2.71e+09 =  2% of the original kernel matrix.

torch.Size([68452, 2])
We keep 1.45e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([32559, 2])
We keep 4.83e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([40723, 2])
We keep 5.67e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([211495, 2])
We keep 2.65e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([102937, 2])
We keep 2.99e+07/3.19e+09 =  0% of the original kernel matrix.

torch.Size([224881, 2])
We keep 2.50e+08/2.04e+10 =  1% of the original kernel matrix.

torch.Size([106478, 2])
We keep 3.45e+07/3.73e+09 =  0% of the original kernel matrix.

torch.Size([37696, 2])
We keep 2.09e+07/7.10e+08 =  2% of the original kernel matrix.

torch.Size([42071, 2])
We keep 8.61e+06/6.96e+08 =  1% of the original kernel matrix.

torch.Size([117185, 2])
We keep 5.98e+07/4.24e+09 =  1% of the original kernel matrix.

torch.Size([75071, 2])
We keep 1.75e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([32891, 2])
We keep 5.61e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([40352, 2])
We keep 6.01e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([23001, 2])
We keep 2.22e+07/6.04e+08 =  3% of the original kernel matrix.

torch.Size([30596, 2])
We keep 7.83e+06/6.42e+08 =  1% of the original kernel matrix.

torch.Size([10547, 2])
We keep 6.25e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([23049, 2])
We keep 2.12e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([18316, 2])
We keep 2.51e+06/9.10e+07 =  2% of the original kernel matrix.

torch.Size([29538, 2])
We keep 3.74e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([295844, 2])
We keep 5.81e+08/2.98e+10 =  1% of the original kernel matrix.

torch.Size([124062, 2])
We keep 4.10e+07/4.50e+09 =  0% of the original kernel matrix.

torch.Size([802947, 2])
We keep 5.01e+09/2.30e+11 =  2% of the original kernel matrix.

torch.Size([207029, 2])
We keep 1.01e+08/1.25e+10 =  0% of the original kernel matrix.

torch.Size([29452, 2])
We keep 8.36e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([37033, 2])
We keep 6.92e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([33791, 2])
We keep 6.70e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([41639, 2])
We keep 6.31e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([48722, 2])
We keep 1.30e+07/7.99e+08 =  1% of the original kernel matrix.

torch.Size([49544, 2])
We keep 8.93e+06/7.38e+08 =  1% of the original kernel matrix.

torch.Size([128642, 2])
We keep 7.15e+07/5.52e+09 =  1% of the original kernel matrix.

torch.Size([79072, 2])
We keep 1.92e+07/1.94e+09 =  0% of the original kernel matrix.

torch.Size([78277, 2])
We keep 2.21e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([61820, 2])
We keep 1.23e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([16969, 2])
We keep 1.48e+06/6.08e+07 =  2% of the original kernel matrix.

torch.Size([28879, 2])
We keep 3.17e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([83112, 2])
We keep 3.14e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([63828, 2])
We keep 1.28e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([265061, 2])
We keep 2.03e+08/2.10e+10 =  0% of the original kernel matrix.

torch.Size([116498, 2])
We keep 3.51e+07/3.78e+09 =  0% of the original kernel matrix.

torch.Size([16582, 2])
We keep 4.04e+06/8.36e+07 =  4% of the original kernel matrix.

torch.Size([28259, 2])
We keep 3.59e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([71441, 2])
We keep 4.19e+07/1.71e+09 =  2% of the original kernel matrix.

torch.Size([58961, 2])
We keep 1.21e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([20235, 2])
We keep 7.33e+06/9.78e+07 =  7% of the original kernel matrix.

torch.Size([31691, 2])
We keep 3.71e+06/2.58e+08 =  1% of the original kernel matrix.

time for making ranges is 6.730448007583618
Sorting X and nu_X
time for sorting X is 0.09367704391479492
Sorting Z and nu_Z
time for sorting Z is 0.00026607513427734375
Starting Optim
sum tnu_Z before tensor(40718664., device='cuda:0')
c= tensor(1383.1038, device='cuda:0')
c= tensor(194701.1406, device='cuda:0')
c= tensor(204671.9062, device='cuda:0')
c= tensor(373216.0625, device='cuda:0')
c= tensor(670150.6250, device='cuda:0')
c= tensor(1134224., device='cuda:0')
c= tensor(1797107.6250, device='cuda:0')
c= tensor(2417614.2500, device='cuda:0')
c= tensor(2489306.5000, device='cuda:0')
c= tensor(8023693.5000, device='cuda:0')
c= tensor(8052656.5000, device='cuda:0')
c= tensor(16125924., device='cuda:0')
c= tensor(16169754., device='cuda:0')
c= tensor(48166472., device='cuda:0')
c= tensor(48417224., device='cuda:0')
c= tensor(49076484., device='cuda:0')
c= tensor(51027624., device='cuda:0')
c= tensor(51681280., device='cuda:0')
c= tensor(59489644., device='cuda:0')
c= tensor(63334024., device='cuda:0')
c= tensor(64058032., device='cuda:0')
c= tensor(92951800., device='cuda:0')
c= tensor(92999616., device='cuda:0')
c= tensor(94487800., device='cuda:0')
c= tensor(94523840., device='cuda:0')
c= tensor(96049496., device='cuda:0')
c= tensor(97916528., device='cuda:0')
c= tensor(97955216., device='cuda:0')
c= tensor(1.0369e+08, device='cuda:0')
c= tensor(5.8468e+08, device='cuda:0')
c= tensor(5.8475e+08, device='cuda:0')
c= tensor(7.7790e+08, device='cuda:0')
c= tensor(7.7853e+08, device='cuda:0')
c= tensor(7.7857e+08, device='cuda:0')
c= tensor(7.7873e+08, device='cuda:0')
c= tensor(7.9358e+08, device='cuda:0')
c= tensor(7.9621e+08, device='cuda:0')
c= tensor(7.9621e+08, device='cuda:0')
c= tensor(7.9622e+08, device='cuda:0')
c= tensor(7.9623e+08, device='cuda:0')
c= tensor(7.9624e+08, device='cuda:0')
c= tensor(7.9624e+08, device='cuda:0')
c= tensor(7.9624e+08, device='cuda:0')
c= tensor(7.9626e+08, device='cuda:0')
c= tensor(7.9626e+08, device='cuda:0')
c= tensor(7.9626e+08, device='cuda:0')
c= tensor(7.9627e+08, device='cuda:0')
c= tensor(7.9628e+08, device='cuda:0')
c= tensor(7.9629e+08, device='cuda:0')
c= tensor(7.9636e+08, device='cuda:0')
c= tensor(7.9644e+08, device='cuda:0')
c= tensor(7.9644e+08, device='cuda:0')
c= tensor(7.9645e+08, device='cuda:0')
c= tensor(7.9646e+08, device='cuda:0')
c= tensor(7.9648e+08, device='cuda:0')
c= tensor(7.9651e+08, device='cuda:0')
c= tensor(7.9651e+08, device='cuda:0')
c= tensor(7.9653e+08, device='cuda:0')
c= tensor(7.9653e+08, device='cuda:0')
c= tensor(7.9654e+08, device='cuda:0')
c= tensor(7.9657e+08, device='cuda:0')
c= tensor(7.9657e+08, device='cuda:0')
c= tensor(7.9663e+08, device='cuda:0')
c= tensor(7.9665e+08, device='cuda:0')
c= tensor(7.9666e+08, device='cuda:0')
c= tensor(7.9666e+08, device='cuda:0')
c= tensor(7.9666e+08, device='cuda:0')
c= tensor(7.9667e+08, device='cuda:0')
c= tensor(7.9669e+08, device='cuda:0')
c= tensor(7.9669e+08, device='cuda:0')
c= tensor(7.9676e+08, device='cuda:0')
c= tensor(7.9677e+08, device='cuda:0')
c= tensor(7.9678e+08, device='cuda:0')
c= tensor(7.9678e+08, device='cuda:0')
c= tensor(7.9680e+08, device='cuda:0')
c= tensor(7.9684e+08, device='cuda:0')
c= tensor(7.9684e+08, device='cuda:0')
c= tensor(7.9685e+08, device='cuda:0')
c= tensor(7.9686e+08, device='cuda:0')
c= tensor(7.9698e+08, device='cuda:0')
c= tensor(7.9698e+08, device='cuda:0')
c= tensor(7.9699e+08, device='cuda:0')
c= tensor(7.9700e+08, device='cuda:0')
c= tensor(7.9700e+08, device='cuda:0')
c= tensor(7.9701e+08, device='cuda:0')
c= tensor(7.9701e+08, device='cuda:0')
c= tensor(7.9702e+08, device='cuda:0')
c= tensor(7.9702e+08, device='cuda:0')
c= tensor(7.9703e+08, device='cuda:0')
c= tensor(7.9704e+08, device='cuda:0')
c= tensor(7.9705e+08, device='cuda:0')
c= tensor(7.9705e+08, device='cuda:0')
c= tensor(7.9706e+08, device='cuda:0')
c= tensor(7.9707e+08, device='cuda:0')
c= tensor(7.9709e+08, device='cuda:0')
c= tensor(7.9710e+08, device='cuda:0')
c= tensor(7.9711e+08, device='cuda:0')
c= tensor(7.9715e+08, device='cuda:0')
c= tensor(7.9720e+08, device='cuda:0')
c= tensor(7.9721e+08, device='cuda:0')
c= tensor(7.9725e+08, device='cuda:0')
c= tensor(7.9727e+08, device='cuda:0')
c= tensor(7.9728e+08, device='cuda:0')
c= tensor(7.9728e+08, device='cuda:0')
c= tensor(7.9731e+08, device='cuda:0')
c= tensor(7.9731e+08, device='cuda:0')
c= tensor(7.9732e+08, device='cuda:0')
c= tensor(7.9732e+08, device='cuda:0')
c= tensor(7.9733e+08, device='cuda:0')
c= tensor(7.9733e+08, device='cuda:0')
c= tensor(7.9734e+08, device='cuda:0')
c= tensor(7.9734e+08, device='cuda:0')
c= tensor(7.9735e+08, device='cuda:0')
c= tensor(7.9736e+08, device='cuda:0')
c= tensor(7.9737e+08, device='cuda:0')
c= tensor(7.9737e+08, device='cuda:0')
c= tensor(7.9738e+08, device='cuda:0')
c= tensor(7.9738e+08, device='cuda:0')
c= tensor(7.9741e+08, device='cuda:0')
c= tensor(7.9741e+08, device='cuda:0')
c= tensor(7.9745e+08, device='cuda:0')
c= tensor(7.9745e+08, device='cuda:0')
c= tensor(7.9746e+08, device='cuda:0')
c= tensor(7.9746e+08, device='cuda:0')
c= tensor(7.9747e+08, device='cuda:0')
c= tensor(7.9747e+08, device='cuda:0')
c= tensor(7.9747e+08, device='cuda:0')
c= tensor(7.9749e+08, device='cuda:0')
c= tensor(7.9754e+08, device='cuda:0')
c= tensor(7.9754e+08, device='cuda:0')
c= tensor(7.9757e+08, device='cuda:0')
c= tensor(7.9757e+08, device='cuda:0')
c= tensor(7.9760e+08, device='cuda:0')
c= tensor(7.9760e+08, device='cuda:0')
c= tensor(7.9762e+08, device='cuda:0')
c= tensor(7.9762e+08, device='cuda:0')
c= tensor(7.9763e+08, device='cuda:0')
c= tensor(7.9763e+08, device='cuda:0')
c= tensor(7.9763e+08, device='cuda:0')
c= tensor(7.9763e+08, device='cuda:0')
c= tensor(7.9764e+08, device='cuda:0')
c= tensor(7.9764e+08, device='cuda:0')
c= tensor(7.9768e+08, device='cuda:0')
c= tensor(7.9774e+08, device='cuda:0')
c= tensor(7.9777e+08, device='cuda:0')
c= tensor(7.9777e+08, device='cuda:0')
c= tensor(7.9778e+08, device='cuda:0')
c= tensor(7.9778e+08, device='cuda:0')
c= tensor(7.9778e+08, device='cuda:0')
c= tensor(7.9778e+08, device='cuda:0')
c= tensor(7.9779e+08, device='cuda:0')
c= tensor(7.9781e+08, device='cuda:0')
c= tensor(7.9781e+08, device='cuda:0')
c= tensor(7.9787e+08, device='cuda:0')
c= tensor(7.9788e+08, device='cuda:0')
c= tensor(7.9814e+08, device='cuda:0')
c= tensor(7.9814e+08, device='cuda:0')
c= tensor(7.9815e+08, device='cuda:0')
c= tensor(7.9816e+08, device='cuda:0')
c= tensor(7.9816e+08, device='cuda:0')
c= tensor(7.9822e+08, device='cuda:0')
c= tensor(7.9823e+08, device='cuda:0')
c= tensor(7.9827e+08, device='cuda:0')
c= tensor(7.9827e+08, device='cuda:0')
c= tensor(7.9828e+08, device='cuda:0')
c= tensor(7.9828e+08, device='cuda:0')
c= tensor(7.9829e+08, device='cuda:0')
c= tensor(7.9829e+08, device='cuda:0')
c= tensor(7.9831e+08, device='cuda:0')
c= tensor(7.9831e+08, device='cuda:0')
c= tensor(7.9831e+08, device='cuda:0')
c= tensor(7.9831e+08, device='cuda:0')
c= tensor(7.9834e+08, device='cuda:0')
c= tensor(7.9834e+08, device='cuda:0')
c= tensor(7.9835e+08, device='cuda:0')
c= tensor(7.9972e+08, device='cuda:0')
c= tensor(7.9973e+08, device='cuda:0')
c= tensor(7.9973e+08, device='cuda:0')
c= tensor(7.9978e+08, device='cuda:0')
c= tensor(7.9978e+08, device='cuda:0')
c= tensor(7.9981e+08, device='cuda:0')
c= tensor(7.9981e+08, device='cuda:0')
c= tensor(7.9986e+08, device='cuda:0')
c= tensor(7.9986e+08, device='cuda:0')
c= tensor(7.9987e+08, device='cuda:0')
c= tensor(7.9989e+08, device='cuda:0')
c= tensor(7.9989e+08, device='cuda:0')
c= tensor(7.9992e+08, device='cuda:0')
c= tensor(7.9993e+08, device='cuda:0')
c= tensor(7.9999e+08, device='cuda:0')
c= tensor(8.0000e+08, device='cuda:0')
c= tensor(8.0000e+08, device='cuda:0')
c= tensor(8.0001e+08, device='cuda:0')
c= tensor(8.0001e+08, device='cuda:0')
c= tensor(8.0003e+08, device='cuda:0')
c= tensor(8.0003e+08, device='cuda:0')
c= tensor(8.0004e+08, device='cuda:0')
c= tensor(8.0004e+08, device='cuda:0')
c= tensor(8.0005e+08, device='cuda:0')
c= tensor(8.0006e+08, device='cuda:0')
c= tensor(8.0009e+08, device='cuda:0')
c= tensor(8.0009e+08, device='cuda:0')
c= tensor(8.0017e+08, device='cuda:0')
c= tensor(8.0018e+08, device='cuda:0')
c= tensor(8.0019e+08, device='cuda:0')
c= tensor(8.0019e+08, device='cuda:0')
c= tensor(8.0021e+08, device='cuda:0')
c= tensor(8.0022e+08, device='cuda:0')
c= tensor(8.0022e+08, device='cuda:0')
c= tensor(8.0030e+08, device='cuda:0')
c= tensor(8.0033e+08, device='cuda:0')
c= tensor(8.0033e+08, device='cuda:0')
c= tensor(8.0033e+08, device='cuda:0')
c= tensor(8.0034e+08, device='cuda:0')
c= tensor(8.0034e+08, device='cuda:0')
c= tensor(8.0034e+08, device='cuda:0')
c= tensor(8.0034e+08, device='cuda:0')
c= tensor(8.0035e+08, device='cuda:0')
c= tensor(8.0044e+08, device='cuda:0')
c= tensor(8.0045e+08, device='cuda:0')
c= tensor(8.0047e+08, device='cuda:0')
c= tensor(8.0047e+08, device='cuda:0')
c= tensor(8.0049e+08, device='cuda:0')
c= tensor(8.0049e+08, device='cuda:0')
c= tensor(8.0050e+08, device='cuda:0')
c= tensor(8.0051e+08, device='cuda:0')
c= tensor(8.0051e+08, device='cuda:0')
c= tensor(8.0051e+08, device='cuda:0')
c= tensor(8.0053e+08, device='cuda:0')
c= tensor(8.0053e+08, device='cuda:0')
c= tensor(8.0053e+08, device='cuda:0')
c= tensor(8.0053e+08, device='cuda:0')
c= tensor(8.0054e+08, device='cuda:0')
c= tensor(8.0055e+08, device='cuda:0')
c= tensor(8.0056e+08, device='cuda:0')
c= tensor(8.0056e+08, device='cuda:0')
c= tensor(8.0058e+08, device='cuda:0')
c= tensor(8.0060e+08, device='cuda:0')
c= tensor(8.0063e+08, device='cuda:0')
c= tensor(8.0090e+08, device='cuda:0')
c= tensor(8.0340e+08, device='cuda:0')
c= tensor(8.0342e+08, device='cuda:0')
c= tensor(8.0345e+08, device='cuda:0')
c= tensor(8.0345e+08, device='cuda:0')
c= tensor(8.0346e+08, device='cuda:0')
c= tensor(8.1531e+08, device='cuda:0')
c= tensor(8.2445e+08, device='cuda:0')
c= tensor(8.2445e+08, device='cuda:0')
c= tensor(8.3073e+08, device='cuda:0')
c= tensor(8.3151e+08, device='cuda:0')
c= tensor(8.3173e+08, device='cuda:0')
c= tensor(8.3765e+08, device='cuda:0')
c= tensor(8.3765e+08, device='cuda:0')
c= tensor(8.3768e+08, device='cuda:0')
c= tensor(8.4943e+08, device='cuda:0')
c= tensor(9.4683e+08, device='cuda:0')
c= tensor(9.4685e+08, device='cuda:0')
c= tensor(9.4731e+08, device='cuda:0')
c= tensor(9.4773e+08, device='cuda:0')
c= tensor(9.4919e+08, device='cuda:0')
c= tensor(9.5204e+08, device='cuda:0')
c= tensor(9.5514e+08, device='cuda:0')
c= tensor(9.5749e+08, device='cuda:0')
c= tensor(9.5766e+08, device='cuda:0')
c= tensor(9.5769e+08, device='cuda:0')
c= tensor(9.7850e+08, device='cuda:0')
c= tensor(9.7859e+08, device='cuda:0')
c= tensor(9.7860e+08, device='cuda:0')
c= tensor(9.7884e+08, device='cuda:0')
c= tensor(9.8094e+08, device='cuda:0')
c= tensor(1.0075e+09, device='cuda:0')
c= tensor(1.0098e+09, device='cuda:0')
c= tensor(1.0098e+09, device='cuda:0')
c= tensor(1.0102e+09, device='cuda:0')
c= tensor(1.0103e+09, device='cuda:0')
c= tensor(1.0110e+09, device='cuda:0')
c= tensor(1.0135e+09, device='cuda:0')
c= tensor(1.0144e+09, device='cuda:0')
c= tensor(1.0170e+09, device='cuda:0')
c= tensor(1.0170e+09, device='cuda:0')
c= tensor(1.0170e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0574e+09, device='cuda:0')
c= tensor(1.0575e+09, device='cuda:0')
c= tensor(1.0579e+09, device='cuda:0')
c= tensor(1.0622e+09, device='cuda:0')
c= tensor(1.0622e+09, device='cuda:0')
c= tensor(1.0669e+09, device='cuda:0')
c= tensor(1.0890e+09, device='cuda:0')
c= tensor(1.1750e+09, device='cuda:0')
c= tensor(1.1755e+09, device='cuda:0')
c= tensor(1.1759e+09, device='cuda:0')
c= tensor(1.1759e+09, device='cuda:0')
c= tensor(1.1759e+09, device='cuda:0')
c= tensor(1.1808e+09, device='cuda:0')
c= tensor(1.1808e+09, device='cuda:0')
c= tensor(1.1814e+09, device='cuda:0')
c= tensor(1.1866e+09, device='cuda:0')
c= tensor(1.1871e+09, device='cuda:0')
c= tensor(1.1875e+09, device='cuda:0')
c= tensor(1.1875e+09, device='cuda:0')
c= tensor(1.1963e+09, device='cuda:0')
c= tensor(1.2036e+09, device='cuda:0')
c= tensor(1.2052e+09, device='cuda:0')
c= tensor(1.2054e+09, device='cuda:0')
c= tensor(1.2207e+09, device='cuda:0')
c= tensor(1.2208e+09, device='cuda:0')
c= tensor(1.2392e+09, device='cuda:0')
c= tensor(1.2392e+09, device='cuda:0')
c= tensor(1.2442e+09, device='cuda:0')
c= tensor(1.2445e+09, device='cuda:0')
c= tensor(1.2549e+09, device='cuda:0')
c= tensor(1.2564e+09, device='cuda:0')
c= tensor(1.2564e+09, device='cuda:0')
c= tensor(1.2611e+09, device='cuda:0')
c= tensor(1.2669e+09, device='cuda:0')
c= tensor(1.2670e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2899e+09, device='cuda:0')
c= tensor(1.3198e+09, device='cuda:0')
c= tensor(1.3261e+09, device='cuda:0')
c= tensor(1.3261e+09, device='cuda:0')
c= tensor(1.3262e+09, device='cuda:0')
c= tensor(1.3281e+09, device='cuda:0')
c= tensor(1.3284e+09, device='cuda:0')
c= tensor(1.3300e+09, device='cuda:0')
c= tensor(1.3300e+09, device='cuda:0')
c= tensor(1.3317e+09, device='cuda:0')
c= tensor(1.3369e+09, device='cuda:0')
c= tensor(1.3369e+09, device='cuda:0')
c= tensor(1.3369e+09, device='cuda:0')
c= tensor(1.3378e+09, device='cuda:0')
c= tensor(1.3378e+09, device='cuda:0')
c= tensor(1.3380e+09, device='cuda:0')
c= tensor(1.3380e+09, device='cuda:0')
c= tensor(1.3380e+09, device='cuda:0')
c= tensor(1.3412e+09, device='cuda:0')
c= tensor(1.3416e+09, device='cuda:0')
c= tensor(1.3419e+09, device='cuda:0')
c= tensor(1.3428e+09, device='cuda:0')
c= tensor(1.3429e+09, device='cuda:0')
c= tensor(1.7485e+09, device='cuda:0')
c= tensor(1.7485e+09, device='cuda:0')
c= tensor(1.7533e+09, device='cuda:0')
c= tensor(1.7533e+09, device='cuda:0')
c= tensor(1.7533e+09, device='cuda:0')
c= tensor(1.7534e+09, device='cuda:0')
c= tensor(1.7535e+09, device='cuda:0')
c= tensor(1.7535e+09, device='cuda:0')
c= tensor(1.7570e+09, device='cuda:0')
c= tensor(1.7570e+09, device='cuda:0')
c= tensor(1.7571e+09, device='cuda:0')
c= tensor(1.7808e+09, device='cuda:0')
c= tensor(1.7816e+09, device='cuda:0')
c= tensor(1.7821e+09, device='cuda:0')
c= tensor(1.7857e+09, device='cuda:0')
c= tensor(1.8063e+09, device='cuda:0')
c= tensor(1.8063e+09, device='cuda:0')
c= tensor(1.8063e+09, device='cuda:0')
c= tensor(1.8066e+09, device='cuda:0')
c= tensor(1.8066e+09, device='cuda:0')
c= tensor(1.8066e+09, device='cuda:0')
c= tensor(1.8067e+09, device='cuda:0')
c= tensor(1.8067e+09, device='cuda:0')
c= tensor(1.8067e+09, device='cuda:0')
c= tensor(1.8068e+09, device='cuda:0')
c= tensor(1.8068e+09, device='cuda:0')
c= tensor(1.8416e+09, device='cuda:0')
c= tensor(1.8419e+09, device='cuda:0')
c= tensor(1.8442e+09, device='cuda:0')
c= tensor(1.8443e+09, device='cuda:0')
c= tensor(1.8443e+09, device='cuda:0')
c= tensor(1.8459e+09, device='cuda:0')
c= tensor(2.0197e+09, device='cuda:0')
c= tensor(2.0625e+09, device='cuda:0')
c= tensor(2.0625e+09, device='cuda:0')
c= tensor(2.0635e+09, device='cuda:0')
c= tensor(2.0635e+09, device='cuda:0')
c= tensor(2.0635e+09, device='cuda:0')
c= tensor(2.1971e+09, device='cuda:0')
c= tensor(2.1973e+09, device='cuda:0')
c= tensor(2.1974e+09, device='cuda:0')
c= tensor(2.1985e+09, device='cuda:0')
c= tensor(2.3501e+09, device='cuda:0')
c= tensor(2.3504e+09, device='cuda:0')
c= tensor(2.3505e+09, device='cuda:0')
c= tensor(2.3506e+09, device='cuda:0')
c= tensor(2.3509e+09, device='cuda:0')
c= tensor(2.3509e+09, device='cuda:0')
c= tensor(2.3630e+09, device='cuda:0')
c= tensor(2.3631e+09, device='cuda:0')
c= tensor(2.3631e+09, device='cuda:0')
c= tensor(2.3640e+09, device='cuda:0')
c= tensor(2.3642e+09, device='cuda:0')
c= tensor(2.3642e+09, device='cuda:0')
c= tensor(2.3660e+09, device='cuda:0')
c= tensor(2.3705e+09, device='cuda:0')
c= tensor(2.3940e+09, device='cuda:0')
c= tensor(2.4247e+09, device='cuda:0')
c= tensor(2.4354e+09, device='cuda:0')
c= tensor(2.4355e+09, device='cuda:0')
c= tensor(2.4360e+09, device='cuda:0')
c= tensor(2.4379e+09, device='cuda:0')
c= tensor(2.4470e+09, device='cuda:0')
c= tensor(2.4470e+09, device='cuda:0')
c= tensor(2.4855e+09, device='cuda:0')
c= tensor(2.5956e+09, device='cuda:0')
c= tensor(2.6085e+09, device='cuda:0')
c= tensor(2.6113e+09, device='cuda:0')
c= tensor(2.6144e+09, device='cuda:0')
c= tensor(2.6145e+09, device='cuda:0')
c= tensor(2.6145e+09, device='cuda:0')
c= tensor(2.6148e+09, device='cuda:0')
c= tensor(2.6184e+09, device='cuda:0')
c= tensor(2.6217e+09, device='cuda:0')
c= tensor(2.6750e+09, device='cuda:0')
c= tensor(2.6816e+09, device='cuda:0')
c= tensor(2.6847e+09, device='cuda:0')
c= tensor(2.6849e+09, device='cuda:0')
c= tensor(2.6923e+09, device='cuda:0')
c= tensor(2.6923e+09, device='cuda:0')
c= tensor(2.6924e+09, device='cuda:0')
c= tensor(2.7013e+09, device='cuda:0')
c= tensor(2.7019e+09, device='cuda:0')
c= tensor(2.7019e+09, device='cuda:0')
c= tensor(2.7021e+09, device='cuda:0')
c= tensor(2.7873e+09, device='cuda:0')
c= tensor(2.7879e+09, device='cuda:0')
c= tensor(2.7903e+09, device='cuda:0')
c= tensor(2.7903e+09, device='cuda:0')
c= tensor(2.7903e+09, device='cuda:0')
c= tensor(2.7903e+09, device='cuda:0')
c= tensor(2.7905e+09, device='cuda:0')
c= tensor(2.7907e+09, device='cuda:0')
c= tensor(2.7913e+09, device='cuda:0')
c= tensor(2.7913e+09, device='cuda:0')
c= tensor(2.8005e+09, device='cuda:0')
c= tensor(2.8005e+09, device='cuda:0')
c= tensor(2.8025e+09, device='cuda:0')
c= tensor(2.8026e+09, device='cuda:0')
c= tensor(2.8038e+09, device='cuda:0')
c= tensor(2.8039e+09, device='cuda:0')
c= tensor(2.8042e+09, device='cuda:0')
c= tensor(2.8044e+09, device='cuda:0')
c= tensor(2.8048e+09, device='cuda:0')
c= tensor(2.8071e+09, device='cuda:0')
c= tensor(2.9593e+09, device='cuda:0')
c= tensor(2.9593e+09, device='cuda:0')
c= tensor(2.9593e+09, device='cuda:0')
c= tensor(2.9638e+09, device='cuda:0')
c= tensor(2.9640e+09, device='cuda:0')
c= tensor(3.0035e+09, device='cuda:0')
c= tensor(3.0035e+09, device='cuda:0')
c= tensor(3.0074e+09, device='cuda:0')
c= tensor(3.0246e+09, device='cuda:0')
c= tensor(3.0246e+09, device='cuda:0')
c= tensor(3.0590e+09, device='cuda:0')
c= tensor(3.0596e+09, device='cuda:0')
c= tensor(3.1366e+09, device='cuda:0')
c= tensor(3.1366e+09, device='cuda:0')
c= tensor(3.1368e+09, device='cuda:0')
c= tensor(3.1369e+09, device='cuda:0')
c= tensor(3.1369e+09, device='cuda:0')
c= tensor(3.1369e+09, device='cuda:0')
c= tensor(3.1373e+09, device='cuda:0')
c= tensor(3.1374e+09, device='cuda:0')
c= tensor(3.1417e+09, device='cuda:0')
c= tensor(3.1417e+09, device='cuda:0')
c= tensor(3.1418e+09, device='cuda:0')
c= tensor(3.1419e+09, device='cuda:0')
c= tensor(3.1736e+09, device='cuda:0')
c= tensor(3.1838e+09, device='cuda:0')
c= tensor(3.2065e+09, device='cuda:0')
c= tensor(3.2066e+09, device='cuda:0')
c= tensor(3.2067e+09, device='cuda:0')
c= tensor(3.2067e+09, device='cuda:0')
c= tensor(3.2067e+09, device='cuda:0')
c= tensor(3.2377e+09, device='cuda:0')
c= tensor(3.2377e+09, device='cuda:0')
c= tensor(3.2379e+09, device='cuda:0')
c= tensor(3.2390e+09, device='cuda:0')
c= tensor(3.2402e+09, device='cuda:0')
c= tensor(3.2403e+09, device='cuda:0')
c= tensor(3.2403e+09, device='cuda:0')
c= tensor(3.2762e+09, device='cuda:0')
c= tensor(3.2768e+09, device='cuda:0')
c= tensor(3.2800e+09, device='cuda:0')
c= tensor(3.2803e+09, device='cuda:0')
c= tensor(3.3042e+09, device='cuda:0')
c= tensor(3.3173e+09, device='cuda:0')
c= tensor(3.3589e+09, device='cuda:0')
c= tensor(3.3637e+09, device='cuda:0')
c= tensor(3.3638e+09, device='cuda:0')
c= tensor(3.3646e+09, device='cuda:0')
c= tensor(3.3653e+09, device='cuda:0')
c= tensor(3.3653e+09, device='cuda:0')
c= tensor(3.3654e+09, device='cuda:0')
c= tensor(3.3655e+09, device='cuda:0')
c= tensor(3.3671e+09, device='cuda:0')
c= tensor(3.3673e+09, device='cuda:0')
c= tensor(3.3673e+09, device='cuda:0')
c= tensor(3.3674e+09, device='cuda:0')
c= tensor(3.3675e+09, device='cuda:0')
c= tensor(3.3690e+09, device='cuda:0')
c= tensor(3.3690e+09, device='cuda:0')
c= tensor(3.3693e+09, device='cuda:0')
c= tensor(3.3697e+09, device='cuda:0')
c= tensor(3.3699e+09, device='cuda:0')
c= tensor(3.3699e+09, device='cuda:0')
c= tensor(3.3700e+09, device='cuda:0')
c= tensor(3.3701e+09, device='cuda:0')
c= tensor(3.3761e+09, device='cuda:0')
c= tensor(3.3761e+09, device='cuda:0')
c= tensor(3.3761e+09, device='cuda:0')
c= tensor(3.3762e+09, device='cuda:0')
c= tensor(3.3866e+09, device='cuda:0')
c= tensor(3.4704e+09, device='cuda:0')
c= tensor(3.4711e+09, device='cuda:0')
c= tensor(3.4711e+09, device='cuda:0')
c= tensor(3.4788e+09, device='cuda:0')
c= tensor(3.4841e+09, device='cuda:0')
c= tensor(3.4841e+09, device='cuda:0')
c= tensor(3.4842e+09, device='cuda:0')
c= tensor(3.4845e+09, device='cuda:0')
c= tensor(3.4958e+09, device='cuda:0')
c= tensor(3.5202e+09, device='cuda:0')
c= tensor(3.5284e+09, device='cuda:0')
c= tensor(3.5285e+09, device='cuda:0')
c= tensor(3.5285e+09, device='cuda:0')
c= tensor(3.5285e+09, device='cuda:0')
c= tensor(3.5298e+09, device='cuda:0')
c= tensor(3.5298e+09, device='cuda:0')
c= tensor(3.5301e+09, device='cuda:0')
c= tensor(3.5331e+09, device='cuda:0')
c= tensor(3.5749e+09, device='cuda:0')
c= tensor(3.5750e+09, device='cuda:0')
c= tensor(3.5750e+09, device='cuda:0')
c= tensor(3.5750e+09, device='cuda:0')
c= tensor(3.5846e+09, device='cuda:0')
c= tensor(3.5853e+09, device='cuda:0')
c= tensor(3.5854e+09, device='cuda:0')
c= tensor(3.5854e+09, device='cuda:0')
c= tensor(3.5865e+09, device='cuda:0')
c= tensor(3.5865e+09, device='cuda:0')
c= tensor(3.5877e+09, device='cuda:0')
c= tensor(3.5877e+09, device='cuda:0')
c= tensor(3.5878e+09, device='cuda:0')
c= tensor(3.5879e+09, device='cuda:0')
c= tensor(3.5879e+09, device='cuda:0')
c= tensor(3.5880e+09, device='cuda:0')
c= tensor(3.5946e+09, device='cuda:0')
c= tensor(3.6230e+09, device='cuda:0')
c= tensor(3.6285e+09, device='cuda:0')
c= tensor(3.6479e+09, device='cuda:0')
c= tensor(3.6480e+09, device='cuda:0')
c= tensor(3.6482e+09, device='cuda:0')
c= tensor(3.6483e+09, device='cuda:0')
c= tensor(3.6517e+09, device='cuda:0')
c= tensor(3.6537e+09, device='cuda:0')
c= tensor(3.6562e+09, device='cuda:0')
c= tensor(3.6562e+09, device='cuda:0')
c= tensor(4.0360e+09, device='cuda:0')
c= tensor(4.0362e+09, device='cuda:0')
c= tensor(4.0374e+09, device='cuda:0')
c= tensor(4.0507e+09, device='cuda:0')
c= tensor(4.0515e+09, device='cuda:0')
c= tensor(4.0527e+09, device='cuda:0')
c= tensor(4.1613e+09, device='cuda:0')
c= tensor(4.1664e+09, device='cuda:0')
c= tensor(4.1689e+09, device='cuda:0')
c= tensor(4.1689e+09, device='cuda:0')
c= tensor(4.1695e+09, device='cuda:0')
c= tensor(4.1695e+09, device='cuda:0')
c= tensor(4.1875e+09, device='cuda:0')
c= tensor(4.4293e+09, device='cuda:0')
c= tensor(4.4374e+09, device='cuda:0')
c= tensor(4.4455e+09, device='cuda:0')
c= tensor(4.4494e+09, device='cuda:0')
c= tensor(4.4499e+09, device='cuda:0')
c= tensor(4.4504e+09, device='cuda:0')
c= tensor(4.4505e+09, device='cuda:0')
c= tensor(4.4609e+09, device='cuda:0')
c= tensor(4.4893e+09, device='cuda:0')
c= tensor(4.4914e+09, device='cuda:0')
c= tensor(4.7871e+09, device='cuda:0')
c= tensor(4.7985e+09, device='cuda:0')
c= tensor(4.7989e+09, device='cuda:0')
c= tensor(4.7990e+09, device='cuda:0')
c= tensor(4.8013e+09, device='cuda:0')
c= tensor(4.8050e+09, device='cuda:0')
c= tensor(4.8051e+09, device='cuda:0')
c= tensor(4.8494e+09, device='cuda:0')
c= tensor(4.8510e+09, device='cuda:0')
c= tensor(4.8515e+09, device='cuda:0')
c= tensor(4.8515e+09, device='cuda:0')
c= tensor(4.8515e+09, device='cuda:0')
c= tensor(4.8515e+09, device='cuda:0')
c= tensor(4.8516e+09, device='cuda:0')
c= tensor(4.8519e+09, device='cuda:0')
c= tensor(4.8538e+09, device='cuda:0')
c= tensor(7.8986e+09, device='cuda:0')
c= tensor(7.8993e+09, device='cuda:0')
c= tensor(7.9025e+09, device='cuda:0')
c= tensor(7.9026e+09, device='cuda:0')
c= tensor(7.9027e+09, device='cuda:0')
c= tensor(7.9028e+09, device='cuda:0')
c= tensor(7.9180e+09, device='cuda:0')
c= tensor(7.9201e+09, device='cuda:0')
c= tensor(8.0748e+09, device='cuda:0')
c= tensor(8.0748e+09, device='cuda:0')
c= tensor(8.0830e+09, device='cuda:0')
c= tensor(8.0848e+09, device='cuda:0')
c= tensor(8.0926e+09, device='cuda:0')
c= tensor(8.1324e+09, device='cuda:0')
c= tensor(8.1326e+09, device='cuda:0')
c= tensor(8.1327e+09, device='cuda:0')
c= tensor(8.1355e+09, device='cuda:0')
c= tensor(8.1356e+09, device='cuda:0')
c= tensor(8.1361e+09, device='cuda:0')
c= tensor(8.1465e+09, device='cuda:0')
c= tensor(8.1476e+09, device='cuda:0')
c= tensor(8.1507e+09, device='cuda:0')
c= tensor(8.1512e+09, device='cuda:0')
c= tensor(8.1584e+09, device='cuda:0')
c= tensor(8.1955e+09, device='cuda:0')
c= tensor(8.1958e+09, device='cuda:0')
c= tensor(8.1958e+09, device='cuda:0')
c= tensor(8.2016e+09, device='cuda:0')
c= tensor(8.2033e+09, device='cuda:0')
c= tensor(8.2659e+09, device='cuda:0')
c= tensor(8.2669e+09, device='cuda:0')
c= tensor(8.2676e+09, device='cuda:0')
c= tensor(8.2682e+09, device='cuda:0')
c= tensor(8.2891e+09, device='cuda:0')
c= tensor(8.2945e+09, device='cuda:0')
c= tensor(8.2947e+09, device='cuda:0')
c= tensor(8.2947e+09, device='cuda:0')
c= tensor(8.2948e+09, device='cuda:0')
c= tensor(8.2973e+09, device='cuda:0')
c= tensor(8.3016e+09, device='cuda:0')
c= tensor(8.3081e+09, device='cuda:0')
c= tensor(8.3082e+09, device='cuda:0')
c= tensor(8.3085e+09, device='cuda:0')
c= tensor(8.3090e+09, device='cuda:0')
c= tensor(8.3151e+09, device='cuda:0')
c= tensor(8.3153e+09, device='cuda:0')
c= tensor(8.3166e+09, device='cuda:0')
c= tensor(8.3171e+09, device='cuda:0')
c= tensor(8.3174e+09, device='cuda:0')
c= tensor(8.3174e+09, device='cuda:0')
c= tensor(8.3175e+09, device='cuda:0')
c= tensor(8.3234e+09, device='cuda:0')
c= tensor(8.3237e+09, device='cuda:0')
c= tensor(8.3250e+09, device='cuda:0')
c= tensor(8.3250e+09, device='cuda:0')
c= tensor(8.3250e+09, device='cuda:0')
c= tensor(8.3254e+09, device='cuda:0')
c= tensor(8.3261e+09, device='cuda:0')
c= tensor(8.3264e+09, device='cuda:0')
c= tensor(8.3264e+09, device='cuda:0')
c= tensor(8.3264e+09, device='cuda:0')
c= tensor(8.3267e+09, device='cuda:0')
c= tensor(8.3269e+09, device='cuda:0')
c= tensor(8.3330e+09, device='cuda:0')
c= tensor(8.3331e+09, device='cuda:0')
c= tensor(8.3333e+09, device='cuda:0')
c= tensor(8.3348e+09, device='cuda:0')
c= tensor(8.3349e+09, device='cuda:0')
c= tensor(8.3537e+09, device='cuda:0')
c= tensor(8.3542e+09, device='cuda:0')
c= tensor(8.3567e+09, device='cuda:0')
c= tensor(8.3619e+09, device='cuda:0')
c= tensor(8.3621e+09, device='cuda:0')
c= tensor(8.3672e+09, device='cuda:0')
c= tensor(8.3714e+09, device='cuda:0')
c= tensor(8.3714e+09, device='cuda:0')
c= tensor(8.3725e+09, device='cuda:0')
c= tensor(8.3726e+09, device='cuda:0')
c= tensor(8.3780e+09, device='cuda:0')
c= tensor(8.3853e+09, device='cuda:0')
c= tensor(8.3856e+09, device='cuda:0')
c= tensor(8.3872e+09, device='cuda:0')
c= tensor(8.3874e+09, device='cuda:0')
c= tensor(8.3897e+09, device='cuda:0')
c= tensor(8.3897e+09, device='cuda:0')
c= tensor(8.3898e+09, device='cuda:0')
c= tensor(8.4057e+09, device='cuda:0')
c= tensor(8.5734e+09, device='cuda:0')
c= tensor(8.5735e+09, device='cuda:0')
c= tensor(8.5737e+09, device='cuda:0')
c= tensor(8.5739e+09, device='cuda:0')
c= tensor(8.5763e+09, device='cuda:0')
c= tensor(8.5767e+09, device='cuda:0')
c= tensor(8.5767e+09, device='cuda:0')
c= tensor(8.5773e+09, device='cuda:0')
c= tensor(8.5821e+09, device='cuda:0')
c= tensor(8.5822e+09, device='cuda:0')
c= tensor(8.5830e+09, device='cuda:0')
c= tensor(8.5831e+09, device='cuda:0')
memory (bytes)
5419061248
time for making loss 2 is 12.186697721481323
p0 True
it  0 : 2904159744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 70% |
shape of L is 
torch.Size([])
memory (bytes)
5419491328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5419909120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  71742220000.0
relative error loss 8.358525
shape of L is 
torch.Size([])
memory (bytes)
5549375488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 15% |
memory (bytes)
5549375488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  71741970000.0
relative error loss 8.358497
shape of L is 
torch.Size([])
memory (bytes)
5553369088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
memory (bytes)
5553369088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  71740850000.0
relative error loss 8.358366
shape of L is 
torch.Size([])
memory (bytes)
5555204096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 15% |
memory (bytes)
5555433472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  71732630000.0
relative error loss 8.357409
shape of L is 
torch.Size([])
memory (bytes)
5557522432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5557587968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  71689450000.0
relative error loss 8.352378
shape of L is 
torch.Size([])
memory (bytes)
5559685120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5559685120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  71216270000.0
relative error loss 8.297248
shape of L is 
torch.Size([])
memory (bytes)
5561671680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5561802752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  68889310000.0
relative error loss 8.026139
shape of L is 
torch.Size([])
memory (bytes)
5563953152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5563969536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  57259590000.0
relative error loss 6.6711864
shape of L is 
torch.Size([])
memory (bytes)
5565952000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5565952000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 15% |
error is  19197972000.0
relative error loss 2.2367127
shape of L is 
torch.Size([])
memory (bytes)
5568139264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5568233472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  12351937000.0
relative error loss 1.4390965
time to take a step is 204.58355975151062
it  1 : 3417516032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5570420736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 15% |
memory (bytes)
5570420736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 15% |
error is  12351937000.0
relative error loss 1.4390965
shape of L is 
torch.Size([])
memory (bytes)
5572431872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5572431872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 15% |
error is  10043322000.0
relative error loss 1.170125
shape of L is 
torch.Size([])
memory (bytes)
5574483968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5574643712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  8686767000.0
relative error loss 1.0120758
shape of L is 
torch.Size([])
memory (bytes)
5576814592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5576814592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  8446454300.0
relative error loss 0.98407745
shape of L is 
torch.Size([])
memory (bytes)
5578727424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 15% |
memory (bytes)
5578952704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  8293650400.0
relative error loss 0.9662746
shape of L is 
torch.Size([])
memory (bytes)
5580836864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 15% |
memory (bytes)
5581086720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 15% |
error is  7705928000.0
relative error loss 0.89780045
shape of L is 
torch.Size([])
memory (bytes)
5583167488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5583196160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 15% |
error is  7831243000.0
relative error loss 0.91240054
shape of L is 
torch.Size([])
memory (bytes)
5585223680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 15% |
memory (bytes)
5585223680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  7505638000.0
relative error loss 0.87446505
shape of L is 
torch.Size([])
memory (bytes)
5587218432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
memory (bytes)
5587361792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  7286073000.0
relative error loss 0.848884
shape of L is 
torch.Size([])
memory (bytes)
5589524480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5589524480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  7037568500.0
relative error loss 0.8199313
time to take a step is 190.43829345703125
it  2 : 3564168192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5591437312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5591633920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  7037568500.0
relative error loss 0.8199313
shape of L is 
torch.Size([])
memory (bytes)
5593755648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5593759744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 15% |
error is  6942591000.0
relative error loss 0.8088657
shape of L is 
torch.Size([])
memory (bytes)
5595844608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5595873280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  6804309500.0
relative error loss 0.7927548
shape of L is 
torch.Size([])
memory (bytes)
5597925376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5597995008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  6347435000.0
relative error loss 0.73952544
shape of L is 
torch.Size([])
memory (bytes)
5600133120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5600133120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  5840777000.0
relative error loss 0.68049586
shape of L is 
torch.Size([])
memory (bytes)
5602107392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5602107392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  5350780000.0
relative error loss 0.62340736
shape of L is 
torch.Size([])
memory (bytes)
5604130816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5604356096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  4978135000.0
relative error loss 0.57999134
shape of L is 
torch.Size([])
memory (bytes)
5606375424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5606498304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 15% |
error is  4645462000.0
relative error loss 0.54123235
shape of L is 
torch.Size([])
memory (bytes)
5608632320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5608632320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  4396466000.0
relative error loss 0.5122224
shape of L is 
torch.Size([])
memory (bytes)
5610770432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5610770432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  4229068800.0
relative error loss 0.49271932
time to take a step is 194.3959584236145
it  3 : 3564168192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5612892160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5612892160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  4229068800.0
relative error loss 0.49271932
shape of L is 
torch.Size([])
memory (bytes)
5615001600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 15% |
memory (bytes)
5615030272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  4158973400.0
relative error loss 0.48455268
shape of L is 
torch.Size([])
memory (bytes)
5617152000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5617152000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  3837095400.0
relative error loss 0.44705138
shape of L is 
torch.Size([])
memory (bytes)
5619159040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 15% |
memory (bytes)
5619159040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  3573933600.0
relative error loss 0.416391
shape of L is 
torch.Size([])
memory (bytes)
5621243904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
memory (bytes)
5621395456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  3242539500.0
relative error loss 0.377781
shape of L is 
torch.Size([])
memory (bytes)
5623549952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
memory (bytes)
5623582720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  3089429500.0
relative error loss 0.3599425
shape of L is 
torch.Size([])
memory (bytes)
5625487360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 15% |
memory (bytes)
5625700352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2967941600.0
relative error loss 0.34578824
shape of L is 
torch.Size([])
memory (bytes)
5627568128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 15% |
memory (bytes)
5627834368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2743590400.0
relative error loss 0.31964958
shape of L is 
torch.Size([])
memory (bytes)
5629976576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 15% |
memory (bytes)
5629976576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2566388700.0
relative error loss 0.2990042
shape of L is 
torch.Size([])
memory (bytes)
5632036864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5632036864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2330060300.0
relative error loss 0.2714701
time to take a step is 202.40758752822876
c= tensor(1383.1038, device='cuda:0')
c= tensor(194701.1406, device='cuda:0')
c= tensor(204671.9062, device='cuda:0')
c= tensor(373216.0625, device='cuda:0')
c= tensor(670150.6250, device='cuda:0')
c= tensor(1134224., device='cuda:0')
c= tensor(1797107.6250, device='cuda:0')
c= tensor(2417614.2500, device='cuda:0')
c= tensor(2489306.5000, device='cuda:0')
c= tensor(8023693.5000, device='cuda:0')
c= tensor(8052656.5000, device='cuda:0')
c= tensor(16125924., device='cuda:0')
c= tensor(16169754., device='cuda:0')
c= tensor(48166472., device='cuda:0')
c= tensor(48417224., device='cuda:0')
c= tensor(49076484., device='cuda:0')
c= tensor(51027624., device='cuda:0')
c= tensor(51681280., device='cuda:0')
c= tensor(59489644., device='cuda:0')
c= tensor(63334024., device='cuda:0')
c= tensor(64058032., device='cuda:0')
c= tensor(92951800., device='cuda:0')
c= tensor(92999616., device='cuda:0')
c= tensor(94487800., device='cuda:0')
c= tensor(94523840., device='cuda:0')
c= tensor(96049496., device='cuda:0')
c= tensor(97916528., device='cuda:0')
c= tensor(97955216., device='cuda:0')
c= tensor(1.0369e+08, device='cuda:0')
c= tensor(5.8468e+08, device='cuda:0')
c= tensor(5.8475e+08, device='cuda:0')
c= tensor(7.7790e+08, device='cuda:0')
c= tensor(7.7853e+08, device='cuda:0')
c= tensor(7.7857e+08, device='cuda:0')
c= tensor(7.7873e+08, device='cuda:0')
c= tensor(7.9358e+08, device='cuda:0')
c= tensor(7.9621e+08, device='cuda:0')
c= tensor(7.9621e+08, device='cuda:0')
c= tensor(7.9622e+08, device='cuda:0')
c= tensor(7.9623e+08, device='cuda:0')
c= tensor(7.9624e+08, device='cuda:0')
c= tensor(7.9624e+08, device='cuda:0')
c= tensor(7.9624e+08, device='cuda:0')
c= tensor(7.9626e+08, device='cuda:0')
c= tensor(7.9626e+08, device='cuda:0')
c= tensor(7.9626e+08, device='cuda:0')
c= tensor(7.9627e+08, device='cuda:0')
c= tensor(7.9628e+08, device='cuda:0')
c= tensor(7.9629e+08, device='cuda:0')
c= tensor(7.9636e+08, device='cuda:0')
c= tensor(7.9644e+08, device='cuda:0')
c= tensor(7.9644e+08, device='cuda:0')
c= tensor(7.9645e+08, device='cuda:0')
c= tensor(7.9646e+08, device='cuda:0')
c= tensor(7.9648e+08, device='cuda:0')
c= tensor(7.9651e+08, device='cuda:0')
c= tensor(7.9651e+08, device='cuda:0')
c= tensor(7.9653e+08, device='cuda:0')
c= tensor(7.9653e+08, device='cuda:0')
c= tensor(7.9654e+08, device='cuda:0')
c= tensor(7.9657e+08, device='cuda:0')
c= tensor(7.9657e+08, device='cuda:0')
c= tensor(7.9663e+08, device='cuda:0')
c= tensor(7.9665e+08, device='cuda:0')
c= tensor(7.9666e+08, device='cuda:0')
c= tensor(7.9666e+08, device='cuda:0')
c= tensor(7.9666e+08, device='cuda:0')
c= tensor(7.9667e+08, device='cuda:0')
c= tensor(7.9669e+08, device='cuda:0')
c= tensor(7.9669e+08, device='cuda:0')
c= tensor(7.9676e+08, device='cuda:0')
c= tensor(7.9677e+08, device='cuda:0')
c= tensor(7.9678e+08, device='cuda:0')
c= tensor(7.9678e+08, device='cuda:0')
c= tensor(7.9680e+08, device='cuda:0')
c= tensor(7.9684e+08, device='cuda:0')
c= tensor(7.9684e+08, device='cuda:0')
c= tensor(7.9685e+08, device='cuda:0')
c= tensor(7.9686e+08, device='cuda:0')
c= tensor(7.9698e+08, device='cuda:0')
c= tensor(7.9698e+08, device='cuda:0')
c= tensor(7.9699e+08, device='cuda:0')
c= tensor(7.9700e+08, device='cuda:0')
c= tensor(7.9700e+08, device='cuda:0')
c= tensor(7.9701e+08, device='cuda:0')
c= tensor(7.9701e+08, device='cuda:0')
c= tensor(7.9702e+08, device='cuda:0')
c= tensor(7.9702e+08, device='cuda:0')
c= tensor(7.9703e+08, device='cuda:0')
c= tensor(7.9704e+08, device='cuda:0')
c= tensor(7.9705e+08, device='cuda:0')
c= tensor(7.9705e+08, device='cuda:0')
c= tensor(7.9706e+08, device='cuda:0')
c= tensor(7.9707e+08, device='cuda:0')
c= tensor(7.9709e+08, device='cuda:0')
c= tensor(7.9710e+08, device='cuda:0')
c= tensor(7.9711e+08, device='cuda:0')
c= tensor(7.9715e+08, device='cuda:0')
c= tensor(7.9720e+08, device='cuda:0')
c= tensor(7.9721e+08, device='cuda:0')
c= tensor(7.9725e+08, device='cuda:0')
c= tensor(7.9727e+08, device='cuda:0')
c= tensor(7.9728e+08, device='cuda:0')
c= tensor(7.9728e+08, device='cuda:0')
c= tensor(7.9731e+08, device='cuda:0')
c= tensor(7.9731e+08, device='cuda:0')
c= tensor(7.9732e+08, device='cuda:0')
c= tensor(7.9732e+08, device='cuda:0')
c= tensor(7.9733e+08, device='cuda:0')
c= tensor(7.9733e+08, device='cuda:0')
c= tensor(7.9734e+08, device='cuda:0')
c= tensor(7.9734e+08, device='cuda:0')
c= tensor(7.9735e+08, device='cuda:0')
c= tensor(7.9736e+08, device='cuda:0')
c= tensor(7.9737e+08, device='cuda:0')
c= tensor(7.9737e+08, device='cuda:0')
c= tensor(7.9738e+08, device='cuda:0')
c= tensor(7.9738e+08, device='cuda:0')
c= tensor(7.9741e+08, device='cuda:0')
c= tensor(7.9741e+08, device='cuda:0')
c= tensor(7.9745e+08, device='cuda:0')
c= tensor(7.9745e+08, device='cuda:0')
c= tensor(7.9746e+08, device='cuda:0')
c= tensor(7.9746e+08, device='cuda:0')
c= tensor(7.9747e+08, device='cuda:0')
c= tensor(7.9747e+08, device='cuda:0')
c= tensor(7.9747e+08, device='cuda:0')
c= tensor(7.9749e+08, device='cuda:0')
c= tensor(7.9754e+08, device='cuda:0')
c= tensor(7.9754e+08, device='cuda:0')
c= tensor(7.9757e+08, device='cuda:0')
c= tensor(7.9757e+08, device='cuda:0')
c= tensor(7.9760e+08, device='cuda:0')
c= tensor(7.9760e+08, device='cuda:0')
c= tensor(7.9762e+08, device='cuda:0')
c= tensor(7.9762e+08, device='cuda:0')
c= tensor(7.9763e+08, device='cuda:0')
c= tensor(7.9763e+08, device='cuda:0')
c= tensor(7.9763e+08, device='cuda:0')
c= tensor(7.9763e+08, device='cuda:0')
c= tensor(7.9764e+08, device='cuda:0')
c= tensor(7.9764e+08, device='cuda:0')
c= tensor(7.9768e+08, device='cuda:0')
c= tensor(7.9774e+08, device='cuda:0')
c= tensor(7.9777e+08, device='cuda:0')
c= tensor(7.9777e+08, device='cuda:0')
c= tensor(7.9778e+08, device='cuda:0')
c= tensor(7.9778e+08, device='cuda:0')
c= tensor(7.9778e+08, device='cuda:0')
c= tensor(7.9778e+08, device='cuda:0')
c= tensor(7.9779e+08, device='cuda:0')
c= tensor(7.9781e+08, device='cuda:0')
c= tensor(7.9781e+08, device='cuda:0')
c= tensor(7.9787e+08, device='cuda:0')
c= tensor(7.9788e+08, device='cuda:0')
c= tensor(7.9814e+08, device='cuda:0')
c= tensor(7.9814e+08, device='cuda:0')
c= tensor(7.9815e+08, device='cuda:0')
c= tensor(7.9816e+08, device='cuda:0')
c= tensor(7.9816e+08, device='cuda:0')
c= tensor(7.9822e+08, device='cuda:0')
c= tensor(7.9823e+08, device='cuda:0')
c= tensor(7.9827e+08, device='cuda:0')
c= tensor(7.9827e+08, device='cuda:0')
c= tensor(7.9828e+08, device='cuda:0')
c= tensor(7.9828e+08, device='cuda:0')
c= tensor(7.9829e+08, device='cuda:0')
c= tensor(7.9829e+08, device='cuda:0')
c= tensor(7.9831e+08, device='cuda:0')
c= tensor(7.9831e+08, device='cuda:0')
c= tensor(7.9831e+08, device='cuda:0')
c= tensor(7.9831e+08, device='cuda:0')
c= tensor(7.9834e+08, device='cuda:0')
c= tensor(7.9834e+08, device='cuda:0')
c= tensor(7.9835e+08, device='cuda:0')
c= tensor(7.9972e+08, device='cuda:0')
c= tensor(7.9973e+08, device='cuda:0')
c= tensor(7.9973e+08, device='cuda:0')
c= tensor(7.9978e+08, device='cuda:0')
c= tensor(7.9978e+08, device='cuda:0')
c= tensor(7.9981e+08, device='cuda:0')
c= tensor(7.9981e+08, device='cuda:0')
c= tensor(7.9986e+08, device='cuda:0')
c= tensor(7.9986e+08, device='cuda:0')
c= tensor(7.9987e+08, device='cuda:0')
c= tensor(7.9989e+08, device='cuda:0')
c= tensor(7.9989e+08, device='cuda:0')
c= tensor(7.9992e+08, device='cuda:0')
c= tensor(7.9993e+08, device='cuda:0')
c= tensor(7.9999e+08, device='cuda:0')
c= tensor(8.0000e+08, device='cuda:0')
c= tensor(8.0000e+08, device='cuda:0')
c= tensor(8.0001e+08, device='cuda:0')
c= tensor(8.0001e+08, device='cuda:0')
c= tensor(8.0003e+08, device='cuda:0')
c= tensor(8.0003e+08, device='cuda:0')
c= tensor(8.0004e+08, device='cuda:0')
c= tensor(8.0004e+08, device='cuda:0')
c= tensor(8.0005e+08, device='cuda:0')
c= tensor(8.0006e+08, device='cuda:0')
c= tensor(8.0009e+08, device='cuda:0')
c= tensor(8.0009e+08, device='cuda:0')
c= tensor(8.0017e+08, device='cuda:0')
c= tensor(8.0018e+08, device='cuda:0')
c= tensor(8.0019e+08, device='cuda:0')
c= tensor(8.0019e+08, device='cuda:0')
c= tensor(8.0021e+08, device='cuda:0')
c= tensor(8.0022e+08, device='cuda:0')
c= tensor(8.0022e+08, device='cuda:0')
c= tensor(8.0030e+08, device='cuda:0')
c= tensor(8.0033e+08, device='cuda:0')
c= tensor(8.0033e+08, device='cuda:0')
c= tensor(8.0033e+08, device='cuda:0')
c= tensor(8.0034e+08, device='cuda:0')
c= tensor(8.0034e+08, device='cuda:0')
c= tensor(8.0034e+08, device='cuda:0')
c= tensor(8.0034e+08, device='cuda:0')
c= tensor(8.0035e+08, device='cuda:0')
c= tensor(8.0044e+08, device='cuda:0')
c= tensor(8.0045e+08, device='cuda:0')
c= tensor(8.0047e+08, device='cuda:0')
c= tensor(8.0047e+08, device='cuda:0')
c= tensor(8.0049e+08, device='cuda:0')
c= tensor(8.0049e+08, device='cuda:0')
c= tensor(8.0050e+08, device='cuda:0')
c= tensor(8.0051e+08, device='cuda:0')
c= tensor(8.0051e+08, device='cuda:0')
c= tensor(8.0051e+08, device='cuda:0')
c= tensor(8.0053e+08, device='cuda:0')
c= tensor(8.0053e+08, device='cuda:0')
c= tensor(8.0053e+08, device='cuda:0')
c= tensor(8.0053e+08, device='cuda:0')
c= tensor(8.0054e+08, device='cuda:0')
c= tensor(8.0055e+08, device='cuda:0')
c= tensor(8.0056e+08, device='cuda:0')
c= tensor(8.0056e+08, device='cuda:0')
c= tensor(8.0058e+08, device='cuda:0')
c= tensor(8.0060e+08, device='cuda:0')
c= tensor(8.0063e+08, device='cuda:0')
c= tensor(8.0090e+08, device='cuda:0')
c= tensor(8.0340e+08, device='cuda:0')
c= tensor(8.0342e+08, device='cuda:0')
c= tensor(8.0345e+08, device='cuda:0')
c= tensor(8.0345e+08, device='cuda:0')
c= tensor(8.0346e+08, device='cuda:0')
c= tensor(8.1531e+08, device='cuda:0')
c= tensor(8.2445e+08, device='cuda:0')
c= tensor(8.2445e+08, device='cuda:0')
c= tensor(8.3073e+08, device='cuda:0')
c= tensor(8.3151e+08, device='cuda:0')
c= tensor(8.3173e+08, device='cuda:0')
c= tensor(8.3765e+08, device='cuda:0')
c= tensor(8.3765e+08, device='cuda:0')
c= tensor(8.3768e+08, device='cuda:0')
c= tensor(8.4943e+08, device='cuda:0')
c= tensor(9.4683e+08, device='cuda:0')
c= tensor(9.4685e+08, device='cuda:0')
c= tensor(9.4731e+08, device='cuda:0')
c= tensor(9.4773e+08, device='cuda:0')
c= tensor(9.4919e+08, device='cuda:0')
c= tensor(9.5204e+08, device='cuda:0')
c= tensor(9.5514e+08, device='cuda:0')
c= tensor(9.5749e+08, device='cuda:0')
c= tensor(9.5766e+08, device='cuda:0')
c= tensor(9.5769e+08, device='cuda:0')
c= tensor(9.7850e+08, device='cuda:0')
c= tensor(9.7859e+08, device='cuda:0')
c= tensor(9.7860e+08, device='cuda:0')
c= tensor(9.7884e+08, device='cuda:0')
c= tensor(9.8094e+08, device='cuda:0')
c= tensor(1.0075e+09, device='cuda:0')
c= tensor(1.0098e+09, device='cuda:0')
c= tensor(1.0098e+09, device='cuda:0')
c= tensor(1.0102e+09, device='cuda:0')
c= tensor(1.0103e+09, device='cuda:0')
c= tensor(1.0110e+09, device='cuda:0')
c= tensor(1.0135e+09, device='cuda:0')
c= tensor(1.0144e+09, device='cuda:0')
c= tensor(1.0170e+09, device='cuda:0')
c= tensor(1.0170e+09, device='cuda:0')
c= tensor(1.0170e+09, device='cuda:0')
c= tensor(1.0209e+09, device='cuda:0')
c= tensor(1.0222e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0574e+09, device='cuda:0')
c= tensor(1.0575e+09, device='cuda:0')
c= tensor(1.0579e+09, device='cuda:0')
c= tensor(1.0622e+09, device='cuda:0')
c= tensor(1.0622e+09, device='cuda:0')
c= tensor(1.0669e+09, device='cuda:0')
c= tensor(1.0890e+09, device='cuda:0')
c= tensor(1.1750e+09, device='cuda:0')
c= tensor(1.1755e+09, device='cuda:0')
c= tensor(1.1759e+09, device='cuda:0')
c= tensor(1.1759e+09, device='cuda:0')
c= tensor(1.1759e+09, device='cuda:0')
c= tensor(1.1808e+09, device='cuda:0')
c= tensor(1.1808e+09, device='cuda:0')
c= tensor(1.1814e+09, device='cuda:0')
c= tensor(1.1866e+09, device='cuda:0')
c= tensor(1.1871e+09, device='cuda:0')
c= tensor(1.1875e+09, device='cuda:0')
c= tensor(1.1875e+09, device='cuda:0')
c= tensor(1.1963e+09, device='cuda:0')
c= tensor(1.2036e+09, device='cuda:0')
c= tensor(1.2052e+09, device='cuda:0')
c= tensor(1.2054e+09, device='cuda:0')
c= tensor(1.2207e+09, device='cuda:0')
c= tensor(1.2208e+09, device='cuda:0')
c= tensor(1.2392e+09, device='cuda:0')
c= tensor(1.2392e+09, device='cuda:0')
c= tensor(1.2442e+09, device='cuda:0')
c= tensor(1.2445e+09, device='cuda:0')
c= tensor(1.2549e+09, device='cuda:0')
c= tensor(1.2564e+09, device='cuda:0')
c= tensor(1.2564e+09, device='cuda:0')
c= tensor(1.2611e+09, device='cuda:0')
c= tensor(1.2669e+09, device='cuda:0')
c= tensor(1.2670e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2899e+09, device='cuda:0')
c= tensor(1.3198e+09, device='cuda:0')
c= tensor(1.3261e+09, device='cuda:0')
c= tensor(1.3261e+09, device='cuda:0')
c= tensor(1.3262e+09, device='cuda:0')
c= tensor(1.3281e+09, device='cuda:0')
c= tensor(1.3284e+09, device='cuda:0')
c= tensor(1.3300e+09, device='cuda:0')
c= tensor(1.3300e+09, device='cuda:0')
c= tensor(1.3317e+09, device='cuda:0')
c= tensor(1.3369e+09, device='cuda:0')
c= tensor(1.3369e+09, device='cuda:0')
c= tensor(1.3369e+09, device='cuda:0')
c= tensor(1.3378e+09, device='cuda:0')
c= tensor(1.3378e+09, device='cuda:0')
c= tensor(1.3380e+09, device='cuda:0')
c= tensor(1.3380e+09, device='cuda:0')
c= tensor(1.3380e+09, device='cuda:0')
c= tensor(1.3412e+09, device='cuda:0')
c= tensor(1.3416e+09, device='cuda:0')
c= tensor(1.3419e+09, device='cuda:0')
c= tensor(1.3428e+09, device='cuda:0')
c= tensor(1.3429e+09, device='cuda:0')
c= tensor(1.7485e+09, device='cuda:0')
c= tensor(1.7485e+09, device='cuda:0')
c= tensor(1.7533e+09, device='cuda:0')
c= tensor(1.7533e+09, device='cuda:0')
c= tensor(1.7533e+09, device='cuda:0')
c= tensor(1.7534e+09, device='cuda:0')
c= tensor(1.7535e+09, device='cuda:0')
c= tensor(1.7535e+09, device='cuda:0')
c= tensor(1.7570e+09, device='cuda:0')
c= tensor(1.7570e+09, device='cuda:0')
c= tensor(1.7571e+09, device='cuda:0')
c= tensor(1.7808e+09, device='cuda:0')
c= tensor(1.7816e+09, device='cuda:0')
c= tensor(1.7821e+09, device='cuda:0')
c= tensor(1.7857e+09, device='cuda:0')
c= tensor(1.8063e+09, device='cuda:0')
c= tensor(1.8063e+09, device='cuda:0')
c= tensor(1.8063e+09, device='cuda:0')
c= tensor(1.8066e+09, device='cuda:0')
c= tensor(1.8066e+09, device='cuda:0')
c= tensor(1.8066e+09, device='cuda:0')
c= tensor(1.8067e+09, device='cuda:0')
c= tensor(1.8067e+09, device='cuda:0')
c= tensor(1.8067e+09, device='cuda:0')
c= tensor(1.8068e+09, device='cuda:0')
c= tensor(1.8068e+09, device='cuda:0')
c= tensor(1.8416e+09, device='cuda:0')
c= tensor(1.8419e+09, device='cuda:0')
c= tensor(1.8442e+09, device='cuda:0')
c= tensor(1.8443e+09, device='cuda:0')
c= tensor(1.8443e+09, device='cuda:0')
c= tensor(1.8459e+09, device='cuda:0')
c= tensor(2.0197e+09, device='cuda:0')
c= tensor(2.0625e+09, device='cuda:0')
c= tensor(2.0625e+09, device='cuda:0')
c= tensor(2.0635e+09, device='cuda:0')
c= tensor(2.0635e+09, device='cuda:0')
c= tensor(2.0635e+09, device='cuda:0')
c= tensor(2.1971e+09, device='cuda:0')
c= tensor(2.1973e+09, device='cuda:0')
c= tensor(2.1974e+09, device='cuda:0')
c= tensor(2.1985e+09, device='cuda:0')
c= tensor(2.3501e+09, device='cuda:0')
c= tensor(2.3504e+09, device='cuda:0')
c= tensor(2.3505e+09, device='cuda:0')
c= tensor(2.3506e+09, device='cuda:0')
c= tensor(2.3509e+09, device='cuda:0')
c= tensor(2.3509e+09, device='cuda:0')
c= tensor(2.3630e+09, device='cuda:0')
c= tensor(2.3631e+09, device='cuda:0')
c= tensor(2.3631e+09, device='cuda:0')
c= tensor(2.3640e+09, device='cuda:0')
c= tensor(2.3642e+09, device='cuda:0')
c= tensor(2.3642e+09, device='cuda:0')
c= tensor(2.3660e+09, device='cuda:0')
c= tensor(2.3705e+09, device='cuda:0')
c= tensor(2.3940e+09, device='cuda:0')
c= tensor(2.4247e+09, device='cuda:0')
c= tensor(2.4354e+09, device='cuda:0')
c= tensor(2.4355e+09, device='cuda:0')
c= tensor(2.4360e+09, device='cuda:0')
c= tensor(2.4379e+09, device='cuda:0')
c= tensor(2.4470e+09, device='cuda:0')
c= tensor(2.4470e+09, device='cuda:0')
c= tensor(2.4855e+09, device='cuda:0')
c= tensor(2.5956e+09, device='cuda:0')
c= tensor(2.6085e+09, device='cuda:0')
c= tensor(2.6113e+09, device='cuda:0')
c= tensor(2.6144e+09, device='cuda:0')
c= tensor(2.6145e+09, device='cuda:0')
c= tensor(2.6145e+09, device='cuda:0')
c= tensor(2.6148e+09, device='cuda:0')
c= tensor(2.6184e+09, device='cuda:0')
c= tensor(2.6217e+09, device='cuda:0')
c= tensor(2.6750e+09, device='cuda:0')
c= tensor(2.6816e+09, device='cuda:0')
c= tensor(2.6847e+09, device='cuda:0')
c= tensor(2.6849e+09, device='cuda:0')
c= tensor(2.6923e+09, device='cuda:0')
c= tensor(2.6923e+09, device='cuda:0')
c= tensor(2.6924e+09, device='cuda:0')
c= tensor(2.7013e+09, device='cuda:0')
c= tensor(2.7019e+09, device='cuda:0')
c= tensor(2.7019e+09, device='cuda:0')
c= tensor(2.7021e+09, device='cuda:0')
c= tensor(2.7873e+09, device='cuda:0')
c= tensor(2.7879e+09, device='cuda:0')
c= tensor(2.7903e+09, device='cuda:0')
c= tensor(2.7903e+09, device='cuda:0')
c= tensor(2.7903e+09, device='cuda:0')
c= tensor(2.7903e+09, device='cuda:0')
c= tensor(2.7905e+09, device='cuda:0')
c= tensor(2.7907e+09, device='cuda:0')
c= tensor(2.7913e+09, device='cuda:0')
c= tensor(2.7913e+09, device='cuda:0')
c= tensor(2.8005e+09, device='cuda:0')
c= tensor(2.8005e+09, device='cuda:0')
c= tensor(2.8025e+09, device='cuda:0')
c= tensor(2.8026e+09, device='cuda:0')
c= tensor(2.8038e+09, device='cuda:0')
c= tensor(2.8039e+09, device='cuda:0')
c= tensor(2.8042e+09, device='cuda:0')
c= tensor(2.8044e+09, device='cuda:0')
c= tensor(2.8048e+09, device='cuda:0')
c= tensor(2.8071e+09, device='cuda:0')
c= tensor(2.9593e+09, device='cuda:0')
c= tensor(2.9593e+09, device='cuda:0')
c= tensor(2.9593e+09, device='cuda:0')
c= tensor(2.9638e+09, device='cuda:0')
c= tensor(2.9640e+09, device='cuda:0')
c= tensor(3.0035e+09, device='cuda:0')
c= tensor(3.0035e+09, device='cuda:0')
c= tensor(3.0074e+09, device='cuda:0')
c= tensor(3.0246e+09, device='cuda:0')
c= tensor(3.0246e+09, device='cuda:0')
c= tensor(3.0590e+09, device='cuda:0')
c= tensor(3.0596e+09, device='cuda:0')
c= tensor(3.1366e+09, device='cuda:0')
c= tensor(3.1366e+09, device='cuda:0')
c= tensor(3.1368e+09, device='cuda:0')
c= tensor(3.1369e+09, device='cuda:0')
c= tensor(3.1369e+09, device='cuda:0')
c= tensor(3.1369e+09, device='cuda:0')
c= tensor(3.1373e+09, device='cuda:0')
c= tensor(3.1374e+09, device='cuda:0')
c= tensor(3.1417e+09, device='cuda:0')
c= tensor(3.1417e+09, device='cuda:0')
c= tensor(3.1418e+09, device='cuda:0')
c= tensor(3.1419e+09, device='cuda:0')
c= tensor(3.1736e+09, device='cuda:0')
c= tensor(3.1838e+09, device='cuda:0')
c= tensor(3.2065e+09, device='cuda:0')
c= tensor(3.2066e+09, device='cuda:0')
c= tensor(3.2067e+09, device='cuda:0')
c= tensor(3.2067e+09, device='cuda:0')
c= tensor(3.2067e+09, device='cuda:0')
c= tensor(3.2377e+09, device='cuda:0')
c= tensor(3.2377e+09, device='cuda:0')
c= tensor(3.2379e+09, device='cuda:0')
c= tensor(3.2390e+09, device='cuda:0')
c= tensor(3.2402e+09, device='cuda:0')
c= tensor(3.2403e+09, device='cuda:0')
c= tensor(3.2403e+09, device='cuda:0')
c= tensor(3.2762e+09, device='cuda:0')
c= tensor(3.2768e+09, device='cuda:0')
c= tensor(3.2800e+09, device='cuda:0')
c= tensor(3.2803e+09, device='cuda:0')
c= tensor(3.3042e+09, device='cuda:0')
c= tensor(3.3173e+09, device='cuda:0')
c= tensor(3.3589e+09, device='cuda:0')
c= tensor(3.3637e+09, device='cuda:0')
c= tensor(3.3638e+09, device='cuda:0')
c= tensor(3.3646e+09, device='cuda:0')
c= tensor(3.3653e+09, device='cuda:0')
c= tensor(3.3653e+09, device='cuda:0')
c= tensor(3.3654e+09, device='cuda:0')
c= tensor(3.3655e+09, device='cuda:0')
c= tensor(3.3671e+09, device='cuda:0')
c= tensor(3.3673e+09, device='cuda:0')
c= tensor(3.3673e+09, device='cuda:0')
c= tensor(3.3674e+09, device='cuda:0')
c= tensor(3.3675e+09, device='cuda:0')
c= tensor(3.3690e+09, device='cuda:0')
c= tensor(3.3690e+09, device='cuda:0')
c= tensor(3.3693e+09, device='cuda:0')
c= tensor(3.3697e+09, device='cuda:0')
c= tensor(3.3699e+09, device='cuda:0')
c= tensor(3.3699e+09, device='cuda:0')
c= tensor(3.3700e+09, device='cuda:0')
c= tensor(3.3701e+09, device='cuda:0')
c= tensor(3.3761e+09, device='cuda:0')
c= tensor(3.3761e+09, device='cuda:0')
c= tensor(3.3761e+09, device='cuda:0')
c= tensor(3.3762e+09, device='cuda:0')
c= tensor(3.3866e+09, device='cuda:0')
c= tensor(3.4704e+09, device='cuda:0')
c= tensor(3.4711e+09, device='cuda:0')
c= tensor(3.4711e+09, device='cuda:0')
c= tensor(3.4788e+09, device='cuda:0')
c= tensor(3.4841e+09, device='cuda:0')
c= tensor(3.4841e+09, device='cuda:0')
c= tensor(3.4842e+09, device='cuda:0')
c= tensor(3.4845e+09, device='cuda:0')
c= tensor(3.4958e+09, device='cuda:0')
c= tensor(3.5202e+09, device='cuda:0')
c= tensor(3.5284e+09, device='cuda:0')
c= tensor(3.5285e+09, device='cuda:0')
c= tensor(3.5285e+09, device='cuda:0')
c= tensor(3.5285e+09, device='cuda:0')
c= tensor(3.5298e+09, device='cuda:0')
c= tensor(3.5298e+09, device='cuda:0')
c= tensor(3.5301e+09, device='cuda:0')
c= tensor(3.5331e+09, device='cuda:0')
c= tensor(3.5749e+09, device='cuda:0')
c= tensor(3.5750e+09, device='cuda:0')
c= tensor(3.5750e+09, device='cuda:0')
c= tensor(3.5750e+09, device='cuda:0')
c= tensor(3.5846e+09, device='cuda:0')
c= tensor(3.5853e+09, device='cuda:0')
c= tensor(3.5854e+09, device='cuda:0')
c= tensor(3.5854e+09, device='cuda:0')
c= tensor(3.5865e+09, device='cuda:0')
c= tensor(3.5865e+09, device='cuda:0')
c= tensor(3.5877e+09, device='cuda:0')
c= tensor(3.5877e+09, device='cuda:0')
c= tensor(3.5878e+09, device='cuda:0')
c= tensor(3.5879e+09, device='cuda:0')
c= tensor(3.5879e+09, device='cuda:0')
c= tensor(3.5880e+09, device='cuda:0')
c= tensor(3.5946e+09, device='cuda:0')
c= tensor(3.6230e+09, device='cuda:0')
c= tensor(3.6285e+09, device='cuda:0')
c= tensor(3.6479e+09, device='cuda:0')
c= tensor(3.6480e+09, device='cuda:0')
c= tensor(3.6482e+09, device='cuda:0')
c= tensor(3.6483e+09, device='cuda:0')
c= tensor(3.6517e+09, device='cuda:0')
c= tensor(3.6537e+09, device='cuda:0')
c= tensor(3.6562e+09, device='cuda:0')
c= tensor(3.6562e+09, device='cuda:0')
c= tensor(4.0360e+09, device='cuda:0')
c= tensor(4.0362e+09, device='cuda:0')
c= tensor(4.0374e+09, device='cuda:0')
c= tensor(4.0507e+09, device='cuda:0')
c= tensor(4.0515e+09, device='cuda:0')
c= tensor(4.0527e+09, device='cuda:0')
c= tensor(4.1613e+09, device='cuda:0')
c= tensor(4.1664e+09, device='cuda:0')
c= tensor(4.1689e+09, device='cuda:0')
c= tensor(4.1689e+09, device='cuda:0')
c= tensor(4.1695e+09, device='cuda:0')
c= tensor(4.1695e+09, device='cuda:0')
c= tensor(4.1875e+09, device='cuda:0')
c= tensor(4.4293e+09, device='cuda:0')
c= tensor(4.4374e+09, device='cuda:0')
c= tensor(4.4455e+09, device='cuda:0')
c= tensor(4.4494e+09, device='cuda:0')
c= tensor(4.4499e+09, device='cuda:0')
c= tensor(4.4504e+09, device='cuda:0')
c= tensor(4.4505e+09, device='cuda:0')
c= tensor(4.4609e+09, device='cuda:0')
c= tensor(4.4893e+09, device='cuda:0')
c= tensor(4.4914e+09, device='cuda:0')
c= tensor(4.7871e+09, device='cuda:0')
c= tensor(4.7985e+09, device='cuda:0')
c= tensor(4.7989e+09, device='cuda:0')
c= tensor(4.7990e+09, device='cuda:0')
c= tensor(4.8013e+09, device='cuda:0')
c= tensor(4.8050e+09, device='cuda:0')
c= tensor(4.8051e+09, device='cuda:0')
c= tensor(4.8494e+09, device='cuda:0')
c= tensor(4.8510e+09, device='cuda:0')
c= tensor(4.8515e+09, device='cuda:0')
c= tensor(4.8515e+09, device='cuda:0')
c= tensor(4.8515e+09, device='cuda:0')
c= tensor(4.8515e+09, device='cuda:0')
c= tensor(4.8516e+09, device='cuda:0')
c= tensor(4.8519e+09, device='cuda:0')
c= tensor(4.8538e+09, device='cuda:0')
c= tensor(7.8986e+09, device='cuda:0')
c= tensor(7.8993e+09, device='cuda:0')
c= tensor(7.9025e+09, device='cuda:0')
c= tensor(7.9026e+09, device='cuda:0')
c= tensor(7.9027e+09, device='cuda:0')
c= tensor(7.9028e+09, device='cuda:0')
c= tensor(7.9180e+09, device='cuda:0')
c= tensor(7.9201e+09, device='cuda:0')
c= tensor(8.0748e+09, device='cuda:0')
c= tensor(8.0748e+09, device='cuda:0')
c= tensor(8.0830e+09, device='cuda:0')
c= tensor(8.0848e+09, device='cuda:0')
c= tensor(8.0926e+09, device='cuda:0')
c= tensor(8.1324e+09, device='cuda:0')
c= tensor(8.1326e+09, device='cuda:0')
c= tensor(8.1327e+09, device='cuda:0')
c= tensor(8.1355e+09, device='cuda:0')
c= tensor(8.1356e+09, device='cuda:0')
c= tensor(8.1361e+09, device='cuda:0')
c= tensor(8.1465e+09, device='cuda:0')
c= tensor(8.1476e+09, device='cuda:0')
c= tensor(8.1507e+09, device='cuda:0')
c= tensor(8.1512e+09, device='cuda:0')
c= tensor(8.1584e+09, device='cuda:0')
c= tensor(8.1955e+09, device='cuda:0')
c= tensor(8.1958e+09, device='cuda:0')
c= tensor(8.1958e+09, device='cuda:0')
c= tensor(8.2016e+09, device='cuda:0')
c= tensor(8.2033e+09, device='cuda:0')
c= tensor(8.2659e+09, device='cuda:0')
c= tensor(8.2669e+09, device='cuda:0')
c= tensor(8.2676e+09, device='cuda:0')
c= tensor(8.2682e+09, device='cuda:0')
c= tensor(8.2891e+09, device='cuda:0')
c= tensor(8.2945e+09, device='cuda:0')
c= tensor(8.2947e+09, device='cuda:0')
c= tensor(8.2947e+09, device='cuda:0')
c= tensor(8.2948e+09, device='cuda:0')
c= tensor(8.2973e+09, device='cuda:0')
c= tensor(8.3016e+09, device='cuda:0')
c= tensor(8.3081e+09, device='cuda:0')
c= tensor(8.3082e+09, device='cuda:0')
c= tensor(8.3085e+09, device='cuda:0')
c= tensor(8.3090e+09, device='cuda:0')
c= tensor(8.3151e+09, device='cuda:0')
c= tensor(8.3153e+09, device='cuda:0')
c= tensor(8.3166e+09, device='cuda:0')
c= tensor(8.3171e+09, device='cuda:0')
c= tensor(8.3174e+09, device='cuda:0')
c= tensor(8.3174e+09, device='cuda:0')
c= tensor(8.3175e+09, device='cuda:0')
c= tensor(8.3234e+09, device='cuda:0')
c= tensor(8.3237e+09, device='cuda:0')
c= tensor(8.3250e+09, device='cuda:0')
c= tensor(8.3250e+09, device='cuda:0')
c= tensor(8.3250e+09, device='cuda:0')
c= tensor(8.3254e+09, device='cuda:0')
c= tensor(8.3261e+09, device='cuda:0')
c= tensor(8.3264e+09, device='cuda:0')
c= tensor(8.3264e+09, device='cuda:0')
c= tensor(8.3264e+09, device='cuda:0')
c= tensor(8.3267e+09, device='cuda:0')
c= tensor(8.3269e+09, device='cuda:0')
c= tensor(8.3330e+09, device='cuda:0')
c= tensor(8.3331e+09, device='cuda:0')
c= tensor(8.3333e+09, device='cuda:0')
c= tensor(8.3348e+09, device='cuda:0')
c= tensor(8.3349e+09, device='cuda:0')
c= tensor(8.3537e+09, device='cuda:0')
c= tensor(8.3542e+09, device='cuda:0')
c= tensor(8.3567e+09, device='cuda:0')
c= tensor(8.3619e+09, device='cuda:0')
c= tensor(8.3621e+09, device='cuda:0')
c= tensor(8.3672e+09, device='cuda:0')
c= tensor(8.3714e+09, device='cuda:0')
c= tensor(8.3714e+09, device='cuda:0')
c= tensor(8.3725e+09, device='cuda:0')
c= tensor(8.3726e+09, device='cuda:0')
c= tensor(8.3780e+09, device='cuda:0')
c= tensor(8.3853e+09, device='cuda:0')
c= tensor(8.3856e+09, device='cuda:0')
c= tensor(8.3872e+09, device='cuda:0')
c= tensor(8.3874e+09, device='cuda:0')
c= tensor(8.3897e+09, device='cuda:0')
c= tensor(8.3897e+09, device='cuda:0')
c= tensor(8.3898e+09, device='cuda:0')
c= tensor(8.4057e+09, device='cuda:0')
c= tensor(8.5734e+09, device='cuda:0')
c= tensor(8.5735e+09, device='cuda:0')
c= tensor(8.5737e+09, device='cuda:0')
c= tensor(8.5739e+09, device='cuda:0')
c= tensor(8.5763e+09, device='cuda:0')
c= tensor(8.5767e+09, device='cuda:0')
c= tensor(8.5767e+09, device='cuda:0')
c= tensor(8.5773e+09, device='cuda:0')
c= tensor(8.5821e+09, device='cuda:0')
c= tensor(8.5822e+09, device='cuda:0')
c= tensor(8.5830e+09, device='cuda:0')
c= tensor(8.5831e+09, device='cuda:0')
time to make c is 8.658580303192139
time for making loss is 8.658600807189941
p0 True
it  0 : 2904473600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5634158592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5634494464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2330060300.0
relative error loss 0.2714701
shape of L is 
torch.Size([])
memory (bytes)
5660475392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 15% |
memory (bytes)
5660475392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 15% |
error is  2310260700.0
relative error loss 0.2691633
shape of L is 
torch.Size([])
memory (bytes)
5663887360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5664120832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2262140000.0
relative error loss 0.26355684
shape of L is 
torch.Size([])
memory (bytes)
5667311616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5667315712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2242024000.0
relative error loss 0.26121318
shape of L is 
torch.Size([])
memory (bytes)
5670428672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5670514688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2207202300.0
relative error loss 0.2571562
shape of L is 
torch.Size([])
memory (bytes)
5673721856
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5673725952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2188842000.0
relative error loss 0.25501707
shape of L is 
torch.Size([])
memory (bytes)
5676920832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5676920832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2172991000.0
relative error loss 0.2531703
shape of L is 
torch.Size([])
memory (bytes)
5680128000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5680132096
| ID | GPU  | MEM |
-------------------
|  0 |   3% |  0% |
|  1 | 100% | 15% |
error is  2164373000.0
relative error loss 0.25216624
shape of L is 
torch.Size([])
memory (bytes)
5683191808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 15% |
memory (bytes)
5683335168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2153972700.0
relative error loss 0.25095454
shape of L is 
torch.Size([])
memory (bytes)
5686558720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 15% |
memory (bytes)
5686558720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2144932900.0
relative error loss 0.24990132
time to take a step is 254.14887404441833
it  1 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5689765888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
5689765888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2144932900.0
relative error loss 0.24990132
shape of L is 
torch.Size([])
memory (bytes)
5692977152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5692977152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2134975000.0
relative error loss 0.24874115
shape of L is 
torch.Size([])
memory (bytes)
5696110592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5696204800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2129241600.0
relative error loss 0.24807316
shape of L is 
torch.Size([])
memory (bytes)
5699411968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5699416064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2122481700.0
relative error loss 0.24728557
shape of L is 
torch.Size([])
memory (bytes)
5702578176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5702578176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2118935600.0
relative error loss 0.24687243
shape of L is 
torch.Size([])
memory (bytes)
5705842688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 15% |
memory (bytes)
5705846784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2111862800.0
relative error loss 0.2460484
shape of L is 
torch.Size([])
memory (bytes)
5708816384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5709017088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2108941800.0
relative error loss 0.24570808
shape of L is 
torch.Size([])
memory (bytes)
5712261120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5712261120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2105573400.0
relative error loss 0.24531564
shape of L is 
torch.Size([])
memory (bytes)
5715288064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 15% |
memory (bytes)
5715472384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2101747700.0
relative error loss 0.24486992
shape of L is 
torch.Size([])
memory (bytes)
5718687744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5718687744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 15% |
error is  2097908700.0
relative error loss 0.24442264
time to take a step is 243.81818389892578
it  2 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5721821184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5721821184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2097908700.0
relative error loss 0.24442264
shape of L is 
torch.Size([])
memory (bytes)
5725097984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5725097984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2095648300.0
relative error loss 0.24415928
shape of L is 
torch.Size([])
memory (bytes)
5728206848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5728305152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2093051900.0
relative error loss 0.24385679
shape of L is 
torch.Size([])
memory (bytes)
5731512320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 15% |
memory (bytes)
5731512320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2089481700.0
relative error loss 0.24344084
shape of L is 
torch.Size([])
memory (bytes)
5734653952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5734719488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2087045100.0
relative error loss 0.24315695
shape of L is 
torch.Size([])
memory (bytes)
5737869312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 15% |
memory (bytes)
5737930752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2084982300.0
relative error loss 0.24291661
shape of L is 
torch.Size([])
memory (bytes)
5741133824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5741137920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2083247600.0
relative error loss 0.24271451
shape of L is 
torch.Size([])
memory (bytes)
5744345088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5744345088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2081283100.0
relative error loss 0.24248563
shape of L is 
torch.Size([])
memory (bytes)
5747544064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5747548160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2079956000.0
relative error loss 0.24233101
shape of L is 
torch.Size([])
memory (bytes)
5750747136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 15% |
memory (bytes)
5750751232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2077899800.0
relative error loss 0.24209145
time to take a step is 244.49904823303223
it  3 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5753966592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5753966592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2077899800.0
relative error loss 0.24209145
shape of L is 
torch.Size([])
memory (bytes)
5756956672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 15% |
memory (bytes)
5757165568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2076635100.0
relative error loss 0.2419441
shape of L is 
torch.Size([])
memory (bytes)
5760368640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5760368640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2075159600.0
relative error loss 0.24177219
shape of L is 
torch.Size([])
memory (bytes)
5763543040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5763579904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2073724900.0
relative error loss 0.24160504
shape of L is 
torch.Size([])
memory (bytes)
5766795264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5766799360
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 99% | 15% |
error is  2071853600.0
relative error loss 0.24138701
shape of L is 
torch.Size([])
memory (bytes)
5769863168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5769961472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2070770200.0
relative error loss 0.2412608
shape of L is 
torch.Size([])
memory (bytes)
5773209600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 15% |
memory (bytes)
5773209600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2069441000.0
relative error loss 0.24110593
shape of L is 
torch.Size([])
memory (bytes)
5776367616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5776424960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2069076500.0
relative error loss 0.24106346
shape of L is 
torch.Size([])
memory (bytes)
5779619840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5779623936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2067345400.0
relative error loss 0.24086177
shape of L is 
torch.Size([])
memory (bytes)
5782732800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 15% |
memory (bytes)
5782818816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2066660400.0
relative error loss 0.24078196
time to take a step is 243.8614981174469
it  4 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5786034176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5786034176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2066660400.0
relative error loss 0.24078196
shape of L is 
torch.Size([])
memory (bytes)
5789237248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 15% |
memory (bytes)
5789237248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2065752600.0
relative error loss 0.24067621
shape of L is 
torch.Size([])
memory (bytes)
5792456704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5792456704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2064486400.0
relative error loss 0.24052869
shape of L is 
torch.Size([])
memory (bytes)
5795647488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5795651584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2063632900.0
relative error loss 0.24042924
shape of L is 
torch.Size([])
memory (bytes)
5798850560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5798850560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2062107100.0
relative error loss 0.24025148
shape of L is 
torch.Size([])
memory (bytes)
5801922560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5802061824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2061431800.0
relative error loss 0.2401728
shape of L is 
torch.Size([])
memory (bytes)
5805281280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5805281280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2060224000.0
relative error loss 0.24003208
shape of L is 
torch.Size([])
memory (bytes)
5808418816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5808480256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2060036600.0
relative error loss 0.24001025
shape of L is 
torch.Size([])
memory (bytes)
5811687424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 15% |
memory (bytes)
5811716096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2058937900.0
relative error loss 0.23988223
shape of L is 
torch.Size([])
memory (bytes)
5814923264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5814923264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2058326500.0
relative error loss 0.239811
time to take a step is 244.7679889202118
it  5 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5818068992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5818130432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2058326500.0
relative error loss 0.239811
shape of L is 
torch.Size([])
memory (bytes)
5821337600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 15% |
memory (bytes)
5821337600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2057561600.0
relative error loss 0.2397219
shape of L is 
torch.Size([])
memory (bytes)
5824544768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 15% |
memory (bytes)
5824544768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2056623100.0
relative error loss 0.23961255
shape of L is 
torch.Size([])
memory (bytes)
5827756032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5827756032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2056206800.0
relative error loss 0.23956405
shape of L is 
torch.Size([])
memory (bytes)
5830901760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5830963200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 15% |
error is  2054698000.0
relative error loss 0.23938826
shape of L is 
torch.Size([])
memory (bytes)
5834166272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 15% |
memory (bytes)
5834166272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2054213100.0
relative error loss 0.23933177
shape of L is 
torch.Size([])
memory (bytes)
5837283328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5837369344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2053394400.0
relative error loss 0.23923638
shape of L is 
torch.Size([])
memory (bytes)
5840592896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 15% |
memory (bytes)
5840592896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2053151200.0
relative error loss 0.23920804
shape of L is 
torch.Size([])
memory (bytes)
5843755008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 15% |
memory (bytes)
5843791872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2052308500.0
relative error loss 0.23910986
shape of L is 
torch.Size([])
memory (bytes)
5846999040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5847003136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 15% |
error is  2051767800.0
relative error loss 0.23904687
time to take a step is 244.22759985923767
it  6 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5850177536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5850206208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2051767800.0
relative error loss 0.23904687
shape of L is 
torch.Size([])
memory (bytes)
5853405184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5853405184
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 15% |
error is  2051260400.0
relative error loss 0.23898776
shape of L is 
torch.Size([])
memory (bytes)
5856481280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5856628736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2050437600.0
relative error loss 0.2388919
shape of L is 
torch.Size([])
memory (bytes)
5859831808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 15% |
memory (bytes)
5859831808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2050876400.0
relative error loss 0.23894301
shape of L is 
torch.Size([])
memory (bytes)
5862858752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 15% |
memory (bytes)
5863038976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2049942000.0
relative error loss 0.23883414
shape of L is 
torch.Size([])
memory (bytes)
5866246144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5866246144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2049410600.0
relative error loss 0.23877223
shape of L is 
torch.Size([])
memory (bytes)
5869367296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 15% |
memory (bytes)
5869461504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2049017300.0
relative error loss 0.23872642
shape of L is 
torch.Size([])
memory (bytes)
5872668672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
5872668672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2048345600.0
relative error loss 0.23864816
shape of L is 
torch.Size([])
memory (bytes)
5875732480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5875875840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2047781400.0
relative error loss 0.23858242
shape of L is 
torch.Size([])
memory (bytes)
5879087104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 15% |
memory (bytes)
5879087104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2047368700.0
relative error loss 0.23853435
time to take a step is 246.46252179145813
it  7 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5882146816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5882290176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2047368700.0
relative error loss 0.23853435
shape of L is 
torch.Size([])
memory (bytes)
5885493248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 15% |
memory (bytes)
5885497344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2046932000.0
relative error loss 0.23848346
shape of L is 
torch.Size([])
memory (bytes)
5888704512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5888708608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2046490100.0
relative error loss 0.23843198
shape of L is 
torch.Size([])
memory (bytes)
5891723264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 15% |
memory (bytes)
5891907584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2046204400.0
relative error loss 0.23839869
shape of L is 
torch.Size([])
memory (bytes)
5895122944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5895127040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2046011400.0
relative error loss 0.2383762
shape of L is 
torch.Size([])
memory (bytes)
5898219520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5898321920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2045864400.0
relative error loss 0.23835908
shape of L is 
torch.Size([])
memory (bytes)
5901557760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5901557760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2045647900.0
relative error loss 0.23833385
shape of L is 
torch.Size([])
memory (bytes)
5904695296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 15% |
memory (bytes)
5904695296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2045382100.0
relative error loss 0.23830289
shape of L is 
torch.Size([])
memory (bytes)
5907976192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5907976192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2045156900.0
relative error loss 0.23827665
shape of L is 
torch.Size([])
memory (bytes)
5911154688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 15% |
memory (bytes)
5911154688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2044942800.0
relative error loss 0.2382517
time to take a step is 246.4988579750061
it  8 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5914271744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5914394624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2044942800.0
relative error loss 0.2382517
shape of L is 
torch.Size([])
memory (bytes)
5917597696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5917597696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2044736000.0
relative error loss 0.2382276
shape of L is 
torch.Size([])
memory (bytes)
5920657408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 15% |
memory (bytes)
5920813056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2044420100.0
relative error loss 0.2381908
shape of L is 
torch.Size([])
memory (bytes)
5924020224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5924020224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2044274200.0
relative error loss 0.2381738
shape of L is 
torch.Size([])
memory (bytes)
5927165952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5927231488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 15% |
error is  2043688400.0
relative error loss 0.23810557
shape of L is 
torch.Size([])
memory (bytes)
5930450944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5930455040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2043413500.0
relative error loss 0.23807353
shape of L is 
torch.Size([])
memory (bytes)
5933547520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5933654016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 15% |
error is  2043057200.0
relative error loss 0.23803201
shape of L is 
torch.Size([])
memory (bytes)
5936873472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5936873472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2042636800.0
relative error loss 0.23798303
shape of L is 
torch.Size([])
memory (bytes)
5939900416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5940076544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2042191400.0
relative error loss 0.23793113
shape of L is 
torch.Size([])
memory (bytes)
5943296000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5943296000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 15% |
error is  2041915400.0
relative error loss 0.23789899
time to take a step is 244.99222826957703
it  9 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5946368000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
5946511360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2041915400.0
relative error loss 0.23789899
shape of L is 
torch.Size([])
memory (bytes)
5949718528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5949718528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2041585200.0
relative error loss 0.23786052
shape of L is 
torch.Size([])
memory (bytes)
5952929792
| ID | GPU | MEM |
------------------
|  0 | 25% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5952933888
| ID | GPU  | MEM |
-------------------
|  0 |  22% |  0% |
|  1 | 100% | 15% |
error is  2041242600.0
relative error loss 0.23782061
shape of L is 
torch.Size([])
memory (bytes)
5956145152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5956149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2040901600.0
relative error loss 0.23778087
shape of L is 
torch.Size([])
memory (bytes)
5959348224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 15% |
memory (bytes)
5959348224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2040711700.0
relative error loss 0.23775874
shape of L is 
torch.Size([])
memory (bytes)
5962493952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
5962563584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2040560100.0
relative error loss 0.23774108
shape of L is 
torch.Size([])
memory (bytes)
5965770752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
5965770752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2040165400.0
relative error loss 0.2376951
shape of L is 
torch.Size([])
memory (bytes)
5968904192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5968986112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 15% |
error is  2039891000.0
relative error loss 0.23766312
shape of L is 
torch.Size([])
memory (bytes)
5972205568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
5972205568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2039627800.0
relative error loss 0.23763247
shape of L is 
torch.Size([])
memory (bytes)
5975347200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 15% |
memory (bytes)
5975408640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2039330300.0
relative error loss 0.23759781
time to take a step is 245.20039892196655
it  10 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
5978619904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
5978619904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2039330300.0
relative error loss 0.23759781
shape of L is 
torch.Size([])
memory (bytes)
5981753344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5981810688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2039041500.0
relative error loss 0.23756416
shape of L is 
torch.Size([])
memory (bytes)
5985030144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
5985030144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 15% |
error is  2038899200.0
relative error loss 0.23754758
shape of L is 
torch.Size([])
memory (bytes)
5988237312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5988237312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2038434800.0
relative error loss 0.23749347
shape of L is 
torch.Size([])
memory (bytes)
5991337984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5991440384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2038214100.0
relative error loss 0.23746777
shape of L is 
torch.Size([])
memory (bytes)
5994651648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
5994651648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2037934100.0
relative error loss 0.23743513
shape of L is 
torch.Size([])
memory (bytes)
5997826048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
5997826048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2037512200.0
relative error loss 0.23738597
shape of L is 
torch.Size([])
memory (bytes)
6001074176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
6001078272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2037431300.0
relative error loss 0.23737656
shape of L is 
torch.Size([])
memory (bytes)
6004211712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
6004285440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2037037600.0
relative error loss 0.23733068
shape of L is 
torch.Size([])
memory (bytes)
6007500800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
6007504896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2036877300.0
relative error loss 0.237312
time to take a step is 245.75040984153748
it  11 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
6010548224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6010703872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2036877300.0
relative error loss 0.237312
shape of L is 
torch.Size([])
memory (bytes)
6013911040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6013911040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2036473900.0
relative error loss 0.237265
shape of L is 
torch.Size([])
memory (bytes)
6017105920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
6017105920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2036160000.0
relative error loss 0.23722844
shape of L is 
torch.Size([])
memory (bytes)
6020325376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6020325376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2035811800.0
relative error loss 0.23718788
shape of L is 
torch.Size([])
memory (bytes)
6023376896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6023532544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2035552300.0
relative error loss 0.23715763
shape of L is 
torch.Size([])
memory (bytes)
6026752000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6026752000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2035222500.0
relative error loss 0.23711921
shape of L is 
torch.Size([])
memory (bytes)
6029946880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
6029950976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2035038700.0
relative error loss 0.2370978
shape of L is 
torch.Size([])
memory (bytes)
6033100800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 15% |
memory (bytes)
6033162240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2034711000.0
relative error loss 0.23705962
shape of L is 
torch.Size([])
memory (bytes)
6036377600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 15% |
memory (bytes)
6036377600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2034508800.0
relative error loss 0.23703606
shape of L is 
torch.Size([])
memory (bytes)
6039441408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6039584768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2034347500.0
relative error loss 0.23701727
time to take a step is 246.18451952934265
it  12 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
6042800128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
6042800128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2034347500.0
relative error loss 0.23701727
shape of L is 
torch.Size([])
memory (bytes)
6045855744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 15% |
memory (bytes)
6046003200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2034120200.0
relative error loss 0.23699078
shape of L is 
torch.Size([])
memory (bytes)
6049214464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 15% |
memory (bytes)
6049214464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2033785900.0
relative error loss 0.23695183
shape of L is 
torch.Size([])
memory (bytes)
6052278272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 15% |
memory (bytes)
6052421632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2033608700.0
relative error loss 0.23693119
shape of L is 
torch.Size([])
memory (bytes)
6055636992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
6055636992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2033417700.0
relative error loss 0.23690894
shape of L is 
torch.Size([])
memory (bytes)
6058786816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 15% |
memory (bytes)
6058786816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2033100800.0
relative error loss 0.23687202
shape of L is 
torch.Size([])
memory (bytes)
6062063616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
6062063616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2032831000.0
relative error loss 0.23684058
shape of L is 
torch.Size([])
memory (bytes)
6065233920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 15% |
memory (bytes)
6065233920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2032652300.0
relative error loss 0.23681976
shape of L is 
torch.Size([])
memory (bytes)
6068486144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
6068486144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 15% |
error is  2032519700.0
relative error loss 0.2368043
shape of L is 
torch.Size([])
memory (bytes)
6071676928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 15% |
memory (bytes)
6071681024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2032234000.0
relative error loss 0.23677103
time to take a step is 245.86469554901123
it  13 : 3567227392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
6074888192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
6074888192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2032234000.0
relative error loss 0.23677103
shape of L is 
torch.Size([])
memory (bytes)
6078091264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6078091264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2031891500.0
relative error loss 0.23673113
shape of L is 
torch.Size([])
memory (bytes)
6081241088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6081302528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2031779800.0
relative error loss 0.23671812
shape of L is 
torch.Size([])
memory (bytes)
6084517888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6084517888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 15% |
error is  2031448600.0
relative error loss 0.23667952
shape of L is 
torch.Size([])
memory (bytes)
6087667712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6087725056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 15% |
error is  2031269900.0
relative error loss 0.2366587
shape of L is 
torch.Size([])
memory (bytes)
6090936320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 15% |
memory (bytes)
6090936320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2031062000.0
relative error loss 0.23663448
shape of L is 
torch.Size([])
memory (bytes)
6093971456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 15% |
memory (bytes)
6094155776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2030843400.0
relative error loss 0.23660901
shape of L is 
torch.Size([])
memory (bytes)
6097371136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6097371136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 15% |
error is  2030658000.0
relative error loss 0.23658742
shape of L is 
torch.Size([])
memory (bytes)
6100508672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6100570112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2031051800.0
relative error loss 0.23663329
shape of L is 
torch.Size([])
memory (bytes)
6103777280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6103781376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 15% |
error is  2030542300.0
relative error loss 0.23657393
time to take a step is 245.58257484436035
it  14 : 3567227904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 15% |
shape of L is 
torch.Size([])
memory (bytes)
6106988544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
6106988544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2030542300.0
relative error loss 0.23657393
shape of L is 
torch.Size([])
memory (bytes)
6110167040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
6110191616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2030381000.0
relative error loss 0.23655514
shape of L is 
torch.Size([])
memory (bytes)
6113402880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 15% |
memory (bytes)
6113406976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 15% |
error is  2030194700.0
relative error loss 0.23653343
shape of L is 
torch.Size([])
memory (bytes)
6116614144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 15% |
memory (bytes)
6116618240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2029931500.0
relative error loss 0.23650277
shape of L is 
torch.Size([])
memory (bytes)
6119825408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 15% |
memory (bytes)
6119825408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2029742100.0
relative error loss 0.2364807
shape of L is 
torch.Size([])
memory (bytes)
6123036672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 15% |
memory (bytes)
6123040768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2029500400.0
relative error loss 0.23645255
shape of L is 
torch.Size([])
memory (bytes)
6126252032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6126252032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 15% |
error is  2029337100.0
relative error loss 0.23643352
shape of L is 
torch.Size([])
memory (bytes)
6129434624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6129463296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2029169200.0
relative error loss 0.23641396
shape of L is 
torch.Size([])
memory (bytes)
6132666368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 15% |
memory (bytes)
6132666368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
error is  2029232100.0
relative error loss 0.23642129
shape of L is 
torch.Size([])
memory (bytes)
6135803904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 15% |
memory (bytes)
6135865344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 15% |
error is  2029050900.0
relative error loss 0.23640017
time to take a step is 245.59061884880066
sum tnnu_Z after tensor(12692178., device='cuda:0')
shape of features
(6528,)
shape of features
(6528,)
number of orig particles 26113
number of new particles after remove low mass 24468
tnuZ shape should be parts x labs
torch.Size([26113, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  2329955600.0
relative error without small mass is  0.2714579
nnu_Z shape should be number of particles by maxV
(26113, 702)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
shape of features
(26113,)
Wed Feb 1 07:13:47 EST 2023
