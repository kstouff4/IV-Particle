Wed Feb 1 00:21:57 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 57786024
numbers of Z: 26025
shape of features
(26025,)
shape of features
(26025,)
ZX	Vol	Parts	Cubes	Eps
Z	0.019808875115770463	26025	26.025	0.09130397638730774
X	0.01879903322032576	1729	1.729	0.22153729095599992
X	0.019398106316517876	32278	32.278	0.08438868885950017
X	0.018792086726355406	3672	3.672	0.17232863387309216
X	0.01940091959738156	23988	23.988	0.09316999922804747
X	0.019226045990982873	48969	48.969	0.07322447172619623
X	0.019356535225750156	56315	56.315	0.0700488771153376
X	0.019615422414743164	63043	63.043	0.06776211162432552
X	0.019358758391250636	65636	65.636	0.06656487657769301
X	0.019218025035364853	16588	16.588	0.10502795498898143
X	0.019320138992055494	28946	28.946	0.08739244627826985
X	0.019651949758471336	12097	12.097	0.11755541928848823
X	0.01928201796825859	127116	127.116	0.05333153554982929
X	0.019333678721289334	12654	12.654	0.11517606120379133
X	0.019426388073991257	456577	456.577	0.03491076320988786
X	0.019368787976380395	32164	32.164	0.08444568518738313
X	0.01939903827363857	56053	56.053	0.07020916039798755
X	0.019372952526552245	78716	78.716	0.06266777359204012
X	0.0192562451827491	29472	29.472	0.08677354351829171
X	0.019393884410991505	201129	201.129	0.04585605639692136
X	0.019353107261791132	118404	118.404	0.05467575344858583
X	0.01919728509563219	60698	60.698	0.06813262856925242
X	0.019650756200150693	281827	281.827	0.04115907032411665
X	0.019117683331373068	14670	14.67	0.10922826867628081
X	0.01933693825893303	64053	64.053	0.06708355639886152
X	0.019021162511918124	5012	5.012	0.15598230410019537
X	0.019398871267803597	93931	93.931	0.059109303010207906
X	0.019477665343292223	93821	93.821	0.059212347701460624
X	0.019293407660330553	20660	20.66	0.09774462269997057
X	0.019133790312718852	108846	108.846	0.05601824674322955
X	0.01979827247779088	1745826	1745.826	0.02246684625420045
X	0.01911887458642671	20906	20.906	0.09706525342637227
X	0.019743816124735237	1055517	1055.517	0.02654536014109674
X	0.019337709776657337	31606	31.606	0.0848942939508196
X	0.01921742011501012	15286	15.286	0.10792790401891023
X	0.01930427130363736	32411	32.411	0.08413699442198196
X	0.019593526410255482	276719	276.719	0.04137054184260562
X	0.019790224485103765	112741	112.741	0.05599160876907713
X	0.018837646954374173	2046	2.046	0.2095915042581901
X	0.019089518673701792	4785	4.785	0.15860042448534525
X	0.018669592655328002	4737	4.737	0.15795879761005624
X	0.01896795094149266	5396	5.396	0.15204873999858265
X	0.018695935139091718	3127	3.127	0.18149852661103508
X	0.018340433878222442	1019	1.019	0.2620666801889017
X	0.019318890906088508	6435	6.435	0.14425954107644215
X	0.019050214207530836	1835	1.835	0.21814968904544368
X	0.018946241152688597	1984	1.984	0.21215851374687708
X	0.019301468630242957	3859	3.859	0.17101670145918352
X	0.018941076426226947	6992	6.992	0.13940159587245574
X	0.019202295405919632	6218	6.218	0.1456243979703778
X	0.018979242399399434	19038	19.038	0.09989701622551202
X	0.01951445859909866	18676	18.676	0.10147464653343388
X	0.018784187770449025	1861	1.861	0.21611355923511835
X	0.019078892081772256	9680	9.68	0.12537932614614156
X	0.019089049706892118	3742	3.742	0.17214464149167036
X	0.019404268340763163	8840	8.84	0.12996158588322923
X	0.01929661642815755	11591	11.591	0.11851866550707
X	0.01974219491986316	3471	3.471	0.17850358412153614
X	0.01892932654380676	7998	7.998	0.13326555770607176
X	0.01912057348718808	4455	4.455	0.16251161336942296
X	0.01869226761245056	4602	4.602	0.1595530554045845
X	0.019380405343548054	10800	10.8	0.12151960995282655
X	0.019263057209694286	2422	2.422	0.19961064219744443
X	0.01925979163620346	10134	10.134	0.12386736378750138
X	0.019256650406679423	14304	14.304	0.11041830638027698
X	0.019214500503064422	4197	4.197	0.16604662139658002
X	0.018861985386463838	2957	2.957	0.18545790940394283
X	0.019183680144726517	4918	4.918	0.1574156112519497
X	0.018516752717468995	5885	5.885	0.14653435371114082
X	0.01905589032659278	6471	6.471	0.14333512019062253
X	0.018742997408307847	5480	5.48	0.15066746995384758
X	0.019234990028170595	9366	9.366	0.12710986886772419
X	0.018991076082385323	7072	7.072	0.13899604421797732
X	0.019247391824226884	4096	4.096	0.1674958225091542
X	0.018841725273887507	4131	4.131	0.165839695140459
X	0.01859050912623567	6555	6.555	0.14154880129058642
X	0.019270781226386677	14565	14.565	0.10978161380934963
X	0.019162424329902915	985	0.985	0.2689499125861711
X	0.01850515716077323	1952	1.952	0.21164320104740475
X	0.01906776473944098	6416	6.416	0.1437733779342977
X	0.018953683264918583	21307	21.307	0.0961738726756154
X	0.019271619009732528	5336	5.336	0.15342664679779836
X	0.018687512882321004	3161	3.161	0.18081828109158743
X	0.019176236381323004	3655	3.655	0.173763543495884
X	0.01868800696394333	3494	3.494	0.17488263792473763
X	0.019043894036028085	2688	2.688	0.19206231707863125
X	0.018695095189885796	2166	2.166	0.205127116989844
X	0.019602720775498475	5618	5.618	0.1516741394731223
X	0.018911601318346606	1662	1.662	0.22492225456273193
X	0.019084374950948047	4685	4.685	0.15970656973905334
X	0.018956628912336945	5870	5.87	0.14781131105532175
X	0.018982613648195947	10860	10.86	0.12045977874318722
X	0.018961806613697875	2603	2.603	0.1938512094611342
X	0.018714769119959222	3358	3.358	0.17729694838110646
X	0.019567670007366936	8732	8.732	0.13086046611816127
X	0.018613435091541706	6396	6.396	0.14277078848817865
X	0.01903247430288224	5523	5.523	0.15104513312526466
X	0.019337556799201815	4380	4.38	0.16404984735188383
X	0.018762737124966466	9288	9.288	0.12641288096583464
X	0.019384450433802877	12070	12.07	0.11710678933045177
X	0.019035137294137985	6040	6.04	0.14661318385135144
X	0.019261194123268793	18714	18.714	0.10096531263308584
X	0.01928841356142784	9232	9.232	0.1278400419391261
X	0.019151606804085966	7513	7.513	0.13660416552080262
X	0.01855213746132837	2341	2.341	0.1993720066247818
X	0.018883962057328847	8300	8.3	0.13152400013950252
X	0.018813340917509355	2822	2.822	0.1882072310667157
X	0.018984827216584406	7319	7.319	0.137399441450614
X	0.018291085759688575	1412	1.412	0.23485596571078488
X	0.01926189908509874	5687	5.687	0.15017764568077133
X	0.019002921879480116	2010	2.01	0.21145020402195555
X	0.018987980842573755	3606	3.606	0.173974002126113
X	0.019429480140668486	4068	4.068	0.16840697650106545
X	0.018601960884149688	3557	3.557	0.17357677719268927
X	0.018859779139901202	4240	4.24	0.16445872361583153
X	0.018991531352877046	8259	8.259	0.13199095553397666
X	0.018744886439945305	5420	5.42	0.15122647962044125
X	0.019223722793287375	5313	5.313	0.15352032924459036
X	0.01877790132188712	1896	1.896	0.21475151673991358
X	0.019675319139456908	10202	10.202	0.12447405049204058
X	0.019096090734578932	1663	1.663	0.22560604912424873
X	0.019380037926905312	16248	16.248	0.10605184719949366
X	0.018950021303214097	3548	3.548	0.17480024236389913
X	0.018878755458492595	3072	3.072	0.18316846947420684
X	0.019272898528030002	5098	5.098	0.15578144737207258
X	0.018902300634478272	3280	3.28	0.17928623238548247
X	0.01875816752554293	3584	3.584	0.17362308542221883
X	0.018882752370121155	1868	1.868	0.21622013888331335
X	0.019256812329597087	7016	7.016	0.1400118967930569
X	0.019221518713682457	11775	11.775	0.11774494224191599
X	0.018885896662273926	3266	3.266	0.1794900903657066
X	0.019209159697370953	9325	9.325	0.12723888133781877
X	0.019019399691267612	2492	2.492	0.1968866437250872
X	0.01922343447047684	5805	5.805	0.14905375424680445
X	0.018791819657136905	2058	2.058	0.2090135725648449
X	0.018813378691815423	4072	4.072	0.166553210786617
X	0.019689487627124948	4635	4.635	0.16195519826936028
X	0.019134395108772982	3424	3.424	0.1774572146138869
X	0.019019241494778096	2800	2.8	0.18938477130549836
X	0.018858512804909335	2622	2.622	0.19303004764887402
X	0.018765503793112287	1929	1.929	0.21347285916775033
X	0.019247145787877416	4268	4.268	0.1652141758830156
X	0.01880879998364285	1963	1.963	0.21239629146618347
X	0.019299716606697544	11686	11.686	0.11820295941550116
X	0.019286022220379992	18428	18.428	0.1015285428244676
X	0.019445017448588942	6803	6.803	0.14191750108779125
X	0.018784739469714533	3841	3.841	0.1697410781266119
X	0.01893095126498626	2697	2.697	0.19146842448448964
X	0.018796587882899504	1817	1.817	0.2178920395663351
X	0.019054658100540633	2165	2.165	0.2064656241016124
X	0.018956001385278004	3701	3.701	0.17237563626050095
X	0.018954152482492204	4047	4.047	0.16731070218753727
X	0.019007057090532884	4911	4.911	0.15700554172076886
X	0.01935294548365761	4372	4.372	0.16419337881578616
X	0.01934083736203344	13351	13.351	0.11314980912761051
X	0.019034879315533746	5096	5.096	0.15515778441680442
X	0.01898017746716986	24475	24.475	0.09187397836506572
X	0.018830014284249724	4296	4.296	0.1636548167908778
X	0.019282143939082785	5547	5.547	0.15148363510667165
X	0.0191240544752484	5733	5.733	0.14941676985755656
X	0.01852191148008445	810	0.81	0.2838356409583999
X	0.0194866624657622	17769	17.769	0.10312362203485329
X	0.018872735379837392	1978	1.978	0.2120978117128499
X	0.019011485012476324	9682	9.682	0.12522287034166307
X	0.019229305160820363	1520	1.52	0.23300934257659503
X	0.01907632754709981	7518	7.518	0.13639469630374865
X	0.019053904412229122	3669	3.669	0.17317243226440027
X	0.019179216970430464	2797	2.797	0.1899821516410476
X	0.01897116474034088	5057	5.057	0.15538187122312577
X	0.01909647751122787	5990	5.99	0.14717774609734724
X	0.018576192620862603	1065	1.065	0.2593397170327757
X	0.01867462131719888	1066	1.066	0.2597156975649256
X	0.018578191361727527	2039	2.039	0.20886326788938445
X	0.019291259876407913	10876	10.876	0.12104972232121
X	0.018997889716899242	2640	2.64	0.19306363483989883
X	0.01930600303276553	6650	6.65	0.14265606540883644
X	0.01924414612138174	11416	11.416	0.11901313496712122
X	0.01899643252098431	6908	6.908	0.14010057147534385
X	0.01924792425193354	3763	3.763	0.17229918672309194
X	0.01903211329853218	8107	8.107	0.1329052939925837
X	0.019138041473488965	3531	3.531	0.1756574570460412
X	0.019379670269713944	11521	11.521	0.11892835239729897
X	0.01882184598236681	3331	3.331	0.17811309934591996
X	0.018886898625530814	6175	6.175	0.14515808466004199
X	0.01883136609908288	2523	2.523	0.19542863201380548
X	0.019497145802467572	5439	5.439	0.15304435904325964
X	0.0192560816761166	8580	8.58	0.13092629468150815
X	0.01884795940344611	3034	3.034	0.18382994351080117
X	0.019246746408747614	8877	8.877	0.1294286341426135
X	0.01960284902638884	7885	7.885	0.13546876320670015
X	0.019122033229058043	13614	13.614	0.11199091012985843
X	0.019230497156956516	4034	4.034	0.168300293252029
X	0.018828330319063787	1138	1.138	0.2548139395197433
X	0.019168338874236638	4198	4.198	0.16590036751970016
X	0.01927186438609473	4602	4.602	0.16118540156077785
X	0.01882329413436563	9209	9.209	0.1269095736806862
X	0.018849686333483492	4401	4.401	0.16239933744045965
X	0.019261527720515596	3475	3.475	0.17697502147828
X	0.018891910067897897	2493	2.493	0.19641946795978332
X	0.018831302269640943	6197	6.197	0.14484370306881958
X	0.019250726070613983	1420	1.42	0.23844392199450473
X	0.019595315969517163	11822	11.822	0.1183460512869977
X	0.019089988844889116	4123	4.123	0.16667256279160111
X	0.01980683612269959	19536	19.536	0.10045999539440614
X	0.019308418539828256	4082	4.082	0.16786412896990008
X	0.018904973738770408	6964	6.964	0.13949943179442792
X	0.018906756356087778	1870	1.87	0.21623457666331627
X	0.019481216989012583	4886	4.886	0.15856994431846583
X	0.01960784201589565	5412	5.412	0.153588018366016
X	0.019565192730749716	4301	4.301	0.16569328029820288
X	0.01913817904282605	9042	9.042	0.12839418951765164
X	0.01913341134642503	7640	7.64	0.13579997290339613
X	0.0192080530100942	4178	4.178	0.16627934432512245
X	0.018939654541252033	3798	3.798	0.17084634714324587
X	0.019025902237200385	5348	5.348	0.15265743258599584
X	0.01874727384000262	1471	1.471	0.23358331455792622
X	0.018556873972683786	2014	2.014	0.20964368458413485
X	0.0184120314920749	1053	1.053	0.2595520395827636
X	0.01896044661418884	1813	1.813	0.21868395495550294
X	0.01893879964980123	17447	17.447	0.1027725729902419
X	0.01888839060987847	2216	2.216	0.2042717924416636
X	0.018870665959505047	4755	4.755	0.15832356798264766
X	0.019294346945993975	1682	1.682	0.2255284650257014
X	0.018796152100217532	8561	8.561	0.12997150600534502
X	0.019179028073929263	4157	4.157	0.16647493607029246
X	0.019035328486589582	3284	3.284	0.17963284258916942
X	0.01890631409547802	4557	4.557	0.16068522553829084
X	0.0188071422681303	2475	2.475	0.19659955728173392
X	0.01884623191081002	2960	2.96	0.1853436052955285
X	0.019266571783886636	5718	5.718	0.1499178796904444
X	0.019263507534112632	1508	1.508	0.23376420375319135
X	0.01907938080691022	2721	2.721	0.1914014593751937
X	0.019488108634080605	5094	5.094	0.1564000606307773
X	0.01930956269473672	3961	3.961	0.16955965802525674
X	0.019425662960073052	6930	6.93	0.14099840663934748
X	0.018556439386414988	2831	2.831	0.18714788611105307
X	0.018799378818826193	4268	4.268	0.1639229254774436
X	0.019533226263677064	9932	9.932	0.12528885009633328
X	0.019542648926133364	7345	7.345	0.13856829609455984
X	0.01918362826761295	3708	3.708	0.17295389286000598
X	0.01963811435375926	27480	27.48	0.08940483762113507
X	0.019331446577133222	64354	64.354	0.06697246271755396
X	0.01973330390026724	7079	7.079	0.14073732668021954
X	0.01956297362150813	9931	9.931	0.12535662638177855
X	0.018261919459174467	1002	1.002	0.26316385150957783
X	0.019215207212163906	9765	9.765	0.12531150372643315
X	0.01924846846924314	14859	14.859	0.10901063406936175
X	0.019353382961489814	154494	154.494	0.05003590480694235
X	0.019303245859627626	3674	3.674	0.17384561073901764
X	0.01923166691475918	196922	196.922	0.04605119420698558
X	0.018984237947023863	93139	93.139	0.058851002596992004
X	0.019303784758805564	45891	45.891	0.07492699495775101
X	0.01967818785814506	548874	548.874	0.032973937806491285
X	0.019236222235724902	4127	4.127	0.16704306073397351
X	0.01943989722216629	10182	10.182	0.12405671890210809
X	0.019369727489178402	254228	254.228	0.0423935751581381
X	0.01940902704814781	513559	513.559	0.033558649374147785
X	0.019417005039062175	5815	5.815	0.14946660662762784
X	0.019281660217725885	61496	61.496	0.06793587693875487
X	0.0193187382186512	23566	23.566	0.09359032629348149
X	0.0196837218046934	60588	60.588	0.06874485504554487
X	0.019382785278966103	132748	132.748	0.052657802667717
X	0.018882739151541178	84485	84.485	0.06068692444996545
X	0.019373689504820612	30679	30.679	0.08579402542577666
X	0.019330472636458377	21692	21.692	0.09623084345101435
X	0.018832287279827095	3743	3.743	0.17135406614706478
X	0.01937814906543724	147184	147.184	0.050872595642983805
X	0.01911383935218448	11780	11.78	0.11750803042325218
X	0.01890547971028575	4456	4.456	0.16188781937709235
X	0.019461251996520542	32216	32.216	0.08453431799295777
X	0.01949758204741862	98999	98.999	0.05818128598619842
X	0.019736393956650118	264937	264.937	0.04207670570191758
X	0.019636496530985065	111547	111.547	0.05604480599949127
X	0.01886752853515153	3284	3.284	0.17910345169305056
X	0.019330151300766672	37371	37.371	0.08027251033373528
X	0.019476500515117665	12459	12.459	0.116058222184174
X	0.01940080836824488	46579	46.579	0.07468095939091664
X	0.019405702984704083	73198	73.198	0.06424066899909857
X	0.019288092039620425	14930	14.93	0.10891218910853814
X	0.01950828011705395	90504	90.504	0.05995845078288509
X	0.018286156291443225	933	0.933	0.2696167863348599
X	0.019204173143879594	8538	8.538	0.13102268272212134
X	0.019347390290568214	106689	106.689	0.056602322228717104
X	0.01914604884778263	89353	89.353	0.05983978274030956
X	0.018934014281018652	88974	88.974	0.05970259315579902
X	0.01934414826465771	6821	6.821	0.1415469516636366
X	0.01973997394418627	308196	308.196	0.040010427565079366
X	0.019386768392189828	18876	18.876	0.10089395606097769
X	0.01936783274311489	37547	37.547	0.08019893376596643
X	0.019361810225862162	102848	102.848	0.05731259014648964
X	0.01909055442187842	2922	2.922	0.18694453616768034
X	0.019379649528167765	160161	160.161	0.04946101780623339
X	0.019775537110798328	273366	273.366	0.041667226806606464
X	0.019786861828772494	552886	552.886	0.03295442786132373
X	0.019262247102469265	54640	54.64	0.07064239580484344
X	0.018778088063609703	34879	34.879	0.08135091626680546
X	0.019043999429456428	8313	8.313	0.13182571223213158
X	0.01929654879882206	7351	7.351	0.13794662296966934
X	0.019601470641567788	166629	166.629	0.048998113137386196
X	0.019331910234352448	11885	11.885	0.11760483978488441
X	0.019314086004063986	38030	38.03	0.07978402556277706
X	0.01940607205343025	48514	48.514	0.07368122741052503
X	0.01937508720971436	24695	24.695	0.09223127489194637
X	0.018984893283105368	21407	21.407	0.09607656122560775
X	0.019287005928099343	6703	6.703	0.1422323977824828
X	0.01938029014197581	139005	139.005	0.0518533259002611
X	0.0193407305456441	70416	70.416	0.06500306593464072
X	0.019304732366150452	31133	31.133	0.08527353582239286
X	0.01967453306976139	21359	21.359	0.09729887962075846
X	0.01927631985844267	77790	77.79	0.06281067359057163
X	0.019158064731881794	24830	24.83	0.09171878766141339
X	0.019765735258171515	278756	278.756	0.041390078496780905
X	0.01915597961867908	8515	8.515	0.1310307533597863
X	0.018800580640415506	39360	39.36	0.07816975021001299
X	0.01933246266184288	12117	12.117	0.11685053111920973
X	0.01935785373495658	169815	169.815	0.048487193970057775
X	0.01956192600388206	52016	52.016	0.07218133108303014
X	0.01954569883816415	17545	17.545	0.10366510934046236
X	0.019340137669871815	139274	139.274	0.05178410851855475
X	0.01967368761296734	216740	216.74	0.04494163836245367
X	0.019361284487119556	34999	34.999	0.08209048915808512
X	0.019750998489582196	176435	176.435	0.048194939341334224
X	0.01974659054254984	227735	227.735	0.04426095495672481
X	0.01982931893313817	495418	495.418	0.03420676510698815
X	0.019388054665220658	23805	23.805	0.09338748577179636
X	0.01878525046242964	4092	4.092	0.16619850834261843
X	0.019297399191543274	18174	18.174	0.10201939703373189
X	0.019368788902230746	14629	14.629	0.10980681007302048
X	0.01929854389705913	27942	27.942	0.08839391130967748
X	0.01961207751281194	70885	70.885	0.0651613018509402
X	0.019347457367396424	3783	3.783	0.17229097116244155
X	0.019340077921609737	89254	89.254	0.06006343656678782
X	0.01971426216169958	200115	200.115	0.04618493053683395
X	0.01889788687474744	15042	15.042	0.1079035882027231
X	0.019353902423392817	11999	11.999	0.11727564529213842
X	0.019215392887662365	49624	49.624	0.0728874069805782
X	0.019138146005866545	14458	14.458	0.10979870409665095
X	0.019090527353544003	14130	14.13	0.11054995951327758
X	0.019327680090235333	21432	21.432	0.09661376558101702
X	0.01933053269036645	17512	17.512	0.10334814625875688
X	0.01899788006328581	48196	48.196	0.0733215068957896
X	0.019369421190750795	51000	51.0	0.0724183828182066
X	0.019618453072256004	32680	32.68	0.08435825173411346
X	0.01962646803505097	49689	49.689	0.07337148317371785
X	0.01927167005466385	9882	9.882	0.1249371229172734
X	0.01977696031161524	1000486	1000.486	0.027038515392075416
X	0.019275855672982026	15598	15.598	0.10731199035405731
X	0.019595165637824593	139175	139.175	0.05202306109450864
X	0.01902283239520225	3119	3.119	0.1827061932022286
X	0.019306153342907566	6232	6.232	0.1457771433565232
X	0.018830182801135548	5121	5.121	0.15434761393385835
X	0.019372014103955976	20574	20.574	0.09801337568626517
X	0.01912314754658182	5673	5.673	0.14993931787985873
X	0.01965374822896818	123701	123.701	0.05416150527901589
X	0.019175596851090767	5862	5.862	0.1484457239761007
X	0.019254462009939233	6761	6.761	0.14174469622200717
X	0.019741529016970624	313076	313.076	0.03980249868702119
X	0.019394802693197145	66706	66.706	0.06624810941167052
X	0.019247968682544376	49568	49.568	0.07295602594329698
X	0.01980623000508061	161925	161.925	0.04963971782746576
X	0.01980230764875232	397933	397.933	0.03678199512215735
X	0.018975766441176017	7429	7.429	0.13669616248514022
X	0.01885553882873867	13372	13.372	0.11213663678944125
X	0.019083357501863303	25647	25.647	0.0906162470435797
X	0.019088614816626014	2471	2.471	0.19768204404431108
X	0.01944778501627215	7789	7.789	0.13566343640812725
X	0.019293071287014706	19032	19.032	0.10045517471333315
X	0.018824685044983788	4306	4.306	0.16351260256852992
X	0.019020850351956727	9709	9.709	0.12512722387025543
X	0.01896357092075516	10834	10.834	0.12051573755365379
X	0.01899239867713238	6950	6.95	0.13980788673567227
X	0.01964418190964757	376948	376.948	0.037352313092105155
X	0.018958600156479724	19106	19.106	0.09974217430431435
X	0.01973373182817427	89908	89.908	0.06032124478834463
X	0.019012515884574685	5757	5.757	0.14891820295958166
X	0.01896334283590592	6178	6.178	0.14533013073652434
X	0.019260867570157398	100157	100.157	0.057720689795749666
X	0.019424605218208865	481411	481.411	0.034298784578169955
X	0.019423561583499935	570359	570.359	0.03241356921949691
X	0.01931534933118777	22259	22.259	0.095381827501656
X	0.019418787599551655	47855	47.855	0.07403406518178922
X	0.019252165212398652	3890	3.89	0.17041585305637966
X	0.019268610185976136	16605	16.605	0.10508413901847073
X	0.019395932375758953	617143	617.143	0.03155791373308585
X	0.019471285900635828	24112	24.112	0.09312232264552375
X	0.019139955301943543	12175	12.175	0.11627615168498899
X	0.01938872682296152	53622	53.622	0.07124188923012735
X	0.019757949078055663	810890	810.89	0.02899078481439903
X	0.019037547825672987	48533	48.533	0.07320228257077338
X	0.019450046958598782	22330	22.33	0.0955015991302501
X	0.01970539965163218	18231	18.231	0.10262620089069437
X	0.019329269276125632	44101	44.101	0.07596070724469489
X	0.01927091307042956	6992	6.992	0.14020611566272437
X	0.01938656953568094	223218	223.218	0.044285061439755004
X	0.019214678769480433	24438	24.438	0.09229733385224854
X	0.018756921859232837	2764	2.764	0.18932494681081635
X	0.0193972976710449	118032	118.032	0.05477476060524565
X	0.01975292089814303	30806	30.806	0.08623138029057914
X	0.019079858191417134	2162	2.162	0.20665209747720834
X	0.019683796350869965	74432	74.432	0.06418739120291914
X	0.01944048996840958	123231	123.231	0.05403341434323813
X	0.01965201250323535	325784	325.784	0.03921863073760399
X	0.019696116586564713	242197	242.197	0.04332487140149119
X	0.01922041300900955	234556	234.556	0.043434889693990605
X	0.018909122807559624	27444	27.444	0.08832317190456974
X	0.01909599816974582	30279	30.279	0.08575647980330409
X	0.01938530421995401	120925	120.925	0.054323220439980426
X	0.01954281361545418	270477	270.477	0.04165037504649872
X	0.019219034286206704	12666	12.666	0.11491164081492267
X	0.01942777901003055	187185	187.185	0.04699489717066758
X	0.019764569910277058	336217	336.217	0.038882653918785356
X	0.019395390165207322	218200	218.2	0.04462873648493477
X	0.019388529632144654	119247	119.247	0.05457986673468487
X	0.019637871248703625	57471	57.471	0.06991148891556248
X	0.019275149678207055	17456	17.456	0.10335964491064638
X	0.019248386278792832	6936	6.936	0.1405276381178446
X	0.018874381060814314	21196	21.196	0.09620691072506456
X	0.01938345303705148	168160	168.16	0.04866717633250581
X	0.019405805791533084	125384	125.384	0.05369038523091781
X	0.01921130195241311	317717	317.717	0.039249920519365995
X	0.019773804645255702	119124	119.124	0.05495792458247965
X	0.019290642695737874	91200	91.2	0.05958230306872795
X	0.019358548016782873	18285	18.285	0.10191996639338408
X	0.019404636730624866	209588	209.588	0.04523900397041914
X	0.01880063407887549	8554	8.554	0.13001728220723446
X	0.0193375935397694	14816	14.816	0.10928414345531837
X	0.01969528005045593	174713	174.713	0.048307249794878604
X	0.019359269600177646	61592	61.592	0.0679915414243518
X	0.019176753933809982	6823	6.823	0.14112368498513958
X	0.019634678637135625	22903	22.903	0.09499712780942887
X	0.01974479792607842	654241	654.241	0.031134243100632265
X	0.01926221677430011	27017	27.017	0.08933528921868847
X	0.01980950989202711	150482	150.482	0.050870154742107015
X	0.01881432742937457	15989	15.989	0.10557371363260432
X	0.01896235527381693	12056	12.056	0.1162955275942952
X	0.01935377266932473	6699	6.699	0.14242466913767698
X	0.018794019972101863	6991	6.991	0.13904652106071638
X	0.01961078057160044	27384	27.384	0.08946764339860401
X	0.019200814657077382	65282	65.282	0.0665031272521473
X	0.01932434520113752	7749	7.749	0.13560833238111605
X	0.01938163239534515	159150	159.15	0.04956722088595853
X	0.019246269350807844	7734	7.734	0.13551294926131846
X	0.019654327254709718	92760	92.76	0.059616409223816315
X	0.018996207831709345	15456	15.456	0.10711655606833441
X	0.019216540607163133	119562	119.562	0.05437016849892518
X	0.01899614584317723	8152	8.152	0.13257667050876223
X	0.019322229884747542	27478	27.478	0.08892503440059332
X	0.01931035410642991	37737	37.737	0.07998482924321602
X	0.019450540795113345	31263	31.263	0.08536902932793489
X	0.019716023512063	89023	89.023	0.06050236722822885
X	0.019372911835158553	531880	531.88	0.033148235375907005
X	0.0190522601144936	11790	11.79	0.11734850623163118
X	0.0188722865760077	6922	6.922	0.13970039773581894
X	0.019654781574637733	94580	94.58	0.05923198731291303
X	0.018945234014300597	26087	26.087	0.08988597742109404
X	0.019688921661752165	567390	567.39	0.032617205605469914
X	0.018831825645931874	4424	4.424	0.16206619484720305
X	0.0197565181452409	160083	160.083	0.04978766183088367
X	0.019798667996334524	383201	383.201	0.03724515446660915
X	0.01951338116309488	6268	6.268	0.1460162464246188
X	0.01902264565835814	221414	221.414	0.04412539574660018
X	0.019360146465906326	20594	20.594	0.09796162411468223
X	0.01978200181000746	621326	621.326	0.03169447555392034
X	0.018916027322580242	12020	12.02	0.11631663284446121
X	0.019148179727965828	25334	25.334	0.0910908125984325
X	0.019579821376062807	14046	14.046	0.11170827713843383
X	0.01915090131881599	5367	5.367	0.15281026928680058
X	0.019160497422960644	13068	13.068	0.11360546350005502
X	0.019353772608582905	56976	56.976	0.06977361522387256
X	0.018846562172049205	6391	6.391	0.14340174671441633
X	0.01976360383868638	184818	184.818	0.04746505165922648
X	0.01921233146058198	21490	21.49	0.0963343604261721
X	0.019105957204050714	7590	7.59	0.13603239412839385
X	0.019489253258077762	26100	26.1	0.09072317290816367
X	0.01978287149625206	258182	258.182	0.042473800523896516
X	0.019694699433274416	134422	134.422	0.05271809011993356
X	0.01977394195237772	376340	376.34	0.037454524806011046
X	0.019225116557616117	24907	24.907	0.09173094204525076
X	0.019179523865227463	9172	9.172	0.1278766523583758
X	0.01961760321344255	7528	7.528	0.1376117270232019
X	0.018573257903638913	5318	5.318	0.15172131054957785
X	0.019620438569616233	254242	254.242	0.04257491649711896
X	0.01918650177698326	9349	9.349	0.1270799044101591
X	0.019408922220550972	35254	35.254	0.08195919292948678
X	0.019236617629529093	28460	28.46	0.08776026761878693
X	0.019654639520071186	29624	29.624	0.0872182017005
X	0.019356260543250032	23249	23.249	0.094074602929209
X	0.019384241197681836	12745	12.745	0.11500136320335308
X	0.01957463774067232	222453	222.453	0.04447867303730093
X	0.01933380826929762	53892	53.892	0.07105550037507702
X	0.01890986090680274	11306	11.306	0.11870247776851348
X	0.01937537775473788	38308	38.308	0.07967466264249645
X	0.019359612209122346	90977	90.977	0.05970192676058871
X	0.019354726429834518	119235	119.235	0.05454995881518024
X	0.019540348603028692	466676	466.676	0.034724735538252643
X	0.019388727095307385	121273	121.273	0.054274403480100916
X	0.019305021187044455	8099	8.099	0.133581493433218
X	0.019398803102970985	44489	44.489	0.07582994895794419
X	0.019588189628565244	49287	49.287	0.07352256304688111
X	0.019323766547505325	13058	13.058	0.11395630859030424
X	0.019177687189753132	15780	15.78	0.1067160504993956
X	0.018985385823794707	6569	6.569	0.1424426534379674
X	0.01951735266543943	29439	29.439	0.08719654664222191
X	0.019084848608661713	37683	37.683	0.07971029497961002
X	0.01881772024938809	4820	4.82	0.15746111529253637
X	0.01924803261683219	27735	27.735	0.08853589704324127
X	0.01935280494101292	8415	8.415	0.1319967585476842
X	0.019040427498541557	31275	31.275	0.08475391962545366
X	0.01918564751311927	8007	8.007	0.1338142027286431
X	0.01928557257319802	32860	32.86	0.08372496848072687
X	0.018604722052585246	16976	16.976	0.10310093564765581
X	0.01926287656923311	18327	18.327	0.101674002574753
X	0.018997746463703485	8207	8.207	0.13228356322736795
X	0.018951586580529232	13885	13.885	0.1109259686752983
X	0.019391717322362046	32808	32.808	0.08392258226841227
X	0.01939204718150393	155920	155.92	0.04991609596779003
X	0.019443096474801184	9356	9.356	0.12761206585400076
X	0.019069584750243316	2925	2.925	0.18681215137685708
X	0.018839473945118906	9020	9.02	0.12782645497703907
X	0.019377112238439513	130071	130.071	0.053011430685108475
X	0.019803433359161685	633829	633.829	0.031496052700379217
X	0.019374804413671044	96103	96.103	0.05863633184396021
X	0.019252634707606766	8347	8.347	0.13212548233730062
X	0.01941825533855946	213547	213.547	0.044968207637981324
X	0.01973072631676579	205580	205.58	0.0457847383832521
X	0.018987635302153177	3458	3.458	0.17642033031511148
X	0.018975671030470324	13062	13.062	0.1132563308510184
X	0.019383820597469223	43302	43.302	0.0764968951940672
X	0.01956429627943499	114488	114.488	0.05549255921375737
X	0.019600202295133158	237291	237.291	0.043550495012216264
X	0.019357815970282095	227868	227.868	0.04395999881980738
X	0.019503379370385807	23904	23.904	0.09344292606878038
X	0.018921473295259124	6377	6.377	0.14369649575999122
X	0.019076968464450086	3387	3.387	0.17792271145192276
X	0.019073864237676888	82420	82.42	0.06139533295997704
X	0.01964514582282699	18750	18.75	0.10156669677976085
X	0.018939582108198082	32704	32.704	0.08335337183480893
X	0.019754360328784606	157567	157.567	0.05004944143119859
X	0.019781921030993733	638438	638.438	0.0314086964124394
X	0.01942132876323178	12909	12.909	0.11458526665447838
X	0.019392083766440974	3180	3.18	0.1826974361606901
X	0.019221530129143558	30952	30.952	0.08531650967079928
X	0.019384171144147495	137971	137.971	0.051986008625330246
X	0.01913686495125732	45586	45.586	0.07487665509969935
X	0.01890377558657668	7825	7.825	0.13418008693678424
X	0.019242084216302035	4861	4.861	0.15818871444206054
X	0.019210193691840828	33259	33.259	0.08328002602421897
X	0.01909465038628224	4671	4.671	0.15989465560548505
X	0.019751401932721995	56569	56.569	0.07041629031488716
X	0.01924450853007644	7671	7.671	0.13587877219302205
X	0.019377233694013625	20436	20.436	0.09824232403636796
X	0.019406815650778585	18074	18.074	0.10240001041897717
X	0.01892005475892162	9683	9.683	0.12501750261201294
X	0.019014579167644995	9603	9.603	0.12557213059951416
X	0.019386167674163266	189488	189.488	0.04677029231410861
X	0.01958686007170675	395377	395.377	0.03672691695538511
X	0.019397448618958313	114539	114.539	0.05532614462585291
X	0.0197642156097182	128607	128.607	0.05356383437202025
X	0.019336176359811656	23066	23.066	0.0942901015783115
X	0.0190412985724914	18088	18.088	0.10172678938221083
X	0.019441471082564422	27432	27.432	0.08915736347053259
X	0.01939418198950089	168778	168.778	0.04861667017931289
X	0.019321027266191927	37840	37.84	0.07992691096143632
X	0.019366075165132248	50567	50.567	0.07262031717722488
X	0.019482785359034183	16462	16.462	0.10577657067045612
X	0.01979703411242327	1482772	1482.772	0.023723308236506455
X	0.019316588376697773	21939	21.939	0.09584538342138357
X	0.0197026026434168	78817	78.817	0.06299429686919777
X	0.019711498006592682	207896	207.896	0.04559926393657942
X	0.01938555331947085	63712	63.712	0.06725929658859262
X	0.01935561284026117	41909	41.909	0.07729769486755622
X	0.019800216117329107	841400	841.4	0.028656455667553475
X	0.0197341872661968	155439	155.439	0.050259683628206805
X	0.01931245316889163	97537	97.537	0.05828489476450521
X	0.01918611971781712	9879	9.879	0.12476460276876175
X	0.019357594442823804	53576	53.576	0.07122411041353191
X	0.019349137213403884	11835	11.835	0.11780519632651786
X	0.019753564932581263	199871	199.871	0.04623440074285345
X	0.019805736572480415	1453259	1453.259	0.023886324376220566
X	0.019705363330060365	97103	97.103	0.05876480053926915
X	0.019347358466425768	190392	190.392	0.04666497033720489
X	0.019680650025264555	108144	108.144	0.05666901576170869
X	0.019328927657641173	29429	29.429	0.08692487707886674
X	0.01962771485605716	30726	30.726	0.0861234141366853
X	0.018950888847925947	15979	15.979	0.1058506012437002
X	0.01965862443434307	193671	193.671	0.046647622115302066
X	0.019760952210695112	544051	544.051	0.03311738866724151
X	0.019405062866555734	109098	109.098	0.056238373921078465
X	0.019384828520425508	938503	938.503	0.027437345965732125
X	0.01963185241513007	180659	180.659	0.047720013682947295
X	0.01937635763378972	47371	47.371	0.07423120826791293
X	0.019010258558026898	10827	10.827	0.12064054643702586
X	0.01938105493628151	107464	107.464	0.056498658611835076
X	0.019616772086464486	171164	171.164	0.04857413100049051
X	0.019471874314950278	10937	10.937	0.12120015684399106
X	0.019505374505955565	701778	701.778	0.030291318768841315
X	0.019382021370023088	22822	22.822	0.09469966783269419
X	0.019118694541158235	38496	38.496	0.07919192068860964
X	0.019330517709186885	6559	6.559	0.14337341116742575
X	0.019234032654910287	11182	11.182	0.11981658955002752
X	0.0189130789646237	3278	3.278	0.17935676503955988
X	0.018644468527665894	7530	7.53	0.13528563276105932
X	0.019250482965983726	21903	21.903	0.09578834628290767
X	0.019239237376568703	88113	88.113	0.0602165618215022
X	0.01975874341915461	3141151	3141.151	0.01845965170397348
X	0.018990902481701016	15755	15.755	0.10642469120323417
X	0.01935601849103169	203583	203.583	0.045641317540672695
X	0.019404223533501647	11023	11.023	0.12074398002485986
X	0.019351483175696616	27377	27.377	0.08907916420824419
X	0.01879962586905914	9053	9.053	0.12758087078263297
X	0.01973449048773486	212511	212.511	0.0452843530606462
X	0.018984191871608852	61841	61.841	0.06745878843493228
X	0.019824733752061716	1143516	1143.516	0.02588143621675363
X	0.0190308171365314	8525	8.525	0.130693608607248
X	0.019715664963815552	189529	189.529	0.0470303906416758
X	0.01970976700596619	25368	25.368	0.09193165681073721
X	0.01924656331311052	183486	183.486	0.04716104501834005
X	0.019375429391156225	259942	259.942	0.04208477018685179
X	0.019095503955923264	23263	23.263	0.09363146126107379
X	0.01924042831615376	13494	13.494	0.11255324225372532
X	0.019579197222986268	132453	132.453	0.05287426710049409
X	0.019233394287814665	32030	32.03	0.08436578115162885
X	0.019190501410803373	55267	55.267	0.07028675238311768
X	0.019366202474612033	248444	248.444	0.04271744986689711
X	0.01930231125606481	23593	23.593	0.09352808644882059
X	0.019702150859120793	92049	92.049	0.05981794950225393
X	0.0190555953390845	31095	31.095	0.08493968645266661
X	0.01971104665972022	169135	169.135	0.048845594459910185
X	0.0196717646537379	288319	288.319	0.04086235122120763
X	0.019360674849341817	36017	36.017	0.08130881877077897
X	0.01924722456730671	13485	13.485	0.11259153003136563
X	0.01916204198557451	152134	152.134	0.050127013928695285
X	0.019126177766570624	111227	111.227	0.05560826196717367
X	0.01956271236923065	418567	418.567	0.03602091119988806
X	0.01925336026815576	45746	45.746	0.07494071009851516
X	0.018866550454132217	40235	40.235	0.07768959042784906
X	0.01969718815663725	31459	31.459	0.08554995262274048
X	0.01894372797791497	194767	194.767	0.04598858261453284
X	0.019392619677067836	165170	165.17	0.04896680528442531
X	0.01928063292458306	34238	34.238	0.08257925609298097
X	0.01919730637850114	9192	9.192	0.12782332000387012
X	0.019177822625611185	10852	10.852	0.1209009845389333
X	0.019208764847371287	55032	55.032	0.07040897919108302
X	0.019334959456130937	110906	110.906	0.05586366283917297
X	0.019389076763163902	88864	88.864	0.06020193132615389
X	0.01938321019036387	16408	16.408	0.10571177196446177
X	0.019393247636508366	31184	31.184	0.08535708471348642
X	0.019339947008544627	53879	53.879	0.07106873486740292
X	0.019628657720968074	61063	61.063	0.06850214250907069
X	0.019294543526430313	34423	34.423	0.08245087404152644
X	0.019341628602962645	72082	72.082	0.06449935978096227
X	0.018884126571779864	13325	13.325	0.11232503475706695
X	0.019378047679318874	45520	45.52	0.07522622274328451
X	0.01922899738813741	2947	2.947	0.18686393995058917
X	0.019465896819714112	8325	8.325	0.1327282571176647
X	0.019406639588322432	203995	203.995	0.04565029683915214
X	0.019293711529890498	22431	22.431	0.09510186822536017
X	0.019399171998501788	104575	104.575	0.05703197878141708
X	0.018958280942974817	3196	3.196	0.18102173707715208
X	0.019322539775812737	8229	8.229	0.13291450632618293
X	0.019206468049501723	50783	50.783	0.07231744128375571
X	0.019246964785006468	76076	76.076	0.06324674712176648
X	0.019400707493095123	43462	43.462	0.0764250890519036
X	0.01904726730758722	6826	6.826	0.14078470146424674
X	0.01895717582615869	12011	12.011	0.11642998023254353
X	0.01936921780573457	45710	45.71	0.07511043672567023
X	0.019357990405599473	24600	24.6	0.09232267676007246
X	0.01923924114874845	119893	119.893	0.05434146848139579
X	0.019136584535400576	6978	6.978	0.1399730699059578
X	0.018750655538409187	8048	8.048	0.1325692591579736
X	0.019115458360475193	64590	64.59	0.06664073993699185
X	0.0188789805849284	15625	15.625	0.1065089666731764
X	0.019577109005960342	228776	228.776	0.04406692017791174
X	0.019663760011972314	17946	17.946	0.10309389955873188
X	0.01938355755471378	112896	112.896	0.05557997039527688
X	0.019397026346233567	165207	165.207	0.04896685787464469
X	0.019522488393988097	95971	95.971	0.058811878462780465
X	0.01975837854832906	164418	164.418	0.049347745189611145
X	0.019677171225940763	160313	160.313	0.04969712986085102
X	0.018992358220080708	4473	4.473	0.16192975729825942
X	0.019324835155157115	53439	53.439	0.07124468847112478
X	0.019330160803148137	20517	20.517	0.09803335583292773
X	0.01962447093922499	135319	135.319	0.05253874879216793
X	0.019370324500504733	133410	133.41	0.05255929171075901
X	0.019082191424136925	35800	35.8	0.08108024901721352
X	0.019413890902710403	75595	75.595	0.06356330997921063
X	0.019265899152214383	22482	22.482	0.09498421686963729
X	0.019226776884089743	35195	35.195	0.08174763033055946
X	0.019188572789617806	5970	5.97	0.14757839435373826
X	0.019649976295825863	15189	15.189	0.10896263980953047
X	0.01966990804324007	203906	203.906	0.04586247008234552
X	0.019778304689984535	571788	571.788	0.03258252037076292
X	0.01945177268077867	28589	28.589	0.08795355683638688
X	0.019364165297219998	19368	19.368	0.09999339984185669
X	0.01933346542040891	38264	38.264	0.07964767681497907
X	0.019542151005762936	167836	167.836	0.04883101460573377
X	0.0193562645686873	48657	48.657	0.07354594630657335
X	0.019339032627327567	11670	11.67	0.11833720205832006
X	0.0197778411626249	68682	68.682	0.06603567941813265
X	0.019420966694397888	190543	190.543	0.04671172960144098
X	0.019008953322233155	10140	10.14	0.12330293560038795
X	0.019347161693905247	59635	59.635	0.06871296345832036
X	0.019264088504339263	30379	30.379	0.08591289038127786
time for making epsilon is 3.0050017833709717
epsilons are
[0.22153729095599992, 0.08438868885950017, 0.17232863387309216, 0.09316999922804747, 0.07322447172619623, 0.0700488771153376, 0.06776211162432552, 0.06656487657769301, 0.10502795498898143, 0.08739244627826985, 0.11755541928848823, 0.05333153554982929, 0.11517606120379133, 0.03491076320988786, 0.08444568518738313, 0.07020916039798755, 0.06266777359204012, 0.08677354351829171, 0.04585605639692136, 0.05467575344858583, 0.06813262856925242, 0.04115907032411665, 0.10922826867628081, 0.06708355639886152, 0.15598230410019537, 0.059109303010207906, 0.059212347701460624, 0.09774462269997057, 0.05601824674322955, 0.02246684625420045, 0.09706525342637227, 0.02654536014109674, 0.0848942939508196, 0.10792790401891023, 0.08413699442198196, 0.04137054184260562, 0.05599160876907713, 0.2095915042581901, 0.15860042448534525, 0.15795879761005624, 0.15204873999858265, 0.18149852661103508, 0.2620666801889017, 0.14425954107644215, 0.21814968904544368, 0.21215851374687708, 0.17101670145918352, 0.13940159587245574, 0.1456243979703778, 0.09989701622551202, 0.10147464653343388, 0.21611355923511835, 0.12537932614614156, 0.17214464149167036, 0.12996158588322923, 0.11851866550707, 0.17850358412153614, 0.13326555770607176, 0.16251161336942296, 0.1595530554045845, 0.12151960995282655, 0.19961064219744443, 0.12386736378750138, 0.11041830638027698, 0.16604662139658002, 0.18545790940394283, 0.1574156112519497, 0.14653435371114082, 0.14333512019062253, 0.15066746995384758, 0.12710986886772419, 0.13899604421797732, 0.1674958225091542, 0.165839695140459, 0.14154880129058642, 0.10978161380934963, 0.2689499125861711, 0.21164320104740475, 0.1437733779342977, 0.0961738726756154, 0.15342664679779836, 0.18081828109158743, 0.173763543495884, 0.17488263792473763, 0.19206231707863125, 0.205127116989844, 0.1516741394731223, 0.22492225456273193, 0.15970656973905334, 0.14781131105532175, 0.12045977874318722, 0.1938512094611342, 0.17729694838110646, 0.13086046611816127, 0.14277078848817865, 0.15104513312526466, 0.16404984735188383, 0.12641288096583464, 0.11710678933045177, 0.14661318385135144, 0.10096531263308584, 0.1278400419391261, 0.13660416552080262, 0.1993720066247818, 0.13152400013950252, 0.1882072310667157, 0.137399441450614, 0.23485596571078488, 0.15017764568077133, 0.21145020402195555, 0.173974002126113, 0.16840697650106545, 0.17357677719268927, 0.16445872361583153, 0.13199095553397666, 0.15122647962044125, 0.15352032924459036, 0.21475151673991358, 0.12447405049204058, 0.22560604912424873, 0.10605184719949366, 0.17480024236389913, 0.18316846947420684, 0.15578144737207258, 0.17928623238548247, 0.17362308542221883, 0.21622013888331335, 0.1400118967930569, 0.11774494224191599, 0.1794900903657066, 0.12723888133781877, 0.1968866437250872, 0.14905375424680445, 0.2090135725648449, 0.166553210786617, 0.16195519826936028, 0.1774572146138869, 0.18938477130549836, 0.19303004764887402, 0.21347285916775033, 0.1652141758830156, 0.21239629146618347, 0.11820295941550116, 0.1015285428244676, 0.14191750108779125, 0.1697410781266119, 0.19146842448448964, 0.2178920395663351, 0.2064656241016124, 0.17237563626050095, 0.16731070218753727, 0.15700554172076886, 0.16419337881578616, 0.11314980912761051, 0.15515778441680442, 0.09187397836506572, 0.1636548167908778, 0.15148363510667165, 0.14941676985755656, 0.2838356409583999, 0.10312362203485329, 0.2120978117128499, 0.12522287034166307, 0.23300934257659503, 0.13639469630374865, 0.17317243226440027, 0.1899821516410476, 0.15538187122312577, 0.14717774609734724, 0.2593397170327757, 0.2597156975649256, 0.20886326788938445, 0.12104972232121, 0.19306363483989883, 0.14265606540883644, 0.11901313496712122, 0.14010057147534385, 0.17229918672309194, 0.1329052939925837, 0.1756574570460412, 0.11892835239729897, 0.17811309934591996, 0.14515808466004199, 0.19542863201380548, 0.15304435904325964, 0.13092629468150815, 0.18382994351080117, 0.1294286341426135, 0.13546876320670015, 0.11199091012985843, 0.168300293252029, 0.2548139395197433, 0.16590036751970016, 0.16118540156077785, 0.1269095736806862, 0.16239933744045965, 0.17697502147828, 0.19641946795978332, 0.14484370306881958, 0.23844392199450473, 0.1183460512869977, 0.16667256279160111, 0.10045999539440614, 0.16786412896990008, 0.13949943179442792, 0.21623457666331627, 0.15856994431846583, 0.153588018366016, 0.16569328029820288, 0.12839418951765164, 0.13579997290339613, 0.16627934432512245, 0.17084634714324587, 0.15265743258599584, 0.23358331455792622, 0.20964368458413485, 0.2595520395827636, 0.21868395495550294, 0.1027725729902419, 0.2042717924416636, 0.15832356798264766, 0.2255284650257014, 0.12997150600534502, 0.16647493607029246, 0.17963284258916942, 0.16068522553829084, 0.19659955728173392, 0.1853436052955285, 0.1499178796904444, 0.23376420375319135, 0.1914014593751937, 0.1564000606307773, 0.16955965802525674, 0.14099840663934748, 0.18714788611105307, 0.1639229254774436, 0.12528885009633328, 0.13856829609455984, 0.17295389286000598, 0.08940483762113507, 0.06697246271755396, 0.14073732668021954, 0.12535662638177855, 0.26316385150957783, 0.12531150372643315, 0.10901063406936175, 0.05003590480694235, 0.17384561073901764, 0.04605119420698558, 0.058851002596992004, 0.07492699495775101, 0.032973937806491285, 0.16704306073397351, 0.12405671890210809, 0.0423935751581381, 0.033558649374147785, 0.14946660662762784, 0.06793587693875487, 0.09359032629348149, 0.06874485504554487, 0.052657802667717, 0.06068692444996545, 0.08579402542577666, 0.09623084345101435, 0.17135406614706478, 0.050872595642983805, 0.11750803042325218, 0.16188781937709235, 0.08453431799295777, 0.05818128598619842, 0.04207670570191758, 0.05604480599949127, 0.17910345169305056, 0.08027251033373528, 0.116058222184174, 0.07468095939091664, 0.06424066899909857, 0.10891218910853814, 0.05995845078288509, 0.2696167863348599, 0.13102268272212134, 0.056602322228717104, 0.05983978274030956, 0.05970259315579902, 0.1415469516636366, 0.040010427565079366, 0.10089395606097769, 0.08019893376596643, 0.05731259014648964, 0.18694453616768034, 0.04946101780623339, 0.041667226806606464, 0.03295442786132373, 0.07064239580484344, 0.08135091626680546, 0.13182571223213158, 0.13794662296966934, 0.048998113137386196, 0.11760483978488441, 0.07978402556277706, 0.07368122741052503, 0.09223127489194637, 0.09607656122560775, 0.1422323977824828, 0.0518533259002611, 0.06500306593464072, 0.08527353582239286, 0.09729887962075846, 0.06281067359057163, 0.09171878766141339, 0.041390078496780905, 0.1310307533597863, 0.07816975021001299, 0.11685053111920973, 0.048487193970057775, 0.07218133108303014, 0.10366510934046236, 0.05178410851855475, 0.04494163836245367, 0.08209048915808512, 0.048194939341334224, 0.04426095495672481, 0.03420676510698815, 0.09338748577179636, 0.16619850834261843, 0.10201939703373189, 0.10980681007302048, 0.08839391130967748, 0.0651613018509402, 0.17229097116244155, 0.06006343656678782, 0.04618493053683395, 0.1079035882027231, 0.11727564529213842, 0.0728874069805782, 0.10979870409665095, 0.11054995951327758, 0.09661376558101702, 0.10334814625875688, 0.0733215068957896, 0.0724183828182066, 0.08435825173411346, 0.07337148317371785, 0.1249371229172734, 0.027038515392075416, 0.10731199035405731, 0.05202306109450864, 0.1827061932022286, 0.1457771433565232, 0.15434761393385835, 0.09801337568626517, 0.14993931787985873, 0.05416150527901589, 0.1484457239761007, 0.14174469622200717, 0.03980249868702119, 0.06624810941167052, 0.07295602594329698, 0.04963971782746576, 0.03678199512215735, 0.13669616248514022, 0.11213663678944125, 0.0906162470435797, 0.19768204404431108, 0.13566343640812725, 0.10045517471333315, 0.16351260256852992, 0.12512722387025543, 0.12051573755365379, 0.13980788673567227, 0.037352313092105155, 0.09974217430431435, 0.06032124478834463, 0.14891820295958166, 0.14533013073652434, 0.057720689795749666, 0.034298784578169955, 0.03241356921949691, 0.095381827501656, 0.07403406518178922, 0.17041585305637966, 0.10508413901847073, 0.03155791373308585, 0.09312232264552375, 0.11627615168498899, 0.07124188923012735, 0.02899078481439903, 0.07320228257077338, 0.0955015991302501, 0.10262620089069437, 0.07596070724469489, 0.14020611566272437, 0.044285061439755004, 0.09229733385224854, 0.18932494681081635, 0.05477476060524565, 0.08623138029057914, 0.20665209747720834, 0.06418739120291914, 0.05403341434323813, 0.03921863073760399, 0.04332487140149119, 0.043434889693990605, 0.08832317190456974, 0.08575647980330409, 0.054323220439980426, 0.04165037504649872, 0.11491164081492267, 0.04699489717066758, 0.038882653918785356, 0.04462873648493477, 0.05457986673468487, 0.06991148891556248, 0.10335964491064638, 0.1405276381178446, 0.09620691072506456, 0.04866717633250581, 0.05369038523091781, 0.039249920519365995, 0.05495792458247965, 0.05958230306872795, 0.10191996639338408, 0.04523900397041914, 0.13001728220723446, 0.10928414345531837, 0.048307249794878604, 0.0679915414243518, 0.14112368498513958, 0.09499712780942887, 0.031134243100632265, 0.08933528921868847, 0.050870154742107015, 0.10557371363260432, 0.1162955275942952, 0.14242466913767698, 0.13904652106071638, 0.08946764339860401, 0.0665031272521473, 0.13560833238111605, 0.04956722088595853, 0.13551294926131846, 0.059616409223816315, 0.10711655606833441, 0.05437016849892518, 0.13257667050876223, 0.08892503440059332, 0.07998482924321602, 0.08536902932793489, 0.06050236722822885, 0.033148235375907005, 0.11734850623163118, 0.13970039773581894, 0.05923198731291303, 0.08988597742109404, 0.032617205605469914, 0.16206619484720305, 0.04978766183088367, 0.03724515446660915, 0.1460162464246188, 0.04412539574660018, 0.09796162411468223, 0.03169447555392034, 0.11631663284446121, 0.0910908125984325, 0.11170827713843383, 0.15281026928680058, 0.11360546350005502, 0.06977361522387256, 0.14340174671441633, 0.04746505165922648, 0.0963343604261721, 0.13603239412839385, 0.09072317290816367, 0.042473800523896516, 0.05271809011993356, 0.037454524806011046, 0.09173094204525076, 0.1278766523583758, 0.1376117270232019, 0.15172131054957785, 0.04257491649711896, 0.1270799044101591, 0.08195919292948678, 0.08776026761878693, 0.0872182017005, 0.094074602929209, 0.11500136320335308, 0.04447867303730093, 0.07105550037507702, 0.11870247776851348, 0.07967466264249645, 0.05970192676058871, 0.05454995881518024, 0.034724735538252643, 0.054274403480100916, 0.133581493433218, 0.07582994895794419, 0.07352256304688111, 0.11395630859030424, 0.1067160504993956, 0.1424426534379674, 0.08719654664222191, 0.07971029497961002, 0.15746111529253637, 0.08853589704324127, 0.1319967585476842, 0.08475391962545366, 0.1338142027286431, 0.08372496848072687, 0.10310093564765581, 0.101674002574753, 0.13228356322736795, 0.1109259686752983, 0.08392258226841227, 0.04991609596779003, 0.12761206585400076, 0.18681215137685708, 0.12782645497703907, 0.053011430685108475, 0.031496052700379217, 0.05863633184396021, 0.13212548233730062, 0.044968207637981324, 0.0457847383832521, 0.17642033031511148, 0.1132563308510184, 0.0764968951940672, 0.05549255921375737, 0.043550495012216264, 0.04395999881980738, 0.09344292606878038, 0.14369649575999122, 0.17792271145192276, 0.06139533295997704, 0.10156669677976085, 0.08335337183480893, 0.05004944143119859, 0.0314086964124394, 0.11458526665447838, 0.1826974361606901, 0.08531650967079928, 0.051986008625330246, 0.07487665509969935, 0.13418008693678424, 0.15818871444206054, 0.08328002602421897, 0.15989465560548505, 0.07041629031488716, 0.13587877219302205, 0.09824232403636796, 0.10240001041897717, 0.12501750261201294, 0.12557213059951416, 0.04677029231410861, 0.03672691695538511, 0.05532614462585291, 0.05356383437202025, 0.0942901015783115, 0.10172678938221083, 0.08915736347053259, 0.04861667017931289, 0.07992691096143632, 0.07262031717722488, 0.10577657067045612, 0.023723308236506455, 0.09584538342138357, 0.06299429686919777, 0.04559926393657942, 0.06725929658859262, 0.07729769486755622, 0.028656455667553475, 0.050259683628206805, 0.05828489476450521, 0.12476460276876175, 0.07122411041353191, 0.11780519632651786, 0.04623440074285345, 0.023886324376220566, 0.05876480053926915, 0.04666497033720489, 0.05666901576170869, 0.08692487707886674, 0.0861234141366853, 0.1058506012437002, 0.046647622115302066, 0.03311738866724151, 0.056238373921078465, 0.027437345965732125, 0.047720013682947295, 0.07423120826791293, 0.12064054643702586, 0.056498658611835076, 0.04857413100049051, 0.12120015684399106, 0.030291318768841315, 0.09469966783269419, 0.07919192068860964, 0.14337341116742575, 0.11981658955002752, 0.17935676503955988, 0.13528563276105932, 0.09578834628290767, 0.0602165618215022, 0.01845965170397348, 0.10642469120323417, 0.045641317540672695, 0.12074398002485986, 0.08907916420824419, 0.12758087078263297, 0.0452843530606462, 0.06745878843493228, 0.02588143621675363, 0.130693608607248, 0.0470303906416758, 0.09193165681073721, 0.04716104501834005, 0.04208477018685179, 0.09363146126107379, 0.11255324225372532, 0.05287426710049409, 0.08436578115162885, 0.07028675238311768, 0.04271744986689711, 0.09352808644882059, 0.05981794950225393, 0.08493968645266661, 0.048845594459910185, 0.04086235122120763, 0.08130881877077897, 0.11259153003136563, 0.050127013928695285, 0.05560826196717367, 0.03602091119988806, 0.07494071009851516, 0.07768959042784906, 0.08554995262274048, 0.04598858261453284, 0.04896680528442531, 0.08257925609298097, 0.12782332000387012, 0.1209009845389333, 0.07040897919108302, 0.05586366283917297, 0.06020193132615389, 0.10571177196446177, 0.08535708471348642, 0.07106873486740292, 0.06850214250907069, 0.08245087404152644, 0.06449935978096227, 0.11232503475706695, 0.07522622274328451, 0.18686393995058917, 0.1327282571176647, 0.04565029683915214, 0.09510186822536017, 0.05703197878141708, 0.18102173707715208, 0.13291450632618293, 0.07231744128375571, 0.06324674712176648, 0.0764250890519036, 0.14078470146424674, 0.11642998023254353, 0.07511043672567023, 0.09232267676007246, 0.05434146848139579, 0.1399730699059578, 0.1325692591579736, 0.06664073993699185, 0.1065089666731764, 0.04406692017791174, 0.10309389955873188, 0.05557997039527688, 0.04896685787464469, 0.058811878462780465, 0.049347745189611145, 0.04969712986085102, 0.16192975729825942, 0.07124468847112478, 0.09803335583292773, 0.05253874879216793, 0.05255929171075901, 0.08108024901721352, 0.06356330997921063, 0.09498421686963729, 0.08174763033055946, 0.14757839435373826, 0.10896263980953047, 0.04586247008234552, 0.03258252037076292, 0.08795355683638688, 0.09999339984185669, 0.07964767681497907, 0.04883101460573377, 0.07354594630657335, 0.11833720205832006, 0.06603567941813265, 0.04671172960144098, 0.12330293560038795, 0.06871296345832036, 0.08591289038127786]
0.09130397638730774
Making ranges
torch.Size([45616, 2])
We keep 7.56e+06/6.77e+08 =  1% of the original kernel matrix.

torch.Size([4362, 2])
We keep 1.64e+05/2.99e+06 =  5% of the original kernel matrix.

torch.Size([15725, 2])
We keep 1.12e+06/4.50e+07 =  2% of the original kernel matrix.

torch.Size([52833, 2])
We keep 1.49e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([51192, 2])
We keep 9.96e+06/8.40e+08 =  1% of the original kernel matrix.

torch.Size([8812, 2])
We keep 4.98e+05/1.35e+07 =  3% of the original kernel matrix.

torch.Size([21029, 2])
We keep 1.89e+06/9.56e+07 =  1% of the original kernel matrix.

torch.Size([34608, 2])
We keep 1.42e+07/5.75e+08 =  2% of the original kernel matrix.

torch.Size([41384, 2])
We keep 7.92e+06/6.24e+08 =  1% of the original kernel matrix.

torch.Size([85202, 2])
We keep 3.18e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([63965, 2])
We keep 1.38e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([96561, 2])
We keep 3.47e+07/3.17e+09 =  1% of the original kernel matrix.

torch.Size([67055, 2])
We keep 1.57e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([106890, 2])
We keep 4.39e+07/3.97e+09 =  1% of the original kernel matrix.

torch.Size([70519, 2])
We keep 1.73e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([114309, 2])
We keep 5.83e+07/4.31e+09 =  1% of the original kernel matrix.

torch.Size([72826, 2])
We keep 1.77e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([28302, 2])
We keep 5.88e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([37524, 2])
We keep 5.87e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([42360, 2])
We keep 3.08e+07/8.38e+08 =  3% of the original kernel matrix.

torch.Size([44911, 2])
We keep 9.22e+06/7.53e+08 =  1% of the original kernel matrix.

torch.Size([21834, 2])
We keep 3.28e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([32649, 2])
We keep 4.60e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([197195, 2])
We keep 1.94e+08/1.62e+10 =  1% of the original kernel matrix.

torch.Size([97275, 2])
We keep 3.13e+07/3.31e+09 =  0% of the original kernel matrix.

torch.Size([24244, 2])
We keep 3.02e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([34503, 2])
We keep 4.76e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([802103, 2])
We keep 1.52e+09/2.08e+11 =  0% of the original kernel matrix.

torch.Size([204221, 2])
We keep 9.78e+07/1.19e+10 =  0% of the original kernel matrix.

torch.Size([51463, 2])
We keep 1.32e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([50154, 2])
We keep 9.88e+06/8.37e+08 =  1% of the original kernel matrix.

torch.Size([97036, 2])
We keep 3.91e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([67293, 2])
We keep 1.56e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([135797, 2])
We keep 7.06e+07/6.20e+09 =  1% of the original kernel matrix.

torch.Size([80131, 2])
We keep 2.09e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([47977, 2])
We keep 1.19e+07/8.69e+08 =  1% of the original kernel matrix.

torch.Size([48261, 2])
We keep 9.13e+06/7.67e+08 =  1% of the original kernel matrix.

torch.Size([316723, 2])
We keep 4.12e+08/4.05e+10 =  1% of the original kernel matrix.

torch.Size([127890, 2])
We keep 4.71e+07/5.23e+09 =  0% of the original kernel matrix.

torch.Size([199727, 2])
We keep 1.54e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([99216, 2])
We keep 2.93e+07/3.08e+09 =  0% of the original kernel matrix.

torch.Size([93717, 2])
We keep 6.28e+07/3.68e+09 =  1% of the original kernel matrix.

torch.Size([65600, 2])
We keep 1.69e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([474665, 2])
We keep 5.47e+08/7.94e+10 =  0% of the original kernel matrix.

torch.Size([156083, 2])
We keep 6.25e+07/7.33e+09 =  0% of the original kernel matrix.

torch.Size([27225, 2])
We keep 3.99e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([36750, 2])
We keep 5.30e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([100910, 2])
We keep 1.01e+08/4.10e+09 =  2% of the original kernel matrix.

torch.Size([69231, 2])
We keep 1.77e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([9522, 2])
We keep 1.92e+06/2.51e+07 =  7% of the original kernel matrix.

torch.Size([21476, 2])
We keep 2.12e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([160304, 2])
We keep 7.65e+07/8.82e+09 =  0% of the original kernel matrix.

torch.Size([87888, 2])
We keep 2.41e+07/2.44e+09 =  0% of the original kernel matrix.

torch.Size([156021, 2])
We keep 9.26e+07/8.80e+09 =  1% of the original kernel matrix.

torch.Size([86464, 2])
We keep 2.43e+07/2.44e+09 =  0% of the original kernel matrix.

torch.Size([35924, 2])
We keep 7.83e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([42723, 2])
We keep 6.84e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([128873, 2])
We keep 3.14e+08/1.18e+10 =  2% of the original kernel matrix.

torch.Size([78092, 2])
We keep 2.73e+07/2.83e+09 =  0% of the original kernel matrix.

torch.Size([3480998, 2])
We keep 1.42e+10/3.05e+12 =  0% of the original kernel matrix.

torch.Size([443683, 2])
We keep 3.35e+08/4.54e+10 =  0% of the original kernel matrix.

torch.Size([33846, 2])
We keep 8.90e+06/4.37e+08 =  2% of the original kernel matrix.

torch.Size([40780, 2])
We keep 6.91e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([1913516, 2])
We keep 6.53e+09/1.11e+12 =  0% of the original kernel matrix.

torch.Size([324570, 2])
We keep 2.12e+08/2.75e+10 =  0% of the original kernel matrix.

torch.Size([49586, 2])
We keep 2.32e+07/9.99e+08 =  2% of the original kernel matrix.

torch.Size([49177, 2])
We keep 9.83e+06/8.23e+08 =  1% of the original kernel matrix.

torch.Size([27516, 2])
We keep 5.08e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([36853, 2])
We keep 5.48e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([51177, 2])
We keep 1.55e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([50155, 2])
We keep 1.01e+07/8.43e+08 =  1% of the original kernel matrix.

torch.Size([433805, 2])
We keep 9.11e+08/7.66e+10 =  1% of the original kernel matrix.

torch.Size([148299, 2])
We keep 6.24e+07/7.20e+09 =  0% of the original kernel matrix.

torch.Size([187200, 2])
We keep 1.44e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([96160, 2])
We keep 2.85e+07/2.93e+09 =  0% of the original kernel matrix.

torch.Size([5236, 2])
We keep 1.94e+05/4.19e+06 =  4% of the original kernel matrix.

torch.Size([17125, 2])
We keep 1.27e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([11239, 2])
We keep 7.80e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([23517, 2])
We keep 2.29e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([10867, 2])
We keep 7.81e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([22863, 2])
We keep 2.27e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([12257, 2])
We keep 8.48e+05/2.91e+07 =  2% of the original kernel matrix.

torch.Size([24288, 2])
We keep 2.48e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([5120, 2])
We keep 6.80e+05/9.78e+06 =  6% of the original kernel matrix.

torch.Size([15268, 2])
We keep 1.72e+06/8.14e+07 =  2% of the original kernel matrix.

torch.Size([2857, 2])
We keep 6.40e+04/1.04e+06 =  6% of the original kernel matrix.

torch.Size([13624, 2])
We keep 7.96e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([13391, 2])
We keep 1.31e+06/4.14e+07 =  3% of the original kernel matrix.

torch.Size([25141, 2])
We keep 2.89e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([4486, 2])
We keep 1.80e+05/3.37e+06 =  5% of the original kernel matrix.

torch.Size([16056, 2])
We keep 1.17e+06/4.78e+07 =  2% of the original kernel matrix.

torch.Size([5232, 2])
We keep 1.78e+05/3.94e+06 =  4% of the original kernel matrix.

torch.Size([17172, 2])
We keep 1.22e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([8725, 2])
We keep 5.28e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([20947, 2])
We keep 1.96e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([15274, 2])
We keep 1.27e+06/4.89e+07 =  2% of the original kernel matrix.

torch.Size([27024, 2])
We keep 3.00e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([12946, 2])
We keep 1.25e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([24878, 2])
We keep 2.79e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([31452, 2])
We keep 9.31e+06/3.62e+08 =  2% of the original kernel matrix.

torch.Size([39651, 2])
We keep 6.57e+06/4.95e+08 =  1% of the original kernel matrix.

torch.Size([31309, 2])
We keep 5.72e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([39665, 2])
We keep 6.45e+06/4.86e+08 =  1% of the original kernel matrix.

torch.Size([4449, 2])
We keep 1.77e+05/3.46e+06 =  5% of the original kernel matrix.

torch.Size([15947, 2])
We keep 1.18e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([17859, 2])
We keep 3.81e+06/9.37e+07 =  4% of the original kernel matrix.

torch.Size([29261, 2])
We keep 3.91e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([8820, 2])
We keep 5.19e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([21096, 2])
We keep 1.94e+06/9.74e+07 =  1% of the original kernel matrix.

torch.Size([18504, 2])
We keep 1.98e+06/7.81e+07 =  2% of the original kernel matrix.

torch.Size([29768, 2])
We keep 3.62e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([19466, 2])
We keep 4.57e+06/1.34e+08 =  3% of the original kernel matrix.

torch.Size([30556, 2])
We keep 4.49e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([7663, 2])
We keep 6.41e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([19847, 2])
We keep 1.84e+06/9.03e+07 =  2% of the original kernel matrix.

torch.Size([17336, 2])
We keep 1.51e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([28761, 2])
We keep 3.32e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([9881, 2])
We keep 7.65e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([22089, 2])
We keep 2.18e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([10240, 2])
We keep 8.38e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([22333, 2])
We keep 2.24e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([21511, 2])
We keep 2.36e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([32460, 2])
We keep 4.19e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([5861, 2])
We keep 2.96e+05/5.87e+06 =  5% of the original kernel matrix.

torch.Size([17948, 2])
We keep 1.42e+06/6.30e+07 =  2% of the original kernel matrix.

torch.Size([18828, 2])
We keep 2.78e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([30191, 2])
We keep 4.04e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([26030, 2])
We keep 3.51e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([35866, 2])
We keep 5.19e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([9454, 2])
We keep 5.64e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([21623, 2])
We keep 2.09e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([6847, 2])
We keep 3.51e+05/8.74e+06 =  4% of the original kernel matrix.

torch.Size([18874, 2])
We keep 1.63e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([10895, 2])
We keep 8.91e+05/2.42e+07 =  3% of the original kernel matrix.

torch.Size([23113, 2])
We keep 2.35e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([13344, 2])
We keep 9.71e+05/3.46e+07 =  2% of the original kernel matrix.

torch.Size([25093, 2])
We keep 2.65e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([14098, 2])
We keep 1.71e+06/4.19e+07 =  4% of the original kernel matrix.

torch.Size([26034, 2])
We keep 2.88e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([11809, 2])
We keep 9.44e+05/3.00e+07 =  3% of the original kernel matrix.

torch.Size([23828, 2])
We keep 2.52e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([16804, 2])
We keep 2.58e+06/8.77e+07 =  2% of the original kernel matrix.

torch.Size([28285, 2])
We keep 3.78e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([14693, 2])
We keep 1.54e+06/5.00e+07 =  3% of the original kernel matrix.

torch.Size([26464, 2])
We keep 3.07e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([9578, 2])
We keep 5.96e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([21922, 2])
We keep 2.07e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([8937, 2])
We keep 6.50e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([20962, 2])
We keep 2.07e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([13393, 2])
We keep 1.20e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([25080, 2])
We keep 2.86e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([25489, 2])
We keep 4.37e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([35447, 2])
We keep 5.30e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([2737, 2])
We keep 5.73e+04/9.70e+05 =  5% of the original kernel matrix.

torch.Size([13423, 2])
We keep 7.81e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([4705, 2])
We keep 1.91e+05/3.81e+06 =  5% of the original kernel matrix.

torch.Size([16372, 2])
We keep 1.22e+06/5.08e+07 =  2% of the original kernel matrix.

torch.Size([13313, 2])
We keep 2.23e+06/4.12e+07 =  5% of the original kernel matrix.

torch.Size([25359, 2])
We keep 2.80e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([36938, 2])
We keep 6.85e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([43015, 2])
We keep 7.13e+06/5.55e+08 =  1% of the original kernel matrix.

torch.Size([11683, 2])
We keep 1.02e+06/2.85e+07 =  3% of the original kernel matrix.

torch.Size([23768, 2])
We keep 2.49e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([7214, 2])
We keep 4.43e+05/9.99e+06 =  4% of the original kernel matrix.

torch.Size([19078, 2])
We keep 1.71e+06/8.23e+07 =  2% of the original kernel matrix.

torch.Size([8039, 2])
We keep 6.74e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([19994, 2])
We keep 1.91e+06/9.51e+07 =  2% of the original kernel matrix.

torch.Size([8425, 2])
We keep 4.59e+05/1.22e+07 =  3% of the original kernel matrix.

torch.Size([20637, 2])
We keep 1.83e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([6304, 2])
We keep 3.08e+05/7.23e+06 =  4% of the original kernel matrix.

torch.Size([18219, 2])
We keep 1.53e+06/7.00e+07 =  2% of the original kernel matrix.

torch.Size([5443, 2])
We keep 2.33e+05/4.69e+06 =  4% of the original kernel matrix.

torch.Size([17345, 2])
We keep 1.31e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([12195, 2])
We keep 1.11e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([24335, 2])
We keep 2.60e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([4250, 2])
We keep 1.36e+05/2.76e+06 =  4% of the original kernel matrix.

torch.Size([15766, 2])
We keep 1.10e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([10604, 2])
We keep 7.03e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([22693, 2])
We keep 2.26e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([12802, 2])
We keep 1.21e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([24713, 2])
We keep 2.67e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([21747, 2])
We keep 2.98e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([32485, 2])
We keep 4.21e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([5887, 2])
We keep 4.29e+05/6.78e+06 =  6% of the original kernel matrix.

torch.Size([17840, 2])
We keep 1.49e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([7803, 2])
We keep 4.66e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([19855, 2])
We keep 1.78e+06/8.74e+07 =  2% of the original kernel matrix.

torch.Size([16812, 2])
We keep 2.13e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([28356, 2])
We keep 3.60e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([13818, 2])
We keep 1.24e+06/4.09e+07 =  3% of the original kernel matrix.

torch.Size([25678, 2])
We keep 2.83e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([12206, 2])
We keep 9.09e+05/3.05e+07 =  2% of the original kernel matrix.

torch.Size([24281, 2])
We keep 2.54e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([10015, 2])
We keep 6.70e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([22180, 2])
We keep 2.17e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([16140, 2])
We keep 2.61e+06/8.63e+07 =  3% of the original kernel matrix.

torch.Size([27390, 2])
We keep 3.75e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([22832, 2])
We keep 2.98e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([33314, 2])
We keep 4.60e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([13330, 2])
We keep 1.03e+06/3.65e+07 =  2% of the original kernel matrix.

torch.Size([25170, 2])
We keep 2.70e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([32077, 2])
We keep 5.85e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([40072, 2])
We keep 6.39e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([18863, 2])
We keep 2.00e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([30074, 2])
We keep 3.74e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([16102, 2])
We keep 1.62e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([27906, 2])
We keep 3.18e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([5383, 2])
We keep 2.65e+05/5.48e+06 =  4% of the original kernel matrix.

torch.Size([16850, 2])
We keep 1.40e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([16126, 2])
We keep 1.82e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([27587, 2])
We keep 3.43e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([6524, 2])
We keep 4.91e+05/7.96e+06 =  6% of the original kernel matrix.

torch.Size([18566, 2])
We keep 1.58e+06/7.34e+07 =  2% of the original kernel matrix.

torch.Size([15832, 2])
We keep 1.46e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([27463, 2])
We keep 3.12e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([3510, 2])
We keep 1.22e+05/1.99e+06 =  6% of the original kernel matrix.

torch.Size([14424, 2])
We keep 9.91e+05/3.67e+07 =  2% of the original kernel matrix.

torch.Size([11994, 2])
We keep 1.14e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([24117, 2])
We keep 2.60e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([4637, 2])
We keep 2.26e+05/4.04e+06 =  5% of the original kernel matrix.

torch.Size([16239, 2])
We keep 1.26e+06/5.23e+07 =  2% of the original kernel matrix.

torch.Size([8532, 2])
We keep 4.86e+05/1.30e+07 =  3% of the original kernel matrix.

torch.Size([20893, 2])
We keep 1.88e+06/9.38e+07 =  2% of the original kernel matrix.

torch.Size([8054, 2])
We keep 8.78e+05/1.65e+07 =  5% of the original kernel matrix.

torch.Size([19932, 2])
We keep 2.09e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([7754, 2])
We keep 5.06e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([19679, 2])
We keep 1.86e+06/9.26e+07 =  2% of the original kernel matrix.

torch.Size([9388, 2])
We keep 6.84e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([21555, 2])
We keep 2.07e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([16626, 2])
We keep 1.82e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([28140, 2])
We keep 3.41e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([10474, 2])
We keep 1.64e+06/2.94e+07 =  5% of the original kernel matrix.

torch.Size([22253, 2])
We keep 2.54e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([11344, 2])
We keep 1.19e+06/2.82e+07 =  4% of the original kernel matrix.

torch.Size([23569, 2])
We keep 2.47e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([4719, 2])
We keep 2.86e+05/3.59e+06 =  7% of the original kernel matrix.

torch.Size([16368, 2])
We keep 1.20e+06/4.93e+07 =  2% of the original kernel matrix.

torch.Size([18897, 2])
We keep 2.54e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([30175, 2])
We keep 4.07e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([3917, 2])
We keep 1.47e+05/2.77e+06 =  5% of the original kernel matrix.

torch.Size([15114, 2])
We keep 1.11e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([29140, 2])
We keep 4.40e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([38242, 2])
We keep 5.73e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([8097, 2])
We keep 5.33e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([20384, 2])
We keep 1.86e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([6839, 2])
We keep 3.89e+05/9.44e+06 =  4% of the original kernel matrix.

torch.Size([18720, 2])
We keep 1.69e+06/7.99e+07 =  2% of the original kernel matrix.

torch.Size([10989, 2])
We keep 8.88e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([23092, 2])
We keep 2.42e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([7686, 2])
We keep 4.31e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([19841, 2])
We keep 1.75e+06/8.54e+07 =  2% of the original kernel matrix.

torch.Size([8593, 2])
We keep 4.72e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([20853, 2])
We keep 1.87e+06/9.33e+07 =  2% of the original kernel matrix.

torch.Size([4699, 2])
We keep 1.76e+05/3.49e+06 =  5% of the original kernel matrix.

torch.Size([16352, 2])
We keep 1.19e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([14981, 2])
We keep 2.22e+06/4.92e+07 =  4% of the original kernel matrix.

torch.Size([26802, 2])
We keep 3.06e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([22450, 2])
We keep 3.06e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([33090, 2])
We keep 4.48e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([7771, 2])
We keep 4.30e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([19943, 2])
We keep 1.73e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([18481, 2])
We keep 1.88e+06/8.70e+07 =  2% of the original kernel matrix.

torch.Size([29643, 2])
We keep 3.75e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([5910, 2])
We keep 2.90e+05/6.21e+06 =  4% of the original kernel matrix.

torch.Size([17843, 2])
We keep 1.45e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([12540, 2])
We keep 1.06e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([24636, 2])
We keep 2.65e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([4761, 2])
We keep 2.41e+05/4.24e+06 =  5% of the original kernel matrix.

torch.Size([16000, 2])
We keep 1.28e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([9292, 2])
We keep 6.70e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([21528, 2])
We keep 2.06e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([10423, 2])
We keep 7.50e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([22721, 2])
We keep 2.26e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([7305, 2])
We keep 5.72e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([19310, 2])
We keep 1.82e+06/8.91e+07 =  2% of the original kernel matrix.

torch.Size([6918, 2])
We keep 3.10e+05/7.84e+06 =  3% of the original kernel matrix.

torch.Size([19147, 2])
We keep 1.58e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([6125, 2])
We keep 3.15e+05/6.87e+06 =  4% of the original kernel matrix.

torch.Size([18083, 2])
We keep 1.52e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([4844, 2])
We keep 1.67e+05/3.72e+06 =  4% of the original kernel matrix.

torch.Size([16490, 2])
We keep 1.21e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([9186, 2])
We keep 6.56e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([21292, 2])
We keep 2.13e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([4908, 2])
We keep 2.09e+05/3.85e+06 =  5% of the original kernel matrix.

torch.Size([16548, 2])
We keep 1.23e+06/5.11e+07 =  2% of the original kernel matrix.

torch.Size([21351, 2])
We keep 2.96e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([32155, 2])
We keep 4.50e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([30855, 2])
We keep 5.52e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([39313, 2])
We keep 6.34e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([13995, 2])
We keep 1.46e+06/4.63e+07 =  3% of the original kernel matrix.

torch.Size([25937, 2])
We keep 3.00e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([9115, 2])
We keep 5.17e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([21300, 2])
We keep 1.95e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([6406, 2])
We keep 4.04e+05/7.27e+06 =  5% of the original kernel matrix.

torch.Size([18524, 2])
We keep 1.54e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([4342, 2])
We keep 1.79e+05/3.30e+06 =  5% of the original kernel matrix.

torch.Size([15687, 2])
We keep 1.16e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([5174, 2])
We keep 2.24e+05/4.69e+06 =  4% of the original kernel matrix.

torch.Size([17019, 2])
We keep 1.33e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([7801, 2])
We keep 6.18e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([19832, 2])
We keep 1.90e+06/9.63e+07 =  1% of the original kernel matrix.

torch.Size([8657, 2])
We keep 8.47e+05/1.64e+07 =  5% of the original kernel matrix.

torch.Size([20611, 2])
We keep 2.01e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([11052, 2])
We keep 8.13e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([23205, 2])
We keep 2.35e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([10080, 2])
We keep 6.73e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([22356, 2])
We keep 2.17e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([19940, 2])
We keep 4.76e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([30759, 2])
We keep 4.95e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([10521, 2])
We keep 9.27e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([22454, 2])
We keep 2.41e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([38365, 2])
We keep 1.04e+07/5.99e+08 =  1% of the original kernel matrix.

torch.Size([42197, 2])
We keep 7.66e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([10301, 2])
We keep 6.06e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([22555, 2])
We keep 2.11e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([11936, 2])
We keep 1.12e+06/3.08e+07 =  3% of the original kernel matrix.

torch.Size([23964, 2])
We keep 2.58e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([12756, 2])
We keep 1.05e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([24767, 2])
We keep 2.60e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([2331, 2])
We keep 5.29e+04/6.56e+05 =  8% of the original kernel matrix.

torch.Size([12575, 2])
We keep 6.83e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([30908, 2])
We keep 5.88e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([39236, 2])
We keep 6.19e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([5069, 2])
We keep 1.86e+05/3.91e+06 =  4% of the original kernel matrix.

torch.Size([16896, 2])
We keep 1.24e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([17667, 2])
We keep 3.63e+06/9.37e+07 =  3% of the original kernel matrix.

torch.Size([29174, 2])
We keep 3.87e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([3675, 2])
We keep 1.47e+05/2.31e+06 =  6% of the original kernel matrix.

torch.Size([14830, 2])
We keep 1.06e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([14725, 2])
We keep 1.74e+06/5.65e+07 =  3% of the original kernel matrix.

torch.Size([26493, 2])
We keep 3.19e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([8796, 2])
We keep 5.04e+05/1.35e+07 =  3% of the original kernel matrix.

torch.Size([20962, 2])
We keep 1.90e+06/9.55e+07 =  1% of the original kernel matrix.

torch.Size([6977, 2])
We keep 3.21e+05/7.82e+06 =  4% of the original kernel matrix.

torch.Size([19300, 2])
We keep 1.57e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([10898, 2])
We keep 9.18e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([22787, 2])
We keep 2.38e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([13284, 2])
We keep 1.03e+06/3.59e+07 =  2% of the original kernel matrix.

torch.Size([25151, 2])
We keep 2.72e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([2898, 2])
We keep 7.32e+04/1.13e+06 =  6% of the original kernel matrix.

torch.Size([13751, 2])
We keep 8.28e+05/2.77e+07 =  2% of the original kernel matrix.

torch.Size([2710, 2])
We keep 8.21e+04/1.14e+06 =  7% of the original kernel matrix.

torch.Size([13244, 2])
We keep 8.30e+05/2.77e+07 =  2% of the original kernel matrix.

torch.Size([4991, 2])
We keep 2.20e+05/4.16e+06 =  5% of the original kernel matrix.

torch.Size([16535, 2])
We keep 1.26e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([20664, 2])
We keep 2.88e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([31613, 2])
We keep 4.24e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([6364, 2])
We keep 2.96e+05/6.97e+06 =  4% of the original kernel matrix.

torch.Size([18382, 2])
We keep 1.50e+06/6.87e+07 =  2% of the original kernel matrix.

torch.Size([14218, 2])
We keep 1.25e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([26089, 2])
We keep 2.91e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([16018, 2])
We keep 3.10e+07/1.30e+08 = 23% of the original kernel matrix.

torch.Size([27316, 2])
We keep 4.49e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([14341, 2])
We keep 1.48e+06/4.77e+07 =  3% of the original kernel matrix.

torch.Size([26188, 2])
We keep 3.01e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([8603, 2])
We keep 5.24e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([20771, 2])
We keep 1.94e+06/9.79e+07 =  1% of the original kernel matrix.

torch.Size([16240, 2])
We keep 1.93e+06/6.57e+07 =  2% of the original kernel matrix.

torch.Size([27853, 2])
We keep 3.37e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([8050, 2])
We keep 5.20e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([20340, 2])
We keep 1.85e+06/9.19e+07 =  2% of the original kernel matrix.

torch.Size([22172, 2])
We keep 2.60e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([32912, 2])
We keep 4.41e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([7332, 2])
We keep 5.41e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([19277, 2])
We keep 1.79e+06/8.67e+07 =  2% of the original kernel matrix.

torch.Size([12161, 2])
We keep 1.44e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([23982, 2])
We keep 2.78e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([6358, 2])
We keep 2.62e+05/6.37e+06 =  4% of the original kernel matrix.

torch.Size([18550, 2])
We keep 1.47e+06/6.57e+07 =  2% of the original kernel matrix.

torch.Size([12292, 2])
We keep 9.11e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([24393, 2])
We keep 2.52e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([17507, 2])
We keep 1.85e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([28936, 2])
We keep 3.52e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([7283, 2])
We keep 3.49e+05/9.21e+06 =  3% of the original kernel matrix.

torch.Size([19563, 2])
We keep 1.66e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([16848, 2])
We keep 2.28e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([28328, 2])
We keep 3.66e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([16316, 2])
We keep 1.70e+06/6.22e+07 =  2% of the original kernel matrix.

torch.Size([28087, 2])
We keep 3.34e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([25276, 2])
We keep 3.46e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([35206, 2])
We keep 5.01e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([9397, 2])
We keep 7.35e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([21671, 2])
We keep 2.05e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([3001, 2])
We keep 7.79e+04/1.30e+06 =  6% of the original kernel matrix.

torch.Size([13722, 2])
We keep 8.50e+05/2.96e+07 =  2% of the original kernel matrix.

torch.Size([9829, 2])
We keep 6.50e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([22099, 2])
We keep 2.07e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([9591, 2])
We keep 1.32e+06/2.12e+07 =  6% of the original kernel matrix.

torch.Size([21786, 2])
We keep 2.26e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([16324, 2])
We keep 3.94e+06/8.48e+07 =  4% of the original kernel matrix.

torch.Size([27869, 2])
We keep 3.75e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([9592, 2])
We keep 9.40e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([21646, 2])
We keep 2.17e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([7759, 2])
We keep 4.74e+05/1.21e+07 =  3% of the original kernel matrix.

torch.Size([19968, 2])
We keep 1.84e+06/9.04e+07 =  2% of the original kernel matrix.

torch.Size([6482, 2])
We keep 2.52e+05/6.22e+06 =  4% of the original kernel matrix.

torch.Size([18632, 2])
We keep 1.43e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([12945, 2])
We keep 1.63e+06/3.84e+07 =  4% of the original kernel matrix.

torch.Size([24952, 2])
We keep 2.78e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([3590, 2])
We keep 1.08e+05/2.02e+06 =  5% of the original kernel matrix.

torch.Size([14839, 2])
We keep 9.98e+05/3.70e+07 =  2% of the original kernel matrix.

torch.Size([22974, 2])
We keep 2.95e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([33540, 2])
We keep 4.49e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([9711, 2])
We keep 6.24e+05/1.70e+07 =  3% of the original kernel matrix.

torch.Size([21762, 2])
We keep 2.06e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([33668, 2])
We keep 5.95e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([41477, 2])
We keep 6.65e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([9429, 2])
We keep 6.23e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([21650, 2])
We keep 2.07e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([15047, 2])
We keep 1.29e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([26797, 2])
We keep 3.01e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([4567, 2])
We keep 1.83e+05/3.50e+06 =  5% of the original kernel matrix.

torch.Size([16244, 2])
We keep 1.20e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([10286, 2])
We keep 1.02e+06/2.39e+07 =  4% of the original kernel matrix.

torch.Size([22360, 2])
We keep 2.36e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([11995, 2])
We keep 9.18e+05/2.93e+07 =  3% of the original kernel matrix.

torch.Size([24073, 2])
We keep 2.53e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([9433, 2])
We keep 7.18e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([21693, 2])
We keep 2.13e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([18430, 2])
We keep 2.16e+06/8.18e+07 =  2% of the original kernel matrix.

torch.Size([29788, 2])
We keep 3.68e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([14916, 2])
We keep 1.97e+06/5.84e+07 =  3% of the original kernel matrix.

torch.Size([26565, 2])
We keep 3.27e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([9103, 2])
We keep 1.29e+06/1.75e+07 =  7% of the original kernel matrix.

torch.Size([21314, 2])
We keep 2.10e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([8602, 2])
We keep 5.83e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([20676, 2])
We keep 1.94e+06/9.88e+07 =  1% of the original kernel matrix.

torch.Size([12224, 2])
We keep 8.83e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([24303, 2])
We keep 2.50e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([3575, 2])
We keep 1.38e+05/2.16e+06 =  6% of the original kernel matrix.

torch.Size([14569, 2])
We keep 1.02e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([4665, 2])
We keep 2.37e+05/4.06e+06 =  5% of the original kernel matrix.

torch.Size([16119, 2])
We keep 1.25e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([2889, 2])
We keep 6.92e+04/1.11e+06 =  6% of the original kernel matrix.

torch.Size([13550, 2])
We keep 8.13e+05/2.74e+07 =  2% of the original kernel matrix.

torch.Size([4556, 2])
We keep 1.63e+05/3.29e+06 =  4% of the original kernel matrix.

torch.Size([16270, 2])
We keep 1.17e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([26927, 2])
We keep 7.50e+06/3.04e+08 =  2% of the original kernel matrix.

torch.Size([35935, 2])
We keep 6.10e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([5484, 2])
We keep 2.70e+05/4.91e+06 =  5% of the original kernel matrix.

torch.Size([17473, 2])
We keep 1.34e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([9839, 2])
We keep 8.45e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([21767, 2])
We keep 2.29e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([4071, 2])
We keep 1.35e+05/2.83e+06 =  4% of the original kernel matrix.

torch.Size([15490, 2])
We keep 1.09e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([17593, 2])
We keep 1.85e+06/7.33e+07 =  2% of the original kernel matrix.

torch.Size([28994, 2])
We keep 3.53e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([8038, 2])
We keep 1.33e+06/1.73e+07 =  7% of the original kernel matrix.

torch.Size([19941, 2])
We keep 2.10e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([7167, 2])
We keep 4.60e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([19030, 2])
We keep 1.76e+06/8.55e+07 =  2% of the original kernel matrix.

torch.Size([9498, 2])
We keep 9.73e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([21469, 2])
We keep 2.23e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([6315, 2])
We keep 2.61e+05/6.13e+06 =  4% of the original kernel matrix.

torch.Size([18399, 2])
We keep 1.44e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([6758, 2])
We keep 4.62e+05/8.76e+06 =  5% of the original kernel matrix.

torch.Size([18615, 2])
We keep 1.63e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([12010, 2])
We keep 1.08e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([23923, 2])
We keep 2.63e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([3752, 2])
We keep 1.29e+05/2.27e+06 =  5% of the original kernel matrix.

torch.Size([14936, 2])
We keep 1.04e+06/3.92e+07 =  2% of the original kernel matrix.

torch.Size([6079, 2])
We keep 4.13e+05/7.40e+06 =  5% of the original kernel matrix.

torch.Size([17753, 2])
We keep 1.56e+06/7.08e+07 =  2% of the original kernel matrix.

torch.Size([10363, 2])
We keep 1.34e+06/2.59e+07 =  5% of the original kernel matrix.

torch.Size([22498, 2])
We keep 2.40e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([8522, 2])
We keep 6.55e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([20638, 2])
We keep 2.01e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([14468, 2])
We keep 1.48e+06/4.80e+07 =  3% of the original kernel matrix.

torch.Size([26497, 2])
We keep 3.03e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([6848, 2])
We keep 3.38e+05/8.01e+06 =  4% of the original kernel matrix.

torch.Size([19011, 2])
We keep 1.58e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([8892, 2])
We keep 1.22e+06/1.82e+07 =  6% of the original kernel matrix.

torch.Size([20938, 2])
We keep 2.14e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([19598, 2])
We keep 2.27e+06/9.86e+07 =  2% of the original kernel matrix.

torch.Size([30684, 2])
We keep 3.97e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([15487, 2])
We keep 1.66e+06/5.39e+07 =  3% of the original kernel matrix.

torch.Size([27404, 2])
We keep 3.15e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([8457, 2])
We keep 7.65e+05/1.37e+07 =  5% of the original kernel matrix.

torch.Size([20839, 2])
We keep 1.94e+06/9.65e+07 =  2% of the original kernel matrix.

torch.Size([41924, 2])
We keep 1.62e+07/7.55e+08 =  2% of the original kernel matrix.

torch.Size([45303, 2])
We keep 8.84e+06/7.15e+08 =  1% of the original kernel matrix.

torch.Size([78470, 2])
We keep 9.30e+07/4.14e+09 =  2% of the original kernel matrix.

torch.Size([59268, 2])
We keep 1.76e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([14133, 2])
We keep 1.97e+06/5.01e+07 =  3% of the original kernel matrix.

torch.Size([26098, 2])
We keep 2.99e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([18799, 2])
We keep 2.85e+06/9.86e+07 =  2% of the original kernel matrix.

torch.Size([30233, 2])
We keep 3.98e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([2609, 2])
We keep 7.22e+04/1.00e+06 =  7% of the original kernel matrix.

torch.Size([12949, 2])
We keep 7.69e+05/2.61e+07 =  2% of the original kernel matrix.

torch.Size([19134, 2])
We keep 2.99e+06/9.54e+07 =  3% of the original kernel matrix.

torch.Size([30393, 2])
We keep 3.87e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([26634, 2])
We keep 1.01e+07/2.21e+08 =  4% of the original kernel matrix.

torch.Size([35991, 2])
We keep 5.18e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([197463, 2])
We keep 3.70e+08/2.39e+10 =  1% of the original kernel matrix.

torch.Size([96765, 2])
We keep 3.74e+07/4.02e+09 =  0% of the original kernel matrix.

torch.Size([8698, 2])
We keep 4.73e+05/1.35e+07 =  3% of the original kernel matrix.

torch.Size([21091, 2])
We keep 1.91e+06/9.56e+07 =  1% of the original kernel matrix.

torch.Size([301249, 2])
We keep 3.29e+08/3.88e+10 =  0% of the original kernel matrix.

torch.Size([122489, 2])
We keep 4.58e+07/5.12e+09 =  0% of the original kernel matrix.

torch.Size([108743, 2])
We keep 4.52e+08/8.67e+09 =  5% of the original kernel matrix.

torch.Size([71173, 2])
We keep 2.32e+07/2.42e+09 =  0% of the original kernel matrix.

torch.Size([69611, 2])
We keep 4.41e+07/2.11e+09 =  2% of the original kernel matrix.

torch.Size([57013, 2])
We keep 1.30e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([360315, 2])
We keep 2.19e+10/3.01e+11 =  7% of the original kernel matrix.

torch.Size([130872, 2])
We keep 1.11e+08/1.43e+10 =  0% of the original kernel matrix.

torch.Size([9490, 2])
We keep 8.88e+05/1.70e+07 =  5% of the original kernel matrix.

torch.Size([21828, 2])
We keep 2.08e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([19637, 2])
We keep 2.46e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([30849, 2])
We keep 4.02e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([397679, 2])
We keep 6.13e+08/6.46e+10 =  0% of the original kernel matrix.

torch.Size([141809, 2])
We keep 5.77e+07/6.62e+09 =  0% of the original kernel matrix.

torch.Size([895088, 2])
We keep 1.77e+09/2.64e+11 =  0% of the original kernel matrix.

torch.Size([218554, 2])
We keep 1.09e+08/1.34e+10 =  0% of the original kernel matrix.

torch.Size([12808, 2])
We keep 1.10e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([24777, 2])
We keep 2.66e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([105909, 2])
We keep 5.30e+07/3.78e+09 =  1% of the original kernel matrix.

torch.Size([69992, 2])
We keep 1.70e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([35809, 2])
We keep 2.06e+07/5.55e+08 =  3% of the original kernel matrix.

torch.Size([41865, 2])
We keep 7.74e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([98307, 2])
We keep 1.80e+08/3.67e+09 =  4% of the original kernel matrix.

torch.Size([68159, 2])
We keep 1.64e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([169812, 2])
We keep 2.59e+08/1.76e+10 =  1% of the original kernel matrix.

torch.Size([89740, 2])
We keep 3.29e+07/3.45e+09 =  0% of the original kernel matrix.

torch.Size([86372, 2])
We keep 2.08e+08/7.14e+09 =  2% of the original kernel matrix.

torch.Size([58822, 2])
We keep 2.24e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([50969, 2])
We keep 1.61e+07/9.41e+08 =  1% of the original kernel matrix.

torch.Size([50256, 2])
We keep 9.60e+06/7.98e+08 =  1% of the original kernel matrix.

torch.Size([36883, 2])
We keep 8.96e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([42850, 2])
We keep 7.22e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([8063, 2])
We keep 7.18e+05/1.40e+07 =  5% of the original kernel matrix.

torch.Size([20145, 2])
We keep 1.93e+06/9.74e+07 =  1% of the original kernel matrix.

torch.Size([178401, 2])
We keep 3.18e+08/2.17e+10 =  1% of the original kernel matrix.

torch.Size([90823, 2])
We keep 3.54e+07/3.83e+09 =  0% of the original kernel matrix.

torch.Size([21402, 2])
We keep 6.97e+06/1.39e+08 =  5% of the original kernel matrix.

torch.Size([32048, 2])
We keep 4.53e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([10260, 2])
We keep 6.31e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([22571, 2])
We keep 2.18e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([50695, 2])
We keep 1.93e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([49921, 2])
We keep 9.84e+06/8.38e+08 =  1% of the original kernel matrix.

torch.Size([134202, 2])
We keep 1.53e+08/9.80e+09 =  1% of the original kernel matrix.

torch.Size([78157, 2])
We keep 2.57e+07/2.58e+09 =  0% of the original kernel matrix.

torch.Size([434174, 2])
We keep 5.46e+08/7.02e+10 =  0% of the original kernel matrix.

torch.Size([150239, 2])
We keep 5.97e+07/6.89e+09 =  0% of the original kernel matrix.

torch.Size([185320, 2])
We keep 1.14e+08/1.24e+10 =  0% of the original kernel matrix.

torch.Size([95214, 2])
We keep 2.81e+07/2.90e+09 =  0% of the original kernel matrix.

torch.Size([7667, 2])
We keep 4.81e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([19849, 2])
We keep 1.75e+06/8.55e+07 =  2% of the original kernel matrix.

torch.Size([56109, 2])
We keep 3.11e+07/1.40e+09 =  2% of the original kernel matrix.

torch.Size([51795, 2])
We keep 1.13e+07/9.73e+08 =  1% of the original kernel matrix.

torch.Size([22394, 2])
We keep 3.61e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([33008, 2])
We keep 4.71e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([69781, 2])
We keep 3.51e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([57455, 2])
We keep 1.35e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([122473, 2])
We keep 8.46e+07/5.36e+09 =  1% of the original kernel matrix.

torch.Size([76638, 2])
We keep 1.95e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([24624, 2])
We keep 9.42e+06/2.23e+08 =  4% of the original kernel matrix.

torch.Size([34438, 2])
We keep 5.44e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([150316, 2])
We keep 8.66e+07/8.19e+09 =  1% of the original kernel matrix.

torch.Size([84680, 2])
We keep 2.35e+07/2.36e+09 =  0% of the original kernel matrix.

torch.Size([2573, 2])
We keep 5.46e+04/8.70e+05 =  6% of the original kernel matrix.

torch.Size([13078, 2])
We keep 7.46e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([17035, 2])
We keep 1.99e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([28475, 2])
We keep 3.49e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([151249, 2])
We keep 1.35e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([83967, 2])
We keep 2.72e+07/2.78e+09 =  0% of the original kernel matrix.

torch.Size([121119, 2])
We keep 1.93e+08/7.98e+09 =  2% of the original kernel matrix.

torch.Size([74904, 2])
We keep 2.30e+07/2.33e+09 =  0% of the original kernel matrix.

torch.Size([128290, 2])
We keep 1.10e+08/7.92e+09 =  1% of the original kernel matrix.

torch.Size([76767, 2])
We keep 2.31e+07/2.32e+09 =  0% of the original kernel matrix.

torch.Size([13955, 2])
We keep 1.44e+06/4.65e+07 =  3% of the original kernel matrix.

torch.Size([25819, 2])
We keep 2.98e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([448212, 2])
We keep 6.22e+08/9.50e+10 =  0% of the original kernel matrix.

torch.Size([149954, 2])
We keep 6.78e+07/8.02e+09 =  0% of the original kernel matrix.

torch.Size([33635, 2])
We keep 5.43e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([41133, 2])
We keep 6.47e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([61086, 2])
We keep 2.03e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([54363, 2])
We keep 1.12e+07/9.77e+08 =  1% of the original kernel matrix.

torch.Size([161987, 2])
We keep 1.12e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([88441, 2])
We keep 2.63e+07/2.68e+09 =  0% of the original kernel matrix.

torch.Size([7172, 2])
We keep 3.48e+05/8.54e+06 =  4% of the original kernel matrix.

torch.Size([19404, 2])
We keep 1.62e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([251166, 2])
We keep 3.22e+08/2.57e+10 =  1% of the original kernel matrix.

torch.Size([111321, 2])
We keep 3.85e+07/4.17e+09 =  0% of the original kernel matrix.

torch.Size([399736, 2])
We keep 1.17e+09/7.47e+10 =  1% of the original kernel matrix.

torch.Size([142520, 2])
We keep 6.23e+07/7.11e+09 =  0% of the original kernel matrix.

torch.Size([905917, 2])
We keep 3.06e+09/3.06e+11 =  1% of the original kernel matrix.

torch.Size([219290, 2])
We keep 1.16e+08/1.44e+10 =  0% of the original kernel matrix.

torch.Size([87295, 2])
We keep 4.08e+07/2.99e+09 =  1% of the original kernel matrix.

torch.Size([63460, 2])
We keep 1.55e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([56406, 2])
We keep 1.88e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([52119, 2])
We keep 1.06e+07/9.08e+08 =  1% of the original kernel matrix.

torch.Size([15141, 2])
We keep 2.34e+06/6.91e+07 =  3% of the original kernel matrix.

torch.Size([26811, 2])
We keep 3.46e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([15751, 2])
We keep 1.33e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([27308, 2])
We keep 3.15e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([226440, 2])
We keep 3.24e+08/2.78e+10 =  1% of the original kernel matrix.

torch.Size([105265, 2])
We keep 4.01e+07/4.34e+09 =  0% of the original kernel matrix.

torch.Size([22276, 2])
We keep 3.71e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([32912, 2])
We keep 4.53e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([55428, 2])
We keep 2.93e+07/1.45e+09 =  2% of the original kernel matrix.

torch.Size([51527, 2])
We keep 1.14e+07/9.90e+08 =  1% of the original kernel matrix.

torch.Size([72295, 2])
We keep 7.53e+07/2.35e+09 =  3% of the original kernel matrix.

torch.Size([58801, 2])
We keep 1.31e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([40869, 2])
We keep 1.88e+07/6.10e+08 =  3% of the original kernel matrix.

torch.Size([45203, 2])
We keep 7.94e+06/6.43e+08 =  1% of the original kernel matrix.

torch.Size([31202, 2])
We keep 1.44e+07/4.58e+08 =  3% of the original kernel matrix.

torch.Size([38382, 2])
We keep 7.22e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([14507, 2])
We keep 1.26e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([26374, 2])
We keep 2.91e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([113509, 2])
We keep 3.76e+08/1.93e+10 =  1% of the original kernel matrix.

torch.Size([69278, 2])
We keep 3.41e+07/3.62e+09 =  0% of the original kernel matrix.

torch.Size([89694, 2])
We keep 6.60e+08/4.96e+09 = 13% of the original kernel matrix.

torch.Size([64872, 2])
We keep 1.93e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([41335, 2])
We keep 5.84e+07/9.69e+08 =  6% of the original kernel matrix.

torch.Size([43973, 2])
We keep 9.77e+06/8.10e+08 =  1% of the original kernel matrix.

torch.Size([35897, 2])
We keep 7.58e+06/4.56e+08 =  1% of the original kernel matrix.

torch.Size([42885, 2])
We keep 7.18e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([112546, 2])
We keep 1.14e+08/6.05e+09 =  1% of the original kernel matrix.

torch.Size([73197, 2])
We keep 2.00e+07/2.02e+09 =  0% of the original kernel matrix.

torch.Size([42559, 2])
We keep 9.15e+06/6.17e+08 =  1% of the original kernel matrix.

torch.Size([44182, 2])
We keep 7.61e+06/6.46e+08 =  1% of the original kernel matrix.

torch.Size([377279, 2])
We keep 1.10e+09/7.77e+10 =  1% of the original kernel matrix.

torch.Size([135268, 2])
We keep 6.31e+07/7.25e+09 =  0% of the original kernel matrix.

torch.Size([15289, 2])
We keep 2.22e+06/7.25e+07 =  3% of the original kernel matrix.

torch.Size([27069, 2])
We keep 3.48e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([48890, 2])
We keep 3.09e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([46667, 2])
We keep 1.17e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([22914, 2])
We keep 3.13e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([33388, 2])
We keep 4.52e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([199544, 2])
We keep 5.32e+08/2.88e+10 =  1% of the original kernel matrix.

torch.Size([95722, 2])
We keep 4.05e+07/4.42e+09 =  0% of the original kernel matrix.

torch.Size([73998, 2])
We keep 2.94e+07/2.71e+09 =  1% of the original kernel matrix.

torch.Size([59398, 2])
We keep 1.46e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([30770, 2])
We keep 6.00e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([39197, 2])
We keep 6.12e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([229056, 2])
We keep 1.48e+08/1.94e+10 =  0% of the original kernel matrix.

torch.Size([106406, 2])
We keep 3.38e+07/3.62e+09 =  0% of the original kernel matrix.

torch.Size([355267, 2])
We keep 3.69e+08/4.70e+10 =  0% of the original kernel matrix.

torch.Size([136189, 2])
We keep 5.02e+07/5.64e+09 =  0% of the original kernel matrix.

torch.Size([39822, 2])
We keep 1.72e+08/1.22e+09 = 14% of the original kernel matrix.

torch.Size([42817, 2])
We keep 1.03e+07/9.11e+08 =  1% of the original kernel matrix.

torch.Size([271626, 2])
We keep 5.31e+08/3.11e+10 =  1% of the original kernel matrix.

torch.Size([117262, 2])
We keep 4.19e+07/4.59e+09 =  0% of the original kernel matrix.

torch.Size([362243, 2])
We keep 3.63e+08/5.19e+10 =  0% of the original kernel matrix.

torch.Size([136594, 2])
We keep 5.23e+07/5.93e+09 =  0% of the original kernel matrix.

torch.Size([854532, 2])
We keep 1.54e+09/2.45e+11 =  0% of the original kernel matrix.

torch.Size([213436, 2])
We keep 1.05e+08/1.29e+10 =  0% of the original kernel matrix.

torch.Size([38848, 2])
We keep 1.75e+07/5.67e+08 =  3% of the original kernel matrix.

torch.Size([43649, 2])
We keep 7.76e+06/6.20e+08 =  1% of the original kernel matrix.

torch.Size([9125, 2])
We keep 6.39e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([21181, 2])
We keep 2.04e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([31093, 2])
We keep 5.45e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([39458, 2])
We keep 6.27e+06/4.73e+08 =  1% of the original kernel matrix.

torch.Size([21828, 2])
We keep 7.37e+06/2.14e+08 =  3% of the original kernel matrix.

torch.Size([32259, 2])
We keep 5.21e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([46489, 2])
We keep 1.09e+07/7.81e+08 =  1% of the original kernel matrix.

torch.Size([48013, 2])
We keep 8.80e+06/7.27e+08 =  1% of the original kernel matrix.

torch.Size([113755, 2])
We keep 1.09e+08/5.02e+09 =  2% of the original kernel matrix.

torch.Size([72908, 2])
We keep 1.93e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([8914, 2])
We keep 5.07e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([21239, 2])
We keep 1.94e+06/9.85e+07 =  1% of the original kernel matrix.

torch.Size([152185, 2])
We keep 1.04e+08/7.97e+09 =  1% of the original kernel matrix.

torch.Size([85340, 2])
We keep 2.33e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([316571, 2])
We keep 3.06e+08/4.00e+10 =  0% of the original kernel matrix.

torch.Size([127735, 2])
We keep 4.67e+07/5.21e+09 =  0% of the original kernel matrix.

torch.Size([13956, 2])
We keep 3.05e+07/2.26e+08 = 13% of the original kernel matrix.

torch.Size([25011, 2])
We keep 4.71e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([21990, 2])
We keep 3.37e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([32682, 2])
We keep 4.58e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([81407, 2])
We keep 3.35e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([62043, 2])
We keep 1.41e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([26662, 2])
We keep 4.51e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([36301, 2])
We keep 5.23e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([25423, 2])
We keep 5.50e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([35263, 2])
We keep 5.16e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([36363, 2])
We keep 6.79e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([42666, 2])
We keep 7.13e+06/5.58e+08 =  1% of the original kernel matrix.

torch.Size([26912, 2])
We keep 3.93e+07/3.07e+08 = 12% of the original kernel matrix.

torch.Size([36113, 2])
We keep 5.88e+06/4.56e+08 =  1% of the original kernel matrix.

torch.Size([66097, 2])
We keep 1.15e+08/2.32e+09 =  4% of the original kernel matrix.

torch.Size([54753, 2])
We keep 1.31e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([87168, 2])
We keep 2.90e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([64499, 2])
We keep 1.44e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([52935, 2])
We keep 1.75e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([51301, 2])
We keep 1.00e+07/8.50e+08 =  1% of the original kernel matrix.

torch.Size([72178, 2])
We keep 4.89e+07/2.47e+09 =  1% of the original kernel matrix.

torch.Size([58155, 2])
We keep 1.42e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([17884, 2])
We keep 2.68e+06/9.77e+07 =  2% of the original kernel matrix.

torch.Size([29195, 2])
We keep 3.97e+06/2.57e+08 =  1% of the original kernel matrix.

torch.Size([1854600, 2])
We keep 9.58e+09/1.00e+12 =  0% of the original kernel matrix.

torch.Size([319302, 2])
We keep 2.02e+08/2.60e+10 =  0% of the original kernel matrix.

torch.Size([27773, 2])
We keep 4.16e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([37096, 2])
We keep 5.57e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([222375, 2])
We keep 1.70e+08/1.94e+10 =  0% of the original kernel matrix.

torch.Size([104767, 2])
We keep 3.40e+07/3.62e+09 =  0% of the original kernel matrix.

torch.Size([7631, 2])
We keep 3.67e+05/9.73e+06 =  3% of the original kernel matrix.

torch.Size([20082, 2])
We keep 1.72e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([13489, 2])
We keep 1.23e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([25333, 2])
We keep 2.79e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([11861, 2])
We keep 8.20e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([23962, 2])
We keep 2.41e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([36393, 2])
We keep 6.96e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([42981, 2])
We keep 6.87e+06/5.35e+08 =  1% of the original kernel matrix.

torch.Size([12158, 2])
We keep 1.04e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([24209, 2])
We keep 2.57e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([179686, 2])
We keep 2.96e+08/1.53e+10 =  1% of the original kernel matrix.

torch.Size([93202, 2])
We keep 3.10e+07/3.22e+09 =  0% of the original kernel matrix.

torch.Size([13245, 2])
We keep 9.94e+05/3.44e+07 =  2% of the original kernel matrix.

torch.Size([25159, 2])
We keep 2.67e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([14095, 2])
We keep 1.31e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([25762, 2])
We keep 2.95e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([506308, 2])
We keep 8.03e+08/9.80e+10 =  0% of the original kernel matrix.

torch.Size([159576, 2])
We keep 7.00e+07/8.15e+09 =  0% of the original kernel matrix.

torch.Size([107322, 2])
We keep 6.26e+07/4.45e+09 =  1% of the original kernel matrix.

torch.Size([70619, 2])
We keep 1.82e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([79725, 2])
We keep 3.51e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([61199, 2])
We keep 1.41e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([256031, 2])
We keep 2.44e+08/2.62e+10 =  0% of the original kernel matrix.

torch.Size([113354, 2])
We keep 3.90e+07/4.21e+09 =  0% of the original kernel matrix.

torch.Size([692776, 2])
We keep 1.05e+09/1.58e+11 =  0% of the original kernel matrix.

torch.Size([188312, 2])
We keep 8.68e+07/1.04e+10 =  0% of the original kernel matrix.

torch.Size([15962, 2])
We keep 1.34e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([27636, 2])
We keep 3.16e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([25619, 2])
We keep 3.21e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([35418, 2])
We keep 4.92e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([37344, 2])
We keep 1.80e+07/6.58e+08 =  2% of the original kernel matrix.

torch.Size([41928, 2])
We keep 8.01e+06/6.67e+08 =  1% of the original kernel matrix.

torch.Size([5677, 2])
We keep 2.76e+05/6.11e+06 =  4% of the original kernel matrix.

torch.Size([17425, 2])
We keep 1.45e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([16141, 2])
We keep 1.56e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([27749, 2])
We keep 3.30e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([32851, 2])
We keep 6.50e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([40484, 2])
We keep 6.53e+06/4.95e+08 =  1% of the original kernel matrix.

torch.Size([9749, 2])
We keep 6.48e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([21942, 2])
We keep 2.12e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([19579, 2])
We keep 2.30e+06/9.43e+07 =  2% of the original kernel matrix.

torch.Size([30759, 2])
We keep 3.88e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([21317, 2])
We keep 3.19e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([32036, 2])
We keep 4.16e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([14602, 2])
We keep 1.43e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([26447, 2])
We keep 3.00e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([617227, 2])
We keep 1.25e+09/1.42e+11 =  0% of the original kernel matrix.

torch.Size([177209, 2])
We keep 8.13e+07/9.81e+09 =  0% of the original kernel matrix.

torch.Size([25078, 2])
We keep 8.46e+06/3.65e+08 =  2% of the original kernel matrix.

torch.Size([34263, 2])
We keep 6.57e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([143134, 2])
We keep 1.08e+08/8.08e+09 =  1% of the original kernel matrix.

torch.Size([83012, 2])
We keep 2.35e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([11802, 2])
We keep 1.30e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([24049, 2])
We keep 2.59e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([12670, 2])
We keep 1.23e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([24474, 2])
We keep 2.77e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([156755, 2])
We keep 1.15e+08/1.00e+10 =  1% of the original kernel matrix.

torch.Size([86484, 2])
We keep 2.57e+07/2.61e+09 =  0% of the original kernel matrix.

torch.Size([804650, 2])
We keep 3.00e+09/2.32e+11 =  1% of the original kernel matrix.

torch.Size([205838, 2])
We keep 9.97e+07/1.25e+10 =  0% of the original kernel matrix.

torch.Size([994786, 2])
We keep 1.84e+09/3.25e+11 =  0% of the original kernel matrix.

torch.Size([231603, 2])
We keep 1.18e+08/1.48e+10 =  0% of the original kernel matrix.

torch.Size([38218, 2])
We keep 8.71e+06/4.95e+08 =  1% of the original kernel matrix.

torch.Size([43982, 2])
We keep 7.25e+06/5.79e+08 =  1% of the original kernel matrix.

torch.Size([80089, 2])
We keep 3.21e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([61992, 2])
We keep 1.37e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([9066, 2])
We keep 5.48e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([21130, 2])
We keep 1.97e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([25064, 2])
We keep 3.08e+07/2.76e+08 = 11% of the original kernel matrix.

torch.Size([34616, 2])
We keep 5.42e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([782712, 2])
We keep 6.72e+09/3.81e+11 =  1% of the original kernel matrix.

torch.Size([198248, 2])
We keep 1.28e+08/1.61e+10 =  0% of the original kernel matrix.

torch.Size([41013, 2])
We keep 1.55e+07/5.81e+08 =  2% of the original kernel matrix.

torch.Size([45666, 2])
We keep 7.60e+06/6.28e+08 =  1% of the original kernel matrix.

torch.Size([22617, 2])
We keep 2.88e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([33120, 2])
We keep 4.59e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([81008, 2])
We keep 4.29e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([61692, 2])
We keep 1.49e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([1257921, 2])
We keep 4.78e+09/6.58e+11 =  0% of the original kernel matrix.

torch.Size([254862, 2])
We keep 1.67e+08/2.11e+10 =  0% of the original kernel matrix.

torch.Size([75117, 2])
We keep 3.74e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([58839, 2])
We keep 1.37e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([34927, 2])
We keep 1.31e+07/4.99e+08 =  2% of the original kernel matrix.

torch.Size([41547, 2])
We keep 7.40e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([28095, 2])
We keep 8.22e+06/3.32e+08 =  2% of the original kernel matrix.

torch.Size([37232, 2])
We keep 6.35e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([75382, 2])
We keep 2.33e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([60358, 2])
We keep 1.28e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([14961, 2])
We keep 1.86e+06/4.89e+07 =  3% of the original kernel matrix.

torch.Size([26835, 2])
We keep 3.03e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([357581, 2])
We keep 4.57e+08/4.98e+10 =  0% of the original kernel matrix.

torch.Size([135853, 2])
We keep 5.14e+07/5.81e+09 =  0% of the original kernel matrix.

torch.Size([38919, 2])
We keep 1.26e+07/5.97e+08 =  2% of the original kernel matrix.

torch.Size([44230, 2])
We keep 8.04e+06/6.36e+08 =  1% of the original kernel matrix.

torch.Size([6780, 2])
We keep 3.60e+05/7.64e+06 =  4% of the original kernel matrix.

torch.Size([18884, 2])
We keep 1.57e+06/7.19e+07 =  2% of the original kernel matrix.

torch.Size([196456, 2])
We keep 1.80e+08/1.39e+10 =  1% of the original kernel matrix.

torch.Size([98736, 2])
We keep 2.92e+07/3.07e+09 =  0% of the original kernel matrix.

torch.Size([51734, 2])
We keep 1.29e+07/9.49e+08 =  1% of the original kernel matrix.

torch.Size([51460, 2])
We keep 9.60e+06/8.02e+08 =  1% of the original kernel matrix.

torch.Size([4961, 2])
We keep 2.39e+05/4.67e+06 =  5% of the original kernel matrix.

torch.Size([16519, 2])
We keep 1.33e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([119694, 2])
We keep 7.67e+07/5.54e+09 =  1% of the original kernel matrix.

torch.Size([74433, 2])
We keep 1.98e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([202825, 2])
We keep 1.47e+08/1.52e+10 =  0% of the original kernel matrix.

torch.Size([99934, 2])
We keep 3.07e+07/3.21e+09 =  0% of the original kernel matrix.

torch.Size([525395, 2])
We keep 1.28e+09/1.06e+11 =  1% of the original kernel matrix.

torch.Size([163633, 2])
We keep 7.26e+07/8.48e+09 =  0% of the original kernel matrix.

torch.Size([364816, 2])
We keep 7.97e+08/5.87e+10 =  1% of the original kernel matrix.

torch.Size([135869, 2])
We keep 5.60e+07/6.30e+09 =  0% of the original kernel matrix.

torch.Size([354567, 2])
We keep 5.52e+08/5.50e+10 =  1% of the original kernel matrix.

torch.Size([133672, 2])
We keep 5.42e+07/6.10e+09 =  0% of the original kernel matrix.

torch.Size([41940, 2])
We keep 1.59e+07/7.53e+08 =  2% of the original kernel matrix.

torch.Size([45293, 2])
We keep 8.86e+06/7.14e+08 =  1% of the original kernel matrix.

torch.Size([48888, 2])
We keep 2.34e+07/9.17e+08 =  2% of the original kernel matrix.

torch.Size([48886, 2])
We keep 9.38e+06/7.88e+08 =  1% of the original kernel matrix.

torch.Size([202826, 2])
We keep 1.29e+08/1.46e+10 =  0% of the original kernel matrix.

torch.Size([100112, 2])
We keep 3.01e+07/3.15e+09 =  0% of the original kernel matrix.

torch.Size([456449, 2])
We keep 5.82e+08/7.32e+10 =  0% of the original kernel matrix.

torch.Size([153676, 2])
We keep 6.06e+07/7.04e+09 =  0% of the original kernel matrix.

torch.Size([23154, 2])
We keep 4.18e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([33636, 2])
We keep 4.74e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([192044, 2])
We keep 2.73e+09/3.50e+10 =  7% of the original kernel matrix.

torch.Size([97976, 2])
We keep 4.38e+07/4.87e+09 =  0% of the original kernel matrix.

torch.Size([490993, 2])
We keep 6.66e+09/1.13e+11 =  5% of the original kernel matrix.

torch.Size([157402, 2])
We keep 7.49e+07/8.75e+09 =  0% of the original kernel matrix.

torch.Size([318807, 2])
We keep 5.82e+08/4.76e+10 =  1% of the original kernel matrix.

torch.Size([127177, 2])
We keep 5.07e+07/5.68e+09 =  0% of the original kernel matrix.

torch.Size([171351, 2])
We keep 1.83e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([89814, 2])
We keep 2.99e+07/3.10e+09 =  0% of the original kernel matrix.

torch.Size([70966, 2])
We keep 9.07e+07/3.30e+09 =  2% of the original kernel matrix.

torch.Size([56391, 2])
We keep 1.61e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([28517, 2])
We keep 7.04e+06/3.05e+08 =  2% of the original kernel matrix.

torch.Size([37096, 2])
We keep 6.11e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([15037, 2])
We keep 1.23e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([26779, 2])
We keep 3.01e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([32237, 2])
We keep 1.44e+07/4.49e+08 =  3% of the original kernel matrix.

torch.Size([39079, 2])
We keep 7.00e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([281070, 2])
We keep 2.01e+08/2.83e+10 =  0% of the original kernel matrix.

torch.Size([119319, 2])
We keep 3.99e+07/4.38e+09 =  0% of the original kernel matrix.

torch.Size([208168, 2])
We keep 1.31e+08/1.57e+10 =  0% of the original kernel matrix.

torch.Size([101460, 2])
We keep 3.09e+07/3.26e+09 =  0% of the original kernel matrix.

torch.Size([455878, 2])
We keep 1.03e+09/1.01e+11 =  1% of the original kernel matrix.

torch.Size([150222, 2])
We keep 7.08e+07/8.27e+09 =  0% of the original kernel matrix.

torch.Size([189545, 2])
We keep 1.77e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([96587, 2])
We keep 2.96e+07/3.10e+09 =  0% of the original kernel matrix.

torch.Size([146012, 2])
We keep 1.11e+08/8.32e+09 =  1% of the original kernel matrix.

torch.Size([83719, 2])
We keep 2.35e+07/2.37e+09 =  0% of the original kernel matrix.

torch.Size([28361, 2])
We keep 5.56e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([36874, 2])
We keep 6.24e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([337666, 2])
We keep 3.84e+08/4.39e+10 =  0% of the original kernel matrix.

torch.Size([131540, 2])
We keep 4.83e+07/5.45e+09 =  0% of the original kernel matrix.

torch.Size([14670, 2])
We keep 2.67e+06/7.32e+07 =  3% of the original kernel matrix.

torch.Size([26308, 2])
We keep 3.56e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([26310, 2])
We keep 4.43e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([35967, 2])
We keep 5.40e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([280303, 2])
We keep 2.57e+08/3.05e+10 =  0% of the original kernel matrix.

torch.Size([119493, 2])
We keep 4.14e+07/4.55e+09 =  0% of the original kernel matrix.

torch.Size([101931, 2])
We keep 5.81e+07/3.79e+09 =  1% of the original kernel matrix.

torch.Size([68787, 2])
We keep 1.69e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([13484, 2])
We keep 1.54e+06/4.66e+07 =  3% of the original kernel matrix.

torch.Size([25546, 2])
We keep 2.99e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([37818, 2])
We keep 7.86e+06/5.25e+08 =  1% of the original kernel matrix.

torch.Size([43197, 2])
We keep 7.57e+06/5.96e+08 =  1% of the original kernel matrix.

torch.Size([922330, 2])
We keep 4.58e+09/4.28e+11 =  1% of the original kernel matrix.

torch.Size([215479, 2])
We keep 1.37e+08/1.70e+10 =  0% of the original kernel matrix.

torch.Size([43730, 2])
We keep 1.48e+07/7.30e+08 =  2% of the original kernel matrix.

torch.Size([46248, 2])
We keep 8.53e+06/7.03e+08 =  1% of the original kernel matrix.

torch.Size([238033, 2])
We keep 2.77e+08/2.26e+10 =  1% of the original kernel matrix.

torch.Size([109331, 2])
We keep 3.65e+07/3.92e+09 =  0% of the original kernel matrix.

torch.Size([20628, 2])
We keep 4.00e+07/2.56e+08 = 15% of the original kernel matrix.

torch.Size([31034, 2])
We keep 5.43e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([21090, 2])
We keep 1.20e+07/1.45e+08 =  8% of the original kernel matrix.

torch.Size([31915, 2])
We keep 4.37e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([13521, 2])
We keep 1.32e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([25219, 2])
We keep 2.96e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([13290, 2])
We keep 1.76e+06/4.89e+07 =  3% of the original kernel matrix.

torch.Size([25095, 2])
We keep 2.98e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([45608, 2])
We keep 1.05e+07/7.50e+08 =  1% of the original kernel matrix.

torch.Size([48053, 2])
We keep 8.63e+06/7.13e+08 =  1% of the original kernel matrix.

torch.Size([103019, 2])
We keep 6.65e+07/4.26e+09 =  1% of the original kernel matrix.

torch.Size([68385, 2])
We keep 1.80e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([15392, 2])
We keep 3.23e+06/6.00e+07 =  5% of the original kernel matrix.

torch.Size([27236, 2])
We keep 3.34e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([254526, 2])
We keep 2.69e+08/2.53e+10 =  1% of the original kernel matrix.

torch.Size([112759, 2])
We keep 3.82e+07/4.14e+09 =  0% of the original kernel matrix.

torch.Size([14873, 2])
We keep 1.91e+06/5.98e+07 =  3% of the original kernel matrix.

torch.Size([26638, 2])
We keep 3.25e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([156966, 2])
We keep 1.61e+08/8.60e+09 =  1% of the original kernel matrix.

torch.Size([87231, 2])
We keep 2.41e+07/2.41e+09 =  0% of the original kernel matrix.

torch.Size([26221, 2])
We keep 5.56e+06/2.39e+08 =  2% of the original kernel matrix.

torch.Size([35594, 2])
We keep 5.55e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([196126, 2])
We keep 1.41e+08/1.43e+10 =  0% of the original kernel matrix.

torch.Size([97656, 2])
We keep 2.98e+07/3.11e+09 =  0% of the original kernel matrix.

torch.Size([15246, 2])
We keep 2.46e+06/6.65e+07 =  3% of the original kernel matrix.

torch.Size([26854, 2])
We keep 3.41e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([45572, 2])
We keep 1.52e+07/7.55e+08 =  2% of the original kernel matrix.

torch.Size([47534, 2])
We keep 8.56e+06/7.15e+08 =  1% of the original kernel matrix.

torch.Size([61361, 2])
We keep 2.07e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([54777, 2])
We keep 1.13e+07/9.82e+08 =  1% of the original kernel matrix.

torch.Size([46922, 2])
We keep 2.11e+07/9.77e+08 =  2% of the original kernel matrix.

torch.Size([47905, 2])
We keep 9.73e+06/8.14e+08 =  1% of the original kernel matrix.

torch.Size([139049, 2])
We keep 9.49e+07/7.93e+09 =  1% of the original kernel matrix.

torch.Size([81949, 2])
We keep 2.33e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([845453, 2])
We keep 2.63e+09/2.83e+11 =  0% of the original kernel matrix.

torch.Size([210254, 2])
We keep 1.12e+08/1.38e+10 =  0% of the original kernel matrix.

torch.Size([23265, 2])
We keep 2.60e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([33745, 2])
We keep 4.47e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([13879, 2])
We keep 1.98e+06/4.79e+07 =  4% of the original kernel matrix.

torch.Size([25960, 2])
We keep 2.97e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([141975, 2])
We keep 4.01e+08/8.95e+09 =  4% of the original kernel matrix.

torch.Size([82542, 2])
We keep 2.37e+07/2.46e+09 =  0% of the original kernel matrix.

torch.Size([43567, 2])
We keep 1.20e+07/6.81e+08 =  1% of the original kernel matrix.

torch.Size([47193, 2])
We keep 8.61e+06/6.79e+08 =  1% of the original kernel matrix.

torch.Size([987138, 2])
We keep 1.72e+09/3.22e+11 =  0% of the original kernel matrix.

torch.Size([231637, 2])
We keep 1.18e+08/1.48e+10 =  0% of the original kernel matrix.

torch.Size([9258, 2])
We keep 1.00e+06/1.96e+07 =  5% of the original kernel matrix.

torch.Size([21280, 2])
We keep 2.16e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([243700, 2])
We keep 2.29e+08/2.56e+10 =  0% of the original kernel matrix.

torch.Size([109276, 2])
We keep 3.86e+07/4.17e+09 =  0% of the original kernel matrix.

torch.Size([665671, 2])
We keep 8.66e+08/1.47e+11 =  0% of the original kernel matrix.

torch.Size([183606, 2])
We keep 8.28e+07/9.97e+09 =  0% of the original kernel matrix.

torch.Size([13028, 2])
We keep 1.22e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([24957, 2])
We keep 2.80e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([302340, 2])
We keep 5.68e+08/4.90e+10 =  1% of the original kernel matrix.

torch.Size([121179, 2])
We keep 5.08e+07/5.76e+09 =  0% of the original kernel matrix.

torch.Size([26367, 2])
We keep 1.15e+07/4.24e+08 =  2% of the original kernel matrix.

torch.Size([35098, 2])
We keep 7.03e+06/5.36e+08 =  1% of the original kernel matrix.

torch.Size([1003906, 2])
We keep 2.92e+09/3.86e+11 =  0% of the original kernel matrix.

torch.Size([234420, 2])
We keep 1.30e+08/1.62e+10 =  0% of the original kernel matrix.

torch.Size([22864, 2])
We keep 2.83e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([33352, 2])
We keep 4.55e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([42299, 2])
We keep 1.47e+07/6.42e+08 =  2% of the original kernel matrix.

torch.Size([43398, 2])
We keep 7.57e+06/6.59e+08 =  1% of the original kernel matrix.

torch.Size([19418, 2])
We keep 6.42e+06/1.97e+08 =  3% of the original kernel matrix.

torch.Size([29703, 2])
We keep 5.20e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([11821, 2])
We keep 1.27e+06/2.88e+07 =  4% of the original kernel matrix.

torch.Size([24005, 2])
We keep 2.50e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([22709, 2])
We keep 3.97e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([33127, 2])
We keep 4.88e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([89398, 2])
We keep 1.46e+08/3.25e+09 =  4% of the original kernel matrix.

torch.Size([65411, 2])
We keep 1.54e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([14138, 2])
We keep 1.25e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([26020, 2])
We keep 2.76e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([292130, 2])
We keep 3.09e+08/3.42e+10 =  0% of the original kernel matrix.

torch.Size([122208, 2])
We keep 4.36e+07/4.81e+09 =  0% of the original kernel matrix.

torch.Size([35801, 2])
We keep 6.76e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([42496, 2])
We keep 7.18e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([15906, 2])
We keep 1.67e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([27614, 2])
We keep 3.24e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([40586, 2])
We keep 1.05e+07/6.81e+08 =  1% of the original kernel matrix.

torch.Size([43795, 2])
We keep 8.18e+06/6.79e+08 =  1% of the original kernel matrix.

torch.Size([307074, 2])
We keep 7.34e+08/6.67e+10 =  1% of the original kernel matrix.

torch.Size([120681, 2])
We keep 5.94e+07/6.72e+09 =  0% of the original kernel matrix.

torch.Size([161508, 2])
We keep 2.65e+08/1.81e+10 =  1% of the original kernel matrix.

torch.Size([86592, 2])
We keep 3.35e+07/3.50e+09 =  0% of the original kernel matrix.

torch.Size([672403, 2])
We keep 9.65e+08/1.42e+11 =  0% of the original kernel matrix.

torch.Size([185756, 2])
We keep 8.21e+07/9.79e+09 =  0% of the original kernel matrix.

torch.Size([41966, 2])
We keep 2.86e+07/6.20e+08 =  4% of the original kernel matrix.

torch.Size([44031, 2])
We keep 7.46e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([15990, 2])
We keep 2.26e+06/8.41e+07 =  2% of the original kernel matrix.

torch.Size([27630, 2])
We keep 3.69e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([16095, 2])
We keep 1.47e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([27782, 2])
We keep 3.21e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([10756, 2])
We keep 1.40e+06/2.83e+07 =  4% of the original kernel matrix.

torch.Size([22887, 2])
We keep 2.46e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([413031, 2])
We keep 1.44e+09/6.46e+10 =  2% of the original kernel matrix.

torch.Size([146438, 2])
We keep 5.58e+07/6.62e+09 =  0% of the original kernel matrix.

torch.Size([18130, 2])
We keep 2.05e+06/8.74e+07 =  2% of the original kernel matrix.

torch.Size([29516, 2])
We keep 3.76e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([58852, 2])
We keep 1.78e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([53833, 2])
We keep 1.07e+07/9.17e+08 =  1% of the original kernel matrix.

torch.Size([46050, 2])
We keep 1.40e+07/8.10e+08 =  1% of the original kernel matrix.

torch.Size([47681, 2])
We keep 8.79e+06/7.41e+08 =  1% of the original kernel matrix.

torch.Size([36976, 2])
We keep 2.02e+07/8.78e+08 =  2% of the original kernel matrix.

torch.Size([41203, 2])
We keep 9.28e+06/7.71e+08 =  1% of the original kernel matrix.

torch.Size([34186, 2])
We keep 1.34e+07/5.41e+08 =  2% of the original kernel matrix.

torch.Size([41036, 2])
We keep 7.74e+06/6.05e+08 =  1% of the original kernel matrix.

torch.Size([23488, 2])
We keep 3.46e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([33858, 2])
We keep 4.80e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([314866, 2])
We keep 5.06e+08/4.95e+10 =  1% of the original kernel matrix.

torch.Size([124614, 2])
We keep 5.17e+07/5.79e+09 =  0% of the original kernel matrix.

torch.Size([87829, 2])
We keep 4.38e+07/2.90e+09 =  1% of the original kernel matrix.

torch.Size([64253, 2])
We keep 1.53e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([19903, 2])
We keep 5.97e+06/1.28e+08 =  4% of the original kernel matrix.

torch.Size([30921, 2])
We keep 4.30e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([61210, 2])
We keep 7.29e+07/1.47e+09 =  4% of the original kernel matrix.

torch.Size([54593, 2])
We keep 1.12e+07/9.97e+08 =  1% of the original kernel matrix.

torch.Size([127244, 2])
We keep 2.97e+08/8.28e+09 =  3% of the original kernel matrix.

torch.Size([77259, 2])
We keep 2.35e+07/2.37e+09 =  0% of the original kernel matrix.

torch.Size([191760, 2])
We keep 1.66e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([96930, 2])
We keep 2.98e+07/3.10e+09 =  0% of the original kernel matrix.

torch.Size([643006, 2])
We keep 2.02e+09/2.18e+11 =  0% of the original kernel matrix.

torch.Size([175998, 2])
We keep 1.01e+08/1.21e+10 =  0% of the original kernel matrix.

torch.Size([201776, 2])
We keep 1.56e+08/1.47e+10 =  1% of the original kernel matrix.

torch.Size([100018, 2])
We keep 3.02e+07/3.16e+09 =  0% of the original kernel matrix.

torch.Size([16315, 2])
We keep 3.22e+06/6.56e+07 =  4% of the original kernel matrix.

torch.Size([28080, 2])
We keep 3.32e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([73607, 2])
We keep 2.25e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([59639, 2])
We keep 1.28e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([59028, 2])
We keep 7.16e+07/2.43e+09 =  2% of the original kernel matrix.

torch.Size([50631, 2])
We keep 1.43e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([24562, 2])
We keep 3.44e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([34754, 2])
We keep 4.84e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([25833, 2])
We keep 4.46e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([35379, 2])
We keep 5.59e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([14425, 2])
We keep 1.17e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([26243, 2])
We keep 2.87e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([48301, 2])
We keep 2.83e+07/8.67e+08 =  3% of the original kernel matrix.

torch.Size([49028, 2])
We keep 9.18e+06/7.66e+08 =  1% of the original kernel matrix.

torch.Size([57066, 2])
We keep 2.46e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([52074, 2])
We keep 1.13e+07/9.81e+08 =  1% of the original kernel matrix.

torch.Size([10359, 2])
We keep 9.92e+05/2.32e+07 =  4% of the original kernel matrix.

torch.Size([22349, 2])
We keep 2.31e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([44771, 2])
We keep 1.05e+07/7.69e+08 =  1% of the original kernel matrix.

torch.Size([47488, 2])
We keep 8.82e+06/7.22e+08 =  1% of the original kernel matrix.

torch.Size([14625, 2])
We keep 2.23e+06/7.08e+07 =  3% of the original kernel matrix.

torch.Size([26210, 2])
We keep 3.45e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([42422, 2])
We keep 2.59e+07/9.78e+08 =  2% of the original kernel matrix.

torch.Size([44659, 2])
We keep 9.40e+06/8.14e+08 =  1% of the original kernel matrix.

torch.Size([15514, 2])
We keep 1.93e+06/6.41e+07 =  3% of the original kernel matrix.

torch.Size([27160, 2])
We keep 3.34e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([49998, 2])
We keep 2.33e+07/1.08e+09 =  2% of the original kernel matrix.

torch.Size([49095, 2])
We keep 1.02e+07/8.55e+08 =  1% of the original kernel matrix.

torch.Size([21559, 2])
We keep 2.59e+07/2.88e+08 =  8% of the original kernel matrix.

torch.Size([31745, 2])
We keep 5.91e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([30253, 2])
We keep 8.84e+06/3.36e+08 =  2% of the original kernel matrix.

torch.Size([38723, 2])
We keep 6.22e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([16446, 2])
We keep 2.04e+06/6.74e+07 =  3% of the original kernel matrix.

torch.Size([27926, 2])
We keep 3.43e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([25622, 2])
We keep 3.60e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([35528, 2])
We keep 5.12e+06/3.61e+08 =  1% of the original kernel matrix.

torch.Size([56167, 2])
We keep 1.64e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([53261, 2])
We keep 1.00e+07/8.54e+08 =  1% of the original kernel matrix.

torch.Size([245434, 2])
We keep 3.29e+08/2.43e+10 =  1% of the original kernel matrix.

torch.Size([110488, 2])
We keep 3.76e+07/4.06e+09 =  0% of the original kernel matrix.

torch.Size([18762, 2])
We keep 2.15e+06/8.75e+07 =  2% of the original kernel matrix.

torch.Size([30113, 2])
We keep 3.77e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([6995, 2])
We keep 3.60e+05/8.56e+06 =  4% of the original kernel matrix.

torch.Size([19083, 2])
We keep 1.62e+06/7.61e+07 =  2% of the original kernel matrix.

torch.Size([17269, 2])
We keep 2.34e+06/8.14e+07 =  2% of the original kernel matrix.

torch.Size([28655, 2])
We keep 3.67e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([193425, 2])
We keep 2.47e+08/1.69e+10 =  1% of the original kernel matrix.

torch.Size([97392, 2])
We keep 3.14e+07/3.39e+09 =  0% of the original kernel matrix.

torch.Size([1093880, 2])
We keep 3.29e+09/4.02e+11 =  0% of the original kernel matrix.

torch.Size([245625, 2])
We keep 1.32e+08/1.65e+10 =  0% of the original kernel matrix.

torch.Size([131918, 2])
We keep 1.82e+08/9.24e+09 =  1% of the original kernel matrix.

torch.Size([78175, 2])
We keep 2.46e+07/2.50e+09 =  0% of the original kernel matrix.

torch.Size([16542, 2])
We keep 1.89e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([28090, 2])
We keep 3.46e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([288416, 2])
We keep 5.55e+08/4.56e+10 =  1% of the original kernel matrix.

torch.Size([119690, 2])
We keep 4.96e+07/5.56e+09 =  0% of the original kernel matrix.

torch.Size([331302, 2])
We keep 2.83e+08/4.23e+10 =  0% of the original kernel matrix.

torch.Size([133551, 2])
We keep 4.83e+07/5.35e+09 =  0% of the original kernel matrix.

torch.Size([8183, 2])
We keep 4.63e+05/1.20e+07 =  3% of the original kernel matrix.

torch.Size([20352, 2])
We keep 1.83e+06/9.00e+07 =  2% of the original kernel matrix.

torch.Size([23226, 2])
We keep 4.47e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([33389, 2])
We keep 4.82e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([72963, 2])
We keep 2.02e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([59412, 2])
We keep 1.26e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([118510, 2])
We keep 3.77e+08/1.31e+10 =  2% of the original kernel matrix.

torch.Size([72949, 2])
We keep 2.89e+07/2.98e+09 =  0% of the original kernel matrix.

torch.Size([242822, 2])
We keep 1.48e+09/5.63e+10 =  2% of the original kernel matrix.

torch.Size([106707, 2])
We keep 5.45e+07/6.18e+09 =  0% of the original kernel matrix.

torch.Size([371350, 2])
We keep 3.82e+08/5.19e+10 =  0% of the original kernel matrix.

torch.Size([138497, 2])
We keep 5.25e+07/5.93e+09 =  0% of the original kernel matrix.

torch.Size([40513, 2])
We keep 1.26e+07/5.71e+08 =  2% of the original kernel matrix.

torch.Size([45409, 2])
We keep 7.83e+06/6.22e+08 =  1% of the original kernel matrix.

torch.Size([14398, 2])
We keep 1.17e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([26352, 2])
We keep 2.81e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([8054, 2])
We keep 4.25e+05/1.15e+07 =  3% of the original kernel matrix.

torch.Size([20315, 2])
We keep 1.80e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([129285, 2])
We keep 1.05e+08/6.79e+09 =  1% of the original kernel matrix.

torch.Size([77989, 2])
We keep 2.15e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([31898, 2])
We keep 6.05e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([39967, 2])
We keep 6.38e+06/4.88e+08 =  1% of the original kernel matrix.

torch.Size([44324, 2])
We keep 4.31e+07/1.07e+09 =  4% of the original kernel matrix.

torch.Size([45037, 2])
We keep 9.84e+06/8.51e+08 =  1% of the original kernel matrix.

torch.Size([238700, 2])
We keep 2.56e+08/2.48e+10 =  1% of the original kernel matrix.

torch.Size([107803, 2])
We keep 3.80e+07/4.10e+09 =  0% of the original kernel matrix.

torch.Size([1083835, 2])
We keep 2.13e+09/4.08e+11 =  0% of the original kernel matrix.

torch.Size([247064, 2])
We keep 1.33e+08/1.66e+10 =  0% of the original kernel matrix.

torch.Size([22800, 2])
We keep 4.23e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([33164, 2])
We keep 4.90e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([7625, 2])
We keep 3.89e+05/1.01e+07 =  3% of the original kernel matrix.

torch.Size([20073, 2])
We keep 1.73e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([50996, 2])
We keep 3.28e+07/9.58e+08 =  3% of the original kernel matrix.

torch.Size([50329, 2])
We keep 9.56e+06/8.06e+08 =  1% of the original kernel matrix.

torch.Size([216233, 2])
We keep 6.84e+08/1.90e+10 =  3% of the original kernel matrix.

torch.Size([103743, 2])
We keep 3.30e+07/3.59e+09 =  0% of the original kernel matrix.

torch.Size([78309, 2])
We keep 4.35e+07/2.08e+09 =  2% of the original kernel matrix.

torch.Size([61364, 2])
We keep 1.32e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([13415, 2])
We keep 6.07e+06/6.12e+07 =  9% of the original kernel matrix.

torch.Size([25321, 2])
We keep 3.31e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([10553, 2])
We keep 9.68e+05/2.36e+07 =  4% of the original kernel matrix.

torch.Size([22547, 2])
We keep 2.34e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([55196, 2])
We keep 1.61e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([51640, 2])
We keep 1.01e+07/8.66e+08 =  1% of the original kernel matrix.

torch.Size([10423, 2])
We keep 8.93e+05/2.18e+07 =  4% of the original kernel matrix.

torch.Size([22558, 2])
We keep 2.26e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([74044, 2])
We keep 6.96e+07/3.20e+09 =  2% of the original kernel matrix.

torch.Size([57817, 2])
We keep 1.59e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([15889, 2])
We keep 1.66e+06/5.88e+07 =  2% of the original kernel matrix.

torch.Size([27615, 2])
We keep 3.27e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([35857, 2])
We keep 6.28e+06/4.18e+08 =  1% of the original kernel matrix.

torch.Size([42661, 2])
We keep 6.89e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([31090, 2])
We keep 5.78e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([39520, 2])
We keep 6.31e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([20185, 2])
We keep 1.99e+06/9.38e+07 =  2% of the original kernel matrix.

torch.Size([31146, 2])
We keep 3.84e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([19123, 2])
We keep 2.15e+06/9.22e+07 =  2% of the original kernel matrix.

torch.Size([30278, 2])
We keep 3.86e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([308980, 2])
We keep 2.95e+08/3.59e+10 =  0% of the original kernel matrix.

torch.Size([125454, 2])
We keep 4.45e+07/4.93e+09 =  0% of the original kernel matrix.

torch.Size([682043, 2])
We keep 1.11e+09/1.56e+11 =  0% of the original kernel matrix.

torch.Size([185350, 2])
We keep 8.60e+07/1.03e+10 =  0% of the original kernel matrix.

torch.Size([174075, 2])
We keep 2.16e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([92204, 2])
We keep 2.86e+07/2.98e+09 =  0% of the original kernel matrix.

torch.Size([166860, 2])
We keep 2.73e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([88043, 2])
We keep 3.21e+07/3.35e+09 =  0% of the original kernel matrix.

torch.Size([39429, 2])
We keep 7.59e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([44622, 2])
We keep 7.60e+06/6.00e+08 =  1% of the original kernel matrix.

torch.Size([31037, 2])
We keep 6.44e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([39284, 2])
We keep 6.22e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([43572, 2])
We keep 1.57e+07/7.53e+08 =  2% of the original kernel matrix.

torch.Size([46134, 2])
We keep 8.65e+06/7.14e+08 =  1% of the original kernel matrix.

torch.Size([275465, 2])
We keep 2.30e+08/2.85e+10 =  0% of the original kernel matrix.

torch.Size([117427, 2])
We keep 4.03e+07/4.39e+09 =  0% of the original kernel matrix.

torch.Size([54094, 2])
We keep 3.14e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([50641, 2])
We keep 1.14e+07/9.85e+08 =  1% of the original kernel matrix.

torch.Size([79086, 2])
We keep 1.23e+08/2.56e+09 =  4% of the original kernel matrix.

torch.Size([61148, 2])
We keep 1.41e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([28616, 2])
We keep 4.87e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([37801, 2])
We keep 5.83e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([2769352, 2])
We keep 1.12e+10/2.20e+12 =  0% of the original kernel matrix.

torch.Size([396422, 2])
We keep 2.90e+08/3.86e+10 =  0% of the original kernel matrix.

torch.Size([37614, 2])
We keep 7.66e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([43487, 2])
We keep 7.33e+06/5.71e+08 =  1% of the original kernel matrix.

torch.Size([128324, 2])
We keep 6.88e+07/6.21e+09 =  1% of the original kernel matrix.

torch.Size([77663, 2])
We keep 2.09e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([317234, 2])
We keep 4.39e+08/4.32e+10 =  1% of the original kernel matrix.

torch.Size([128619, 2])
We keep 4.83e+07/5.41e+09 =  0% of the original kernel matrix.

torch.Size([113184, 2])
We keep 5.49e+07/4.06e+09 =  1% of the original kernel matrix.

torch.Size([73014, 2])
We keep 1.73e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([66714, 2])
We keep 3.70e+07/1.76e+09 =  2% of the original kernel matrix.

torch.Size([56513, 2])
We keep 1.22e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([1430872, 2])
We keep 5.58e+09/7.08e+11 =  0% of the original kernel matrix.

torch.Size([275431, 2])
We keep 1.72e+08/2.19e+10 =  0% of the original kernel matrix.

torch.Size([246351, 2])
We keep 1.88e+08/2.42e+10 =  0% of the original kernel matrix.

torch.Size([111127, 2])
We keep 3.75e+07/4.05e+09 =  0% of the original kernel matrix.

torch.Size([156525, 2])
We keep 1.21e+08/9.51e+09 =  1% of the original kernel matrix.

torch.Size([86745, 2])
We keep 2.50e+07/2.54e+09 =  0% of the original kernel matrix.

torch.Size([19027, 2])
We keep 3.39e+06/9.76e+07 =  3% of the original kernel matrix.

torch.Size([30451, 2])
We keep 3.89e+06/2.57e+08 =  1% of the original kernel matrix.

torch.Size([85123, 2])
We keep 5.84e+07/2.87e+09 =  2% of the original kernel matrix.

torch.Size([63479, 2])
We keep 1.51e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([21030, 2])
We keep 4.78e+06/1.40e+08 =  3% of the original kernel matrix.

torch.Size([31962, 2])
We keep 4.43e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([314661, 2])
We keep 7.56e+08/3.99e+10 =  1% of the original kernel matrix.

torch.Size([126422, 2])
We keep 4.69e+07/5.20e+09 =  0% of the original kernel matrix.

torch.Size([2736938, 2])
We keep 1.17e+10/2.11e+12 =  0% of the original kernel matrix.

torch.Size([392919, 2])
We keep 2.84e+08/3.78e+10 =  0% of the original kernel matrix.

torch.Size([146568, 2])
We keep 2.95e+08/9.43e+09 =  3% of the original kernel matrix.

torch.Size([83545, 2])
We keep 2.53e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([276256, 2])
We keep 3.38e+08/3.62e+10 =  0% of the original kernel matrix.

torch.Size([116995, 2])
We keep 4.49e+07/4.95e+09 =  0% of the original kernel matrix.

torch.Size([165015, 2])
We keep 3.10e+08/1.17e+10 =  2% of the original kernel matrix.

torch.Size([89447, 2])
We keep 2.78e+07/2.81e+09 =  0% of the original kernel matrix.

torch.Size([41253, 2])
We keep 2.38e+07/8.66e+08 =  2% of the original kernel matrix.

torch.Size([44439, 2])
We keep 9.24e+06/7.66e+08 =  1% of the original kernel matrix.

torch.Size([47526, 2])
We keep 1.67e+07/9.44e+08 =  1% of the original kernel matrix.

torch.Size([48233, 2])
We keep 9.59e+06/8.00e+08 =  1% of the original kernel matrix.

torch.Size([25068, 2])
We keep 7.30e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([34586, 2])
We keep 5.62e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([174381, 2])
We keep 1.27e+09/3.75e+10 =  3% of the original kernel matrix.

torch.Size([90391, 2])
We keep 4.51e+07/5.04e+09 =  0% of the original kernel matrix.

torch.Size([941851, 2])
We keep 1.57e+09/2.96e+11 =  0% of the original kernel matrix.

torch.Size([225831, 2])
We keep 1.14e+08/1.42e+10 =  0% of the original kernel matrix.

torch.Size([119564, 2])
We keep 1.77e+08/1.19e+10 =  1% of the original kernel matrix.

torch.Size([70664, 2])
We keep 2.79e+07/2.84e+09 =  0% of the original kernel matrix.

torch.Size([1336811, 2])
We keep 1.14e+10/8.81e+11 =  1% of the original kernel matrix.

torch.Size([255584, 2])
We keep 1.89e+08/2.44e+10 =  0% of the original kernel matrix.

torch.Size([236749, 2])
We keep 4.40e+08/3.26e+10 =  1% of the original kernel matrix.

torch.Size([107580, 2])
We keep 4.33e+07/4.70e+09 =  0% of the original kernel matrix.

torch.Size([78530, 2])
We keep 2.81e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([61233, 2])
We keep 1.36e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([19239, 2])
We keep 3.97e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([30276, 2])
We keep 4.26e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([169450, 2])
We keep 2.53e+08/1.15e+10 =  2% of the original kernel matrix.

torch.Size([90667, 2])
We keep 2.68e+07/2.80e+09 =  0% of the original kernel matrix.

torch.Size([280777, 2])
We keep 2.51e+08/2.93e+10 =  0% of the original kernel matrix.

torch.Size([119495, 2])
We keep 4.05e+07/4.45e+09 =  0% of the original kernel matrix.

torch.Size([18037, 2])
We keep 3.82e+06/1.20e+08 =  3% of the original kernel matrix.

torch.Size([29223, 2])
We keep 4.29e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([1256846, 2])
We keep 2.52e+09/4.92e+11 =  0% of the original kernel matrix.

torch.Size([262654, 2])
We keep 1.45e+08/1.83e+10 =  0% of the original kernel matrix.

torch.Size([36717, 2])
We keep 1.26e+07/5.21e+08 =  2% of the original kernel matrix.

torch.Size([42445, 2])
We keep 7.25e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([58589, 2])
We keep 2.70e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([52820, 2])
We keep 1.14e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([13608, 2])
We keep 1.24e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([25907, 2])
We keep 2.82e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([21463, 2])
We keep 2.52e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([32171, 2])
We keep 4.30e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([7605, 2])
We keep 4.23e+05/1.07e+07 =  3% of the original kernel matrix.

torch.Size([19690, 2])
We keep 1.74e+06/8.53e+07 =  2% of the original kernel matrix.

torch.Size([14633, 2])
We keep 2.03e+06/5.67e+07 =  3% of the original kernel matrix.

torch.Size([26296, 2])
We keep 3.13e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([37126, 2])
We keep 1.06e+07/4.80e+08 =  2% of the original kernel matrix.

torch.Size([42915, 2])
We keep 7.32e+06/5.70e+08 =  1% of the original kernel matrix.

torch.Size([136566, 2])
We keep 2.93e+08/7.76e+09 =  3% of the original kernel matrix.

torch.Size([80472, 2])
We keep 2.25e+07/2.29e+09 =  0% of the original kernel matrix.

torch.Size([4857239, 2])
We keep 6.33e+10/9.87e+12 =  0% of the original kernel matrix.

torch.Size([487865, 2])
We keep 5.89e+08/8.17e+10 =  0% of the original kernel matrix.

torch.Size([24830, 2])
We keep 7.33e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([34286, 2])
We keep 5.60e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([324878, 2])
We keep 2.92e+08/4.14e+10 =  0% of the original kernel matrix.

torch.Size([131364, 2])
We keep 4.78e+07/5.30e+09 =  0% of the original kernel matrix.

torch.Size([20774, 2])
We keep 2.90e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([31745, 2])
We keep 4.28e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([46251, 2])
We keep 1.12e+07/7.50e+08 =  1% of the original kernel matrix.

torch.Size([48038, 2])
We keep 8.70e+06/7.12e+08 =  1% of the original kernel matrix.

torch.Size([16656, 2])
We keep 2.48e+06/8.20e+07 =  3% of the original kernel matrix.

torch.Size([28196, 2])
We keep 3.67e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([253273, 2])
We keep 5.98e+08/4.52e+10 =  1% of the original kernel matrix.

torch.Size([109989, 2])
We keep 4.96e+07/5.53e+09 =  0% of the original kernel matrix.

torch.Size([83781, 2])
We keep 1.33e+08/3.82e+09 =  3% of the original kernel matrix.

torch.Size([62394, 2])
We keep 1.71e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([2119141, 2])
We keep 6.60e+09/1.31e+12 =  0% of the original kernel matrix.

torch.Size([341992, 2])
We keep 2.26e+08/2.98e+10 =  0% of the original kernel matrix.

torch.Size([17255, 2])
We keep 1.85e+06/7.27e+07 =  2% of the original kernel matrix.

torch.Size([28690, 2])
We keep 3.53e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([305362, 2])
We keep 3.02e+08/3.59e+10 =  0% of the original kernel matrix.

torch.Size([125517, 2])
We keep 4.47e+07/4.93e+09 =  0% of the original kernel matrix.

torch.Size([41775, 2])
We keep 1.72e+07/6.44e+08 =  2% of the original kernel matrix.

torch.Size([44976, 2])
We keep 7.86e+06/6.60e+08 =  1% of the original kernel matrix.

torch.Size([290627, 2])
We keep 3.27e+08/3.37e+10 =  0% of the original kernel matrix.

torch.Size([121565, 2])
We keep 4.32e+07/4.78e+09 =  0% of the original kernel matrix.

torch.Size([377471, 2])
We keep 6.29e+08/6.76e+10 =  0% of the original kernel matrix.

torch.Size([138151, 2])
We keep 5.88e+07/6.76e+09 =  0% of the original kernel matrix.

torch.Size([40265, 2])
We keep 8.67e+06/5.41e+08 =  1% of the original kernel matrix.

torch.Size([44685, 2])
We keep 7.60e+06/6.05e+08 =  1% of the original kernel matrix.

torch.Size([24329, 2])
We keep 4.25e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([34432, 2])
We keep 5.00e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([221563, 2])
We keep 1.45e+08/1.75e+10 =  0% of the original kernel matrix.

torch.Size([105591, 2])
We keep 3.25e+07/3.45e+09 =  0% of the original kernel matrix.

torch.Size([52438, 2])
We keep 3.00e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([50862, 2])
We keep 9.67e+06/8.34e+08 =  1% of the original kernel matrix.

torch.Size([86901, 2])
We keep 6.00e+07/3.05e+09 =  1% of the original kernel matrix.

torch.Size([63784, 2])
We keep 1.53e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([394252, 2])
We keep 6.53e+08/6.17e+10 =  1% of the original kernel matrix.

torch.Size([141169, 2])
We keep 5.66e+07/6.47e+09 =  0% of the original kernel matrix.

torch.Size([31895, 2])
We keep 4.43e+07/5.57e+08 =  7% of the original kernel matrix.

torch.Size([38702, 2])
We keep 7.45e+06/6.14e+08 =  1% of the original kernel matrix.

torch.Size([137444, 2])
We keep 1.43e+08/8.47e+09 =  1% of the original kernel matrix.

torch.Size([80729, 2])
We keep 2.40e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([50534, 2])
We keep 1.70e+07/9.67e+08 =  1% of the original kernel matrix.

torch.Size([49292, 2])
We keep 9.59e+06/8.09e+08 =  1% of the original kernel matrix.

torch.Size([263858, 2])
We keep 2.68e+08/2.86e+10 =  0% of the original kernel matrix.

torch.Size([114968, 2])
We keep 4.02e+07/4.40e+09 =  0% of the original kernel matrix.

torch.Size([452303, 2])
We keep 8.39e+08/8.31e+10 =  1% of the original kernel matrix.

torch.Size([150657, 2])
We keep 6.42e+07/7.50e+09 =  0% of the original kernel matrix.

torch.Size([58228, 2])
We keep 2.23e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([53405, 2])
We keep 1.09e+07/9.37e+08 =  1% of the original kernel matrix.

torch.Size([23599, 2])
We keep 6.21e+06/1.82e+08 =  3% of the original kernel matrix.

torch.Size([33929, 2])
We keep 5.04e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([221541, 2])
We keep 2.88e+08/2.31e+10 =  1% of the original kernel matrix.

torch.Size([104021, 2])
We keep 3.67e+07/3.96e+09 =  0% of the original kernel matrix.

torch.Size([171127, 2])
We keep 1.35e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([90602, 2])
We keep 2.82e+07/2.89e+09 =  0% of the original kernel matrix.

torch.Size([427382, 2])
We keep 4.05e+09/1.75e+11 =  2% of the original kernel matrix.

torch.Size([140270, 2])
We keep 8.98e+07/1.09e+10 =  0% of the original kernel matrix.

torch.Size([75100, 2])
We keep 3.96e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([59741, 2])
We keep 1.31e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([60511, 2])
We keep 3.90e+07/1.62e+09 =  2% of the original kernel matrix.

torch.Size([53191, 2])
We keep 1.17e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([49549, 2])
We keep 2.20e+07/9.90e+08 =  2% of the original kernel matrix.

torch.Size([49166, 2])
We keep 9.62e+06/8.19e+08 =  1% of the original kernel matrix.

torch.Size([256009, 2])
We keep 5.21e+08/3.79e+10 =  1% of the original kernel matrix.

torch.Size([109475, 2])
We keep 4.55e+07/5.07e+09 =  0% of the original kernel matrix.

torch.Size([229471, 2])
We keep 4.24e+08/2.73e+10 =  1% of the original kernel matrix.

torch.Size([106375, 2])
We keep 3.98e+07/4.30e+09 =  0% of the original kernel matrix.

torch.Size([56778, 2])
We keep 1.65e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([52973, 2])
We keep 1.04e+07/8.91e+08 =  1% of the original kernel matrix.

torch.Size([17343, 2])
We keep 2.25e+06/8.45e+07 =  2% of the original kernel matrix.

torch.Size([28700, 2])
We keep 3.71e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([20414, 2])
We keep 2.94e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([31564, 2])
We keep 4.22e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([79740, 2])
We keep 6.47e+07/3.03e+09 =  2% of the original kernel matrix.

torch.Size([60452, 2])
We keep 1.53e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([160203, 2])
We keep 2.53e+08/1.23e+10 =  2% of the original kernel matrix.

torch.Size([87155, 2])
We keep 2.82e+07/2.89e+09 =  0% of the original kernel matrix.

torch.Size([153066, 2])
We keep 8.40e+07/7.90e+09 =  1% of the original kernel matrix.

torch.Size([85815, 2])
We keep 2.31e+07/2.31e+09 =  0% of the original kernel matrix.

torch.Size([28165, 2])
We keep 5.26e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([37369, 2])
We keep 5.81e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([51007, 2])
We keep 1.26e+07/9.72e+08 =  1% of the original kernel matrix.

torch.Size([50042, 2])
We keep 9.64e+06/8.12e+08 =  1% of the original kernel matrix.

torch.Size([88116, 2])
We keep 6.29e+07/2.90e+09 =  2% of the original kernel matrix.

torch.Size([64284, 2])
We keep 1.50e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([106741, 2])
We keep 4.01e+07/3.73e+09 =  1% of the original kernel matrix.

torch.Size([71078, 2])
We keep 1.67e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([57512, 2])
We keep 2.29e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([53278, 2])
We keep 1.05e+07/8.96e+08 =  1% of the original kernel matrix.

torch.Size([124412, 2])
We keep 5.72e+07/5.20e+09 =  1% of the original kernel matrix.

torch.Size([76162, 2])
We keep 1.94e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([20440, 2])
We keep 6.83e+06/1.78e+08 =  3% of the original kernel matrix.

torch.Size([31168, 2])
We keep 4.87e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([75423, 2])
We keep 2.87e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([60140, 2])
We keep 1.31e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([6787, 2])
We keep 4.45e+05/8.68e+06 =  5% of the original kernel matrix.

torch.Size([18864, 2])
We keep 1.66e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([16757, 2])
We keep 1.90e+06/6.93e+07 =  2% of the original kernel matrix.

torch.Size([28304, 2])
We keep 3.47e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([334042, 2])
We keep 3.36e+08/4.16e+10 =  0% of the original kernel matrix.

torch.Size([133818, 2])
We keep 4.79e+07/5.31e+09 =  0% of the original kernel matrix.

torch.Size([35611, 2])
We keep 1.01e+07/5.03e+08 =  2% of the original kernel matrix.

torch.Size([41616, 2])
We keep 7.44e+06/5.84e+08 =  1% of the original kernel matrix.

torch.Size([175950, 2])
We keep 1.25e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([92576, 2])
We keep 2.64e+07/2.72e+09 =  0% of the original kernel matrix.

torch.Size([8037, 2])
We keep 3.82e+05/1.02e+07 =  3% of the original kernel matrix.

torch.Size([20406, 2])
We keep 1.72e+06/8.32e+07 =  2% of the original kernel matrix.

torch.Size([15890, 2])
We keep 3.19e+06/6.77e+07 =  4% of the original kernel matrix.

torch.Size([27637, 2])
We keep 3.46e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([78254, 2])
We keep 1.00e+08/2.58e+09 =  3% of the original kernel matrix.

torch.Size([60259, 2])
We keep 1.39e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([129200, 2])
We keep 6.39e+07/5.79e+09 =  1% of the original kernel matrix.

torch.Size([77774, 2])
We keep 2.03e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([72377, 2])
We keep 2.67e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([59120, 2])
We keep 1.26e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([14156, 2])
We keep 1.65e+06/4.66e+07 =  3% of the original kernel matrix.

torch.Size([25882, 2])
We keep 3.00e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([21358, 2])
We keep 3.84e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([32055, 2])
We keep 4.57e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([74079, 2])
We keep 4.02e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([59606, 2])
We keep 1.32e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([40235, 2])
We keep 9.91e+06/6.05e+08 =  1% of the original kernel matrix.

torch.Size([45303, 2])
We keep 8.04e+06/6.40e+08 =  1% of the original kernel matrix.

torch.Size([189423, 2])
We keep 2.66e+08/1.44e+10 =  1% of the original kernel matrix.

torch.Size([96136, 2])
We keep 3.01e+07/3.12e+09 =  0% of the original kernel matrix.

torch.Size([13930, 2])
We keep 1.37e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([25725, 2])
We keep 3.02e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([15120, 2])
We keep 2.05e+06/6.48e+07 =  3% of the original kernel matrix.

torch.Size([26704, 2])
We keep 3.31e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([88661, 2])
We keep 7.09e+07/4.17e+09 =  1% of the original kernel matrix.

torch.Size([62132, 2])
We keep 1.78e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([27811, 2])
We keep 4.51e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([37009, 2])
We keep 5.51e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([339188, 2])
We keep 8.11e+08/5.23e+10 =  1% of the original kernel matrix.

torch.Size([131366, 2])
We keep 5.14e+07/5.95e+09 =  0% of the original kernel matrix.

torch.Size([29382, 2])
We keep 2.45e+07/3.22e+08 =  7% of the original kernel matrix.

torch.Size([37814, 2])
We keep 6.25e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([160000, 2])
We keep 2.58e+08/1.27e+10 =  2% of the original kernel matrix.

torch.Size([87395, 2])
We keep 2.81e+07/2.94e+09 =  0% of the original kernel matrix.

torch.Size([276556, 2])
We keep 2.48e+08/2.73e+10 =  0% of the original kernel matrix.

torch.Size([118324, 2])
We keep 3.96e+07/4.30e+09 =  0% of the original kernel matrix.

torch.Size([149819, 2])
We keep 2.52e+08/9.21e+09 =  2% of the original kernel matrix.

torch.Size([84862, 2])
We keep 2.45e+07/2.50e+09 =  0% of the original kernel matrix.

torch.Size([251141, 2])
We keep 3.22e+08/2.70e+10 =  1% of the original kernel matrix.

torch.Size([112175, 2])
We keep 3.98e+07/4.28e+09 =  0% of the original kernel matrix.

torch.Size([262622, 2])
We keep 2.22e+08/2.57e+10 =  0% of the original kernel matrix.

torch.Size([115019, 2])
We keep 3.85e+07/4.17e+09 =  0% of the original kernel matrix.

torch.Size([9869, 2])
We keep 8.04e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([22102, 2])
We keep 2.13e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([89860, 2])
We keep 3.57e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([65038, 2])
We keep 1.51e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([36399, 2])
We keep 7.84e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([42989, 2])
We keep 6.94e+06/5.34e+08 =  1% of the original kernel matrix.

torch.Size([218165, 2])
We keep 3.18e+08/1.83e+10 =  1% of the original kernel matrix.

torch.Size([103271, 2])
We keep 3.30e+07/3.52e+09 =  0% of the original kernel matrix.

torch.Size([202859, 2])
We keep 1.89e+08/1.78e+10 =  1% of the original kernel matrix.

torch.Size([99526, 2])
We keep 3.26e+07/3.47e+09 =  0% of the original kernel matrix.

torch.Size([50979, 2])
We keep 3.32e+07/1.28e+09 =  2% of the original kernel matrix.

torch.Size([48804, 2])
We keep 1.08e+07/9.32e+08 =  1% of the original kernel matrix.

torch.Size([127918, 2])
We keep 6.08e+07/5.71e+09 =  1% of the original kernel matrix.

torch.Size([77849, 2])
We keep 2.01e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([37700, 2])
We keep 9.86e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([43215, 2])
We keep 7.46e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([36223, 2])
We keep 3.51e+07/1.24e+09 =  2% of the original kernel matrix.

torch.Size([39333, 2])
We keep 1.07e+07/9.16e+08 =  1% of the original kernel matrix.

torch.Size([13138, 2])
We keep 1.02e+06/3.56e+07 =  2% of the original kernel matrix.

torch.Size([25128, 2])
We keep 2.67e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([26599, 2])
We keep 4.81e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([36254, 2])
We keep 5.45e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([327453, 2])
We keep 6.38e+08/4.16e+10 =  1% of the original kernel matrix.

torch.Size([129894, 2])
We keep 4.76e+07/5.31e+09 =  0% of the original kernel matrix.

torch.Size([967350, 2])
We keep 2.49e+09/3.27e+11 =  0% of the original kernel matrix.

torch.Size([230567, 2])
We keep 1.20e+08/1.49e+10 =  0% of the original kernel matrix.

torch.Size([41422, 2])
We keep 1.38e+07/8.17e+08 =  1% of the original kernel matrix.

torch.Size([45040, 2])
We keep 9.20e+06/7.44e+08 =  1% of the original kernel matrix.

torch.Size([33022, 2])
We keep 6.06e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([40716, 2])
We keep 6.57e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([63587, 2])
We keep 1.92e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([55802, 2])
We keep 1.15e+07/9.96e+08 =  1% of the original kernel matrix.

torch.Size([254981, 2])
We keep 5.67e+08/2.82e+10 =  2% of the original kernel matrix.

torch.Size([113406, 2])
We keep 3.91e+07/4.37e+09 =  0% of the original kernel matrix.

torch.Size([83625, 2])
We keep 2.83e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([63078, 2])
We keep 1.40e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([22029, 2])
We keep 5.20e+06/1.36e+08 =  3% of the original kernel matrix.

torch.Size([32599, 2])
We keep 4.33e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([114960, 2])
We keep 1.05e+08/4.72e+09 =  2% of the original kernel matrix.

torch.Size([73981, 2])
We keep 1.83e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([311364, 2])
We keep 3.20e+08/3.63e+10 =  0% of the original kernel matrix.

torch.Size([126752, 2])
We keep 4.49e+07/4.96e+09 =  0% of the original kernel matrix.

torch.Size([18098, 2])
We keep 3.15e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([29439, 2])
We keep 3.95e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([95883, 2])
We keep 7.98e+07/3.56e+09 =  2% of the original kernel matrix.

torch.Size([67016, 2])
We keep 1.66e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([46591, 2])
We keep 4.48e+07/9.23e+08 =  4% of the original kernel matrix.

torch.Size([47654, 2])
We keep 9.21e+06/7.91e+08 =  1% of the original kernel matrix.

time for making ranges is 7.501182794570923
Sorting X and nu_X
time for sorting X is 0.10117292404174805
Sorting Z and nu_Z
time for sorting Z is 0.0002453327178955078
Starting Optim
sum tnu_Z before tensor(46270360., device='cuda:0')
c= tensor(2429.4951, device='cuda:0')
c= tensor(242974.2188, device='cuda:0')
c= tensor(250000.3594, device='cuda:0')
c= tensor(469806.9375, device='cuda:0')
c= tensor(1105111.2500, device='cuda:0')
c= tensor(1697148.1250, device='cuda:0')
c= tensor(2474606.7500, device='cuda:0')
c= tensor(3480212.2500, device='cuda:0')
c= tensor(3577513.5000, device='cuda:0')
c= tensor(9045536., device='cuda:0')
c= tensor(9090705., device='cuda:0')
c= tensor(16392162., device='cuda:0')
c= tensor(16434375., device='cuda:0')
c= tensor(59852192., device='cuda:0')
c= tensor(60171332., device='cuda:0')
c= tensor(60795608., device='cuda:0')
c= tensor(62235172., device='cuda:0')
c= tensor(62655928., device='cuda:0')
c= tensor(75092936., device='cuda:0')
c= tensor(79230768., device='cuda:0')
c= tensor(80298296., device='cuda:0')
c= tensor(98780704., device='cuda:0')
c= tensor(98834096., device='cuda:0')
c= tensor(1.0073e+08, device='cuda:0')
c= tensor(1.0083e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0587e+08, device='cuda:0')
c= tensor(1.0602e+08, device='cuda:0')
c= tensor(1.1424e+08, device='cuda:0')
c= tensor(6.4612e+08, device='cuda:0')
c= tensor(6.4626e+08, device='cuda:0')
c= tensor(8.9415e+08, device='cuda:0')
c= tensor(8.9461e+08, device='cuda:0')
c= tensor(8.9468e+08, device='cuda:0')
c= tensor(8.9495e+08, device='cuda:0')
c= tensor(9.1945e+08, device='cuda:0')
c= tensor(9.2331e+08, device='cuda:0')
c= tensor(9.2331e+08, device='cuda:0')
c= tensor(9.2332e+08, device='cuda:0')
c= tensor(9.2333e+08, device='cuda:0')
c= tensor(9.2334e+08, device='cuda:0')
c= tensor(9.2335e+08, device='cuda:0')
c= tensor(9.2335e+08, device='cuda:0')
c= tensor(9.2337e+08, device='cuda:0')
c= tensor(9.2337e+08, device='cuda:0')
c= tensor(9.2337e+08, device='cuda:0')
c= tensor(9.2338e+08, device='cuda:0')
c= tensor(9.2340e+08, device='cuda:0')
c= tensor(9.2341e+08, device='cuda:0')
c= tensor(9.2357e+08, device='cuda:0')
c= tensor(9.2366e+08, device='cuda:0')
c= tensor(9.2366e+08, device='cuda:0')
c= tensor(9.2372e+08, device='cuda:0')
c= tensor(9.2373e+08, device='cuda:0')
c= tensor(9.2375e+08, device='cuda:0')
c= tensor(9.2382e+08, device='cuda:0')
c= tensor(9.2383e+08, device='cuda:0')
c= tensor(9.2385e+08, device='cuda:0')
c= tensor(9.2386e+08, device='cuda:0')
c= tensor(9.2387e+08, device='cuda:0')
c= tensor(9.2391e+08, device='cuda:0')
c= tensor(9.2391e+08, device='cuda:0')
c= tensor(9.2394e+08, device='cuda:0')
c= tensor(9.2399e+08, device='cuda:0')
c= tensor(9.2400e+08, device='cuda:0')
c= tensor(9.2401e+08, device='cuda:0')
c= tensor(9.2402e+08, device='cuda:0')
c= tensor(9.2403e+08, device='cuda:0')
c= tensor(9.2406e+08, device='cuda:0')
c= tensor(9.2407e+08, device='cuda:0')
c= tensor(9.2412e+08, device='cuda:0')
c= tensor(9.2414e+08, device='cuda:0')
c= tensor(9.2415e+08, device='cuda:0')
c= tensor(9.2416e+08, device='cuda:0')
c= tensor(9.2417e+08, device='cuda:0')
c= tensor(9.2423e+08, device='cuda:0')
c= tensor(9.2423e+08, device='cuda:0')
c= tensor(9.2424e+08, device='cuda:0')
c= tensor(9.2426e+08, device='cuda:0')
c= tensor(9.2438e+08, device='cuda:0')
c= tensor(9.2439e+08, device='cuda:0')
c= tensor(9.2440e+08, device='cuda:0')
c= tensor(9.2441e+08, device='cuda:0')
c= tensor(9.2441e+08, device='cuda:0')
c= tensor(9.2442e+08, device='cuda:0')
c= tensor(9.2442e+08, device='cuda:0')
c= tensor(9.2443e+08, device='cuda:0')
c= tensor(9.2444e+08, device='cuda:0')
c= tensor(9.2445e+08, device='cuda:0')
c= tensor(9.2446e+08, device='cuda:0')
c= tensor(9.2451e+08, device='cuda:0')
c= tensor(9.2452e+08, device='cuda:0')
c= tensor(9.2452e+08, device='cuda:0')
c= tensor(9.2455e+08, device='cuda:0')
c= tensor(9.2457e+08, device='cuda:0')
c= tensor(9.2458e+08, device='cuda:0')
c= tensor(9.2459e+08, device='cuda:0')
c= tensor(9.2462e+08, device='cuda:0')
c= tensor(9.2466e+08, device='cuda:0')
c= tensor(9.2467e+08, device='cuda:0')
c= tensor(9.2477e+08, device='cuda:0')
c= tensor(9.2479e+08, device='cuda:0')
c= tensor(9.2481e+08, device='cuda:0')
c= tensor(9.2482e+08, device='cuda:0')
c= tensor(9.2484e+08, device='cuda:0')
c= tensor(9.2485e+08, device='cuda:0')
c= tensor(9.2487e+08, device='cuda:0')
c= tensor(9.2487e+08, device='cuda:0')
c= tensor(9.2488e+08, device='cuda:0')
c= tensor(9.2489e+08, device='cuda:0')
c= tensor(9.2489e+08, device='cuda:0')
c= tensor(9.2490e+08, device='cuda:0')
c= tensor(9.2491e+08, device='cuda:0')
c= tensor(9.2492e+08, device='cuda:0')
c= tensor(9.2494e+08, device='cuda:0')
c= tensor(9.2496e+08, device='cuda:0')
c= tensor(9.2498e+08, device='cuda:0')
c= tensor(9.2498e+08, device='cuda:0')
c= tensor(9.2501e+08, device='cuda:0')
c= tensor(9.2502e+08, device='cuda:0')
c= tensor(9.2508e+08, device='cuda:0')
c= tensor(9.2509e+08, device='cuda:0')
c= tensor(9.2509e+08, device='cuda:0')
c= tensor(9.2510e+08, device='cuda:0')
c= tensor(9.2511e+08, device='cuda:0')
c= tensor(9.2511e+08, device='cuda:0')
c= tensor(9.2512e+08, device='cuda:0')
c= tensor(9.2516e+08, device='cuda:0')
c= tensor(9.2520e+08, device='cuda:0')
c= tensor(9.2520e+08, device='cuda:0')
c= tensor(9.2523e+08, device='cuda:0')
c= tensor(9.2523e+08, device='cuda:0')
c= tensor(9.2525e+08, device='cuda:0')
c= tensor(9.2525e+08, device='cuda:0')
c= tensor(9.2526e+08, device='cuda:0')
c= tensor(9.2527e+08, device='cuda:0')
c= tensor(9.2528e+08, device='cuda:0')
c= tensor(9.2529e+08, device='cuda:0')
c= tensor(9.2529e+08, device='cuda:0')
c= tensor(9.2529e+08, device='cuda:0')
c= tensor(9.2530e+08, device='cuda:0')
c= tensor(9.2530e+08, device='cuda:0')
c= tensor(9.2535e+08, device='cuda:0')
c= tensor(9.2543e+08, device='cuda:0')
c= tensor(9.2545e+08, device='cuda:0')
c= tensor(9.2546e+08, device='cuda:0')
c= tensor(9.2546e+08, device='cuda:0')
c= tensor(9.2547e+08, device='cuda:0')
c= tensor(9.2547e+08, device='cuda:0')
c= tensor(9.2548e+08, device='cuda:0')
c= tensor(9.2549e+08, device='cuda:0')
c= tensor(9.2550e+08, device='cuda:0')
c= tensor(9.2550e+08, device='cuda:0')
c= tensor(9.2557e+08, device='cuda:0')
c= tensor(9.2558e+08, device='cuda:0')
c= tensor(9.2575e+08, device='cuda:0')
c= tensor(9.2576e+08, device='cuda:0')
c= tensor(9.2577e+08, device='cuda:0')
c= tensor(9.2578e+08, device='cuda:0')
c= tensor(9.2578e+08, device='cuda:0')
c= tensor(9.2587e+08, device='cuda:0')
c= tensor(9.2587e+08, device='cuda:0')
c= tensor(9.2592e+08, device='cuda:0')
c= tensor(9.2592e+08, device='cuda:0')
c= tensor(9.2595e+08, device='cuda:0')
c= tensor(9.2596e+08, device='cuda:0')
c= tensor(9.2596e+08, device='cuda:0')
c= tensor(9.2598e+08, device='cuda:0')
c= tensor(9.2599e+08, device='cuda:0')
c= tensor(9.2599e+08, device='cuda:0')
c= tensor(9.2599e+08, device='cuda:0')
c= tensor(9.2599e+08, device='cuda:0')
c= tensor(9.2603e+08, device='cuda:0')
c= tensor(9.2603e+08, device='cuda:0')
c= tensor(9.2605e+08, device='cuda:0')
c= tensor(9.2683e+08, device='cuda:0')
c= tensor(9.2685e+08, device='cuda:0')
c= tensor(9.2685e+08, device='cuda:0')
c= tensor(9.2688e+08, device='cuda:0')
c= tensor(9.2689e+08, device='cuda:0')
c= tensor(9.2692e+08, device='cuda:0')
c= tensor(9.2693e+08, device='cuda:0')
c= tensor(9.2694e+08, device='cuda:0')
c= tensor(9.2695e+08, device='cuda:0')
c= tensor(9.2696e+08, device='cuda:0')
c= tensor(9.2698e+08, device='cuda:0')
c= tensor(9.2699e+08, device='cuda:0')
c= tensor(9.2702e+08, device='cuda:0')
c= tensor(9.2704e+08, device='cuda:0')
c= tensor(9.2709e+08, device='cuda:0')
c= tensor(9.2710e+08, device='cuda:0')
c= tensor(9.2710e+08, device='cuda:0')
c= tensor(9.2711e+08, device='cuda:0')
c= tensor(9.2713e+08, device='cuda:0')
c= tensor(9.2719e+08, device='cuda:0')
c= tensor(9.2720e+08, device='cuda:0')
c= tensor(9.2720e+08, device='cuda:0')
c= tensor(9.2721e+08, device='cuda:0')
c= tensor(9.2723e+08, device='cuda:0')
c= tensor(9.2723e+08, device='cuda:0')
c= tensor(9.2727e+08, device='cuda:0')
c= tensor(9.2728e+08, device='cuda:0')
c= tensor(9.2736e+08, device='cuda:0')
c= tensor(9.2737e+08, device='cuda:0')
c= tensor(9.2739e+08, device='cuda:0')
c= tensor(9.2739e+08, device='cuda:0')
c= tensor(9.2740e+08, device='cuda:0')
c= tensor(9.2742e+08, device='cuda:0')
c= tensor(9.2742e+08, device='cuda:0')
c= tensor(9.2746e+08, device='cuda:0')
c= tensor(9.2748e+08, device='cuda:0')
c= tensor(9.2750e+08, device='cuda:0')
c= tensor(9.2751e+08, device='cuda:0')
c= tensor(9.2752e+08, device='cuda:0')
c= tensor(9.2752e+08, device='cuda:0')
c= tensor(9.2753e+08, device='cuda:0')
c= tensor(9.2753e+08, device='cuda:0')
c= tensor(9.2753e+08, device='cuda:0')
c= tensor(9.2765e+08, device='cuda:0')
c= tensor(9.2766e+08, device='cuda:0')
c= tensor(9.2767e+08, device='cuda:0')
c= tensor(9.2767e+08, device='cuda:0')
c= tensor(9.2770e+08, device='cuda:0')
c= tensor(9.2771e+08, device='cuda:0')
c= tensor(9.2772e+08, device='cuda:0')
c= tensor(9.2773e+08, device='cuda:0')
c= tensor(9.2773e+08, device='cuda:0')
c= tensor(9.2774e+08, device='cuda:0')
c= tensor(9.2775e+08, device='cuda:0')
c= tensor(9.2776e+08, device='cuda:0')
c= tensor(9.2776e+08, device='cuda:0')
c= tensor(9.2778e+08, device='cuda:0')
c= tensor(9.2779e+08, device='cuda:0')
c= tensor(9.2781e+08, device='cuda:0')
c= tensor(9.2781e+08, device='cuda:0')
c= tensor(9.2783e+08, device='cuda:0')
c= tensor(9.2786e+08, device='cuda:0')
c= tensor(9.2788e+08, device='cuda:0')
c= tensor(9.2789e+08, device='cuda:0')
c= tensor(9.2823e+08, device='cuda:0')
c= tensor(9.3059e+08, device='cuda:0')
c= tensor(9.3064e+08, device='cuda:0')
c= tensor(9.3068e+08, device='cuda:0')
c= tensor(9.3068e+08, device='cuda:0')
c= tensor(9.3074e+08, device='cuda:0')
c= tensor(9.3109e+08, device='cuda:0')
c= tensor(9.4147e+08, device='cuda:0')
c= tensor(9.4148e+08, device='cuda:0')
c= tensor(9.5041e+08, device='cuda:0')
c= tensor(9.5995e+08, device='cuda:0')
c= tensor(9.6077e+08, device='cuda:0')
c= tensor(1.6827e+09, device='cuda:0')
c= tensor(1.6827e+09, device='cuda:0')
c= tensor(1.6827e+09, device='cuda:0')
c= tensor(1.6991e+09, device='cuda:0')
c= tensor(1.7706e+09, device='cuda:0')
c= tensor(1.7706e+09, device='cuda:0')
c= tensor(1.7714e+09, device='cuda:0')
c= tensor(1.7720e+09, device='cuda:0')
c= tensor(1.7757e+09, device='cuda:0')
c= tensor(1.7824e+09, device='cuda:0')
c= tensor(1.7874e+09, device='cuda:0')
c= tensor(1.7879e+09, device='cuda:0')
c= tensor(1.7882e+09, device='cuda:0')
c= tensor(1.7882e+09, device='cuda:0')
c= tensor(1.7991e+09, device='cuda:0')
c= tensor(1.7993e+09, device='cuda:0')
c= tensor(1.7993e+09, device='cuda:0')
c= tensor(1.7997e+09, device='cuda:0')
c= tensor(1.8028e+09, device='cuda:0')
c= tensor(1.8227e+09, device='cuda:0')
c= tensor(1.8253e+09, device='cuda:0')
c= tensor(1.8253e+09, device='cuda:0')
c= tensor(1.8259e+09, device='cuda:0')
c= tensor(1.8259e+09, device='cuda:0')
c= tensor(1.8270e+09, device='cuda:0')
c= tensor(1.8307e+09, device='cuda:0')
c= tensor(1.8318e+09, device='cuda:0')
c= tensor(1.8340e+09, device='cuda:0')
c= tensor(1.8340e+09, device='cuda:0')
c= tensor(1.8340e+09, device='cuda:0')
c= tensor(1.8370e+09, device='cuda:0')
c= tensor(1.8414e+09, device='cuda:0')
c= tensor(1.8437e+09, device='cuda:0')
c= tensor(1.8437e+09, device='cuda:0')
c= tensor(1.8805e+09, device='cuda:0')
c= tensor(1.8806e+09, device='cuda:0')
c= tensor(1.8811e+09, device='cuda:0')
c= tensor(1.8837e+09, device='cuda:0')
c= tensor(1.8837e+09, device='cuda:0')
c= tensor(1.8912e+09, device='cuda:0')
c= tensor(1.9236e+09, device='cuda:0')
c= tensor(2.0467e+09, device='cuda:0')
c= tensor(2.0474e+09, device='cuda:0')
c= tensor(2.0478e+09, device='cuda:0')
c= tensor(2.0478e+09, device='cuda:0')
c= tensor(2.0479e+09, device='cuda:0')
c= tensor(2.0584e+09, device='cuda:0')
c= tensor(2.0585e+09, device='cuda:0')
c= tensor(2.0596e+09, device='cuda:0')
c= tensor(2.0659e+09, device='cuda:0')
c= tensor(2.0665e+09, device='cuda:0')
c= tensor(2.0668e+09, device='cuda:0')
c= tensor(2.0668e+09, device='cuda:0')
c= tensor(2.0782e+09, device='cuda:0')
c= tensor(2.1030e+09, device='cuda:0')
c= tensor(2.1052e+09, device='cuda:0')
c= tensor(2.1053e+09, device='cuda:0')
c= tensor(2.1097e+09, device='cuda:0')
c= tensor(2.1098e+09, device='cuda:0')
c= tensor(2.1404e+09, device='cuda:0')
c= tensor(2.1404e+09, device='cuda:0')
c= tensor(2.1429e+09, device='cuda:0')
c= tensor(2.1431e+09, device='cuda:0')
c= tensor(2.1579e+09, device='cuda:0')
c= tensor(2.1596e+09, device='cuda:0')
c= tensor(2.1597e+09, device='cuda:0')
c= tensor(2.1653e+09, device='cuda:0')
c= tensor(2.1771e+09, device='cuda:0')
c= tensor(2.1798e+09, device='cuda:0')
c= tensor(2.1922e+09, device='cuda:0')
c= tensor(2.2093e+09, device='cuda:0')
c= tensor(2.2595e+09, device='cuda:0')
c= tensor(2.2601e+09, device='cuda:0')
c= tensor(2.2601e+09, device='cuda:0')
c= tensor(2.2602e+09, device='cuda:0')
c= tensor(2.2607e+09, device='cuda:0')
c= tensor(2.2610e+09, device='cuda:0')
c= tensor(2.2631e+09, device='cuda:0')
c= tensor(2.2631e+09, device='cuda:0')
c= tensor(2.2656e+09, device='cuda:0')
c= tensor(2.2734e+09, device='cuda:0')
c= tensor(2.2744e+09, device='cuda:0')
c= tensor(2.2745e+09, device='cuda:0')
c= tensor(2.2760e+09, device='cuda:0')
c= tensor(2.2761e+09, device='cuda:0')
c= tensor(2.2762e+09, device='cuda:0')
c= tensor(2.2763e+09, device='cuda:0')
c= tensor(2.2768e+09, device='cuda:0')
c= tensor(2.2842e+09, device='cuda:0')
c= tensor(2.2849e+09, device='cuda:0')
c= tensor(2.2853e+09, device='cuda:0')
c= tensor(2.2866e+09, device='cuda:0')
c= tensor(2.2866e+09, device='cuda:0')
c= tensor(2.7660e+09, device='cuda:0')
c= tensor(2.7661e+09, device='cuda:0')
c= tensor(2.7732e+09, device='cuda:0')
c= tensor(2.7732e+09, device='cuda:0')
c= tensor(2.7733e+09, device='cuda:0')
c= tensor(2.7733e+09, device='cuda:0')
c= tensor(2.7734e+09, device='cuda:0')
c= tensor(2.7734e+09, device='cuda:0')
c= tensor(2.7807e+09, device='cuda:0')
c= tensor(2.7807e+09, device='cuda:0')
c= tensor(2.7807e+09, device='cuda:0')
c= tensor(2.8016e+09, device='cuda:0')
c= tensor(2.8029e+09, device='cuda:0')
c= tensor(2.8037e+09, device='cuda:0')
c= tensor(2.8098e+09, device='cuda:0')
c= tensor(2.8383e+09, device='cuda:0')
c= tensor(2.8383e+09, device='cuda:0')
c= tensor(2.8384e+09, device='cuda:0')
c= tensor(2.8387e+09, device='cuda:0')
c= tensor(2.8387e+09, device='cuda:0')
c= tensor(2.8387e+09, device='cuda:0')
c= tensor(2.8388e+09, device='cuda:0')
c= tensor(2.8389e+09, device='cuda:0')
c= tensor(2.8389e+09, device='cuda:0')
c= tensor(2.8389e+09, device='cuda:0')
c= tensor(2.8389e+09, device='cuda:0')
c= tensor(2.8857e+09, device='cuda:0')
c= tensor(2.8858e+09, device='cuda:0')
c= tensor(2.8884e+09, device='cuda:0')
c= tensor(2.8884e+09, device='cuda:0')
c= tensor(2.8884e+09, device='cuda:0')
c= tensor(2.8908e+09, device='cuda:0')
c= tensor(3.0302e+09, device='cuda:0')
c= tensor(3.1008e+09, device='cuda:0')
c= tensor(3.1010e+09, device='cuda:0')
c= tensor(3.1017e+09, device='cuda:0')
c= tensor(3.1017e+09, device='cuda:0')
c= tensor(3.1024e+09, device='cuda:0')
c= tensor(3.3335e+09, device='cuda:0')
c= tensor(3.3339e+09, device='cuda:0')
c= tensor(3.3339e+09, device='cuda:0')
c= tensor(3.3355e+09, device='cuda:0')
c= tensor(3.4978e+09, device='cuda:0')
c= tensor(3.4985e+09, device='cuda:0')
c= tensor(3.4987e+09, device='cuda:0')
c= tensor(3.4989e+09, device='cuda:0')
c= tensor(3.4993e+09, device='cuda:0')
c= tensor(3.4994e+09, device='cuda:0')
c= tensor(3.5130e+09, device='cuda:0')
c= tensor(3.5132e+09, device='cuda:0')
c= tensor(3.5132e+09, device='cuda:0')
c= tensor(3.5170e+09, device='cuda:0')
c= tensor(3.5172e+09, device='cuda:0')
c= tensor(3.5172e+09, device='cuda:0')
c= tensor(3.5187e+09, device='cuda:0')
c= tensor(3.5226e+09, device='cuda:0')
c= tensor(3.5583e+09, device='cuda:0')
c= tensor(3.5797e+09, device='cuda:0')
c= tensor(3.5945e+09, device='cuda:0')
c= tensor(3.5948e+09, device='cuda:0')
c= tensor(3.5953e+09, device='cuda:0')
c= tensor(3.5981e+09, device='cuda:0')
c= tensor(3.6148e+09, device='cuda:0')
c= tensor(3.6149e+09, device='cuda:0')
c= tensor(3.7520e+09, device='cuda:0')
c= tensor(4.0932e+09, device='cuda:0')
c= tensor(4.1123e+09, device='cuda:0')
c= tensor(4.1165e+09, device='cuda:0')
c= tensor(4.1192e+09, device='cuda:0')
c= tensor(4.1193e+09, device='cuda:0')
c= tensor(4.1193e+09, device='cuda:0')
c= tensor(4.1196e+09, device='cuda:0')
c= tensor(4.1245e+09, device='cuda:0')
c= tensor(4.1292e+09, device='cuda:0')
c= tensor(4.1694e+09, device='cuda:0')
c= tensor(4.1751e+09, device='cuda:0')
c= tensor(4.1782e+09, device='cuda:0')
c= tensor(4.1786e+09, device='cuda:0')
c= tensor(4.1884e+09, device='cuda:0')
c= tensor(4.1884e+09, device='cuda:0')
c= tensor(4.1885e+09, device='cuda:0')
c= tensor(4.1959e+09, device='cuda:0')
c= tensor(4.1969e+09, device='cuda:0')
c= tensor(4.1969e+09, device='cuda:0')
c= tensor(4.1972e+09, device='cuda:0')
c= tensor(4.3743e+09, device='cuda:0')
c= tensor(4.3746e+09, device='cuda:0')
c= tensor(4.3812e+09, device='cuda:0')
c= tensor(4.3817e+09, device='cuda:0')
c= tensor(4.3819e+09, device='cuda:0')
c= tensor(4.3819e+09, device='cuda:0')
c= tensor(4.3820e+09, device='cuda:0')
c= tensor(4.3822e+09, device='cuda:0')
c= tensor(4.3833e+09, device='cuda:0')
c= tensor(4.3833e+09, device='cuda:0')
c= tensor(4.3906e+09, device='cuda:0')
c= tensor(4.3906e+09, device='cuda:0')
c= tensor(4.3953e+09, device='cuda:0')
c= tensor(4.3955e+09, device='cuda:0')
c= tensor(4.3983e+09, device='cuda:0')
c= tensor(4.3983e+09, device='cuda:0')
c= tensor(4.3986e+09, device='cuda:0')
c= tensor(4.3990e+09, device='cuda:0')
c= tensor(4.3994e+09, device='cuda:0')
c= tensor(4.4023e+09, device='cuda:0')
c= tensor(4.5247e+09, device='cuda:0')
c= tensor(4.5247e+09, device='cuda:0')
c= tensor(4.5247e+09, device='cuda:0')
c= tensor(4.5334e+09, device='cuda:0')
c= tensor(4.5336e+09, device='cuda:0')
c= tensor(4.5983e+09, device='cuda:0')
c= tensor(4.5983e+09, device='cuda:0')
c= tensor(4.6038e+09, device='cuda:0')
c= tensor(4.6266e+09, device='cuda:0')
c= tensor(4.6266e+09, device='cuda:0')
c= tensor(4.6493e+09, device='cuda:0')
c= tensor(4.6500e+09, device='cuda:0')
c= tensor(4.7436e+09, device='cuda:0')
c= tensor(4.7436e+09, device='cuda:0')
c= tensor(4.7440e+09, device='cuda:0')
c= tensor(4.7442e+09, device='cuda:0')
c= tensor(4.7443e+09, device='cuda:0')
c= tensor(4.7443e+09, device='cuda:0')
c= tensor(4.7480e+09, device='cuda:0')
c= tensor(4.7480e+09, device='cuda:0')
c= tensor(4.7570e+09, device='cuda:0')
c= tensor(4.7571e+09, device='cuda:0')
c= tensor(4.7571e+09, device='cuda:0')
c= tensor(4.7573e+09, device='cuda:0')
c= tensor(4.7761e+09, device='cuda:0')
c= tensor(4.7822e+09, device='cuda:0')
c= tensor(4.8095e+09, device='cuda:0')
c= tensor(4.8101e+09, device='cuda:0')
c= tensor(4.8102e+09, device='cuda:0')
c= tensor(4.8102e+09, device='cuda:0')
c= tensor(4.8102e+09, device='cuda:0')
c= tensor(4.8776e+09, device='cuda:0')
c= tensor(4.8776e+09, device='cuda:0')
c= tensor(4.8780e+09, device='cuda:0')
c= tensor(4.8795e+09, device='cuda:0')
c= tensor(4.8806e+09, device='cuda:0')
c= tensor(4.8809e+09, device='cuda:0')
c= tensor(4.8809e+09, device='cuda:0')
c= tensor(4.9006e+09, device='cuda:0')
c= tensor(4.9014e+09, device='cuda:0')
c= tensor(4.9016e+09, device='cuda:0')
c= tensor(4.9029e+09, device='cuda:0')
c= tensor(4.9132e+09, device='cuda:0')
c= tensor(4.9171e+09, device='cuda:0')
c= tensor(4.9760e+09, device='cuda:0')
c= tensor(4.9803e+09, device='cuda:0')
c= tensor(4.9804e+09, device='cuda:0')
c= tensor(4.9811e+09, device='cuda:0')
c= tensor(4.9825e+09, device='cuda:0')
c= tensor(4.9826e+09, device='cuda:0')
c= tensor(4.9827e+09, device='cuda:0')
c= tensor(4.9827e+09, device='cuda:0')
c= tensor(4.9841e+09, device='cuda:0')
c= tensor(4.9846e+09, device='cuda:0')
c= tensor(4.9846e+09, device='cuda:0')
c= tensor(4.9848e+09, device='cuda:0')
c= tensor(4.9848e+09, device='cuda:0')
c= tensor(4.9857e+09, device='cuda:0')
c= tensor(4.9858e+09, device='cuda:0')
c= tensor(4.9862e+09, device='cuda:0')
c= tensor(4.9868e+09, device='cuda:0')
c= tensor(4.9870e+09, device='cuda:0')
c= tensor(4.9870e+09, device='cuda:0')
c= tensor(4.9871e+09, device='cuda:0')
c= tensor(4.9874e+09, device='cuda:0')
c= tensor(4.9998e+09, device='cuda:0')
c= tensor(4.9998e+09, device='cuda:0')
c= tensor(4.9998e+09, device='cuda:0')
c= tensor(4.9999e+09, device='cuda:0')
c= tensor(5.0076e+09, device='cuda:0')
c= tensor(5.1280e+09, device='cuda:0')
c= tensor(5.1321e+09, device='cuda:0')
c= tensor(5.1321e+09, device='cuda:0')
c= tensor(5.1469e+09, device='cuda:0')
c= tensor(5.1537e+09, device='cuda:0')
c= tensor(5.1538e+09, device='cuda:0')
c= tensor(5.1539e+09, device='cuda:0')
c= tensor(5.1542e+09, device='cuda:0')
c= tensor(5.1695e+09, device='cuda:0')
c= tensor(5.2209e+09, device='cuda:0')
c= tensor(5.2314e+09, device='cuda:0')
c= tensor(5.2317e+09, device='cuda:0')
c= tensor(5.2317e+09, device='cuda:0')
c= tensor(5.2317e+09, device='cuda:0')
c= tensor(5.2340e+09, device='cuda:0')
c= tensor(5.2341e+09, device='cuda:0')
c= tensor(5.2353e+09, device='cuda:0')
c= tensor(5.2420e+09, device='cuda:0')
c= tensor(5.3075e+09, device='cuda:0')
c= tensor(5.3076e+09, device='cuda:0')
c= tensor(5.3076e+09, device='cuda:0')
c= tensor(5.3083e+09, device='cuda:0')
c= tensor(5.3281e+09, device='cuda:0')
c= tensor(5.3293e+09, device='cuda:0')
c= tensor(5.3294e+09, device='cuda:0')
c= tensor(5.3294e+09, device='cuda:0')
c= tensor(5.3298e+09, device='cuda:0')
c= tensor(5.3298e+09, device='cuda:0')
c= tensor(5.3314e+09, device='cuda:0')
c= tensor(5.3314e+09, device='cuda:0')
c= tensor(5.3315e+09, device='cuda:0')
c= tensor(5.3316e+09, device='cuda:0')
c= tensor(5.3316e+09, device='cuda:0')
c= tensor(5.3317e+09, device='cuda:0')
c= tensor(5.3402e+09, device='cuda:0')
c= tensor(5.3748e+09, device='cuda:0')
c= tensor(5.3804e+09, device='cuda:0')
c= tensor(5.3895e+09, device='cuda:0')
c= tensor(5.3897e+09, device='cuda:0')
c= tensor(5.3898e+09, device='cuda:0')
c= tensor(5.3901e+09, device='cuda:0')
c= tensor(5.3951e+09, device='cuda:0')
c= tensor(5.3957e+09, device='cuda:0')
c= tensor(5.3984e+09, device='cuda:0')
c= tensor(5.3985e+09, device='cuda:0')
c= tensor(5.8310e+09, device='cuda:0')
c= tensor(5.8311e+09, device='cuda:0')
c= tensor(5.8326e+09, device='cuda:0')
c= tensor(5.8509e+09, device='cuda:0')
c= tensor(5.8522e+09, device='cuda:0')
c= tensor(5.8531e+09, device='cuda:0')
c= tensor(6.0408e+09, device='cuda:0')
c= tensor(6.0491e+09, device='cuda:0')
c= tensor(6.0516e+09, device='cuda:0')
c= tensor(6.0517e+09, device='cuda:0')
c= tensor(6.0527e+09, device='cuda:0')
c= tensor(6.0528e+09, device='cuda:0')
c= tensor(6.0721e+09, device='cuda:0')
c= tensor(6.4843e+09, device='cuda:0')
c= tensor(6.4917e+09, device='cuda:0')
c= tensor(6.5002e+09, device='cuda:0')
c= tensor(6.5071e+09, device='cuda:0')
c= tensor(6.5075e+09, device='cuda:0')
c= tensor(6.5079e+09, device='cuda:0')
c= tensor(6.5080e+09, device='cuda:0')
c= tensor(6.5407e+09, device='cuda:0')
c= tensor(6.5953e+09, device='cuda:0')
c= tensor(6.5989e+09, device='cuda:0')
c= tensor(7.0219e+09, device='cuda:0')
c= tensor(7.0347e+09, device='cuda:0')
c= tensor(7.0353e+09, device='cuda:0')
c= tensor(7.0354e+09, device='cuda:0')
c= tensor(7.0411e+09, device='cuda:0')
c= tensor(7.0472e+09, device='cuda:0')
c= tensor(7.0472e+09, device='cuda:0')
c= tensor(7.1268e+09, device='cuda:0')
c= tensor(7.1275e+09, device='cuda:0')
c= tensor(7.1281e+09, device='cuda:0')
c= tensor(7.1282e+09, device='cuda:0')
c= tensor(7.1282e+09, device='cuda:0')
c= tensor(7.1282e+09, device='cuda:0')
c= tensor(7.1282e+09, device='cuda:0')
c= tensor(7.1285e+09, device='cuda:0')
c= tensor(7.1344e+09, device='cuda:0')
c= tensor(9.7371e+09, device='cuda:0')
c= tensor(9.7379e+09, device='cuda:0')
c= tensor(9.7452e+09, device='cuda:0')
c= tensor(9.7453e+09, device='cuda:0')
c= tensor(9.7455e+09, device='cuda:0')
c= tensor(9.7455e+09, device='cuda:0')
c= tensor(9.7621e+09, device='cuda:0')
c= tensor(9.7650e+09, device='cuda:0')
c= tensor(1.0035e+10, device='cuda:0')
c= tensor(1.0035e+10, device='cuda:0')
c= tensor(1.0043e+10, device='cuda:0')
c= tensor(1.0044e+10, device='cuda:0')
c= tensor(1.0053e+10, device='cuda:0')
c= tensor(1.0079e+10, device='cuda:0')
c= tensor(1.0079e+10, device='cuda:0')
c= tensor(1.0079e+10, device='cuda:0')
c= tensor(1.0083e+10, device='cuda:0')
c= tensor(1.0083e+10, device='cuda:0')
c= tensor(1.0085e+10, device='cuda:0')
c= tensor(1.0103e+10, device='cuda:0')
c= tensor(1.0105e+10, device='cuda:0')
c= tensor(1.0108e+10, device='cuda:0')
c= tensor(1.0109e+10, device='cuda:0')
c= tensor(1.0116e+10, device='cuda:0')
c= tensor(1.0140e+10, device='cuda:0')
c= tensor(1.0141e+10, device='cuda:0')
c= tensor(1.0141e+10, device='cuda:0')
c= tensor(1.0150e+10, device='cuda:0')
c= tensor(1.0153e+10, device='cuda:0')
c= tensor(1.0261e+10, device='cuda:0')
c= tensor(1.0261e+10, device='cuda:0')
c= tensor(1.0263e+10, device='cuda:0')
c= tensor(1.0263e+10, device='cuda:0')
c= tensor(1.0290e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0304e+10, device='cuda:0')
c= tensor(1.0310e+10, device='cuda:0')
c= tensor(1.0311e+10, device='cuda:0')
c= tensor(1.0311e+10, device='cuda:0')
c= tensor(1.0312e+10, device='cuda:0')
c= tensor(1.0313e+10, device='cuda:0')
c= tensor(1.0314e+10, device='cuda:0')
c= tensor(1.0315e+10, device='cuda:0')
c= tensor(1.0316e+10, device='cuda:0')
c= tensor(1.0317e+10, device='cuda:0')
c= tensor(1.0318e+10, device='cuda:0')
c= tensor(1.0318e+10, device='cuda:0')
c= tensor(1.0318e+10, device='cuda:0')
c= tensor(1.0330e+10, device='cuda:0')
c= tensor(1.0330e+10, device='cuda:0')
c= tensor(1.0334e+10, device='cuda:0')
c= tensor(1.0334e+10, device='cuda:0')
c= tensor(1.0334e+10, device='cuda:0')
c= tensor(1.0336e+10, device='cuda:0')
c= tensor(1.0337e+10, device='cuda:0')
c= tensor(1.0338e+10, device='cuda:0')
c= tensor(1.0338e+10, device='cuda:0')
c= tensor(1.0338e+10, device='cuda:0')
c= tensor(1.0338e+10, device='cuda:0')
c= tensor(1.0339e+10, device='cuda:0')
c= tensor(1.0346e+10, device='cuda:0')
c= tensor(1.0346e+10, device='cuda:0')
c= tensor(1.0346e+10, device='cuda:0')
c= tensor(1.0347e+10, device='cuda:0')
c= tensor(1.0347e+10, device='cuda:0')
c= tensor(1.0374e+10, device='cuda:0')
c= tensor(1.0374e+10, device='cuda:0')
c= tensor(1.0382e+10, device='cuda:0')
c= tensor(1.0388e+10, device='cuda:0')
c= tensor(1.0393e+10, device='cuda:0')
c= tensor(1.0400e+10, device='cuda:0')
c= tensor(1.0405e+10, device='cuda:0')
c= tensor(1.0405e+10, device='cuda:0')
c= tensor(1.0406e+10, device='cuda:0')
c= tensor(1.0406e+10, device='cuda:0')
c= tensor(1.0413e+10, device='cuda:0')
c= tensor(1.0418e+10, device='cuda:0')
c= tensor(1.0419e+10, device='cuda:0')
c= tensor(1.0421e+10, device='cuda:0')
c= tensor(1.0421e+10, device='cuda:0')
c= tensor(1.0425e+10, device='cuda:0')
c= tensor(1.0425e+10, device='cuda:0')
c= tensor(1.0425e+10, device='cuda:0')
c= tensor(1.0444e+10, device='cuda:0')
c= tensor(1.0535e+10, device='cuda:0')
c= tensor(1.0535e+10, device='cuda:0')
c= tensor(1.0536e+10, device='cuda:0')
c= tensor(1.0536e+10, device='cuda:0')
c= tensor(1.0552e+10, device='cuda:0')
c= tensor(1.0553e+10, device='cuda:0')
c= tensor(1.0553e+10, device='cuda:0')
c= tensor(1.0555e+10, device='cuda:0')
c= tensor(1.0563e+10, device='cuda:0')
c= tensor(1.0563e+10, device='cuda:0')
c= tensor(1.0564e+10, device='cuda:0')
c= tensor(1.0565e+10, device='cuda:0')
memory (bytes)
5963087872
time for making loss 2 is 10.340891599655151
p0 True
it  0 : 3365104128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 73% |
shape of L is 
torch.Size([])
memory (bytes)
5963419648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 18% |
memory (bytes)
5963825152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  92401020000.0
relative error loss 8.745845
shape of L is 
torch.Size([])
memory (bytes)
6040969216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 19% |
memory (bytes)
6040969216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  92400720000.0
relative error loss 8.745817
shape of L is 
torch.Size([])
memory (bytes)
6043025408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
6043144192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  92399370000.0
relative error loss 8.745689
shape of L is 
torch.Size([])
memory (bytes)
6044987392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6044987392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  92391840000.0
relative error loss 8.744976
shape of L is 
torch.Size([])
memory (bytes)
6047301632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6047363072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  92339570000.0
relative error loss 8.740028
shape of L is 
torch.Size([])
memory (bytes)
6049357824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
6049357824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  91788390000.0
relative error loss 8.687859
shape of L is 
torch.Size([])
memory (bytes)
6051602432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6051602432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  89072600000.0
relative error loss 8.430807
shape of L is 
torch.Size([])
memory (bytes)
6053675008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 19% |
memory (bytes)
6053675008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  63618208000.0
relative error loss 6.0215244
shape of L is 
torch.Size([])
memory (bytes)
6055845888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
6055845888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 19% |
error is  22991102000.0
relative error loss 2.1761298
shape of L is 
torch.Size([])
memory (bytes)
6057836544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6057836544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  14814501000.0
relative error loss 1.4022067
time to take a step is 183.50290369987488
it  1 : 3876976640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6060134400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6060163072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  14814501000.0
relative error loss 1.4022067
shape of L is 
torch.Size([])
memory (bytes)
6062321664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6062321664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  11664925000.0
relative error loss 1.1040963
shape of L is 
torch.Size([])
memory (bytes)
6064308224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6064308224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  10375761000.0
relative error loss 0.9820757
shape of L is 
torch.Size([])
memory (bytes)
6066475008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6066589696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  38997447000.0
relative error loss 3.6911457
shape of L is 
torch.Size([])
memory (bytes)
6068621312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 19% |
memory (bytes)
6068621312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  9740717000.0
relative error loss 0.9219682
shape of L is 
torch.Size([])
memory (bytes)
6070743040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6070743040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  9108304000.0
relative error loss 0.8621097
shape of L is 
torch.Size([])
memory (bytes)
6072852480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6072967168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  8719972000.0
relative error loss 0.82535374
shape of L is 
torch.Size([])
memory (bytes)
6075052032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
6075052032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  8794753000.0
relative error loss 0.8324318
shape of L is 
torch.Size([])
memory (bytes)
6076895232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
6077145088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  7983464000.0
relative error loss 0.75564253
shape of L is 
torch.Size([])
memory (bytes)
6079164416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
6079275008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  7323638300.0
relative error loss 0.6931894
time to take a step is 177.80546188354492
it  2 : 4023134208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6081159168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6081159168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  7323638300.0
relative error loss 0.6931894
shape of L is 
torch.Size([])
memory (bytes)
6083399680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6083399680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  6643095600.0
relative error loss 0.62877536
shape of L is 
torch.Size([])
memory (bytes)
6085509120
| ID | GPU | MEM |
------------------
|  0 |  4% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6085509120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  6242049000.0
relative error loss 0.5908159
shape of L is 
torch.Size([])
memory (bytes)
6087618560
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6087618560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  5940281000.0
relative error loss 0.5622533
shape of L is 
torch.Size([])
memory (bytes)
6089760768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6089875456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  5465155600.0
relative error loss 0.5172822
shape of L is 
torch.Size([])
memory (bytes)
6091886592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 19% |
memory (bytes)
6091886592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  5154216000.0
relative error loss 0.48785147
shape of L is 
torch.Size([])
memory (bytes)
6093942784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 19% |
memory (bytes)
6093942784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  4788376600.0
relative error loss 0.45322442
shape of L is 
torch.Size([])
memory (bytes)
6096228352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6096228352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  4389247000.0
relative error loss 0.41544643
shape of L is 
torch.Size([])
memory (bytes)
6098403328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 19% |
memory (bytes)
6098403328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  4005763000.0
relative error loss 0.37914932
shape of L is 
torch.Size([])
memory (bytes)
6100258816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6100258816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  3695075800.0
relative error loss 0.34974247
time to take a step is 178.67507815361023
it  3 : 4022812160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6102601728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 19% |
memory (bytes)
6102671360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 19% |
error is  3695075800.0
relative error loss 0.34974247
shape of L is 
torch.Size([])
memory (bytes)
6104690688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
6104821760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  3471822800.0
relative error loss 0.32861137
shape of L is 
torch.Size([])
memory (bytes)
6106910720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6106910720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  3188659200.0
relative error loss 0.30180964
shape of L is 
torch.Size([])
memory (bytes)
6109089792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
6109089792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  3094203400.0
relative error loss 0.29286933
shape of L is 
torch.Size([])
memory (bytes)
6111072256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6111072256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2759815200.0
relative error loss 0.26121914
shape of L is 
torch.Size([])
memory (bytes)
6113251328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
6113251328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  2551939000.0
relative error loss 0.24154349
shape of L is 
torch.Size([])
memory (bytes)
6115479552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
6115479552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 19% |
error is  2348688400.0
relative error loss 0.22230561
shape of L is 
torch.Size([])
memory (bytes)
6117638144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6117638144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  3142331000.0
relative error loss 0.29742464
shape of L is 
torch.Size([])
memory (bytes)
6119694336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6119694336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2226896000.0
relative error loss 0.21077783
shape of L is 
torch.Size([])
memory (bytes)
6121832448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6121922560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2080631800.0
relative error loss 0.19693379
time to take a step is 179.0255627632141
c= tensor(2429.4951, device='cuda:0')
c= tensor(242974.2188, device='cuda:0')
c= tensor(250000.3594, device='cuda:0')
c= tensor(469806.9375, device='cuda:0')
c= tensor(1105111.2500, device='cuda:0')
c= tensor(1697148.1250, device='cuda:0')
c= tensor(2474606.7500, device='cuda:0')
c= tensor(3480212.2500, device='cuda:0')
c= tensor(3577513.5000, device='cuda:0')
c= tensor(9045536., device='cuda:0')
c= tensor(9090705., device='cuda:0')
c= tensor(16392162., device='cuda:0')
c= tensor(16434375., device='cuda:0')
c= tensor(59852192., device='cuda:0')
c= tensor(60171332., device='cuda:0')
c= tensor(60795608., device='cuda:0')
c= tensor(62235172., device='cuda:0')
c= tensor(62655928., device='cuda:0')
c= tensor(75092936., device='cuda:0')
c= tensor(79230768., device='cuda:0')
c= tensor(80298296., device='cuda:0')
c= tensor(98780704., device='cuda:0')
c= tensor(98834096., device='cuda:0')
c= tensor(1.0073e+08, device='cuda:0')
c= tensor(1.0083e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0587e+08, device='cuda:0')
c= tensor(1.0602e+08, device='cuda:0')
c= tensor(1.1424e+08, device='cuda:0')
c= tensor(6.4612e+08, device='cuda:0')
c= tensor(6.4626e+08, device='cuda:0')
c= tensor(8.9415e+08, device='cuda:0')
c= tensor(8.9461e+08, device='cuda:0')
c= tensor(8.9468e+08, device='cuda:0')
c= tensor(8.9495e+08, device='cuda:0')
c= tensor(9.1945e+08, device='cuda:0')
c= tensor(9.2331e+08, device='cuda:0')
c= tensor(9.2331e+08, device='cuda:0')
c= tensor(9.2332e+08, device='cuda:0')
c= tensor(9.2333e+08, device='cuda:0')
c= tensor(9.2334e+08, device='cuda:0')
c= tensor(9.2335e+08, device='cuda:0')
c= tensor(9.2335e+08, device='cuda:0')
c= tensor(9.2337e+08, device='cuda:0')
c= tensor(9.2337e+08, device='cuda:0')
c= tensor(9.2337e+08, device='cuda:0')
c= tensor(9.2338e+08, device='cuda:0')
c= tensor(9.2340e+08, device='cuda:0')
c= tensor(9.2341e+08, device='cuda:0')
c= tensor(9.2357e+08, device='cuda:0')
c= tensor(9.2366e+08, device='cuda:0')
c= tensor(9.2366e+08, device='cuda:0')
c= tensor(9.2372e+08, device='cuda:0')
c= tensor(9.2373e+08, device='cuda:0')
c= tensor(9.2375e+08, device='cuda:0')
c= tensor(9.2382e+08, device='cuda:0')
c= tensor(9.2383e+08, device='cuda:0')
c= tensor(9.2385e+08, device='cuda:0')
c= tensor(9.2386e+08, device='cuda:0')
c= tensor(9.2387e+08, device='cuda:0')
c= tensor(9.2391e+08, device='cuda:0')
c= tensor(9.2391e+08, device='cuda:0')
c= tensor(9.2394e+08, device='cuda:0')
c= tensor(9.2399e+08, device='cuda:0')
c= tensor(9.2400e+08, device='cuda:0')
c= tensor(9.2401e+08, device='cuda:0')
c= tensor(9.2402e+08, device='cuda:0')
c= tensor(9.2403e+08, device='cuda:0')
c= tensor(9.2406e+08, device='cuda:0')
c= tensor(9.2407e+08, device='cuda:0')
c= tensor(9.2412e+08, device='cuda:0')
c= tensor(9.2414e+08, device='cuda:0')
c= tensor(9.2415e+08, device='cuda:0')
c= tensor(9.2416e+08, device='cuda:0')
c= tensor(9.2417e+08, device='cuda:0')
c= tensor(9.2423e+08, device='cuda:0')
c= tensor(9.2423e+08, device='cuda:0')
c= tensor(9.2424e+08, device='cuda:0')
c= tensor(9.2426e+08, device='cuda:0')
c= tensor(9.2438e+08, device='cuda:0')
c= tensor(9.2439e+08, device='cuda:0')
c= tensor(9.2440e+08, device='cuda:0')
c= tensor(9.2441e+08, device='cuda:0')
c= tensor(9.2441e+08, device='cuda:0')
c= tensor(9.2442e+08, device='cuda:0')
c= tensor(9.2442e+08, device='cuda:0')
c= tensor(9.2443e+08, device='cuda:0')
c= tensor(9.2444e+08, device='cuda:0')
c= tensor(9.2445e+08, device='cuda:0')
c= tensor(9.2446e+08, device='cuda:0')
c= tensor(9.2451e+08, device='cuda:0')
c= tensor(9.2452e+08, device='cuda:0')
c= tensor(9.2452e+08, device='cuda:0')
c= tensor(9.2455e+08, device='cuda:0')
c= tensor(9.2457e+08, device='cuda:0')
c= tensor(9.2458e+08, device='cuda:0')
c= tensor(9.2459e+08, device='cuda:0')
c= tensor(9.2462e+08, device='cuda:0')
c= tensor(9.2466e+08, device='cuda:0')
c= tensor(9.2467e+08, device='cuda:0')
c= tensor(9.2477e+08, device='cuda:0')
c= tensor(9.2479e+08, device='cuda:0')
c= tensor(9.2481e+08, device='cuda:0')
c= tensor(9.2482e+08, device='cuda:0')
c= tensor(9.2484e+08, device='cuda:0')
c= tensor(9.2485e+08, device='cuda:0')
c= tensor(9.2487e+08, device='cuda:0')
c= tensor(9.2487e+08, device='cuda:0')
c= tensor(9.2488e+08, device='cuda:0')
c= tensor(9.2489e+08, device='cuda:0')
c= tensor(9.2489e+08, device='cuda:0')
c= tensor(9.2490e+08, device='cuda:0')
c= tensor(9.2491e+08, device='cuda:0')
c= tensor(9.2492e+08, device='cuda:0')
c= tensor(9.2494e+08, device='cuda:0')
c= tensor(9.2496e+08, device='cuda:0')
c= tensor(9.2498e+08, device='cuda:0')
c= tensor(9.2498e+08, device='cuda:0')
c= tensor(9.2501e+08, device='cuda:0')
c= tensor(9.2502e+08, device='cuda:0')
c= tensor(9.2508e+08, device='cuda:0')
c= tensor(9.2509e+08, device='cuda:0')
c= tensor(9.2509e+08, device='cuda:0')
c= tensor(9.2510e+08, device='cuda:0')
c= tensor(9.2511e+08, device='cuda:0')
c= tensor(9.2511e+08, device='cuda:0')
c= tensor(9.2512e+08, device='cuda:0')
c= tensor(9.2516e+08, device='cuda:0')
c= tensor(9.2520e+08, device='cuda:0')
c= tensor(9.2520e+08, device='cuda:0')
c= tensor(9.2523e+08, device='cuda:0')
c= tensor(9.2523e+08, device='cuda:0')
c= tensor(9.2525e+08, device='cuda:0')
c= tensor(9.2525e+08, device='cuda:0')
c= tensor(9.2526e+08, device='cuda:0')
c= tensor(9.2527e+08, device='cuda:0')
c= tensor(9.2528e+08, device='cuda:0')
c= tensor(9.2529e+08, device='cuda:0')
c= tensor(9.2529e+08, device='cuda:0')
c= tensor(9.2529e+08, device='cuda:0')
c= tensor(9.2530e+08, device='cuda:0')
c= tensor(9.2530e+08, device='cuda:0')
c= tensor(9.2535e+08, device='cuda:0')
c= tensor(9.2543e+08, device='cuda:0')
c= tensor(9.2545e+08, device='cuda:0')
c= tensor(9.2546e+08, device='cuda:0')
c= tensor(9.2546e+08, device='cuda:0')
c= tensor(9.2547e+08, device='cuda:0')
c= tensor(9.2547e+08, device='cuda:0')
c= tensor(9.2548e+08, device='cuda:0')
c= tensor(9.2549e+08, device='cuda:0')
c= tensor(9.2550e+08, device='cuda:0')
c= tensor(9.2550e+08, device='cuda:0')
c= tensor(9.2557e+08, device='cuda:0')
c= tensor(9.2558e+08, device='cuda:0')
c= tensor(9.2575e+08, device='cuda:0')
c= tensor(9.2576e+08, device='cuda:0')
c= tensor(9.2577e+08, device='cuda:0')
c= tensor(9.2578e+08, device='cuda:0')
c= tensor(9.2578e+08, device='cuda:0')
c= tensor(9.2587e+08, device='cuda:0')
c= tensor(9.2587e+08, device='cuda:0')
c= tensor(9.2592e+08, device='cuda:0')
c= tensor(9.2592e+08, device='cuda:0')
c= tensor(9.2595e+08, device='cuda:0')
c= tensor(9.2596e+08, device='cuda:0')
c= tensor(9.2596e+08, device='cuda:0')
c= tensor(9.2598e+08, device='cuda:0')
c= tensor(9.2599e+08, device='cuda:0')
c= tensor(9.2599e+08, device='cuda:0')
c= tensor(9.2599e+08, device='cuda:0')
c= tensor(9.2599e+08, device='cuda:0')
c= tensor(9.2603e+08, device='cuda:0')
c= tensor(9.2603e+08, device='cuda:0')
c= tensor(9.2605e+08, device='cuda:0')
c= tensor(9.2683e+08, device='cuda:0')
c= tensor(9.2685e+08, device='cuda:0')
c= tensor(9.2685e+08, device='cuda:0')
c= tensor(9.2688e+08, device='cuda:0')
c= tensor(9.2689e+08, device='cuda:0')
c= tensor(9.2692e+08, device='cuda:0')
c= tensor(9.2693e+08, device='cuda:0')
c= tensor(9.2694e+08, device='cuda:0')
c= tensor(9.2695e+08, device='cuda:0')
c= tensor(9.2696e+08, device='cuda:0')
c= tensor(9.2698e+08, device='cuda:0')
c= tensor(9.2699e+08, device='cuda:0')
c= tensor(9.2702e+08, device='cuda:0')
c= tensor(9.2704e+08, device='cuda:0')
c= tensor(9.2709e+08, device='cuda:0')
c= tensor(9.2710e+08, device='cuda:0')
c= tensor(9.2710e+08, device='cuda:0')
c= tensor(9.2711e+08, device='cuda:0')
c= tensor(9.2713e+08, device='cuda:0')
c= tensor(9.2719e+08, device='cuda:0')
c= tensor(9.2720e+08, device='cuda:0')
c= tensor(9.2720e+08, device='cuda:0')
c= tensor(9.2721e+08, device='cuda:0')
c= tensor(9.2723e+08, device='cuda:0')
c= tensor(9.2723e+08, device='cuda:0')
c= tensor(9.2727e+08, device='cuda:0')
c= tensor(9.2728e+08, device='cuda:0')
c= tensor(9.2736e+08, device='cuda:0')
c= tensor(9.2737e+08, device='cuda:0')
c= tensor(9.2739e+08, device='cuda:0')
c= tensor(9.2739e+08, device='cuda:0')
c= tensor(9.2740e+08, device='cuda:0')
c= tensor(9.2742e+08, device='cuda:0')
c= tensor(9.2742e+08, device='cuda:0')
c= tensor(9.2746e+08, device='cuda:0')
c= tensor(9.2748e+08, device='cuda:0')
c= tensor(9.2750e+08, device='cuda:0')
c= tensor(9.2751e+08, device='cuda:0')
c= tensor(9.2752e+08, device='cuda:0')
c= tensor(9.2752e+08, device='cuda:0')
c= tensor(9.2753e+08, device='cuda:0')
c= tensor(9.2753e+08, device='cuda:0')
c= tensor(9.2753e+08, device='cuda:0')
c= tensor(9.2765e+08, device='cuda:0')
c= tensor(9.2766e+08, device='cuda:0')
c= tensor(9.2767e+08, device='cuda:0')
c= tensor(9.2767e+08, device='cuda:0')
c= tensor(9.2770e+08, device='cuda:0')
c= tensor(9.2771e+08, device='cuda:0')
c= tensor(9.2772e+08, device='cuda:0')
c= tensor(9.2773e+08, device='cuda:0')
c= tensor(9.2773e+08, device='cuda:0')
c= tensor(9.2774e+08, device='cuda:0')
c= tensor(9.2775e+08, device='cuda:0')
c= tensor(9.2776e+08, device='cuda:0')
c= tensor(9.2776e+08, device='cuda:0')
c= tensor(9.2778e+08, device='cuda:0')
c= tensor(9.2779e+08, device='cuda:0')
c= tensor(9.2781e+08, device='cuda:0')
c= tensor(9.2781e+08, device='cuda:0')
c= tensor(9.2783e+08, device='cuda:0')
c= tensor(9.2786e+08, device='cuda:0')
c= tensor(9.2788e+08, device='cuda:0')
c= tensor(9.2789e+08, device='cuda:0')
c= tensor(9.2823e+08, device='cuda:0')
c= tensor(9.3059e+08, device='cuda:0')
c= tensor(9.3064e+08, device='cuda:0')
c= tensor(9.3068e+08, device='cuda:0')
c= tensor(9.3068e+08, device='cuda:0')
c= tensor(9.3074e+08, device='cuda:0')
c= tensor(9.3109e+08, device='cuda:0')
c= tensor(9.4147e+08, device='cuda:0')
c= tensor(9.4148e+08, device='cuda:0')
c= tensor(9.5041e+08, device='cuda:0')
c= tensor(9.5995e+08, device='cuda:0')
c= tensor(9.6077e+08, device='cuda:0')
c= tensor(1.6827e+09, device='cuda:0')
c= tensor(1.6827e+09, device='cuda:0')
c= tensor(1.6827e+09, device='cuda:0')
c= tensor(1.6991e+09, device='cuda:0')
c= tensor(1.7706e+09, device='cuda:0')
c= tensor(1.7706e+09, device='cuda:0')
c= tensor(1.7714e+09, device='cuda:0')
c= tensor(1.7720e+09, device='cuda:0')
c= tensor(1.7757e+09, device='cuda:0')
c= tensor(1.7824e+09, device='cuda:0')
c= tensor(1.7874e+09, device='cuda:0')
c= tensor(1.7879e+09, device='cuda:0')
c= tensor(1.7882e+09, device='cuda:0')
c= tensor(1.7882e+09, device='cuda:0')
c= tensor(1.7991e+09, device='cuda:0')
c= tensor(1.7993e+09, device='cuda:0')
c= tensor(1.7993e+09, device='cuda:0')
c= tensor(1.7997e+09, device='cuda:0')
c= tensor(1.8028e+09, device='cuda:0')
c= tensor(1.8227e+09, device='cuda:0')
c= tensor(1.8253e+09, device='cuda:0')
c= tensor(1.8253e+09, device='cuda:0')
c= tensor(1.8259e+09, device='cuda:0')
c= tensor(1.8259e+09, device='cuda:0')
c= tensor(1.8270e+09, device='cuda:0')
c= tensor(1.8307e+09, device='cuda:0')
c= tensor(1.8318e+09, device='cuda:0')
c= tensor(1.8340e+09, device='cuda:0')
c= tensor(1.8340e+09, device='cuda:0')
c= tensor(1.8340e+09, device='cuda:0')
c= tensor(1.8370e+09, device='cuda:0')
c= tensor(1.8414e+09, device='cuda:0')
c= tensor(1.8437e+09, device='cuda:0')
c= tensor(1.8437e+09, device='cuda:0')
c= tensor(1.8805e+09, device='cuda:0')
c= tensor(1.8806e+09, device='cuda:0')
c= tensor(1.8811e+09, device='cuda:0')
c= tensor(1.8837e+09, device='cuda:0')
c= tensor(1.8837e+09, device='cuda:0')
c= tensor(1.8912e+09, device='cuda:0')
c= tensor(1.9236e+09, device='cuda:0')
c= tensor(2.0467e+09, device='cuda:0')
c= tensor(2.0474e+09, device='cuda:0')
c= tensor(2.0478e+09, device='cuda:0')
c= tensor(2.0478e+09, device='cuda:0')
c= tensor(2.0479e+09, device='cuda:0')
c= tensor(2.0584e+09, device='cuda:0')
c= tensor(2.0585e+09, device='cuda:0')
c= tensor(2.0596e+09, device='cuda:0')
c= tensor(2.0659e+09, device='cuda:0')
c= tensor(2.0665e+09, device='cuda:0')
c= tensor(2.0668e+09, device='cuda:0')
c= tensor(2.0668e+09, device='cuda:0')
c= tensor(2.0782e+09, device='cuda:0')
c= tensor(2.1030e+09, device='cuda:0')
c= tensor(2.1052e+09, device='cuda:0')
c= tensor(2.1053e+09, device='cuda:0')
c= tensor(2.1097e+09, device='cuda:0')
c= tensor(2.1098e+09, device='cuda:0')
c= tensor(2.1404e+09, device='cuda:0')
c= tensor(2.1404e+09, device='cuda:0')
c= tensor(2.1429e+09, device='cuda:0')
c= tensor(2.1431e+09, device='cuda:0')
c= tensor(2.1579e+09, device='cuda:0')
c= tensor(2.1596e+09, device='cuda:0')
c= tensor(2.1597e+09, device='cuda:0')
c= tensor(2.1653e+09, device='cuda:0')
c= tensor(2.1771e+09, device='cuda:0')
c= tensor(2.1798e+09, device='cuda:0')
c= tensor(2.1922e+09, device='cuda:0')
c= tensor(2.2093e+09, device='cuda:0')
c= tensor(2.2595e+09, device='cuda:0')
c= tensor(2.2601e+09, device='cuda:0')
c= tensor(2.2601e+09, device='cuda:0')
c= tensor(2.2602e+09, device='cuda:0')
c= tensor(2.2607e+09, device='cuda:0')
c= tensor(2.2610e+09, device='cuda:0')
c= tensor(2.2631e+09, device='cuda:0')
c= tensor(2.2631e+09, device='cuda:0')
c= tensor(2.2656e+09, device='cuda:0')
c= tensor(2.2734e+09, device='cuda:0')
c= tensor(2.2744e+09, device='cuda:0')
c= tensor(2.2745e+09, device='cuda:0')
c= tensor(2.2760e+09, device='cuda:0')
c= tensor(2.2761e+09, device='cuda:0')
c= tensor(2.2762e+09, device='cuda:0')
c= tensor(2.2763e+09, device='cuda:0')
c= tensor(2.2768e+09, device='cuda:0')
c= tensor(2.2842e+09, device='cuda:0')
c= tensor(2.2849e+09, device='cuda:0')
c= tensor(2.2853e+09, device='cuda:0')
c= tensor(2.2866e+09, device='cuda:0')
c= tensor(2.2866e+09, device='cuda:0')
c= tensor(2.7660e+09, device='cuda:0')
c= tensor(2.7661e+09, device='cuda:0')
c= tensor(2.7732e+09, device='cuda:0')
c= tensor(2.7732e+09, device='cuda:0')
c= tensor(2.7733e+09, device='cuda:0')
c= tensor(2.7733e+09, device='cuda:0')
c= tensor(2.7734e+09, device='cuda:0')
c= tensor(2.7734e+09, device='cuda:0')
c= tensor(2.7807e+09, device='cuda:0')
c= tensor(2.7807e+09, device='cuda:0')
c= tensor(2.7807e+09, device='cuda:0')
c= tensor(2.8016e+09, device='cuda:0')
c= tensor(2.8029e+09, device='cuda:0')
c= tensor(2.8037e+09, device='cuda:0')
c= tensor(2.8098e+09, device='cuda:0')
c= tensor(2.8383e+09, device='cuda:0')
c= tensor(2.8383e+09, device='cuda:0')
c= tensor(2.8384e+09, device='cuda:0')
c= tensor(2.8387e+09, device='cuda:0')
c= tensor(2.8387e+09, device='cuda:0')
c= tensor(2.8387e+09, device='cuda:0')
c= tensor(2.8388e+09, device='cuda:0')
c= tensor(2.8389e+09, device='cuda:0')
c= tensor(2.8389e+09, device='cuda:0')
c= tensor(2.8389e+09, device='cuda:0')
c= tensor(2.8389e+09, device='cuda:0')
c= tensor(2.8857e+09, device='cuda:0')
c= tensor(2.8858e+09, device='cuda:0')
c= tensor(2.8884e+09, device='cuda:0')
c= tensor(2.8884e+09, device='cuda:0')
c= tensor(2.8884e+09, device='cuda:0')
c= tensor(2.8908e+09, device='cuda:0')
c= tensor(3.0302e+09, device='cuda:0')
c= tensor(3.1008e+09, device='cuda:0')
c= tensor(3.1010e+09, device='cuda:0')
c= tensor(3.1017e+09, device='cuda:0')
c= tensor(3.1017e+09, device='cuda:0')
c= tensor(3.1024e+09, device='cuda:0')
c= tensor(3.3335e+09, device='cuda:0')
c= tensor(3.3339e+09, device='cuda:0')
c= tensor(3.3339e+09, device='cuda:0')
c= tensor(3.3355e+09, device='cuda:0')
c= tensor(3.4978e+09, device='cuda:0')
c= tensor(3.4985e+09, device='cuda:0')
c= tensor(3.4987e+09, device='cuda:0')
c= tensor(3.4989e+09, device='cuda:0')
c= tensor(3.4993e+09, device='cuda:0')
c= tensor(3.4994e+09, device='cuda:0')
c= tensor(3.5130e+09, device='cuda:0')
c= tensor(3.5132e+09, device='cuda:0')
c= tensor(3.5132e+09, device='cuda:0')
c= tensor(3.5170e+09, device='cuda:0')
c= tensor(3.5172e+09, device='cuda:0')
c= tensor(3.5172e+09, device='cuda:0')
c= tensor(3.5187e+09, device='cuda:0')
c= tensor(3.5226e+09, device='cuda:0')
c= tensor(3.5583e+09, device='cuda:0')
c= tensor(3.5797e+09, device='cuda:0')
c= tensor(3.5945e+09, device='cuda:0')
c= tensor(3.5948e+09, device='cuda:0')
c= tensor(3.5953e+09, device='cuda:0')
c= tensor(3.5981e+09, device='cuda:0')
c= tensor(3.6148e+09, device='cuda:0')
c= tensor(3.6149e+09, device='cuda:0')
c= tensor(3.7520e+09, device='cuda:0')
c= tensor(4.0932e+09, device='cuda:0')
c= tensor(4.1123e+09, device='cuda:0')
c= tensor(4.1165e+09, device='cuda:0')
c= tensor(4.1192e+09, device='cuda:0')
c= tensor(4.1193e+09, device='cuda:0')
c= tensor(4.1193e+09, device='cuda:0')
c= tensor(4.1196e+09, device='cuda:0')
c= tensor(4.1245e+09, device='cuda:0')
c= tensor(4.1292e+09, device='cuda:0')
c= tensor(4.1694e+09, device='cuda:0')
c= tensor(4.1751e+09, device='cuda:0')
c= tensor(4.1782e+09, device='cuda:0')
c= tensor(4.1786e+09, device='cuda:0')
c= tensor(4.1884e+09, device='cuda:0')
c= tensor(4.1884e+09, device='cuda:0')
c= tensor(4.1885e+09, device='cuda:0')
c= tensor(4.1959e+09, device='cuda:0')
c= tensor(4.1969e+09, device='cuda:0')
c= tensor(4.1969e+09, device='cuda:0')
c= tensor(4.1972e+09, device='cuda:0')
c= tensor(4.3743e+09, device='cuda:0')
c= tensor(4.3746e+09, device='cuda:0')
c= tensor(4.3812e+09, device='cuda:0')
c= tensor(4.3817e+09, device='cuda:0')
c= tensor(4.3819e+09, device='cuda:0')
c= tensor(4.3819e+09, device='cuda:0')
c= tensor(4.3820e+09, device='cuda:0')
c= tensor(4.3822e+09, device='cuda:0')
c= tensor(4.3833e+09, device='cuda:0')
c= tensor(4.3833e+09, device='cuda:0')
c= tensor(4.3906e+09, device='cuda:0')
c= tensor(4.3906e+09, device='cuda:0')
c= tensor(4.3953e+09, device='cuda:0')
c= tensor(4.3955e+09, device='cuda:0')
c= tensor(4.3983e+09, device='cuda:0')
c= tensor(4.3983e+09, device='cuda:0')
c= tensor(4.3986e+09, device='cuda:0')
c= tensor(4.3990e+09, device='cuda:0')
c= tensor(4.3994e+09, device='cuda:0')
c= tensor(4.4023e+09, device='cuda:0')
c= tensor(4.5247e+09, device='cuda:0')
c= tensor(4.5247e+09, device='cuda:0')
c= tensor(4.5247e+09, device='cuda:0')
c= tensor(4.5334e+09, device='cuda:0')
c= tensor(4.5336e+09, device='cuda:0')
c= tensor(4.5983e+09, device='cuda:0')
c= tensor(4.5983e+09, device='cuda:0')
c= tensor(4.6038e+09, device='cuda:0')
c= tensor(4.6266e+09, device='cuda:0')
c= tensor(4.6266e+09, device='cuda:0')
c= tensor(4.6493e+09, device='cuda:0')
c= tensor(4.6500e+09, device='cuda:0')
c= tensor(4.7436e+09, device='cuda:0')
c= tensor(4.7436e+09, device='cuda:0')
c= tensor(4.7440e+09, device='cuda:0')
c= tensor(4.7442e+09, device='cuda:0')
c= tensor(4.7443e+09, device='cuda:0')
c= tensor(4.7443e+09, device='cuda:0')
c= tensor(4.7480e+09, device='cuda:0')
c= tensor(4.7480e+09, device='cuda:0')
c= tensor(4.7570e+09, device='cuda:0')
c= tensor(4.7571e+09, device='cuda:0')
c= tensor(4.7571e+09, device='cuda:0')
c= tensor(4.7573e+09, device='cuda:0')
c= tensor(4.7761e+09, device='cuda:0')
c= tensor(4.7822e+09, device='cuda:0')
c= tensor(4.8095e+09, device='cuda:0')
c= tensor(4.8101e+09, device='cuda:0')
c= tensor(4.8102e+09, device='cuda:0')
c= tensor(4.8102e+09, device='cuda:0')
c= tensor(4.8102e+09, device='cuda:0')
c= tensor(4.8776e+09, device='cuda:0')
c= tensor(4.8776e+09, device='cuda:0')
c= tensor(4.8780e+09, device='cuda:0')
c= tensor(4.8795e+09, device='cuda:0')
c= tensor(4.8806e+09, device='cuda:0')
c= tensor(4.8809e+09, device='cuda:0')
c= tensor(4.8809e+09, device='cuda:0')
c= tensor(4.9006e+09, device='cuda:0')
c= tensor(4.9014e+09, device='cuda:0')
c= tensor(4.9016e+09, device='cuda:0')
c= tensor(4.9029e+09, device='cuda:0')
c= tensor(4.9132e+09, device='cuda:0')
c= tensor(4.9171e+09, device='cuda:0')
c= tensor(4.9760e+09, device='cuda:0')
c= tensor(4.9803e+09, device='cuda:0')
c= tensor(4.9804e+09, device='cuda:0')
c= tensor(4.9811e+09, device='cuda:0')
c= tensor(4.9825e+09, device='cuda:0')
c= tensor(4.9826e+09, device='cuda:0')
c= tensor(4.9827e+09, device='cuda:0')
c= tensor(4.9827e+09, device='cuda:0')
c= tensor(4.9841e+09, device='cuda:0')
c= tensor(4.9846e+09, device='cuda:0')
c= tensor(4.9846e+09, device='cuda:0')
c= tensor(4.9848e+09, device='cuda:0')
c= tensor(4.9848e+09, device='cuda:0')
c= tensor(4.9857e+09, device='cuda:0')
c= tensor(4.9858e+09, device='cuda:0')
c= tensor(4.9862e+09, device='cuda:0')
c= tensor(4.9868e+09, device='cuda:0')
c= tensor(4.9870e+09, device='cuda:0')
c= tensor(4.9870e+09, device='cuda:0')
c= tensor(4.9871e+09, device='cuda:0')
c= tensor(4.9874e+09, device='cuda:0')
c= tensor(4.9998e+09, device='cuda:0')
c= tensor(4.9998e+09, device='cuda:0')
c= tensor(4.9998e+09, device='cuda:0')
c= tensor(4.9999e+09, device='cuda:0')
c= tensor(5.0076e+09, device='cuda:0')
c= tensor(5.1280e+09, device='cuda:0')
c= tensor(5.1321e+09, device='cuda:0')
c= tensor(5.1321e+09, device='cuda:0')
c= tensor(5.1469e+09, device='cuda:0')
c= tensor(5.1537e+09, device='cuda:0')
c= tensor(5.1538e+09, device='cuda:0')
c= tensor(5.1539e+09, device='cuda:0')
c= tensor(5.1542e+09, device='cuda:0')
c= tensor(5.1695e+09, device='cuda:0')
c= tensor(5.2209e+09, device='cuda:0')
c= tensor(5.2314e+09, device='cuda:0')
c= tensor(5.2317e+09, device='cuda:0')
c= tensor(5.2317e+09, device='cuda:0')
c= tensor(5.2317e+09, device='cuda:0')
c= tensor(5.2340e+09, device='cuda:0')
c= tensor(5.2341e+09, device='cuda:0')
c= tensor(5.2353e+09, device='cuda:0')
c= tensor(5.2420e+09, device='cuda:0')
c= tensor(5.3075e+09, device='cuda:0')
c= tensor(5.3076e+09, device='cuda:0')
c= tensor(5.3076e+09, device='cuda:0')
c= tensor(5.3083e+09, device='cuda:0')
c= tensor(5.3281e+09, device='cuda:0')
c= tensor(5.3293e+09, device='cuda:0')
c= tensor(5.3294e+09, device='cuda:0')
c= tensor(5.3294e+09, device='cuda:0')
c= tensor(5.3298e+09, device='cuda:0')
c= tensor(5.3298e+09, device='cuda:0')
c= tensor(5.3314e+09, device='cuda:0')
c= tensor(5.3314e+09, device='cuda:0')
c= tensor(5.3315e+09, device='cuda:0')
c= tensor(5.3316e+09, device='cuda:0')
c= tensor(5.3316e+09, device='cuda:0')
c= tensor(5.3317e+09, device='cuda:0')
c= tensor(5.3402e+09, device='cuda:0')
c= tensor(5.3748e+09, device='cuda:0')
c= tensor(5.3804e+09, device='cuda:0')
c= tensor(5.3895e+09, device='cuda:0')
c= tensor(5.3897e+09, device='cuda:0')
c= tensor(5.3898e+09, device='cuda:0')
c= tensor(5.3901e+09, device='cuda:0')
c= tensor(5.3951e+09, device='cuda:0')
c= tensor(5.3957e+09, device='cuda:0')
c= tensor(5.3984e+09, device='cuda:0')
c= tensor(5.3985e+09, device='cuda:0')
c= tensor(5.8310e+09, device='cuda:0')
c= tensor(5.8311e+09, device='cuda:0')
c= tensor(5.8326e+09, device='cuda:0')
c= tensor(5.8509e+09, device='cuda:0')
c= tensor(5.8522e+09, device='cuda:0')
c= tensor(5.8531e+09, device='cuda:0')
c= tensor(6.0408e+09, device='cuda:0')
c= tensor(6.0491e+09, device='cuda:0')
c= tensor(6.0516e+09, device='cuda:0')
c= tensor(6.0517e+09, device='cuda:0')
c= tensor(6.0527e+09, device='cuda:0')
c= tensor(6.0528e+09, device='cuda:0')
c= tensor(6.0721e+09, device='cuda:0')
c= tensor(6.4843e+09, device='cuda:0')
c= tensor(6.4917e+09, device='cuda:0')
c= tensor(6.5002e+09, device='cuda:0')
c= tensor(6.5071e+09, device='cuda:0')
c= tensor(6.5075e+09, device='cuda:0')
c= tensor(6.5079e+09, device='cuda:0')
c= tensor(6.5080e+09, device='cuda:0')
c= tensor(6.5407e+09, device='cuda:0')
c= tensor(6.5953e+09, device='cuda:0')
c= tensor(6.5989e+09, device='cuda:0')
c= tensor(7.0219e+09, device='cuda:0')
c= tensor(7.0347e+09, device='cuda:0')
c= tensor(7.0353e+09, device='cuda:0')
c= tensor(7.0354e+09, device='cuda:0')
c= tensor(7.0411e+09, device='cuda:0')
c= tensor(7.0472e+09, device='cuda:0')
c= tensor(7.0472e+09, device='cuda:0')
c= tensor(7.1268e+09, device='cuda:0')
c= tensor(7.1275e+09, device='cuda:0')
c= tensor(7.1281e+09, device='cuda:0')
c= tensor(7.1282e+09, device='cuda:0')
c= tensor(7.1282e+09, device='cuda:0')
c= tensor(7.1282e+09, device='cuda:0')
c= tensor(7.1282e+09, device='cuda:0')
c= tensor(7.1285e+09, device='cuda:0')
c= tensor(7.1344e+09, device='cuda:0')
c= tensor(9.7371e+09, device='cuda:0')
c= tensor(9.7379e+09, device='cuda:0')
c= tensor(9.7452e+09, device='cuda:0')
c= tensor(9.7453e+09, device='cuda:0')
c= tensor(9.7455e+09, device='cuda:0')
c= tensor(9.7455e+09, device='cuda:0')
c= tensor(9.7621e+09, device='cuda:0')
c= tensor(9.7650e+09, device='cuda:0')
c= tensor(1.0035e+10, device='cuda:0')
c= tensor(1.0035e+10, device='cuda:0')
c= tensor(1.0043e+10, device='cuda:0')
c= tensor(1.0044e+10, device='cuda:0')
c= tensor(1.0053e+10, device='cuda:0')
c= tensor(1.0079e+10, device='cuda:0')
c= tensor(1.0079e+10, device='cuda:0')
c= tensor(1.0079e+10, device='cuda:0')
c= tensor(1.0083e+10, device='cuda:0')
c= tensor(1.0083e+10, device='cuda:0')
c= tensor(1.0085e+10, device='cuda:0')
c= tensor(1.0103e+10, device='cuda:0')
c= tensor(1.0105e+10, device='cuda:0')
c= tensor(1.0108e+10, device='cuda:0')
c= tensor(1.0109e+10, device='cuda:0')
c= tensor(1.0116e+10, device='cuda:0')
c= tensor(1.0140e+10, device='cuda:0')
c= tensor(1.0141e+10, device='cuda:0')
c= tensor(1.0141e+10, device='cuda:0')
c= tensor(1.0150e+10, device='cuda:0')
c= tensor(1.0153e+10, device='cuda:0')
c= tensor(1.0261e+10, device='cuda:0')
c= tensor(1.0261e+10, device='cuda:0')
c= tensor(1.0263e+10, device='cuda:0')
c= tensor(1.0263e+10, device='cuda:0')
c= tensor(1.0290e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0304e+10, device='cuda:0')
c= tensor(1.0310e+10, device='cuda:0')
c= tensor(1.0311e+10, device='cuda:0')
c= tensor(1.0311e+10, device='cuda:0')
c= tensor(1.0312e+10, device='cuda:0')
c= tensor(1.0313e+10, device='cuda:0')
c= tensor(1.0314e+10, device='cuda:0')
c= tensor(1.0315e+10, device='cuda:0')
c= tensor(1.0316e+10, device='cuda:0')
c= tensor(1.0317e+10, device='cuda:0')
c= tensor(1.0318e+10, device='cuda:0')
c= tensor(1.0318e+10, device='cuda:0')
c= tensor(1.0318e+10, device='cuda:0')
c= tensor(1.0330e+10, device='cuda:0')
c= tensor(1.0330e+10, device='cuda:0')
c= tensor(1.0334e+10, device='cuda:0')
c= tensor(1.0334e+10, device='cuda:0')
c= tensor(1.0334e+10, device='cuda:0')
c= tensor(1.0336e+10, device='cuda:0')
c= tensor(1.0337e+10, device='cuda:0')
c= tensor(1.0338e+10, device='cuda:0')
c= tensor(1.0338e+10, device='cuda:0')
c= tensor(1.0338e+10, device='cuda:0')
c= tensor(1.0338e+10, device='cuda:0')
c= tensor(1.0339e+10, device='cuda:0')
c= tensor(1.0346e+10, device='cuda:0')
c= tensor(1.0346e+10, device='cuda:0')
c= tensor(1.0346e+10, device='cuda:0')
c= tensor(1.0347e+10, device='cuda:0')
c= tensor(1.0347e+10, device='cuda:0')
c= tensor(1.0374e+10, device='cuda:0')
c= tensor(1.0374e+10, device='cuda:0')
c= tensor(1.0382e+10, device='cuda:0')
c= tensor(1.0388e+10, device='cuda:0')
c= tensor(1.0393e+10, device='cuda:0')
c= tensor(1.0400e+10, device='cuda:0')
c= tensor(1.0405e+10, device='cuda:0')
c= tensor(1.0405e+10, device='cuda:0')
c= tensor(1.0406e+10, device='cuda:0')
c= tensor(1.0406e+10, device='cuda:0')
c= tensor(1.0413e+10, device='cuda:0')
c= tensor(1.0418e+10, device='cuda:0')
c= tensor(1.0419e+10, device='cuda:0')
c= tensor(1.0421e+10, device='cuda:0')
c= tensor(1.0421e+10, device='cuda:0')
c= tensor(1.0425e+10, device='cuda:0')
c= tensor(1.0425e+10, device='cuda:0')
c= tensor(1.0425e+10, device='cuda:0')
c= tensor(1.0444e+10, device='cuda:0')
c= tensor(1.0535e+10, device='cuda:0')
c= tensor(1.0535e+10, device='cuda:0')
c= tensor(1.0536e+10, device='cuda:0')
c= tensor(1.0536e+10, device='cuda:0')
c= tensor(1.0552e+10, device='cuda:0')
c= tensor(1.0553e+10, device='cuda:0')
c= tensor(1.0553e+10, device='cuda:0')
c= tensor(1.0555e+10, device='cuda:0')
c= tensor(1.0563e+10, device='cuda:0')
c= tensor(1.0563e+10, device='cuda:0')
c= tensor(1.0564e+10, device='cuda:0')
c= tensor(1.0565e+10, device='cuda:0')
time to make c is 7.472679853439331
time for making loss is 7.472711801528931
p0 True
it  0 : 3365416960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6124085248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
6124163072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 19% |
error is  2080631800.0
relative error loss 0.19693379
shape of L is 
torch.Size([])
memory (bytes)
6149853184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6149971968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2067071500.0
relative error loss 0.1956503
shape of L is 
torch.Size([])
memory (bytes)
6153400320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6153400320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2011892700.0
relative error loss 0.19042757
shape of L is 
torch.Size([])
memory (bytes)
6156718080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6156775424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1964624900.0
relative error loss 0.18595363
shape of L is 
torch.Size([])
memory (bytes)
6159941632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
6159941632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1917626400.0
relative error loss 0.18150517
shape of L is 
torch.Size([])
memory (bytes)
6162997248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
6163181568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1897000000.0
relative error loss 0.17955287
shape of L is 
torch.Size([])
memory (bytes)
6166228992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6166380544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1880833000.0
relative error loss 0.17802265
shape of L is 
torch.Size([])
memory (bytes)
6169427968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6169595904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1868517400.0
relative error loss 0.17685696
shape of L is 
torch.Size([])
memory (bytes)
6172798976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6172798976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1858092000.0
relative error loss 0.1758702
shape of L is 
torch.Size([])
memory (bytes)
6175911936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6175911936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1847389200.0
relative error loss 0.17485715
time to take a step is 231.54663848876953
it  1 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6179115008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
6179205120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1847389200.0
relative error loss 0.17485715
shape of L is 
torch.Size([])
memory (bytes)
6182354944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6182354944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1838004200.0
relative error loss 0.17396887
shape of L is 
torch.Size([])
memory (bytes)
6185660416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6185660416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 19% |
error is  1833133000.0
relative error loss 0.1735078
shape of L is 
torch.Size([])
memory (bytes)
6188781568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6188781568
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 99% | 19% |
error is  1828069400.0
relative error loss 0.17302851
shape of L is 
torch.Size([])
memory (bytes)
6192017408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6192082944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 19% |
error is  1822421000.0
relative error loss 0.17249389
shape of L is 
torch.Size([])
memory (bytes)
6195175424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6195175424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1819805700.0
relative error loss 0.17224635
shape of L is 
torch.Size([])
memory (bytes)
6198439936
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6198509568
| ID | GPU  | MEM |
-------------------
|  0 |  10% |  0% |
|  1 | 100% | 19% |
error is  1814841300.0
relative error loss 0.17177647
shape of L is 
torch.Size([])
memory (bytes)
6201667584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6201667584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1812374500.0
relative error loss 0.17154299
shape of L is 
torch.Size([])
memory (bytes)
6204932096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
6204932096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1810339800.0
relative error loss 0.1713504
shape of L is 
torch.Size([])
memory (bytes)
6208139264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6208139264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1807158300.0
relative error loss 0.17104927
time to take a step is 228.2437539100647
it  2 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6211366912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6211366912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 19% |
error is  1807158300.0
relative error loss 0.17104927
shape of L is 
torch.Size([])
memory (bytes)
6214549504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6214549504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1804321800.0
relative error loss 0.1707808
shape of L is 
torch.Size([])
memory (bytes)
6217633792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6217781248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1802865700.0
relative error loss 0.17064296
shape of L is 
torch.Size([])
memory (bytes)
6220955648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6220955648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1801059300.0
relative error loss 0.170472
shape of L is 
torch.Size([])
memory (bytes)
6224187392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6224187392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1799077900.0
relative error loss 0.17028445
shape of L is 
torch.Size([])
memory (bytes)
6227283968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
6227283968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1797270500.0
relative error loss 0.17011338
shape of L is 
torch.Size([])
memory (bytes)
6230614016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6230614016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1795876900.0
relative error loss 0.16998146
shape of L is 
torch.Size([])
memory (bytes)
6233731072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
6233731072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1794524200.0
relative error loss 0.16985343
shape of L is 
torch.Size([])
memory (bytes)
6236893184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 19% |
memory (bytes)
6237032448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1792757800.0
relative error loss 0.16968624
shape of L is 
torch.Size([])
memory (bytes)
6240141312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6240239616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1791952900.0
relative error loss 0.16961005
time to take a step is 227.56497931480408
it  3 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6243270656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
6243450880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1791952900.0
relative error loss 0.16961005
shape of L is 
torch.Size([])
memory (bytes)
6246662144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6246662144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1790940200.0
relative error loss 0.16951421
shape of L is 
torch.Size([])
memory (bytes)
6249791488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6249791488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1790298100.0
relative error loss 0.16945343
shape of L is 
torch.Size([])
memory (bytes)
6253088768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6253088768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1789363200.0
relative error loss 0.16936494
shape of L is 
torch.Size([])
memory (bytes)
6256291840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6256291840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1788204000.0
relative error loss 0.16925523
shape of L is 
torch.Size([])
memory (bytes)
6259425280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 19% |
memory (bytes)
6259425280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1787150300.0
relative error loss 0.1691555
shape of L is 
torch.Size([])
memory (bytes)
6262624256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6262624256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1786524700.0
relative error loss 0.16909628
shape of L is 
torch.Size([])
memory (bytes)
6265835520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6265921536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1785491500.0
relative error loss 0.16899848
shape of L is 
torch.Size([])
memory (bytes)
6269083648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
6269083648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1784814600.0
relative error loss 0.16893442
shape of L is 
torch.Size([])
memory (bytes)
6272245760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 19% |
memory (bytes)
6272331776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1784161300.0
relative error loss 0.16887258
time to take a step is 228.3954312801361
it  4 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6275416064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
6275416064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1784161300.0
relative error loss 0.16887258
shape of L is 
torch.Size([])
memory (bytes)
6278615040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 19% |
memory (bytes)
6278750208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1783564300.0
relative error loss 0.16881607
shape of L is 
torch.Size([])
memory (bytes)
6281818112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
6281818112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1783163900.0
relative error loss 0.16877817
shape of L is 
torch.Size([])
memory (bytes)
6285082624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6285172736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1782442000.0
relative error loss 0.16870984
shape of L is 
torch.Size([])
memory (bytes)
6288195584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6288195584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1781703700.0
relative error loss 0.16863996
shape of L is 
torch.Size([])
memory (bytes)
6291599360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6291599360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 19% |
error is  1781358600.0
relative error loss 0.1686073
shape of L is 
torch.Size([])
memory (bytes)
6294794240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6294794240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1780882400.0
relative error loss 0.16856223
shape of L is 
torch.Size([])
memory (bytes)
6298021888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6298021888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1780201500.0
relative error loss 0.16849777
shape of L is 
torch.Size([])
memory (bytes)
6301061120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6301061120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1779930100.0
relative error loss 0.1684721
shape of L is 
torch.Size([])
memory (bytes)
6304448512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6304448512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1779381200.0
relative error loss 0.16842014
time to take a step is 228.48173642158508
it  5 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6307610624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6307610624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1779381200.0
relative error loss 0.16842014
shape of L is 
torch.Size([])
memory (bytes)
6310862848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6310862848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1779133400.0
relative error loss 0.16839668
shape of L is 
torch.Size([])
memory (bytes)
6313914368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
6313914368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1778602000.0
relative error loss 0.16834639
shape of L is 
torch.Size([])
memory (bytes)
6317281280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
6317281280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1778348000.0
relative error loss 0.16832234
shape of L is 
torch.Size([])
memory (bytes)
6320427008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6320427008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1777828900.0
relative error loss 0.16827321
shape of L is 
torch.Size([])
memory (bytes)
6323519488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 19% |
memory (bytes)
6323699712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1777494000.0
relative error loss 0.16824152
shape of L is 
torch.Size([])
memory (bytes)
6326829056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 19% |
memory (bytes)
6326829056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1777153000.0
relative error loss 0.16820924
shape of L is 
torch.Size([])
memory (bytes)
6330048512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6330048512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1776653300.0
relative error loss 0.16816194
shape of L is 
torch.Size([])
memory (bytes)
6333325312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6333325312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1776571400.0
relative error loss 0.16815418
shape of L is 
torch.Size([])
memory (bytes)
6336368640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6336528384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1776093200.0
relative error loss 0.16810893
time to take a step is 227.89556312561035
it  6 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6339735552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6339735552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1776093200.0
relative error loss 0.16810893
shape of L is 
torch.Size([])
memory (bytes)
6342938624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6342942720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1775948800.0
relative error loss 0.16809526
shape of L is 
torch.Size([])
memory (bytes)
6346002432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
6346162176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1775558700.0
relative error loss 0.16805834
shape of L is 
torch.Size([])
memory (bytes)
6349295616
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6349295616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1775260700.0
relative error loss 0.16803013
shape of L is 
torch.Size([])
memory (bytes)
6352572416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6352572416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1774920700.0
relative error loss 0.16799794
shape of L is 
torch.Size([])
memory (bytes)
6355718144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6355718144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 19% |
error is  1774607400.0
relative error loss 0.16796829
shape of L is 
torch.Size([])
memory (bytes)
6358851584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6358986752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1774349300.0
relative error loss 0.16794387
shape of L is 
torch.Size([])
memory (bytes)
6362071040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
6362071040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1774128100.0
relative error loss 0.16792293
shape of L is 
torch.Size([])
memory (bytes)
6365319168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6365401088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1773962200.0
relative error loss 0.16790722
shape of L is 
torch.Size([])
memory (bytes)
6368604160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6368604160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1773754400.0
relative error loss 0.16788755
time to take a step is 227.4596185684204
it  7 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6371663872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6371823616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1773754400.0
relative error loss 0.16788755
shape of L is 
torch.Size([])
memory (bytes)
6374957056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6374957056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1773556700.0
relative error loss 0.16786885
shape of L is 
torch.Size([])
memory (bytes)
6378192896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
6378233856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1773393900.0
relative error loss 0.16785343
shape of L is 
torch.Size([])
memory (bytes)
6381404160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
6381404160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1773197300.0
relative error loss 0.16783483
shape of L is 
torch.Size([])
memory (bytes)
6384656384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6384656384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1773058000.0
relative error loss 0.16782165
shape of L is 
torch.Size([])
memory (bytes)
6387757056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
6387757056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1772909600.0
relative error loss 0.1678076
shape of L is 
torch.Size([])
memory (bytes)
6391078912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
6391078912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1772792800.0
relative error loss 0.16779654
shape of L is 
torch.Size([])
memory (bytes)
6394122240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6394122240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1772560400.0
relative error loss 0.16777454
shape of L is 
torch.Size([])
memory (bytes)
6397427712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6397427712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1772426200.0
relative error loss 0.16776185
shape of L is 
torch.Size([])
memory (bytes)
6400671744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6400671744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 19% |
error is  1772221400.0
relative error loss 0.16774246
time to take a step is 222.87721371650696
it  8 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6403670016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
6403854336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1772221400.0
relative error loss 0.16774246
shape of L is 
torch.Size([])
memory (bytes)
6407106560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 19% |
memory (bytes)
6407106560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1771992000.0
relative error loss 0.16772075
shape of L is 
torch.Size([])
memory (bytes)
6410326016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
6410326016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1772037100.0
relative error loss 0.16772501
shape of L is 
torch.Size([])
memory (bytes)
6413467648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% | 19% |
memory (bytes)
6413467648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1771855900.0
relative error loss 0.16770786
shape of L is 
torch.Size([])
memory (bytes)
6416633856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 44% | 19% |
memory (bytes)
6416633856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1771734000.0
relative error loss 0.16769633
shape of L is 
torch.Size([])
memory (bytes)
6419812352
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 30% | 19% |
memory (bytes)
6419972096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1771633700.0
relative error loss 0.16768682
shape of L is 
torch.Size([])
memory (bytes)
6423044096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6423044096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1771419600.0
relative error loss 0.16766657
shape of L is 
torch.Size([])
memory (bytes)
6426296320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6426296320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1771292700.0
relative error loss 0.16765454
shape of L is 
torch.Size([])
memory (bytes)
6429597696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6429597696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1771183100.0
relative error loss 0.16764417
shape of L is 
torch.Size([])
memory (bytes)
6432698368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 42% | 19% |
memory (bytes)
6432698368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1771077600.0
relative error loss 0.16763419
time to take a step is 146.05190706253052
it  9 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6435926016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6435926016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1771077600.0
relative error loss 0.16763419
shape of L is 
torch.Size([])
memory (bytes)
6439215104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
6439215104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1770920000.0
relative error loss 0.16761927
shape of L is 
torch.Size([])
memory (bytes)
6442315776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6442315776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1770742800.0
relative error loss 0.16760251
shape of L is 
torch.Size([])
memory (bytes)
6445461504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 45% | 19% |
memory (bytes)
6445461504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1770570800.0
relative error loss 0.16758622
shape of L is 
torch.Size([])
memory (bytes)
6448828416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6448840704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1770470400.0
relative error loss 0.16757672
shape of L is 
torch.Size([])
memory (bytes)
6451941376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
6451941376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1770312700.0
relative error loss 0.1675618
shape of L is 
torch.Size([])
memory (bytes)
6455050240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
6455263232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1770195000.0
relative error loss 0.16755065
shape of L is 
torch.Size([])
memory (bytes)
6458441728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 42% | 19% |
memory (bytes)
6458441728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1770036200.0
relative error loss 0.16753563
shape of L is 
torch.Size([])
memory (bytes)
6461673472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6461673472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1769811000.0
relative error loss 0.16751431
shape of L is 
torch.Size([])
memory (bytes)
6464868352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
6464868352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1769702400.0
relative error loss 0.16750403
time to take a step is 143.08345699310303
it  10 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6467907584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6468108288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1769702400.0
relative error loss 0.16750403
shape of L is 
torch.Size([])
memory (bytes)
6471331840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
6471331840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1769516000.0
relative error loss 0.16748638
shape of L is 
torch.Size([])
memory (bytes)
6474350592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 45% | 19% |
memory (bytes)
6474350592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1769357300.0
relative error loss 0.16747136
shape of L is 
torch.Size([])
memory (bytes)
6477590528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
6477758464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 19% |
error is  1769174000.0
relative error loss 0.16745402
shape of L is 
torch.Size([])
memory (bytes)
6480961536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
6480961536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1769073700.0
relative error loss 0.16744451
shape of L is 
torch.Size([])
memory (bytes)
6484041728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
6484041728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 19% |
error is  1768973300.0
relative error loss 0.16743502
shape of L is 
torch.Size([])
memory (bytes)
6487224320
| ID | GPU | MEM |
------------------
|  0 | 24% |  0% |
|  1 | 35% | 19% |
memory (bytes)
6487224320
| ID | GPU  | MEM |
-------------------
|  0 |  21% |  0% |
|  1 | 100% | 19% |
error is  1768861700.0
relative error loss 0.16742446
shape of L is 
torch.Size([])
memory (bytes)
6490591232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6490591232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1768757200.0
relative error loss 0.16741458
shape of L is 
torch.Size([])
memory (bytes)
6493802496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6493802496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 19% |
error is  1768649700.0
relative error loss 0.1674044
shape of L is 
torch.Size([])
memory (bytes)
6496997376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
6496997376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1768526800.0
relative error loss 0.16739276
time to take a step is 143.2298436164856
it  11 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6500212736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 19% |
memory (bytes)
6500212736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1768526800.0
relative error loss 0.16739276
shape of L is 
torch.Size([])
memory (bytes)
6503288832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6503288832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1768441900.0
relative error loss 0.16738471
shape of L is 
torch.Size([])
memory (bytes)
6506418176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 39% | 19% |
memory (bytes)
6506418176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1768261600.0
relative error loss 0.16736765
shape of L is 
torch.Size([])
memory (bytes)
6509842432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
6509842432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1768272900.0
relative error loss 0.16736872
shape of L is 
torch.Size([])
memory (bytes)
6513000448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6513000448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1768175600.0
relative error loss 0.16735952
shape of L is 
torch.Size([])
memory (bytes)
6516097024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6516260864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 19% |
error is  1768095700.0
relative error loss 0.16735196
shape of L is 
torch.Size([])
memory (bytes)
6519349248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6519472128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767941100.0
relative error loss 0.16733733
shape of L is 
torch.Size([])
memory (bytes)
6522560512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6522675200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767875600.0
relative error loss 0.16733111
shape of L is 
torch.Size([])
memory (bytes)
6525796352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6525878272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767784400.0
relative error loss 0.16732249
shape of L is 
torch.Size([])
memory (bytes)
6528950272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
6529101824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767728100.0
relative error loss 0.16731717
time to take a step is 203.82850122451782
it  12 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6532304896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
6532308992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767728100.0
relative error loss 0.16731717
shape of L is 
torch.Size([])
memory (bytes)
6535462912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6535462912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767673900.0
relative error loss 0.16731203
shape of L is 
torch.Size([])
memory (bytes)
6538731520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6538731520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767546900.0
relative error loss 0.1673
shape of L is 
torch.Size([])
memory (bytes)
6541844480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 19% |
memory (bytes)
6541844480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767515100.0
relative error loss 0.167297
shape of L is 
torch.Size([])
memory (bytes)
6545141760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
6545141760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767385100.0
relative error loss 0.1672847
shape of L is 
torch.Size([])
memory (bytes)
6548258816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6548258816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767319600.0
relative error loss 0.16727848
shape of L is 
torch.Size([])
memory (bytes)
6551572480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
6551572480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767199700.0
relative error loss 0.16726714
shape of L is 
torch.Size([])
memory (bytes)
6554750976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6554750976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1767077900.0
relative error loss 0.16725561
shape of L is 
torch.Size([])
memory (bytes)
6557876224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6558015488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 19% |
error is  1766998000.0
relative error loss 0.16724806
shape of L is 
torch.Size([])
memory (bytes)
6561218560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
6561218560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766927400.0
relative error loss 0.16724136
time to take a step is 228.25091171264648
it  13 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6564347904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6564347904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766927400.0
relative error loss 0.16724136
shape of L is 
torch.Size([])
memory (bytes)
6567641088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6567641088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1766870000.0
relative error loss 0.16723594
shape of L is 
torch.Size([])
memory (bytes)
6570749952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6570749952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1766786000.0
relative error loss 0.167228
shape of L is 
torch.Size([])
memory (bytes)
6574071808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
6574071808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766790100.0
relative error loss 0.16722839
shape of L is 
torch.Size([])
memory (bytes)
6577143808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6577143808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766712300.0
relative error loss 0.16722101
shape of L is 
torch.Size([])
memory (bytes)
6580477952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6580477952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766592500.0
relative error loss 0.16720967
shape of L is 
torch.Size([])
memory (bytes)
6583660544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
6583660544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766480900.0
relative error loss 0.1671991
shape of L is 
torch.Size([])
memory (bytes)
6586896384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
6586896384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766384600.0
relative error loss 0.16719
shape of L is 
torch.Size([])
memory (bytes)
6590095360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 19% |
memory (bytes)
6590095360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766323200.0
relative error loss 0.16718419
shape of L is 
torch.Size([])
memory (bytes)
6593310720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 19% |
memory (bytes)
6593310720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766236200.0
relative error loss 0.16717595
time to take a step is 229.21199679374695
it  14 : 4025935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6596358144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6596358144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766236200.0
relative error loss 0.16717595
shape of L is 
torch.Size([])
memory (bytes)
6599733248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6599733248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766189000.0
relative error loss 0.1671715
shape of L is 
torch.Size([])
memory (bytes)
6602788864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
6602940416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 19% |
error is  1766134800.0
relative error loss 0.16716635
shape of L is 
torch.Size([])
memory (bytes)
6606135296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6606135296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1766091800.0
relative error loss 0.16716228
shape of L is 
torch.Size([])
memory (bytes)
6609223680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6609346560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1765962800.0
relative error loss 0.16715007
shape of L is 
torch.Size([])
memory (bytes)
6612463616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6612463616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1765924900.0
relative error loss 0.16714649
shape of L is 
torch.Size([])
memory (bytes)
6615748608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
6615748608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1765877800.0
relative error loss 0.16714202
shape of L is 
torch.Size([])
memory (bytes)
6618951680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
6618951680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1765797900.0
relative error loss 0.16713446
shape of L is 
torch.Size([])
memory (bytes)
6622158848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 19% |
memory (bytes)
6622162944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1765718000.0
relative error loss 0.16712691
shape of L is 
torch.Size([])
memory (bytes)
6625218560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
6625218560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1765658600.0
relative error loss 0.16712128
time to take a step is 230.00697898864746
sum tnnu_Z after tensor(21083694., device='cuda:0')
shape of features
(6506,)
shape of features
(6506,)
number of orig particles 26025
number of new particles after remove low mass 23975
tnuZ shape should be parts x labs
torch.Size([26025, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  2080513700.0
relative error without small mass is  0.19692262
nnu_Z shape should be number of particles by maxV
(26025, 702)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
shape of features
(26025,)
Wed Feb 1 01:29:45 EST 2023
