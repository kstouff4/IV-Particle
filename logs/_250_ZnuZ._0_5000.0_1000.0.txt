Thu Feb 2 19:41:51 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 34528941
numbers of Z: 19068
shape of features
(19068,)
shape of features
(19068,)
ZX	Vol	Parts	Cubes	Eps
Z	0.018284369847634988	19068	19.068	0.09861090655195659
X	0.015293885713886403	1499	1.499	0.21688960403898755
X	0.01612082242776301	20779	20.779	0.09188702351599957
X	0.018077067283423873	4448	4.448	0.15958340671867444
X	0.015424276371773443	2821	2.821	0.1761707908952255
X	0.015353982645304486	61471	61.471	0.0629772349901649
X	0.015395313815394648	41529	41.529	0.07183658703496244
X	0.015395082472986219	42385	42.385	0.07134933736672075
X	0.015386803054594049	46751	46.751	0.06904292204829864
X	0.015340267866118052	7555	7.555	0.12662889725065307
X	0.016983478021024553	47914	47.914	0.07077090267172202
X	0.01538392927575374	7371	7.371	0.12779492903677248
X	0.015357811185231931	131448	131.448	0.048886819135063285
X	0.015321924485264657	8449	8.449	0.12194649879963969
X	0.01564174147850135	130796	130.796	0.04926784280017905
X	0.015361103098767287	18629	18.629	0.09377295901620353
X	0.015364160255152814	64033	64.033	0.062139581832139905
X	0.015405000299701592	33458	33.458	0.07721836079253515
X	0.01563755510511906	28893	28.893	0.08149395688148656
X	0.01545256226975482	275090	275.09	0.03829792400283263
X	0.015368947937798333	90144	90.144	0.055450113321787696
X	0.015321027351541523	17736	17.736	0.09523810291296674
X	0.016067672713625723	413746	413.746	0.033864114424350056
X	0.015337137823970576	19151	19.151	0.09286476299214103
X	0.0157264559683866	9885	9.885	0.11673956299317653
X	0.015340873124629265	26658	26.658	0.08317781445538823
X	0.015523727096260157	56653	56.653	0.06495177575012291
X	0.015339119793759412	29138	29.138	0.0807446147443226
X	0.0153766387008604	7717	7.717	0.1258358257224406
X	0.015350350403358545	45114	45.114	0.06981289581949894
X	0.016647153900480192	1639684	1639.684	0.021653425591090642
X	0.015347839053878978	3833	3.833	0.15879474968244064
X	0.017341635689970922	246223	246.223	0.04129718792428312
X	0.015344943305999371	10088	10.088	0.11500588691070675
X	0.015398165489619869	7061	7.061	0.12967838417223398
X	0.015364217203971178	11179	11.179	0.11118234439161365
X	0.015343299108830885	48165	48.165	0.06829607242137133
X	0.015404980761565549	59677	59.677	0.06367242529053126
X	0.015327784284365034	1638	1.638	0.21072779723181984
X	0.015323174520601775	3183	3.183	0.1688511552843563
X	0.015321319778970525	2951	2.951	0.1731579049115363
X	0.01636648729467377	6599	6.599	0.13536065579684453
X	0.01575274563141616	910	0.91	0.2586852239731821
X	0.01514007169216451	996	0.996	0.24771722013742897
X	0.015085191877594047	3891	3.891	0.15709530385204407
X	0.015332448839276891	1151	1.151	0.237052969618411
X	0.015276472653991856	1246	1.246	0.23058710397346657
X	0.015381165313109403	4912	4.912	0.1462997645708633
X	0.016039698527173758	4741	4.741	0.15012121890136565
X	0.01531349372223672	1105	1.105	0.24019870904074359
X	0.015668706945115282	6955	6.955	0.13109275016865285
X	0.017341146056323017	11776	11.776	0.11376971400930658
X	0.015257418859051946	1226	1.226	0.2317377928464041
X	0.015386664036097768	4983	4.983	0.14561893864438433
X	0.015225665116754078	3131	3.131	0.1694198925358579
X	0.015295742168921033	5014	5.014	0.14503141190134977
X	0.016521902874771992	4303	4.303	0.15658959693887078
X	0.015819133734122397	5990	5.99	0.1382245840523142
X	0.015312448145550675	2313	2.313	0.18776861349810778
X	0.015314612910792719	2370	2.37	0.1862598402096885
X	0.015242976053561102	2244	2.244	0.18938647974356435
X	0.017295249648130204	4392	4.392	0.15791431517636123
X	0.015344553899615875	1866	1.866	0.20184324015047808
X	0.015647152052808432	4543	4.543	0.15101874447684432
X	0.015386135116226028	6785	6.785	0.13137918496685871
X	0.015353618915610226	3779	3.779	0.1595675664457793
X	0.015293538938602923	2592	2.592	0.1806997972723802
X	0.01530940709884961	3475	3.475	0.16393355399471415
X	0.015261058102538676	6822	6.822	0.13078491172372803
X	0.015358493964056103	3490	3.49	0.16387312469054183
X	0.015258202589304447	1818	1.818	0.20322158425476033
X	0.015372986242734414	7198	7.198	0.1287801120819821
X	0.015127574789292678	3154	3.154	0.1686433445463906
X	0.015311778464750772	3886	3.886	0.15794562967821313
X	0.015256535134562616	1797	1.797	0.20400271412268242
X	0.015236710665404737	2907	2.907	0.1737062306152698
X	0.015319868008716408	3309	3.309	0.16666820269701424
X	0.01535383593921417	1341	1.341	0.22538726450709165
X	0.015306754347185062	1140	1.14	0.23768006288050805
X	0.015366275133253365	4696	4.696	0.1484613842466192
X	0.016335064219191134	21449	21.449	0.09132112766194292
X	0.016049307923773255	3650	3.65	0.16382788139174745
X	0.01532588722761645	1218	1.218	0.23259093208775933
X	0.015914355364631736	3455	3.455	0.16638480674317388
X	0.015307970735793965	1561	1.561	0.21404491098793466
X	0.015332966800452296	2765	2.765	0.17700152978725134
X	0.015626068727280305	3277	3.277	0.1683156232560079
X	0.015235235257591424	1919	1.919	0.19949165350580272
X	0.015069992989075536	855	0.855	0.26024495058642566
X	0.015376032489410767	3172	3.172	0.16924026750415708
X	0.01536167439278307	4666	4.666	0.1487640318929319
X	0.015269620493383537	3756	3.756	0.15960048983481862
X	0.015292681468554115	1767	1.767	0.20531264489324533
X	0.015267384354132003	1706	1.706	0.2076165025350908
X	0.015812382648243393	3683	3.683	0.16252937508209161
X	0.015312585556670404	3438	3.438	0.16453092978752518
X	0.015267464412963783	3681	3.681	0.16066959525204022
X	0.015347309402186945	3626	3.626	0.16175888683833234
X	0.015337916266616673	6451	6.451	0.13346880545206796
X	0.01535571686217386	2598	2.598	0.18080495004369074
X	0.015313638752992059	3207	3.207	0.16839394808840438
X	0.015367750289225357	6015	6.015	0.13670725230659161
X	0.015334135348065691	2521	2.521	0.1825417244181359
X	0.015311673243861212	3961	3.961	0.15694203154056596
X	0.015040339450280892	1260	1.26	0.22854007434747853
X	0.015301103208611219	2654	2.654	0.1793111415196856
X	0.015320547429295527	1652	1.652	0.21009775586170873
X	0.015731637038979816	5328	5.328	0.14346189633486844
X	0.015350255431699296	1048	1.048	0.24467228318679227
X	0.015297061599684001	2348	2.348	0.18676836101749222
X	0.015327871243267698	1163	1.163	0.23621132167046746
X	0.015334421663520234	1761	1.761	0.20573239494084541
X	0.015167857851606325	1344	1.344	0.22430639751272277
X	0.015295897482809281	1811	1.811	0.20365051121944347
X	0.015398963787933956	3774	3.774	0.1597950053521767
X	0.01692506713788759	5432	5.432	0.14605739356846129
X	0.015302935614329072	2565	2.565	0.18136875028428295
X	0.01598658398834076	4959	4.959	0.14772495117860768
X	0.015328596354345522	1195	1.195	0.23408746629139457
X	0.015659265728503752	6440	6.44	0.13447096108106146
X	0.015583509340520635	1139	1.139	0.2391739341100746
X	0.015314481432078671	14096	14.096	0.10280213515373232
X	0.015238241560749462	2063	2.063	0.19475048233604078
X	0.016069265393473652	1883	1.883	0.2043534156740207
X	0.01627411042927694	3076	3.076	0.17424900592263887
X	0.015293269764926166	1796	1.796	0.20420420097275951
X	0.01680211509451728	2424	2.424	0.1906676936375954
X	0.015330308259097723	1402	1.402	0.2219563757102935
X	0.015235540484651502	2099	2.099	0.19361922420299033
X	0.01533603683879972	10030	10.03	0.11520484307363381
X	0.015305400634370098	3053	3.053	0.17114831692124052
X	0.015381547153093674	9785	9.785	0.11627322821247293
X	0.015298262272447904	1391	1.391	0.22238474409659742
X	0.015222255835675777	2978	2.978	0.17226013150861236
X	0.015300374144573102	1025	1.025	0.24622155051037592
X	0.015326265237126059	2410	2.41	0.18527056739177508
X	0.015375388522415455	1861	1.861	0.20215907506541475
X	0.015338205775135838	1392	1.392	0.2225248113585276
X	0.01519988036458978	1733	1.733	0.2062277969153514
X	0.015199055318375995	1773	1.773	0.20466140599927066
X	0.015350049268396556	2306	2.306	0.18811213979465916
X	0.015307642286178817	1727	1.727	0.2069531252263234
X	0.015244471272551767	1386	1.386	0.2223905754720556
X	0.015358433143559468	7369	7.369	0.1277358449158434
X	0.015335674923655765	6507	6.507	0.1330783361810086
X	0.015275564961867378	3356	3.356	0.16572643051109662
X	0.015368679171900886	2611	2.611	0.1805551543830509
X	0.015315367763186559	3025	3.025	0.17171201461472818
X	0.01674763218767488	2061	2.061	0.20104431990672217
X	0.015287355409768923	1805	1.805	0.2038379542671005
X	0.015290466080268074	1437	1.437	0.2199486590162657
X	0.016303937222019713	2000	2.0	0.2012584697520268
X	0.015495149439800512	3681	3.681	0.1614643507193324
X	0.015303442914837338	2137	2.137	0.19275021681136686
X	0.0163562451727716	6815	6.815	0.1338872589897432
X	0.015288971592334782	2847	2.847	0.175117790440568
X	0.015329728146650958	6367	6.367	0.13402933654256804
X	0.015350323582673181	3104	3.104	0.1703721513372701
X	0.015336844947793672	2784	2.784	0.17661283639715047
X	0.015685709369071852	3817	3.817	0.1601747302761437
X	0.015250210753484473	1072	1.072	0.2423038715574138
X	0.01727653776889166	7540	7.54	0.13183431167345325
X	0.015296060099581527	2063	2.063	0.19499648571499714
X	0.015318498922577578	2078	2.078	0.1946212342231452
X	0.015293448240514654	1433	1.433	0.22016743140571438
X	0.015350413683987265	2738	2.738	0.17764877347688549
X	0.015332880338394738	2409	2.409	0.18532285499535722
X	0.015354643218720248	3782	3.782	0.15952891141888412
X	0.01533983297574795	2740	2.74	0.1775647234781147
X	0.016562140336845763	4341	4.341	0.15625798686020112
X	0.01539266549764632	827	0.827	0.26501438048675036
X	0.015297941088374322	1242	1.242	0.23094246451535447
X	0.015363766277134292	1225	1.225	0.2323381539660268
X	0.01547564505062074	7018	7.018	0.13016027737877037
X	0.015352149098337257	1973	1.973	0.1981591047164664
X	0.017666806952377493	5070	5.07	0.15160610891024076
X	0.015353509395371825	2901	2.901	0.1742689348542964
X	0.015274868455246178	3811	3.811	0.15884718028813896
X	0.015318145868028698	4348	4.348	0.15216177127500016
X	0.015301115372750173	3328	3.328	0.16628251737482327
X	0.015232517702441804	3283	3.283	0.1667888200464775
X	0.016412969210912055	4105	4.105	0.15871744527940826
X	0.015315405241569578	2013	2.013	0.19668062177501003
X	0.01534300315288835	3319	3.319	0.166584417269394
X	0.015144351017018642	1829	1.829	0.20230765708477652
X	0.015358193898733377	4399	4.399	0.15170342808566498
X	0.015294747791702706	5269	5.269	0.14264986788877296
X	0.015157365736714763	1721	1.721	0.20651310659771852
X	0.01537781888402044	2180	2.18	0.1917841664609163
X	0.01815331232827365	9350	9.35	0.12475216452503057
X	0.015351907774047973	7098	7.098	0.12932290526820622
X	0.015227391272632456	2400	2.4	0.18512770693922073
X	0.016279489963976157	1405	1.405	0.2262845086620932
X	0.015265990034659681	2607	2.607	0.1802442065645232
X	0.015386075177311899	3292	3.292	0.1671947647327282
X	0.01531057042978747	3064	3.064	0.1709625040506157
X	0.015290777556527415	2597	2.597	0.18057288644500355
X	0.01534275488543169	2211	2.211	0.19073842905981375
X	0.015328179837086136	1168	1.168	0.23587536295201494
X	0.015286972845332956	2157	2.157	0.1920836721252337
X	0.015288804569453961	1908	1.908	0.20010827719013813
X	0.015993833043417786	7187	7.187	0.13055744361604996
X	0.015249372501324143	1805	1.805	0.20366899590850066
X	0.015634645335478425	6854	6.854	0.13163801088756003
X	0.015350739385503947	4020	4.02	0.15630315647707682
X	0.016542755040727546	5809	5.809	0.14174317420340363
X	0.015217979618931963	1678	1.678	0.20853950925831266
X	0.017992385495676347	4132	4.132	0.16329622493706308
X	0.016191830087900327	3405	3.405	0.16816138432368558
X	0.01533713309869972	2336	2.336	0.18725084348275936
X	0.016579689092122676	6838	6.838	0.13434322262147336
X	0.015264364775284549	3534	3.534	0.16285611247625234
X	0.015341294014147374	2096	2.096	0.1941587221196165
X	0.015345657854009907	3402	3.402	0.16522803344743156
X	0.015357610472029253	3986	3.986	0.15676969661494616
X	0.015282632774785913	875	0.875	0.2594557947990315
X	0.01550052645762096	1960	1.96	0.19923400220005547
X	0.015255524011354379	1374	1.374	0.22309001483006208
X	0.015302078764785542	711	0.711	0.2781593166723442
X	0.01530675811881497	3568	3.568	0.16248731335779543
X	0.0153328817954032	2886	2.886	0.17449212045280274
X	0.015356500631108019	5164	5.164	0.14380324164263406
X	0.015351970834544697	1200	1.2	0.23388065253931453
X	0.01626705085216255	5968	5.968	0.13968829075045572
X	0.0153443683386567	2182	2.182	0.19158643502029857
X	0.015347696706669698	2062	2.062	0.19524721557923921
X	0.015319686486875818	2560	2.56	0.18155294753003456
X	0.015306485058963452	2475	2.475	0.18355522701389484
X	0.015137758680441121	1246	1.246	0.22988705237675963
X	0.015348322832777594	3074	3.074	0.17091716770107152
X	0.015149819959366866	1208	1.208	0.23233439743707376
X	0.016283360461987008	1314	1.314	0.23141040933738463
X	0.015302050186661465	2338	2.338	0.1870545909542671
X	0.016721966190353776	2065	2.065	0.20081173803733696
X	0.017744108912834854	5224	5.224	0.15032008364172234
X	0.01521878164271108	1390	1.39	0.22205217351601672
X	0.015269628815278746	2106	2.106	0.1935486015427126
X	0.016652566758908117	3378	3.378	0.1701925371999817
X	0.016579764595199293	7175	7.175	0.13220630016552637
X	0.015319504014341954	1506	1.506	0.21667388862410558
X	0.015383366821322901	7259	7.259	0.1284472665348592
X	0.015359734521564772	37241	37.241	0.07443675948804064
X	0.01564876653840575	17329	17.329	0.09665749520126281
X	0.015402217129563227	6826	6.826	0.13116128187926768
X	0.01486209130507152	859	0.859	0.2586399377498623
X	0.015321567838821685	6920	6.92	0.13033639502127453
X	0.015351873339231657	42837	42.837	0.07103092060328728
X	0.015299084868669863	126877	126.877	0.04940385641773688
X	0.015387827451587037	3193	3.193	0.16891159506982256
X	0.015359072936625678	72123	72.123	0.05971687504072643
X	0.015339812401793426	21026	21.026	0.09002320338064872
X	0.01537169155955117	26077	26.077	0.08384709327297517
X	0.016829549813967743	392549	392.549	0.034999314731495874
X	0.015457401654312378	2069	2.069	0.19549035433487122
X	0.015342455534867317	5111	5.111	0.14425459775943916
X	0.016999476433251115	203897	203.897	0.043685928430781285
X	0.015997147278569476	619220	619.22	0.029561912194615368
X	0.01633759748056563	4419	4.419	0.15462797322436975
X	0.016372163007457797	28598	28.598	0.08303416054076626
X	0.016679983990911872	20544	20.544	0.0932903826356526
X	0.01577754357961032	95103	95.103	0.05494755729287848
X	0.01539718209105486	23801	23.801	0.0864865713732507
X	0.015363132214275219	111341	111.341	0.05167433346877288
X	0.015388313727959143	31387	31.387	0.07885218669334147
X	0.015357814074032285	9001	9.001	0.11949403177245621
X	0.01529662985918268	6369	6.369	0.1339187857905917
X	0.015421216488657406	193840	193.84	0.04300890750837206
X	0.015290505073840036	6121	6.121	0.13568540977401908
X	0.015327631097701666	2498	2.498	0.18307436443437078
X	0.01532260233199543	63962	63.962	0.062106468031641895
X	0.01669545772196421	27874	27.874	0.08429458524694208
X	0.017256396047150378	152726	152.726	0.048344347991857446
X	0.015354463649069075	122988	122.988	0.04997935385171266
X	0.015737715800057256	1763	1.763	0.20744189065879062
X	0.01589803826464339	15451	15.451	0.10095526702030794
X	0.01533253539915141	4482	4.482	0.15067714219984077
X	0.015406043657586178	23274	23.274	0.08715119867257282
X	0.015789178179741372	65493	65.493	0.06223789283168028
X	0.015296248327718944	19566	19.566	0.09212149106832504
X	0.015340582278185454	69639	69.639	0.06039436778721227
X	0.01525998048547446	1021	1.021	0.2463255221446192
X	0.01534173025732185	4382	4.382	0.1518450561341161
X	0.015369337832891773	30078	30.078	0.07994698081048979
X	0.01540648536768201	94939	94.939	0.0545447523527593
X	0.015334479471099907	43217	43.217	0.07079536210639409
X	0.015255551065727263	10535	10.535	0.11313520983274093
X	0.015386901092163832	210075	210.075	0.04184005659451767
X	0.015405598025205797	16428	16.428	0.0978809004232505
X	0.015366140402103481	17724	17.724	0.0953529979173701
X	0.015414315987411055	54896	54.896	0.06548289213019737
X	0.01520101186594604	3839	3.839	0.1582042435983967
X	0.01533069543560749	77568	77.568	0.0582496187711677
X	0.01679685711701706	98717	98.717	0.05541304636208566
X	0.015321169587950928	210948	210.948	0.041722677330899606
X	0.015377387222538855	12573	12.573	0.10694189570432724
X	0.015362642341830119	20206	20.206	0.09127009944662234
X	0.015231534359184876	20602	20.602	0.09042282903223743
X	0.015390177864928844	6305	6.305	0.13464374501247955
X	0.015384789475642233	13705	13.705	0.10392918125473237
X	0.016044367790327335	9112	9.112	0.12075440044046058
X	0.015357247100912236	16555	16.555	0.09752771796998656
X	0.016385213102213074	88869	88.869	0.056915942716299316
X	0.015418978931814925	58909	58.909	0.06396729532125396
X	0.015341115260260583	4650	4.65	0.14886799070273582
X	0.01761343492206039	10226	10.226	0.11987062146107724
X	0.01633531805177429	30473	30.473	0.08123392936775313
X	0.015472396621636643	15606	15.606	0.09971381421080364
X	0.015318028845623895	5194	5.194	0.1434058867306065
X	0.01647070024382449	7371	7.371	0.13073600729672033
X	0.016100729498185824	207644	207.644	0.0426424408533501
X	0.015336037124642609	8787	8.787	0.12039935904500836
X	0.015375264274506827	288031	288.031	0.037652555067463594
X	0.015294759783091364	5655	5.655	0.13932744842268924
X	0.015138960032101741	22627	22.627	0.08746272856439741
X	0.01580784816718289	20810	20.81	0.09124314057927702
X	0.016782806141246085	161243	161.243	0.04703934178441655
X	0.015382965982567364	38968	38.968	0.07335740969976129
X	0.015388514481789241	9987	9.987	0.11550139136410416
X	0.015387045252574309	90192	90.192	0.05546202699086907
X	0.016370938202203825	104706	104.706	0.05387252525303594
X	0.015404408821650747	9165	9.165	0.11889698874303672
X	0.0153560219232472	76051	76.051	0.05866664663778087
X	0.015393872050892456	82265	82.265	0.057197571538304987
X	0.015873870329752345	209740	209.74	0.042299365692532606
X	0.015395957124605905	49967	49.967	0.06754208648100393
X	0.015260827324112467	2521	2.521	0.1822503667292515
X	0.01528963330397198	15968	15.968	0.09856336344883863
X	0.01633000680678373	86357	86.357	0.05739793564724134
X	0.015331971325222177	124316	124.316	0.04977641984057214
X	0.015388503117800231	13588	13.588	0.10423501011712992
X	0.015364421900616486	1897	1.897	0.20082431661211822
X	0.015401114176673527	80247	80.247	0.057682106451259325
X	0.015358152568068002	132117	132.117	0.04880452512019991
X	0.015289517598912608	52284	52.284	0.06637557949376155
X	0.015348924517507072	7463	7.463	0.12717102437075237
X	0.015316667728701534	15079	15.079	0.10052264712942376
X	0.015386970850070791	8764	8.764	0.12063784965434587
X	0.015288036772128987	4666	4.666	0.14852594629877902
X	0.015344571804192976	10488	10.488	0.11352391635801501
X	0.015366431051219041	7220	7.22	0.12863088905556397
X	0.015328347595415714	53820	53.82	0.06579363516233692
X	0.015412909377150931	33389	33.389	0.07728473775288527
X	0.01540139647828145	17394	17.394	0.09602556660733226
X	0.01565142464426307	6599	6.599	0.13335989618642485
X	0.015303738171050652	6162	6.162	0.13542284841263938
X	0.015561462163743956	557065	557.065	0.030342277509267993
X	0.015331774499876251	8569	8.569	0.12140057956508445
X	0.015402538601122088	114593	114.593	0.0512245406036541
X	0.015363070862173762	2426	2.426	0.1850102311447101
X	0.015359138653655577	4245	4.245	0.15351932615173192
X	0.015370487261860485	2868	2.868	0.17499924007245216
X	0.015410260248375987	23222	23.222	0.08722415767250806
X	0.01556142205455442	3801	3.801	0.15997439290326562
X	0.015309367851689347	11434	11.434	0.11021812622736382
X	0.015388504747096587	5865	5.865	0.13792493950061105
X	0.015609826777070368	4055	4.055	0.15672408454384426
X	0.01536679995593735	240819	240.819	0.03996049840908121
X	0.015394400291943345	47699	47.699	0.06859374273844088
X	0.017396991588806216	16489	16.489	0.1018028520610543
X	0.01539720586680058	74016	74.016	0.05925233030105531
X	0.015358894298196151	97356	97.356	0.05403382889109589
X	0.015328949764095129	4060	4.06	0.15571440446436957
X	0.015233968470232244	5071	5.071	0.1442911896026119
X	0.016650084701133	7576	7.576	0.1300146108399064
X	0.01546451967067619	1295	1.295	0.22857157575834447
X	0.01683369070059046	7966	7.966	0.12832551437004763
X	0.015347625611814531	12611	12.611	0.10676540625618947
X	0.015301107360287784	2830	2.83	0.17551415288245217
X	0.015340567376336047	5557	5.557	0.14028148088902137
X	0.016334254790612485	10880	10.88	0.11450474311404779
X	0.015394008918776814	4719	4.719	0.14830891311105526
X	0.015356431921218655	329852	329.852	0.03597413822404221
X	0.015324479243773004	9733	9.733	0.11633569819422102
X	0.016804114281289976	34328	34.328	0.0788113998698265
X	0.016229510164740397	6583	6.583	0.13509123926509567
X	0.016561869995982133	35320	35.32	0.07768960120235477
X	0.015329999860194534	30072	30.072	0.07988402631503716
X	0.01706714243364161	760322	760.322	0.028209003970633476
X	0.015653585869750963	490967	490.967	0.03170933828633765
X	0.01542063506552521	29549	29.549	0.08051061742063548
X	0.01781088870842375	41779	41.779	0.07526201641767521
X	0.015308144226967546	2794	2.794	0.17629177392284498
X	0.015313723633900813	28720	28.72	0.08108965571455169
X	0.015404753588518056	39428	39.428	0.07310549172833575
X	0.01638261110081381	68430	68.43	0.06209341236260321
X	0.015364721769559449	5558	5.558	0.14034665061399337
X	0.01535627943016618	73911	73.911	0.059227805652137325
X	0.015335611722364513	160535	160.535	0.045713428088623546
X	0.015301491643353135	38606	38.606	0.07345583857605972
X	0.015335956460565152	11477	11.477	0.11014399596884784
X	0.01773909030406741	19707	19.707	0.09655400127466726
X	0.015252006973635058	6210	6.21	0.13492066248856877
X	0.015390056251955954	3157	3.157	0.16955940156034813
X	0.018228017600408295	112519	112.519	0.05451355757375426
X	0.015344452523203024	33548	33.548	0.07704803866884324
X	0.015369940621373058	3832	3.832	0.15888475541456037
X	0.015396150981977436	98195	98.195	0.05392302683975999
X	0.01649549981003822	10278	10.278	0.11708078400324996
X	0.01523222177838933	1500	1.5	0.21654957302503444
X	0.015334072058237502	59581	59.581	0.06360870743595998
X	0.015418345333169102	89492	89.492	0.05564393716682934
X	0.015337335006760894	43483	43.483	0.07065509230424669
X	0.016332299135499512	112407	112.407	0.05257161549343619
X	0.015391233792121444	102537	102.537	0.05314522818779206
X	0.015322387436364647	5524	5.524	0.14050472304072353
X	0.015360645697993164	52786	52.786	0.06626693839874283
X	0.016348810662474306	111267	111.267	0.052768319296841254
X	0.016111114155288278	126928	126.928	0.050256168978529255
X	0.015381207861677508	9416	9.416	0.11777180480018172
X	0.01638261584331764	128267	128.267	0.05036043082546638
X	0.016378573385641602	117211	117.211	0.051892257751357466
X	0.015645708938738644	50775	50.775	0.06754322164020993
X	0.016377283060897592	48369	48.369	0.06969873814831411
X	0.015334622363266532	16535	16.535	0.0975190874598477
X	0.015376864194982822	16012	16.012	0.0986599139755526
X	0.015377609011922366	6934	6.934	0.1304072266793048
X	0.015294042544603259	4229	4.229	0.1534952249345785
X	0.015380603404102317	140145	140.145	0.04787755038343448
X	0.015442693472688522	96075	96.075	0.05437144635430575
X	0.015338060503385405	180305	180.305	0.04397989877587239
X	0.015387456033881228	110293	110.293	0.05186482996249765
X	0.015349475082117207	52311	52.311	0.06645079309272126
X	0.01528641315868033	10338	10.338	0.11392604525768339
X	0.01538908971470781	67005	67.005	0.061240008173149556
X	0.016390563204249708	3298	3.298	0.17065318905789964
X	0.015338148858571748	12545	12.545	0.10693029399908294
X	0.01659949916934864	143572	143.572	0.04871638771832602
X	0.015986476031964528	28660	28.66	0.082317502388747
X	0.015229659747224176	3481	3.481	0.1635543348347432
X	0.015369502708517677	8265	8.265	0.12297183470485384
X	0.015363190927020872	77113	77.113	0.05840516782471312
X	0.015253642486335752	30276	30.276	0.0795716485070913
X	0.018082346510802254	60614	60.614	0.0668180731419994
X	0.015351103530604784	21610	21.61	0.0892267233086064
X	0.015281109944103439	23583	23.583	0.08653370189170442
X	0.015363585826053254	4081	4.081	0.1555638437467957
X	0.015351472707598483	1692	1.692	0.20856906554931398
X	0.016728756193032688	41553	41.553	0.07383921810870614
X	0.015297056796075646	18295	18.295	0.0942088758909589
X	0.016634837852857243	3725	3.725	0.16467617274415344
X	0.015404487618781045	172356	172.356	0.0447102347694822
X	0.015315504015941868	3757	3.757	0.15974601488874504
X	0.016666387348033343	18263	18.263	0.09699658646017949
X	0.015396518633982587	5675	5.675	0.1394715268617684
X	0.015340706738628451	91631	91.631	0.055114725304602805
X	0.015785332147733675	5763	5.763	0.13991631815402644
X	0.015337580848480803	34146	34.146	0.07658416995286911
X	0.015331949084952655	15739	15.739	0.09913037391701164
X	0.015368962970949168	14865	14.865	0.10111755274383628
X	0.015392954198061078	34873	34.873	0.07613965227940134
X	0.015413591582512579	474952	474.952	0.03189711289112624
X	0.015654769998341426	11429	11.429	0.11105705503811704
X	0.015291958907591609	6527	6.527	0.13281582874805967
X	0.015381560289204442	89104	89.104	0.05568023554923333
X	0.015359165434915657	12345	12.345	0.10755373780763874
X	0.015893544419369552	290197	290.197	0.037976005404485966
X	0.015361069165848968	5235	5.235	0.14316436112072972
X	0.015358554192726144	90344	90.344	0.05539667176944616
X	0.015362628173241745	272538	272.538	0.038342416893040465
X	0.015353265851113983	4378	4.378	0.15192934690143295
X	0.015375236295132355	300293	300.293	0.03713289960811514
X	0.01645032899449049	10603	10.603	0.11576624378922173
X	0.01667265553970956	339902	339.902	0.036605837928524246
X	0.015341304814479317	9256	9.256	0.11834402000956092
X	0.015331162241615617	124385	124.385	0.049766338555021376
X	0.015392133067322717	2526	2.526	0.18265090189287914
X	0.015384941000460284	4127	4.127	0.15505548082736514
X	0.015991918348733463	5707	5.707	0.14098218280086483
X	0.015394762892724043	35911	35.911	0.07540182111689805
X	0.01632545540650836	35604	35.604	0.07711203615165378
X	0.015403115063759115	145779	145.779	0.04727568870538148
X	0.015379818681896936	11762	11.762	0.10935114801163284
X	0.015423039575140327	5195	5.195	0.14372361768859154
X	0.016573074489490993	8553	8.553	0.12466987865690131
X	0.015314220219714125	106224	106.224	0.052435338853592806
X	0.016765801255450588	40372	40.372	0.07460730452020325
X	0.016350915403359995	191705	191.705	0.04401859844438987
X	0.016236963793649226	35628	35.628	0.07695516839912729
X	0.015386205472846397	8457	8.457	0.1220782792553163
X	0.015278591363895382	2208	2.208	0.1905583932042503
X	0.015346341913332631	7629	7.629	0.1262347967200228
X	0.01579949631227679	914880	914.88	0.02584796574124452
X	0.015297966199476832	6111	6.111	0.13578145904789127
X	0.015301117845863372	11835	11.835	0.10893939671672997
X	0.01534389921946297	103142	103.142	0.052986682557884926
X	0.01534583490524028	43641	43.641	0.07058275557459744
X	0.015387798737357418	5951	5.951	0.13725521329974194
X	0.015293369440666595	2775	2.775	0.17663634276996473
X	0.015369154281495543	29717	29.717	0.08026908923410293
X	0.015953394276045713	35402	35.402	0.07666699365954575
X	0.015327732560581418	8574	8.574	0.12136630918389926
X	0.015399964124446565	42568	42.568	0.07125447661328702
X	0.015664961707471538	98086	98.086	0.05425512534396541
X	0.016765894278175205	54645	54.645	0.06744639852117691
X	0.015349157369835704	202210	202.21	0.042340953755226576
X	0.01637475476671222	79017	79.017	0.05917679259381223
X	0.01614003586036485	11205	11.205	0.11293560380753408
X	0.01593737838370701	71429	71.429	0.06065232282846036
X	0.015345793045657027	17379	17.379	0.09593745304243666
X	0.015355001694242573	7397	7.397	0.1275649657167813
X	0.015463481150412788	15580	15.58	0.0997500844370986
X	0.015357752864199953	9377	9.377	0.1178748770870558
X	0.015370269126200506	55307	55.307	0.06525800457347353
X	0.015282655066536342	19739	19.739	0.09182435533655267
X	0.014896628903601343	4762	4.762	0.14625123293348682
X	0.015379885978506303	19173	19.173	0.09291539578405525
X	0.015408347365770653	13292	13.292	0.10504819052611213
X	0.015329754544800907	17265	17.265	0.09611463908356224
X	0.015330114553266562	46651	46.651	0.06900726484564654
X	0.015329800045672123	10919	10.919	0.1119741758713535
X	0.015277465688129974	8438	8.438	0.12188135262302402
X	0.01619785475134439	18793	18.793	0.09516716430459021
X	0.015357995529587476	3307	3.307	0.16683997418464366
X	0.015253219299461659	7110	7.11	0.12897255530787594
X	0.015787011981184443	20789	20.789	0.09123373325979245
X	0.015402285893823787	127187	127.187	0.04947443223048749
X	0.015633975662678653	5716	5.716	0.1398489050747123
X	0.0153101305748105	1420	1.42	0.2209175323088376
X	0.015332057964769764	2785	2.785	0.17657332059439473
X	0.01613231658722339	98963	98.963	0.05462707681695362
X	0.017934237519406065	286925	286.925	0.03968607745305529
X	0.015346398883937198	42898	42.898	0.07098879633038684
X	0.017575578938748695	5543	5.543	0.1469112047928967
X	0.015407250797448079	68183	68.183	0.06090921682670286
X	0.015648377517169177	171077	171.077	0.045056684366726274
X	0.016487684815058686	2321	2.321	0.1922331040758321
X	0.0163099337985405	9132	9.132	0.12132829608503032
X	0.015398936119370992	30509	30.509	0.07961976928999348
X	0.015353216270613942	10946	10.946	0.11193897037942853
X	0.015384089740159455	60536	60.536	0.06334116223323472
X	0.015398753419555124	62773	62.773	0.06259950417423756
X	0.015381132356682323	22061	22.061	0.08867224517393119
X	0.015374455595992141	7064	7.064	0.12959344014160987
X	0.015330467473710481	4835	4.835	0.14691054613285998
X	0.015382264951445743	27623	27.623	0.08227157561320113
X	0.016023398705048146	38526	38.526	0.07464491703012756
X	0.01536041322064967	45890	45.89	0.06943230990522573
X	0.015322150641649317	36545	36.545	0.07484519436520631
X	0.016685711024495172	250248	250.248	0.04055004752481202
X	0.015363411050952222	7079	7.079	0.12947082419375794
X	0.015348981639976191	3061	3.061	0.1711612353526152
X	0.015374906537963239	12130	12.13	0.10822241669527055
X	0.015643827630325145	47881	47.881	0.06887473432892313
X	0.015564944423897113	31750	31.75	0.0788499244539566
X	0.015375974658212381	7115	7.115	0.12928731513616437
X	0.015323654165194697	2475	2.475	0.18362383195678134
X	0.015381904556595433	37617	37.617	0.07422359493102511
X	0.01536830853417449	4000	4.0	0.1566229356368101
X	0.016027847197940974	17658	17.658	0.09682285704652346
X	0.01611831107068328	3913	3.913	0.16030112184684925
X	0.0153650280685955	12964	12.964	0.10582739646363103
X	0.015524736977095744	8904	8.904	0.12035931292640079
X	0.015315429950983008	6362	6.362	0.13402274523755148
X	0.015323807523877561	5468	5.468	0.14098710540134918
X	0.015407414189694154	96943	96.943	0.054167372213701465
X	0.016234819388765594	170163	170.163	0.0456941681952287
X	0.015787518534187205	136840	136.84	0.04868183454282416
X	0.015349489752801516	111929	111.929	0.051568413811825205
X	0.015405117988127982	25703	25.703	0.08431284373021743
X	0.015262748152625375	7411	7.411	0.1272287640434064
X	0.01732343980390897	13629	13.629	0.10832367360653061
X	0.016097843921880378	114595	114.595	0.05198371916585663
X	0.015365526190617719	10787	10.787	0.11251633726191562
X	0.015407540642037173	23754	23.754	0.08656297820508119
X	0.015305098808630414	6472	6.472	0.13322913545729786
X	0.0168495105665425	227859	227.859	0.04197335660393231
X	0.01538829884742106	45060	45.06	0.06989827770274343
X	0.016661904984992367	19766	19.766	0.09446449796626462
X	0.01655949647682891	154891	154.891	0.04746137483667044
X	0.015355800460327784	33800	33.8	0.07687502203497615
X	0.015401211642416505	28075	28.075	0.08186125051793602
X	0.016521594351908385	192939	192.939	0.04407685956618071
X	0.015347023771638463	105189	105.189	0.05264429084283937
X	0.01537046864154535	87120	87.12	0.056086251378746205
X	0.015330204915604152	15498	15.498	0.09963779343691419
X	0.016089382966496993	76361	76.361	0.05950533812453688
X	0.015389530597505422	7456	7.456	0.12732289105572253
X	0.015332088238323006	60559	60.559	0.06326170193965101
X	0.015353226374764078	432719	432.719	0.03285977329709666
X	0.016343377926474147	35788	35.788	0.07700781451643779
X	0.017985693955122454	38851	38.851	0.07735855208295773
X	0.015655324375104387	11220	11.22	0.11174370612581061
X	0.015369948728138873	51506	51.506	0.06682488167210496
X	0.016712202118114576	20055	20.055	0.09410304378712653
X	0.015324341225871974	7945	7.945	0.12447896560417518
X	0.015309969538630092	26988	26.988	0.08278173829522177
X	0.017736213065194933	254983	254.983	0.04112600812083428
X	0.01740900333952724	14051	14.051	0.1074044374952255
X	0.016539269886298728	240327	240.327	0.0409799341570573
X	0.01536553969254623	28031	28.031	0.08184077672523876
X	0.015348666586645228	30558	30.558	0.07949050240539257
X	0.015243298108782761	16437	16.437	0.09751815199411121
X	0.016587220382589196	51169	51.169	0.06869454137794763
X	0.01806679338678345	176013	176.013	0.04682151308470707
X	0.01569135477541995	2551	2.551	0.18322440823379288
X	0.016344424473367515	513251	513.251	0.03169664356436734
X	0.015405188208091346	33790	33.79	0.07696494066934441
X	0.015511554529314228	29456	29.456	0.08075334438116531
X	0.015394211286347595	4194	4.194	0.15425633391197976
X	0.015362366581350241	19160	19.16	0.09290110360034193
X	0.01531555707717568	3084	3.084	0.17061065210323226
X	0.01604671024743588	2028	2.028	0.19926882196319817
X	0.015345507508312484	10522	10.522	0.11340381187289052
X	0.015801872192221622	85298	85.298	0.05700634964232079
X	0.015450253085042151	1225787	1225.787	0.02327234763063783
X	0.017043739765860808	49078	49.078	0.07028986017770993
X	0.015344152236562084	98065	98.065	0.053886042421977476
X	0.015341814229864558	9499	9.499	0.1173274442915607
X	0.016418643665006873	30551	30.551	0.08130250917327665
X	0.015341674120419048	14803	14.803	0.10119856124220288
X	0.015392507719095507	79845	79.845	0.05776798487464702
X	0.01633474215524256	8176	8.176	0.12594776550864892
X	0.017443019819680366	707091	707.091	0.029110441238130867
X	0.015367580165069582	4387	4.387	0.15187254976035397
X	0.015398353587256745	97133	97.133	0.054121417393656564
X	0.01767300042191726	13672	13.672	0.10893298708125815
X	0.015377324360713438	72329	72.329	0.05968375031419886
X	0.015343094075768672	262356	262.356	0.03881569044430167
X	0.01532110588989129	23825	23.825	0.08631489269181629
X	0.015981385317201523	6862	6.862	0.1325524938453197
X	0.015473976338879553	52604	52.604	0.06650603447350302
X	0.01589018258350254	8685	8.685	0.12230748095861117
X	0.015200804469577568	8551	8.551	0.12113878740330526
X	0.015340115803548595	144630	144.63	0.04733583952790564
X	0.016347142995921753	302242	302.242	0.037817749638885116
X	0.01823217220868837	31229	31.229	0.08357828311513094
X	0.015292975002933511	35829	35.829	0.07529262359717083
X	0.01663534872121621	160283	160.283	0.046994620724677666
X	0.016090722734208043	128315	128.315	0.05005329908152498
X	0.015383688072404356	42594	42.594	0.0712148689762823
X	0.01617571670649311	5897	5.897	0.139983699911019
X	0.015364451784171396	101936	101.936	0.053218565054999056
X	0.015334151157412783	59247	59.247	0.06372812268875622
X	0.016819072404031875	56147	56.147	0.06691012295328658
X	0.015317102310814188	32522	32.522	0.07780361734912321
X	0.01604645726398578	31345	31.345	0.07999637296788117
X	0.015358235882344806	39582	39.582	0.07293699498417387
X	0.0151780882165105	288730	288.73	0.03746062801771273
X	0.015398569084733721	47347	47.347	0.06876951578854804
X	0.015344015013039607	16046	16.046	0.0985199256977886
X	0.01661570865840609	2733	2.733	0.18251269541970652
X	0.015742655985668545	17990	17.99	0.09564940672458645
X	0.015347000454259432	49149	49.149	0.06784267131589361
X	0.01529128694927982	94669	94.669	0.05446013950662755
X	0.015406930351319615	146941	146.941	0.047154633753434524
X	0.015566290558441996	7226	7.226	0.1291503909462608
X	0.015366909536994844	18736	18.736	0.09360589907249003
X	0.015363954470585604	42150	42.15	0.07143348046659173
X	0.017987977289000576	49636	49.636	0.07129545608017036
X	0.015333456313669206	35778	35.778	0.0753947895825867
X	0.015403646933597371	62423	62.423	0.06272292549387326
X	0.0158004423513893	67086	67.086	0.0617559939437198
X	0.015417452246223387	26704	26.704	0.08326812091722635
X	0.01538673275104514	1987	1.987	0.19784094654727247
X	0.0166921172543391	4972	4.972	0.14973618672957506
X	0.016390949006127628	157952	157.952	0.04699225904889599
X	0.015365228946051062	32504	32.504	0.07789939333304412
X	0.015418739299529617	71494	71.494	0.05996894700866094
X	0.01526007688946967	2006	2.006	0.19667172684720233
X	0.01527336373096953	2845	2.845	0.17509919168821783
X	0.016375358952874418	109612	109.612	0.05306126207065486
X	0.015306766958606713	77095	77.095	0.05833811861429859
X	0.015394034515460816	43904	43.904	0.0705152075888289
X	0.015393858238615858	3401	3.401	0.16541705463187092
X	0.015359368709800796	7092	7.092	0.12938031763507649
X	0.015346921760788907	22518	22.518	0.08800293462351338
X	0.015312824688132316	21850	21.85	0.0888248786919127
X	0.01539348886364405	32555	32.555	0.07790639719246552
X	0.015377858304712432	8285	8.285	0.12289506606475846
X	0.01531614299453792	10172	10.172	0.11461664370388668
X	0.01532969254535031	21159	21.159	0.08981442669293105
X	0.015298393862264502	11402	11.402	0.11029477363707606
X	0.01641246885053136	167217	167.217	0.04612798626721595
X	0.015391393482097566	6648	6.648	0.13229059713132552
X	0.01593730936536596	28540	28.54	0.08234811798096177
X	0.016376520614807363	99345	99.345	0.05483087890076831
X	0.015393931371078215	60988	60.988	0.06319776478062022
X	0.01537614034735901	65335	65.335	0.06174007409169219
X	0.01632464323155485	125018	125.018	0.050732981271571644
X	0.015376289368130127	4450	4.45	0.15118098739569688
X	0.015783146797396612	65445	65.445	0.06224517724444026
X	0.01540896448043155	24333	24.333	0.08587352015060633
X	0.015772546847152246	221657	221.657	0.04143876385679987
X	0.01633719384542693	125671	125.671	0.0506579328219927
X	0.015283064066133873	30040	30.04	0.07983074246553172
X	0.015648885643023084	77217	77.217	0.058738587322779846
X	0.015336472886915566	11717	11.717	0.10938800128937616
X	0.0152503757061616	12932	12.932	0.10565051434540361
X	0.015402476462158086	5708	5.708	0.13922018176808643
X	0.017358378874556796	23980	23.98	0.08978835083732117
X	0.015404703163374492	85693	85.693	0.05643770290180355
X	0.016057018014997572	69168	69.168	0.061459151313720234
X	0.017771597716751568	9322	9.322	0.1239954621884046
X	0.01539378447776058	33486	33.486	0.07717809293689869
X	0.015388651659179919	9833	9.833	0.1161015924608941
X	0.01671439326735638	327876	327.876	0.03707891317970725
X	0.015368648569726001	27877	27.877	0.08199673283721508
X	0.015335710287911739	11358	11.358	0.11052673569659956
X	0.016438782045266557	72584	72.584	0.06095503427889575
X	0.016383261078148987	83986	83.986	0.05799597021625148
X	0.015329616498232652	10063	10.063	0.11506271195225523
X	0.01537780848557917	49226	49.226	0.06785262183486251
X	0.015366179577287345	25702	25.702	0.08424283915048265
time for making epsilon is 1.7913944721221924
epsilons are
[0.21688960403898755, 0.09188702351599957, 0.15958340671867444, 0.1761707908952255, 0.0629772349901649, 0.07183658703496244, 0.07134933736672075, 0.06904292204829864, 0.12662889725065307, 0.07077090267172202, 0.12779492903677248, 0.048886819135063285, 0.12194649879963969, 0.04926784280017905, 0.09377295901620353, 0.062139581832139905, 0.07721836079253515, 0.08149395688148656, 0.03829792400283263, 0.055450113321787696, 0.09523810291296674, 0.033864114424350056, 0.09286476299214103, 0.11673956299317653, 0.08317781445538823, 0.06495177575012291, 0.0807446147443226, 0.1258358257224406, 0.06981289581949894, 0.021653425591090642, 0.15879474968244064, 0.04129718792428312, 0.11500588691070675, 0.12967838417223398, 0.11118234439161365, 0.06829607242137133, 0.06367242529053126, 0.21072779723181984, 0.1688511552843563, 0.1731579049115363, 0.13536065579684453, 0.2586852239731821, 0.24771722013742897, 0.15709530385204407, 0.237052969618411, 0.23058710397346657, 0.1462997645708633, 0.15012121890136565, 0.24019870904074359, 0.13109275016865285, 0.11376971400930658, 0.2317377928464041, 0.14561893864438433, 0.1694198925358579, 0.14503141190134977, 0.15658959693887078, 0.1382245840523142, 0.18776861349810778, 0.1862598402096885, 0.18938647974356435, 0.15791431517636123, 0.20184324015047808, 0.15101874447684432, 0.13137918496685871, 0.1595675664457793, 0.1806997972723802, 0.16393355399471415, 0.13078491172372803, 0.16387312469054183, 0.20322158425476033, 0.1287801120819821, 0.1686433445463906, 0.15794562967821313, 0.20400271412268242, 0.1737062306152698, 0.16666820269701424, 0.22538726450709165, 0.23768006288050805, 0.1484613842466192, 0.09132112766194292, 0.16382788139174745, 0.23259093208775933, 0.16638480674317388, 0.21404491098793466, 0.17700152978725134, 0.1683156232560079, 0.19949165350580272, 0.26024495058642566, 0.16924026750415708, 0.1487640318929319, 0.15960048983481862, 0.20531264489324533, 0.2076165025350908, 0.16252937508209161, 0.16453092978752518, 0.16066959525204022, 0.16175888683833234, 0.13346880545206796, 0.18080495004369074, 0.16839394808840438, 0.13670725230659161, 0.1825417244181359, 0.15694203154056596, 0.22854007434747853, 0.1793111415196856, 0.21009775586170873, 0.14346189633486844, 0.24467228318679227, 0.18676836101749222, 0.23621132167046746, 0.20573239494084541, 0.22430639751272277, 0.20365051121944347, 0.1597950053521767, 0.14605739356846129, 0.18136875028428295, 0.14772495117860768, 0.23408746629139457, 0.13447096108106146, 0.2391739341100746, 0.10280213515373232, 0.19475048233604078, 0.2043534156740207, 0.17424900592263887, 0.20420420097275951, 0.1906676936375954, 0.2219563757102935, 0.19361922420299033, 0.11520484307363381, 0.17114831692124052, 0.11627322821247293, 0.22238474409659742, 0.17226013150861236, 0.24622155051037592, 0.18527056739177508, 0.20215907506541475, 0.2225248113585276, 0.2062277969153514, 0.20466140599927066, 0.18811213979465916, 0.2069531252263234, 0.2223905754720556, 0.1277358449158434, 0.1330783361810086, 0.16572643051109662, 0.1805551543830509, 0.17171201461472818, 0.20104431990672217, 0.2038379542671005, 0.2199486590162657, 0.2012584697520268, 0.1614643507193324, 0.19275021681136686, 0.1338872589897432, 0.175117790440568, 0.13402933654256804, 0.1703721513372701, 0.17661283639715047, 0.1601747302761437, 0.2423038715574138, 0.13183431167345325, 0.19499648571499714, 0.1946212342231452, 0.22016743140571438, 0.17764877347688549, 0.18532285499535722, 0.15952891141888412, 0.1775647234781147, 0.15625798686020112, 0.26501438048675036, 0.23094246451535447, 0.2323381539660268, 0.13016027737877037, 0.1981591047164664, 0.15160610891024076, 0.1742689348542964, 0.15884718028813896, 0.15216177127500016, 0.16628251737482327, 0.1667888200464775, 0.15871744527940826, 0.19668062177501003, 0.166584417269394, 0.20230765708477652, 0.15170342808566498, 0.14264986788877296, 0.20651310659771852, 0.1917841664609163, 0.12475216452503057, 0.12932290526820622, 0.18512770693922073, 0.2262845086620932, 0.1802442065645232, 0.1671947647327282, 0.1709625040506157, 0.18057288644500355, 0.19073842905981375, 0.23587536295201494, 0.1920836721252337, 0.20010827719013813, 0.13055744361604996, 0.20366899590850066, 0.13163801088756003, 0.15630315647707682, 0.14174317420340363, 0.20853950925831266, 0.16329622493706308, 0.16816138432368558, 0.18725084348275936, 0.13434322262147336, 0.16285611247625234, 0.1941587221196165, 0.16522803344743156, 0.15676969661494616, 0.2594557947990315, 0.19923400220005547, 0.22309001483006208, 0.2781593166723442, 0.16248731335779543, 0.17449212045280274, 0.14380324164263406, 0.23388065253931453, 0.13968829075045572, 0.19158643502029857, 0.19524721557923921, 0.18155294753003456, 0.18355522701389484, 0.22988705237675963, 0.17091716770107152, 0.23233439743707376, 0.23141040933738463, 0.1870545909542671, 0.20081173803733696, 0.15032008364172234, 0.22205217351601672, 0.1935486015427126, 0.1701925371999817, 0.13220630016552637, 0.21667388862410558, 0.1284472665348592, 0.07443675948804064, 0.09665749520126281, 0.13116128187926768, 0.2586399377498623, 0.13033639502127453, 0.07103092060328728, 0.04940385641773688, 0.16891159506982256, 0.05971687504072643, 0.09002320338064872, 0.08384709327297517, 0.034999314731495874, 0.19549035433487122, 0.14425459775943916, 0.043685928430781285, 0.029561912194615368, 0.15462797322436975, 0.08303416054076626, 0.0932903826356526, 0.05494755729287848, 0.0864865713732507, 0.05167433346877288, 0.07885218669334147, 0.11949403177245621, 0.1339187857905917, 0.04300890750837206, 0.13568540977401908, 0.18307436443437078, 0.062106468031641895, 0.08429458524694208, 0.048344347991857446, 0.04997935385171266, 0.20744189065879062, 0.10095526702030794, 0.15067714219984077, 0.08715119867257282, 0.06223789283168028, 0.09212149106832504, 0.06039436778721227, 0.2463255221446192, 0.1518450561341161, 0.07994698081048979, 0.0545447523527593, 0.07079536210639409, 0.11313520983274093, 0.04184005659451767, 0.0978809004232505, 0.0953529979173701, 0.06548289213019737, 0.1582042435983967, 0.0582496187711677, 0.05541304636208566, 0.041722677330899606, 0.10694189570432724, 0.09127009944662234, 0.09042282903223743, 0.13464374501247955, 0.10392918125473237, 0.12075440044046058, 0.09752771796998656, 0.056915942716299316, 0.06396729532125396, 0.14886799070273582, 0.11987062146107724, 0.08123392936775313, 0.09971381421080364, 0.1434058867306065, 0.13073600729672033, 0.0426424408533501, 0.12039935904500836, 0.037652555067463594, 0.13932744842268924, 0.08746272856439741, 0.09124314057927702, 0.04703934178441655, 0.07335740969976129, 0.11550139136410416, 0.05546202699086907, 0.05387252525303594, 0.11889698874303672, 0.05866664663778087, 0.057197571538304987, 0.042299365692532606, 0.06754208648100393, 0.1822503667292515, 0.09856336344883863, 0.05739793564724134, 0.04977641984057214, 0.10423501011712992, 0.20082431661211822, 0.057682106451259325, 0.04880452512019991, 0.06637557949376155, 0.12717102437075237, 0.10052264712942376, 0.12063784965434587, 0.14852594629877902, 0.11352391635801501, 0.12863088905556397, 0.06579363516233692, 0.07728473775288527, 0.09602556660733226, 0.13335989618642485, 0.13542284841263938, 0.030342277509267993, 0.12140057956508445, 0.0512245406036541, 0.1850102311447101, 0.15351932615173192, 0.17499924007245216, 0.08722415767250806, 0.15997439290326562, 0.11021812622736382, 0.13792493950061105, 0.15672408454384426, 0.03996049840908121, 0.06859374273844088, 0.1018028520610543, 0.05925233030105531, 0.05403382889109589, 0.15571440446436957, 0.1442911896026119, 0.1300146108399064, 0.22857157575834447, 0.12832551437004763, 0.10676540625618947, 0.17551415288245217, 0.14028148088902137, 0.11450474311404779, 0.14830891311105526, 0.03597413822404221, 0.11633569819422102, 0.0788113998698265, 0.13509123926509567, 0.07768960120235477, 0.07988402631503716, 0.028209003970633476, 0.03170933828633765, 0.08051061742063548, 0.07526201641767521, 0.17629177392284498, 0.08108965571455169, 0.07310549172833575, 0.06209341236260321, 0.14034665061399337, 0.059227805652137325, 0.045713428088623546, 0.07345583857605972, 0.11014399596884784, 0.09655400127466726, 0.13492066248856877, 0.16955940156034813, 0.05451355757375426, 0.07704803866884324, 0.15888475541456037, 0.05392302683975999, 0.11708078400324996, 0.21654957302503444, 0.06360870743595998, 0.05564393716682934, 0.07065509230424669, 0.05257161549343619, 0.05314522818779206, 0.14050472304072353, 0.06626693839874283, 0.052768319296841254, 0.050256168978529255, 0.11777180480018172, 0.05036043082546638, 0.051892257751357466, 0.06754322164020993, 0.06969873814831411, 0.0975190874598477, 0.0986599139755526, 0.1304072266793048, 0.1534952249345785, 0.04787755038343448, 0.05437144635430575, 0.04397989877587239, 0.05186482996249765, 0.06645079309272126, 0.11392604525768339, 0.061240008173149556, 0.17065318905789964, 0.10693029399908294, 0.04871638771832602, 0.082317502388747, 0.1635543348347432, 0.12297183470485384, 0.05840516782471312, 0.0795716485070913, 0.0668180731419994, 0.0892267233086064, 0.08653370189170442, 0.1555638437467957, 0.20856906554931398, 0.07383921810870614, 0.0942088758909589, 0.16467617274415344, 0.0447102347694822, 0.15974601488874504, 0.09699658646017949, 0.1394715268617684, 0.055114725304602805, 0.13991631815402644, 0.07658416995286911, 0.09913037391701164, 0.10111755274383628, 0.07613965227940134, 0.03189711289112624, 0.11105705503811704, 0.13281582874805967, 0.05568023554923333, 0.10755373780763874, 0.037976005404485966, 0.14316436112072972, 0.05539667176944616, 0.038342416893040465, 0.15192934690143295, 0.03713289960811514, 0.11576624378922173, 0.036605837928524246, 0.11834402000956092, 0.049766338555021376, 0.18265090189287914, 0.15505548082736514, 0.14098218280086483, 0.07540182111689805, 0.07711203615165378, 0.04727568870538148, 0.10935114801163284, 0.14372361768859154, 0.12466987865690131, 0.052435338853592806, 0.07460730452020325, 0.04401859844438987, 0.07695516839912729, 0.1220782792553163, 0.1905583932042503, 0.1262347967200228, 0.02584796574124452, 0.13578145904789127, 0.10893939671672997, 0.052986682557884926, 0.07058275557459744, 0.13725521329974194, 0.17663634276996473, 0.08026908923410293, 0.07666699365954575, 0.12136630918389926, 0.07125447661328702, 0.05425512534396541, 0.06744639852117691, 0.042340953755226576, 0.05917679259381223, 0.11293560380753408, 0.06065232282846036, 0.09593745304243666, 0.1275649657167813, 0.0997500844370986, 0.1178748770870558, 0.06525800457347353, 0.09182435533655267, 0.14625123293348682, 0.09291539578405525, 0.10504819052611213, 0.09611463908356224, 0.06900726484564654, 0.1119741758713535, 0.12188135262302402, 0.09516716430459021, 0.16683997418464366, 0.12897255530787594, 0.09123373325979245, 0.04947443223048749, 0.1398489050747123, 0.2209175323088376, 0.17657332059439473, 0.05462707681695362, 0.03968607745305529, 0.07098879633038684, 0.1469112047928967, 0.06090921682670286, 0.045056684366726274, 0.1922331040758321, 0.12132829608503032, 0.07961976928999348, 0.11193897037942853, 0.06334116223323472, 0.06259950417423756, 0.08867224517393119, 0.12959344014160987, 0.14691054613285998, 0.08227157561320113, 0.07464491703012756, 0.06943230990522573, 0.07484519436520631, 0.04055004752481202, 0.12947082419375794, 0.1711612353526152, 0.10822241669527055, 0.06887473432892313, 0.0788499244539566, 0.12928731513616437, 0.18362383195678134, 0.07422359493102511, 0.1566229356368101, 0.09682285704652346, 0.16030112184684925, 0.10582739646363103, 0.12035931292640079, 0.13402274523755148, 0.14098710540134918, 0.054167372213701465, 0.0456941681952287, 0.04868183454282416, 0.051568413811825205, 0.08431284373021743, 0.1272287640434064, 0.10832367360653061, 0.05198371916585663, 0.11251633726191562, 0.08656297820508119, 0.13322913545729786, 0.04197335660393231, 0.06989827770274343, 0.09446449796626462, 0.04746137483667044, 0.07687502203497615, 0.08186125051793602, 0.04407685956618071, 0.05264429084283937, 0.056086251378746205, 0.09963779343691419, 0.05950533812453688, 0.12732289105572253, 0.06326170193965101, 0.03285977329709666, 0.07700781451643779, 0.07735855208295773, 0.11174370612581061, 0.06682488167210496, 0.09410304378712653, 0.12447896560417518, 0.08278173829522177, 0.04112600812083428, 0.1074044374952255, 0.0409799341570573, 0.08184077672523876, 0.07949050240539257, 0.09751815199411121, 0.06869454137794763, 0.04682151308470707, 0.18322440823379288, 0.03169664356436734, 0.07696494066934441, 0.08075334438116531, 0.15425633391197976, 0.09290110360034193, 0.17061065210323226, 0.19926882196319817, 0.11340381187289052, 0.05700634964232079, 0.02327234763063783, 0.07028986017770993, 0.053886042421977476, 0.1173274442915607, 0.08130250917327665, 0.10119856124220288, 0.05776798487464702, 0.12594776550864892, 0.029110441238130867, 0.15187254976035397, 0.054121417393656564, 0.10893298708125815, 0.05968375031419886, 0.03881569044430167, 0.08631489269181629, 0.1325524938453197, 0.06650603447350302, 0.12230748095861117, 0.12113878740330526, 0.04733583952790564, 0.037817749638885116, 0.08357828311513094, 0.07529262359717083, 0.046994620724677666, 0.05005329908152498, 0.0712148689762823, 0.139983699911019, 0.053218565054999056, 0.06372812268875622, 0.06691012295328658, 0.07780361734912321, 0.07999637296788117, 0.07293699498417387, 0.03746062801771273, 0.06876951578854804, 0.0985199256977886, 0.18251269541970652, 0.09564940672458645, 0.06784267131589361, 0.05446013950662755, 0.047154633753434524, 0.1291503909462608, 0.09360589907249003, 0.07143348046659173, 0.07129545608017036, 0.0753947895825867, 0.06272292549387326, 0.0617559939437198, 0.08326812091722635, 0.19784094654727247, 0.14973618672957506, 0.04699225904889599, 0.07789939333304412, 0.05996894700866094, 0.19667172684720233, 0.17509919168821783, 0.05306126207065486, 0.05833811861429859, 0.0705152075888289, 0.16541705463187092, 0.12938031763507649, 0.08800293462351338, 0.0888248786919127, 0.07790639719246552, 0.12289506606475846, 0.11461664370388668, 0.08981442669293105, 0.11029477363707606, 0.04612798626721595, 0.13229059713132552, 0.08234811798096177, 0.05483087890076831, 0.06319776478062022, 0.06174007409169219, 0.050732981271571644, 0.15118098739569688, 0.06224517724444026, 0.08587352015060633, 0.04143876385679987, 0.0506579328219927, 0.07983074246553172, 0.058738587322779846, 0.10938800128937616, 0.10565051434540361, 0.13922018176808643, 0.08978835083732117, 0.05643770290180355, 0.061459151313720234, 0.1239954621884046, 0.07717809293689869, 0.1161015924608941, 0.03707891317970725, 0.08199673283721508, 0.11052673569659956, 0.06095503427889575, 0.05799597021625148, 0.11506271195225523, 0.06785262183486251, 0.08424283915048265]
0.09861090655195659
Making ranges
torch.Size([29183, 2])
We keep 5.64e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([3228, 2])
We keep 1.46e+05/2.25e+06 =  6% of the original kernel matrix.

torch.Size([9989, 2])
We keep 9.39e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([26728, 2])
We keep 1.18e+07/4.32e+08 =  2% of the original kernel matrix.

torch.Size([27104, 2])
We keep 6.93e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([7959, 2])
We keep 8.67e+05/1.98e+07 =  4% of the original kernel matrix.

torch.Size([15493, 2])
We keep 2.13e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([5266, 2])
We keep 4.53e+05/7.96e+06 =  5% of the original kernel matrix.

torch.Size([12188, 2])
We keep 1.48e+06/5.38e+07 =  2% of the original kernel matrix.

torch.Size([77051, 2])
We keep 1.23e+08/3.78e+09 =  3% of the original kernel matrix.

torch.Size([44288, 2])
We keep 1.68e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([57493, 2])
We keep 3.62e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([38604, 2])
We keep 1.21e+07/7.92e+08 =  1% of the original kernel matrix.

torch.Size([57956, 2])
We keep 3.43e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([38633, 2])
We keep 1.23e+07/8.08e+08 =  1% of the original kernel matrix.

torch.Size([65716, 2])
We keep 3.53e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([40985, 2])
We keep 1.33e+07/8.91e+08 =  1% of the original kernel matrix.

torch.Size([11525, 2])
We keep 7.52e+06/5.71e+07 = 13% of the original kernel matrix.

torch.Size([17643, 2])
We keep 3.14e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([49028, 2])
We keep 8.53e+07/2.30e+09 =  3% of the original kernel matrix.

torch.Size([35731, 2])
We keep 1.32e+07/9.14e+08 =  1% of the original kernel matrix.

torch.Size([12087, 2])
We keep 2.07e+06/5.43e+07 =  3% of the original kernel matrix.

torch.Size([17991, 2])
We keep 2.97e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([160431, 2])
We keep 2.42e+08/1.73e+10 =  1% of the original kernel matrix.

torch.Size([65685, 2])
We keep 3.29e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([13676, 2])
We keep 2.51e+06/7.14e+07 =  3% of the original kernel matrix.

torch.Size([19141, 2])
We keep 3.35e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([169933, 2])
We keep 2.47e+08/1.71e+10 =  1% of the original kernel matrix.

torch.Size([68540, 2])
We keep 3.25e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([25449, 2])
We keep 7.34e+06/3.47e+08 =  2% of the original kernel matrix.

torch.Size([26110, 2])
We keep 6.25e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([89302, 2])
We keep 7.87e+07/4.10e+09 =  1% of the original kernel matrix.

torch.Size([47692, 2])
We keep 1.74e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([45189, 2])
We keep 1.93e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([34780, 2])
We keep 9.95e+06/6.38e+08 =  1% of the original kernel matrix.

torch.Size([37398, 2])
We keep 1.82e+07/8.35e+08 =  2% of the original kernel matrix.

torch.Size([31370, 2])
We keep 8.85e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([371312, 2])
We keep 1.51e+09/7.57e+10 =  1% of the original kernel matrix.

torch.Size([100552, 2])
We keep 6.33e+07/5.25e+09 =  1% of the original kernel matrix.

torch.Size([118835, 2])
We keep 1.28e+08/8.13e+09 =  1% of the original kernel matrix.

torch.Size([56330, 2])
We keep 2.34e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([21858, 2])
We keep 1.08e+07/3.15e+08 =  3% of the original kernel matrix.

torch.Size([23995, 2])
We keep 6.03e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([561231, 2])
We keep 1.59e+09/1.71e+11 =  0% of the original kernel matrix.

torch.Size([128051, 2])
We keep 9.04e+07/7.89e+09 =  1% of the original kernel matrix.

torch.Size([25767, 2])
We keep 8.91e+06/3.67e+08 =  2% of the original kernel matrix.

torch.Size([26420, 2])
We keep 6.40e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([15311, 2])
We keep 3.45e+06/9.77e+07 =  3% of the original kernel matrix.

torch.Size([20381, 2])
We keep 3.75e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([26928, 2])
We keep 5.17e+07/7.11e+08 =  7% of the original kernel matrix.

torch.Size([25355, 2])
We keep 8.10e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([78555, 2])
We keep 4.53e+07/3.21e+09 =  1% of the original kernel matrix.

torch.Size([45063, 2])
We keep 1.56e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([25136, 2])
We keep 6.40e+07/8.49e+08 =  7% of the original kernel matrix.

torch.Size([24724, 2])
We keep 9.07e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([12312, 2])
We keep 2.89e+06/5.96e+07 =  4% of the original kernel matrix.

torch.Size([18040, 2])
We keep 3.10e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([55146, 2])
We keep 6.23e+07/2.04e+09 =  3% of the original kernel matrix.

torch.Size([37625, 2])
We keep 1.30e+07/8.60e+08 =  1% of the original kernel matrix.

torch.Size([2561463, 2])
We keep 1.76e+10/2.69e+12 =  0% of the original kernel matrix.

torch.Size([281782, 2])
We keep 3.27e+08/3.13e+10 =  1% of the original kernel matrix.

torch.Size([6704, 2])
We keep 1.20e+06/1.47e+07 =  8% of the original kernel matrix.

torch.Size([13666, 2])
We keep 1.83e+06/7.31e+07 =  2% of the original kernel matrix.

torch.Size([284147, 2])
We keep 1.36e+09/6.06e+10 =  2% of the original kernel matrix.

torch.Size([89201, 2])
We keep 5.77e+07/4.69e+09 =  1% of the original kernel matrix.

torch.Size([14812, 2])
We keep 3.85e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([19934, 2])
We keep 3.85e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([11719, 2])
We keep 1.60e+06/4.99e+07 =  3% of the original kernel matrix.

torch.Size([17689, 2])
We keep 2.87e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([14532, 2])
We keep 6.15e+06/1.25e+08 =  4% of the original kernel matrix.

torch.Size([19698, 2])
We keep 4.17e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([62225, 2])
We keep 9.47e+07/2.32e+09 =  4% of the original kernel matrix.

torch.Size([39630, 2])
We keep 1.36e+07/9.18e+08 =  1% of the original kernel matrix.

torch.Size([83808, 2])
We keep 7.92e+07/3.56e+09 =  2% of the original kernel matrix.

torch.Size([46523, 2])
We keep 1.64e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([3349, 2])
We keep 1.90e+05/2.68e+06 =  7% of the original kernel matrix.

torch.Size([10187, 2])
We keep 1.03e+06/3.12e+07 =  3% of the original kernel matrix.

torch.Size([6208, 2])
We keep 5.27e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([12939, 2])
We keep 1.64e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([5371, 2])
We keep 5.81e+05/8.71e+06 =  6% of the original kernel matrix.

torch.Size([12099, 2])
We keep 1.55e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([10840, 2])
We keep 2.45e+06/4.35e+07 =  5% of the original kernel matrix.

torch.Size([17139, 2])
We keep 2.81e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([1774, 2])
We keep 9.50e+04/8.28e+05 = 11% of the original kernel matrix.

torch.Size([7820, 2])
We keep 6.81e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([2078, 2])
We keep 7.60e+04/9.92e+05 =  7% of the original kernel matrix.

torch.Size([8448, 2])
We keep 7.20e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([7204, 2])
We keep 9.22e+05/1.51e+07 =  6% of the original kernel matrix.

torch.Size([13771, 2])
We keep 1.87e+06/7.42e+07 =  2% of the original kernel matrix.

torch.Size([2440, 2])
We keep 9.86e+04/1.32e+06 =  7% of the original kernel matrix.

torch.Size([9217, 2])
We keep 7.87e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([2891, 2])
We keep 1.13e+05/1.55e+06 =  7% of the original kernel matrix.

torch.Size([9814, 2])
We keep 8.33e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([8194, 2])
We keep 1.58e+06/2.41e+07 =  6% of the original kernel matrix.

torch.Size([14765, 2])
We keep 2.21e+06/9.37e+07 =  2% of the original kernel matrix.

torch.Size([8155, 2])
We keep 9.88e+05/2.25e+07 =  4% of the original kernel matrix.

torch.Size([14634, 2])
We keep 2.20e+06/9.04e+07 =  2% of the original kernel matrix.

torch.Size([2497, 2])
We keep 9.29e+04/1.22e+06 =  7% of the original kernel matrix.

torch.Size([9209, 2])
We keep 7.69e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([11354, 2])
We keep 2.33e+06/4.84e+07 =  4% of the original kernel matrix.

torch.Size([17300, 2])
We keep 2.90e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([15951, 2])
We keep 5.05e+06/1.39e+08 =  3% of the original kernel matrix.

torch.Size([21069, 2])
We keep 4.48e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([2575, 2])
We keep 1.20e+05/1.50e+06 =  8% of the original kernel matrix.

torch.Size([9038, 2])
We keep 8.20e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([8165, 2])
We keep 1.51e+06/2.48e+07 =  6% of the original kernel matrix.

torch.Size([14808, 2])
We keep 2.25e+06/9.50e+07 =  2% of the original kernel matrix.

torch.Size([6058, 2])
We keep 6.29e+05/9.80e+06 =  6% of the original kernel matrix.

torch.Size([12817, 2])
We keep 1.60e+06/5.97e+07 =  2% of the original kernel matrix.

torch.Size([9015, 2])
We keep 1.11e+06/2.51e+07 =  4% of the original kernel matrix.

torch.Size([15345, 2])
We keep 2.26e+06/9.56e+07 =  2% of the original kernel matrix.

torch.Size([6747, 2])
We keep 2.03e+06/1.85e+07 = 10% of the original kernel matrix.

torch.Size([13507, 2])
We keep 1.74e+06/8.20e+07 =  2% of the original kernel matrix.

torch.Size([9369, 2])
We keep 3.15e+06/3.59e+07 =  8% of the original kernel matrix.

torch.Size([15885, 2])
We keep 2.60e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([4902, 2])
We keep 3.02e+05/5.35e+06 =  5% of the original kernel matrix.

torch.Size([11830, 2])
We keep 1.29e+06/4.41e+07 =  2% of the original kernel matrix.

torch.Size([4889, 2])
We keep 3.25e+05/5.62e+06 =  5% of the original kernel matrix.

torch.Size([11949, 2])
We keep 1.30e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([4103, 2])
We keep 4.21e+05/5.04e+06 =  8% of the original kernel matrix.

torch.Size([10804, 2])
We keep 1.26e+06/4.28e+07 =  2% of the original kernel matrix.

torch.Size([7476, 2])
We keep 1.02e+06/1.93e+07 =  5% of the original kernel matrix.

torch.Size([14439, 2])
We keep 2.12e+06/8.37e+07 =  2% of the original kernel matrix.

torch.Size([3806, 2])
We keep 2.36e+05/3.48e+06 =  6% of the original kernel matrix.

torch.Size([10765, 2])
We keep 1.11e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([7900, 2])
We keep 1.33e+06/2.06e+07 =  6% of the original kernel matrix.

torch.Size([14541, 2])
We keep 2.10e+06/8.66e+07 =  2% of the original kernel matrix.

torch.Size([11492, 2])
We keep 1.67e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([17417, 2])
We keep 2.85e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([6684, 2])
We keep 6.66e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([13431, 2])
We keep 1.82e+06/7.21e+07 =  2% of the original kernel matrix.

torch.Size([5070, 2])
We keep 4.41e+05/6.72e+06 =  6% of the original kernel matrix.

torch.Size([12071, 2])
We keep 1.40e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([6740, 2])
We keep 6.00e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([13534, 2])
We keep 1.72e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([11218, 2])
We keep 1.73e+06/4.65e+07 =  3% of the original kernel matrix.

torch.Size([17013, 2])
We keep 2.88e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([6932, 2])
We keep 5.32e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([13854, 2])
We keep 1.69e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([3813, 2])
We keep 2.17e+05/3.31e+06 =  6% of the original kernel matrix.

torch.Size([10576, 2])
We keep 1.09e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([11212, 2])
We keep 2.09e+06/5.18e+07 =  4% of the original kernel matrix.

torch.Size([17049, 2])
We keep 3.01e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([6152, 2])
We keep 4.94e+05/9.95e+06 =  4% of the original kernel matrix.

torch.Size([12870, 2])
We keep 1.61e+06/6.01e+07 =  2% of the original kernel matrix.

torch.Size([7391, 2])
We keep 7.49e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([14143, 2])
We keep 1.88e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([3494, 2])
We keep 2.53e+05/3.23e+06 =  7% of the original kernel matrix.

torch.Size([10235, 2])
We keep 1.07e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([5516, 2])
We keep 4.71e+05/8.45e+06 =  5% of the original kernel matrix.

torch.Size([12298, 2])
We keep 1.51e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([6102, 2])
We keep 7.25e+05/1.09e+07 =  6% of the original kernel matrix.

torch.Size([12974, 2])
We keep 1.66e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([2753, 2])
We keep 1.44e+05/1.80e+06 =  7% of the original kernel matrix.

torch.Size([9628, 2])
We keep 8.74e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([2333, 2])
We keep 1.04e+05/1.30e+06 =  8% of the original kernel matrix.

torch.Size([8975, 2])
We keep 7.87e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([8369, 2])
We keep 1.18e+06/2.21e+07 =  5% of the original kernel matrix.

torch.Size([14949, 2])
We keep 2.15e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([26056, 2])
We keep 1.10e+07/4.60e+08 =  2% of the original kernel matrix.

torch.Size([26586, 2])
We keep 7.15e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([6722, 2])
We keep 8.32e+05/1.33e+07 =  6% of the original kernel matrix.

torch.Size([13460, 2])
We keep 1.82e+06/6.96e+07 =  2% of the original kernel matrix.

torch.Size([2390, 2])
We keep 1.22e+05/1.48e+06 =  8% of the original kernel matrix.

torch.Size([8915, 2])
We keep 8.20e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([5868, 2])
We keep 8.19e+05/1.19e+07 =  6% of the original kernel matrix.

torch.Size([12682, 2])
We keep 1.70e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([3241, 2])
We keep 1.95e+05/2.44e+06 =  8% of the original kernel matrix.

torch.Size([10049, 2])
We keep 9.89e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([4884, 2])
We keep 9.60e+05/7.65e+06 = 12% of the original kernel matrix.

torch.Size([11758, 2])
We keep 1.46e+06/5.27e+07 =  2% of the original kernel matrix.

torch.Size([6295, 2])
We keep 7.10e+05/1.07e+07 =  6% of the original kernel matrix.

torch.Size([13356, 2])
We keep 1.67e+06/6.25e+07 =  2% of the original kernel matrix.

torch.Size([3740, 2])
We keep 2.33e+05/3.68e+06 =  6% of the original kernel matrix.

torch.Size([10563, 2])
We keep 1.12e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([1876, 2])
We keep 7.25e+04/7.31e+05 =  9% of the original kernel matrix.

torch.Size([8149, 2])
We keep 6.49e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([6224, 2])
We keep 5.26e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([13058, 2])
We keep 1.61e+06/6.05e+07 =  2% of the original kernel matrix.

torch.Size([8613, 2])
We keep 9.81e+05/2.18e+07 =  4% of the original kernel matrix.

torch.Size([15178, 2])
We keep 2.15e+06/8.90e+07 =  2% of the original kernel matrix.

torch.Size([6784, 2])
We keep 6.60e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([13401, 2])
We keep 1.84e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([3717, 2])
We keep 1.93e+05/3.12e+06 =  6% of the original kernel matrix.

torch.Size([10753, 2])
We keep 1.05e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([3621, 2])
We keep 2.11e+05/2.91e+06 =  7% of the original kernel matrix.

torch.Size([10686, 2])
We keep 1.04e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([6676, 2])
We keep 6.71e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([13442, 2])
We keep 1.82e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([6344, 2])
We keep 6.71e+05/1.18e+07 =  5% of the original kernel matrix.

torch.Size([13093, 2])
We keep 1.70e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([6745, 2])
We keep 6.50e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([13500, 2])
We keep 1.80e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([7028, 2])
We keep 6.73e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([13922, 2])
We keep 1.79e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([8770, 2])
We keep 3.61e+06/4.16e+07 =  8% of the original kernel matrix.

torch.Size([15047, 2])
We keep 2.75e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([5138, 2])
We keep 3.91e+05/6.75e+06 =  5% of the original kernel matrix.

torch.Size([12128, 2])
We keep 1.40e+06/4.95e+07 =  2% of the original kernel matrix.

torch.Size([6314, 2])
We keep 5.09e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([13099, 2])
We keep 1.62e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([10148, 2])
We keep 1.48e+06/3.62e+07 =  4% of the original kernel matrix.

torch.Size([16218, 2])
We keep 2.58e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([4572, 2])
We keep 4.69e+05/6.36e+06 =  7% of the original kernel matrix.

torch.Size([11345, 2])
We keep 1.34e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([7613, 2])
We keep 6.96e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([14282, 2])
We keep 1.90e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([2640, 2])
We keep 1.50e+05/1.59e+06 =  9% of the original kernel matrix.

torch.Size([9234, 2])
We keep 8.49e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([5306, 2])
We keep 4.15e+05/7.04e+06 =  5% of the original kernel matrix.

torch.Size([12106, 2])
We keep 1.41e+06/5.06e+07 =  2% of the original kernel matrix.

torch.Size([3035, 2])
We keep 2.32e+05/2.73e+06 =  8% of the original kernel matrix.

torch.Size([9730, 2])
We keep 1.03e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([9371, 2])
We keep 1.20e+06/2.84e+07 =  4% of the original kernel matrix.

torch.Size([15808, 2])
We keep 2.36e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([2481, 2])
We keep 8.58e+04/1.10e+06 =  7% of the original kernel matrix.

torch.Size([9310, 2])
We keep 7.39e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([4830, 2])
We keep 3.11e+05/5.51e+06 =  5% of the original kernel matrix.

torch.Size([11926, 2])
We keep 1.29e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([2437, 2])
We keep 1.01e+05/1.35e+06 =  7% of the original kernel matrix.

torch.Size([9064, 2])
We keep 8.08e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([3642, 2])
We keep 2.31e+05/3.10e+06 =  7% of the original kernel matrix.

torch.Size([10527, 2])
We keep 1.08e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([2793, 2])
We keep 1.80e+05/1.81e+06 =  9% of the original kernel matrix.

torch.Size([9532, 2])
We keep 8.80e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([3799, 2])
We keep 2.32e+05/3.28e+06 =  7% of the original kernel matrix.

torch.Size([10659, 2])
We keep 1.08e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([6898, 2])
We keep 7.27e+05/1.42e+07 =  5% of the original kernel matrix.

torch.Size([13674, 2])
We keep 1.82e+06/7.20e+07 =  2% of the original kernel matrix.

torch.Size([9035, 2])
We keep 1.43e+06/2.95e+07 =  4% of the original kernel matrix.

torch.Size([15858, 2])
We keep 2.45e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([4179, 2])
We keep 6.15e+05/6.58e+06 =  9% of the original kernel matrix.

torch.Size([10891, 2])
We keep 1.40e+06/4.89e+07 =  2% of the original kernel matrix.

torch.Size([8210, 2])
We keep 1.94e+06/2.46e+07 =  7% of the original kernel matrix.

torch.Size([14813, 2])
We keep 2.27e+06/9.46e+07 =  2% of the original kernel matrix.

torch.Size([2327, 2])
We keep 1.20e+05/1.43e+06 =  8% of the original kernel matrix.

torch.Size([8810, 2])
We keep 8.21e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([11018, 2])
We keep 1.51e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([17156, 2])
We keep 2.73e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([2391, 2])
We keep 1.16e+05/1.30e+06 =  8% of the original kernel matrix.

torch.Size([8886, 2])
We keep 8.05e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([19606, 2])
We keep 5.29e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([23545, 2])
We keep 5.05e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([4364, 2])
We keep 2.56e+05/4.26e+06 =  6% of the original kernel matrix.

torch.Size([11400, 2])
We keep 1.19e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([3812, 2])
We keep 2.40e+05/3.55e+06 =  6% of the original kernel matrix.

torch.Size([11071, 2])
We keep 1.13e+06/3.59e+07 =  3% of the original kernel matrix.

torch.Size([5635, 2])
We keep 4.92e+05/9.46e+06 =  5% of the original kernel matrix.

torch.Size([12917, 2])
We keep 1.58e+06/5.87e+07 =  2% of the original kernel matrix.

torch.Size([3768, 2])
We keep 2.03e+05/3.23e+06 =  6% of the original kernel matrix.

torch.Size([10624, 2])
We keep 1.09e+06/3.42e+07 =  3% of the original kernel matrix.

torch.Size([4341, 2])
We keep 3.70e+05/5.88e+06 =  6% of the original kernel matrix.

torch.Size([11326, 2])
We keep 1.38e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([3071, 2])
We keep 1.33e+05/1.97e+06 =  6% of the original kernel matrix.

torch.Size([10022, 2])
We keep 9.03e+05/2.67e+07 =  3% of the original kernel matrix.

torch.Size([4491, 2])
We keep 2.63e+05/4.41e+06 =  5% of the original kernel matrix.

torch.Size([11523, 2])
We keep 1.20e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([15096, 2])
We keep 3.42e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([20220, 2])
We keep 3.81e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([5918, 2])
We keep 5.64e+05/9.32e+06 =  6% of the original kernel matrix.

torch.Size([12764, 2])
We keep 1.57e+06/5.82e+07 =  2% of the original kernel matrix.

torch.Size([15448, 2])
We keep 3.13e+06/9.57e+07 =  3% of the original kernel matrix.

torch.Size([20424, 2])
We keep 3.74e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([2905, 2])
We keep 1.46e+05/1.93e+06 =  7% of the original kernel matrix.

torch.Size([9777, 2])
We keep 8.90e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([5919, 2])
We keep 4.78e+05/8.87e+06 =  5% of the original kernel matrix.

torch.Size([12843, 2])
We keep 1.55e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([2282, 2])
We keep 1.04e+05/1.05e+06 =  9% of the original kernel matrix.

torch.Size([8771, 2])
We keep 7.37e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([4934, 2])
We keep 3.26e+05/5.81e+06 =  5% of the original kernel matrix.

torch.Size([11938, 2])
We keep 1.33e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([3853, 2])
We keep 2.08e+05/3.46e+06 =  6% of the original kernel matrix.

torch.Size([10887, 2])
We keep 1.10e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([2814, 2])
We keep 1.54e+05/1.94e+06 =  7% of the original kernel matrix.

torch.Size([9473, 2])
We keep 9.08e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([3672, 2])
We keep 2.33e+05/3.00e+06 =  7% of the original kernel matrix.

torch.Size([10653, 2])
We keep 1.06e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([3728, 2])
We keep 2.13e+05/3.14e+06 =  6% of the original kernel matrix.

torch.Size([10644, 2])
We keep 1.07e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([4593, 2])
We keep 3.17e+05/5.32e+06 =  5% of the original kernel matrix.

torch.Size([11645, 2])
We keep 1.28e+06/4.40e+07 =  2% of the original kernel matrix.

torch.Size([3612, 2])
We keep 2.02e+05/2.98e+06 =  6% of the original kernel matrix.

torch.Size([10351, 2])
We keep 1.04e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([2645, 2])
We keep 2.65e+05/1.92e+06 = 13% of the original kernel matrix.

torch.Size([9243, 2])
We keep 9.15e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([11797, 2])
We keep 2.17e+06/5.43e+07 =  3% of the original kernel matrix.

torch.Size([17571, 2])
We keep 3.01e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([10165, 2])
We keep 1.89e+06/4.23e+07 =  4% of the original kernel matrix.

torch.Size([16191, 2])
We keep 2.75e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([6122, 2])
We keep 5.97e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([12911, 2])
We keep 1.67e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([5335, 2])
We keep 3.70e+05/6.82e+06 =  5% of the original kernel matrix.

torch.Size([12310, 2])
We keep 1.42e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([6186, 2])
We keep 5.27e+05/9.15e+06 =  5% of the original kernel matrix.

torch.Size([13456, 2])
We keep 1.57e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([3860, 2])
We keep 3.39e+05/4.25e+06 =  7% of the original kernel matrix.

torch.Size([10953, 2])
We keep 1.22e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([3866, 2])
We keep 2.01e+05/3.26e+06 =  6% of the original kernel matrix.

torch.Size([10942, 2])
We keep 1.08e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([3155, 2])
We keep 1.48e+05/2.06e+06 =  7% of the original kernel matrix.

torch.Size([10075, 2])
We keep 9.17e+05/2.74e+07 =  3% of the original kernel matrix.

torch.Size([3551, 2])
We keep 2.86e+05/4.00e+06 =  7% of the original kernel matrix.

torch.Size([10304, 2])
We keep 1.18e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([6779, 2])
We keep 7.59e+05/1.35e+07 =  5% of the original kernel matrix.

torch.Size([13527, 2])
We keep 1.82e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([4434, 2])
We keep 3.04e+05/4.57e+06 =  6% of the original kernel matrix.

torch.Size([11494, 2])
We keep 1.22e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([9188, 2])
We keep 3.16e+06/4.64e+07 =  6% of the original kernel matrix.

torch.Size([15495, 2])
We keep 2.88e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([5130, 2])
We keep 6.75e+05/8.11e+06 =  8% of the original kernel matrix.

torch.Size([12020, 2])
We keep 1.51e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([10214, 2])
We keep 1.90e+06/4.05e+07 =  4% of the original kernel matrix.

torch.Size([16309, 2])
We keep 2.67e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([6050, 2])
We keep 4.84e+05/9.63e+06 =  5% of the original kernel matrix.

torch.Size([12915, 2])
We keep 1.59e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([5555, 2])
We keep 4.38e+05/7.75e+06 =  5% of the original kernel matrix.

torch.Size([12578, 2])
We keep 1.47e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([6382, 2])
We keep 9.58e+05/1.46e+07 =  6% of the original kernel matrix.

torch.Size([13114, 2])
We keep 1.87e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([2549, 2])
We keep 8.84e+04/1.15e+06 =  7% of the original kernel matrix.

torch.Size([9259, 2])
We keep 7.64e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([12395, 2])
We keep 1.90e+06/5.69e+07 =  3% of the original kernel matrix.

torch.Size([18882, 2])
We keep 3.14e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([4230, 2])
We keep 2.92e+05/4.26e+06 =  6% of the original kernel matrix.

torch.Size([11156, 2])
We keep 1.19e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([4235, 2])
We keep 3.07e+05/4.32e+06 =  7% of the original kernel matrix.

torch.Size([11039, 2])
We keep 1.21e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([3066, 2])
We keep 1.65e+05/2.05e+06 =  8% of the original kernel matrix.

torch.Size([9737, 2])
We keep 9.26e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([5320, 2])
We keep 4.81e+05/7.50e+06 =  6% of the original kernel matrix.

torch.Size([12197, 2])
We keep 1.45e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([4955, 2])
We keep 3.54e+05/5.80e+06 =  6% of the original kernel matrix.

torch.Size([11947, 2])
We keep 1.34e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([6763, 2])
We keep 6.81e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([13662, 2])
We keep 1.79e+06/7.21e+07 =  2% of the original kernel matrix.

torch.Size([5433, 2])
We keep 4.38e+05/7.51e+06 =  5% of the original kernel matrix.

torch.Size([12275, 2])
We keep 1.44e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([7587, 2])
We keep 9.49e+05/1.88e+07 =  5% of the original kernel matrix.

torch.Size([14238, 2])
We keep 2.07e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([1806, 2])
We keep 9.13e+04/6.84e+05 = 13% of the original kernel matrix.

torch.Size([7958, 2])
We keep 6.44e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([2329, 2])
We keep 1.86e+05/1.54e+06 = 12% of the original kernel matrix.

torch.Size([8521, 2])
We keep 8.39e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([2374, 2])
We keep 1.29e+05/1.50e+06 =  8% of the original kernel matrix.

torch.Size([8658, 2])
We keep 8.35e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([11503, 2])
We keep 1.71e+06/4.93e+07 =  3% of the original kernel matrix.

torch.Size([17595, 2])
We keep 2.94e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([4044, 2])
We keep 2.49e+05/3.89e+06 =  6% of the original kernel matrix.

torch.Size([10951, 2])
We keep 1.17e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([8523, 2])
We keep 1.27e+06/2.57e+07 =  4% of the original kernel matrix.

torch.Size([15783, 2])
We keep 2.31e+06/9.67e+07 =  2% of the original kernel matrix.

torch.Size([5353, 2])
We keep 4.96e+05/8.42e+06 =  5% of the original kernel matrix.

torch.Size([12425, 2])
We keep 1.48e+06/5.53e+07 =  2% of the original kernel matrix.

torch.Size([6958, 2])
We keep 8.84e+05/1.45e+07 =  6% of the original kernel matrix.

torch.Size([13607, 2])
We keep 1.84e+06/7.27e+07 =  2% of the original kernel matrix.

torch.Size([8222, 2])
We keep 8.65e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([14739, 2])
We keep 2.04e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([5763, 2])
We keep 7.64e+05/1.11e+07 =  6% of the original kernel matrix.

torch.Size([12541, 2])
We keep 1.68e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([6052, 2])
We keep 5.60e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([12718, 2])
We keep 1.66e+06/6.26e+07 =  2% of the original kernel matrix.

torch.Size([7368, 2])
We keep 8.22e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([14225, 2])
We keep 1.99e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([3938, 2])
We keep 3.19e+05/4.05e+06 =  7% of the original kernel matrix.

torch.Size([10676, 2])
We keep 1.18e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([5932, 2])
We keep 7.69e+05/1.10e+07 =  6% of the original kernel matrix.

torch.Size([12720, 2])
We keep 1.67e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([3756, 2])
We keep 2.39e+05/3.35e+06 =  7% of the original kernel matrix.

torch.Size([10673, 2])
We keep 1.10e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([8226, 2])
We keep 9.41e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([14817, 2])
We keep 2.06e+06/8.39e+07 =  2% of the original kernel matrix.

torch.Size([9323, 2])
We keep 1.07e+06/2.78e+07 =  3% of the original kernel matrix.

torch.Size([15730, 2])
We keep 2.35e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([3638, 2])
We keep 1.97e+05/2.96e+06 =  6% of the original kernel matrix.

torch.Size([10521, 2])
We keep 1.05e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([4309, 2])
We keep 2.88e+05/4.75e+06 =  6% of the original kernel matrix.

torch.Size([11249, 2])
We keep 1.22e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([14591, 2])
We keep 2.59e+06/8.74e+07 =  2% of the original kernel matrix.

torch.Size([20994, 2])
We keep 3.75e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([10343, 2])
We keep 3.36e+06/5.04e+07 =  6% of the original kernel matrix.

torch.Size([16325, 2])
We keep 2.95e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([4772, 2])
We keep 3.35e+05/5.76e+06 =  5% of the original kernel matrix.

torch.Size([11686, 2])
We keep 1.32e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([2563, 2])
We keep 3.10e+05/1.97e+06 = 15% of the original kernel matrix.

torch.Size([9058, 2])
We keep 9.43e+05/2.68e+07 =  3% of the original kernel matrix.

torch.Size([5301, 2])
We keep 4.01e+05/6.80e+06 =  5% of the original kernel matrix.

torch.Size([12327, 2])
We keep 1.39e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([6016, 2])
We keep 6.84e+05/1.08e+07 =  6% of the original kernel matrix.

torch.Size([12919, 2])
We keep 1.65e+06/6.28e+07 =  2% of the original kernel matrix.

torch.Size([5696, 2])
We keep 5.97e+05/9.39e+06 =  6% of the original kernel matrix.

torch.Size([12517, 2])
We keep 1.58e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([5120, 2])
We keep 4.16e+05/6.74e+06 =  6% of the original kernel matrix.

torch.Size([11939, 2])
We keep 1.40e+06/4.95e+07 =  2% of the original kernel matrix.

torch.Size([4231, 2])
We keep 3.05e+05/4.89e+06 =  6% of the original kernel matrix.

torch.Size([11153, 2])
We keep 1.25e+06/4.22e+07 =  2% of the original kernel matrix.

torch.Size([2345, 2])
We keep 1.47e+05/1.36e+06 = 10% of the original kernel matrix.

torch.Size([8998, 2])
We keep 8.04e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([4435, 2])
We keep 2.61e+05/4.65e+06 =  5% of the original kernel matrix.

torch.Size([11455, 2])
We keep 1.23e+06/4.11e+07 =  2% of the original kernel matrix.

torch.Size([3442, 2])
We keep 2.65e+05/3.64e+06 =  7% of the original kernel matrix.

torch.Size([10179, 2])
We keep 1.12e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([11924, 2])
We keep 1.80e+06/5.17e+07 =  3% of the original kernel matrix.

torch.Size([18124, 2])
We keep 3.00e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([3741, 2])
We keep 2.22e+05/3.26e+06 =  6% of the original kernel matrix.

torch.Size([10643, 2])
We keep 1.09e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([11579, 2])
We keep 2.01e+06/4.70e+07 =  4% of the original kernel matrix.

torch.Size([17565, 2])
We keep 2.86e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([7414, 2])
We keep 8.53e+05/1.62e+07 =  5% of the original kernel matrix.

torch.Size([14184, 2])
We keep 1.91e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([9489, 2])
We keep 1.40e+06/3.37e+07 =  4% of the original kernel matrix.

torch.Size([15807, 2])
We keep 2.57e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([3608, 2])
We keep 2.10e+05/2.82e+06 =  7% of the original kernel matrix.

torch.Size([10578, 2])
We keep 1.02e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([7102, 2])
We keep 1.06e+06/1.71e+07 =  6% of the original kernel matrix.

torch.Size([14455, 2])
We keep 2.04e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([6146, 2])
We keep 7.08e+05/1.16e+07 =  6% of the original kernel matrix.

torch.Size([13052, 2])
We keep 1.74e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([4615, 2])
We keep 3.97e+05/5.46e+06 =  7% of the original kernel matrix.

torch.Size([11704, 2])
We keep 1.28e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([10626, 2])
We keep 1.89e+06/4.68e+07 =  4% of the original kernel matrix.

torch.Size([16647, 2])
We keep 2.91e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([6565, 2])
We keep 6.68e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([13207, 2])
We keep 1.73e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([4066, 2])
We keep 3.17e+05/4.39e+06 =  7% of the original kernel matrix.

torch.Size([10963, 2])
We keep 1.20e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([6319, 2])
We keep 5.94e+05/1.16e+07 =  5% of the original kernel matrix.

torch.Size([13061, 2])
We keep 1.71e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([7588, 2])
We keep 7.38e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([14288, 2])
We keep 1.88e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([1810, 2])
We keep 8.74e+04/7.66e+05 = 11% of the original kernel matrix.

torch.Size([8110, 2])
We keep 6.60e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([3730, 2])
We keep 3.02e+05/3.84e+06 =  7% of the original kernel matrix.

torch.Size([10523, 2])
We keep 1.16e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([2776, 2])
We keep 1.39e+05/1.89e+06 =  7% of the original kernel matrix.

torch.Size([9562, 2])
We keep 8.88e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([1757, 2])
We keep 4.41e+04/5.06e+05 =  8% of the original kernel matrix.

torch.Size([8166, 2])
We keep 5.66e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([6649, 2])
We keep 9.27e+05/1.27e+07 =  7% of the original kernel matrix.

torch.Size([13523, 2])
We keep 1.75e+06/6.80e+07 =  2% of the original kernel matrix.

torch.Size([5684, 2])
We keep 4.33e+05/8.33e+06 =  5% of the original kernel matrix.

torch.Size([12589, 2])
We keep 1.51e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([7432, 2])
We keep 2.35e+06/2.67e+07 =  8% of the original kernel matrix.

torch.Size([13808, 2])
We keep 2.32e+06/9.85e+07 =  2% of the original kernel matrix.

torch.Size([2610, 2])
We keep 9.82e+04/1.44e+06 =  6% of the original kernel matrix.

torch.Size([9523, 2])
We keep 8.05e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([9766, 2])
We keep 1.57e+06/3.56e+07 =  4% of the original kernel matrix.

torch.Size([15904, 2])
We keep 2.63e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([4341, 2])
We keep 3.43e+05/4.76e+06 =  7% of the original kernel matrix.

torch.Size([11269, 2])
We keep 1.24e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([4125, 2])
We keep 2.97e+05/4.25e+06 =  6% of the original kernel matrix.

torch.Size([11067, 2])
We keep 1.19e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([4702, 2])
We keep 4.22e+05/6.55e+06 =  6% of the original kernel matrix.

torch.Size([11507, 2])
We keep 1.36e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([4783, 2])
We keep 3.66e+05/6.13e+06 =  5% of the original kernel matrix.

torch.Size([11518, 2])
We keep 1.35e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([2894, 2])
We keep 1.32e+05/1.55e+06 =  8% of the original kernel matrix.

torch.Size([9761, 2])
We keep 8.36e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([6050, 2])
We keep 5.35e+05/9.45e+06 =  5% of the original kernel matrix.

torch.Size([12937, 2])
We keep 1.57e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([2649, 2])
We keep 1.32e+05/1.46e+06 =  9% of the original kernel matrix.

torch.Size([9236, 2])
We keep 8.17e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([2791, 2])
We keep 1.37e+05/1.73e+06 =  7% of the original kernel matrix.

torch.Size([9538, 2])
We keep 8.70e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([4447, 2])
We keep 3.89e+05/5.47e+06 =  7% of the original kernel matrix.

torch.Size([11223, 2])
We keep 1.31e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([3838, 2])
We keep 2.93e+05/4.26e+06 =  6% of the original kernel matrix.

torch.Size([10792, 2])
We keep 1.22e+06/3.94e+07 =  3% of the original kernel matrix.

torch.Size([8938, 2])
We keep 1.44e+06/2.73e+07 =  5% of the original kernel matrix.

torch.Size([16250, 2])
We keep 2.42e+06/9.96e+07 =  2% of the original kernel matrix.

torch.Size([2900, 2])
We keep 1.65e+05/1.93e+06 =  8% of the original kernel matrix.

torch.Size([9525, 2])
We keep 9.07e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([4310, 2])
We keep 3.40e+05/4.44e+06 =  7% of the original kernel matrix.

torch.Size([11295, 2])
We keep 1.21e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([6056, 2])
We keep 5.99e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([13067, 2])
We keep 1.72e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([11136, 2])
We keep 2.05e+06/5.15e+07 =  3% of the original kernel matrix.

torch.Size([17224, 2])
We keep 3.00e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([3049, 2])
We keep 1.59e+05/2.27e+06 =  6% of the original kernel matrix.

torch.Size([9826, 2])
We keep 9.61e+05/2.87e+07 =  3% of the original kernel matrix.

torch.Size([11421, 2])
We keep 2.23e+06/5.27e+07 =  4% of the original kernel matrix.

torch.Size([17420, 2])
We keep 2.95e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([41757, 2])
We keep 6.76e+07/1.39e+09 =  4% of the original kernel matrix.

torch.Size([32823, 2])
We keep 1.10e+07/7.10e+08 =  1% of the original kernel matrix.

torch.Size([22301, 2])
We keep 1.96e+07/3.00e+08 =  6% of the original kernel matrix.

torch.Size([24949, 2])
We keep 5.29e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([11277, 2])
We keep 2.28e+06/4.66e+07 =  4% of the original kernel matrix.

torch.Size([17339, 2])
We keep 2.85e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([1914, 2])
We keep 6.33e+04/7.38e+05 =  8% of the original kernel matrix.

torch.Size([8173, 2])
We keep 6.48e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([11410, 2])
We keep 1.70e+06/4.79e+07 =  3% of the original kernel matrix.

torch.Size([17385, 2])
We keep 2.87e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([53779, 2])
We keep 2.38e+08/1.84e+09 = 12% of the original kernel matrix.

torch.Size([37219, 2])
We keep 1.23e+07/8.17e+08 =  1% of the original kernel matrix.

torch.Size([121915, 2])
We keep 9.22e+08/1.61e+10 =  5% of the original kernel matrix.

torch.Size([56648, 2])
We keep 3.17e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([6235, 2])
We keep 7.09e+05/1.02e+07 =  6% of the original kernel matrix.

torch.Size([13322, 2])
We keep 1.59e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([86845, 2])
We keep 1.62e+08/5.20e+09 =  3% of the original kernel matrix.

torch.Size([47287, 2])
We keep 1.93e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([26012, 2])
We keep 1.73e+07/4.42e+08 =  3% of the original kernel matrix.

torch.Size([25981, 2])
We keep 6.88e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([32106, 2])
We keep 3.28e+07/6.80e+08 =  4% of the original kernel matrix.

torch.Size([29295, 2])
We keep 8.33e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([461993, 2])
We keep 2.89e+09/1.54e+11 =  1% of the original kernel matrix.

torch.Size([113754, 2])
We keep 8.82e+07/7.49e+09 =  1% of the original kernel matrix.

torch.Size([4230, 2])
We keep 3.05e+05/4.28e+06 =  7% of the original kernel matrix.

torch.Size([11229, 2])
We keep 1.18e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([8329, 2])
We keep 1.47e+06/2.61e+07 =  5% of the original kernel matrix.

torch.Size([14823, 2])
We keep 2.28e+06/9.75e+07 =  2% of the original kernel matrix.

torch.Size([229059, 2])
We keep 7.24e+08/4.16e+10 =  1% of the original kernel matrix.

torch.Size([79207, 2])
We keep 4.88e+07/3.89e+09 =  1% of the original kernel matrix.

torch.Size([877354, 2])
We keep 3.37e+09/3.83e+11 =  0% of the original kernel matrix.

torch.Size([160437, 2])
We keep 1.33e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([7338, 2])
We keep 9.26e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([14032, 2])
We keep 2.05e+06/8.43e+07 =  2% of the original kernel matrix.

torch.Size([34521, 2])
We keep 2.24e+07/8.18e+08 =  2% of the original kernel matrix.

torch.Size([30143, 2])
We keep 8.90e+06/5.45e+08 =  1% of the original kernel matrix.

torch.Size([22609, 2])
We keep 1.29e+08/4.22e+08 = 30% of the original kernel matrix.

torch.Size([24503, 2])
We keep 5.48e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([114690, 2])
We keep 3.74e+08/9.04e+09 =  4% of the original kernel matrix.

torch.Size([55209, 2])
We keep 2.40e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([28442, 2])
We keep 2.55e+07/5.66e+08 =  4% of the original kernel matrix.

torch.Size([27023, 2])
We keep 7.62e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([122839, 2])
We keep 3.57e+08/1.24e+10 =  2% of the original kernel matrix.

torch.Size([55579, 2])
We keep 2.83e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([39692, 2])
We keep 3.92e+07/9.85e+08 =  3% of the original kernel matrix.

torch.Size([32540, 2])
We keep 9.43e+06/5.98e+08 =  1% of the original kernel matrix.

torch.Size([14332, 2])
We keep 2.54e+06/8.10e+07 =  3% of the original kernel matrix.

torch.Size([19572, 2])
We keep 3.46e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([9436, 2])
We keep 4.55e+06/4.06e+07 = 11% of the original kernel matrix.

torch.Size([15641, 2])
We keep 2.78e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([129243, 2])
We keep 1.47e+09/3.76e+10 =  3% of the original kernel matrix.

torch.Size([56411, 2])
We keep 4.54e+07/3.70e+09 =  1% of the original kernel matrix.

torch.Size([10196, 2])
We keep 2.16e+06/3.75e+07 =  5% of the original kernel matrix.

torch.Size([16455, 2])
We keep 2.53e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([4623, 2])
We keep 3.91e+05/6.24e+06 =  6% of the original kernel matrix.

torch.Size([11681, 2])
We keep 1.37e+06/4.76e+07 =  2% of the original kernel matrix.

torch.Size([84144, 2])
We keep 8.50e+07/4.09e+09 =  2% of the original kernel matrix.

torch.Size([46273, 2])
We keep 1.72e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([30037, 2])
We keep 3.76e+07/7.77e+08 =  4% of the original kernel matrix.

torch.Size([27875, 2])
We keep 8.78e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([171911, 2])
We keep 4.49e+08/2.33e+10 =  1% of the original kernel matrix.

torch.Size([69260, 2])
We keep 3.80e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([163075, 2])
We keep 2.01e+08/1.51e+10 =  1% of the original kernel matrix.

torch.Size([66628, 2])
We keep 3.09e+07/2.35e+09 =  1% of the original kernel matrix.

torch.Size([3307, 2])
We keep 2.65e+05/3.11e+06 =  8% of the original kernel matrix.

torch.Size([9892, 2])
We keep 1.06e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([20279, 2])
We keep 9.15e+06/2.39e+08 =  3% of the original kernel matrix.

torch.Size([23586, 2])
We keep 5.50e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([7901, 2])
We keep 1.39e+06/2.01e+07 =  6% of the original kernel matrix.

torch.Size([14579, 2])
We keep 2.07e+06/8.55e+07 =  2% of the original kernel matrix.

torch.Size([29325, 2])
We keep 2.77e+07/5.42e+08 =  5% of the original kernel matrix.

torch.Size([28056, 2])
We keep 7.39e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([87122, 2])
We keep 7.07e+07/4.29e+09 =  1% of the original kernel matrix.

torch.Size([47954, 2])
We keep 1.74e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([22346, 2])
We keep 9.27e+07/3.83e+08 = 24% of the original kernel matrix.

torch.Size([23862, 2])
We keep 6.15e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([93048, 2])
We keep 7.41e+07/4.85e+09 =  1% of the original kernel matrix.

torch.Size([49004, 2])
We keep 1.89e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([2092, 2])
We keep 9.26e+04/1.04e+06 =  8% of the original kernel matrix.

torch.Size([8418, 2])
We keep 7.19e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([8059, 2])
We keep 9.53e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([14718, 2])
We keep 2.02e+06/8.36e+07 =  2% of the original kernel matrix.

torch.Size([34373, 2])
We keep 5.01e+07/9.05e+08 =  5% of the original kernel matrix.

torch.Size([29852, 2])
We keep 9.19e+06/5.74e+08 =  1% of the original kernel matrix.

torch.Size([106522, 2])
We keep 2.50e+08/9.01e+09 =  2% of the original kernel matrix.

torch.Size([52124, 2])
We keep 2.45e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([51562, 2])
We keep 6.16e+07/1.87e+09 =  3% of the original kernel matrix.

torch.Size([35646, 2])
We keep 1.25e+07/8.24e+08 =  1% of the original kernel matrix.

torch.Size([13602, 2])
We keep 1.33e+07/1.11e+08 = 11% of the original kernel matrix.

torch.Size([18567, 2])
We keep 3.97e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([263331, 2])
We keep 4.27e+08/4.41e+10 =  0% of the original kernel matrix.

torch.Size([85474, 2])
We keep 4.90e+07/4.01e+09 =  1% of the original kernel matrix.

torch.Size([23322, 2])
We keep 6.04e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([25504, 2])
We keep 5.48e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([23923, 2])
We keep 6.74e+06/3.14e+08 =  2% of the original kernel matrix.

torch.Size([25526, 2])
We keep 6.05e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([72263, 2])
We keep 5.06e+07/3.01e+09 =  1% of the original kernel matrix.

torch.Size([43528, 2])
We keep 1.51e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([6865, 2])
We keep 9.65e+05/1.47e+07 =  6% of the original kernel matrix.

torch.Size([13694, 2])
We keep 1.88e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([97635, 2])
We keep 1.54e+08/6.02e+09 =  2% of the original kernel matrix.

torch.Size([50367, 2])
We keep 2.05e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([102901, 2])
We keep 4.74e+08/9.75e+09 =  4% of the original kernel matrix.

torch.Size([52520, 2])
We keep 2.57e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([273052, 2])
We keep 9.59e+08/4.45e+10 =  2% of the original kernel matrix.

torch.Size([86745, 2])
We keep 4.92e+07/4.02e+09 =  1% of the original kernel matrix.

torch.Size([18173, 2])
We keep 4.85e+06/1.58e+08 =  3% of the original kernel matrix.

torch.Size([22129, 2])
We keep 4.57e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([27021, 2])
We keep 9.81e+06/4.08e+08 =  2% of the original kernel matrix.

torch.Size([26916, 2])
We keep 6.66e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([25096, 2])
We keep 1.26e+07/4.24e+08 =  2% of the original kernel matrix.

torch.Size([25150, 2])
We keep 6.78e+06/3.93e+08 =  1% of the original kernel matrix.

torch.Size([10878, 2])
We keep 1.45e+06/3.98e+07 =  3% of the original kernel matrix.

torch.Size([16861, 2])
We keep 2.68e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([18097, 2])
We keep 7.13e+06/1.88e+08 =  3% of the original kernel matrix.

torch.Size([22044, 2])
We keep 4.67e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([13365, 2])
We keep 3.28e+06/8.30e+07 =  3% of the original kernel matrix.

torch.Size([19122, 2])
We keep 3.55e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([22820, 2])
We keep 6.61e+06/2.74e+08 =  2% of the original kernel matrix.

torch.Size([24993, 2])
We keep 5.72e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([95512, 2])
We keep 2.59e+08/7.90e+09 =  3% of the original kernel matrix.

torch.Size([51056, 2])
We keep 2.11e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([70755, 2])
We keep 1.81e+08/3.47e+09 =  5% of the original kernel matrix.

torch.Size([43133, 2])
We keep 1.62e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([7218, 2])
We keep 1.20e+06/2.16e+07 =  5% of the original kernel matrix.

torch.Size([13927, 2])
We keep 2.14e+06/8.87e+07 =  2% of the original kernel matrix.

torch.Size([16641, 2])
We keep 3.18e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([22664, 2])
We keep 3.91e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([24081, 2])
We keep 3.29e+07/9.29e+08 =  3% of the original kernel matrix.

torch.Size([24947, 2])
We keep 8.49e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([20989, 2])
We keep 7.62e+06/2.44e+08 =  3% of the original kernel matrix.

torch.Size([24279, 2])
We keep 5.32e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([8398, 2])
We keep 1.73e+06/2.70e+07 =  6% of the original kernel matrix.

torch.Size([14870, 2])
We keep 2.27e+06/9.90e+07 =  2% of the original kernel matrix.

torch.Size([11436, 2])
We keep 2.78e+06/5.43e+07 =  5% of the original kernel matrix.

torch.Size([17561, 2])
We keep 3.09e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([174112, 2])
We keep 1.39e+09/4.31e+10 =  3% of the original kernel matrix.

torch.Size([67073, 2])
We keep 4.94e+07/3.96e+09 =  1% of the original kernel matrix.

torch.Size([13890, 2])
We keep 3.03e+06/7.72e+07 =  3% of the original kernel matrix.

torch.Size([19155, 2])
We keep 3.43e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([255706, 2])
We keep 3.88e+09/8.30e+10 =  4% of the original kernel matrix.

torch.Size([78958, 2])
We keep 6.65e+07/5.49e+09 =  1% of the original kernel matrix.

torch.Size([8916, 2])
We keep 2.38e+06/3.20e+07 =  7% of the original kernel matrix.

torch.Size([15426, 2])
We keep 2.50e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([19793, 2])
We keep 2.85e+07/5.12e+08 =  5% of the original kernel matrix.

torch.Size([21647, 2])
We keep 7.21e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([26959, 2])
We keep 1.47e+07/4.33e+08 =  3% of the original kernel matrix.

torch.Size([27026, 2])
We keep 6.53e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([73824, 2])
We keep 1.44e+09/2.60e+10 =  5% of the original kernel matrix.

torch.Size([40468, 2])
We keep 3.98e+07/3.07e+09 =  1% of the original kernel matrix.

torch.Size([48607, 2])
We keep 2.40e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([35679, 2])
We keep 1.14e+07/7.43e+08 =  1% of the original kernel matrix.

torch.Size([15351, 2])
We keep 2.57e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([20402, 2])
We keep 3.76e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([119939, 2])
We keep 9.57e+07/8.13e+09 =  1% of the original kernel matrix.

torch.Size([56660, 2])
We keep 2.33e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([131907, 2])
We keep 1.26e+08/1.10e+10 =  1% of the original kernel matrix.

torch.Size([59811, 2])
We keep 2.64e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([14404, 2])
We keep 4.02e+06/8.40e+07 =  4% of the original kernel matrix.

torch.Size([19837, 2])
We keep 3.37e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([101272, 2])
We keep 1.41e+08/5.78e+09 =  2% of the original kernel matrix.

torch.Size([51347, 2])
We keep 2.02e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([107413, 2])
We keep 9.34e+07/6.77e+09 =  1% of the original kernel matrix.

torch.Size([53543, 2])
We keep 2.17e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([274740, 2])
We keep 4.62e+08/4.40e+10 =  1% of the original kernel matrix.

torch.Size([88049, 2])
We keep 4.93e+07/4.00e+09 =  1% of the original kernel matrix.

torch.Size([65227, 2])
We keep 6.99e+07/2.50e+09 =  2% of the original kernel matrix.

torch.Size([40944, 2])
We keep 1.42e+07/9.53e+08 =  1% of the original kernel matrix.

torch.Size([4535, 2])
We keep 6.48e+05/6.36e+06 = 10% of the original kernel matrix.

torch.Size([11304, 2])
We keep 1.39e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([22443, 2])
We keep 5.86e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([23164, 2])
We keep 5.12e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([73908, 2])
We keep 4.85e+08/7.46e+09 =  6% of the original kernel matrix.

torch.Size([43942, 2])
We keep 2.27e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([149229, 2])
We keep 7.64e+08/1.55e+10 =  4% of the original kernel matrix.

torch.Size([63052, 2])
We keep 3.12e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([20141, 2])
We keep 5.77e+06/1.85e+08 =  3% of the original kernel matrix.

torch.Size([23451, 2])
We keep 4.74e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([3784, 2])
We keep 2.78e+05/3.60e+06 =  7% of the original kernel matrix.

torch.Size([10642, 2])
We keep 1.10e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([108869, 2])
We keep 1.95e+08/6.44e+09 =  3% of the original kernel matrix.

torch.Size([53828, 2])
We keep 2.12e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([167023, 2])
We keep 2.30e+08/1.75e+10 =  1% of the original kernel matrix.

torch.Size([67340, 2])
We keep 3.26e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([43483, 2])
We keep 1.54e+08/2.73e+09 =  5% of the original kernel matrix.

torch.Size([30960, 2])
We keep 1.44e+07/9.97e+08 =  1% of the original kernel matrix.

torch.Size([12115, 2])
We keep 3.10e+06/5.57e+07 =  5% of the original kernel matrix.

torch.Size([17955, 2])
We keep 3.08e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([19797, 2])
We keep 1.09e+07/2.27e+08 =  4% of the original kernel matrix.

torch.Size([23151, 2])
We keep 5.28e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([13415, 2])
We keep 2.47e+06/7.68e+07 =  3% of the original kernel matrix.

torch.Size([18818, 2])
We keep 3.44e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([8202, 2])
We keep 9.72e+05/2.18e+07 =  4% of the original kernel matrix.

torch.Size([14693, 2])
We keep 2.15e+06/8.90e+07 =  2% of the original kernel matrix.

torch.Size([16033, 2])
We keep 2.93e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([20682, 2])
We keep 3.93e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([11208, 2])
We keep 2.62e+06/5.21e+07 =  5% of the original kernel matrix.

torch.Size([17355, 2])
We keep 2.86e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([61276, 2])
We keep 1.19e+08/2.90e+09 =  4% of the original kernel matrix.

torch.Size([38755, 2])
We keep 1.51e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([45442, 2])
We keep 1.83e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([34925, 2])
We keep 9.86e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([24777, 2])
We keep 6.60e+06/3.03e+08 =  2% of the original kernel matrix.

torch.Size([26303, 2])
We keep 5.70e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([10384, 2])
We keep 2.13e+06/4.35e+07 =  4% of the original kernel matrix.

torch.Size([16514, 2])
We keep 2.76e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([10031, 2])
We keep 2.03e+06/3.80e+07 =  5% of the original kernel matrix.

torch.Size([16261, 2])
We keep 2.64e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([781915, 2])
We keep 2.83e+09/3.10e+11 =  0% of the original kernel matrix.

torch.Size([151933, 2])
We keep 1.20e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([12730, 2])
We keep 2.83e+06/7.34e+07 =  3% of the original kernel matrix.

torch.Size([18297, 2])
We keep 3.36e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([152165, 2])
We keep 1.81e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([64046, 2])
We keep 2.90e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([4930, 2])
We keep 3.28e+05/5.89e+06 =  5% of the original kernel matrix.

torch.Size([11991, 2])
We keep 1.28e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([7599, 2])
We keep 1.05e+06/1.80e+07 =  5% of the original kernel matrix.

torch.Size([14146, 2])
We keep 2.00e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([5586, 2])
We keep 4.17e+05/8.23e+06 =  5% of the original kernel matrix.

torch.Size([12526, 2])
We keep 1.52e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([31687, 2])
We keep 1.56e+07/5.39e+08 =  2% of the original kernel matrix.

torch.Size([29657, 2])
We keep 6.90e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([6918, 2])
We keep 7.22e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([13772, 2])
We keep 1.84e+06/7.25e+07 =  2% of the original kernel matrix.

torch.Size([13322, 2])
We keep 2.14e+07/1.31e+08 = 16% of the original kernel matrix.

torch.Size([18586, 2])
We keep 4.11e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([9666, 2])
We keep 1.37e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([15844, 2])
We keep 2.54e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([7435, 2])
We keep 8.59e+05/1.64e+07 =  5% of the original kernel matrix.

torch.Size([14179, 2])
We keep 1.92e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([316104, 2])
We keep 8.01e+08/5.80e+10 =  1% of the original kernel matrix.

torch.Size([92931, 2])
We keep 5.60e+07/4.59e+09 =  1% of the original kernel matrix.

torch.Size([60722, 2])
We keep 8.71e+07/2.28e+09 =  3% of the original kernel matrix.

torch.Size([39848, 2])
We keep 1.37e+07/9.10e+08 =  1% of the original kernel matrix.

torch.Size([20259, 2])
We keep 1.02e+07/2.72e+08 =  3% of the original kernel matrix.

torch.Size([23795, 2])
We keep 5.83e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([97799, 2])
We keep 1.66e+08/5.48e+09 =  3% of the original kernel matrix.

torch.Size([50969, 2])
We keep 1.97e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([72532, 2])
We keep 4.52e+08/9.48e+09 =  4% of the original kernel matrix.

torch.Size([42722, 2])
We keep 2.49e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([7404, 2])
We keep 7.66e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([14069, 2])
We keep 1.92e+06/7.74e+07 =  2% of the original kernel matrix.

torch.Size([9224, 2])
We keep 1.00e+06/2.57e+07 =  3% of the original kernel matrix.

torch.Size([15565, 2])
We keep 2.26e+06/9.67e+07 =  2% of the original kernel matrix.

torch.Size([11607, 2])
We keep 2.52e+06/5.74e+07 =  4% of the original kernel matrix.

torch.Size([17724, 2])
We keep 2.88e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([2724, 2])
We keep 1.39e+05/1.68e+06 =  8% of the original kernel matrix.

torch.Size([9448, 2])
We keep 8.75e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([12519, 2])
We keep 2.21e+06/6.35e+07 =  3% of the original kernel matrix.

torch.Size([18583, 2])
We keep 3.28e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([17768, 2])
We keep 6.78e+06/1.59e+08 =  4% of the original kernel matrix.

torch.Size([21844, 2])
We keep 4.58e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([5047, 2])
We keep 5.07e+05/8.01e+06 =  6% of the original kernel matrix.

torch.Size([11977, 2])
We keep 1.49e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([10070, 2])
We keep 1.24e+06/3.09e+07 =  4% of the original kernel matrix.

torch.Size([16274, 2])
We keep 2.44e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([15163, 2])
We keep 3.44e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([20086, 2])
We keep 4.16e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([8292, 2])
We keep 1.01e+06/2.23e+07 =  4% of the original kernel matrix.

torch.Size([14836, 2])
We keep 2.15e+06/9.00e+07 =  2% of the original kernel matrix.

torch.Size([389519, 2])
We keep 1.60e+09/1.09e+11 =  1% of the original kernel matrix.

torch.Size([101755, 2])
We keep 7.45e+07/6.29e+09 =  1% of the original kernel matrix.

torch.Size([12821, 2])
We keep 4.73e+06/9.47e+07 =  4% of the original kernel matrix.

torch.Size([18189, 2])
We keep 3.72e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([39453, 2])
We keep 6.27e+07/1.18e+09 =  5% of the original kernel matrix.

torch.Size([32180, 2])
We keep 1.04e+07/6.55e+08 =  1% of the original kernel matrix.

torch.Size([9910, 2])
We keep 2.28e+06/4.33e+07 =  5% of the original kernel matrix.

torch.Size([16317, 2])
We keep 2.78e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([34232, 2])
We keep 6.12e+07/1.25e+09 =  4% of the original kernel matrix.

torch.Size([29150, 2])
We keep 1.08e+07/6.73e+08 =  1% of the original kernel matrix.

torch.Size([25361, 2])
We keep 6.57e+07/9.04e+08 =  7% of the original kernel matrix.

torch.Size([24781, 2])
We keep 9.30e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([855085, 2])
We keep 1.19e+10/5.78e+11 =  2% of the original kernel matrix.

torch.Size([155725, 2])
We keep 1.55e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([689006, 2])
We keep 1.82e+09/2.41e+11 =  0% of the original kernel matrix.

torch.Size([143420, 2])
We keep 1.05e+08/9.36e+09 =  1% of the original kernel matrix.

torch.Size([40268, 2])
We keep 2.08e+07/8.73e+08 =  2% of the original kernel matrix.

torch.Size([33056, 2])
We keep 8.84e+06/5.63e+08 =  1% of the original kernel matrix.

torch.Size([48070, 2])
We keep 8.15e+07/1.75e+09 =  4% of the original kernel matrix.

torch.Size([35726, 2])
We keep 1.25e+07/7.97e+08 =  1% of the original kernel matrix.

torch.Size([5347, 2])
We keep 4.10e+05/7.81e+06 =  5% of the original kernel matrix.

torch.Size([12289, 2])
We keep 1.46e+06/5.33e+07 =  2% of the original kernel matrix.

torch.Size([30465, 2])
We keep 3.99e+07/8.25e+08 =  4% of the original kernel matrix.

torch.Size([27639, 2])
We keep 9.00e+06/5.48e+08 =  1% of the original kernel matrix.

torch.Size([37282, 2])
We keep 1.42e+08/1.55e+09 =  9% of the original kernel matrix.

torch.Size([30478, 2])
We keep 1.13e+07/7.52e+08 =  1% of the original kernel matrix.

torch.Size([86782, 2])
We keep 1.22e+08/4.68e+09 =  2% of the original kernel matrix.

torch.Size([47759, 2])
We keep 1.85e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([10032, 2])
We keep 1.22e+06/3.09e+07 =  3% of the original kernel matrix.

torch.Size([16320, 2])
We keep 2.42e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([98789, 2])
We keep 1.01e+08/5.46e+09 =  1% of the original kernel matrix.

torch.Size([50676, 2])
We keep 1.99e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([187915, 2])
We keep 5.30e+08/2.58e+10 =  2% of the original kernel matrix.

torch.Size([70950, 2])
We keep 3.88e+07/3.06e+09 =  1% of the original kernel matrix.

torch.Size([46425, 2])
We keep 3.70e+07/1.49e+09 =  2% of the original kernel matrix.

torch.Size([33690, 2])
We keep 1.14e+07/7.36e+08 =  1% of the original kernel matrix.

torch.Size([15732, 2])
We keep 4.52e+06/1.32e+08 =  3% of the original kernel matrix.

torch.Size([20102, 2])
We keep 4.26e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([21939, 2])
We keep 3.11e+07/3.88e+08 =  8% of the original kernel matrix.

torch.Size([24972, 2])
We keep 6.76e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([10247, 2])
We keep 1.46e+06/3.86e+07 =  3% of the original kernel matrix.

torch.Size([16328, 2])
We keep 2.63e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([6065, 2])
We keep 4.95e+05/9.97e+06 =  4% of the original kernel matrix.

torch.Size([13055, 2])
We keep 1.56e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([129282, 2])
We keep 1.87e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([59858, 2])
We keep 2.90e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([41397, 2])
We keep 3.16e+07/1.13e+09 =  2% of the original kernel matrix.

torch.Size([32160, 2])
We keep 1.01e+07/6.40e+08 =  1% of the original kernel matrix.

torch.Size([6488, 2])
We keep 1.15e+06/1.47e+07 =  7% of the original kernel matrix.

torch.Size([13217, 2])
We keep 1.89e+06/7.31e+07 =  2% of the original kernel matrix.

torch.Size([130678, 2])
We keep 1.54e+08/9.64e+09 =  1% of the original kernel matrix.

torch.Size([59391, 2])
We keep 2.50e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([15908, 2])
We keep 3.12e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([21460, 2])
We keep 3.95e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([3006, 2])
We keep 2.32e+05/2.25e+06 = 10% of the original kernel matrix.

torch.Size([9756, 2])
We keep 9.48e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([71886, 2])
We keep 2.00e+08/3.55e+09 =  5% of the original kernel matrix.

torch.Size([42868, 2])
We keep 1.64e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([120643, 2])
We keep 1.10e+08/8.01e+09 =  1% of the original kernel matrix.

torch.Size([57034, 2])
We keep 2.33e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([56444, 2])
We keep 6.23e+07/1.89e+09 =  3% of the original kernel matrix.

torch.Size([38161, 2])
We keep 1.25e+07/8.29e+08 =  1% of the original kernel matrix.

torch.Size([128966, 2])
We keep 3.47e+08/1.26e+10 =  2% of the original kernel matrix.

torch.Size([58076, 2])
We keep 2.87e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([114582, 2])
We keep 2.81e+08/1.05e+10 =  2% of the original kernel matrix.

torch.Size([54334, 2])
We keep 2.62e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([8959, 2])
We keep 1.89e+06/3.05e+07 =  6% of the original kernel matrix.

torch.Size([15321, 2])
We keep 2.41e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([64250, 2])
We keep 7.01e+07/2.79e+09 =  2% of the original kernel matrix.

torch.Size([39747, 2])
We keep 1.47e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([142406, 2])
We keep 1.40e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([62191, 2])
We keep 2.81e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([165591, 2])
We keep 1.90e+08/1.61e+10 =  1% of the original kernel matrix.

torch.Size([67736, 2])
We keep 3.12e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([14335, 2])
We keep 2.62e+06/8.87e+07 =  2% of the original kernel matrix.

torch.Size([19595, 2])
We keep 3.60e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([119404, 2])
We keep 4.42e+08/1.65e+10 =  2% of the original kernel matrix.

torch.Size([57274, 2])
We keep 2.86e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([138589, 2])
We keep 2.49e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([61682, 2])
We keep 2.76e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([67133, 2])
We keep 4.98e+07/2.58e+09 =  1% of the original kernel matrix.

torch.Size([42040, 2])
We keep 1.40e+07/9.68e+08 =  1% of the original kernel matrix.

torch.Size([45971, 2])
We keep 9.22e+07/2.34e+09 =  3% of the original kernel matrix.

torch.Size([34597, 2])
We keep 1.31e+07/9.22e+08 =  1% of the original kernel matrix.

torch.Size([14816, 2])
We keep 2.86e+07/2.73e+08 = 10% of the original kernel matrix.

torch.Size([19145, 2])
We keep 5.85e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([21108, 2])
We keep 1.20e+07/2.56e+08 =  4% of the original kernel matrix.

torch.Size([22746, 2])
We keep 5.15e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([11349, 2])
We keep 1.63e+06/4.81e+07 =  3% of the original kernel matrix.

torch.Size([17349, 2])
We keep 2.87e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([7115, 2])
We keep 1.81e+06/1.79e+07 = 10% of the original kernel matrix.

torch.Size([13688, 2])
We keep 1.94e+06/8.06e+07 =  2% of the original kernel matrix.

torch.Size([189554, 2])
We keep 2.15e+08/1.96e+10 =  1% of the original kernel matrix.

torch.Size([72860, 2])
We keep 3.44e+07/2.67e+09 =  1% of the original kernel matrix.

torch.Size([128762, 2])
We keep 1.14e+08/9.23e+09 =  1% of the original kernel matrix.

torch.Size([59068, 2])
We keep 2.43e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([217204, 2])
We keep 5.96e+08/3.25e+10 =  1% of the original kernel matrix.

torch.Size([76999, 2])
We keep 4.30e+07/3.44e+09 =  1% of the original kernel matrix.

torch.Size([105736, 2])
We keep 6.32e+08/1.22e+10 =  5% of the original kernel matrix.

torch.Size([52561, 2])
We keep 2.82e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([66129, 2])
We keep 6.04e+07/2.74e+09 =  2% of the original kernel matrix.

torch.Size([40777, 2])
We keep 1.46e+07/9.97e+08 =  1% of the original kernel matrix.

torch.Size([14248, 2])
We keep 2.91e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([19319, 2])
We keep 3.87e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([93428, 2])
We keep 8.86e+07/4.49e+09 =  1% of the original kernel matrix.

torch.Size([49477, 2])
We keep 1.78e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([6147, 2])
We keep 7.06e+05/1.09e+07 =  6% of the original kernel matrix.

torch.Size([13640, 2])
We keep 1.70e+06/6.29e+07 =  2% of the original kernel matrix.

torch.Size([17418, 2])
We keep 5.42e+06/1.57e+08 =  3% of the original kernel matrix.

torch.Size([21428, 2])
We keep 4.58e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([171606, 2])
We keep 3.13e+08/2.06e+10 =  1% of the original kernel matrix.

torch.Size([68483, 2])
We keep 3.57e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([36352, 2])
We keep 2.62e+07/8.21e+08 =  3% of the original kernel matrix.

torch.Size([31262, 2])
We keep 8.99e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([6741, 2])
We keep 6.60e+05/1.21e+07 =  5% of the original kernel matrix.

torch.Size([13454, 2])
We keep 1.72e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([12870, 2])
We keep 2.02e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([18501, 2])
We keep 3.24e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([84654, 2])
We keep 2.05e+08/5.95e+09 =  3% of the original kernel matrix.

torch.Size([46304, 2])
We keep 2.08e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([39043, 2])
We keep 2.52e+07/9.17e+08 =  2% of the original kernel matrix.

torch.Size([31683, 2])
We keep 9.23e+06/5.77e+08 =  1% of the original kernel matrix.

torch.Size([70455, 2])
We keep 1.01e+08/3.67e+09 =  2% of the original kernel matrix.

torch.Size([42837, 2])
We keep 1.68e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([24121, 2])
We keep 1.51e+07/4.67e+08 =  3% of the original kernel matrix.

torch.Size([24596, 2])
We keep 7.11e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([24173, 2])
We keep 2.12e+07/5.56e+08 =  3% of the original kernel matrix.

torch.Size([24369, 2])
We keep 7.68e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([7141, 2])
We keep 9.82e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([13896, 2])
We keep 1.94e+06/7.78e+07 =  2% of the original kernel matrix.

torch.Size([3461, 2])
We keep 1.85e+05/2.86e+06 =  6% of the original kernel matrix.

torch.Size([10474, 2])
We keep 1.03e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([50479, 2])
We keep 4.08e+07/1.73e+09 =  2% of the original kernel matrix.

torch.Size([36173, 2])
We keep 1.23e+07/7.92e+08 =  1% of the original kernel matrix.

torch.Size([25015, 2])
We keep 1.44e+07/3.35e+08 =  4% of the original kernel matrix.

torch.Size([25953, 2])
We keep 6.11e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([6039, 2])
We keep 1.35e+06/1.39e+07 =  9% of the original kernel matrix.

torch.Size([12820, 2])
We keep 1.68e+06/7.10e+07 =  2% of the original kernel matrix.

torch.Size([217327, 2])
We keep 3.52e+08/2.97e+10 =  1% of the original kernel matrix.

torch.Size([78613, 2])
We keep 4.15e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([6751, 2])
We keep 7.60e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([13566, 2])
We keep 1.82e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([23435, 2])
We keep 7.10e+06/3.34e+08 =  2% of the original kernel matrix.

torch.Size([25961, 2])
We keep 6.33e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([9475, 2])
We keep 1.57e+06/3.22e+07 =  4% of the original kernel matrix.

torch.Size([16107, 2])
We keep 2.38e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([123694, 2])
We keep 1.05e+08/8.40e+09 =  1% of the original kernel matrix.

torch.Size([57568, 2])
We keep 2.38e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([9380, 2])
We keep 1.88e+06/3.32e+07 =  5% of the original kernel matrix.

torch.Size([15721, 2])
We keep 2.50e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([43392, 2])
We keep 4.07e+07/1.17e+09 =  3% of the original kernel matrix.

torch.Size([33493, 2])
We keep 1.03e+07/6.51e+08 =  1% of the original kernel matrix.

torch.Size([22325, 2])
We keep 7.68e+06/2.48e+08 =  3% of the original kernel matrix.

torch.Size([23967, 2])
We keep 5.27e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([19902, 2])
We keep 7.51e+06/2.21e+08 =  3% of the original kernel matrix.

torch.Size([22978, 2])
We keep 5.11e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([47101, 2])
We keep 3.74e+07/1.22e+09 =  3% of the original kernel matrix.

torch.Size([35348, 2])
We keep 1.04e+07/6.65e+08 =  1% of the original kernel matrix.

torch.Size([605379, 2])
We keep 2.32e+09/2.26e+11 =  1% of the original kernel matrix.

torch.Size([131866, 2])
We keep 1.04e+08/9.06e+09 =  1% of the original kernel matrix.

torch.Size([16495, 2])
We keep 3.64e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([21249, 2])
We keep 4.27e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([9590, 2])
We keep 2.42e+06/4.26e+07 =  5% of the original kernel matrix.

torch.Size([15753, 2])
We keep 2.73e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([111803, 2])
We keep 1.95e+08/7.94e+09 =  2% of the original kernel matrix.

torch.Size([54383, 2])
We keep 2.31e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([17776, 2])
We keep 4.82e+06/1.52e+08 =  3% of the original kernel matrix.

torch.Size([21817, 2])
We keep 4.51e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([388424, 2])
We keep 1.36e+09/8.42e+10 =  1% of the original kernel matrix.

torch.Size([104057, 2])
We keep 6.56e+07/5.53e+09 =  1% of the original kernel matrix.

torch.Size([8668, 2])
We keep 1.97e+06/2.74e+07 =  7% of the original kernel matrix.

torch.Size([15109, 2])
We keep 2.33e+06/9.98e+07 =  2% of the original kernel matrix.

torch.Size([111705, 2])
We keep 1.21e+08/8.16e+09 =  1% of the original kernel matrix.

torch.Size([53781, 2])
We keep 2.35e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([383917, 2])
We keep 6.80e+08/7.43e+10 =  0% of the original kernel matrix.

torch.Size([102590, 2])
We keep 6.19e+07/5.20e+09 =  1% of the original kernel matrix.

torch.Size([8143, 2])
We keep 8.93e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([14971, 2])
We keep 2.05e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([348208, 2])
We keep 1.11e+09/9.02e+10 =  1% of the original kernel matrix.

torch.Size([95392, 2])
We keep 6.87e+07/5.73e+09 =  1% of the original kernel matrix.

torch.Size([10325, 2])
We keep 1.07e+07/1.12e+08 =  9% of the original kernel matrix.

torch.Size([16510, 2])
We keep 4.04e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([372819, 2])
We keep 3.71e+09/1.16e+11 =  3% of the original kernel matrix.

torch.Size([100345, 2])
We keep 7.65e+07/6.48e+09 =  1% of the original kernel matrix.

torch.Size([13999, 2])
We keep 2.77e+06/8.57e+07 =  3% of the original kernel matrix.

torch.Size([19320, 2])
We keep 3.57e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([148611, 2])
We keep 4.68e+08/1.55e+10 =  3% of the original kernel matrix.

torch.Size([62815, 2])
We keep 3.14e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([4048, 2])
We keep 4.30e+05/6.38e+06 =  6% of the original kernel matrix.

torch.Size([10864, 2])
We keep 1.35e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([7455, 2])
We keep 7.91e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([14204, 2])
We keep 1.96e+06/7.87e+07 =  2% of the original kernel matrix.

torch.Size([8645, 2])
We keep 2.05e+06/3.26e+07 =  6% of the original kernel matrix.

torch.Size([15144, 2])
We keep 2.53e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([48539, 2])
We keep 3.18e+07/1.29e+09 =  2% of the original kernel matrix.

torch.Size([35831, 2])
We keep 1.03e+07/6.85e+08 =  1% of the original kernel matrix.

torch.Size([43382, 2])
We keep 4.30e+07/1.27e+09 =  3% of the original kernel matrix.

torch.Size([33526, 2])
We keep 1.09e+07/6.79e+08 =  1% of the original kernel matrix.

torch.Size([177678, 2])
We keep 7.03e+08/2.13e+10 =  3% of the original kernel matrix.

torch.Size([70090, 2])
We keep 3.55e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([17385, 2])
We keep 3.89e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([21716, 2])
We keep 4.31e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([8979, 2])
We keep 1.23e+06/2.70e+07 =  4% of the original kernel matrix.

torch.Size([15247, 2])
We keep 2.33e+06/9.91e+07 =  2% of the original kernel matrix.

torch.Size([12012, 2])
We keep 6.45e+06/7.32e+07 =  8% of the original kernel matrix.

torch.Size([17972, 2])
We keep 3.46e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([98956, 2])
We keep 3.09e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([50126, 2])
We keep 2.72e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([39708, 2])
We keep 9.70e+07/1.63e+09 =  5% of the original kernel matrix.

torch.Size([31648, 2])
We keep 1.18e+07/7.70e+08 =  1% of the original kernel matrix.

torch.Size([242200, 2])
We keep 4.18e+08/3.68e+10 =  1% of the original kernel matrix.

torch.Size([83534, 2])
We keep 4.55e+07/3.66e+09 =  1% of the original kernel matrix.

torch.Size([45422, 2])
We keep 3.59e+07/1.27e+09 =  2% of the original kernel matrix.

torch.Size([34650, 2])
We keep 1.06e+07/6.79e+08 =  1% of the original kernel matrix.

torch.Size([11067, 2])
We keep 4.54e+06/7.15e+07 =  6% of the original kernel matrix.

torch.Size([17207, 2])
We keep 3.40e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([4163, 2])
We keep 4.07e+05/4.88e+06 =  8% of the original kernel matrix.

torch.Size([10968, 2])
We keep 1.24e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([9708, 2])
We keep 6.92e+06/5.82e+07 = 11% of the original kernel matrix.

torch.Size([15746, 2])
We keep 3.06e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([1384436, 2])
We keep 8.58e+09/8.37e+11 =  1% of the original kernel matrix.

torch.Size([205932, 2])
We keep 1.84e+08/1.74e+10 =  1% of the original kernel matrix.

torch.Size([10693, 2])
We keep 2.04e+06/3.73e+07 =  5% of the original kernel matrix.

torch.Size([16868, 2])
We keep 2.62e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([17397, 2])
We keep 4.79e+06/1.40e+08 =  3% of the original kernel matrix.

torch.Size([21834, 2])
We keep 4.31e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([130941, 2])
We keep 2.15e+08/1.06e+10 =  2% of the original kernel matrix.

torch.Size([58947, 2])
We keep 2.66e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([45016, 2])
We keep 6.38e+07/1.90e+09 =  3% of the original kernel matrix.

torch.Size([32214, 2])
We keep 1.26e+07/8.32e+08 =  1% of the original kernel matrix.

torch.Size([9597, 2])
We keep 1.76e+06/3.54e+07 =  4% of the original kernel matrix.

torch.Size([15987, 2])
We keep 2.52e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([5225, 2])
We keep 4.35e+05/7.70e+06 =  5% of the original kernel matrix.

torch.Size([12106, 2])
We keep 1.47e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([35457, 2])
We keep 4.83e+07/8.83e+08 =  5% of the original kernel matrix.

torch.Size([30523, 2])
We keep 9.00e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([45311, 2])
We keep 3.71e+07/1.25e+09 =  2% of the original kernel matrix.

torch.Size([34930, 2])
We keep 1.05e+07/6.75e+08 =  1% of the original kernel matrix.

torch.Size([11962, 2])
We keep 3.25e+06/7.35e+07 =  4% of the original kernel matrix.

torch.Size([17604, 2])
We keep 3.44e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([55738, 2])
We keep 4.71e+07/1.81e+09 =  2% of the original kernel matrix.

torch.Size([38114, 2])
We keep 1.23e+07/8.12e+08 =  1% of the original kernel matrix.

torch.Size([97383, 2])
We keep 5.41e+08/9.62e+09 =  5% of the original kernel matrix.

torch.Size([50581, 2])
We keep 2.49e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([62740, 2])
We keep 1.43e+08/2.99e+09 =  4% of the original kernel matrix.

torch.Size([39991, 2])
We keep 1.54e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([223606, 2])
We keep 8.50e+08/4.09e+10 =  2% of the original kernel matrix.

torch.Size([76837, 2])
We keep 4.75e+07/3.86e+09 =  1% of the original kernel matrix.

torch.Size([102312, 2])
We keep 9.45e+07/6.24e+09 =  1% of the original kernel matrix.

torch.Size([52103, 2])
We keep 2.07e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([14654, 2])
We keep 8.68e+06/1.26e+08 =  6% of the original kernel matrix.

torch.Size([19867, 2])
We keep 4.29e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([93475, 2])
We keep 1.09e+08/5.10e+09 =  2% of the original kernel matrix.

torch.Size([49905, 2])
We keep 1.90e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([19775, 2])
We keep 3.76e+07/3.02e+08 = 12% of the original kernel matrix.

torch.Size([22774, 2])
We keep 5.92e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([12161, 2])
We keep 2.48e+06/5.47e+07 =  4% of the original kernel matrix.

torch.Size([17966, 2])
We keep 3.00e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([20978, 2])
We keep 6.32e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([24055, 2])
We keep 5.56e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([14208, 2])
We keep 2.98e+06/8.79e+07 =  3% of the original kernel matrix.

torch.Size([19503, 2])
We keep 3.63e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([65081, 2])
We keep 8.40e+07/3.06e+09 =  2% of the original kernel matrix.

torch.Size([41019, 2])
We keep 1.54e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([24223, 2])
We keep 1.20e+07/3.90e+08 =  3% of the original kernel matrix.

torch.Size([24910, 2])
We keep 6.55e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([8182, 2])
We keep 1.31e+06/2.27e+07 =  5% of the original kernel matrix.

torch.Size([14527, 2])
We keep 2.15e+06/9.08e+07 =  2% of the original kernel matrix.

torch.Size([24639, 2])
We keep 1.04e+07/3.68e+08 =  2% of the original kernel matrix.

torch.Size([25582, 2])
We keep 6.44e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([12384, 2])
We keep 9.33e+06/1.77e+08 =  5% of the original kernel matrix.

torch.Size([17745, 2])
We keep 4.23e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([23261, 2])
We keep 1.08e+07/2.98e+08 =  3% of the original kernel matrix.

torch.Size([24808, 2])
We keep 5.88e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([48240, 2])
We keep 1.38e+08/2.18e+09 =  6% of the original kernel matrix.

torch.Size([33898, 2])
We keep 1.38e+07/8.90e+08 =  1% of the original kernel matrix.

torch.Size([13579, 2])
We keep 5.30e+06/1.19e+08 =  4% of the original kernel matrix.

torch.Size([18743, 2])
We keep 4.04e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([11258, 2])
We keep 6.85e+06/7.12e+07 =  9% of the original kernel matrix.

torch.Size([17161, 2])
We keep 3.25e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([20992, 2])
We keep 1.98e+07/3.53e+08 =  5% of the original kernel matrix.

torch.Size([23674, 2])
We keep 6.49e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([6118, 2])
We keep 6.28e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([12940, 2])
We keep 1.65e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([11535, 2])
We keep 1.86e+06/5.06e+07 =  3% of the original kernel matrix.

torch.Size([17228, 2])
We keep 2.93e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([27825, 2])
We keep 8.39e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([27841, 2])
We keep 6.76e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([167268, 2])
We keep 2.69e+08/1.62e+10 =  1% of the original kernel matrix.

torch.Size([67774, 2])
We keep 3.14e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([10154, 2])
We keep 1.48e+06/3.27e+07 =  4% of the original kernel matrix.

torch.Size([16535, 2])
We keep 2.51e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([2969, 2])
We keep 1.52e+05/2.02e+06 =  7% of the original kernel matrix.

torch.Size([9775, 2])
We keep 9.28e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([5068, 2])
We keep 5.05e+05/7.76e+06 =  6% of the original kernel matrix.

torch.Size([11749, 2])
We keep 1.46e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([116094, 2])
We keep 2.20e+08/9.79e+09 =  2% of the original kernel matrix.

torch.Size([55370, 2])
We keep 2.58e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([338879, 2])
We keep 1.45e+09/8.23e+10 =  1% of the original kernel matrix.

torch.Size([97203, 2])
We keep 6.63e+07/5.47e+09 =  1% of the original kernel matrix.

torch.Size([50298, 2])
We keep 5.68e+07/1.84e+09 =  3% of the original kernel matrix.

torch.Size([34997, 2])
We keep 1.24e+07/8.18e+08 =  1% of the original kernel matrix.

torch.Size([9568, 2])
We keep 1.53e+06/3.07e+07 =  4% of the original kernel matrix.

torch.Size([16575, 2])
We keep 2.53e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([75602, 2])
We keep 2.11e+08/4.65e+09 =  4% of the original kernel matrix.

torch.Size([44302, 2])
We keep 1.84e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([222544, 2])
We keep 2.90e+08/2.93e+10 =  0% of the original kernel matrix.

torch.Size([80081, 2])
We keep 4.12e+07/3.26e+09 =  1% of the original kernel matrix.

torch.Size([4555, 2])
We keep 3.03e+05/5.39e+06 =  5% of the original kernel matrix.

torch.Size([11608, 2])
We keep 1.32e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([12942, 2])
We keep 2.67e+06/8.34e+07 =  3% of the original kernel matrix.

torch.Size([18557, 2])
We keep 3.56e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([41087, 2])
We keep 1.47e+07/9.31e+08 =  1% of the original kernel matrix.

torch.Size([33383, 2])
We keep 9.20e+06/5.82e+08 =  1% of the original kernel matrix.

torch.Size([12837, 2])
We keep 8.65e+06/1.20e+08 =  7% of the original kernel matrix.

torch.Size([18518, 2])
We keep 4.14e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([70162, 2])
We keep 1.15e+08/3.66e+09 =  3% of the original kernel matrix.

torch.Size([42579, 2])
We keep 1.68e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([85743, 2])
We keep 6.85e+07/3.94e+09 =  1% of the original kernel matrix.

torch.Size([47283, 2])
We keep 1.70e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([29545, 2])
We keep 9.64e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([28717, 2])
We keep 7.09e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([11488, 2])
We keep 3.90e+06/4.99e+07 =  7% of the original kernel matrix.

torch.Size([17385, 2])
We keep 2.95e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([8814, 2])
We keep 1.16e+06/2.34e+07 =  4% of the original kernel matrix.

torch.Size([15272, 2])
We keep 2.23e+06/9.22e+07 =  2% of the original kernel matrix.

torch.Size([34049, 2])
We keep 2.38e+07/7.63e+08 =  3% of the original kernel matrix.

torch.Size([29657, 2])
We keep 8.62e+06/5.27e+08 =  1% of the original kernel matrix.

torch.Size([49262, 2])
We keep 4.46e+07/1.48e+09 =  3% of the original kernel matrix.

torch.Size([35868, 2])
We keep 1.15e+07/7.35e+08 =  1% of the original kernel matrix.

torch.Size([52202, 2])
We keep 5.32e+07/2.11e+09 =  2% of the original kernel matrix.

torch.Size([35875, 2])
We keep 1.33e+07/8.75e+08 =  1% of the original kernel matrix.

torch.Size([49174, 2])
We keep 3.36e+07/1.34e+09 =  2% of the original kernel matrix.

torch.Size([35491, 2])
We keep 1.08e+07/6.97e+08 =  1% of the original kernel matrix.

torch.Size([321442, 2])
We keep 6.70e+08/6.26e+10 =  1% of the original kernel matrix.

torch.Size([94226, 2])
We keep 5.80e+07/4.77e+09 =  1% of the original kernel matrix.

torch.Size([11339, 2])
We keep 2.34e+06/5.01e+07 =  4% of the original kernel matrix.

torch.Size([17289, 2])
We keep 2.91e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([5635, 2])
We keep 6.11e+05/9.37e+06 =  6% of the original kernel matrix.

torch.Size([12351, 2])
We keep 1.53e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([17659, 2])
We keep 4.26e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([21971, 2])
We keep 4.42e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([64169, 2])
We keep 5.14e+07/2.29e+09 =  2% of the original kernel matrix.

torch.Size([40994, 2])
We keep 1.36e+07/9.13e+08 =  1% of the original kernel matrix.

torch.Size([42128, 2])
We keep 1.92e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([33297, 2])
We keep 9.76e+06/6.05e+08 =  1% of the original kernel matrix.

torch.Size([10883, 2])
We keep 2.69e+06/5.06e+07 =  5% of the original kernel matrix.

torch.Size([16904, 2])
We keep 2.87e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([4809, 2])
We keep 4.05e+05/6.13e+06 =  6% of the original kernel matrix.

torch.Size([11666, 2])
We keep 1.37e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([50921, 2])
We keep 2.67e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([36504, 2])
We keep 1.12e+07/7.17e+08 =  1% of the original kernel matrix.

torch.Size([7770, 2])
We keep 7.39e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([14597, 2])
We keep 1.90e+06/7.63e+07 =  2% of the original kernel matrix.

torch.Size([20385, 2])
We keep 2.03e+07/3.12e+08 =  6% of the original kernel matrix.

torch.Size([23203, 2])
We keep 5.99e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([6914, 2])
We keep 7.12e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([13763, 2])
We keep 1.91e+06/7.46e+07 =  2% of the original kernel matrix.

torch.Size([18898, 2])
We keep 4.00e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([22844, 2])
We keep 4.60e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([13672, 2])
We keep 3.56e+06/7.93e+07 =  4% of the original kernel matrix.

torch.Size([19148, 2])
We keep 3.41e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([10696, 2])
We keep 1.32e+06/4.05e+07 =  3% of the original kernel matrix.

torch.Size([16787, 2])
We keep 2.67e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([9732, 2])
We keep 1.20e+06/2.99e+07 =  4% of the original kernel matrix.

torch.Size([15943, 2])
We keep 2.42e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([131934, 2])
We keep 1.19e+08/9.40e+09 =  1% of the original kernel matrix.

torch.Size([59700, 2])
We keep 2.49e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([212114, 2])
We keep 4.83e+08/2.90e+10 =  1% of the original kernel matrix.

torch.Size([76990, 2])
We keep 4.11e+07/3.24e+09 =  1% of the original kernel matrix.

torch.Size([168869, 2])
We keep 3.51e+08/1.87e+10 =  1% of the original kernel matrix.

torch.Size([68401, 2])
We keep 3.37e+07/2.61e+09 =  1% of the original kernel matrix.

torch.Size([139196, 2])
We keep 5.13e+08/1.25e+10 =  4% of the original kernel matrix.

torch.Size([60798, 2])
We keep 2.86e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([35727, 2])
We keep 1.34e+07/6.61e+08 =  2% of the original kernel matrix.

torch.Size([31060, 2])
We keep 8.11e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([11528, 2])
We keep 2.88e+06/5.49e+07 =  5% of the original kernel matrix.

torch.Size([17444, 2])
We keep 3.07e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([16891, 2])
We keep 1.05e+07/1.86e+08 =  5% of the original kernel matrix.

torch.Size([21723, 2])
We keep 5.01e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([147839, 2])
We keep 1.62e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([62935, 2])
We keep 2.89e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([15051, 2])
We keep 5.67e+06/1.16e+08 =  4% of the original kernel matrix.

torch.Size([19908, 2])
We keep 3.99e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([31848, 2])
We keep 1.64e+07/5.64e+08 =  2% of the original kernel matrix.

torch.Size([29328, 2])
We keep 7.47e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([10619, 2])
We keep 1.68e+06/4.19e+07 =  4% of the original kernel matrix.

torch.Size([16605, 2])
We keep 2.76e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([213724, 2])
We keep 1.55e+09/5.19e+10 =  2% of the original kernel matrix.

torch.Size([75868, 2])
We keep 5.32e+07/4.34e+09 =  1% of the original kernel matrix.

torch.Size([59204, 2])
We keep 6.46e+07/2.03e+09 =  3% of the original kernel matrix.

torch.Size([39031, 2])
We keep 1.30e+07/8.59e+08 =  1% of the original kernel matrix.

torch.Size([25185, 2])
We keep 1.31e+07/3.91e+08 =  3% of the original kernel matrix.

torch.Size([26423, 2])
We keep 6.68e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([185273, 2])
We keep 9.32e+08/2.40e+10 =  3% of the original kernel matrix.

torch.Size([71647, 2])
We keep 3.84e+07/2.95e+09 =  1% of the original kernel matrix.

torch.Size([46590, 2])
We keep 2.03e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([35202, 2])
We keep 1.01e+07/6.44e+08 =  1% of the original kernel matrix.

torch.Size([36454, 2])
We keep 3.11e+07/7.88e+08 =  3% of the original kernel matrix.

torch.Size([31279, 2])
We keep 8.51e+06/5.35e+08 =  1% of the original kernel matrix.

torch.Size([219307, 2])
We keep 7.77e+08/3.72e+10 =  2% of the original kernel matrix.

torch.Size([78021, 2])
We keep 4.61e+07/3.68e+09 =  1% of the original kernel matrix.

torch.Size([139545, 2])
We keep 1.30e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([61319, 2])
We keep 2.67e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([115021, 2])
We keep 2.20e+08/7.59e+09 =  2% of the original kernel matrix.

torch.Size([55148, 2])
We keep 2.29e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([20448, 2])
We keep 7.27e+06/2.40e+08 =  3% of the original kernel matrix.

torch.Size([23577, 2])
We keep 5.43e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([93851, 2])
We keep 1.61e+08/5.83e+09 =  2% of the original kernel matrix.

torch.Size([49599, 2])
We keep 2.05e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([12243, 2])
We keep 1.75e+06/5.56e+07 =  3% of the original kernel matrix.

torch.Size([18262, 2])
We keep 2.96e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([83200, 2])
We keep 5.89e+07/3.67e+09 =  1% of the original kernel matrix.

torch.Size([46011, 2])
We keep 1.64e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([582756, 2])
We keep 2.16e+09/1.87e+11 =  1% of the original kernel matrix.

torch.Size([131083, 2])
We keep 9.52e+07/8.25e+09 =  1% of the original kernel matrix.

torch.Size([44882, 2])
We keep 6.55e+07/1.28e+09 =  5% of the original kernel matrix.

torch.Size([34299, 2])
We keep 1.06e+07/6.82e+08 =  1% of the original kernel matrix.

torch.Size([33916, 2])
We keep 9.62e+07/1.51e+09 =  6% of the original kernel matrix.

torch.Size([29566, 2])
We keep 1.16e+07/7.41e+08 =  1% of the original kernel matrix.

torch.Size([16815, 2])
We keep 3.88e+06/1.26e+08 =  3% of the original kernel matrix.

torch.Size([21726, 2])
We keep 4.16e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([55872, 2])
We keep 1.50e+08/2.65e+09 =  5% of the original kernel matrix.

torch.Size([37610, 2])
We keep 1.46e+07/9.82e+08 =  1% of the original kernel matrix.

torch.Size([22933, 2])
We keep 3.65e+07/4.02e+08 =  9% of the original kernel matrix.

torch.Size([24614, 2])
We keep 6.61e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([11313, 2])
We keep 2.66e+06/6.31e+07 =  4% of the original kernel matrix.

torch.Size([17130, 2])
We keep 3.19e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([35672, 2])
We keep 2.89e+07/7.28e+08 =  3% of the original kernel matrix.

torch.Size([30720, 2])
We keep 8.37e+06/5.15e+08 =  1% of the original kernel matrix.

torch.Size([303437, 2])
We keep 6.11e+08/6.50e+10 =  0% of the original kernel matrix.

torch.Size([92569, 2])
We keep 5.92e+07/4.86e+09 =  1% of the original kernel matrix.

torch.Size([13858, 2])
We keep 1.68e+07/1.97e+08 =  8% of the original kernel matrix.

torch.Size([19116, 2])
We keep 5.06e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([208830, 2])
We keep 2.12e+09/5.78e+10 =  3% of the original kernel matrix.

torch.Size([73995, 2])
We keep 5.63e+07/4.58e+09 =  1% of the original kernel matrix.

torch.Size([37548, 2])
We keep 2.69e+07/7.86e+08 =  3% of the original kernel matrix.

torch.Size([32014, 2])
We keep 8.63e+06/5.34e+08 =  1% of the original kernel matrix.

torch.Size([40968, 2])
We keep 1.98e+07/9.34e+08 =  2% of the original kernel matrix.

torch.Size([32685, 2])
We keep 9.23e+06/5.83e+08 =  1% of the original kernel matrix.

torch.Size([18632, 2])
We keep 1.89e+07/2.70e+08 =  6% of the original kernel matrix.

torch.Size([22123, 2])
We keep 5.82e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([65516, 2])
We keep 4.86e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([41502, 2])
We keep 1.44e+07/9.76e+08 =  1% of the original kernel matrix.

torch.Size([200680, 2])
We keep 3.51e+08/3.10e+10 =  1% of the original kernel matrix.

torch.Size([75511, 2])
We keep 4.31e+07/3.36e+09 =  1% of the original kernel matrix.

torch.Size([4779, 2])
We keep 3.59e+05/6.51e+06 =  5% of the original kernel matrix.

torch.Size([11738, 2])
We keep 1.39e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([668228, 2])
We keep 2.10e+09/2.63e+11 =  0% of the original kernel matrix.

torch.Size([143242, 2])
We keep 1.11e+08/9.79e+09 =  1% of the original kernel matrix.

torch.Size([42866, 2])
We keep 2.88e+07/1.14e+09 =  2% of the original kernel matrix.

torch.Size([33866, 2])
We keep 9.63e+06/6.44e+08 =  1% of the original kernel matrix.

torch.Size([36367, 2])
We keep 3.13e+07/8.68e+08 =  3% of the original kernel matrix.

torch.Size([30649, 2])
We keep 9.12e+06/5.62e+08 =  1% of the original kernel matrix.

torch.Size([7069, 2])
We keep 7.40e+05/1.76e+07 =  4% of the original kernel matrix.

torch.Size([14025, 2])
We keep 1.88e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([25207, 2])
We keep 9.19e+06/3.67e+08 =  2% of the original kernel matrix.

torch.Size([26196, 2])
We keep 6.35e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([5358, 2])
We keep 1.03e+06/9.51e+06 = 10% of the original kernel matrix.

torch.Size([12095, 2])
We keep 1.57e+06/5.88e+07 =  2% of the original kernel matrix.

torch.Size([3429, 2])
We keep 4.70e+05/4.11e+06 = 11% of the original kernel matrix.

torch.Size([10162, 2])
We keep 1.15e+06/3.87e+07 =  2% of the original kernel matrix.

torch.Size([16246, 2])
We keep 3.62e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([20903, 2])
We keep 3.92e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([99706, 2])
We keep 1.89e+08/7.28e+09 =  2% of the original kernel matrix.

torch.Size([50835, 2])
We keep 2.16e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([927122, 2])
We keep 2.59e+10/1.50e+12 =  1% of the original kernel matrix.

torch.Size([138549, 2])
We keep 2.50e+08/2.34e+10 =  1% of the original kernel matrix.

torch.Size([42016, 2])
We keep 4.11e+08/2.41e+09 = 17% of the original kernel matrix.

torch.Size([32583, 2])
We keep 1.45e+07/9.36e+08 =  1% of the original kernel matrix.

torch.Size([130317, 2])
We keep 1.69e+08/9.62e+09 =  1% of the original kernel matrix.

torch.Size([59087, 2])
We keep 2.49e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([14618, 2])
We keep 3.56e+06/9.02e+07 =  3% of the original kernel matrix.

torch.Size([19918, 2])
We keep 3.68e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([37854, 2])
We keep 1.89e+07/9.33e+08 =  2% of the original kernel matrix.

torch.Size([32018, 2])
We keep 9.53e+06/5.83e+08 =  1% of the original kernel matrix.

torch.Size([15299, 2])
We keep 2.09e+07/2.19e+08 =  9% of the original kernel matrix.

torch.Size([19604, 2])
We keep 5.39e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([94919, 2])
We keep 1.38e+08/6.38e+09 =  2% of the original kernel matrix.

torch.Size([49549, 2])
We keep 2.13e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([12377, 2])
We keep 2.69e+06/6.68e+07 =  4% of the original kernel matrix.

torch.Size([18158, 2])
We keep 3.29e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([933988, 2])
We keep 4.29e+09/5.00e+11 =  0% of the original kernel matrix.

torch.Size([165952, 2])
We keep 1.50e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([8063, 2])
We keep 9.57e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([14676, 2])
We keep 2.04e+06/8.37e+07 =  2% of the original kernel matrix.

torch.Size([129855, 2])
We keep 2.76e+08/9.43e+09 =  2% of the original kernel matrix.

torch.Size([59325, 2])
We keep 2.49e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([18470, 2])
We keep 5.05e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([23373, 2])
We keep 4.87e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([91558, 2])
We keep 1.36e+08/5.23e+09 =  2% of the original kernel matrix.

torch.Size([48832, 2])
We keep 1.93e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([338531, 2])
We keep 7.63e+08/6.88e+10 =  1% of the original kernel matrix.

torch.Size([96762, 2])
We keep 6.07e+07/5.00e+09 =  1% of the original kernel matrix.

torch.Size([32389, 2])
We keep 1.17e+07/5.68e+08 =  2% of the original kernel matrix.

torch.Size([29633, 2])
We keep 7.73e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([11306, 2])
We keep 1.96e+06/4.71e+07 =  4% of the original kernel matrix.

torch.Size([17272, 2])
We keep 2.87e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([73224, 2])
We keep 4.61e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([43408, 2])
We keep 1.47e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([13852, 2])
We keep 2.61e+06/7.54e+07 =  3% of the original kernel matrix.

torch.Size([19476, 2])
We keep 3.44e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([11560, 2])
We keep 4.66e+06/7.31e+07 =  6% of the original kernel matrix.

torch.Size([17287, 2])
We keep 3.43e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([189897, 2])
We keep 2.78e+08/2.09e+10 =  1% of the original kernel matrix.

torch.Size([72300, 2])
We keep 3.55e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([218954, 2])
We keep 5.02e+09/9.14e+10 =  5% of the original kernel matrix.

torch.Size([75976, 2])
We keep 6.87e+07/5.76e+09 =  1% of the original kernel matrix.

torch.Size([45454, 2])
We keep 1.98e+07/9.75e+08 =  2% of the original kernel matrix.

torch.Size([38731, 2])
We keep 9.65e+06/5.95e+08 =  1% of the original kernel matrix.

torch.Size([44653, 2])
We keep 2.72e+07/1.28e+09 =  2% of the original kernel matrix.

torch.Size([33784, 2])
We keep 1.08e+07/6.83e+08 =  1% of the original kernel matrix.

torch.Size([164650, 2])
We keep 6.10e+08/2.57e+10 =  2% of the original kernel matrix.

torch.Size([66234, 2])
We keep 3.93e+07/3.06e+09 =  1% of the original kernel matrix.

torch.Size([115633, 2])
We keep 8.37e+08/1.65e+10 =  5% of the original kernel matrix.

torch.Size([54314, 2])
We keep 3.20e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([59022, 2])
We keep 3.35e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([39015, 2])
We keep 1.25e+07/8.12e+08 =  1% of the original kernel matrix.

torch.Size([10192, 2])
We keep 1.30e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([16583, 2])
We keep 2.55e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([114595, 2])
We keep 3.38e+08/1.04e+10 =  3% of the original kernel matrix.

torch.Size([54638, 2])
We keep 2.60e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([70547, 2])
We keep 1.00e+08/3.51e+09 =  2% of the original kernel matrix.

torch.Size([41733, 2])
We keep 1.64e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([68777, 2])
We keep 7.98e+07/3.15e+09 =  2% of the original kernel matrix.

torch.Size([41468, 2])
We keep 1.59e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([41706, 2])
We keep 4.53e+07/1.06e+09 =  4% of the original kernel matrix.

torch.Size([32763, 2])
We keep 9.91e+06/6.20e+08 =  1% of the original kernel matrix.

torch.Size([30909, 2])
We keep 6.19e+07/9.83e+08 =  6% of the original kernel matrix.

torch.Size([27828, 2])
We keep 9.71e+06/5.98e+08 =  1% of the original kernel matrix.

torch.Size([46070, 2])
We keep 5.46e+07/1.57e+09 =  3% of the original kernel matrix.

torch.Size([33866, 2])
We keep 1.18e+07/7.55e+08 =  1% of the original kernel matrix.

torch.Size([267041, 2])
We keep 2.12e+09/8.34e+10 =  2% of the original kernel matrix.

torch.Size([79085, 2])
We keep 6.58e+07/5.51e+09 =  1% of the original kernel matrix.

torch.Size([58497, 2])
We keep 7.16e+07/2.24e+09 =  3% of the original kernel matrix.

torch.Size([38796, 2])
We keep 1.35e+07/9.03e+08 =  1% of the original kernel matrix.

torch.Size([22441, 2])
We keep 6.46e+06/2.57e+08 =  2% of the original kernel matrix.

torch.Size([23283, 2])
We keep 5.15e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([4730, 2])
We keep 5.21e+05/7.47e+06 =  6% of the original kernel matrix.

torch.Size([11496, 2])
We keep 1.49e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([22072, 2])
We keep 1.28e+07/3.24e+08 =  3% of the original kernel matrix.

torch.Size([23937, 2])
We keep 6.18e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([58226, 2])
We keep 7.88e+07/2.42e+09 =  3% of the original kernel matrix.

torch.Size([37951, 2])
We keep 1.41e+07/9.37e+08 =  1% of the original kernel matrix.

torch.Size([114766, 2])
We keep 1.94e+08/8.96e+09 =  2% of the original kernel matrix.

torch.Size([54156, 2])
We keep 2.44e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([194788, 2])
We keep 4.49e+08/2.16e+10 =  2% of the original kernel matrix.

torch.Size([74022, 2])
We keep 3.62e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([11801, 2])
We keep 2.00e+06/5.22e+07 =  3% of the original kernel matrix.

torch.Size([17676, 2])
We keep 2.97e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([25636, 2])
We keep 7.31e+06/3.51e+08 =  2% of the original kernel matrix.

torch.Size([26172, 2])
We keep 6.25e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([56118, 2])
We keep 4.19e+07/1.78e+09 =  2% of the original kernel matrix.

torch.Size([37821, 2])
We keep 1.24e+07/8.04e+08 =  1% of the original kernel matrix.

torch.Size([65720, 2])
We keep 4.23e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([44031, 2])
We keep 1.39e+07/9.46e+08 =  1% of the original kernel matrix.

torch.Size([47471, 2])
We keep 3.05e+07/1.28e+09 =  2% of the original kernel matrix.

torch.Size([35103, 2])
We keep 1.07e+07/6.82e+08 =  1% of the original kernel matrix.

torch.Size([87583, 2])
We keep 5.86e+07/3.90e+09 =  1% of the original kernel matrix.

torch.Size([47668, 2])
We keep 1.69e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([44044, 2])
We keep 7.73e+08/4.50e+09 = 17% of the original kernel matrix.

torch.Size([32293, 2])
We keep 1.79e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([36799, 2])
We keep 1.26e+07/7.13e+08 =  1% of the original kernel matrix.

torch.Size([31769, 2])
We keep 8.07e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([4009, 2])
We keep 2.46e+05/3.95e+06 =  6% of the original kernel matrix.

torch.Size([10947, 2])
We keep 1.14e+06/3.79e+07 =  3% of the original kernel matrix.

torch.Size([8340, 2])
We keep 1.11e+06/2.47e+07 =  4% of the original kernel matrix.

torch.Size([14902, 2])
We keep 2.28e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([197482, 2])
We keep 2.44e+08/2.49e+10 =  0% of the original kernel matrix.

torch.Size([75422, 2])
We keep 3.76e+07/3.01e+09 =  1% of the original kernel matrix.

torch.Size([39940, 2])
We keep 2.76e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([32217, 2])
We keep 9.90e+06/6.20e+08 =  1% of the original kernel matrix.

torch.Size([100207, 2])
We keep 6.56e+07/5.11e+09 =  1% of the original kernel matrix.

torch.Size([51378, 2])
We keep 1.88e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([4203, 2])
We keep 2.29e+05/4.02e+06 =  5% of the original kernel matrix.

torch.Size([11321, 2])
We keep 1.16e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([5676, 2])
We keep 4.40e+05/8.09e+06 =  5% of the original kernel matrix.

torch.Size([12551, 2])
We keep 1.51e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([129857, 2])
We keep 2.79e+08/1.20e+10 =  2% of the original kernel matrix.

torch.Size([58695, 2])
We keep 2.78e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([100201, 2])
We keep 1.02e+08/5.94e+09 =  1% of the original kernel matrix.

torch.Size([50980, 2])
We keep 2.05e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([60627, 2])
We keep 3.71e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([39508, 2])
We keep 1.26e+07/8.37e+08 =  1% of the original kernel matrix.

torch.Size([6448, 2])
We keep 5.61e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([13502, 2])
We keep 1.66e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([11795, 2])
We keep 2.00e+06/5.03e+07 =  3% of the original kernel matrix.

torch.Size([17667, 2])
We keep 2.93e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([30817, 2])
We keep 1.30e+07/5.07e+08 =  2% of the original kernel matrix.

torch.Size([28859, 2])
We keep 7.32e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([28673, 2])
We keep 1.25e+07/4.77e+08 =  2% of the original kernel matrix.

torch.Size([27615, 2])
We keep 7.16e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([43095, 2])
We keep 3.88e+07/1.06e+09 =  3% of the original kernel matrix.

torch.Size([33832, 2])
We keep 9.69e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([12258, 2])
We keep 3.70e+06/6.86e+07 =  5% of the original kernel matrix.

torch.Size([18156, 2])
We keep 3.28e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([11692, 2])
We keep 8.50e+06/1.03e+08 =  8% of the original kernel matrix.

torch.Size([17118, 2])
We keep 3.75e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([23151, 2])
We keep 2.17e+07/4.48e+08 =  4% of the original kernel matrix.

torch.Size([24289, 2])
We keep 6.96e+06/4.03e+08 =  1% of the original kernel matrix.

torch.Size([14492, 2])
We keep 9.09e+06/1.30e+08 =  6% of the original kernel matrix.

torch.Size([19175, 2])
We keep 4.29e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([200800, 2])
We keep 3.82e+08/2.80e+10 =  1% of the original kernel matrix.

torch.Size([75394, 2])
We keep 3.88e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([11412, 2])
We keep 1.69e+06/4.42e+07 =  3% of the original kernel matrix.

torch.Size([17439, 2])
We keep 2.73e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([36111, 2])
We keep 1.95e+07/8.15e+08 =  2% of the original kernel matrix.

torch.Size([30906, 2])
We keep 8.93e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([125920, 2])
We keep 1.67e+08/9.87e+09 =  1% of the original kernel matrix.

torch.Size([58417, 2])
We keep 2.58e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([81273, 2])
We keep 2.72e+08/3.72e+09 =  7% of the original kernel matrix.

torch.Size([45954, 2])
We keep 1.63e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([88124, 2])
We keep 1.09e+08/4.27e+09 =  2% of the original kernel matrix.

torch.Size([47995, 2])
We keep 1.76e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([159917, 2])
We keep 2.09e+08/1.56e+10 =  1% of the original kernel matrix.

torch.Size([66018, 2])
We keep 3.13e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([8156, 2])
We keep 9.42e+05/1.98e+07 =  4% of the original kernel matrix.

torch.Size([14729, 2])
We keep 2.08e+06/8.49e+07 =  2% of the original kernel matrix.

torch.Size([88522, 2])
We keep 9.70e+07/4.28e+09 =  2% of the original kernel matrix.

torch.Size([47834, 2])
We keep 1.80e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([33831, 2])
We keep 1.11e+07/5.92e+08 =  1% of the original kernel matrix.

torch.Size([30636, 2])
We keep 7.64e+06/4.64e+08 =  1% of the original kernel matrix.

torch.Size([296757, 2])
We keep 1.01e+09/4.91e+10 =  2% of the original kernel matrix.

torch.Size([90749, 2])
We keep 5.22e+07/4.23e+09 =  1% of the original kernel matrix.

torch.Size([151955, 2])
We keep 2.15e+08/1.58e+10 =  1% of the original kernel matrix.

torch.Size([64249, 2])
We keep 3.19e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([32999, 2])
We keep 2.98e+07/9.02e+08 =  3% of the original kernel matrix.

torch.Size([28591, 2])
We keep 9.29e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([102727, 2])
We keep 1.27e+08/5.96e+09 =  2% of the original kernel matrix.

torch.Size([52451, 2])
We keep 2.05e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([17358, 2])
We keep 3.71e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([21531, 2])
We keep 4.33e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([11262, 2])
We keep 1.35e+07/1.67e+08 =  8% of the original kernel matrix.

torch.Size([16632, 2])
We keep 4.71e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([9641, 2])
We keep 2.82e+06/3.26e+07 =  8% of the original kernel matrix.

torch.Size([16119, 2])
We keep 2.43e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([28929, 2])
We keep 1.68e+07/5.75e+08 =  2% of the original kernel matrix.

torch.Size([29047, 2])
We keep 7.87e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([115632, 2])
We keep 1.16e+08/7.34e+09 =  1% of the original kernel matrix.

torch.Size([55592, 2])
We keep 2.21e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([83036, 2])
We keep 1.67e+08/4.78e+09 =  3% of the original kernel matrix.

torch.Size([46194, 2])
We keep 1.88e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([12366, 2])
We keep 7.79e+06/8.69e+07 =  8% of the original kernel matrix.

torch.Size([18832, 2])
We keep 3.77e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([44424, 2])
We keep 3.25e+07/1.12e+09 =  2% of the original kernel matrix.

torch.Size([34360, 2])
We keep 9.85e+06/6.39e+08 =  1% of the original kernel matrix.

torch.Size([15650, 2])
We keep 2.64e+06/9.67e+07 =  2% of the original kernel matrix.

torch.Size([20684, 2])
We keep 3.72e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([399403, 2])
We keep 1.35e+09/1.08e+11 =  1% of the original kernel matrix.

torch.Size([105453, 2])
We keep 7.45e+07/6.25e+09 =  1% of the original kernel matrix.

torch.Size([38018, 2])
We keep 1.38e+07/7.77e+08 =  1% of the original kernel matrix.

torch.Size([32217, 2])
We keep 8.72e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([16490, 2])
We keep 4.38e+06/1.29e+08 =  3% of the original kernel matrix.

torch.Size([21007, 2])
We keep 4.17e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([90634, 2])
We keep 1.30e+08/5.27e+09 =  2% of the original kernel matrix.

torch.Size([48984, 2])
We keep 1.95e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([107979, 2])
We keep 1.01e+08/7.05e+09 =  1% of the original kernel matrix.

torch.Size([53765, 2])
We keep 2.21e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([14524, 2])
We keep 4.79e+06/1.01e+08 =  4% of the original kernel matrix.

torch.Size([19644, 2])
We keep 3.88e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([65213, 2])
We keep 7.10e+07/2.42e+09 =  2% of the original kernel matrix.

torch.Size([40743, 2])
We keep 1.40e+07/9.39e+08 =  1% of the original kernel matrix.

torch.Size([28917, 2])
We keep 3.94e+07/6.61e+08 =  5% of the original kernel matrix.

torch.Size([27772, 2])
We keep 8.10e+06/4.90e+08 =  1% of the original kernel matrix.

time for making ranges is 3.0648913383483887
Sorting X and nu_X
time for sorting X is 0.08613419532775879
Sorting Z and nu_Z
time for sorting Z is 0.00027179718017578125
Starting Optim
sum tnu_Z before tensor(38551072., device='cuda:0')
c= tensor(2187.0647, device='cuda:0')
c= tensor(194168.8438, device='cuda:0')
c= tensor(205812.3750, device='cuda:0')
c= tensor(211584.9844, device='cuda:0')
c= tensor(3338689.5000, device='cuda:0')
c= tensor(3971000.5000, device='cuda:0')
c= tensor(4620325., device='cuda:0')
c= tensor(5251159., device='cuda:0')
c= tensor(5523305., device='cuda:0')
c= tensor(19578388., device='cuda:0')
c= tensor(19604854., device='cuda:0')
c= tensor(27041268., device='cuda:0')
c= tensor(27072814., device='cuda:0')
c= tensor(33520264., device='cuda:0')
c= tensor(33700224., device='cuda:0')
c= tensor(35179596., device='cuda:0')
c= tensor(35584788., device='cuda:0')
c= tensor(36052892., device='cuda:0')
c= tensor(82019296., device='cuda:0')
c= tensor(85041832., device='cuda:0')
c= tensor(85234456., device='cuda:0')
c= tensor(1.4666e+08, device='cuda:0')
c= tensor(1.4681e+08, device='cuda:0')
c= tensor(1.4687e+08, device='cuda:0')
c= tensor(1.4899e+08, device='cuda:0')
c= tensor(1.4995e+08, device='cuda:0')
c= tensor(1.5122e+08, device='cuda:0')
c= tensor(1.5128e+08, device='cuda:0')
c= tensor(1.5339e+08, device='cuda:0')
c= tensor(8.6912e+08, device='cuda:0')
c= tensor(8.6913e+08, device='cuda:0')
c= tensor(9.0819e+08, device='cuda:0')
c= tensor(9.0825e+08, device='cuda:0')
c= tensor(9.0827e+08, device='cuda:0')
c= tensor(9.0835e+08, device='cuda:0')
c= tensor(9.1021e+08, device='cuda:0')
c= tensor(9.1192e+08, device='cuda:0')
c= tensor(9.1192e+08, device='cuda:0')
c= tensor(9.1193e+08, device='cuda:0')
c= tensor(9.1194e+08, device='cuda:0')
c= tensor(9.1197e+08, device='cuda:0')
c= tensor(9.1197e+08, device='cuda:0')
c= tensor(9.1197e+08, device='cuda:0')
c= tensor(9.1198e+08, device='cuda:0')
c= tensor(9.1198e+08, device='cuda:0')
c= tensor(9.1199e+08, device='cuda:0')
c= tensor(9.1200e+08, device='cuda:0')
c= tensor(9.1202e+08, device='cuda:0')
c= tensor(9.1202e+08, device='cuda:0')
c= tensor(9.1204e+08, device='cuda:0')
c= tensor(9.1211e+08, device='cuda:0')
c= tensor(9.1211e+08, device='cuda:0')
c= tensor(9.1213e+08, device='cuda:0')
c= tensor(9.1213e+08, device='cuda:0')
c= tensor(9.1215e+08, device='cuda:0')
c= tensor(9.1231e+08, device='cuda:0')
c= tensor(9.1235e+08, device='cuda:0')
c= tensor(9.1235e+08, device='cuda:0')
c= tensor(9.1235e+08, device='cuda:0')
c= tensor(9.1236e+08, device='cuda:0')
c= tensor(9.1237e+08, device='cuda:0')
c= tensor(9.1237e+08, device='cuda:0')
c= tensor(9.1239e+08, device='cuda:0')
c= tensor(9.1241e+08, device='cuda:0')
c= tensor(9.1242e+08, device='cuda:0')
c= tensor(9.1242e+08, device='cuda:0')
c= tensor(9.1243e+08, device='cuda:0')
c= tensor(9.1245e+08, device='cuda:0')
c= tensor(9.1246e+08, device='cuda:0')
c= tensor(9.1246e+08, device='cuda:0')
c= tensor(9.1249e+08, device='cuda:0')
c= tensor(9.1249e+08, device='cuda:0')
c= tensor(9.1250e+08, device='cuda:0')
c= tensor(9.1251e+08, device='cuda:0')
c= tensor(9.1251e+08, device='cuda:0')
c= tensor(9.1252e+08, device='cuda:0')
c= tensor(9.1252e+08, device='cuda:0')
c= tensor(9.1252e+08, device='cuda:0')
c= tensor(9.1254e+08, device='cuda:0')
c= tensor(9.1273e+08, device='cuda:0')
c= tensor(9.1274e+08, device='cuda:0')
c= tensor(9.1274e+08, device='cuda:0')
c= tensor(9.1275e+08, device='cuda:0')
c= tensor(9.1275e+08, device='cuda:0')
c= tensor(9.1276e+08, device='cuda:0')
c= tensor(9.1277e+08, device='cuda:0')
c= tensor(9.1277e+08, device='cuda:0')
c= tensor(9.1277e+08, device='cuda:0')
c= tensor(9.1278e+08, device='cuda:0')
c= tensor(9.1279e+08, device='cuda:0')
c= tensor(9.1280e+08, device='cuda:0')
c= tensor(9.1280e+08, device='cuda:0')
c= tensor(9.1280e+08, device='cuda:0')
c= tensor(9.1281e+08, device='cuda:0')
c= tensor(9.1282e+08, device='cuda:0')
c= tensor(9.1283e+08, device='cuda:0')
c= tensor(9.1284e+08, device='cuda:0')
c= tensor(9.1288e+08, device='cuda:0')
c= tensor(9.1288e+08, device='cuda:0')
c= tensor(9.1289e+08, device='cuda:0')
c= tensor(9.1291e+08, device='cuda:0')
c= tensor(9.1291e+08, device='cuda:0')
c= tensor(9.1292e+08, device='cuda:0')
c= tensor(9.1292e+08, device='cuda:0')
c= tensor(9.1293e+08, device='cuda:0')
c= tensor(9.1293e+08, device='cuda:0')
c= tensor(9.1295e+08, device='cuda:0')
c= tensor(9.1295e+08, device='cuda:0')
c= tensor(9.1295e+08, device='cuda:0')
c= tensor(9.1295e+08, device='cuda:0')
c= tensor(9.1296e+08, device='cuda:0')
c= tensor(9.1296e+08, device='cuda:0')
c= tensor(9.1296e+08, device='cuda:0')
c= tensor(9.1297e+08, device='cuda:0')
c= tensor(9.1299e+08, device='cuda:0')
c= tensor(9.1299e+08, device='cuda:0')
c= tensor(9.1301e+08, device='cuda:0')
c= tensor(9.1301e+08, device='cuda:0')
c= tensor(9.1303e+08, device='cuda:0')
c= tensor(9.1304e+08, device='cuda:0')
c= tensor(9.1312e+08, device='cuda:0')
c= tensor(9.1312e+08, device='cuda:0')
c= tensor(9.1312e+08, device='cuda:0')
c= tensor(9.1313e+08, device='cuda:0')
c= tensor(9.1313e+08, device='cuda:0')
c= tensor(9.1314e+08, device='cuda:0')
c= tensor(9.1314e+08, device='cuda:0')
c= tensor(9.1314e+08, device='cuda:0')
c= tensor(9.1319e+08, device='cuda:0')
c= tensor(9.1320e+08, device='cuda:0')
c= tensor(9.1324e+08, device='cuda:0')
c= tensor(9.1324e+08, device='cuda:0')
c= tensor(9.1325e+08, device='cuda:0')
c= tensor(9.1325e+08, device='cuda:0')
c= tensor(9.1325e+08, device='cuda:0')
c= tensor(9.1325e+08, device='cuda:0')
c= tensor(9.1326e+08, device='cuda:0')
c= tensor(9.1326e+08, device='cuda:0')
c= tensor(9.1326e+08, device='cuda:0')
c= tensor(9.1327e+08, device='cuda:0')
c= tensor(9.1327e+08, device='cuda:0')
c= tensor(9.1327e+08, device='cuda:0')
c= tensor(9.1330e+08, device='cuda:0')
c= tensor(9.1333e+08, device='cuda:0')
c= tensor(9.1333e+08, device='cuda:0')
c= tensor(9.1334e+08, device='cuda:0')
c= tensor(9.1334e+08, device='cuda:0')
c= tensor(9.1335e+08, device='cuda:0')
c= tensor(9.1335e+08, device='cuda:0')
c= tensor(9.1335e+08, device='cuda:0')
c= tensor(9.1336e+08, device='cuda:0')
c= tensor(9.1337e+08, device='cuda:0')
c= tensor(9.1337e+08, device='cuda:0')
c= tensor(9.1341e+08, device='cuda:0')
c= tensor(9.1341e+08, device='cuda:0')
c= tensor(9.1344e+08, device='cuda:0')
c= tensor(9.1344e+08, device='cuda:0')
c= tensor(9.1345e+08, device='cuda:0')
c= tensor(9.1346e+08, device='cuda:0')
c= tensor(9.1346e+08, device='cuda:0')
c= tensor(9.1348e+08, device='cuda:0')
c= tensor(9.1349e+08, device='cuda:0')
c= tensor(9.1349e+08, device='cuda:0')
c= tensor(9.1349e+08, device='cuda:0')
c= tensor(9.1350e+08, device='cuda:0')
c= tensor(9.1350e+08, device='cuda:0')
c= tensor(9.1351e+08, device='cuda:0')
c= tensor(9.1352e+08, device='cuda:0')
c= tensor(9.1353e+08, device='cuda:0')
c= tensor(9.1353e+08, device='cuda:0')
c= tensor(9.1353e+08, device='cuda:0')
c= tensor(9.1354e+08, device='cuda:0')
c= tensor(9.1356e+08, device='cuda:0')
c= tensor(9.1356e+08, device='cuda:0')
c= tensor(9.1358e+08, device='cuda:0')
c= tensor(9.1358e+08, device='cuda:0')
c= tensor(9.1359e+08, device='cuda:0')
c= tensor(9.1360e+08, device='cuda:0')
c= tensor(9.1361e+08, device='cuda:0')
c= tensor(9.1362e+08, device='cuda:0')
c= tensor(9.1363e+08, device='cuda:0')
c= tensor(9.1363e+08, device='cuda:0')
c= tensor(9.1364e+08, device='cuda:0')
c= tensor(9.1364e+08, device='cuda:0')
c= tensor(9.1365e+08, device='cuda:0')
c= tensor(9.1367e+08, device='cuda:0')
c= tensor(9.1367e+08, device='cuda:0')
c= tensor(9.1367e+08, device='cuda:0')
c= tensor(9.1370e+08, device='cuda:0')
c= tensor(9.1374e+08, device='cuda:0')
c= tensor(9.1375e+08, device='cuda:0')
c= tensor(9.1375e+08, device='cuda:0')
c= tensor(9.1375e+08, device='cuda:0')
c= tensor(9.1376e+08, device='cuda:0')
c= tensor(9.1377e+08, device='cuda:0')
c= tensor(9.1377e+08, device='cuda:0')
c= tensor(9.1378e+08, device='cuda:0')
c= tensor(9.1378e+08, device='cuda:0')
c= tensor(9.1378e+08, device='cuda:0')
c= tensor(9.1379e+08, device='cuda:0')
c= tensor(9.1381e+08, device='cuda:0')
c= tensor(9.1381e+08, device='cuda:0')
c= tensor(9.1383e+08, device='cuda:0')
c= tensor(9.1385e+08, device='cuda:0')
c= tensor(9.1386e+08, device='cuda:0')
c= tensor(9.1386e+08, device='cuda:0')
c= tensor(9.1388e+08, device='cuda:0')
c= tensor(9.1388e+08, device='cuda:0')
c= tensor(9.1389e+08, device='cuda:0')
c= tensor(9.1391e+08, device='cuda:0')
c= tensor(9.1392e+08, device='cuda:0')
c= tensor(9.1393e+08, device='cuda:0')
c= tensor(9.1393e+08, device='cuda:0')
c= tensor(9.1394e+08, device='cuda:0')
c= tensor(9.1394e+08, device='cuda:0')
c= tensor(9.1395e+08, device='cuda:0')
c= tensor(9.1395e+08, device='cuda:0')
c= tensor(9.1395e+08, device='cuda:0')
c= tensor(9.1396e+08, device='cuda:0')
c= tensor(9.1397e+08, device='cuda:0')
c= tensor(9.1399e+08, device='cuda:0')
c= tensor(9.1399e+08, device='cuda:0')
c= tensor(9.1401e+08, device='cuda:0')
c= tensor(9.1402e+08, device='cuda:0')
c= tensor(9.1402e+08, device='cuda:0')
c= tensor(9.1402e+08, device='cuda:0')
c= tensor(9.1403e+08, device='cuda:0')
c= tensor(9.1403e+08, device='cuda:0')
c= tensor(9.1404e+08, device='cuda:0')
c= tensor(9.1404e+08, device='cuda:0')
c= tensor(9.1404e+08, device='cuda:0')
c= tensor(9.1405e+08, device='cuda:0')
c= tensor(9.1405e+08, device='cuda:0')
c= tensor(9.1407e+08, device='cuda:0')
c= tensor(9.1407e+08, device='cuda:0')
c= tensor(9.1407e+08, device='cuda:0')
c= tensor(9.1408e+08, device='cuda:0')
c= tensor(9.1411e+08, device='cuda:0')
c= tensor(9.1412e+08, device='cuda:0')
c= tensor(9.1416e+08, device='cuda:0')
c= tensor(9.1545e+08, device='cuda:0')
c= tensor(9.1627e+08, device='cuda:0')
c= tensor(9.1631e+08, device='cuda:0')
c= tensor(9.1631e+08, device='cuda:0')
c= tensor(9.1633e+08, device='cuda:0')
c= tensor(9.2109e+08, device='cuda:0')
c= tensor(9.4627e+08, device='cuda:0')
c= tensor(9.4628e+08, device='cuda:0')
c= tensor(9.4974e+08, device='cuda:0')
c= tensor(9.5029e+08, device='cuda:0')
c= tensor(9.5097e+08, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0783e+09, device='cuda:0')
c= tensor(1.2058e+09, device='cuda:0')
c= tensor(1.2058e+09, device='cuda:0')
c= tensor(1.2064e+09, device='cuda:0')
c= tensor(1.2208e+09, device='cuda:0')
c= tensor(1.2357e+09, device='cuda:0')
c= tensor(1.2367e+09, device='cuda:0')
c= tensor(1.2463e+09, device='cuda:0')
c= tensor(1.2474e+09, device='cuda:0')
c= tensor(1.2474e+09, device='cuda:0')
c= tensor(1.2475e+09, device='cuda:0')
c= tensor(1.2979e+09, device='cuda:0')
c= tensor(1.2980e+09, device='cuda:0')
c= tensor(1.2980e+09, device='cuda:0')
c= tensor(1.2999e+09, device='cuda:0')
c= tensor(1.3006e+09, device='cuda:0')
c= tensor(1.3149e+09, device='cuda:0')
c= tensor(1.3197e+09, device='cuda:0')
c= tensor(1.3197e+09, device='cuda:0')
c= tensor(1.3198e+09, device='cuda:0')
c= tensor(1.3199e+09, device='cuda:0')
c= tensor(1.3204e+09, device='cuda:0')
c= tensor(1.3227e+09, device='cuda:0')
c= tensor(1.3268e+09, device='cuda:0')
c= tensor(1.3284e+09, device='cuda:0')
c= tensor(1.3284e+09, device='cuda:0')
c= tensor(1.3285e+09, device='cuda:0')
c= tensor(1.3293e+09, device='cuda:0')
c= tensor(1.3365e+09, device='cuda:0')
c= tensor(1.3377e+09, device='cuda:0')
c= tensor(1.3383e+09, device='cuda:0')
c= tensor(1.3642e+09, device='cuda:0')
c= tensor(1.3643e+09, device='cuda:0')
c= tensor(1.3645e+09, device='cuda:0')
c= tensor(1.3658e+09, device='cuda:0')
c= tensor(1.3659e+09, device='cuda:0')
c= tensor(1.3693e+09, device='cuda:0')
c= tensor(1.3800e+09, device='cuda:0')
c= tensor(1.4151e+09, device='cuda:0')
c= tensor(1.4152e+09, device='cuda:0')
c= tensor(1.4154e+09, device='cuda:0')
c= tensor(1.4156e+09, device='cuda:0')
c= tensor(1.4156e+09, device='cuda:0')
c= tensor(1.4160e+09, device='cuda:0')
c= tensor(1.4160e+09, device='cuda:0')
c= tensor(1.4163e+09, device='cuda:0')
c= tensor(1.4383e+09, device='cuda:0')
c= tensor(1.4428e+09, device='cuda:0')
c= tensor(1.4429e+09, device='cuda:0')
c= tensor(1.4429e+09, device='cuda:0')
c= tensor(1.4457e+09, device='cuda:0')
c= tensor(1.4460e+09, device='cuda:0')
c= tensor(1.4461e+09, device='cuda:0')
c= tensor(1.4461e+09, device='cuda:0')
c= tensor(1.4886e+09, device='cuda:0')
c= tensor(1.4886e+09, device='cuda:0')
c= tensor(1.6063e+09, device='cuda:0')
c= tensor(1.6064e+09, device='cuda:0')
c= tensor(1.6076e+09, device='cuda:0')
c= tensor(1.6085e+09, device='cuda:0')
c= tensor(1.6457e+09, device='cuda:0')
c= tensor(1.6469e+09, device='cuda:0')
c= tensor(1.6469e+09, device='cuda:0')
c= tensor(1.6504e+09, device='cuda:0')
c= tensor(1.6537e+09, device='cuda:0')
c= tensor(1.6538e+09, device='cuda:0')
c= tensor(1.6567e+09, device='cuda:0')
c= tensor(1.6610e+09, device='cuda:0')
c= tensor(1.6739e+09, device='cuda:0')
c= tensor(1.6757e+09, device='cuda:0')
c= tensor(1.6758e+09, device='cuda:0')
c= tensor(1.6758e+09, device='cuda:0')
c= tensor(1.6897e+09, device='cuda:0')
c= tensor(1.7356e+09, device='cuda:0')
c= tensor(1.7357e+09, device='cuda:0')
c= tensor(1.7357e+09, device='cuda:0')
c= tensor(1.7398e+09, device='cuda:0')
c= tensor(1.7459e+09, device='cuda:0')
c= tensor(1.7525e+09, device='cuda:0')
c= tensor(1.7525e+09, device='cuda:0')
c= tensor(1.7528e+09, device='cuda:0')
c= tensor(1.7529e+09, device='cuda:0')
c= tensor(1.7529e+09, device='cuda:0')
c= tensor(1.7529e+09, device='cuda:0')
c= tensor(1.7530e+09, device='cuda:0')
c= tensor(1.7571e+09, device='cuda:0')
c= tensor(1.7575e+09, device='cuda:0')
c= tensor(1.7577e+09, device='cuda:0')
c= tensor(1.7578e+09, device='cuda:0')
c= tensor(1.7578e+09, device='cuda:0')
c= tensor(1.8745e+09, device='cuda:0')
c= tensor(1.8745e+09, device='cuda:0')
c= tensor(1.8796e+09, device='cuda:0')
c= tensor(1.8796e+09, device='cuda:0')
c= tensor(1.8796e+09, device='cuda:0')
c= tensor(1.8796e+09, device='cuda:0')
c= tensor(1.8805e+09, device='cuda:0')
c= tensor(1.8805e+09, device='cuda:0')
c= tensor(1.8809e+09, device='cuda:0')
c= tensor(1.8809e+09, device='cuda:0')
c= tensor(1.8809e+09, device='cuda:0')
c= tensor(1.9043e+09, device='cuda:0')
c= tensor(1.9064e+09, device='cuda:0')
c= tensor(1.9067e+09, device='cuda:0')
c= tensor(1.9101e+09, device='cuda:0')
c= tensor(1.9192e+09, device='cuda:0')
c= tensor(1.9192e+09, device='cuda:0')
c= tensor(1.9192e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9195e+09, device='cuda:0')
c= tensor(1.9195e+09, device='cuda:0')
c= tensor(1.9195e+09, device='cuda:0')
c= tensor(1.9196e+09, device='cuda:0')
c= tensor(1.9196e+09, device='cuda:0')
c= tensor(1.9838e+09, device='cuda:0')
c= tensor(1.9840e+09, device='cuda:0')
c= tensor(1.9851e+09, device='cuda:0')
c= tensor(1.9851e+09, device='cuda:0')
c= tensor(1.9868e+09, device='cuda:0')
c= tensor(1.9878e+09, device='cuda:0')
c= tensor(2.5357e+09, device='cuda:0')
c= tensor(2.6067e+09, device='cuda:0')
c= tensor(2.6075e+09, device='cuda:0')
c= tensor(2.6090e+09, device='cuda:0')
c= tensor(2.6090e+09, device='cuda:0')
c= tensor(2.6106e+09, device='cuda:0')
c= tensor(2.6134e+09, device='cuda:0')
c= tensor(2.6172e+09, device='cuda:0')
c= tensor(2.6172e+09, device='cuda:0')
c= tensor(2.6205e+09, device='cuda:0')
c= tensor(2.6352e+09, device='cuda:0')
c= tensor(2.6360e+09, device='cuda:0')
c= tensor(2.6361e+09, device='cuda:0')
c= tensor(2.6367e+09, device='cuda:0')
c= tensor(2.6367e+09, device='cuda:0')
c= tensor(2.6367e+09, device='cuda:0')
c= tensor(2.6416e+09, device='cuda:0')
c= tensor(2.6426e+09, device='cuda:0')
c= tensor(2.6426e+09, device='cuda:0')
c= tensor(2.6463e+09, device='cuda:0')
c= tensor(2.6464e+09, device='cuda:0')
c= tensor(2.6464e+09, device='cuda:0')
c= tensor(2.6497e+09, device='cuda:0')
c= tensor(2.6525e+09, device='cuda:0')
c= tensor(2.6539e+09, device='cuda:0')
c= tensor(2.6622e+09, device='cuda:0')
c= tensor(2.6689e+09, device='cuda:0')
c= tensor(2.6689e+09, device='cuda:0')
c= tensor(2.6708e+09, device='cuda:0')
c= tensor(2.6741e+09, device='cuda:0')
c= tensor(2.6793e+09, device='cuda:0')
c= tensor(2.6793e+09, device='cuda:0')
c= tensor(2.7285e+09, device='cuda:0')
c= tensor(2.7484e+09, device='cuda:0')
c= tensor(2.7502e+09, device='cuda:0')
c= tensor(2.7535e+09, device='cuda:0')
c= tensor(2.7542e+09, device='cuda:0')
c= tensor(2.7544e+09, device='cuda:0')
c= tensor(2.7544e+09, device='cuda:0')
c= tensor(2.7545e+09, device='cuda:0')
c= tensor(2.7595e+09, device='cuda:0')
c= tensor(2.7636e+09, device='cuda:0')
c= tensor(2.7863e+09, device='cuda:0')
c= tensor(2.8018e+09, device='cuda:0')
c= tensor(2.8032e+09, device='cuda:0')
c= tensor(2.8034e+09, device='cuda:0')
c= tensor(2.8053e+09, device='cuda:0')
c= tensor(2.8053e+09, device='cuda:0')
c= tensor(2.8054e+09, device='cuda:0')
c= tensor(2.8142e+09, device='cuda:0')
c= tensor(2.8147e+09, device='cuda:0')
c= tensor(2.8147e+09, device='cuda:0')
c= tensor(2.8148e+09, device='cuda:0')
c= tensor(2.8229e+09, device='cuda:0')
c= tensor(2.8236e+09, device='cuda:0')
c= tensor(2.8256e+09, device='cuda:0')
c= tensor(2.8259e+09, device='cuda:0')
c= tensor(2.8263e+09, device='cuda:0')
c= tensor(2.8264e+09, device='cuda:0')
c= tensor(2.8264e+09, device='cuda:0')
c= tensor(2.8272e+09, device='cuda:0')
c= tensor(2.8275e+09, device='cuda:0')
c= tensor(2.8275e+09, device='cuda:0')
c= tensor(2.8371e+09, device='cuda:0')
c= tensor(2.8371e+09, device='cuda:0')
c= tensor(2.8372e+09, device='cuda:0')
c= tensor(2.8373e+09, device='cuda:0')
c= tensor(2.8394e+09, device='cuda:0')
c= tensor(2.8394e+09, device='cuda:0')
c= tensor(2.8404e+09, device='cuda:0')
c= tensor(2.8406e+09, device='cuda:0')
c= tensor(2.8408e+09, device='cuda:0')
c= tensor(2.8416e+09, device='cuda:0')
c= tensor(2.9332e+09, device='cuda:0')
c= tensor(2.9332e+09, device='cuda:0')
c= tensor(2.9333e+09, device='cuda:0')
c= tensor(2.9387e+09, device='cuda:0')
c= tensor(2.9388e+09, device='cuda:0')
c= tensor(2.9810e+09, device='cuda:0')
c= tensor(2.9810e+09, device='cuda:0')
c= tensor(2.9841e+09, device='cuda:0')
c= tensor(3.0020e+09, device='cuda:0')
c= tensor(3.0021e+09, device='cuda:0')
c= tensor(3.0431e+09, device='cuda:0')
c= tensor(3.0433e+09, device='cuda:0')
c= tensor(3.1408e+09, device='cuda:0')
c= tensor(3.1409e+09, device='cuda:0')
c= tensor(3.1539e+09, device='cuda:0')
c= tensor(3.1539e+09, device='cuda:0')
c= tensor(3.1539e+09, device='cuda:0')
c= tensor(3.1539e+09, device='cuda:0')
c= tensor(3.1555e+09, device='cuda:0')
c= tensor(3.1564e+09, device='cuda:0')
c= tensor(3.1742e+09, device='cuda:0')
c= tensor(3.1743e+09, device='cuda:0')
c= tensor(3.1743e+09, device='cuda:0')
c= tensor(3.1744e+09, device='cuda:0')
c= tensor(3.1816e+09, device='cuda:0')
c= tensor(3.1835e+09, device='cuda:0')
c= tensor(3.1947e+09, device='cuda:0')
c= tensor(3.1958e+09, device='cuda:0')
c= tensor(3.1958e+09, device='cuda:0')
c= tensor(3.1958e+09, device='cuda:0')
c= tensor(3.1960e+09, device='cuda:0')
c= tensor(3.6201e+09, device='cuda:0')
c= tensor(3.6201e+09, device='cuda:0')
c= tensor(3.6202e+09, device='cuda:0')
c= tensor(3.6264e+09, device='cuda:0')
c= tensor(3.6287e+09, device='cuda:0')
c= tensor(3.6287e+09, device='cuda:0')
c= tensor(3.6287e+09, device='cuda:0')
c= tensor(3.6311e+09, device='cuda:0')
c= tensor(3.6318e+09, device='cuda:0')
c= tensor(3.6319e+09, device='cuda:0')
c= tensor(3.6328e+09, device='cuda:0')
c= tensor(3.6464e+09, device='cuda:0')
c= tensor(3.6493e+09, device='cuda:0')
c= tensor(3.6760e+09, device='cuda:0')
c= tensor(3.6783e+09, device='cuda:0')
c= tensor(3.6784e+09, device='cuda:0')
c= tensor(3.6814e+09, device='cuda:0')
c= tensor(3.6820e+09, device='cuda:0')
c= tensor(3.6821e+09, device='cuda:0')
c= tensor(3.6822e+09, device='cuda:0')
c= tensor(3.6823e+09, device='cuda:0')
c= tensor(3.6857e+09, device='cuda:0')
c= tensor(3.6859e+09, device='cuda:0')
c= tensor(3.6859e+09, device='cuda:0')
c= tensor(3.6861e+09, device='cuda:0')
c= tensor(3.6868e+09, device='cuda:0')
c= tensor(3.6871e+09, device='cuda:0')
c= tensor(3.6904e+09, device='cuda:0')
c= tensor(3.6905e+09, device='cuda:0')
c= tensor(3.6906e+09, device='cuda:0')
c= tensor(3.6909e+09, device='cuda:0')
c= tensor(3.6909e+09, device='cuda:0')
c= tensor(3.6910e+09, device='cuda:0')
c= tensor(3.6911e+09, device='cuda:0')
c= tensor(3.6986e+09, device='cuda:0')
c= tensor(3.6986e+09, device='cuda:0')
c= tensor(3.6986e+09, device='cuda:0')
c= tensor(3.6986e+09, device='cuda:0')
c= tensor(3.7042e+09, device='cuda:0')
c= tensor(3.7496e+09, device='cuda:0')
c= tensor(3.7513e+09, device='cuda:0')
c= tensor(3.7513e+09, device='cuda:0')
c= tensor(3.7556e+09, device='cuda:0')
c= tensor(3.7628e+09, device='cuda:0')
c= tensor(3.7628e+09, device='cuda:0')
c= tensor(3.7629e+09, device='cuda:0')
c= tensor(3.7632e+09, device='cuda:0')
c= tensor(3.7638e+09, device='cuda:0')
c= tensor(3.7691e+09, device='cuda:0')
c= tensor(3.7705e+09, device='cuda:0')
c= tensor(3.7706e+09, device='cuda:0')
c= tensor(3.7707e+09, device='cuda:0')
c= tensor(3.7707e+09, device='cuda:0')
c= tensor(3.7713e+09, device='cuda:0')
c= tensor(3.7722e+09, device='cuda:0')
c= tensor(3.7745e+09, device='cuda:0')
c= tensor(3.7754e+09, device='cuda:0')
c= tensor(3.7920e+09, device='cuda:0')
c= tensor(3.7921e+09, device='cuda:0')
c= tensor(3.7921e+09, device='cuda:0')
c= tensor(3.7922e+09, device='cuda:0')
c= tensor(3.7935e+09, device='cuda:0')
c= tensor(3.7939e+09, device='cuda:0')
c= tensor(3.7939e+09, device='cuda:0')
c= tensor(3.7939e+09, device='cuda:0')
c= tensor(3.7945e+09, device='cuda:0')
c= tensor(3.7945e+09, device='cuda:0')
c= tensor(3.7949e+09, device='cuda:0')
c= tensor(3.7949e+09, device='cuda:0')
c= tensor(3.7950e+09, device='cuda:0')
c= tensor(3.7951e+09, device='cuda:0')
c= tensor(3.7952e+09, device='cuda:0')
c= tensor(3.7952e+09, device='cuda:0')
c= tensor(3.7983e+09, device='cuda:0')
c= tensor(3.8119e+09, device='cuda:0')
c= tensor(3.8221e+09, device='cuda:0')
c= tensor(3.8346e+09, device='cuda:0')
c= tensor(3.8349e+09, device='cuda:0')
c= tensor(3.8349e+09, device='cuda:0')
c= tensor(3.8351e+09, device='cuda:0')
c= tensor(3.8383e+09, device='cuda:0')
c= tensor(3.8384e+09, device='cuda:0')
c= tensor(3.8388e+09, device='cuda:0')
c= tensor(3.8389e+09, device='cuda:0')
c= tensor(3.8831e+09, device='cuda:0')
c= tensor(3.8843e+09, device='cuda:0')
c= tensor(3.8846e+09, device='cuda:0')
c= tensor(3.9172e+09, device='cuda:0')
c= tensor(3.9176e+09, device='cuda:0')
c= tensor(3.9183e+09, device='cuda:0')
c= tensor(3.9419e+09, device='cuda:0')
c= tensor(3.9467e+09, device='cuda:0')
c= tensor(3.9516e+09, device='cuda:0')
c= tensor(3.9518e+09, device='cuda:0')
c= tensor(3.9554e+09, device='cuda:0')
c= tensor(3.9554e+09, device='cuda:0')
c= tensor(3.9566e+09, device='cuda:0')
c= tensor(4.0324e+09, device='cuda:0')
c= tensor(4.0339e+09, device='cuda:0')
c= tensor(4.0356e+09, device='cuda:0')
c= tensor(4.0357e+09, device='cuda:0')
c= tensor(4.0385e+09, device='cuda:0')
c= tensor(4.0391e+09, device='cuda:0')
c= tensor(4.0392e+09, device='cuda:0')
c= tensor(4.0397e+09, device='cuda:0')
c= tensor(4.0575e+09, device='cuda:0')
c= tensor(4.0577e+09, device='cuda:0')
c= tensor(4.1384e+09, device='cuda:0')
c= tensor(4.1390e+09, device='cuda:0')
c= tensor(4.1395e+09, device='cuda:0')
c= tensor(4.1398e+09, device='cuda:0')
c= tensor(4.1410e+09, device='cuda:0')
c= tensor(4.1499e+09, device='cuda:0')
c= tensor(4.1499e+09, device='cuda:0')
c= tensor(4.2161e+09, device='cuda:0')
c= tensor(4.2176e+09, device='cuda:0')
c= tensor(4.2183e+09, device='cuda:0')
c= tensor(4.2183e+09, device='cuda:0')
c= tensor(4.2184e+09, device='cuda:0')
c= tensor(4.2184e+09, device='cuda:0')
c= tensor(4.2184e+09, device='cuda:0')
c= tensor(4.2185e+09, device='cuda:0')
c= tensor(4.2244e+09, device='cuda:0')
c= tensor(5.2679e+09, device='cuda:0')
c= tensor(5.2824e+09, device='cuda:0')
c= tensor(5.2859e+09, device='cuda:0')
c= tensor(5.2859e+09, device='cuda:0')
c= tensor(5.2863e+09, device='cuda:0')
c= tensor(5.2866e+09, device='cuda:0')
c= tensor(5.2904e+09, device='cuda:0')
c= tensor(5.2904e+09, device='cuda:0')
c= tensor(5.4449e+09, device='cuda:0')
c= tensor(5.4449e+09, device='cuda:0')
c= tensor(5.4510e+09, device='cuda:0')
c= tensor(5.4512e+09, device='cuda:0')
c= tensor(5.4545e+09, device='cuda:0')
c= tensor(5.4816e+09, device='cuda:0')
c= tensor(5.4818e+09, device='cuda:0')
c= tensor(5.4818e+09, device='cuda:0')
c= tensor(5.4828e+09, device='cuda:0')
c= tensor(5.4828e+09, device='cuda:0')
c= tensor(5.4829e+09, device='cuda:0')
c= tensor(5.4909e+09, device='cuda:0')
c= tensor(5.6770e+09, device='cuda:0')
c= tensor(5.6774e+09, device='cuda:0')
c= tensor(5.6780e+09, device='cuda:0')
c= tensor(5.6940e+09, device='cuda:0')
c= tensor(5.7152e+09, device='cuda:0')
c= tensor(5.7158e+09, device='cuda:0')
c= tensor(5.7158e+09, device='cuda:0')
c= tensor(5.7327e+09, device='cuda:0')
c= tensor(5.7348e+09, device='cuda:0')
c= tensor(5.7365e+09, device='cuda:0')
c= tensor(5.7374e+09, device='cuda:0')
c= tensor(5.7388e+09, device='cuda:0')
c= tensor(5.7399e+09, device='cuda:0')
c= tensor(5.8280e+09, device='cuda:0')
c= tensor(5.8296e+09, device='cuda:0')
c= tensor(5.8297e+09, device='cuda:0')
c= tensor(5.8297e+09, device='cuda:0')
c= tensor(5.8301e+09, device='cuda:0')
c= tensor(5.8320e+09, device='cuda:0')
c= tensor(5.8369e+09, device='cuda:0')
c= tensor(5.8486e+09, device='cuda:0')
c= tensor(5.8486e+09, device='cuda:0')
c= tensor(5.8488e+09, device='cuda:0')
c= tensor(5.8499e+09, device='cuda:0')
c= tensor(5.8512e+09, device='cuda:0')
c= tensor(5.8519e+09, device='cuda:0')
c= tensor(5.8533e+09, device='cuda:0')
c= tensor(5.8970e+09, device='cuda:0')
c= tensor(5.8974e+09, device='cuda:0')
c= tensor(5.8974e+09, device='cuda:0')
c= tensor(5.8974e+09, device='cuda:0')
c= tensor(5.9060e+09, device='cuda:0')
c= tensor(5.9068e+09, device='cuda:0')
c= tensor(5.9084e+09, device='cuda:0')
c= tensor(5.9084e+09, device='cuda:0')
c= tensor(5.9084e+09, device='cuda:0')
c= tensor(5.9166e+09, device='cuda:0')
c= tensor(5.9189e+09, device='cuda:0')
c= tensor(5.9196e+09, device='cuda:0')
c= tensor(5.9196e+09, device='cuda:0')
c= tensor(5.9196e+09, device='cuda:0')
c= tensor(5.9199e+09, device='cuda:0')
c= tensor(5.9201e+09, device='cuda:0')
c= tensor(5.9218e+09, device='cuda:0')
c= tensor(5.9219e+09, device='cuda:0')
c= tensor(5.9225e+09, device='cuda:0')
c= tensor(5.9229e+09, device='cuda:0')
c= tensor(5.9231e+09, device='cuda:0')
c= tensor(5.9461e+09, device='cuda:0')
c= tensor(5.9462e+09, device='cuda:0')
c= tensor(5.9467e+09, device='cuda:0')
c= tensor(5.9503e+09, device='cuda:0')
c= tensor(5.9550e+09, device='cuda:0')
c= tensor(5.9572e+09, device='cuda:0')
c= tensor(5.9623e+09, device='cuda:0')
c= tensor(5.9623e+09, device='cuda:0')
c= tensor(5.9643e+09, device='cuda:0')
c= tensor(5.9645e+09, device='cuda:0')
c= tensor(5.9895e+09, device='cuda:0')
c= tensor(5.9955e+09, device='cuda:0')
c= tensor(5.9961e+09, device='cuda:0')
c= tensor(5.9991e+09, device='cuda:0')
c= tensor(5.9992e+09, device='cuda:0')
c= tensor(6.0002e+09, device='cuda:0')
c= tensor(6.0002e+09, device='cuda:0')
c= tensor(6.0006e+09, device='cuda:0')
c= tensor(6.0036e+09, device='cuda:0')
c= tensor(6.0075e+09, device='cuda:0')
c= tensor(6.0076e+09, device='cuda:0')
c= tensor(6.0084e+09, device='cuda:0')
c= tensor(6.0084e+09, device='cuda:0')
c= tensor(6.0560e+09, device='cuda:0')
c= tensor(6.0563e+09, device='cuda:0')
c= tensor(6.0563e+09, device='cuda:0')
c= tensor(6.0592e+09, device='cuda:0')
c= tensor(6.0613e+09, device='cuda:0')
c= tensor(6.0614e+09, device='cuda:0')
c= tensor(6.0628e+09, device='cuda:0')
c= tensor(6.0635e+09, device='cuda:0')
memory (bytes)
4713975808
time for making loss 2 is 13.365241527557373
p0 True
it  0 : 1763280896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 34% |
shape of L is 
torch.Size([])
memory (bytes)
4714242048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4714803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  8% |
error is  87356190000.0
relative error loss 14.406893
shape of L is 
torch.Size([])
memory (bytes)
4888252416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
4888338432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  87355800000.0
relative error loss 14.40683
shape of L is 
torch.Size([])
memory (bytes)
4890271744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  8% |
memory (bytes)
4890271744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  87354000000.0
relative error loss 14.406532
shape of L is 
torch.Size([])
memory (bytes)
4891578368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4891578368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  87347230000.0
relative error loss 14.4054165
shape of L is 
torch.Size([])
memory (bytes)
4893728768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  8% |
memory (bytes)
4893745152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  87306930000.0
relative error loss 14.398769
shape of L is 
torch.Size([])
memory (bytes)
4895776768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4895776768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  87085300000.0
relative error loss 14.362219
shape of L is 
torch.Size([])
memory (bytes)
4897980416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4897980416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  85874260000.0
relative error loss 14.162493
shape of L is 
torch.Size([])
memory (bytes)
4899868672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4899868672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  79468770000.0
relative error loss 13.1060915
shape of L is 
torch.Size([])
memory (bytes)
4902264832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4902264832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  51248660000.0
relative error loss 8.451994
shape of L is 
torch.Size([])
memory (bytes)
4904361984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4904361984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  17343076000.0
relative error loss 2.8602421
time to take a step is 220.04940152168274
it  1 : 2030999552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4906491904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4906491904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  17343076000.0
relative error loss 2.8602421
shape of L is 
torch.Size([])
memory (bytes)
4908679168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4908679168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  9866623000.0
relative error loss 1.627216
shape of L is 
torch.Size([])
memory (bytes)
4910665728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4910829568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  7134669000.0
relative error loss 1.1766586
shape of L is 
torch.Size([])
memory (bytes)
4912820224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4912820224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  6268166700.0
relative error loss 1.033754
shape of L is 
torch.Size([])
memory (bytes)
4915105792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  9% |
memory (bytes)
4915105792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  10052463000.0
relative error loss 1.6578649
shape of L is 
torch.Size([])
memory (bytes)
4917223424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4917223424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  5810977000.0
relative error loss 0.9583537
shape of L is 
torch.Size([])
memory (bytes)
4919332864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4919332864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  5415582000.0
relative error loss 0.89314467
shape of L is 
torch.Size([])
memory (bytes)
4921507840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4921511936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  5016521700.0
relative error loss 0.8273311
shape of L is 
torch.Size([])
memory (bytes)
4923617280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4923617280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  4624506400.0
relative error loss 0.76267946
shape of L is 
torch.Size([])
memory (bytes)
4925558784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  9% |
memory (bytes)
4925558784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  4523904500.0
relative error loss 0.7460881
shape of L is 
torch.Size([])
memory (bytes)
4927488000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
4927746048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  4364220400.0
relative error loss 0.7197528
time to take a step is 232.6997094154358
it  2 : 2249105920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4929966080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4929966080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  4364220400.0
relative error loss 0.7197528
shape of L is 
torch.Size([])
memory (bytes)
4931944448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4931944448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  4200990700.0
relative error loss 0.6928327
shape of L is 
torch.Size([])
memory (bytes)
4934123520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4934123520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  3884172000.0
relative error loss 0.64058256
shape of L is 
torch.Size([])
memory (bytes)
4936310784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4936310784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  3737466400.0
relative error loss 0.6163877
shape of L is 
torch.Size([])
memory (bytes)
4938342400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
4938342400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  9% |
error is  5995953700.0
relative error loss 0.9888603
shape of L is 
torch.Size([])
memory (bytes)
4940521472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4940521472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  9% |
error is  3254928600.0
relative error loss 0.53680694
shape of L is 
torch.Size([])
memory (bytes)
4942598144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4942598144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  9% |
error is  3017623000.0
relative error loss 0.49767023
shape of L is 
torch.Size([])
memory (bytes)
4944830464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4944830464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  2658269400.0
relative error loss 0.4384052
shape of L is 
torch.Size([])
memory (bytes)
4946894848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4946894848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  2620232000.0
relative error loss 0.43213198
shape of L is 
torch.Size([])
memory (bytes)
4949024768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4949024768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  2490114800.0
relative error loss 0.4106729
time to take a step is 211.56077790260315
it  3 : 2250088448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4951269376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4951269376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  2490114800.0
relative error loss 0.4106729
shape of L is 
torch.Size([])
memory (bytes)
4953137152
| ID | GPU | MEM |
------------------
|  0 |  3% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4953407488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  2284566300.0
relative error loss 0.37677357
shape of L is 
torch.Size([])
memory (bytes)
4955537408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4955537408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  2158359000.0
relative error loss 0.35595933
shape of L is 
torch.Size([])
memory (bytes)
4957683712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4957683712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1927158500.0
relative error loss 0.31782943
shape of L is 
torch.Size([])
memory (bytes)
4959772672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
4959772672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1747457500.0
relative error loss 0.28819293
shape of L is 
torch.Size([])
memory (bytes)
4961931264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4961931264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1630071300.0
relative error loss 0.26883343
shape of L is 
torch.Size([])
memory (bytes)
4964110336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4964110336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  9% |
error is  1598540800.0
relative error loss 0.26363337
shape of L is 
torch.Size([])
memory (bytes)
4966260736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
4966260736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1505656800.0
relative error loss 0.24831484
shape of L is 
torch.Size([])
memory (bytes)
4968300544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4968300544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1371317200.0
relative error loss 0.22615938
shape of L is 
torch.Size([])
memory (bytes)
4970364928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4970364928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  9% |
error is  1277229000.0
relative error loss 0.21064223
time to take a step is 213.2287244796753
c= tensor(2187.0647, device='cuda:0')
c= tensor(194168.8438, device='cuda:0')
c= tensor(205812.3750, device='cuda:0')
c= tensor(211584.9844, device='cuda:0')
c= tensor(3338689.5000, device='cuda:0')
c= tensor(3971000.5000, device='cuda:0')
c= tensor(4620325., device='cuda:0')
c= tensor(5251159., device='cuda:0')
c= tensor(5523305., device='cuda:0')
c= tensor(19578388., device='cuda:0')
c= tensor(19604854., device='cuda:0')
c= tensor(27041268., device='cuda:0')
c= tensor(27072814., device='cuda:0')
c= tensor(33520264., device='cuda:0')
c= tensor(33700224., device='cuda:0')
c= tensor(35179596., device='cuda:0')
c= tensor(35584788., device='cuda:0')
c= tensor(36052892., device='cuda:0')
c= tensor(82019296., device='cuda:0')
c= tensor(85041832., device='cuda:0')
c= tensor(85234456., device='cuda:0')
c= tensor(1.4666e+08, device='cuda:0')
c= tensor(1.4681e+08, device='cuda:0')
c= tensor(1.4687e+08, device='cuda:0')
c= tensor(1.4899e+08, device='cuda:0')
c= tensor(1.4995e+08, device='cuda:0')
c= tensor(1.5122e+08, device='cuda:0')
c= tensor(1.5128e+08, device='cuda:0')
c= tensor(1.5339e+08, device='cuda:0')
c= tensor(8.6912e+08, device='cuda:0')
c= tensor(8.6913e+08, device='cuda:0')
c= tensor(9.0819e+08, device='cuda:0')
c= tensor(9.0825e+08, device='cuda:0')
c= tensor(9.0827e+08, device='cuda:0')
c= tensor(9.0835e+08, device='cuda:0')
c= tensor(9.1021e+08, device='cuda:0')
c= tensor(9.1192e+08, device='cuda:0')
c= tensor(9.1192e+08, device='cuda:0')
c= tensor(9.1193e+08, device='cuda:0')
c= tensor(9.1194e+08, device='cuda:0')
c= tensor(9.1197e+08, device='cuda:0')
c= tensor(9.1197e+08, device='cuda:0')
c= tensor(9.1197e+08, device='cuda:0')
c= tensor(9.1198e+08, device='cuda:0')
c= tensor(9.1198e+08, device='cuda:0')
c= tensor(9.1199e+08, device='cuda:0')
c= tensor(9.1200e+08, device='cuda:0')
c= tensor(9.1202e+08, device='cuda:0')
c= tensor(9.1202e+08, device='cuda:0')
c= tensor(9.1204e+08, device='cuda:0')
c= tensor(9.1211e+08, device='cuda:0')
c= tensor(9.1211e+08, device='cuda:0')
c= tensor(9.1213e+08, device='cuda:0')
c= tensor(9.1213e+08, device='cuda:0')
c= tensor(9.1215e+08, device='cuda:0')
c= tensor(9.1231e+08, device='cuda:0')
c= tensor(9.1235e+08, device='cuda:0')
c= tensor(9.1235e+08, device='cuda:0')
c= tensor(9.1235e+08, device='cuda:0')
c= tensor(9.1236e+08, device='cuda:0')
c= tensor(9.1237e+08, device='cuda:0')
c= tensor(9.1237e+08, device='cuda:0')
c= tensor(9.1239e+08, device='cuda:0')
c= tensor(9.1241e+08, device='cuda:0')
c= tensor(9.1242e+08, device='cuda:0')
c= tensor(9.1242e+08, device='cuda:0')
c= tensor(9.1243e+08, device='cuda:0')
c= tensor(9.1245e+08, device='cuda:0')
c= tensor(9.1246e+08, device='cuda:0')
c= tensor(9.1246e+08, device='cuda:0')
c= tensor(9.1249e+08, device='cuda:0')
c= tensor(9.1249e+08, device='cuda:0')
c= tensor(9.1250e+08, device='cuda:0')
c= tensor(9.1251e+08, device='cuda:0')
c= tensor(9.1251e+08, device='cuda:0')
c= tensor(9.1252e+08, device='cuda:0')
c= tensor(9.1252e+08, device='cuda:0')
c= tensor(9.1252e+08, device='cuda:0')
c= tensor(9.1254e+08, device='cuda:0')
c= tensor(9.1273e+08, device='cuda:0')
c= tensor(9.1274e+08, device='cuda:0')
c= tensor(9.1274e+08, device='cuda:0')
c= tensor(9.1275e+08, device='cuda:0')
c= tensor(9.1275e+08, device='cuda:0')
c= tensor(9.1276e+08, device='cuda:0')
c= tensor(9.1277e+08, device='cuda:0')
c= tensor(9.1277e+08, device='cuda:0')
c= tensor(9.1277e+08, device='cuda:0')
c= tensor(9.1278e+08, device='cuda:0')
c= tensor(9.1279e+08, device='cuda:0')
c= tensor(9.1280e+08, device='cuda:0')
c= tensor(9.1280e+08, device='cuda:0')
c= tensor(9.1280e+08, device='cuda:0')
c= tensor(9.1281e+08, device='cuda:0')
c= tensor(9.1282e+08, device='cuda:0')
c= tensor(9.1283e+08, device='cuda:0')
c= tensor(9.1284e+08, device='cuda:0')
c= tensor(9.1288e+08, device='cuda:0')
c= tensor(9.1288e+08, device='cuda:0')
c= tensor(9.1289e+08, device='cuda:0')
c= tensor(9.1291e+08, device='cuda:0')
c= tensor(9.1291e+08, device='cuda:0')
c= tensor(9.1292e+08, device='cuda:0')
c= tensor(9.1292e+08, device='cuda:0')
c= tensor(9.1293e+08, device='cuda:0')
c= tensor(9.1293e+08, device='cuda:0')
c= tensor(9.1295e+08, device='cuda:0')
c= tensor(9.1295e+08, device='cuda:0')
c= tensor(9.1295e+08, device='cuda:0')
c= tensor(9.1295e+08, device='cuda:0')
c= tensor(9.1296e+08, device='cuda:0')
c= tensor(9.1296e+08, device='cuda:0')
c= tensor(9.1296e+08, device='cuda:0')
c= tensor(9.1297e+08, device='cuda:0')
c= tensor(9.1299e+08, device='cuda:0')
c= tensor(9.1299e+08, device='cuda:0')
c= tensor(9.1301e+08, device='cuda:0')
c= tensor(9.1301e+08, device='cuda:0')
c= tensor(9.1303e+08, device='cuda:0')
c= tensor(9.1304e+08, device='cuda:0')
c= tensor(9.1312e+08, device='cuda:0')
c= tensor(9.1312e+08, device='cuda:0')
c= tensor(9.1312e+08, device='cuda:0')
c= tensor(9.1313e+08, device='cuda:0')
c= tensor(9.1313e+08, device='cuda:0')
c= tensor(9.1314e+08, device='cuda:0')
c= tensor(9.1314e+08, device='cuda:0')
c= tensor(9.1314e+08, device='cuda:0')
c= tensor(9.1319e+08, device='cuda:0')
c= tensor(9.1320e+08, device='cuda:0')
c= tensor(9.1324e+08, device='cuda:0')
c= tensor(9.1324e+08, device='cuda:0')
c= tensor(9.1325e+08, device='cuda:0')
c= tensor(9.1325e+08, device='cuda:0')
c= tensor(9.1325e+08, device='cuda:0')
c= tensor(9.1325e+08, device='cuda:0')
c= tensor(9.1326e+08, device='cuda:0')
c= tensor(9.1326e+08, device='cuda:0')
c= tensor(9.1326e+08, device='cuda:0')
c= tensor(9.1327e+08, device='cuda:0')
c= tensor(9.1327e+08, device='cuda:0')
c= tensor(9.1327e+08, device='cuda:0')
c= tensor(9.1330e+08, device='cuda:0')
c= tensor(9.1333e+08, device='cuda:0')
c= tensor(9.1333e+08, device='cuda:0')
c= tensor(9.1334e+08, device='cuda:0')
c= tensor(9.1334e+08, device='cuda:0')
c= tensor(9.1335e+08, device='cuda:0')
c= tensor(9.1335e+08, device='cuda:0')
c= tensor(9.1335e+08, device='cuda:0')
c= tensor(9.1336e+08, device='cuda:0')
c= tensor(9.1337e+08, device='cuda:0')
c= tensor(9.1337e+08, device='cuda:0')
c= tensor(9.1341e+08, device='cuda:0')
c= tensor(9.1341e+08, device='cuda:0')
c= tensor(9.1344e+08, device='cuda:0')
c= tensor(9.1344e+08, device='cuda:0')
c= tensor(9.1345e+08, device='cuda:0')
c= tensor(9.1346e+08, device='cuda:0')
c= tensor(9.1346e+08, device='cuda:0')
c= tensor(9.1348e+08, device='cuda:0')
c= tensor(9.1349e+08, device='cuda:0')
c= tensor(9.1349e+08, device='cuda:0')
c= tensor(9.1349e+08, device='cuda:0')
c= tensor(9.1350e+08, device='cuda:0')
c= tensor(9.1350e+08, device='cuda:0')
c= tensor(9.1351e+08, device='cuda:0')
c= tensor(9.1352e+08, device='cuda:0')
c= tensor(9.1353e+08, device='cuda:0')
c= tensor(9.1353e+08, device='cuda:0')
c= tensor(9.1353e+08, device='cuda:0')
c= tensor(9.1354e+08, device='cuda:0')
c= tensor(9.1356e+08, device='cuda:0')
c= tensor(9.1356e+08, device='cuda:0')
c= tensor(9.1358e+08, device='cuda:0')
c= tensor(9.1358e+08, device='cuda:0')
c= tensor(9.1359e+08, device='cuda:0')
c= tensor(9.1360e+08, device='cuda:0')
c= tensor(9.1361e+08, device='cuda:0')
c= tensor(9.1362e+08, device='cuda:0')
c= tensor(9.1363e+08, device='cuda:0')
c= tensor(9.1363e+08, device='cuda:0')
c= tensor(9.1364e+08, device='cuda:0')
c= tensor(9.1364e+08, device='cuda:0')
c= tensor(9.1365e+08, device='cuda:0')
c= tensor(9.1367e+08, device='cuda:0')
c= tensor(9.1367e+08, device='cuda:0')
c= tensor(9.1367e+08, device='cuda:0')
c= tensor(9.1370e+08, device='cuda:0')
c= tensor(9.1374e+08, device='cuda:0')
c= tensor(9.1375e+08, device='cuda:0')
c= tensor(9.1375e+08, device='cuda:0')
c= tensor(9.1375e+08, device='cuda:0')
c= tensor(9.1376e+08, device='cuda:0')
c= tensor(9.1377e+08, device='cuda:0')
c= tensor(9.1377e+08, device='cuda:0')
c= tensor(9.1378e+08, device='cuda:0')
c= tensor(9.1378e+08, device='cuda:0')
c= tensor(9.1378e+08, device='cuda:0')
c= tensor(9.1379e+08, device='cuda:0')
c= tensor(9.1381e+08, device='cuda:0')
c= tensor(9.1381e+08, device='cuda:0')
c= tensor(9.1383e+08, device='cuda:0')
c= tensor(9.1385e+08, device='cuda:0')
c= tensor(9.1386e+08, device='cuda:0')
c= tensor(9.1386e+08, device='cuda:0')
c= tensor(9.1388e+08, device='cuda:0')
c= tensor(9.1388e+08, device='cuda:0')
c= tensor(9.1389e+08, device='cuda:0')
c= tensor(9.1391e+08, device='cuda:0')
c= tensor(9.1392e+08, device='cuda:0')
c= tensor(9.1393e+08, device='cuda:0')
c= tensor(9.1393e+08, device='cuda:0')
c= tensor(9.1394e+08, device='cuda:0')
c= tensor(9.1394e+08, device='cuda:0')
c= tensor(9.1395e+08, device='cuda:0')
c= tensor(9.1395e+08, device='cuda:0')
c= tensor(9.1395e+08, device='cuda:0')
c= tensor(9.1396e+08, device='cuda:0')
c= tensor(9.1397e+08, device='cuda:0')
c= tensor(9.1399e+08, device='cuda:0')
c= tensor(9.1399e+08, device='cuda:0')
c= tensor(9.1401e+08, device='cuda:0')
c= tensor(9.1402e+08, device='cuda:0')
c= tensor(9.1402e+08, device='cuda:0')
c= tensor(9.1402e+08, device='cuda:0')
c= tensor(9.1403e+08, device='cuda:0')
c= tensor(9.1403e+08, device='cuda:0')
c= tensor(9.1404e+08, device='cuda:0')
c= tensor(9.1404e+08, device='cuda:0')
c= tensor(9.1404e+08, device='cuda:0')
c= tensor(9.1405e+08, device='cuda:0')
c= tensor(9.1405e+08, device='cuda:0')
c= tensor(9.1407e+08, device='cuda:0')
c= tensor(9.1407e+08, device='cuda:0')
c= tensor(9.1407e+08, device='cuda:0')
c= tensor(9.1408e+08, device='cuda:0')
c= tensor(9.1411e+08, device='cuda:0')
c= tensor(9.1412e+08, device='cuda:0')
c= tensor(9.1416e+08, device='cuda:0')
c= tensor(9.1545e+08, device='cuda:0')
c= tensor(9.1627e+08, device='cuda:0')
c= tensor(9.1631e+08, device='cuda:0')
c= tensor(9.1631e+08, device='cuda:0')
c= tensor(9.1633e+08, device='cuda:0')
c= tensor(9.2109e+08, device='cuda:0')
c= tensor(9.4627e+08, device='cuda:0')
c= tensor(9.4628e+08, device='cuda:0')
c= tensor(9.4974e+08, device='cuda:0')
c= tensor(9.5029e+08, device='cuda:0')
c= tensor(9.5097e+08, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0783e+09, device='cuda:0')
c= tensor(1.2058e+09, device='cuda:0')
c= tensor(1.2058e+09, device='cuda:0')
c= tensor(1.2064e+09, device='cuda:0')
c= tensor(1.2208e+09, device='cuda:0')
c= tensor(1.2357e+09, device='cuda:0')
c= tensor(1.2367e+09, device='cuda:0')
c= tensor(1.2463e+09, device='cuda:0')
c= tensor(1.2474e+09, device='cuda:0')
c= tensor(1.2474e+09, device='cuda:0')
c= tensor(1.2475e+09, device='cuda:0')
c= tensor(1.2979e+09, device='cuda:0')
c= tensor(1.2980e+09, device='cuda:0')
c= tensor(1.2980e+09, device='cuda:0')
c= tensor(1.2999e+09, device='cuda:0')
c= tensor(1.3006e+09, device='cuda:0')
c= tensor(1.3149e+09, device='cuda:0')
c= tensor(1.3197e+09, device='cuda:0')
c= tensor(1.3197e+09, device='cuda:0')
c= tensor(1.3198e+09, device='cuda:0')
c= tensor(1.3199e+09, device='cuda:0')
c= tensor(1.3204e+09, device='cuda:0')
c= tensor(1.3227e+09, device='cuda:0')
c= tensor(1.3268e+09, device='cuda:0')
c= tensor(1.3284e+09, device='cuda:0')
c= tensor(1.3284e+09, device='cuda:0')
c= tensor(1.3285e+09, device='cuda:0')
c= tensor(1.3293e+09, device='cuda:0')
c= tensor(1.3365e+09, device='cuda:0')
c= tensor(1.3377e+09, device='cuda:0')
c= tensor(1.3383e+09, device='cuda:0')
c= tensor(1.3642e+09, device='cuda:0')
c= tensor(1.3643e+09, device='cuda:0')
c= tensor(1.3645e+09, device='cuda:0')
c= tensor(1.3658e+09, device='cuda:0')
c= tensor(1.3659e+09, device='cuda:0')
c= tensor(1.3693e+09, device='cuda:0')
c= tensor(1.3800e+09, device='cuda:0')
c= tensor(1.4151e+09, device='cuda:0')
c= tensor(1.4152e+09, device='cuda:0')
c= tensor(1.4154e+09, device='cuda:0')
c= tensor(1.4156e+09, device='cuda:0')
c= tensor(1.4156e+09, device='cuda:0')
c= tensor(1.4160e+09, device='cuda:0')
c= tensor(1.4160e+09, device='cuda:0')
c= tensor(1.4163e+09, device='cuda:0')
c= tensor(1.4383e+09, device='cuda:0')
c= tensor(1.4428e+09, device='cuda:0')
c= tensor(1.4429e+09, device='cuda:0')
c= tensor(1.4429e+09, device='cuda:0')
c= tensor(1.4457e+09, device='cuda:0')
c= tensor(1.4460e+09, device='cuda:0')
c= tensor(1.4461e+09, device='cuda:0')
c= tensor(1.4461e+09, device='cuda:0')
c= tensor(1.4886e+09, device='cuda:0')
c= tensor(1.4886e+09, device='cuda:0')
c= tensor(1.6063e+09, device='cuda:0')
c= tensor(1.6064e+09, device='cuda:0')
c= tensor(1.6076e+09, device='cuda:0')
c= tensor(1.6085e+09, device='cuda:0')
c= tensor(1.6457e+09, device='cuda:0')
c= tensor(1.6469e+09, device='cuda:0')
c= tensor(1.6469e+09, device='cuda:0')
c= tensor(1.6504e+09, device='cuda:0')
c= tensor(1.6537e+09, device='cuda:0')
c= tensor(1.6538e+09, device='cuda:0')
c= tensor(1.6567e+09, device='cuda:0')
c= tensor(1.6610e+09, device='cuda:0')
c= tensor(1.6739e+09, device='cuda:0')
c= tensor(1.6757e+09, device='cuda:0')
c= tensor(1.6758e+09, device='cuda:0')
c= tensor(1.6758e+09, device='cuda:0')
c= tensor(1.6897e+09, device='cuda:0')
c= tensor(1.7356e+09, device='cuda:0')
c= tensor(1.7357e+09, device='cuda:0')
c= tensor(1.7357e+09, device='cuda:0')
c= tensor(1.7398e+09, device='cuda:0')
c= tensor(1.7459e+09, device='cuda:0')
c= tensor(1.7525e+09, device='cuda:0')
c= tensor(1.7525e+09, device='cuda:0')
c= tensor(1.7528e+09, device='cuda:0')
c= tensor(1.7529e+09, device='cuda:0')
c= tensor(1.7529e+09, device='cuda:0')
c= tensor(1.7529e+09, device='cuda:0')
c= tensor(1.7530e+09, device='cuda:0')
c= tensor(1.7571e+09, device='cuda:0')
c= tensor(1.7575e+09, device='cuda:0')
c= tensor(1.7577e+09, device='cuda:0')
c= tensor(1.7578e+09, device='cuda:0')
c= tensor(1.7578e+09, device='cuda:0')
c= tensor(1.8745e+09, device='cuda:0')
c= tensor(1.8745e+09, device='cuda:0')
c= tensor(1.8796e+09, device='cuda:0')
c= tensor(1.8796e+09, device='cuda:0')
c= tensor(1.8796e+09, device='cuda:0')
c= tensor(1.8796e+09, device='cuda:0')
c= tensor(1.8805e+09, device='cuda:0')
c= tensor(1.8805e+09, device='cuda:0')
c= tensor(1.8809e+09, device='cuda:0')
c= tensor(1.8809e+09, device='cuda:0')
c= tensor(1.8809e+09, device='cuda:0')
c= tensor(1.9043e+09, device='cuda:0')
c= tensor(1.9064e+09, device='cuda:0')
c= tensor(1.9067e+09, device='cuda:0')
c= tensor(1.9101e+09, device='cuda:0')
c= tensor(1.9192e+09, device='cuda:0')
c= tensor(1.9192e+09, device='cuda:0')
c= tensor(1.9192e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9195e+09, device='cuda:0')
c= tensor(1.9195e+09, device='cuda:0')
c= tensor(1.9195e+09, device='cuda:0')
c= tensor(1.9196e+09, device='cuda:0')
c= tensor(1.9196e+09, device='cuda:0')
c= tensor(1.9838e+09, device='cuda:0')
c= tensor(1.9840e+09, device='cuda:0')
c= tensor(1.9851e+09, device='cuda:0')
c= tensor(1.9851e+09, device='cuda:0')
c= tensor(1.9868e+09, device='cuda:0')
c= tensor(1.9878e+09, device='cuda:0')
c= tensor(2.5357e+09, device='cuda:0')
c= tensor(2.6067e+09, device='cuda:0')
c= tensor(2.6075e+09, device='cuda:0')
c= tensor(2.6090e+09, device='cuda:0')
c= tensor(2.6090e+09, device='cuda:0')
c= tensor(2.6106e+09, device='cuda:0')
c= tensor(2.6134e+09, device='cuda:0')
c= tensor(2.6172e+09, device='cuda:0')
c= tensor(2.6172e+09, device='cuda:0')
c= tensor(2.6205e+09, device='cuda:0')
c= tensor(2.6352e+09, device='cuda:0')
c= tensor(2.6360e+09, device='cuda:0')
c= tensor(2.6361e+09, device='cuda:0')
c= tensor(2.6367e+09, device='cuda:0')
c= tensor(2.6367e+09, device='cuda:0')
c= tensor(2.6367e+09, device='cuda:0')
c= tensor(2.6416e+09, device='cuda:0')
c= tensor(2.6426e+09, device='cuda:0')
c= tensor(2.6426e+09, device='cuda:0')
c= tensor(2.6463e+09, device='cuda:0')
c= tensor(2.6464e+09, device='cuda:0')
c= tensor(2.6464e+09, device='cuda:0')
c= tensor(2.6497e+09, device='cuda:0')
c= tensor(2.6525e+09, device='cuda:0')
c= tensor(2.6539e+09, device='cuda:0')
c= tensor(2.6622e+09, device='cuda:0')
c= tensor(2.6689e+09, device='cuda:0')
c= tensor(2.6689e+09, device='cuda:0')
c= tensor(2.6708e+09, device='cuda:0')
c= tensor(2.6741e+09, device='cuda:0')
c= tensor(2.6793e+09, device='cuda:0')
c= tensor(2.6793e+09, device='cuda:0')
c= tensor(2.7285e+09, device='cuda:0')
c= tensor(2.7484e+09, device='cuda:0')
c= tensor(2.7502e+09, device='cuda:0')
c= tensor(2.7535e+09, device='cuda:0')
c= tensor(2.7542e+09, device='cuda:0')
c= tensor(2.7544e+09, device='cuda:0')
c= tensor(2.7544e+09, device='cuda:0')
c= tensor(2.7545e+09, device='cuda:0')
c= tensor(2.7595e+09, device='cuda:0')
c= tensor(2.7636e+09, device='cuda:0')
c= tensor(2.7863e+09, device='cuda:0')
c= tensor(2.8018e+09, device='cuda:0')
c= tensor(2.8032e+09, device='cuda:0')
c= tensor(2.8034e+09, device='cuda:0')
c= tensor(2.8053e+09, device='cuda:0')
c= tensor(2.8053e+09, device='cuda:0')
c= tensor(2.8054e+09, device='cuda:0')
c= tensor(2.8142e+09, device='cuda:0')
c= tensor(2.8147e+09, device='cuda:0')
c= tensor(2.8147e+09, device='cuda:0')
c= tensor(2.8148e+09, device='cuda:0')
c= tensor(2.8229e+09, device='cuda:0')
c= tensor(2.8236e+09, device='cuda:0')
c= tensor(2.8256e+09, device='cuda:0')
c= tensor(2.8259e+09, device='cuda:0')
c= tensor(2.8263e+09, device='cuda:0')
c= tensor(2.8264e+09, device='cuda:0')
c= tensor(2.8264e+09, device='cuda:0')
c= tensor(2.8272e+09, device='cuda:0')
c= tensor(2.8275e+09, device='cuda:0')
c= tensor(2.8275e+09, device='cuda:0')
c= tensor(2.8371e+09, device='cuda:0')
c= tensor(2.8371e+09, device='cuda:0')
c= tensor(2.8372e+09, device='cuda:0')
c= tensor(2.8373e+09, device='cuda:0')
c= tensor(2.8394e+09, device='cuda:0')
c= tensor(2.8394e+09, device='cuda:0')
c= tensor(2.8404e+09, device='cuda:0')
c= tensor(2.8406e+09, device='cuda:0')
c= tensor(2.8408e+09, device='cuda:0')
c= tensor(2.8416e+09, device='cuda:0')
c= tensor(2.9332e+09, device='cuda:0')
c= tensor(2.9332e+09, device='cuda:0')
c= tensor(2.9333e+09, device='cuda:0')
c= tensor(2.9387e+09, device='cuda:0')
c= tensor(2.9388e+09, device='cuda:0')
c= tensor(2.9810e+09, device='cuda:0')
c= tensor(2.9810e+09, device='cuda:0')
c= tensor(2.9841e+09, device='cuda:0')
c= tensor(3.0020e+09, device='cuda:0')
c= tensor(3.0021e+09, device='cuda:0')
c= tensor(3.0431e+09, device='cuda:0')
c= tensor(3.0433e+09, device='cuda:0')
c= tensor(3.1408e+09, device='cuda:0')
c= tensor(3.1409e+09, device='cuda:0')
c= tensor(3.1539e+09, device='cuda:0')
c= tensor(3.1539e+09, device='cuda:0')
c= tensor(3.1539e+09, device='cuda:0')
c= tensor(3.1539e+09, device='cuda:0')
c= tensor(3.1555e+09, device='cuda:0')
c= tensor(3.1564e+09, device='cuda:0')
c= tensor(3.1742e+09, device='cuda:0')
c= tensor(3.1743e+09, device='cuda:0')
c= tensor(3.1743e+09, device='cuda:0')
c= tensor(3.1744e+09, device='cuda:0')
c= tensor(3.1816e+09, device='cuda:0')
c= tensor(3.1835e+09, device='cuda:0')
c= tensor(3.1947e+09, device='cuda:0')
c= tensor(3.1958e+09, device='cuda:0')
c= tensor(3.1958e+09, device='cuda:0')
c= tensor(3.1958e+09, device='cuda:0')
c= tensor(3.1960e+09, device='cuda:0')
c= tensor(3.6201e+09, device='cuda:0')
c= tensor(3.6201e+09, device='cuda:0')
c= tensor(3.6202e+09, device='cuda:0')
c= tensor(3.6264e+09, device='cuda:0')
c= tensor(3.6287e+09, device='cuda:0')
c= tensor(3.6287e+09, device='cuda:0')
c= tensor(3.6287e+09, device='cuda:0')
c= tensor(3.6311e+09, device='cuda:0')
c= tensor(3.6318e+09, device='cuda:0')
c= tensor(3.6319e+09, device='cuda:0')
c= tensor(3.6328e+09, device='cuda:0')
c= tensor(3.6464e+09, device='cuda:0')
c= tensor(3.6493e+09, device='cuda:0')
c= tensor(3.6760e+09, device='cuda:0')
c= tensor(3.6783e+09, device='cuda:0')
c= tensor(3.6784e+09, device='cuda:0')
c= tensor(3.6814e+09, device='cuda:0')
c= tensor(3.6820e+09, device='cuda:0')
c= tensor(3.6821e+09, device='cuda:0')
c= tensor(3.6822e+09, device='cuda:0')
c= tensor(3.6823e+09, device='cuda:0')
c= tensor(3.6857e+09, device='cuda:0')
c= tensor(3.6859e+09, device='cuda:0')
c= tensor(3.6859e+09, device='cuda:0')
c= tensor(3.6861e+09, device='cuda:0')
c= tensor(3.6868e+09, device='cuda:0')
c= tensor(3.6871e+09, device='cuda:0')
c= tensor(3.6904e+09, device='cuda:0')
c= tensor(3.6905e+09, device='cuda:0')
c= tensor(3.6906e+09, device='cuda:0')
c= tensor(3.6909e+09, device='cuda:0')
c= tensor(3.6909e+09, device='cuda:0')
c= tensor(3.6910e+09, device='cuda:0')
c= tensor(3.6911e+09, device='cuda:0')
c= tensor(3.6986e+09, device='cuda:0')
c= tensor(3.6986e+09, device='cuda:0')
c= tensor(3.6986e+09, device='cuda:0')
c= tensor(3.6986e+09, device='cuda:0')
c= tensor(3.7042e+09, device='cuda:0')
c= tensor(3.7496e+09, device='cuda:0')
c= tensor(3.7513e+09, device='cuda:0')
c= tensor(3.7513e+09, device='cuda:0')
c= tensor(3.7556e+09, device='cuda:0')
c= tensor(3.7628e+09, device='cuda:0')
c= tensor(3.7628e+09, device='cuda:0')
c= tensor(3.7629e+09, device='cuda:0')
c= tensor(3.7632e+09, device='cuda:0')
c= tensor(3.7638e+09, device='cuda:0')
c= tensor(3.7691e+09, device='cuda:0')
c= tensor(3.7705e+09, device='cuda:0')
c= tensor(3.7706e+09, device='cuda:0')
c= tensor(3.7707e+09, device='cuda:0')
c= tensor(3.7707e+09, device='cuda:0')
c= tensor(3.7713e+09, device='cuda:0')
c= tensor(3.7722e+09, device='cuda:0')
c= tensor(3.7745e+09, device='cuda:0')
c= tensor(3.7754e+09, device='cuda:0')
c= tensor(3.7920e+09, device='cuda:0')
c= tensor(3.7921e+09, device='cuda:0')
c= tensor(3.7921e+09, device='cuda:0')
c= tensor(3.7922e+09, device='cuda:0')
c= tensor(3.7935e+09, device='cuda:0')
c= tensor(3.7939e+09, device='cuda:0')
c= tensor(3.7939e+09, device='cuda:0')
c= tensor(3.7939e+09, device='cuda:0')
c= tensor(3.7945e+09, device='cuda:0')
c= tensor(3.7945e+09, device='cuda:0')
c= tensor(3.7949e+09, device='cuda:0')
c= tensor(3.7949e+09, device='cuda:0')
c= tensor(3.7950e+09, device='cuda:0')
c= tensor(3.7951e+09, device='cuda:0')
c= tensor(3.7952e+09, device='cuda:0')
c= tensor(3.7952e+09, device='cuda:0')
c= tensor(3.7983e+09, device='cuda:0')
c= tensor(3.8119e+09, device='cuda:0')
c= tensor(3.8221e+09, device='cuda:0')
c= tensor(3.8346e+09, device='cuda:0')
c= tensor(3.8349e+09, device='cuda:0')
c= tensor(3.8349e+09, device='cuda:0')
c= tensor(3.8351e+09, device='cuda:0')
c= tensor(3.8383e+09, device='cuda:0')
c= tensor(3.8384e+09, device='cuda:0')
c= tensor(3.8388e+09, device='cuda:0')
c= tensor(3.8389e+09, device='cuda:0')
c= tensor(3.8831e+09, device='cuda:0')
c= tensor(3.8843e+09, device='cuda:0')
c= tensor(3.8846e+09, device='cuda:0')
c= tensor(3.9172e+09, device='cuda:0')
c= tensor(3.9176e+09, device='cuda:0')
c= tensor(3.9183e+09, device='cuda:0')
c= tensor(3.9419e+09, device='cuda:0')
c= tensor(3.9467e+09, device='cuda:0')
c= tensor(3.9516e+09, device='cuda:0')
c= tensor(3.9518e+09, device='cuda:0')
c= tensor(3.9554e+09, device='cuda:0')
c= tensor(3.9554e+09, device='cuda:0')
c= tensor(3.9566e+09, device='cuda:0')
c= tensor(4.0324e+09, device='cuda:0')
c= tensor(4.0339e+09, device='cuda:0')
c= tensor(4.0356e+09, device='cuda:0')
c= tensor(4.0357e+09, device='cuda:0')
c= tensor(4.0385e+09, device='cuda:0')
c= tensor(4.0391e+09, device='cuda:0')
c= tensor(4.0392e+09, device='cuda:0')
c= tensor(4.0397e+09, device='cuda:0')
c= tensor(4.0575e+09, device='cuda:0')
c= tensor(4.0577e+09, device='cuda:0')
c= tensor(4.1384e+09, device='cuda:0')
c= tensor(4.1390e+09, device='cuda:0')
c= tensor(4.1395e+09, device='cuda:0')
c= tensor(4.1398e+09, device='cuda:0')
c= tensor(4.1410e+09, device='cuda:0')
c= tensor(4.1499e+09, device='cuda:0')
c= tensor(4.1499e+09, device='cuda:0')
c= tensor(4.2161e+09, device='cuda:0')
c= tensor(4.2176e+09, device='cuda:0')
c= tensor(4.2183e+09, device='cuda:0')
c= tensor(4.2183e+09, device='cuda:0')
c= tensor(4.2184e+09, device='cuda:0')
c= tensor(4.2184e+09, device='cuda:0')
c= tensor(4.2184e+09, device='cuda:0')
c= tensor(4.2185e+09, device='cuda:0')
c= tensor(4.2244e+09, device='cuda:0')
c= tensor(5.2679e+09, device='cuda:0')
c= tensor(5.2824e+09, device='cuda:0')
c= tensor(5.2859e+09, device='cuda:0')
c= tensor(5.2859e+09, device='cuda:0')
c= tensor(5.2863e+09, device='cuda:0')
c= tensor(5.2866e+09, device='cuda:0')
c= tensor(5.2904e+09, device='cuda:0')
c= tensor(5.2904e+09, device='cuda:0')
c= tensor(5.4449e+09, device='cuda:0')
c= tensor(5.4449e+09, device='cuda:0')
c= tensor(5.4510e+09, device='cuda:0')
c= tensor(5.4512e+09, device='cuda:0')
c= tensor(5.4545e+09, device='cuda:0')
c= tensor(5.4816e+09, device='cuda:0')
c= tensor(5.4818e+09, device='cuda:0')
c= tensor(5.4818e+09, device='cuda:0')
c= tensor(5.4828e+09, device='cuda:0')
c= tensor(5.4828e+09, device='cuda:0')
c= tensor(5.4829e+09, device='cuda:0')
c= tensor(5.4909e+09, device='cuda:0')
c= tensor(5.6770e+09, device='cuda:0')
c= tensor(5.6774e+09, device='cuda:0')
c= tensor(5.6780e+09, device='cuda:0')
c= tensor(5.6940e+09, device='cuda:0')
c= tensor(5.7152e+09, device='cuda:0')
c= tensor(5.7158e+09, device='cuda:0')
c= tensor(5.7158e+09, device='cuda:0')
c= tensor(5.7327e+09, device='cuda:0')
c= tensor(5.7348e+09, device='cuda:0')
c= tensor(5.7365e+09, device='cuda:0')
c= tensor(5.7374e+09, device='cuda:0')
c= tensor(5.7388e+09, device='cuda:0')
c= tensor(5.7399e+09, device='cuda:0')
c= tensor(5.8280e+09, device='cuda:0')
c= tensor(5.8296e+09, device='cuda:0')
c= tensor(5.8297e+09, device='cuda:0')
c= tensor(5.8297e+09, device='cuda:0')
c= tensor(5.8301e+09, device='cuda:0')
c= tensor(5.8320e+09, device='cuda:0')
c= tensor(5.8369e+09, device='cuda:0')
c= tensor(5.8486e+09, device='cuda:0')
c= tensor(5.8486e+09, device='cuda:0')
c= tensor(5.8488e+09, device='cuda:0')
c= tensor(5.8499e+09, device='cuda:0')
c= tensor(5.8512e+09, device='cuda:0')
c= tensor(5.8519e+09, device='cuda:0')
c= tensor(5.8533e+09, device='cuda:0')
c= tensor(5.8970e+09, device='cuda:0')
c= tensor(5.8974e+09, device='cuda:0')
c= tensor(5.8974e+09, device='cuda:0')
c= tensor(5.8974e+09, device='cuda:0')
c= tensor(5.9060e+09, device='cuda:0')
c= tensor(5.9068e+09, device='cuda:0')
c= tensor(5.9084e+09, device='cuda:0')
c= tensor(5.9084e+09, device='cuda:0')
c= tensor(5.9084e+09, device='cuda:0')
c= tensor(5.9166e+09, device='cuda:0')
c= tensor(5.9189e+09, device='cuda:0')
c= tensor(5.9196e+09, device='cuda:0')
c= tensor(5.9196e+09, device='cuda:0')
c= tensor(5.9196e+09, device='cuda:0')
c= tensor(5.9199e+09, device='cuda:0')
c= tensor(5.9201e+09, device='cuda:0')
c= tensor(5.9218e+09, device='cuda:0')
c= tensor(5.9219e+09, device='cuda:0')
c= tensor(5.9225e+09, device='cuda:0')
c= tensor(5.9229e+09, device='cuda:0')
c= tensor(5.9231e+09, device='cuda:0')
c= tensor(5.9461e+09, device='cuda:0')
c= tensor(5.9462e+09, device='cuda:0')
c= tensor(5.9467e+09, device='cuda:0')
c= tensor(5.9503e+09, device='cuda:0')
c= tensor(5.9550e+09, device='cuda:0')
c= tensor(5.9572e+09, device='cuda:0')
c= tensor(5.9623e+09, device='cuda:0')
c= tensor(5.9623e+09, device='cuda:0')
c= tensor(5.9643e+09, device='cuda:0')
c= tensor(5.9645e+09, device='cuda:0')
c= tensor(5.9895e+09, device='cuda:0')
c= tensor(5.9955e+09, device='cuda:0')
c= tensor(5.9961e+09, device='cuda:0')
c= tensor(5.9991e+09, device='cuda:0')
c= tensor(5.9992e+09, device='cuda:0')
c= tensor(6.0002e+09, device='cuda:0')
c= tensor(6.0002e+09, device='cuda:0')
c= tensor(6.0006e+09, device='cuda:0')
c= tensor(6.0036e+09, device='cuda:0')
c= tensor(6.0075e+09, device='cuda:0')
c= tensor(6.0076e+09, device='cuda:0')
c= tensor(6.0084e+09, device='cuda:0')
c= tensor(6.0084e+09, device='cuda:0')
c= tensor(6.0560e+09, device='cuda:0')
c= tensor(6.0563e+09, device='cuda:0')
c= tensor(6.0563e+09, device='cuda:0')
c= tensor(6.0592e+09, device='cuda:0')
c= tensor(6.0613e+09, device='cuda:0')
c= tensor(6.0614e+09, device='cuda:0')
c= tensor(6.0628e+09, device='cuda:0')
c= tensor(6.0635e+09, device='cuda:0')
time to make c is 10.716823816299438
time for making loss is 10.716858148574829
p0 True
it  0 : 1764492288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4972724224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
4972871680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1277229000.0
relative error loss 0.21064223
shape of L is 
torch.Size([])
memory (bytes)
4999761920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4999819264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1267438100.0
relative error loss 0.2090275
shape of L is 
torch.Size([])
memory (bytes)
5003157504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  8% |
memory (bytes)
5003157504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1225673200.0
relative error loss 0.20213959
shape of L is 
torch.Size([])
memory (bytes)
5006467072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
5006467072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1195256300.0
relative error loss 0.19712318
shape of L is 
torch.Size([])
memory (bytes)
5009731584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5009731584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1173786600.0
relative error loss 0.19358239
shape of L is 
torch.Size([])
memory (bytes)
5012938752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5012938752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1158357500.0
relative error loss 0.19103779
shape of L is 
torch.Size([])
memory (bytes)
5016166400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5016170496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  9% |
error is  1147399200.0
relative error loss 0.18923053
shape of L is 
torch.Size([])
memory (bytes)
5019385856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5019385856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1139853300.0
relative error loss 0.18798606
shape of L is 
torch.Size([])
memory (bytes)
5022601216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5022605312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1134031400.0
relative error loss 0.18702589
shape of L is 
torch.Size([])
memory (bytes)
5025669120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5025669120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1128932400.0
relative error loss 0.18618496
time to take a step is 277.0068151950836
it  1 : 2250704384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5029027840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5029036032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1128932400.0
relative error loss 0.18618496
shape of L is 
torch.Size([])
memory (bytes)
5032026112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  9% |
memory (bytes)
5032243200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1124359700.0
relative error loss 0.18543082
shape of L is 
torch.Size([])
memory (bytes)
5035470848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5035470848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1120831500.0
relative error loss 0.18484895
shape of L is 
torch.Size([])
memory (bytes)
5038608384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  9% |
memory (bytes)
5038698496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1118319600.0
relative error loss 0.1844347
shape of L is 
torch.Size([])
memory (bytes)
5041913856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5041913856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1115787300.0
relative error loss 0.18401705
shape of L is 
torch.Size([])
memory (bytes)
5045129216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  9% |
memory (bytes)
5045129216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1113137700.0
relative error loss 0.18358007
shape of L is 
torch.Size([])
memory (bytes)
5048344576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5048344576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1110828500.0
relative error loss 0.18319926
shape of L is 
torch.Size([])
memory (bytes)
5051564032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5051564032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1109204500.0
relative error loss 0.18293141
shape of L is 
torch.Size([])
memory (bytes)
5054734336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5054734336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  9% |
error is  1107724300.0
relative error loss 0.1826873
shape of L is 
torch.Size([])
memory (bytes)
5057998848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  9% |
memory (bytes)
5057998848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1105536000.0
relative error loss 0.1823264
time to take a step is 271.1373119354248
it  2 : 2250704384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5061111808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5061111808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1105536000.0
relative error loss 0.1823264
shape of L is 
torch.Size([])
memory (bytes)
5064417280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
5064421376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1104114200.0
relative error loss 0.1820919
shape of L is 
torch.Size([])
memory (bytes)
5067452416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5067640832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1102558700.0
relative error loss 0.18183538
shape of L is 
torch.Size([])
memory (bytes)
5070856192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5070860288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1100998100.0
relative error loss 0.18157801
shape of L is 
torch.Size([])
memory (bytes)
5074010112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5074010112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1100032000.0
relative error loss 0.18141867
shape of L is 
torch.Size([])
memory (bytes)
5077307392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5077307392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1098772500.0
relative error loss 0.18121095
shape of L is 
torch.Size([])
memory (bytes)
5080510464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5080510464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1097165300.0
relative error loss 0.1809459
shape of L is 
torch.Size([])
memory (bytes)
5083697152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  9% |
memory (bytes)
5083697152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1096289800.0
relative error loss 0.18080151
shape of L is 
torch.Size([])
memory (bytes)
5086949376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
5086949376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1095441900.0
relative error loss 0.18066168
shape of L is 
torch.Size([])
memory (bytes)
5090050048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5090050048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1094741000.0
relative error loss 0.18054608
time to take a step is 270.94200110435486
it  3 : 2251458560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5093384192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
5093384192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1094741000.0
relative error loss 0.18054608
shape of L is 
torch.Size([])
memory (bytes)
5096529920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  9% |
memory (bytes)
5096529920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1094144000.0
relative error loss 0.18044762
shape of L is 
torch.Size([])
memory (bytes)
5099814912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5099814912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  9% |
error is  1093401100.0
relative error loss 0.18032509
shape of L is 
torch.Size([])
memory (bytes)
5102899200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5102899200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1092552200.0
relative error loss 0.1801851
shape of L is 
torch.Size([])
memory (bytes)
5106245632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5106245632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1092037100.0
relative error loss 0.18010014
shape of L is 
torch.Size([])
memory (bytes)
5109338112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
5109493760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1091387900.0
relative error loss 0.17999308
shape of L is 
torch.Size([])
memory (bytes)
5112705024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
5112705024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1090814000.0
relative error loss 0.17989843
shape of L is 
torch.Size([])
memory (bytes)
5115813888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5115916288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1090396700.0
relative error loss 0.1798296
shape of L is 
torch.Size([])
memory (bytes)
5119131648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5119131648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1089589200.0
relative error loss 0.17969644
shape of L is 
torch.Size([])
memory (bytes)
5122277376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
5122277376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1089349100.0
relative error loss 0.17965683
time to take a step is 270.6253755092621
it  4 : 2248441856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5125562368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
5125562368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1089349100.0
relative error loss 0.17965683
shape of L is 
torch.Size([])
memory (bytes)
5128720384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5128720384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  9% |
error is  1088683000.0
relative error loss 0.17954698
shape of L is 
torch.Size([])
memory (bytes)
5131923456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
5132001280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1088336400.0
relative error loss 0.17948982
shape of L is 
torch.Size([])
memory (bytes)
5135208448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5135212544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1087948800.0
relative error loss 0.1794259
shape of L is 
torch.Size([])
memory (bytes)
5138329600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5138440192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1087661000.0
relative error loss 0.17937845
shape of L is 
torch.Size([])
memory (bytes)
5141655552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
5141655552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1087185900.0
relative error loss 0.17930008
shape of L is 
torch.Size([])
memory (bytes)
5144866816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
5144866816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1086841300.0
relative error loss 0.17924325
shape of L is 
torch.Size([])
memory (bytes)
5148086272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  9% |
memory (bytes)
5148086272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1086586400.0
relative error loss 0.1792012
shape of L is 
torch.Size([])
memory (bytes)
5151236096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  9% |
memory (bytes)
5151236096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1086054400.0
relative error loss 0.17911348
shape of L is 
torch.Size([])
memory (bytes)
5154516992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5154521088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1085679100.0
relative error loss 0.17905158
time to take a step is 271.9645688533783
it  5 : 2251458560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5157728256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5157728256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1085679100.0
relative error loss 0.17905158
shape of L is 
torch.Size([])
memory (bytes)
5160960000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5160960000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1085457900.0
relative error loss 0.1790151
shape of L is 
torch.Size([])
memory (bytes)
5164093440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5164101632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1085203000.0
relative error loss 0.17897305
shape of L is 
torch.Size([])
memory (bytes)
5167378432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5167382528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1084727800.0
relative error loss 0.17889468
shape of L is 
torch.Size([])
memory (bytes)
5170561024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5170561024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1084594700.0
relative error loss 0.17887273
shape of L is 
torch.Size([])
memory (bytes)
5173809152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  9% |
memory (bytes)
5173809152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1084063200.0
relative error loss 0.17878509
shape of L is 
torch.Size([])
memory (bytes)
5176946688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5176946688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1083898900.0
relative error loss 0.17875798
shape of L is 
torch.Size([])
memory (bytes)
5180235776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
5180235776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1083644900.0
relative error loss 0.1787161
shape of L is 
torch.Size([])
memory (bytes)
5183410176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5183410176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1083458600.0
relative error loss 0.17868537
shape of L is 
torch.Size([])
memory (bytes)
5186662400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  9% |
memory (bytes)
5186662400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1083160000.0
relative error loss 0.17863613
time to take a step is 273.84616255760193
it  6 : 2251458560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5189734400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5189869568
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 99% |  9% |
error is  1083160000.0
relative error loss 0.17863613
shape of L is 
torch.Size([])
memory (bytes)
5193080832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  9% |
memory (bytes)
5193084928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1082914800.0
relative error loss 0.17859569
shape of L is 
torch.Size([])
memory (bytes)
5196296192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5196296192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1082713600.0
relative error loss 0.1785625
shape of L is 
torch.Size([])
memory (bytes)
5199413248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5199413248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1082451500.0
relative error loss 0.17851926
shape of L is 
torch.Size([])
memory (bytes)
5202710528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5202714624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1082120200.0
relative error loss 0.17846464
shape of L is 
torch.Size([])
memory (bytes)
5205815296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5205938176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1081924600.0
relative error loss 0.17843238
shape of L is 
torch.Size([])
memory (bytes)
5209149440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
5209149440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1081705000.0
relative error loss 0.17839615
shape of L is 
torch.Size([])
memory (bytes)
5212237824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5212360704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1081557000.0
relative error loss 0.17837176
shape of L is 
torch.Size([])
memory (bytes)
5215580160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5215580160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1081239600.0
relative error loss 0.1783194
shape of L is 
torch.Size([])
memory (bytes)
5218746368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
5218746368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1081056800.0
relative error loss 0.17828925
time to take a step is 271.57125997543335
it  7 : 2249950208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5221842944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
5222014976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1081056800.0
relative error loss 0.17828925
shape of L is 
torch.Size([])
memory (bytes)
5225213952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5225213952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  9% |
error is  1080918500.0
relative error loss 0.17826645
shape of L is 
torch.Size([])
memory (bytes)
5228437504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5228437504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1080636400.0
relative error loss 0.17821993
shape of L is 
torch.Size([])
memory (bytes)
5231652864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5231656960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1080642000.0
relative error loss 0.17822085
shape of L is 
torch.Size([])
memory (bytes)
5234700288
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5234843648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1080461800.0
relative error loss 0.17819114
shape of L is 
torch.Size([])
memory (bytes)
5238087680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  9% |
memory (bytes)
5238087680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1080295400.0
relative error loss 0.17816369
shape of L is 
torch.Size([])
memory (bytes)
5241155584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
5241311232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1080130600.0
relative error loss 0.1781365
shape of L is 
torch.Size([])
memory (bytes)
5244514304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5244514304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1079948300.0
relative error loss 0.17810644
shape of L is 
torch.Size([])
memory (bytes)
5247721472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5247721472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1079778800.0
relative error loss 0.17807849
shape of L is 
torch.Size([])
memory (bytes)
5250940928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5250940928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1079676400.0
relative error loss 0.1780616
time to take a step is 271.41715455055237
it  8 : 2252212736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5254160384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5254164480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1079676400.0
relative error loss 0.1780616
shape of L is 
torch.Size([])
memory (bytes)
5257293824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5257367552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1079560700.0
relative error loss 0.17804252
shape of L is 
torch.Size([])
memory (bytes)
5260595200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5260595200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1079399400.0
relative error loss 0.17801592
shape of L is 
torch.Size([])
memory (bytes)
5263708160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5263708160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1079265800.0
relative error loss 0.17799388
shape of L is 
torch.Size([])
memory (bytes)
5267034112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  9% |
memory (bytes)
5267034112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1079197700.0
relative error loss 0.17798266
shape of L is 
torch.Size([])
memory (bytes)
5270110208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5270249472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1079107100.0
relative error loss 0.17796771
shape of L is 
torch.Size([])
memory (bytes)
5273473024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5273473024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1078955500.0
relative error loss 0.17794271
shape of L is 
torch.Size([])
memory (bytes)
5276680192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5276684288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1078971900.0
relative error loss 0.17794542
shape of L is 
torch.Size([])
memory (bytes)
5279756288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5279899648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  9% |
error is  1078842900.0
relative error loss 0.17792414
shape of L is 
torch.Size([])
memory (bytes)
5283106816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  9% |
memory (bytes)
5283106816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1078693400.0
relative error loss 0.17789948
time to take a step is 272.22305488586426
it  9 : 2251458560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5286309888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
5286309888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1078693400.0
relative error loss 0.17789948
shape of L is 
torch.Size([])
memory (bytes)
5289529344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
5289529344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1078569000.0
relative error loss 0.17787896
shape of L is 
torch.Size([])
memory (bytes)
5292736512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5292736512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  9% |
error is  1078434800.0
relative error loss 0.17785683
shape of L is 
torch.Size([])
memory (bytes)
5295951872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5295951872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1078305800.0
relative error loss 0.17783555
shape of L is 
torch.Size([])
memory (bytes)
5299032064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5299167232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1078214700.0
relative error loss 0.17782053
shape of L is 
torch.Size([])
memory (bytes)
5302382592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
5302382592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1078096900.0
relative error loss 0.1778011
shape of L is 
torch.Size([])
memory (bytes)
5305589760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  9% |
memory (bytes)
5305593856
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% |  9% |
error is  1077998100.0
relative error loss 0.17778482
shape of L is 
torch.Size([])
memory (bytes)
5308743680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5308817408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1077921300.0
relative error loss 0.17777215
shape of L is 
torch.Size([])
memory (bytes)
5312036864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5312036864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1077839900.0
relative error loss 0.17775872
shape of L is 
torch.Size([])
memory (bytes)
5315244032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
5315248128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1077780000.0
relative error loss 0.17774884
time to take a step is 272.3111414909363
it  10 : 2249196032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5318463488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5318463488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1077780000.0
relative error loss 0.17774884
shape of L is 
torch.Size([])
memory (bytes)
5321666560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5321666560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1077699600.0
relative error loss 0.17773558
shape of L is 
torch.Size([])
memory (bytes)
5324894208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
5324894208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1077627900.0
relative error loss 0.17772377
shape of L is 
torch.Size([])
memory (bytes)
5328011264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
5328109568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1077535200.0
relative error loss 0.17770848
shape of L is 
torch.Size([])
memory (bytes)
5331324928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
5331324928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1077472300.0
relative error loss 0.17769809
shape of L is 
torch.Size([])
memory (bytes)
5334536192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5334536192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  9% |
error is  1077348900.0
relative error loss 0.17767774
shape of L is 
torch.Size([])
memory (bytes)
5337743360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
5337743360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1077339100.0
relative error loss 0.17767614
shape of L is 
torch.Size([])
memory (bytes)
5340975104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
5340979200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1077286400.0
relative error loss 0.17766744
shape of L is 
torch.Size([])
memory (bytes)
5344112640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  9% |
memory (bytes)
5344182272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1077175800.0
relative error loss 0.1776492
shape of L is 
torch.Size([])
memory (bytes)
5347397632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
5347397632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1077024800.0
relative error loss 0.1776243
time to take a step is 272.9032292366028
it  11 : 2252966912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5350539264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5350539264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1077024800.0
relative error loss 0.1776243
shape of L is 
torch.Size([])
memory (bytes)
5353820160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5353820160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1076910600.0
relative error loss 0.17760547
shape of L is 
torch.Size([])
memory (bytes)
5357035520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5357035520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1076790800.0
relative error loss 0.1775857
shape of L is 
torch.Size([])
memory (bytes)
5360250880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
5360250880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1076674600.0
relative error loss 0.17756654
shape of L is 
torch.Size([])
memory (bytes)
5363458048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5363462144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1076560900.0
relative error loss 0.1775478
shape of L is 
torch.Size([])
memory (bytes)
5366673408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
5366673408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1076471800.0
relative error loss 0.1775331
shape of L is 
torch.Size([])
memory (bytes)
5369892864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5369896960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1076389900.0
relative error loss 0.17751959
shape of L is 
torch.Size([])
memory (bytes)
5372964864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5373112320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1076335600.0
relative error loss 0.17751063
shape of L is 
torch.Size([])
memory (bytes)
5376335872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5376335872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1076280300.0
relative error loss 0.17750151
shape of L is 
torch.Size([])
memory (bytes)
5379403776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5379547136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1076207100.0
relative error loss 0.17748944
time to take a step is 273.2681016921997
it  12 : 2251458560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5382774784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5382774784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1076207100.0
relative error loss 0.17748944
shape of L is 
torch.Size([])
memory (bytes)
5385904128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  9% |
memory (bytes)
5385977856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1076149800.0
relative error loss 0.17747998
shape of L is 
torch.Size([])
memory (bytes)
5389201408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5389201408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1076124700.0
relative error loss 0.17747585
shape of L is 
torch.Size([])
memory (bytes)
5392257024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5392400384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1076081200.0
relative error loss 0.17746867
shape of L is 
torch.Size([])
memory (bytes)
5395623936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5395623936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1076034600.0
relative error loss 0.17746098
shape of L is 
torch.Size([])
memory (bytes)
5398675456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5398831104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1075966000.0
relative error loss 0.17744967
shape of L is 
torch.Size([])
memory (bytes)
5402058752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5402058752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1075845600.0
relative error loss 0.17742983
shape of L is 
torch.Size([])
memory (bytes)
5405270016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5405270016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1075799600.0
relative error loss 0.17742223
shape of L is 
torch.Size([])
memory (bytes)
5408477184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
5408481280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1075733000.0
relative error loss 0.17741126
shape of L is 
torch.Size([])
memory (bytes)
5411700736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5411700736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1075641300.0
relative error loss 0.17739613
time to take a step is 274.1004054546356
it  13 : 2249950208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5414928384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
5414928384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1075641300.0
relative error loss 0.17739613
shape of L is 
torch.Size([])
memory (bytes)
5418131456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  9% |
memory (bytes)
5418131456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1075593700.0
relative error loss 0.17738828
shape of L is 
torch.Size([])
memory (bytes)
5421277184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
5421277184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1075503100.0
relative error loss 0.17737333
shape of L is 
torch.Size([])
memory (bytes)
5424562176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5424562176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1075434000.0
relative error loss 0.17736194
shape of L is 
torch.Size([])
memory (bytes)
5427724288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5427769344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1075354600.0
relative error loss 0.17734885
shape of L is 
torch.Size([])
memory (bytes)
5430988800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5430988800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1075291100.0
relative error loss 0.17733838
shape of L is 
torch.Size([])
memory (bytes)
5434208256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5434208256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1075223000.0
relative error loss 0.17732716
shape of L is 
torch.Size([])
memory (bytes)
5437423616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
5437423616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1075162100.0
relative error loss 0.1773171
shape of L is 
torch.Size([])
memory (bytes)
5440573440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5440573440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1075124700.0
relative error loss 0.17731094
shape of L is 
torch.Size([])
memory (bytes)
5443870720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5443870720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  9% |
error is  1075084300.0
relative error loss 0.17730427
time to take a step is 272.9899847507477
it  14 : 2250704384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
5446959104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
5447086080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1075084300.0
relative error loss 0.17730427
shape of L is 
torch.Size([])
memory (bytes)
5450285056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5450285056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1075026400.0
relative error loss 0.17729473
shape of L is 
torch.Size([])
memory (bytes)
5453500416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
5453500416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1074987500.0
relative error loss 0.17728831
shape of L is 
torch.Size([])
memory (bytes)
5456715776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5456715776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  9% |
error is  1074938400.0
relative error loss 0.1772802
shape of L is 
torch.Size([])
memory (bytes)
5459927040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5459927040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1074885100.0
relative error loss 0.17727143
shape of L is 
torch.Size([])
memory (bytes)
5463044096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5463044096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1074842100.0
relative error loss 0.17726433
shape of L is 
torch.Size([])
memory (bytes)
5466353664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5466353664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1074812400.0
relative error loss 0.17725943
shape of L is 
torch.Size([])
memory (bytes)
5469474816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
5469474816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1074803700.0
relative error loss 0.177258
shape of L is 
torch.Size([])
memory (bytes)
5472784384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5472784384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1074759700.0
relative error loss 0.17725073
shape of L is 
torch.Size([])
memory (bytes)
5475917824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
5475917824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1074738200.0
relative error loss 0.17724718
time to take a step is 273.858544588089
sum tnnu_Z after tensor(12797592., device='cuda:0')
shape of features
(4767,)
shape of features
(4767,)
number of orig particles 19068
number of new particles after remove low mass 15864
tnuZ shape should be parts x labs
torch.Size([19068, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1277143900.0
relative error without small mass is  0.2106282
nnu_Z shape should be number of particles by maxV
(19068, 702)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
shape of features
(19068,)
Thu Feb 2 21:06:52 EST 2023
