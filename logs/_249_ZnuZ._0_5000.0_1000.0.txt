Thu Feb 2 18:23:28 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 45321284
numbers of Z: 26532
shape of features
(26532,)
shape of features
(26532,)
ZX	Vol	Parts	Cubes	Eps
Z	0.020008286489785466	26532	26.532	0.09102205497476076
X	0.017935504407411384	1473	1.473	0.23005779311114807
X	0.019339405428929916	28200	28.2	0.08818566682606244
X	0.01895577571008976	3718	3.718	0.17211183118681397
X	0.018707930886628407	4787	4.787	0.1575145861356963
X	0.018505667897558958	25169	25.169	0.09025673127071139
X	0.01884206907964863	39929	39.929	0.07785384259094436
X	0.018921385820975824	65586	65.586	0.06607653571640264
X	0.019413414153316867	53057	53.057	0.07152421113461749
X	0.018476248487687197	8567	8.567	0.12919975075747503
X	0.018289509342388564	33691	33.691	0.08157605848714417
X	0.018599469162556864	12304	12.304	0.11476727649383527
X	0.01963765874145928	118774	118.774	0.05488530838003448
X	0.018687419774775883	6807	6.807	0.14002251165481922
X	0.01804060360544095	272794	272.794	0.04043946642833723
X	0.01788544456862962	21832	21.832	0.09356973632519726
X	0.019079602248631155	60401	60.401	0.06810438298208085
X	0.0190806147994814	56064	56.064	0.06981832539473093
X	0.018329572489678175	43316	43.316	0.07507603537107448
X	0.019319573855189964	244267	244.267	0.042925060376205214
X	0.018896701065503403	132547	132.547	0.05224026404916307
X	0.01843588491690331	20381	20.381	0.09671180743037837
X	0.019638908118201598	382309	382.309	0.03717357009563395
X	0.018441255432897784	16067	16.067	0.10470125721474748
X	0.019163614707474602	27396	27.396	0.0887694285949937
X	0.019003018134665296	19809	19.809	0.09862492295187124
X	0.01831538198057219	87850	87.85	0.05929590997284326
X	0.01937173327736161	66802	66.802	0.06619009512264838
X	0.017949845715541153	10665	10.665	0.11895059536435731
X	0.0184323030183213	75958	75.958	0.06237377463381629
X	0.01994620468320062	1692156	1692.156	0.02275830732198188
X	0.018356261148313804	8458	8.458	0.12947090456678967
X	0.019956057593765272	642640	642.64	0.031431781604542526
X	0.019286959376497114	15684	15.684	0.10713605338285831
X	0.01842957606762514	8992	8.992	0.12702416508822956
X	0.01800285677514616	9840	9.84	0.122306285315626
X	0.018773756261494183	159774	159.774	0.04897957612464276
X	0.019314571884560677	111422	111.422	0.05575768219845146
X	0.01788640431381963	1805	1.805	0.21479060392185123
X	0.01841105908991586	5447	5.447	0.15007457847536915
X	0.01858905961524335	3373	3.373	0.1766364629698908
X	0.018838463198451547	6072	6.072	0.1458493739909453
X	0.017136563207968688	1636	1.636	0.21879974387655146
X	0.017620945704848717	1337	1.337	0.23621053789347415
X	0.017681568410280766	3390	3.39	0.173423133553626
X	0.018044319756544865	1537	1.537	0.22727703182544837
X	0.01764346273862543	2341	2.341	0.19606231983777106
X	0.018424225100254793	10129	10.129	0.12206960894659453
X	0.018303894091728844	5049	5.049	0.15361939174199638
X	0.017417768980126574	1889	1.889	0.20969441133323052
X	0.018404055890520712	14002	14.002	0.10954044226861596
X	0.01933137258361995	15153	15.153	0.10845627627847328
X	0.018453479211130528	2601	2.601	0.19215246903465127
X	0.017915483613324748	6273	6.273	0.14187887936092933
X	0.01768070081218306	3115	3.115	0.17838040326606708
X	0.018601521509466864	7152	7.152	0.1375224669396413
X	0.01810408673488299	6529	6.529	0.14048932234402522
X	0.01854752114703457	3198	3.198	0.17966734018234212
X	0.0176662114993486	4318	4.318	0.1599387125694696
X	0.01793205238712123	4325	4.325	0.16065020930651777
X	0.01806980632897416	2523	2.523	0.19275785393227773
X	0.018767404434805103	7332	7.332	0.13679196418522305
X	0.018758377761065197	2691	2.691	0.19102660615557931
X	0.019885548556232764	11690	11.69	0.11937343423963663
X	0.01793856257252642	6994	6.994	0.13688441489968017
X	0.017578108068419144	3443	3.443	0.1721914973003338
X	0.01794581064402057	3450	3.45	0.17326652364620665
X	0.01796834047510423	4902	4.902	0.15418598058099472
X	0.018602590628306975	5815	5.815	0.1473469791867461
X	0.018288521120868012	5773	5.773	0.14686749140693034
X	0.0174275168981438	2073	2.073	0.20333496725987643
X	0.01782684738080055	3240	3.24	0.17654002296141194
X	0.017432352984234782	3046	3.046	0.17887186592931223
X	0.017571269696494224	3833	3.833	0.1661198161546268
X	0.017198440062229765	1971	1.971	0.20587367210712326
X	0.01820367258214566	3998	3.998	0.16574435347113903
X	0.018555854161926073	5929	5.929	0.14627378522657195
X	0.01894614689388043	2657	2.657	0.19247593969155405
X	0.01786207100152277	1035	1.035	0.2584225973599625
X	0.017601807568250367	3408	3.408	0.17285657127153625
X	0.019103030030265443	11906	11.906	0.11706995733021339
X	0.017922050074868502	4947	4.947	0.15358492572307766
X	0.017630570280364747	1576	1.576	0.22365059043192537
X	0.019647532775951614	8089	8.089	0.13442221957298767
X	0.01752588732408721	1825	1.825	0.21255551716511276
X	0.018403341321911956	2826	2.826	0.18674178588069346
X	0.018132512444982686	1888	1.888	0.2125618549821733
X	0.018422484638758015	4879	4.879	0.1557181115810996
X	0.0175006653158286	2068	2.068	0.2037830266943953
X	0.0177954467444035	4444	4.444	0.15879797073142665
X	0.017766536726816596	4227	4.227	0.16138265030675272
X	0.01770477292791458	4445	4.445	0.1585159125724856
X	0.01787395357665698	2273	2.273	0.1988567771775638
X	0.017985180257514756	1767	1.767	0.2167168843872027
X	0.018801595951243713	4555	4.555	0.16041147960848262
X	0.018401966015279023	4725	4.725	0.15733335503640503
X	0.018689125625670665	4646	4.646	0.15903886192767921
X	0.019077720554370122	4050	4.05	0.16763208750934616
X	0.01798216809469633	8357	8.357	0.12910128062878828
X	0.018640306606325836	4315	4.315	0.16286362005275476
X	0.0186450156751425	3320	3.32	0.1777494325833186
X	0.01842102903205548	15774	15.774	0.1053070265442792
X	0.017720866580680682	5539	5.539	0.14735035852484224
X	0.018814554544854072	7853	7.853	0.13380924316624024
X	0.017955983834586856	1992	1.992	0.2081165528797999
X	0.01834528291264729	4035	4.035	0.16566354793433563
X	0.018412781146286902	2467	2.467	0.1954264819190203
X	0.01872323295236346	8758	8.758	0.12882257225251567
X	0.01767769290120974	921	0.921	0.26774537951285204
X	0.01886826315605581	3739	3.739	0.17152423321511645
X	0.017542730790700543	1968	1.968	0.20734363192341898
X	0.017355416526569186	2024	2.024	0.20467973419812918
X	0.017667014639171898	1535	1.535	0.2257797197824972
X	0.017564969078569175	2050	2.05	0.20462774083763563
X	0.019077183698682866	5104	5.104	0.15519147659361854
X	0.018845641800090112	4876	4.876	0.15693353050000686
X	0.01754391607634802	2205	2.205	0.19963620972121615
X	0.01798754381401568	2720	2.72	0.18770146124349388
X	0.017735056333421373	1543	1.543	0.22567782213236387
X	0.01923427689261009	8418	8.418	0.1317110809820752
X	0.01734981479910575	2258	2.258	0.19732872141266686
X	0.01875356286286482	13020	13.02	0.11293388492262434
X	0.01798408557250915	3507	3.507	0.1724451025249245
X	0.017640925768082072	2432	2.432	0.1935764854632986
X	0.018719205403379072	3475	3.475	0.1752982310515903
X	0.01785802638750306	2057	2.057	0.2055258199711663
X	0.017251028573065284	2578	2.578	0.18844185055744583
X	0.017605931809120928	2482	2.482	0.19214057207252652
X	0.017836000506477465	2656	2.656	0.18866433179945383
X	0.018578073277508104	13702	13.702	0.11068078638054984
X	0.017465452716049157	3022	3.022	0.17945757537766405
X	0.01791206174750762	7342	7.342	0.13462025199554195
X	0.017890849211697177	1851	1.851	0.21301401530384728
X	0.018198048134305096	5081	5.081	0.15300015525117155
X	0.017679983882752926	1625	1.625	0.22158626293679395
X	0.017889469081308024	3548	3.548	0.17147650119015645
X	0.01788270101514599	2669	2.669	0.18852177229733308
X	0.018824881593744527	1568	1.568	0.2289788649775227
X	0.017449040795266102	2328	2.328	0.19570241908052063
X	0.017775101154357476	2218	2.218	0.20011678314161516
X	0.018030506870784346	3584	3.584	0.171348370317516
X	0.017814946957365498	4377	4.377	0.15966240336632176
X	0.01743019163801523	1824	1.824	0.2122067085944791
X	0.01914804042106475	10727	10.727	0.12130592625658723
X	0.018834797934131228	13663	13.663	0.11129396943365676
X	0.01882215958490328	6275	6.275	0.14421770285896884
X	0.017672636905180953	3125	3.125	0.17816283346453277
X	0.017477478945897608	2098	2.098	0.20271743446182372
X	0.018794425084766215	3592	3.592	0.17360580346752114
X	0.017434488823225874	2204	2.204	0.19925040343914602
X	0.018244549160429623	2896	2.896	0.18469062862749241
X	0.017328170161249774	2273	2.273	0.1968117801095602
X	0.01882271125548897	5128	5.128	0.15425694326910094
X	0.018050994625563005	4823	4.823	0.15526061126576815
X	0.018564837194772445	14100	14.1	0.10960339601438782
X	0.018409654019717874	3296	3.296	0.17742690405782474
X	0.01752166633481762	13354	13.354	0.10947661096882928
X	0.017895987666994375	4158	4.158	0.1626636059601236
X	0.018009127748677786	3471	3.471	0.173119512992479
X	0.018105448793785787	5216	5.216	0.1514109335756711
X	0.018001783432889353	875	0.875	0.2740117332981618
X	0.01994303031773571	8371	8.371	0.13355841891937079
X	0.01728947201656017	2600	2.6	0.18804832137862543
X	0.018197936934633088	4477	4.477	0.15959220937338983
X	0.018008140081802505	1650	1.65	0.22181707391487523
X	0.017894321673590452	2957	2.957	0.18223060003464103
X	0.01769154098826186	1780	1.78	0.21500499697009587
X	0.018019336300442085	4545	4.545	0.15827110496729377
X	0.01799969975940157	4095	4.095	0.16380875829942967
X	0.017901590836008056	4209	4.209	0.1620208477657382
X	0.017550505356162465	863	0.863	0.27295618747359124
X	0.017326207785112267	1562	1.562	0.22301846875050363
X	0.01773992744240889	1706	1.706	0.2182684364691281
X	0.018211253003522467	7099	7.099	0.13689288740548092
X	0.017642954228804963	3478	3.478	0.17182271479160913
X	0.018383011837022886	5981	5.981	0.14539438054474527
X	0.01859297966107881	3823	3.823	0.16942658820183995
X	0.01796616721307363	5504	5.504	0.14834024913067065
X	0.018326051107531115	3766	3.766	0.16945830545120624
X	0.017937366974598894	6463	6.463	0.1405318766402035
X	0.017399075789584547	2841	2.841	0.18295810303282178
X	0.018694626588774216	3588	3.588	0.17336235057283542
X	0.017713410318771022	3028	3.028	0.18018367370099048
X	0.017748526520953538	3818	3.818	0.16689453913407937
X	0.01722104720219549	2343	2.343	0.19442961797145172
X	0.01875293015552765	8561	8.561	0.1298718059417532
X	0.019482062365132936	7650	7.65	0.13656031127420742
X	0.017393934009416613	3173	3.173	0.17632311458862368
X	0.018494258850732647	4916	4.916	0.15552791594407822
X	0.018625734899854765	6145	6.145	0.144720651862928
X	0.019831868712970352	17245	17.245	0.1047691643659387
X	0.01727084481192919	2127	2.127	0.20099350823492818
X	0.017769477133610902	787	0.787	0.2826397924483447
X	0.01796566311185879	5721	5.721	0.14643911692678868
X	0.0179735252666313	2532	2.532	0.19218663748622772
X	0.018042019290757366	3369	3.369	0.1749556789427248
X	0.018339185174624496	3905	3.905	0.16746330799958653
X	0.0179442592720028	2709	2.709	0.18780429069087487
X	0.017793794985982728	1874	1.874	0.21175467081767457
X	0.017645716020097988	3085	3.085	0.17883864571061117
X	0.017431891825857502	1719	1.719	0.21644931472257212
X	0.01860462702226859	4967	4.967	0.15530148290443185
X	0.018697247748753445	1806	1.806	0.21794821287064106
X	0.018924629433219622	8359	8.359	0.1313079495647903
X	0.01799361204341208	5536	5.536	0.14812922717319163
X	0.019412303849556035	3977	3.977	0.16963175807153374
X	0.017347271515397018	1284	1.284	0.2381709107754688
X	0.01949006605611246	5668	5.668	0.15093657462212562
X	0.01784456433590878	3808	3.808	0.16734124787500892
X	0.018061068104915333	5262	5.262	0.15084498013709352
X	0.01862555533742735	7813	7.813	0.13358687855205514
X	0.017846017653597473	4543	4.543	0.15778517426338154
X	0.017789063561292267	3864	3.864	0.16635607166536096
X	0.017718636013212805	3331	3.331	0.17456287446181837
X	0.017678295262846994	2724	2.724	0.18652816879943163
X	0.01790955317299323	1264	1.264	0.24197986249055586
X	0.017437774004187407	1680	1.68	0.21813595441360067
X	0.017830419251943876	1598	1.598	0.22345749537835263
X	0.017561569570680877	957	0.957	0.2637649777882661
X	0.017898769136876377	4219	4.219	0.16188423116565462
X	0.01752764647647814	2885	2.885	0.18247047218279522
X	0.01728163226979729	2721	2.721	0.18519051494706562
X	0.017934577926465793	2482	2.482	0.1933287580742472
X	0.01760993914904919	4366	4.366	0.15918105349538084
X	0.0174247500532402	3394	3.394	0.17251157480107002
X	0.01867549106098167	4327	4.327	0.16281523671937573
X	0.018791066394879066	4133	4.133	0.16566420240900478
X	0.017591098637069685	1977	1.977	0.207218614127796
X	0.017542210744854167	1736	1.736	0.21619461274492707
X	0.01798224996370349	4974	4.974	0.1534779830141872
X	0.017403693485162963	886	0.886	0.2698169536833906
X	0.017997680602024783	1816	1.816	0.21479964801694845
X	0.018116905072803578	3052	3.052	0.18106443751541165
X	0.018193649206338418	4395	4.395	0.1605660212115675
X	0.01933513316336569	6237	6.237	0.14581106245513453
X	0.018679444794462235	2038	2.038	0.2092762419782972
X	0.017864644342216837	2276	2.276	0.1987348534619076
X	0.0180017520383451	3488	3.488	0.17281420309807816
X	0.01882807764562501	8410	8.41	0.13081874997094267
X	0.017657297053620753	2307	2.307	0.19707228082225944
X	0.018149514283072942	11457	11.457	0.11657301560182379
X	0.019638935054170532	62718	62.718	0.0679060670146475
X	0.018505990629863075	14786	14.786	0.10776735206975217
X	0.01817915789471594	5417	5.417	0.14971718126973707
X	0.017152763441174654	887	0.887	0.268412967036778
X	0.017939628170670765	6811	6.811	0.13810228118238377
X	0.019899875947999395	103235	103.235	0.057766289045452945
X	0.019881018280602146	272592	272.592	0.041780648138873985
X	0.018194930142919598	3880	3.88	0.16738102471553531
X	0.019181268571357237	96004	96.004	0.058460518917380626
X	0.018460806987753185	32457	32.457	0.0828541186745128
X	0.019111170839245302	22354	22.354	0.09490971793914117
X	0.019906418444589843	167150	167.15	0.04919967674450999
X	0.017936812623180594	2164	2.164	0.20237770733104388
X	0.017920042410486694	5659	5.659	0.14684746250632982
X	0.018802812619282138	244818	244.818	0.04250693825042847
X	0.018842088663456796	424072	424.072	0.035418471482975764
X	0.018042715489044736	5373	5.373	0.1497482261986816
X	0.019413153880900806	32780	32.78	0.08397739190963226
X	0.01992594710596517	76034	76.034	0.06399367404733507
X	0.019276005575095696	57108	57.108	0.069626307553479
X	0.018017085568363064	54960	54.96	0.06895185255664056
X	0.019174145205296516	51321	51.321	0.07202340244491022
X	0.018860140318150864	42702	42.702	0.07615508393253068
X	0.01883876562586978	15603	15.603	0.10648328603946518
X	0.01742029545077055	3298	3.298	0.17415460198070423
X	0.019978684403710447	507853	507.853	0.034010234110626585
X	0.018156774457218625	4003	4.003	0.16553291752177
X	0.01836440512466224	2630	2.63	0.19113506656258963
X	0.018848318200152633	52296	52.296	0.07116526065056136
X	0.01858830570756423	35674	35.674	0.08046913497743635
X	0.019759895393155546	331519	331.519	0.039062383472754904
X	0.01906299527702527	122332	122.332	0.05381256434241818
X	0.017984213278539735	1576	1.576	0.22513606809473888
X	0.018352941774906085	22393	22.393	0.09358318077548548
X	0.01802631768275666	9976	9.976	0.12180081473607199
X	0.018038635681187486	38874	38.874	0.07741910606650743
X	0.01844621562326258	78286	78.286	0.061764802798135926
X	0.019663699311577933	13316	13.316	0.11387554785024931
X	0.017934953077325	42937	42.937	0.07475200558903086
X	0.017517735746154384	837	0.837	0.27558199361564695
X	0.01845872846844485	6602	6.602	0.14087764630490374
X	0.017676273974579465	68435	68.435	0.06368504855199214
X	0.01929968557024019	71277	71.277	0.06469443739330136
X	0.01871230378746824	39154	39.154	0.07818384792057813
X	0.017986299852148533	13334	13.334	0.11049105647970979
X	0.01922340575044939	284453	284.453	0.04073239419703533
X	0.018337615094032342	15660	15.66	0.1054023672740131
X	0.018836382847466058	26540	26.54	0.08920013878886156
X	0.018760445797271826	72326	72.326	0.0637748680945775
X	0.017929222418744707	5379	5.379	0.14937799876718513
X	0.01794995271661167	97549	97.549	0.05687832901101066
X	0.01995474736014376	259065	259.065	0.042548001266139
X	0.019928096960125218	270245	270.245	0.04193429892485531
X	0.01876987016597645	16789	16.789	0.1037876024942562
X	0.017820664376352	15319	15.319	0.10517144113016505
X	0.01791353837887036	17306	17.306	0.10115675589667159
X	0.017531514149971644	5371	5.371	0.14833879885154103
X	0.018866231716348398	25868	25.868	0.09001345617865579
X	0.01859438747879617	11711	11.711	0.1166619680575755
X	0.018030973979824647	19719	19.719	0.09706100114013279
X	0.018984724172041	147658	147.658	0.05047183601162703
X	0.018568119913960592	59753	59.753	0.06773337860514658
X	0.01797917715622423	11161	11.161	0.1172255379845675
X	0.019119774060329588	9728	9.728	0.12526211240553017
X	0.01924325879868094	82462	82.462	0.06156609180723151
X	0.018112011905547527	16932	16.932	0.10227107282480115
X	0.019311255072793243	25318	25.318	0.09136791228172822
X	0.018986835990129704	13580	13.58	0.11181951639645082
X	0.019918755400464564	373849	373.849	0.037628940628605875
X	0.018269105319187337	10377	10.377	0.12074852126404093
X	0.018853850183820714	67791	67.791	0.06527436723280902
X	0.017981395657153596	6155	6.155	0.1429547026944443
X	0.01908394790855228	47780	47.78	0.07364456349031102
X	0.018313901673384137	25589	25.589	0.08944907515726114
X	0.01994592531788686	346087	346.087	0.03862688033047592
X	0.018070379251197692	57648	57.648	0.06792990504806043
X	0.017976731346106674	6340	6.34	0.14153825058646208
X	0.018690720254940767	130521	130.521	0.052317688567421014
X	0.01966619123092184	129546	129.546	0.05334561791398243
X	0.018284857770375605	6618	6.618	0.1403206564969765
X	0.01880990660177719	91786	91.786	0.05895718397497116
X	0.018824170550593256	123402	123.402	0.05343156361474201
X	0.01995771515044644	315635	315.635	0.03983901158225296
X	0.01930181947333476	54780	54.78	0.0706304663611654
X	0.017275164300059404	2218	2.218	0.1982227734189056
X	0.017932025832185443	17814	17.814	0.1002203625035967
X	0.018512593608126206	56872	56.872	0.06878962185751675
X	0.01862185379365175	73401	73.401	0.06330533508917846
X	0.018788808816508424	20074	20.074	0.09781867500806568
X	0.018000853757158406	4026	4.026	0.16474280049024034
X	0.01960599093660448	66670	66.67	0.06649966346456052
X	0.019536462626100655	183062	183.062	0.04743321653979947
X	0.018283393630342128	70938	70.938	0.06363938242805348
X	0.017786215365641286	7341	7.341	0.1343103376420584
X	0.018338162596928737	19288	19.288	0.09833079277125334
X	0.01883823560931065	12412	12.412	0.11492102365434641
X	0.018840680960649486	5847	5.847	0.14770249580950132
X	0.018554696152077742	12892	12.892	0.11290448219619119
X	0.018027628454557215	7880	7.88	0.1317662320329441
X	0.018436206463250053	40518	40.518	0.0769144350315007
X	0.018701141790432665	49596	49.596	0.07224489383890226
X	0.01974700555909359	21775	21.775	0.09679383451936027
X	0.019803056741542154	27040	27.04	0.09013812395592306
X	0.018574858185987364	6981	6.981	0.13857002316649858
X	0.01946434321399829	450169	450.169	0.03509845958055895
X	0.017987944670376226	8311	8.311	0.12935287533079035
X	0.01941701181786685	115180	115.18	0.05524187874268434
X	0.01866967531141457	3953	3.953	0.1677786324795527
X	0.018789657617224145	6698	6.698	0.1410342438538564
X	0.01876776971684864	4176	4.176	0.16502537386491264
X	0.018773858408949625	36339	36.339	0.08024048075452495
X	0.019377369438807517	4433	4.433	0.16350549873738976
X	0.01994946625134739	140968	140.968	0.05211190075723312
X	0.01842493254076856	4396	4.396	0.16123131925786877
X	0.017835027080740022	6077	6.077	0.14317312762615952
X	0.019799076866349408	223873	223.873	0.04455342280598399
X	0.019485550429910813	49931	49.931	0.07307701592146691
X	0.019541256760054037	40415	40.415	0.0784880072219999
X	0.01984281386443656	187552	187.552	0.04729630208584129
X	0.019966218204562344	226979	226.979	0.04447371188598825
X	0.01802868873801447	3865	3.865	0.16708528899729733
X	0.017954793689089373	4728	4.728	0.1560154652171169
X	0.0182105463278709	19067	19.067	0.09847973459916642
X	0.017489356377099842	1580	1.58	0.22286348295330077
X	0.018201498942293243	6057	6.057	0.14430560293068603
X	0.018777727041720187	19595	19.595	0.09858993598930671
X	0.017562078955244687	3470	3.47	0.17169150867333519
X	0.01811790115589257	8062	8.062	0.1309850401006994
X	0.017676346775478448	4060	4.06	0.16328845999315883
X	0.018034715698417476	5456	5.456	0.14896296667656928
X	0.019210204501235145	337350	337.35	0.03847250143769768
X	0.017484690262666025	10043	10.043	0.12029977826763764
X	0.018771215971436075	81788	81.788	0.06122577240064036
X	0.018006329762265603	11629	11.629	0.11568958849413039
X	0.017937702026607716	26559	26.559	0.08773745422330609
X	0.0185732690480128	23885	23.885	0.0919576155238289
X	0.019903991342431548	768976	768.976	0.029580746490801193
X	0.019555545666607655	671127	671.127	0.030771954092129655
X	0.018071510802915237	26433	26.433	0.08809460959156469
X	0.018630439214141804	79814	79.814	0.06157172754511847
X	0.017999627571872196	3795	3.795	0.16801597622329237
X	0.017576155904168434	26650	26.65	0.08704467891336527
X	0.01887568723580828	56067	56.067	0.0695662326155991
X	0.019892712989910456	48242	48.242	0.0744314085062436
X	0.017513680061631495	6183	6.183	0.14149010051071217
X	0.019596147743384255	81460	81.46	0.06219308498266904
X	0.019590668156945545	385558	385.558	0.03703848177885746
X	0.017966835658113498	40652	40.652	0.07617221060148911
X	0.017838986988930234	9325	9.325	0.12413867799407376
X	0.019948571096587757	42804	42.804	0.0775310762181063
X	0.01881166753745875	10446	10.446	0.12166315964458155
X	0.017969665479058698	5010	5.01	0.15307376192803035
X	0.01971416617277711	151056	151.056	0.05072399809345309
X	0.018456921469698725	33179	33.179	0.0822429457447803
X	0.01776424495720589	1886	1.886	0.2111875709022715
X	0.018741108708826802	114774	114.774	0.054657603700134914
X	0.01914216892654247	17590	17.59	0.10285887460890636
X	0.01789506759037392	1810	1.81	0.21462728049560623
X	0.018978938834863755	139455	139.455	0.05143743203475628
X	0.019225219080693242	107820	107.82	0.056284739015727225
X	0.01894027893558428	91474	91.474	0.05916019090009618
X	0.019773432907547402	188872	188.872	0.04713080220001062
X	0.01908947997071081	154606	154.606	0.04979540295972392
X	0.01880901852043032	13198	13.198	0.11253458231727366
X	0.01824514541863169	51779	51.779	0.07063141228981581
X	0.019365665137790953	147469	147.469	0.05082888241621386
X	0.01892552691548808	191704	191.704	0.0462174097326035
X	0.01795733686988194	7685	7.685	0.13269850172157938
X	0.018810659071259964	197163	197.163	0.045694037262884644
X	0.01991825870323901	249464	249.464	0.043060707622152784
X	0.01805383943610634	89771	89.771	0.05858833699155458
X	0.01883191032872478	56822	56.822	0.06920316445631855
X	0.019526554637800262	27721	27.721	0.08897586989975227
X	0.018737825607763707	10014	10.014	0.12322643052290115
X	0.01798680218534456	5405	5.405	0.14929757490382456
X	0.017749831707705895	6394	6.394	0.1405423472556698
X	0.019468881860593288	151463	151.463	0.05046746436861443
X	0.018682955821851165	121267	121.267	0.053608581732760605
X	0.019047629340087274	311272	311.272	0.03940641787355391
X	0.0199439272407765	221103	221.103	0.0448475457883806
X	0.019131678197519562	80273	80.273	0.06200040471831121
X	0.01879803665788191	14990	14.99	0.1078376180437613
X	0.019515627286913508	129321	129.321	0.0532399722397816
X	0.0180035522782373	3191	3.191	0.17802341726271906
X	0.018812544572050293	12265	12.265	0.11532584970646216
X	0.019892964598986818	227895	227.895	0.044359662945658254
X	0.01778383154126744	10477	10.477	0.11928782790158982
X	0.017845258921343282	3389	3.389	0.17397376474970794
X	0.018436376425917212	13786	13.786	0.11017399498634392
X	0.019526773406693808	124890	124.89	0.05387255919285841
X	0.01900868688091709	31693	31.693	0.08433274863593392
X	0.01992211886416232	160047	160.047	0.0499301261152095
X	0.01786465242082957	13627	13.627	0.10944554922244991
X	0.01789762512202598	20332	20.332	0.0958381530077087
X	0.01838644739489664	5700	5.7	0.14775459277499606
X	0.018783617166233342	3854	3.854	0.16954663417493418
X	0.01991884010145501	56022	56.022	0.07084379408214904
X	0.018362968589229103	53308	53.308	0.07009980891032283
X	0.017700282299067425	5259	5.259	0.14986228174082372
X	0.01805546695767468	141532	141.532	0.050340558212938806
X	0.017693958586971804	3303	3.303	0.17497344566751194
X	0.018993952505379705	31268	31.268	0.08469122502477736
X	0.01796597698003028	7125	7.125	0.13610956836304255
X	0.0180099221502152	96563	96.563	0.05713475400936697
X	0.018951059556398305	13116	13.116	0.11305176909717943
X	0.01841027844971601	46505	46.505	0.07342662547771085
X	0.018003139074734626	17110	17.11	0.10171056706466354
X	0.019290682308509905	26919	26.919	0.08948760578040123
X	0.01967440389628403	64034	64.034	0.06747822505098615
X	0.018826475197792937	362994	362.994	0.03729264525457114
X	0.018579293545537997	12212	12.212	0.11501314173788545
X	0.01804113189444314	11225	11.225	0.11713656310860932
X	0.018981196354629276	108729	108.729	0.05588896443913736
X	0.018461467020293672	12131	12.131	0.11502437143571016
X	0.019923291221306646	591110	591.11	0.0323021125153709
X	0.01842071979525712	2379	2.379	0.1978354209736694
X	0.0184117171575979	98764	98.764	0.05712574469440248
X	0.01982650141277138	262843	262.843	0.042252254784376185
X	0.01796030526989608	3434	3.434	0.1735819146431219
X	0.018452888756533274	205111	205.111	0.044808300328527485
X	0.019234834035839313	33980	33.98	0.08272212641025226
X	0.01998147786392379	734922	734.922	0.03006967858828707
X	0.018784793583568268	8966	8.966	0.12795852665946425
X	0.018435188089963625	46503	46.503	0.07346077977360282
X	0.018128447692614586	6259	6.259	0.1425449673193192
X	0.0185311486460991	6455	6.455	0.1421244174917496
X	0.018429491667855645	8410	8.41	0.12988902402087932
X	0.018780980389048546	49724	49.724	0.07228542417109018
X	0.01898886103629317	26251	26.251	0.08976720188604011
X	0.019534912816201934	89384	89.384	0.060235228734941755
X	0.018467922865489258	19689	19.689	0.09788845282694214
X	0.018373176575989875	3527	3.527	0.17335096478398263
X	0.018541850623295223	7562	7.562	0.1348462277032955
X	0.019954871836859508	230612	230.612	0.044230550719317634
X	0.019940979172766594	55477	55.477	0.07110134845690605
X	0.019953797339834347	300962	300.962	0.04047354989324887
X	0.01962928102783079	42026	42.026	0.07758815871544866
X	0.01801932546461642	10186	10.186	0.12094231738780963
X	0.017850338660624743	4092	4.092	0.16339431613328279
X	0.017597833367127072	15185	15.185	0.10503840899948111
X	0.01997244212848886	957143	957.143	0.027530754559310897
X	0.01873137294478049	11475	11.475	0.11774405059636638
X	0.01833185518865348	20480	20.48	0.09637376413935116
X	0.0187434776540107	93960	93.96	0.0584299778340266
X	0.018973338138779418	52714	52.714	0.07113324015528419
X	0.01843634690703118	5363	5.363	0.15092309108373034
X	0.01766290269751496	3477	3.477	0.17190392596000936
X	0.01980808138129785	114542	114.542	0.055713342754214086
X	0.01836136370099872	41460	41.46	0.07622402509918781
X	0.018031250571737906	26612	26.612	0.0878313270544017
X	0.018605133304486432	41765	41.765	0.07637304390946237
X	0.019921459548446693	186112	186.112	0.04748053376818404
X	0.01989353165755209	118363	118.363	0.05518638864198209
X	0.01978080818214466	250480	250.48	0.042903263198993735
X	0.018481724336263196	118345	118.345	0.05385146527413144
X	0.018334086972414523	15818	15.818	0.10504351326822854
X	0.01930054609255929	70931	70.931	0.0648004226017038
X	0.01830551343334329	56102	56.102	0.06884427914476708
X	0.01830478288962432	13117	13.117	0.11174892513547618
X	0.018505828297088328	18270	18.27	0.10042842697432001
X	0.018410578846972808	10289	10.289	0.12140356093402596
X	0.018800639589455843	58966	58.966	0.06831619224275076
X	0.019067019206628403	13329	13.329	0.1126752193039639
X	0.017421322580567198	2171	2.171	0.20020446838550968
X	0.01875077864432621	14887	14.887	0.10799509716142701
X	0.018044195379234462	21547	21.547	0.09425771407116709
X	0.018968811754474544	57754	57.754	0.0689952754314821
X	0.01796437301043727	13672	13.672	0.10952838183342847
X	0.018563141975553324	24071	24.071	0.09170347318062849
X	0.019160065435961538	10055	10.055	0.1239761953474641
X	0.018022488210124085	23920	23.92	0.09099506891799294
X	0.017697407188194	4905	4.905	0.15337581279561388
X	0.017969556371303763	11606	11.606	0.1156870987386583
X	0.018642538934178655	23007	23.007	0.09322842674100926
X	0.01993017851358649	254206	254.206	0.04279980568096394
X	0.01877360647556396	8771	8.771	0.1288742644926746
X	0.017966502324955932	1887	1.887	0.21194859692057916
X	0.017433778361353718	4531	4.531	0.15669885367527792
X	0.018421303472475288	218572	218.572	0.043843842636837806
X	0.01988374464010285	253236	253.236	0.04282107613564841
X	0.017873866220502563	38469	38.469	0.07745257638032073
X	0.018956842803404036	10923	10.923	0.12017331046818953
X	0.01886469023107006	86269	86.269	0.06024647452075092
X	0.019153342948456217	170642	170.642	0.048237658373427486
X	0.01857316916742995	2963	2.963	0.18438182996029848
X	0.017812201272849732	10975	10.975	0.11751799005881493
X	0.0184327432101172	29425	29.425	0.08556401735321628
X	0.01873255399269665	29450	29.45	0.08600107858032054
X	0.017994947581800577	36386	36.386	0.07908098867429979
X	0.018848748427250202	134426	134.426	0.051951698079744826
X	0.018442004626941212	15623	15.623	0.10568529515510348
X	0.019002885823402753	5015	5.015	0.15590122961072583
X	0.017491649937604513	3116	3.116	0.17772333029008355
X	0.018420062580095355	28088	28.088	0.08688072338795821
X	0.018655669610985543	25646	25.646	0.08993534423789112
X	0.01863885141406875	47834	47.834	0.07304000860239461
X	0.019577208740957107	150066	150.066	0.050717303467670574
X	0.019894559188266377	423485	423.485	0.0360826780142946
X	0.018103873262122462	9436	9.436	0.12425900055074093
X	0.018674808191866357	3050	3.05	0.18294425259802152
X	0.018553277468939075	11219	11.219	0.11825572196795056
X	0.01869080651723789	47136	47.136	0.07346690971264751
X	0.01867061349601597	31104	31.104	0.08435563813438864
X	0.01842373682293566	3517	3.517	0.1736741242565885
X	0.017722307614313468	4526	4.526	0.15761658258409655
X	0.018439583227752077	50639	50.639	0.07140939057101799
X	0.01802215696521937	3331	3.331	0.17555399289090368
X	0.01992587246683846	79259	79.259	0.06311359508908472
X	0.01822544583909091	5919	5.919	0.14548222695433077
X	0.01875309989783949	17733	17.733	0.10188188017050961
X	0.01868411742345533	9867	9.867	0.12371693532716312
X	0.018427215286837363	6886	6.886	0.13883455219093763
X	0.01737579556554001	3092	3.092	0.17778771067223365
X	0.01889750331461738	130402	130.402	0.05252588670926636
X	0.01980954343917379	374454	374.454	0.03753980370352731
X	0.018086814742091558	171496	171.496	0.047246469910321116
X	0.018886742175372694	72234	72.234	0.06394478463027395
X	0.018641308660296403	32087	32.087	0.08344156245903589
X	0.018112163641978166	8280	8.28	0.12981154898158656
X	0.019501539617788787	16788	16.788	0.10512112007692494
X	0.019409177866117885	161749	161.749	0.04932364672698593
X	0.019562602402824897	19037	19.037	0.10091197501461584
X	0.01895769649684775	28699	28.699	0.0870909311585168
X	0.01788516385743278	9585	9.585	0.12311196656996579
X	0.0199615729021233	562106	562.106	0.03286943049074252
X	0.017987907052987116	30459	30.459	0.08389865487801987
X	0.019411956213642858	44840	44.84	0.07564865886256779
X	0.019878346264563557	203553	203.553	0.04605048918418958
X	0.01843404277070351	47989	47.989	0.07269306213734397
X	0.01889484267473881	30279	30.279	0.08545429929221705
X	0.019953927283982008	174283	174.283	0.048557664048776296
X	0.018606651838966502	78421	78.421	0.06190778777392105
X	0.018039783225854557	41814	41.814	0.07556195744838516
X	0.0188522352283331	15761	15.761	0.1061515549652147
X	0.018039697260545228	45777	45.777	0.07331519135305649
X	0.018053448164210433	13028	13.028	0.11148783223829178
X	0.018819106165161595	106265	106.265	0.05615687279218799
X	0.019970415646823402	844763	844.763	0.028700143842807
X	0.01995379363023441	147485	147.485	0.0513364548918277
X	0.01992771623556789	147177	147.177	0.051349851823435926
X	0.018032972884387512	11661	11.661	0.11564064662366193
X	0.019734086764141054	79634	79.634	0.06281154858585102
X	0.018287973211718885	36917	36.917	0.07912487891932167
X	0.017907119233117694	12292	12.292	0.1133620587815989
X	0.019555084781035802	35427	35.427	0.08203045276168289
X	0.01945177439871945	361479	361.479	0.03775363017531603
X	0.018294207247683807	27980	27.98	0.08679378504225058
X	0.018351355836549954	147395	147.395	0.049933859503002415
X	0.019774880503056355	90043	90.043	0.06033296061463884
X	0.018269010236205	42410	42.41	0.07552352444015664
X	0.017790375572780504	5234	5.234	0.15035474303908508
X	0.019262308524282463	68705	68.705	0.06544954980097184
X	0.018943663773303015	145061	145.061	0.05073462289239782
X	0.018414105977877022	3580	3.58	0.17261924146993882
X	0.01993614996089662	718820	718.82	0.030269625409877793
X	0.01878424333733007	48807	48.807	0.07273953669991463
X	0.01938049444617349	65915	65.915	0.06649569539758665
X	0.018041674340282414	9166	9.166	0.12532344620565686
X	0.01789084617310284	13132	13.132	0.1108579055571869
X	0.01830406433347573	3248	3.248	0.17795512679613007
X	0.01809933741154404	4572	4.572	0.15819235261416223
X	0.018841632806532962	27505	27.505	0.08815270205962486
X	0.01874445035590328	62308	62.308	0.06700537359122133
X	0.019987076005751475	2400991	2400.991	0.02026684956902919
X	0.018035891547187634	8367	8.367	0.1291782158749856
X	0.019068592092425028	75109	75.109	0.06332019100740094
X	0.018769588531570836	8316	8.316	0.13117369647714702
X	0.018268846160037953	20529	20.529	0.09618657070586784
X	0.017795605477622694	12260	12.26	0.11322459249139404
X	0.01987789731946456	224819	224.819	0.04454980530487963
X	0.018035656634477627	11972	11.972	0.11463619172264342
X	0.0199051730772271	1068638	1068.638	0.026508090723133794
X	0.018677097014483918	6234	6.234	0.14416091348167634
X	0.019922714486090776	178333	178.333	0.04816212034124272
X	0.018609887407413157	40184	40.184	0.07736838417939834
X	0.018672118278082905	111965	111.965	0.0550432119469044
X	0.019845939971253158	271470	271.47	0.041813508516108426
X	0.01796236477539198	18364	18.364	0.09926559327046365
X	0.018023628438575603	7948	7.948	0.131379656616232
X	0.018837935742848484	86767	86.767	0.060102551572628296
X	0.01883146608738086	11819	11.819	0.11679774732029184
X	0.01855972210469757	20817	20.817	0.09624638852115713
X	0.018779013956534595	72782	72.782	0.06366238916100043
X	0.019253374576716723	102702	102.702	0.05723249157785794
X	0.019795602705893207	54899	54.899	0.07117619462388307
X	0.017994186805956304	28308	28.308	0.08598202700401467
X	0.01929243737384491	233184	233.184	0.04357420280938574
X	0.0197895954108766	314772	314.772	0.03976310353039949
X	0.019953270085494548	36243	36.243	0.08195899934581237
X	0.01853050542625619	7575	7.575	0.13474155113595343
X	0.01815818792573491	133664	133.664	0.05140665564522647
X	0.017871592421221005	37213	37.213	0.07831101557525212
X	0.019226705433042134	54725	54.725	0.07056234968825002
X	0.01839282606749059	36652	36.652	0.07946641031409439
X	0.01896976141212123	71758	71.758	0.06417962585285908
X	0.019162681787379916	77771	77.771	0.06269210729348187
X	0.018829375535903175	190446	190.446	0.04624037568487403
X	0.01962199167268094	36337	36.337	0.08143254464531267
X	0.018133435929569963	17371	17.371	0.10144214581129073
X	0.018179528400805053	6795	6.795	0.13882394571333212
X	0.019234590614416083	23237	23.237	0.09389323368647215
X	0.019713558342424633	56556	56.556	0.0703766805891937
X	0.019117585722589533	127031	127.031	0.05319136186675786
X	0.019772483633597578	139514	139.514	0.05213721290428717
X	0.018482262797100434	9688	9.688	0.12402436257929954
X	0.019152513828498707	24913	24.913	0.0916079684708808
X	0.018718412639979237	71795	71.795	0.06388392713857205
X	0.01807340074511947	109589	109.589	0.05483918242701086
X	0.019792096896727018	69341	69.341	0.0658416293056888
X	0.01846011048896763	74545	74.545	0.06279695599106529
X	0.01979188380450463	103332	103.332	0.05764355618778213
X	0.01869990269281801	32747	32.747	0.08296392220985818
X	0.018752540684547916	2861	2.861	0.18714608781097555
X	0.018682627545035545	7242	7.242	0.1371491578214907
X	0.01885747246867271	137359	137.359	0.05158721952094943
X	0.01803625251754116	34889	34.889	0.08025755562205593
X	0.018781005153042176	72387	72.387	0.06378023051907662
X	0.01808510990715422	2184	2.184	0.20231255716426041
X	0.01864424987621683	4630	4.63	0.15909430775605454
X	0.019328964436336034	104236	104.236	0.057024779755974093
X	0.01876111785666012	80557	80.557	0.06152502645135822
X	0.01845403101664226	41614	41.614	0.0762577395720254
X	0.017975109760059806	4332	4.332	0.16069204128562217
X	0.018430814252088316	5070	5.07	0.15376076143155362
X	0.018483981860383608	43945	43.945	0.07492533411575718
X	0.018989804582267213	38265	38.265	0.07917223881051241
X	0.01891914946204966	120923	120.923	0.05388454644465978
X	0.018231487953388432	7032	7.032	0.13737712318870865
X	0.019130793999081224	42362	42.362	0.07672181113247738
X	0.01796450839273328	15410	15.41	0.10524566892573028
X	0.01787602017777371	15529	15.529	0.10480349893043207
X	0.019934614372192586	361435	361.435	0.03806500381928255
X	0.018736487898482902	9939	9.939	0.12353267109932006
X	0.018542112115062633	42202	42.202	0.07602248411343664
X	0.019661900070604733	125046	125.046	0.05397407731830358
X	0.018692384652252602	91975	91.975	0.05879381447381618
X	0.019871434322100416	145943	145.943	0.05144564881522275
X	0.019884228555698397	191522	191.522	0.046999873897985885
X	0.018285014980833212	3808	3.808	0.16870687991054487
X	0.01926402558524695	49184	49.184	0.07316573395199556
X	0.01876562042360582	21400	21.4	0.09571567169259043
X	0.01867363487663376	123634	123.634	0.05325539956574994
X	0.01801421737944837	128697	128.697	0.051921710567343474
X	0.01878512589645554	33969	33.969	0.08208121617927681
X	0.019886120221863795	84477	84.477	0.06174529350315924
X	0.017956301523648993	10206	10.206	0.1207221908900702
X	0.01792454681618329	23500	23.5	0.09136785739031865
X	0.01796016348881908	6584	6.584	0.13972479716116773
X	0.01886099167174345	15609	15.609	0.10651149509605198
X	0.019042899890293494	171690	171.69	0.048046603778730705
X	0.019924530905786348	279895	279.895	0.041444253518680224
X	0.01930374324799031	13102	13.102	0.11378927031735528
X	0.018250575145661332	25704	25.704	0.08921241190632284
X	0.018394830862014516	17332	17.332	0.10200364853615974
X	0.019744752676807	240343	240.343	0.04347171162199735
X	0.017897974325184772	21992	21.992	0.09336405762015952
X	0.018434129739242747	12263	12.263	0.11455357233843434
X	0.019754747154150733	53389	53.389	0.07179153341875628
X	0.01950599060400942	116831	116.831	0.055064280926058605
X	0.017670596697215858	10100	10.1	0.12049710522426876
X	0.019484344286074023	61664	61.664	0.06811112191615074
X	0.017996774083424524	5710	5.71	0.14661762678670034
time for making epsilon is 2.3637382984161377
epsilons are
[0.23005779311114807, 0.08818566682606244, 0.17211183118681397, 0.1575145861356963, 0.09025673127071139, 0.07785384259094436, 0.06607653571640264, 0.07152421113461749, 0.12919975075747503, 0.08157605848714417, 0.11476727649383527, 0.05488530838003448, 0.14002251165481922, 0.04043946642833723, 0.09356973632519726, 0.06810438298208085, 0.06981832539473093, 0.07507603537107448, 0.042925060376205214, 0.05224026404916307, 0.09671180743037837, 0.03717357009563395, 0.10470125721474748, 0.0887694285949937, 0.09862492295187124, 0.05929590997284326, 0.06619009512264838, 0.11895059536435731, 0.06237377463381629, 0.02275830732198188, 0.12947090456678967, 0.031431781604542526, 0.10713605338285831, 0.12702416508822956, 0.122306285315626, 0.04897957612464276, 0.05575768219845146, 0.21479060392185123, 0.15007457847536915, 0.1766364629698908, 0.1458493739909453, 0.21879974387655146, 0.23621053789347415, 0.173423133553626, 0.22727703182544837, 0.19606231983777106, 0.12206960894659453, 0.15361939174199638, 0.20969441133323052, 0.10954044226861596, 0.10845627627847328, 0.19215246903465127, 0.14187887936092933, 0.17838040326606708, 0.1375224669396413, 0.14048932234402522, 0.17966734018234212, 0.1599387125694696, 0.16065020930651777, 0.19275785393227773, 0.13679196418522305, 0.19102660615557931, 0.11937343423963663, 0.13688441489968017, 0.1721914973003338, 0.17326652364620665, 0.15418598058099472, 0.1473469791867461, 0.14686749140693034, 0.20333496725987643, 0.17654002296141194, 0.17887186592931223, 0.1661198161546268, 0.20587367210712326, 0.16574435347113903, 0.14627378522657195, 0.19247593969155405, 0.2584225973599625, 0.17285657127153625, 0.11706995733021339, 0.15358492572307766, 0.22365059043192537, 0.13442221957298767, 0.21255551716511276, 0.18674178588069346, 0.2125618549821733, 0.1557181115810996, 0.2037830266943953, 0.15879797073142665, 0.16138265030675272, 0.1585159125724856, 0.1988567771775638, 0.2167168843872027, 0.16041147960848262, 0.15733335503640503, 0.15903886192767921, 0.16763208750934616, 0.12910128062878828, 0.16286362005275476, 0.1777494325833186, 0.1053070265442792, 0.14735035852484224, 0.13380924316624024, 0.2081165528797999, 0.16566354793433563, 0.1954264819190203, 0.12882257225251567, 0.26774537951285204, 0.17152423321511645, 0.20734363192341898, 0.20467973419812918, 0.2257797197824972, 0.20462774083763563, 0.15519147659361854, 0.15693353050000686, 0.19963620972121615, 0.18770146124349388, 0.22567782213236387, 0.1317110809820752, 0.19732872141266686, 0.11293388492262434, 0.1724451025249245, 0.1935764854632986, 0.1752982310515903, 0.2055258199711663, 0.18844185055744583, 0.19214057207252652, 0.18866433179945383, 0.11068078638054984, 0.17945757537766405, 0.13462025199554195, 0.21301401530384728, 0.15300015525117155, 0.22158626293679395, 0.17147650119015645, 0.18852177229733308, 0.2289788649775227, 0.19570241908052063, 0.20011678314161516, 0.171348370317516, 0.15966240336632176, 0.2122067085944791, 0.12130592625658723, 0.11129396943365676, 0.14421770285896884, 0.17816283346453277, 0.20271743446182372, 0.17360580346752114, 0.19925040343914602, 0.18469062862749241, 0.1968117801095602, 0.15425694326910094, 0.15526061126576815, 0.10960339601438782, 0.17742690405782474, 0.10947661096882928, 0.1626636059601236, 0.173119512992479, 0.1514109335756711, 0.2740117332981618, 0.13355841891937079, 0.18804832137862543, 0.15959220937338983, 0.22181707391487523, 0.18223060003464103, 0.21500499697009587, 0.15827110496729377, 0.16380875829942967, 0.1620208477657382, 0.27295618747359124, 0.22301846875050363, 0.2182684364691281, 0.13689288740548092, 0.17182271479160913, 0.14539438054474527, 0.16942658820183995, 0.14834024913067065, 0.16945830545120624, 0.1405318766402035, 0.18295810303282178, 0.17336235057283542, 0.18018367370099048, 0.16689453913407937, 0.19442961797145172, 0.1298718059417532, 0.13656031127420742, 0.17632311458862368, 0.15552791594407822, 0.144720651862928, 0.1047691643659387, 0.20099350823492818, 0.2826397924483447, 0.14643911692678868, 0.19218663748622772, 0.1749556789427248, 0.16746330799958653, 0.18780429069087487, 0.21175467081767457, 0.17883864571061117, 0.21644931472257212, 0.15530148290443185, 0.21794821287064106, 0.1313079495647903, 0.14812922717319163, 0.16963175807153374, 0.2381709107754688, 0.15093657462212562, 0.16734124787500892, 0.15084498013709352, 0.13358687855205514, 0.15778517426338154, 0.16635607166536096, 0.17456287446181837, 0.18652816879943163, 0.24197986249055586, 0.21813595441360067, 0.22345749537835263, 0.2637649777882661, 0.16188423116565462, 0.18247047218279522, 0.18519051494706562, 0.1933287580742472, 0.15918105349538084, 0.17251157480107002, 0.16281523671937573, 0.16566420240900478, 0.207218614127796, 0.21619461274492707, 0.1534779830141872, 0.2698169536833906, 0.21479964801694845, 0.18106443751541165, 0.1605660212115675, 0.14581106245513453, 0.2092762419782972, 0.1987348534619076, 0.17281420309807816, 0.13081874997094267, 0.19707228082225944, 0.11657301560182379, 0.0679060670146475, 0.10776735206975217, 0.14971718126973707, 0.268412967036778, 0.13810228118238377, 0.057766289045452945, 0.041780648138873985, 0.16738102471553531, 0.058460518917380626, 0.0828541186745128, 0.09490971793914117, 0.04919967674450999, 0.20237770733104388, 0.14684746250632982, 0.04250693825042847, 0.035418471482975764, 0.1497482261986816, 0.08397739190963226, 0.06399367404733507, 0.069626307553479, 0.06895185255664056, 0.07202340244491022, 0.07615508393253068, 0.10648328603946518, 0.17415460198070423, 0.034010234110626585, 0.16553291752177, 0.19113506656258963, 0.07116526065056136, 0.08046913497743635, 0.039062383472754904, 0.05381256434241818, 0.22513606809473888, 0.09358318077548548, 0.12180081473607199, 0.07741910606650743, 0.061764802798135926, 0.11387554785024931, 0.07475200558903086, 0.27558199361564695, 0.14087764630490374, 0.06368504855199214, 0.06469443739330136, 0.07818384792057813, 0.11049105647970979, 0.04073239419703533, 0.1054023672740131, 0.08920013878886156, 0.0637748680945775, 0.14937799876718513, 0.05687832901101066, 0.042548001266139, 0.04193429892485531, 0.1037876024942562, 0.10517144113016505, 0.10115675589667159, 0.14833879885154103, 0.09001345617865579, 0.1166619680575755, 0.09706100114013279, 0.05047183601162703, 0.06773337860514658, 0.1172255379845675, 0.12526211240553017, 0.06156609180723151, 0.10227107282480115, 0.09136791228172822, 0.11181951639645082, 0.037628940628605875, 0.12074852126404093, 0.06527436723280902, 0.1429547026944443, 0.07364456349031102, 0.08944907515726114, 0.03862688033047592, 0.06792990504806043, 0.14153825058646208, 0.052317688567421014, 0.05334561791398243, 0.1403206564969765, 0.05895718397497116, 0.05343156361474201, 0.03983901158225296, 0.0706304663611654, 0.1982227734189056, 0.1002203625035967, 0.06878962185751675, 0.06330533508917846, 0.09781867500806568, 0.16474280049024034, 0.06649966346456052, 0.04743321653979947, 0.06363938242805348, 0.1343103376420584, 0.09833079277125334, 0.11492102365434641, 0.14770249580950132, 0.11290448219619119, 0.1317662320329441, 0.0769144350315007, 0.07224489383890226, 0.09679383451936027, 0.09013812395592306, 0.13857002316649858, 0.03509845958055895, 0.12935287533079035, 0.05524187874268434, 0.1677786324795527, 0.1410342438538564, 0.16502537386491264, 0.08024048075452495, 0.16350549873738976, 0.05211190075723312, 0.16123131925786877, 0.14317312762615952, 0.04455342280598399, 0.07307701592146691, 0.0784880072219999, 0.04729630208584129, 0.04447371188598825, 0.16708528899729733, 0.1560154652171169, 0.09847973459916642, 0.22286348295330077, 0.14430560293068603, 0.09858993598930671, 0.17169150867333519, 0.1309850401006994, 0.16328845999315883, 0.14896296667656928, 0.03847250143769768, 0.12029977826763764, 0.06122577240064036, 0.11568958849413039, 0.08773745422330609, 0.0919576155238289, 0.029580746490801193, 0.030771954092129655, 0.08809460959156469, 0.06157172754511847, 0.16801597622329237, 0.08704467891336527, 0.0695662326155991, 0.0744314085062436, 0.14149010051071217, 0.06219308498266904, 0.03703848177885746, 0.07617221060148911, 0.12413867799407376, 0.0775310762181063, 0.12166315964458155, 0.15307376192803035, 0.05072399809345309, 0.0822429457447803, 0.2111875709022715, 0.054657603700134914, 0.10285887460890636, 0.21462728049560623, 0.05143743203475628, 0.056284739015727225, 0.05916019090009618, 0.04713080220001062, 0.04979540295972392, 0.11253458231727366, 0.07063141228981581, 0.05082888241621386, 0.0462174097326035, 0.13269850172157938, 0.045694037262884644, 0.043060707622152784, 0.05858833699155458, 0.06920316445631855, 0.08897586989975227, 0.12322643052290115, 0.14929757490382456, 0.1405423472556698, 0.05046746436861443, 0.053608581732760605, 0.03940641787355391, 0.0448475457883806, 0.06200040471831121, 0.1078376180437613, 0.0532399722397816, 0.17802341726271906, 0.11532584970646216, 0.044359662945658254, 0.11928782790158982, 0.17397376474970794, 0.11017399498634392, 0.05387255919285841, 0.08433274863593392, 0.0499301261152095, 0.10944554922244991, 0.0958381530077087, 0.14775459277499606, 0.16954663417493418, 0.07084379408214904, 0.07009980891032283, 0.14986228174082372, 0.050340558212938806, 0.17497344566751194, 0.08469122502477736, 0.13610956836304255, 0.05713475400936697, 0.11305176909717943, 0.07342662547771085, 0.10171056706466354, 0.08948760578040123, 0.06747822505098615, 0.03729264525457114, 0.11501314173788545, 0.11713656310860932, 0.05588896443913736, 0.11502437143571016, 0.0323021125153709, 0.1978354209736694, 0.05712574469440248, 0.042252254784376185, 0.1735819146431219, 0.044808300328527485, 0.08272212641025226, 0.03006967858828707, 0.12795852665946425, 0.07346077977360282, 0.1425449673193192, 0.1421244174917496, 0.12988902402087932, 0.07228542417109018, 0.08976720188604011, 0.060235228734941755, 0.09788845282694214, 0.17335096478398263, 0.1348462277032955, 0.044230550719317634, 0.07110134845690605, 0.04047354989324887, 0.07758815871544866, 0.12094231738780963, 0.16339431613328279, 0.10503840899948111, 0.027530754559310897, 0.11774405059636638, 0.09637376413935116, 0.0584299778340266, 0.07113324015528419, 0.15092309108373034, 0.17190392596000936, 0.055713342754214086, 0.07622402509918781, 0.0878313270544017, 0.07637304390946237, 0.04748053376818404, 0.05518638864198209, 0.042903263198993735, 0.05385146527413144, 0.10504351326822854, 0.0648004226017038, 0.06884427914476708, 0.11174892513547618, 0.10042842697432001, 0.12140356093402596, 0.06831619224275076, 0.1126752193039639, 0.20020446838550968, 0.10799509716142701, 0.09425771407116709, 0.0689952754314821, 0.10952838183342847, 0.09170347318062849, 0.1239761953474641, 0.09099506891799294, 0.15337581279561388, 0.1156870987386583, 0.09322842674100926, 0.04279980568096394, 0.1288742644926746, 0.21194859692057916, 0.15669885367527792, 0.043843842636837806, 0.04282107613564841, 0.07745257638032073, 0.12017331046818953, 0.06024647452075092, 0.048237658373427486, 0.18438182996029848, 0.11751799005881493, 0.08556401735321628, 0.08600107858032054, 0.07908098867429979, 0.051951698079744826, 0.10568529515510348, 0.15590122961072583, 0.17772333029008355, 0.08688072338795821, 0.08993534423789112, 0.07304000860239461, 0.050717303467670574, 0.0360826780142946, 0.12425900055074093, 0.18294425259802152, 0.11825572196795056, 0.07346690971264751, 0.08435563813438864, 0.1736741242565885, 0.15761658258409655, 0.07140939057101799, 0.17555399289090368, 0.06311359508908472, 0.14548222695433077, 0.10188188017050961, 0.12371693532716312, 0.13883455219093763, 0.17778771067223365, 0.05252588670926636, 0.03753980370352731, 0.047246469910321116, 0.06394478463027395, 0.08344156245903589, 0.12981154898158656, 0.10512112007692494, 0.04932364672698593, 0.10091197501461584, 0.0870909311585168, 0.12311196656996579, 0.03286943049074252, 0.08389865487801987, 0.07564865886256779, 0.04605048918418958, 0.07269306213734397, 0.08545429929221705, 0.048557664048776296, 0.06190778777392105, 0.07556195744838516, 0.1061515549652147, 0.07331519135305649, 0.11148783223829178, 0.05615687279218799, 0.028700143842807, 0.0513364548918277, 0.051349851823435926, 0.11564064662366193, 0.06281154858585102, 0.07912487891932167, 0.1133620587815989, 0.08203045276168289, 0.03775363017531603, 0.08679378504225058, 0.049933859503002415, 0.06033296061463884, 0.07552352444015664, 0.15035474303908508, 0.06544954980097184, 0.05073462289239782, 0.17261924146993882, 0.030269625409877793, 0.07273953669991463, 0.06649569539758665, 0.12532344620565686, 0.1108579055571869, 0.17795512679613007, 0.15819235261416223, 0.08815270205962486, 0.06700537359122133, 0.02026684956902919, 0.1291782158749856, 0.06332019100740094, 0.13117369647714702, 0.09618657070586784, 0.11322459249139404, 0.04454980530487963, 0.11463619172264342, 0.026508090723133794, 0.14416091348167634, 0.04816212034124272, 0.07736838417939834, 0.0550432119469044, 0.041813508516108426, 0.09926559327046365, 0.131379656616232, 0.060102551572628296, 0.11679774732029184, 0.09624638852115713, 0.06366238916100043, 0.05723249157785794, 0.07117619462388307, 0.08598202700401467, 0.04357420280938574, 0.03976310353039949, 0.08195899934581237, 0.13474155113595343, 0.05140665564522647, 0.07831101557525212, 0.07056234968825002, 0.07946641031409439, 0.06417962585285908, 0.06269210729348187, 0.04624037568487403, 0.08143254464531267, 0.10144214581129073, 0.13882394571333212, 0.09389323368647215, 0.0703766805891937, 0.05319136186675786, 0.05213721290428717, 0.12402436257929954, 0.0916079684708808, 0.06388392713857205, 0.05483918242701086, 0.0658416293056888, 0.06279695599106529, 0.05764355618778213, 0.08296392220985818, 0.18714608781097555, 0.1371491578214907, 0.05158721952094943, 0.08025755562205593, 0.06378023051907662, 0.20231255716426041, 0.15909430775605454, 0.057024779755974093, 0.06152502645135822, 0.0762577395720254, 0.16069204128562217, 0.15376076143155362, 0.07492533411575718, 0.07917223881051241, 0.05388454644465978, 0.13737712318870865, 0.07672181113247738, 0.10524566892573028, 0.10480349893043207, 0.03806500381928255, 0.12353267109932006, 0.07602248411343664, 0.05397407731830358, 0.05879381447381618, 0.05144564881522275, 0.046999873897985885, 0.16870687991054487, 0.07316573395199556, 0.09571567169259043, 0.05325539956574994, 0.051921710567343474, 0.08208121617927681, 0.06174529350315924, 0.1207221908900702, 0.09136785739031865, 0.13972479716116773, 0.10651149509605198, 0.048046603778730705, 0.041444253518680224, 0.11378927031735528, 0.08921241190632284, 0.10200364853615974, 0.04347171162199735, 0.09336405762015952, 0.11455357233843434, 0.07179153341875628, 0.055064280926058605, 0.12049710522426876, 0.06811112191615074, 0.14661762678670034]
0.09102205497476076
Making ranges
torch.Size([48149, 2])
We keep 7.58e+06/7.04e+08 =  1% of the original kernel matrix.

torch.Size([3690, 2])
We keep 1.15e+05/2.17e+06 =  5% of the original kernel matrix.

torch.Size([14564, 2])
We keep 9.99e+05/3.91e+07 =  2% of the original kernel matrix.

torch.Size([44687, 2])
We keep 1.29e+07/7.95e+08 =  1% of the original kernel matrix.

torch.Size([47044, 2])
We keep 8.98e+06/7.48e+08 =  1% of the original kernel matrix.

torch.Size([8204, 2])
We keep 7.67e+05/1.38e+07 =  5% of the original kernel matrix.

torch.Size([20479, 2])
We keep 1.94e+06/9.86e+07 =  1% of the original kernel matrix.

torch.Size([9247, 2])
We keep 2.15e+06/2.29e+07 =  9% of the original kernel matrix.

torch.Size([20987, 2])
We keep 2.28e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([41142, 2])
We keep 1.48e+07/6.33e+08 =  2% of the original kernel matrix.

torch.Size([44370, 2])
We keep 7.98e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([65410, 2])
We keep 2.42e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([55487, 2])
We keep 1.18e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([107036, 2])
We keep 5.81e+07/4.30e+09 =  1% of the original kernel matrix.

torch.Size([69715, 2])
We keep 1.79e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([86124, 2])
We keep 3.28e+07/2.82e+09 =  1% of the original kernel matrix.

torch.Size([62576, 2])
We keep 1.49e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([16805, 2])
We keep 2.94e+06/7.34e+07 =  4% of the original kernel matrix.

torch.Size([28281, 2])
We keep 3.53e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([42808, 2])
We keep 4.51e+07/1.14e+09 =  3% of the original kernel matrix.

torch.Size([44258, 2])
We keep 1.02e+07/8.94e+08 =  1% of the original kernel matrix.

torch.Size([22427, 2])
We keep 4.50e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([32685, 2])
We keep 4.65e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([158640, 2])
We keep 2.08e+08/1.41e+10 =  1% of the original kernel matrix.

torch.Size([84601, 2])
We keep 2.94e+07/3.15e+09 =  0% of the original kernel matrix.

torch.Size([14469, 2])
We keep 1.28e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([25961, 2])
We keep 2.93e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([477600, 2])
We keep 6.71e+08/7.44e+10 =  0% of the original kernel matrix.

torch.Size([152691, 2])
We keep 6.12e+07/7.24e+09 =  0% of the original kernel matrix.

torch.Size([37709, 2])
We keep 6.74e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([42416, 2])
We keep 7.15e+06/5.79e+08 =  1% of the original kernel matrix.

torch.Size([102009, 2])
We keep 4.79e+07/3.65e+09 =  1% of the original kernel matrix.

torch.Size([67447, 2])
We keep 1.67e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([93740, 2])
We keep 4.99e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([65615, 2])
We keep 1.57e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([70434, 2])
We keep 2.75e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([56980, 2])
We keep 1.24e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([377114, 2])
We keep 6.44e+08/5.97e+10 =  1% of the original kernel matrix.

torch.Size([136451, 2])
We keep 5.51e+07/6.48e+09 =  0% of the original kernel matrix.

torch.Size([215209, 2])
We keep 2.85e+08/1.76e+10 =  1% of the original kernel matrix.

torch.Size([101657, 2])
We keep 3.26e+07/3.52e+09 =  0% of the original kernel matrix.

torch.Size([31083, 2])
We keep 1.46e+07/4.15e+08 =  3% of the original kernel matrix.

torch.Size([38056, 2])
We keep 6.89e+06/5.41e+08 =  1% of the original kernel matrix.

torch.Size([639946, 2])
We keep 1.24e+09/1.46e+11 =  0% of the original kernel matrix.

torch.Size([178328, 2])
We keep 8.20e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([28193, 2])
We keep 4.97e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([36928, 2])
We keep 5.62e+06/4.26e+08 =  1% of the original kernel matrix.

torch.Size([44741, 2])
We keep 2.75e+07/7.51e+08 =  3% of the original kernel matrix.

torch.Size([46646, 2])
We keep 8.71e+06/7.27e+08 =  1% of the original kernel matrix.

torch.Size([21555, 2])
We keep 3.23e+07/3.92e+08 =  8% of the original kernel matrix.

torch.Size([30282, 2])
We keep 6.36e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([150448, 2])
We keep 7.16e+07/7.72e+09 =  0% of the original kernel matrix.

torch.Size([83543, 2])
We keep 2.26e+07/2.33e+09 =  0% of the original kernel matrix.

torch.Size([90451, 2])
We keep 8.36e+07/4.46e+09 =  1% of the original kernel matrix.

torch.Size([63758, 2])
We keep 1.84e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([20733, 2])
We keep 3.31e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([31181, 2])
We keep 4.07e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([113282, 2])
We keep 1.46e+08/5.77e+09 =  2% of the original kernel matrix.

torch.Size([71722, 2])
We keep 2.03e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([3168284, 2])
We keep 1.45e+10/2.86e+12 =  0% of the original kernel matrix.

torch.Size([416380, 2])
We keep 3.26e+08/4.49e+10 =  0% of the original kernel matrix.

torch.Size([16164, 2])
We keep 3.05e+06/7.15e+07 =  4% of the original kernel matrix.

torch.Size([27474, 2])
We keep 3.45e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([982676, 2])
We keep 4.99e+09/4.13e+11 =  1% of the original kernel matrix.

torch.Size([230429, 2])
We keep 1.35e+08/1.71e+10 =  0% of the original kernel matrix.

torch.Size([25923, 2])
We keep 6.50e+06/2.46e+08 =  2% of the original kernel matrix.

torch.Size([34946, 2])
We keep 5.61e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([18159, 2])
We keep 1.95e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([28996, 2])
We keep 3.61e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([17321, 2])
We keep 3.22e+06/9.68e+07 =  3% of the original kernel matrix.

torch.Size([28292, 2])
We keep 3.89e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([239790, 2])
We keep 4.81e+08/2.55e+10 =  1% of the original kernel matrix.

torch.Size([107371, 2])
We keep 3.88e+07/4.24e+09 =  0% of the original kernel matrix.

torch.Size([179846, 2])
We keep 2.24e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([93117, 2])
We keep 2.83e+07/2.96e+09 =  0% of the original kernel matrix.

torch.Size([4505, 2])
We keep 1.73e+05/3.26e+06 =  5% of the original kernel matrix.

torch.Size([15700, 2])
We keep 1.15e+06/4.79e+07 =  2% of the original kernel matrix.

torch.Size([11766, 2])
We keep 9.83e+05/2.97e+07 =  3% of the original kernel matrix.

torch.Size([23428, 2])
We keep 2.50e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([7343, 2])
We keep 5.51e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([19174, 2])
We keep 1.81e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([12786, 2])
We keep 1.13e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([24565, 2])
We keep 2.70e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([2843, 2])
We keep 2.30e+05/2.68e+06 =  8% of the original kernel matrix.

torch.Size([12020, 2])
We keep 1.06e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([3584, 2])
We keep 9.97e+04/1.79e+06 =  5% of the original kernel matrix.

torch.Size([14507, 2])
We keep 9.38e+05/3.55e+07 =  2% of the original kernel matrix.

torch.Size([7779, 2])
We keep 4.79e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([19236, 2])
We keep 1.75e+06/8.99e+07 =  1% of the original kernel matrix.

torch.Size([3696, 2])
We keep 1.45e+05/2.36e+06 =  6% of the original kernel matrix.

torch.Size([14625, 2])
We keep 1.04e+06/4.08e+07 =  2% of the original kernel matrix.

torch.Size([5934, 2])
We keep 2.37e+05/5.48e+06 =  4% of the original kernel matrix.

torch.Size([17684, 2])
We keep 1.36e+06/6.21e+07 =  2% of the original kernel matrix.

torch.Size([18653, 2])
We keep 3.53e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([29571, 2])
We keep 4.02e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([11048, 2])
We keep 9.04e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([22792, 2])
We keep 2.38e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([4737, 2])
We keep 1.72e+05/3.57e+06 =  4% of the original kernel matrix.

torch.Size([16050, 2])
We keep 1.18e+06/5.01e+07 =  2% of the original kernel matrix.

torch.Size([24626, 2])
We keep 4.56e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([34097, 2])
We keep 5.12e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([25421, 2])
We keep 5.59e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([35000, 2])
We keep 5.51e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([6216, 2])
We keep 2.93e+05/6.77e+06 =  4% of the original kernel matrix.

torch.Size([17880, 2])
We keep 1.48e+06/6.90e+07 =  2% of the original kernel matrix.

torch.Size([12394, 2])
We keep 1.54e+06/3.94e+07 =  3% of the original kernel matrix.

torch.Size([24052, 2])
We keep 2.78e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([6743, 2])
We keep 5.31e+05/9.70e+06 =  5% of the original kernel matrix.

torch.Size([18083, 2])
We keep 1.68e+06/8.26e+07 =  2% of the original kernel matrix.

torch.Size([15285, 2])
We keep 1.47e+06/5.12e+07 =  2% of the original kernel matrix.

torch.Size([26786, 2])
We keep 3.07e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([13199, 2])
We keep 1.53e+06/4.26e+07 =  3% of the original kernel matrix.

torch.Size([24681, 2])
We keep 2.87e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([7448, 2])
We keep 3.99e+05/1.02e+07 =  3% of the original kernel matrix.

torch.Size([19380, 2])
We keep 1.70e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([10064, 2])
We keep 6.58e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([21778, 2])
We keep 2.10e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([9479, 2])
We keep 6.59e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([21356, 2])
We keep 2.10e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([5904, 2])
We keep 2.93e+05/6.37e+06 =  4% of the original kernel matrix.

torch.Size([17581, 2])
We keep 1.44e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([15399, 2])
We keep 1.46e+06/5.38e+07 =  2% of the original kernel matrix.

torch.Size([26936, 2])
We keep 3.13e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([6055, 2])
We keep 3.62e+05/7.24e+06 =  5% of the original kernel matrix.

torch.Size([17842, 2])
We keep 1.53e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([20928, 2])
We keep 3.35e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([31966, 2])
We keep 4.54e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([14900, 2])
We keep 1.36e+06/4.89e+07 =  2% of the original kernel matrix.

torch.Size([26145, 2])
We keep 3.00e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([7705, 2])
We keep 4.63e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([19396, 2])
We keep 1.78e+06/9.13e+07 =  1% of the original kernel matrix.

torch.Size([8067, 2])
We keep 4.93e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([19968, 2])
We keep 1.81e+06/9.15e+07 =  1% of the original kernel matrix.

torch.Size([11482, 2])
We keep 7.79e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([23361, 2])
We keep 2.31e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([11783, 2])
We keep 1.13e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([23140, 2])
We keep 2.63e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([12409, 2])
We keep 1.10e+06/3.33e+07 =  3% of the original kernel matrix.

torch.Size([24011, 2])
We keep 2.61e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([5163, 2])
We keep 2.11e+05/4.30e+06 =  4% of the original kernel matrix.

torch.Size([16501, 2])
We keep 1.26e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([7467, 2])
We keep 5.03e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([19080, 2])
We keep 1.69e+06/8.60e+07 =  1% of the original kernel matrix.

torch.Size([7023, 2])
We keep 3.74e+05/9.28e+06 =  4% of the original kernel matrix.

torch.Size([18422, 2])
We keep 1.62e+06/8.08e+07 =  2% of the original kernel matrix.

torch.Size([9098, 2])
We keep 4.92e+05/1.47e+07 =  3% of the original kernel matrix.

torch.Size([20949, 2])
We keep 1.91e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([4791, 2])
We keep 2.02e+05/3.88e+06 =  5% of the original kernel matrix.

torch.Size([16003, 2])
We keep 1.20e+06/5.23e+07 =  2% of the original kernel matrix.

torch.Size([8842, 2])
We keep 6.47e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([20579, 2])
We keep 2.00e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([12715, 2])
We keep 1.11e+06/3.52e+07 =  3% of the original kernel matrix.

torch.Size([24222, 2])
We keep 2.67e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([5967, 2])
We keep 3.60e+05/7.06e+06 =  5% of the original kernel matrix.

torch.Size([17809, 2])
We keep 1.51e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([2900, 2])
We keep 7.00e+04/1.07e+06 =  6% of the original kernel matrix.

torch.Size([13569, 2])
We keep 7.99e+05/2.75e+07 =  2% of the original kernel matrix.

torch.Size([8122, 2])
We keep 4.17e+05/1.16e+07 =  3% of the original kernel matrix.

torch.Size([19825, 2])
We keep 1.74e+06/9.04e+07 =  1% of the original kernel matrix.

torch.Size([20599, 2])
We keep 3.14e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([30864, 2])
We keep 4.52e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([11240, 2])
We keep 8.38e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([22931, 2])
We keep 2.30e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([3806, 2])
We keep 1.43e+05/2.48e+06 =  5% of the original kernel matrix.

torch.Size([14450, 2])
We keep 1.04e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([15091, 2])
We keep 2.10e+06/6.54e+07 =  3% of the original kernel matrix.

torch.Size([26602, 2])
We keep 3.43e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([4461, 2])
We keep 1.73e+05/3.33e+06 =  5% of the original kernel matrix.

torch.Size([15538, 2])
We keep 1.15e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([6604, 2])
We keep 3.26e+05/7.99e+06 =  4% of the original kernel matrix.

torch.Size([18470, 2])
We keep 1.55e+06/7.50e+07 =  2% of the original kernel matrix.

torch.Size([4992, 2])
We keep 1.65e+05/3.56e+06 =  4% of the original kernel matrix.

torch.Size([16655, 2])
We keep 1.19e+06/5.01e+07 =  2% of the original kernel matrix.

torch.Size([10951, 2])
We keep 8.37e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([22846, 2])
We keep 2.32e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([4983, 2])
We keep 2.31e+05/4.28e+06 =  5% of the original kernel matrix.

torch.Size([16309, 2])
We keep 1.26e+06/5.49e+07 =  2% of the original kernel matrix.

torch.Size([10222, 2])
We keep 7.42e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([22098, 2])
We keep 2.16e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([9897, 2])
We keep 6.52e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([22002, 2])
We keep 2.08e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([10231, 2])
We keep 6.50e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([22048, 2])
We keep 2.14e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([5557, 2])
We keep 2.32e+05/5.17e+06 =  4% of the original kernel matrix.

torch.Size([17099, 2])
We keep 1.34e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([4163, 2])
We keep 1.70e+05/3.12e+06 =  5% of the original kernel matrix.

torch.Size([15237, 2])
We keep 1.14e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([9987, 2])
We keep 7.52e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([21947, 2])
We keep 2.23e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([9459, 2])
We keep 9.50e+05/2.23e+07 =  4% of the original kernel matrix.

torch.Size([20986, 2])
We keep 2.28e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([10272, 2])
We keep 7.17e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([22070, 2])
We keep 2.24e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([9014, 2])
We keep 6.22e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([20955, 2])
We keep 2.06e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([15100, 2])
We keep 2.50e+06/6.98e+07 =  3% of the original kernel matrix.

torch.Size([26187, 2])
We keep 3.40e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([9476, 2])
We keep 7.34e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([21413, 2])
We keep 2.15e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([7763, 2])
We keep 4.11e+05/1.10e+07 =  3% of the original kernel matrix.

torch.Size([19711, 2])
We keep 1.76e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([28450, 2])
We keep 4.50e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([36968, 2])
We keep 5.58e+06/4.19e+08 =  1% of the original kernel matrix.

torch.Size([12482, 2])
We keep 9.53e+05/3.07e+07 =  3% of the original kernel matrix.

torch.Size([24063, 2])
We keep 2.52e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([16285, 2])
We keep 1.60e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([27337, 2])
We keep 3.29e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([4663, 2])
We keep 2.05e+05/3.97e+06 =  5% of the original kernel matrix.

torch.Size([15840, 2])
We keep 1.24e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([8897, 2])
We keep 6.00e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([20599, 2])
We keep 2.02e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([5706, 2])
We keep 3.23e+05/6.09e+06 =  5% of the original kernel matrix.

torch.Size([17300, 2])
We keep 1.45e+06/6.55e+07 =  2% of the original kernel matrix.

torch.Size([17629, 2])
We keep 1.89e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([28713, 2])
We keep 3.58e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([2653, 2])
We keep 5.77e+04/8.48e+05 =  6% of the original kernel matrix.

torch.Size([13129, 2])
We keep 7.36e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([8376, 2])
We keep 5.25e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([20251, 2])
We keep 1.92e+06/9.92e+07 =  1% of the original kernel matrix.

torch.Size([4702, 2])
We keep 2.06e+05/3.87e+06 =  5% of the original kernel matrix.

torch.Size([15956, 2])
We keep 1.22e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([5068, 2])
We keep 2.02e+05/4.10e+06 =  4% of the original kernel matrix.

torch.Size([16360, 2])
We keep 1.24e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([4052, 2])
We keep 1.24e+05/2.36e+06 =  5% of the original kernel matrix.

torch.Size([15195, 2])
We keep 1.02e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([4827, 2])
We keep 2.32e+05/4.20e+06 =  5% of the original kernel matrix.

torch.Size([15991, 2])
We keep 1.26e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([10599, 2])
We keep 1.03e+06/2.61e+07 =  3% of the original kernel matrix.

torch.Size([22493, 2])
We keep 2.43e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([9838, 2])
We keep 9.89e+05/2.38e+07 =  4% of the original kernel matrix.

torch.Size([21538, 2])
We keep 2.33e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([5004, 2])
We keep 2.49e+05/4.86e+06 =  5% of the original kernel matrix.

torch.Size([16036, 2])
We keep 1.32e+06/5.85e+07 =  2% of the original kernel matrix.

torch.Size([6498, 2])
We keep 3.49e+05/7.40e+06 =  4% of the original kernel matrix.

torch.Size([18307, 2])
We keep 1.52e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([4156, 2])
We keep 1.22e+05/2.38e+06 =  5% of the original kernel matrix.

torch.Size([15414, 2])
We keep 1.02e+06/4.09e+07 =  2% of the original kernel matrix.

torch.Size([16172, 2])
We keep 1.95e+06/7.09e+07 =  2% of the original kernel matrix.

torch.Size([27669, 2])
We keep 3.49e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([5539, 2])
We keep 2.65e+05/5.10e+06 =  5% of the original kernel matrix.

torch.Size([16978, 2])
We keep 1.34e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([23662, 2])
We keep 3.48e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([33532, 2])
We keep 4.79e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([8027, 2])
We keep 4.76e+05/1.23e+07 =  3% of the original kernel matrix.

torch.Size([19703, 2])
We keep 1.81e+06/9.30e+07 =  1% of the original kernel matrix.

torch.Size([5680, 2])
We keep 3.08e+05/5.91e+06 =  5% of the original kernel matrix.

torch.Size([17018, 2])
We keep 1.40e+06/6.45e+07 =  2% of the original kernel matrix.

torch.Size([8018, 2])
We keep 5.45e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([20101, 2])
We keep 1.83e+06/9.22e+07 =  1% of the original kernel matrix.

torch.Size([5463, 2])
We keep 1.89e+05/4.23e+06 =  4% of the original kernel matrix.

torch.Size([17253, 2])
We keep 1.25e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([6366, 2])
We keep 2.83e+05/6.65e+06 =  4% of the original kernel matrix.

torch.Size([18009, 2])
We keep 1.43e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([6365, 2])
We keep 2.69e+05/6.16e+06 =  4% of the original kernel matrix.

torch.Size([18178, 2])
We keep 1.42e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([6274, 2])
We keep 3.31e+05/7.05e+06 =  4% of the original kernel matrix.

torch.Size([17872, 2])
We keep 1.50e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([24428, 2])
We keep 3.64e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([34181, 2])
We keep 4.99e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([7392, 2])
We keep 3.55e+05/9.13e+06 =  3% of the original kernel matrix.

torch.Size([19057, 2])
We keep 1.61e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([15321, 2])
We keep 1.60e+06/5.39e+07 =  2% of the original kernel matrix.

torch.Size([26748, 2])
We keep 3.11e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([4600, 2])
We keep 1.91e+05/3.43e+06 =  5% of the original kernel matrix.

torch.Size([15890, 2])
We keep 1.17e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([10840, 2])
We keep 9.41e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([22506, 2])
We keep 2.38e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([3872, 2])
We keep 1.58e+05/2.64e+06 =  5% of the original kernel matrix.

torch.Size([14661, 2])
We keep 1.07e+06/4.31e+07 =  2% of the original kernel matrix.

torch.Size([8120, 2])
We keep 4.85e+05/1.26e+07 =  3% of the original kernel matrix.

torch.Size([19913, 2])
We keep 1.83e+06/9.41e+07 =  1% of the original kernel matrix.

torch.Size([6510, 2])
We keep 2.99e+05/7.12e+06 =  4% of the original kernel matrix.

torch.Size([18256, 2])
We keep 1.50e+06/7.08e+07 =  2% of the original kernel matrix.

torch.Size([3984, 2])
We keep 1.44e+05/2.46e+06 =  5% of the original kernel matrix.

torch.Size([15238, 2])
We keep 1.07e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([5829, 2])
We keep 2.60e+05/5.42e+06 =  4% of the original kernel matrix.

torch.Size([17489, 2])
We keep 1.36e+06/6.18e+07 =  2% of the original kernel matrix.

torch.Size([5271, 2])
We keep 2.41e+05/4.92e+06 =  4% of the original kernel matrix.

torch.Size([16596, 2])
We keep 1.32e+06/5.88e+07 =  2% of the original kernel matrix.

torch.Size([7948, 2])
We keep 6.01e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([19775, 2])
We keep 1.86e+06/9.51e+07 =  1% of the original kernel matrix.

torch.Size([9508, 2])
We keep 6.78e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([21128, 2])
We keep 2.12e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([4652, 2])
We keep 1.65e+05/3.33e+06 =  4% of the original kernel matrix.

torch.Size([16000, 2])
We keep 1.14e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([19281, 2])
We keep 3.14e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([30163, 2])
We keep 4.22e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([24354, 2])
We keep 4.38e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([34340, 2])
We keep 5.05e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([13211, 2])
We keep 1.23e+06/3.94e+07 =  3% of the original kernel matrix.

torch.Size([24941, 2])
We keep 2.79e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([7873, 2])
We keep 3.67e+05/9.77e+06 =  3% of the original kernel matrix.

torch.Size([19746, 2])
We keep 1.67e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([5436, 2])
We keep 1.94e+05/4.40e+06 =  4% of the original kernel matrix.

torch.Size([17037, 2])
We keep 1.26e+06/5.57e+07 =  2% of the original kernel matrix.

torch.Size([7779, 2])
We keep 6.22e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([19745, 2])
We keep 1.89e+06/9.53e+07 =  1% of the original kernel matrix.

torch.Size([5543, 2])
We keep 2.23e+05/4.86e+06 =  4% of the original kernel matrix.

torch.Size([17032, 2])
We keep 1.30e+06/5.85e+07 =  2% of the original kernel matrix.

torch.Size([6860, 2])
We keep 3.73e+05/8.39e+06 =  4% of the original kernel matrix.

torch.Size([18678, 2])
We keep 1.60e+06/7.68e+07 =  2% of the original kernel matrix.

torch.Size([5314, 2])
We keep 2.38e+05/5.17e+06 =  4% of the original kernel matrix.

torch.Size([16561, 2])
We keep 1.34e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([10998, 2])
We keep 9.24e+05/2.63e+07 =  3% of the original kernel matrix.

torch.Size([22886, 2])
We keep 2.43e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([11057, 2])
We keep 8.52e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([22922, 2])
We keep 2.31e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([22904, 2])
We keep 5.97e+06/1.99e+08 =  3% of the original kernel matrix.

torch.Size([32932, 2])
We keep 5.20e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([7650, 2])
We keep 4.31e+05/1.09e+07 =  3% of the original kernel matrix.

torch.Size([19369, 2])
We keep 1.74e+06/8.74e+07 =  1% of the original kernel matrix.

torch.Size([23239, 2])
We keep 4.57e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([32965, 2])
We keep 4.87e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([9940, 2])
We keep 5.78e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([21987, 2])
We keep 2.05e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([7896, 2])
We keep 5.34e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([19611, 2])
We keep 1.81e+06/9.21e+07 =  1% of the original kernel matrix.

torch.Size([11475, 2])
We keep 8.53e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([23344, 2])
We keep 2.40e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([2544, 2])
We keep 5.02e+04/7.66e+05 =  6% of the original kernel matrix.

torch.Size([12879, 2])
We keep 7.21e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([16439, 2])
We keep 2.13e+06/7.01e+07 =  3% of the original kernel matrix.

torch.Size([28204, 2])
We keep 3.53e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([6059, 2])
We keep 3.34e+05/6.76e+06 =  4% of the original kernel matrix.

torch.Size([17508, 2])
We keep 1.47e+06/6.90e+07 =  2% of the original kernel matrix.

torch.Size([9672, 2])
We keep 7.41e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([21408, 2])
We keep 2.18e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([4066, 2])
We keep 1.66e+05/2.72e+06 =  6% of the original kernel matrix.

torch.Size([15025, 2])
We keep 1.10e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([6615, 2])
We keep 4.16e+05/8.74e+06 =  4% of the original kernel matrix.

torch.Size([18241, 2])
We keep 1.63e+06/7.85e+07 =  2% of the original kernel matrix.

torch.Size([4674, 2])
We keep 1.50e+05/3.17e+06 =  4% of the original kernel matrix.

torch.Size([16058, 2])
We keep 1.12e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([10287, 2])
We keep 8.67e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([22250, 2])
We keep 2.17e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([9499, 2])
We keep 6.17e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([21350, 2])
We keep 2.02e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([9730, 2])
We keep 6.01e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([21526, 2])
We keep 2.06e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([2409, 2])
We keep 5.02e+04/7.45e+05 =  6% of the original kernel matrix.

torch.Size([12404, 2])
We keep 6.86e+05/2.29e+07 =  2% of the original kernel matrix.

torch.Size([3906, 2])
We keep 1.36e+05/2.44e+06 =  5% of the original kernel matrix.

torch.Size([14834, 2])
We keep 1.02e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([4040, 2])
We keep 1.52e+05/2.91e+06 =  5% of the original kernel matrix.

torch.Size([15014, 2])
We keep 1.11e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([15315, 2])
We keep 1.34e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([26688, 2])
We keep 3.02e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([7127, 2])
We keep 6.31e+05/1.21e+07 =  5% of the original kernel matrix.

torch.Size([18626, 2])
We keep 1.81e+06/9.23e+07 =  1% of the original kernel matrix.

torch.Size([11920, 2])
We keep 1.33e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([23689, 2])
We keep 2.69e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([8587, 2])
We keep 6.75e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([20859, 2])
We keep 1.97e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([12071, 2])
We keep 1.00e+06/3.03e+07 =  3% of the original kernel matrix.

torch.Size([23683, 2])
We keep 2.52e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([8818, 2])
We keep 5.31e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([20566, 2])
We keep 1.91e+06/9.99e+07 =  1% of the original kernel matrix.

torch.Size([12708, 2])
We keep 1.66e+06/4.18e+07 =  3% of the original kernel matrix.

torch.Size([24301, 2])
We keep 2.84e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([6944, 2])
We keep 3.54e+05/8.07e+06 =  4% of the original kernel matrix.

torch.Size([18610, 2])
We keep 1.56e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([8330, 2])
We keep 5.03e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([20354, 2])
We keep 1.88e+06/9.52e+07 =  1% of the original kernel matrix.

torch.Size([7028, 2])
We keep 4.08e+05/9.17e+06 =  4% of the original kernel matrix.

torch.Size([18555, 2])
We keep 1.64e+06/8.03e+07 =  2% of the original kernel matrix.

torch.Size([7684, 2])
We keep 7.23e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([19242, 2])
We keep 1.94e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([5912, 2])
We keep 2.40e+05/5.49e+06 =  4% of the original kernel matrix.

torch.Size([17477, 2])
We keep 1.35e+06/6.22e+07 =  2% of the original kernel matrix.

torch.Size([16719, 2])
We keep 2.11e+06/7.33e+07 =  2% of the original kernel matrix.

torch.Size([27838, 2])
We keep 3.54e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([15606, 2])
We keep 1.50e+06/5.85e+07 =  2% of the original kernel matrix.

torch.Size([26940, 2])
We keep 3.25e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([7870, 2])
We keep 4.00e+05/1.01e+07 =  3% of the original kernel matrix.

torch.Size([19793, 2])
We keep 1.68e+06/8.42e+07 =  1% of the original kernel matrix.

torch.Size([10311, 2])
We keep 9.44e+05/2.42e+07 =  3% of the original kernel matrix.

torch.Size([22094, 2])
We keep 2.34e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([13030, 2])
We keep 1.12e+06/3.78e+07 =  2% of the original kernel matrix.

torch.Size([24651, 2])
We keep 2.75e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([27793, 2])
We keep 7.71e+06/2.97e+08 =  2% of the original kernel matrix.

torch.Size([36633, 2])
We keep 6.15e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([5312, 2])
We keep 2.18e+05/4.52e+06 =  4% of the original kernel matrix.

torch.Size([16705, 2])
We keep 1.27e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([1999, 2])
We keep 4.11e+04/6.19e+05 =  6% of the original kernel matrix.

torch.Size([11648, 2])
We keep 6.66e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([12789, 2])
We keep 9.89e+05/3.27e+07 =  3% of the original kernel matrix.

torch.Size([24400, 2])
We keep 2.58e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([6123, 2])
We keep 2.85e+05/6.41e+06 =  4% of the original kernel matrix.

torch.Size([17725, 2])
We keep 1.43e+06/6.72e+07 =  2% of the original kernel matrix.

torch.Size([7454, 2])
We keep 5.45e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([19114, 2])
We keep 1.78e+06/8.94e+07 =  1% of the original kernel matrix.

torch.Size([9174, 2])
We keep 5.46e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([21086, 2])
We keep 1.98e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([6231, 2])
We keep 3.07e+05/7.34e+06 =  4% of the original kernel matrix.

torch.Size([17872, 2])
We keep 1.52e+06/7.19e+07 =  2% of the original kernel matrix.

torch.Size([4516, 2])
We keep 2.13e+05/3.51e+06 =  6% of the original kernel matrix.

torch.Size([15734, 2])
We keep 1.19e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([7262, 2])
We keep 3.65e+05/9.52e+06 =  3% of the original kernel matrix.

torch.Size([18980, 2])
We keep 1.64e+06/8.19e+07 =  2% of the original kernel matrix.

torch.Size([4487, 2])
We keep 1.45e+05/2.95e+06 =  4% of the original kernel matrix.

torch.Size([15951, 2])
We keep 1.10e+06/4.56e+07 =  2% of the original kernel matrix.

torch.Size([11334, 2])
We keep 7.96e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([23264, 2])
We keep 2.35e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([4285, 2])
We keep 1.55e+05/3.26e+06 =  4% of the original kernel matrix.

torch.Size([15554, 2])
We keep 1.16e+06/4.79e+07 =  2% of the original kernel matrix.

torch.Size([16261, 2])
We keep 1.79e+06/6.99e+07 =  2% of the original kernel matrix.

torch.Size([27407, 2])
We keep 3.46e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([12394, 2])
We keep 9.91e+05/3.06e+07 =  3% of the original kernel matrix.

torch.Size([24223, 2])
We keep 2.53e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([8766, 2])
We keep 5.83e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([20748, 2])
We keep 2.03e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([3555, 2])
We keep 9.03e+04/1.65e+06 =  5% of the original kernel matrix.

torch.Size([14447, 2])
We keep 9.02e+05/3.41e+07 =  2% of the original kernel matrix.

torch.Size([11386, 2])
We keep 1.25e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([23561, 2])
We keep 2.66e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([8442, 2])
We keep 6.84e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([20046, 2])
We keep 1.95e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([11690, 2])
We keep 9.68e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([23380, 2])
We keep 2.44e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([15997, 2])
We keep 1.62e+06/6.10e+07 =  2% of the original kernel matrix.

torch.Size([27210, 2])
We keep 3.26e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([9830, 2])
We keep 7.52e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([21417, 2])
We keep 2.19e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([8303, 2])
We keep 7.13e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([19965, 2])
We keep 1.95e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([7985, 2])
We keep 4.35e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([19723, 2])
We keep 1.74e+06/8.84e+07 =  1% of the original kernel matrix.

torch.Size([6772, 2])
We keep 2.96e+05/7.42e+06 =  3% of the original kernel matrix.

torch.Size([18626, 2])
We keep 1.51e+06/7.23e+07 =  2% of the original kernel matrix.

torch.Size([3351, 2])
We keep 1.00e+05/1.60e+06 =  6% of the original kernel matrix.

torch.Size([14146, 2])
We keep 9.10e+05/3.35e+07 =  2% of the original kernel matrix.

torch.Size([3942, 2])
We keep 1.61e+05/2.82e+06 =  5% of the original kernel matrix.

torch.Size([14696, 2])
We keep 1.09e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([4201, 2])
We keep 1.40e+05/2.55e+06 =  5% of the original kernel matrix.

torch.Size([15587, 2])
We keep 1.05e+06/4.24e+07 =  2% of the original kernel matrix.

torch.Size([2412, 2])
We keep 6.07e+04/9.16e+05 =  6% of the original kernel matrix.

torch.Size([12450, 2])
We keep 7.45e+05/2.54e+07 =  2% of the original kernel matrix.

torch.Size([9716, 2])
We keep 6.57e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([21515, 2])
We keep 2.05e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([7406, 2])
We keep 3.29e+05/8.32e+06 =  3% of the original kernel matrix.

torch.Size([19447, 2])
We keep 1.59e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([5844, 2])
We keep 3.87e+05/7.40e+06 =  5% of the original kernel matrix.

torch.Size([17235, 2])
We keep 1.50e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([5969, 2])
We keep 3.42e+05/6.16e+06 =  5% of the original kernel matrix.

torch.Size([17781, 2])
We keep 1.43e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([9474, 2])
We keep 7.13e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([21102, 2])
We keep 2.11e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([7596, 2])
We keep 5.30e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([19203, 2])
We keep 1.77e+06/9.00e+07 =  1% of the original kernel matrix.

torch.Size([9333, 2])
We keep 7.34e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([21116, 2])
We keep 2.12e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([9212, 2])
We keep 6.47e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([21030, 2])
We keep 2.04e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([4551, 2])
We keep 1.82e+05/3.91e+06 =  4% of the original kernel matrix.

torch.Size([15611, 2])
We keep 1.21e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([4349, 2])
We keep 1.68e+05/3.01e+06 =  5% of the original kernel matrix.

torch.Size([15558, 2])
We keep 1.11e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([10935, 2])
We keep 8.36e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([22674, 2])
We keep 2.32e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([2493, 2])
We keep 5.36e+04/7.85e+05 =  6% of the original kernel matrix.

torch.Size([12701, 2])
We keep 7.17e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([4699, 2])
We keep 1.78e+05/3.30e+06 =  5% of the original kernel matrix.

torch.Size([16195, 2])
We keep 1.16e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([6896, 2])
We keep 4.21e+05/9.31e+06 =  4% of the original kernel matrix.

torch.Size([18593, 2])
We keep 1.66e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([10157, 2])
We keep 6.85e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([22012, 2])
We keep 2.14e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([12702, 2])
We keep 1.27e+06/3.89e+07 =  3% of the original kernel matrix.

torch.Size([24685, 2])
We keep 2.80e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([4613, 2])
We keep 2.06e+05/4.15e+06 =  4% of the original kernel matrix.

torch.Size([15918, 2])
We keep 1.28e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([5225, 2])
We keep 2.62e+05/5.18e+06 =  5% of the original kernel matrix.

torch.Size([16444, 2])
We keep 1.35e+06/6.04e+07 =  2% of the original kernel matrix.

torch.Size([8007, 2])
We keep 4.72e+05/1.22e+07 =  3% of the original kernel matrix.

torch.Size([19975, 2])
We keep 1.81e+06/9.25e+07 =  1% of the original kernel matrix.

torch.Size([16787, 2])
We keep 1.81e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([28150, 2])
We keep 3.45e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([5576, 2])
We keep 2.44e+05/5.32e+06 =  4% of the original kernel matrix.

torch.Size([17161, 2])
We keep 1.35e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([20931, 2])
We keep 3.28e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([31484, 2])
We keep 4.37e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([73547, 2])
We keep 1.15e+08/3.93e+09 =  2% of the original kernel matrix.

torch.Size([57201, 2])
We keep 1.74e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([23375, 2])
We keep 9.80e+06/2.19e+08 =  4% of the original kernel matrix.

torch.Size([33305, 2])
We keep 5.14e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([11942, 2])
We keep 9.25e+05/2.93e+07 =  3% of the original kernel matrix.

torch.Size([23602, 2])
We keep 2.46e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([2385, 2])
We keep 5.42e+04/7.87e+05 =  6% of the original kernel matrix.

torch.Size([12244, 2])
We keep 7.10e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([14217, 2])
We keep 1.36e+06/4.64e+07 =  2% of the original kernel matrix.

torch.Size([25723, 2])
We keep 2.94e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([108473, 2])
We keep 1.61e+09/1.07e+10 = 15% of the original kernel matrix.

torch.Size([70071, 2])
We keep 2.69e+07/2.74e+09 =  0% of the original kernel matrix.

torch.Size([306088, 2])
We keep 2.23e+09/7.43e+10 =  3% of the original kernel matrix.

torch.Size([118635, 2])
We keep 6.27e+07/7.23e+09 =  0% of the original kernel matrix.

torch.Size([8872, 2])
We keep 5.58e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([20928, 2])
We keep 1.95e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([138732, 2])
We keep 1.64e+08/9.22e+09 =  1% of the original kernel matrix.

torch.Size([79394, 2])
We keep 2.49e+07/2.55e+09 =  0% of the original kernel matrix.

torch.Size([48515, 2])
We keep 2.19e+07/1.05e+09 =  2% of the original kernel matrix.

torch.Size([46926, 2])
We keep 9.82e+06/8.61e+08 =  1% of the original kernel matrix.

torch.Size([31341, 2])
We keep 1.93e+07/5.00e+08 =  3% of the original kernel matrix.

torch.Size([38508, 2])
We keep 7.56e+06/5.93e+08 =  1% of the original kernel matrix.

torch.Size([226122, 2])
We keep 5.86e+08/2.79e+10 =  2% of the original kernel matrix.

torch.Size([106435, 2])
We keep 3.86e+07/4.43e+09 =  0% of the original kernel matrix.

torch.Size([5326, 2])
We keep 2.15e+05/4.68e+06 =  4% of the original kernel matrix.

torch.Size([16982, 2])
We keep 1.30e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([11531, 2])
We keep 1.40e+06/3.20e+07 =  4% of the original kernel matrix.

torch.Size([23300, 2])
We keep 2.52e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([379708, 2])
We keep 6.20e+08/5.99e+10 =  1% of the original kernel matrix.

torch.Size([135620, 2])
We keep 5.61e+07/6.50e+09 =  0% of the original kernel matrix.

torch.Size([738155, 2])
We keep 1.42e+09/1.80e+11 =  0% of the original kernel matrix.

torch.Size([191625, 2])
We keep 9.02e+07/1.13e+10 =  0% of the original kernel matrix.

torch.Size([11417, 2])
We keep 1.02e+06/2.89e+07 =  3% of the original kernel matrix.

torch.Size([23261, 2])
We keep 2.45e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([50878, 2])
We keep 1.60e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([49312, 2])
We keep 1.02e+07/8.70e+08 =  1% of the original kernel matrix.

torch.Size([86061, 2])
We keep 2.58e+09/5.78e+09 = 44% of the original kernel matrix.

torch.Size([61042, 2])
We keep 1.81e+07/2.02e+09 =  0% of the original kernel matrix.

torch.Size([84289, 2])
We keep 1.78e+08/3.26e+09 =  5% of the original kernel matrix.

torch.Size([61952, 2])
We keep 1.59e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([76038, 2])
We keep 1.10e+08/3.02e+09 =  3% of the original kernel matrix.

torch.Size([57340, 2])
We keep 1.53e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([67092, 2])
We keep 5.56e+07/2.63e+09 =  2% of the original kernel matrix.

torch.Size([53914, 2])
We keep 1.43e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([67462, 2])
We keep 4.12e+07/1.82e+09 =  2% of the original kernel matrix.

torch.Size([56175, 2])
We keep 1.25e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([26608, 2])
We keep 5.51e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([35835, 2])
We keep 5.54e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([7506, 2])
We keep 5.36e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([19203, 2])
We keep 1.67e+06/8.75e+07 =  1% of the original kernel matrix.

torch.Size([513608, 2])
We keep 4.30e+09/2.58e+11 =  1% of the original kernel matrix.

torch.Size([151483, 2])
We keep 1.10e+08/1.35e+10 =  0% of the original kernel matrix.

torch.Size([8886, 2])
We keep 6.19e+05/1.60e+07 =  3% of the original kernel matrix.

torch.Size([20690, 2])
We keep 1.99e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([6134, 2])
We keep 2.93e+05/6.92e+06 =  4% of the original kernel matrix.

torch.Size([17934, 2])
We keep 1.50e+06/6.98e+07 =  2% of the original kernel matrix.

torch.Size([81340, 2])
We keep 6.14e+07/2.73e+09 =  2% of the original kernel matrix.

torch.Size([60555, 2])
We keep 1.44e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([46686, 2])
We keep 3.22e+07/1.27e+09 =  2% of the original kernel matrix.

torch.Size([44887, 2])
We keep 1.08e+07/9.47e+08 =  1% of the original kernel matrix.

torch.Size([502218, 2])
We keep 1.32e+09/1.10e+11 =  1% of the original kernel matrix.

torch.Size([156548, 2])
We keep 7.41e+07/8.80e+09 =  0% of the original kernel matrix.

torch.Size([197225, 2])
We keep 1.70e+08/1.50e+10 =  1% of the original kernel matrix.

torch.Size([96787, 2])
We keep 3.02e+07/3.25e+09 =  0% of the original kernel matrix.

torch.Size([4313, 2])
We keep 1.20e+05/2.48e+06 =  4% of the original kernel matrix.

torch.Size([15596, 2])
We keep 1.04e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([37161, 2])
We keep 2.58e+07/5.01e+08 =  5% of the original kernel matrix.

torch.Size([42232, 2])
We keep 7.52e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([18848, 2])
We keep 3.08e+06/9.95e+07 =  3% of the original kernel matrix.

torch.Size([29500, 2])
We keep 3.93e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([51063, 2])
We keep 5.83e+07/1.51e+09 =  3% of the original kernel matrix.

torch.Size([47947, 2])
We keep 1.16e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([129598, 2])
We keep 8.00e+07/6.13e+09 =  1% of the original kernel matrix.

torch.Size([77669, 2])
We keep 2.04e+07/2.08e+09 =  0% of the original kernel matrix.

torch.Size([20455, 2])
We keep 5.96e+07/1.77e+08 = 33% of the original kernel matrix.

torch.Size([30719, 2])
We keep 4.56e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([72973, 2])
We keep 2.69e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([57959, 2])
We keep 1.23e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([2299, 2])
We keep 4.66e+04/7.01e+05 =  6% of the original kernel matrix.

torch.Size([12096, 2])
We keep 6.92e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([13756, 2])
We keep 1.28e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([25370, 2])
We keep 2.89e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([84297, 2])
We keep 1.06e+08/4.68e+09 =  2% of the original kernel matrix.

torch.Size([59264, 2])
We keep 1.86e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([99450, 2])
We keep 9.90e+07/5.08e+09 =  1% of the original kernel matrix.

torch.Size([66539, 2])
We keep 1.88e+07/1.89e+09 =  0% of the original kernel matrix.

torch.Size([54292, 2])
We keep 2.78e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([48788, 2])
We keep 1.16e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([21833, 2])
We keep 1.55e+07/1.78e+08 =  8% of the original kernel matrix.

torch.Size([31886, 2])
We keep 4.93e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([426429, 2])
We keep 5.39e+08/8.09e+10 =  0% of the original kernel matrix.

torch.Size([144865, 2])
We keep 6.32e+07/7.55e+09 =  0% of the original kernel matrix.

torch.Size([28788, 2])
We keep 4.44e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([37229, 2])
We keep 5.48e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([43629, 2])
We keep 9.67e+06/7.04e+08 =  1% of the original kernel matrix.

torch.Size([46178, 2])
We keep 8.50e+06/7.04e+08 =  1% of the original kernel matrix.

torch.Size([110764, 2])
We keep 8.06e+07/5.23e+09 =  1% of the original kernel matrix.

torch.Size([71200, 2])
We keep 1.92e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([12072, 2])
We keep 1.18e+06/2.89e+07 =  4% of the original kernel matrix.

torch.Size([23941, 2])
We keep 2.47e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([157035, 2])
We keep 1.87e+08/9.52e+09 =  1% of the original kernel matrix.

torch.Size([84731, 2])
We keep 2.49e+07/2.59e+09 =  0% of the original kernel matrix.

torch.Size([332375, 2])
We keep 1.12e+09/6.71e+10 =  1% of the original kernel matrix.

torch.Size([127711, 2])
We keep 6.00e+07/6.87e+09 =  0% of the original kernel matrix.

torch.Size([390039, 2])
We keep 8.02e+08/7.30e+10 =  1% of the original kernel matrix.

torch.Size([138347, 2])
We keep 6.15e+07/7.17e+09 =  0% of the original kernel matrix.

torch.Size([26714, 2])
We keep 6.70e+06/2.82e+08 =  2% of the original kernel matrix.

torch.Size([35615, 2])
We keep 5.89e+06/4.45e+08 =  1% of the original kernel matrix.

torch.Size([26877, 2])
We keep 5.51e+06/2.35e+08 =  2% of the original kernel matrix.

torch.Size([35406, 2])
We keep 5.39e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([27764, 2])
We keep 7.11e+06/2.99e+08 =  2% of the original kernel matrix.

torch.Size([35577, 2])
We keep 5.91e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([12422, 2])
We keep 8.71e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([24268, 2])
We keep 2.44e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([35522, 2])
We keep 2.66e+07/6.69e+08 =  3% of the original kernel matrix.

torch.Size([40814, 2])
We keep 8.24e+06/6.86e+08 =  1% of the original kernel matrix.

torch.Size([21177, 2])
We keep 3.41e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([31713, 2])
We keep 4.44e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([33240, 2])
We keep 6.78e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([39648, 2])
We keep 6.55e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([185231, 2])
We keep 1.54e+09/2.18e+10 =  7% of the original kernel matrix.

torch.Size([94326, 2])
We keep 3.55e+07/3.92e+09 =  0% of the original kernel matrix.

torch.Size([89225, 2])
We keep 9.53e+07/3.57e+09 =  2% of the original kernel matrix.

torch.Size([63960, 2])
We keep 1.66e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([17613, 2])
We keep 4.26e+06/1.25e+08 =  3% of the original kernel matrix.

torch.Size([28259, 2])
We keep 4.23e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([18765, 2])
We keep 3.03e+06/9.46e+07 =  3% of the original kernel matrix.

torch.Size([29867, 2])
We keep 3.88e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([62645, 2])
We keep 1.90e+08/6.80e+09 =  2% of the original kernel matrix.

torch.Size([51438, 2])
We keep 2.16e+07/2.19e+09 =  0% of the original kernel matrix.

torch.Size([28142, 2])
We keep 7.61e+06/2.87e+08 =  2% of the original kernel matrix.

torch.Size([36521, 2])
We keep 5.78e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([30646, 2])
We keep 4.05e+07/6.41e+08 =  6% of the original kernel matrix.

torch.Size([35915, 2])
We keep 7.69e+06/6.72e+08 =  1% of the original kernel matrix.

torch.Size([24728, 2])
We keep 4.51e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([34908, 2])
We keep 5.05e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([471184, 2])
We keep 2.63e+09/1.40e+11 =  1% of the original kernel matrix.

torch.Size([150757, 2])
We keep 8.33e+07/9.92e+09 =  0% of the original kernel matrix.

torch.Size([19893, 2])
We keep 3.40e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([30516, 2])
We keep 4.03e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([77223, 2])
We keep 4.13e+08/4.60e+09 =  8% of the original kernel matrix.

torch.Size([56879, 2])
We keep 1.70e+07/1.80e+09 =  0% of the original kernel matrix.

torch.Size([12355, 2])
We keep 1.29e+06/3.79e+07 =  3% of the original kernel matrix.

torch.Size([24195, 2])
We keep 2.70e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([49046, 2])
We keep 5.63e+07/2.28e+09 =  2% of the original kernel matrix.

torch.Size([44324, 2])
We keep 1.39e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([40574, 2])
We keep 4.27e+07/6.55e+08 =  6% of the original kernel matrix.

torch.Size([44341, 2])
We keep 8.27e+06/6.79e+08 =  1% of the original kernel matrix.

torch.Size([347120, 2])
We keep 2.74e+09/1.20e+11 =  2% of the original kernel matrix.

torch.Size([118690, 2])
We keep 7.79e+07/9.18e+09 =  0% of the original kernel matrix.

torch.Size([86697, 2])
We keep 3.51e+07/3.32e+09 =  1% of the original kernel matrix.

torch.Size([63145, 2])
We keep 1.59e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([14133, 2])
We keep 1.09e+06/4.02e+07 =  2% of the original kernel matrix.

torch.Size([25631, 2])
We keep 2.77e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([214848, 2])
We keep 1.45e+08/1.70e+10 =  0% of the original kernel matrix.

torch.Size([101241, 2])
We keep 3.20e+07/3.46e+09 =  0% of the original kernel matrix.

torch.Size([203975, 2])
We keep 1.53e+08/1.68e+10 =  0% of the original kernel matrix.

torch.Size([98851, 2])
We keep 3.18e+07/3.44e+09 =  0% of the original kernel matrix.

torch.Size([13396, 2])
We keep 2.36e+06/4.38e+07 =  5% of the original kernel matrix.

torch.Size([25264, 2])
We keep 2.86e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([148059, 2])
We keep 1.74e+08/8.42e+09 =  2% of the original kernel matrix.

torch.Size([82649, 2])
We keep 2.37e+07/2.44e+09 =  0% of the original kernel matrix.

torch.Size([195497, 2])
We keep 1.29e+08/1.52e+10 =  0% of the original kernel matrix.

torch.Size([96827, 2])
We keep 3.03e+07/3.27e+09 =  0% of the original kernel matrix.

torch.Size([514456, 2])
We keep 7.18e+08/9.96e+10 =  0% of the original kernel matrix.

torch.Size([161869, 2])
We keep 7.02e+07/8.37e+09 =  0% of the original kernel matrix.

torch.Size([74607, 2])
We keep 2.05e+08/3.00e+09 =  6% of the original kernel matrix.

torch.Size([58537, 2])
We keep 1.55e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([5489, 2])
We keep 2.30e+05/4.92e+06 =  4% of the original kernel matrix.

torch.Size([16959, 2])
We keep 1.28e+06/5.88e+07 =  2% of the original kernel matrix.

torch.Size([31492, 2])
We keep 5.13e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([39267, 2])
We keep 6.13e+06/4.73e+08 =  1% of the original kernel matrix.

torch.Size([67159, 2])
We keep 1.58e+08/3.23e+09 =  4% of the original kernel matrix.

torch.Size([54757, 2])
We keep 1.60e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([112085, 2])
We keep 3.32e+08/5.39e+09 =  6% of the original kernel matrix.

torch.Size([71334, 2])
We keep 1.93e+07/1.95e+09 =  0% of the original kernel matrix.

torch.Size([33629, 2])
We keep 1.83e+07/4.03e+08 =  4% of the original kernel matrix.

torch.Size([40451, 2])
We keep 6.78e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([9684, 2])
We keep 5.42e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([21449, 2])
We keep 1.98e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([107226, 2])
We keep 7.56e+07/4.44e+09 =  1% of the original kernel matrix.

torch.Size([69996, 2])
We keep 1.79e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([273987, 2])
We keep 2.98e+08/3.35e+10 =  0% of the original kernel matrix.

torch.Size([115873, 2])
We keep 4.32e+07/4.86e+09 =  0% of the original kernel matrix.

torch.Size([69289, 2])
We keep 1.90e+08/5.03e+09 =  3% of the original kernel matrix.

torch.Size([53074, 2])
We keep 1.87e+07/1.88e+09 =  0% of the original kernel matrix.

torch.Size([14341, 2])
We keep 1.85e+06/5.39e+07 =  3% of the original kernel matrix.

torch.Size([25717, 2])
We keep 3.11e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([29960, 2])
We keep 8.33e+06/3.72e+08 =  2% of the original kernel matrix.

torch.Size([37420, 2])
We keep 6.59e+06/5.12e+08 =  1% of the original kernel matrix.

torch.Size([23175, 2])
We keep 3.22e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([33404, 2])
We keep 4.65e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([12367, 2])
We keep 1.06e+06/3.42e+07 =  3% of the original kernel matrix.

torch.Size([24324, 2])
We keep 2.66e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([23630, 2])
We keep 3.14e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([33459, 2])
We keep 4.75e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([15965, 2])
We keep 2.22e+06/6.21e+07 =  3% of the original kernel matrix.

torch.Size([27251, 2])
We keep 3.22e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([57136, 2])
We keep 6.05e+07/1.64e+09 =  3% of the original kernel matrix.

torch.Size([50572, 2])
We keep 1.17e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([83755, 2])
We keep 3.16e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([62261, 2])
We keep 1.41e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([34981, 2])
We keep 1.30e+07/4.74e+08 =  2% of the original kernel matrix.

torch.Size([41582, 2])
We keep 7.29e+06/5.78e+08 =  1% of the original kernel matrix.

torch.Size([39328, 2])
We keep 3.26e+07/7.31e+08 =  4% of the original kernel matrix.

torch.Size([43412, 2])
We keep 8.61e+06/7.17e+08 =  1% of the original kernel matrix.

torch.Size([13283, 2])
We keep 1.86e+06/4.87e+07 =  3% of the original kernel matrix.

torch.Size([24799, 2])
We keep 3.03e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([695410, 2])
We keep 1.78e+09/2.03e+11 =  0% of the original kernel matrix.

torch.Size([187107, 2])
We keep 9.53e+07/1.19e+10 =  0% of the original kernel matrix.

torch.Size([17414, 2])
We keep 1.73e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([28507, 2])
We keep 3.40e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([181244, 2])
We keep 1.28e+08/1.33e+10 =  0% of the original kernel matrix.

torch.Size([92375, 2])
We keep 2.88e+07/3.06e+09 =  0% of the original kernel matrix.

torch.Size([8135, 2])
We keep 1.49e+06/1.56e+07 =  9% of the original kernel matrix.

torch.Size([20245, 2])
We keep 2.01e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([14151, 2])
We keep 1.48e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([25690, 2])
We keep 2.96e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([9484, 2])
We keep 6.17e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([21329, 2])
We keep 2.09e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([57522, 2])
We keep 1.25e+08/1.32e+09 =  9% of the original kernel matrix.

torch.Size([52042, 2])
We keep 1.09e+07/9.64e+08 =  1% of the original kernel matrix.

torch.Size([9673, 2])
We keep 7.67e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([21737, 2])
We keep 2.21e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([143589, 2])
We keep 6.38e+08/1.99e+10 =  3% of the original kernel matrix.

torch.Size([77928, 2])
We keep 3.50e+07/3.74e+09 =  0% of the original kernel matrix.

torch.Size([9419, 2])
We keep 7.52e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([21142, 2])
We keep 2.16e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([13627, 2])
We keep 1.07e+06/3.69e+07 =  2% of the original kernel matrix.

torch.Size([25101, 2])
We keep 2.69e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([324295, 2])
We keep 5.51e+08/5.01e+10 =  1% of the original kernel matrix.

torch.Size([126815, 2])
We keep 5.12e+07/5.94e+09 =  0% of the original kernel matrix.

torch.Size([75692, 2])
We keep 5.05e+07/2.49e+09 =  2% of the original kernel matrix.

torch.Size([59316, 2])
We keep 1.40e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([57863, 2])
We keep 4.46e+07/1.63e+09 =  2% of the original kernel matrix.

torch.Size([52144, 2])
We keep 1.20e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([264752, 2])
We keep 6.36e+08/3.52e+10 =  1% of the original kernel matrix.

torch.Size([115945, 2])
We keep 4.47e+07/4.98e+09 =  0% of the original kernel matrix.

torch.Size([228406, 2])
We keep 6.65e+08/5.15e+10 =  1% of the original kernel matrix.

torch.Size([99122, 2])
We keep 5.30e+07/6.02e+09 =  0% of the original kernel matrix.

torch.Size([8831, 2])
We keep 6.22e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([20810, 2])
We keep 1.96e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([11035, 2])
We keep 6.94e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([22857, 2])
We keep 2.23e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([30313, 2])
We keep 4.19e+07/3.64e+08 = 11% of the original kernel matrix.

torch.Size([37842, 2])
We keep 6.68e+06/5.06e+08 =  1% of the original kernel matrix.

torch.Size([3745, 2])
We keep 1.50e+05/2.50e+06 =  5% of the original kernel matrix.

torch.Size([14558, 2])
We keep 1.05e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([13259, 2])
We keep 1.08e+06/3.67e+07 =  2% of the original kernel matrix.

torch.Size([24692, 2])
We keep 2.68e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([29670, 2])
We keep 9.05e+06/3.84e+08 =  2% of the original kernel matrix.

torch.Size([37434, 2])
We keep 6.64e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([7354, 2])
We keep 6.28e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([19032, 2])
We keep 1.78e+06/9.21e+07 =  1% of the original kernel matrix.

torch.Size([16629, 2])
We keep 1.58e+06/6.50e+07 =  2% of the original kernel matrix.

torch.Size([27653, 2])
We keep 3.32e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([9463, 2])
We keep 5.71e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([21068, 2])
We keep 1.98e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([12059, 2])
We keep 9.06e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([23944, 2])
We keep 2.50e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([530964, 2])
We keep 1.13e+09/1.14e+11 =  0% of the original kernel matrix.

torch.Size([160979, 2])
We keep 7.43e+07/8.95e+09 =  0% of the original kernel matrix.

torch.Size([16821, 2])
We keep 3.26e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([27638, 2])
We keep 3.92e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([118548, 2])
We keep 1.46e+08/6.69e+09 =  2% of the original kernel matrix.

torch.Size([72921, 2])
We keep 2.17e+07/2.17e+09 =  0% of the original kernel matrix.

torch.Size([18825, 2])
We keep 6.62e+06/1.35e+08 =  4% of the original kernel matrix.

torch.Size([29726, 2])
We keep 4.39e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([35899, 2])
We keep 2.77e+07/7.05e+08 =  3% of the original kernel matrix.

torch.Size([40395, 2])
We keep 8.42e+06/7.05e+08 =  1% of the original kernel matrix.

torch.Size([35582, 2])
We keep 1.43e+07/5.70e+08 =  2% of the original kernel matrix.

torch.Size([40903, 2])
We keep 7.75e+06/6.34e+08 =  1% of the original kernel matrix.

torch.Size([1150364, 2])
We keep 8.52e+09/5.91e+11 =  1% of the original kernel matrix.

torch.Size([241422, 2])
We keep 1.55e+08/2.04e+10 =  0% of the original kernel matrix.

torch.Size([1133710, 2])
We keep 2.57e+09/4.50e+11 =  0% of the original kernel matrix.

torch.Size([243710, 2])
We keep 1.38e+08/1.78e+10 =  0% of the original kernel matrix.

torch.Size([45354, 2])
We keep 1.54e+07/6.99e+08 =  2% of the original kernel matrix.

torch.Size([46710, 2])
We keep 8.29e+06/7.01e+08 =  1% of the original kernel matrix.

torch.Size([125705, 2])
We keep 3.63e+08/6.37e+09 =  5% of the original kernel matrix.

torch.Size([75901, 2])
We keep 2.14e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([8979, 2])
We keep 5.54e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([20863, 2])
We keep 1.92e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([32882, 2])
We keep 3.77e+07/7.10e+08 =  5% of the original kernel matrix.

torch.Size([37880, 2])
We keep 8.42e+06/7.07e+08 =  1% of the original kernel matrix.

torch.Size([68133, 2])
We keep 1.42e+08/3.14e+09 =  4% of the original kernel matrix.

torch.Size([54909, 2])
We keep 1.58e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([73516, 2])
We keep 4.23e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([59197, 2])
We keep 1.36e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([13613, 2])
We keep 1.11e+06/3.82e+07 =  2% of the original kernel matrix.

torch.Size([25144, 2])
We keep 2.71e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([121614, 2])
We keep 8.42e+07/6.64e+09 =  1% of the original kernel matrix.

torch.Size([74496, 2])
We keep 2.14e+07/2.16e+09 =  0% of the original kernel matrix.

torch.Size([560790, 2])
We keep 1.99e+09/1.49e+11 =  1% of the original kernel matrix.

torch.Size([164679, 2])
We keep 8.50e+07/1.02e+10 =  0% of the original kernel matrix.

torch.Size([61666, 2])
We keep 2.80e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([51897, 2])
We keep 1.19e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([16251, 2])
We keep 2.54e+06/8.70e+07 =  2% of the original kernel matrix.

torch.Size([27265, 2])
We keep 3.67e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([58928, 2])
We keep 7.56e+07/1.83e+09 =  4% of the original kernel matrix.

torch.Size([52714, 2])
We keep 1.28e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([19646, 2])
We keep 2.46e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([30347, 2])
We keep 4.09e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([11330, 2])
We keep 9.33e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([23423, 2])
We keep 2.35e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([231146, 2])
We keep 3.28e+08/2.28e+10 =  1% of the original kernel matrix.

torch.Size([105832, 2])
We keep 3.67e+07/4.01e+09 =  0% of the original kernel matrix.

torch.Size([49647, 2])
We keep 2.42e+07/1.10e+09 =  2% of the original kernel matrix.

torch.Size([47733, 2])
We keep 9.97e+06/8.80e+08 =  1% of the original kernel matrix.

torch.Size([4721, 2])
We keep 1.73e+05/3.56e+06 =  4% of the original kernel matrix.

torch.Size([15856, 2])
We keep 1.16e+06/5.00e+07 =  2% of the original kernel matrix.

torch.Size([184992, 2])
We keep 1.52e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([93919, 2])
We keep 2.88e+07/3.05e+09 =  0% of the original kernel matrix.

torch.Size([30890, 2])
We keep 5.29e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([39271, 2])
We keep 6.14e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([4838, 2])
We keep 1.71e+05/3.28e+06 =  5% of the original kernel matrix.

torch.Size([16421, 2])
We keep 1.15e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([187396, 2])
We keep 3.31e+08/1.94e+10 =  1% of the original kernel matrix.

torch.Size([92865, 2])
We keep 3.45e+07/3.70e+09 =  0% of the original kernel matrix.

torch.Size([168482, 2])
We keep 1.23e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([89007, 2])
We keep 2.72e+07/2.86e+09 =  0% of the original kernel matrix.

torch.Size([144107, 2])
We keep 1.73e+08/8.37e+09 =  2% of the original kernel matrix.

torch.Size([81807, 2])
We keep 2.36e+07/2.43e+09 =  0% of the original kernel matrix.

torch.Size([268285, 2])
We keep 5.95e+08/3.57e+10 =  1% of the original kernel matrix.

torch.Size([114708, 2])
We keep 4.47e+07/5.01e+09 =  0% of the original kernel matrix.

torch.Size([212028, 2])
We keep 4.38e+08/2.39e+10 =  1% of the original kernel matrix.

torch.Size([99216, 2])
We keep 3.74e+07/4.10e+09 =  0% of the original kernel matrix.

torch.Size([22559, 2])
We keep 5.20e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([32354, 2])
We keep 4.86e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([77376, 2])
We keep 5.63e+07/2.68e+09 =  2% of the original kernel matrix.

torch.Size([58613, 2])
We keep 1.44e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([236120, 2])
We keep 1.75e+08/2.17e+10 =  0% of the original kernel matrix.

torch.Size([106629, 2])
We keep 3.57e+07/3.91e+09 =  0% of the original kernel matrix.

torch.Size([307196, 2])
We keep 2.90e+08/3.68e+10 =  0% of the original kernel matrix.

torch.Size([123943, 2])
We keep 4.48e+07/5.09e+09 =  0% of the original kernel matrix.

torch.Size([16307, 2])
We keep 1.60e+06/5.91e+07 =  2% of the original kernel matrix.

torch.Size([27501, 2])
We keep 3.21e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([232881, 2])
We keep 3.25e+09/3.89e+10 =  8% of the original kernel matrix.

torch.Size([106695, 2])
We keep 4.58e+07/5.23e+09 =  0% of the original kernel matrix.

torch.Size([326892, 2])
We keep 2.69e+09/6.22e+10 =  4% of the original kernel matrix.

torch.Size([125676, 2])
We keep 5.68e+07/6.62e+09 =  0% of the original kernel matrix.

torch.Size([147534, 2])
We keep 1.33e+08/8.06e+09 =  1% of the original kernel matrix.

torch.Size([83035, 2])
We keep 2.31e+07/2.38e+09 =  0% of the original kernel matrix.

torch.Size([72475, 2])
We keep 1.16e+08/3.23e+09 =  3% of the original kernel matrix.

torch.Size([57259, 2])
We keep 1.54e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([27355, 2])
We keep 5.57e+07/7.68e+08 =  7% of the original kernel matrix.

torch.Size([34568, 2])
We keep 8.96e+06/7.35e+08 =  1% of the original kernel matrix.

torch.Size([18567, 2])
We keep 2.90e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([29401, 2])
We keep 3.92e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([12168, 2])
We keep 9.09e+05/2.92e+07 =  3% of the original kernel matrix.

torch.Size([23881, 2])
We keep 2.49e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([11748, 2])
We keep 1.92e+06/4.09e+07 =  4% of the original kernel matrix.

torch.Size([23185, 2])
We keep 2.72e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([238521, 2])
We keep 1.98e+08/2.29e+10 =  0% of the original kernel matrix.

torch.Size([107098, 2])
We keep 3.64e+07/4.02e+09 =  0% of the original kernel matrix.

torch.Size([199446, 2])
We keep 1.81e+08/1.47e+10 =  1% of the original kernel matrix.

torch.Size([97415, 2])
We keep 2.99e+07/3.22e+09 =  0% of the original kernel matrix.

torch.Size([462096, 2])
We keep 9.53e+08/9.69e+10 =  0% of the original kernel matrix.

torch.Size([148645, 2])
We keep 6.93e+07/8.26e+09 =  0% of the original kernel matrix.

torch.Size([301696, 2])
We keep 9.28e+08/4.89e+10 =  1% of the original kernel matrix.

torch.Size([122020, 2])
We keep 5.21e+07/5.87e+09 =  0% of the original kernel matrix.

torch.Size([124265, 2])
We keep 9.06e+07/6.44e+09 =  1% of the original kernel matrix.

torch.Size([75229, 2])
We keep 2.11e+07/2.13e+09 =  0% of the original kernel matrix.

torch.Size([23602, 2])
We keep 4.13e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([33149, 2])
We keep 5.33e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([202577, 2])
We keep 1.96e+08/1.67e+10 =  1% of the original kernel matrix.

torch.Size([98235, 2])
We keep 3.19e+07/3.43e+09 =  0% of the original kernel matrix.

torch.Size([6788, 2])
We keep 5.27e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([18416, 2])
We keep 1.71e+06/8.47e+07 =  2% of the original kernel matrix.

torch.Size([21198, 2])
We keep 4.15e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([31191, 2])
We keep 4.61e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([339052, 2])
We keep 6.41e+08/5.19e+10 =  1% of the original kernel matrix.

torch.Size([129123, 2])
We keep 5.31e+07/6.05e+09 =  0% of the original kernel matrix.

torch.Size([20351, 2])
We keep 2.76e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([30925, 2])
We keep 4.04e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([7956, 2])
We keep 4.46e+05/1.15e+07 =  3% of the original kernel matrix.

torch.Size([19687, 2])
We keep 1.77e+06/8.99e+07 =  1% of the original kernel matrix.

torch.Size([23818, 2])
We keep 3.81e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([33462, 2])
We keep 5.01e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([154302, 2])
We keep 3.54e+08/1.56e+10 =  2% of the original kernel matrix.

torch.Size([84079, 2])
We keep 3.09e+07/3.31e+09 =  0% of the original kernel matrix.

torch.Size([48169, 2])
We keep 1.90e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([47135, 2])
We keep 9.68e+06/8.41e+08 =  1% of the original kernel matrix.

torch.Size([237075, 2])
We keep 4.06e+08/2.56e+10 =  1% of the original kernel matrix.

torch.Size([107969, 2])
We keep 3.88e+07/4.25e+09 =  0% of the original kernel matrix.

torch.Size([17935, 2])
We keep 8.25e+06/1.86e+08 =  4% of the original kernel matrix.

torch.Size([28216, 2])
We keep 4.89e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([29009, 2])
We keep 1.54e+07/4.13e+08 =  3% of the original kernel matrix.

torch.Size([36811, 2])
We keep 6.76e+06/5.39e+08 =  1% of the original kernel matrix.

torch.Size([11476, 2])
We keep 1.26e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([23117, 2])
We keep 2.62e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([8883, 2])
We keep 5.98e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([20975, 2])
We keep 1.97e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([88488, 2])
We keep 5.67e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([64448, 2])
We keep 1.59e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([81450, 2])
We keep 6.91e+07/2.84e+09 =  2% of the original kernel matrix.

torch.Size([59817, 2])
We keep 1.51e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([12009, 2])
We keep 9.08e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([23668, 2])
We keep 2.34e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([211782, 2])
We keep 2.59e+08/2.00e+10 =  1% of the original kernel matrix.

torch.Size([99809, 2])
We keep 3.38e+07/3.76e+09 =  0% of the original kernel matrix.

torch.Size([7852, 2])
We keep 4.75e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([19742, 2])
We keep 1.74e+06/8.76e+07 =  1% of the original kernel matrix.

torch.Size([51513, 2])
We keep 1.82e+07/9.78e+08 =  1% of the original kernel matrix.

torch.Size([50312, 2])
We keep 9.66e+06/8.30e+08 =  1% of the original kernel matrix.

torch.Size([12943, 2])
We keep 2.03e+06/5.08e+07 =  3% of the original kernel matrix.

torch.Size([24753, 2])
We keep 3.00e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([163261, 2])
We keep 9.05e+07/9.32e+09 =  0% of the original kernel matrix.

torch.Size([87448, 2])
We keep 2.44e+07/2.56e+09 =  0% of the original kernel matrix.

torch.Size([19606, 2])
We keep 1.08e+07/1.72e+08 =  6% of the original kernel matrix.

torch.Size([29965, 2])
We keep 4.98e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([74029, 2])
We keep 6.36e+07/2.16e+09 =  2% of the original kernel matrix.

torch.Size([57970, 2])
We keep 1.35e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([29326, 2])
We keep 6.22e+06/2.93e+08 =  2% of the original kernel matrix.

torch.Size([37558, 2])
We keep 5.91e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([41589, 2])
We keep 1.31e+07/7.25e+08 =  1% of the original kernel matrix.

torch.Size([45733, 2])
We keep 8.77e+06/7.14e+08 =  1% of the original kernel matrix.

torch.Size([100090, 2])
We keep 6.00e+07/4.10e+09 =  1% of the original kernel matrix.

torch.Size([68129, 2])
We keep 1.77e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([494272, 2])
We keep 1.57e+09/1.32e+11 =  1% of the original kernel matrix.

torch.Size([152906, 2])
We keep 7.84e+07/9.63e+09 =  0% of the original kernel matrix.

torch.Size([22772, 2])
We keep 4.31e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([32849, 2])
We keep 4.65e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([18236, 2])
We keep 6.10e+06/1.26e+08 =  4% of the original kernel matrix.

torch.Size([28734, 2])
We keep 4.28e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([170392, 2])
We keep 1.90e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([89133, 2])
We keep 2.74e+07/2.88e+09 =  0% of the original kernel matrix.

torch.Size([22419, 2])
We keep 4.08e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([32547, 2])
We keep 4.57e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([955386, 2])
We keep 2.31e+09/3.49e+11 =  0% of the original kernel matrix.

torch.Size([223831, 2])
We keep 1.25e+08/1.57e+10 =  0% of the original kernel matrix.

torch.Size([5720, 2])
We keep 2.70e+05/5.66e+06 =  4% of the original kernel matrix.

torch.Size([17337, 2])
We keep 1.40e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([153360, 2])
We keep 1.07e+08/9.75e+09 =  1% of the original kernel matrix.

torch.Size([84037, 2])
We keep 2.51e+07/2.62e+09 =  0% of the original kernel matrix.

torch.Size([410364, 2])
We keep 4.64e+08/6.91e+10 =  0% of the original kernel matrix.

torch.Size([144355, 2])
We keep 5.97e+07/6.97e+09 =  0% of the original kernel matrix.

torch.Size([7739, 2])
We keep 4.60e+05/1.18e+07 =  3% of the original kernel matrix.

torch.Size([19433, 2])
We keep 1.78e+06/9.11e+07 =  1% of the original kernel matrix.

torch.Size([227907, 2])
We keep 5.84e+08/4.21e+10 =  1% of the original kernel matrix.

torch.Size([99197, 2])
We keep 4.67e+07/5.44e+09 =  0% of the original kernel matrix.

torch.Size([35453, 2])
We keep 7.37e+07/1.15e+09 =  6% of the original kernel matrix.

torch.Size([39060, 2])
We keep 1.06e+07/9.02e+08 =  1% of the original kernel matrix.

torch.Size([1106812, 2])
We keep 5.70e+09/5.40e+11 =  1% of the original kernel matrix.

torch.Size([237728, 2])
We keep 1.53e+08/1.95e+10 =  0% of the original kernel matrix.

torch.Size([17358, 2])
We keep 2.01e+06/8.04e+07 =  2% of the original kernel matrix.

torch.Size([28408, 2])
We keep 3.63e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([71844, 2])
We keep 6.29e+07/2.16e+09 =  2% of the original kernel matrix.

torch.Size([56988, 2])
We keep 1.28e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([9540, 2])
We keep 2.92e+06/3.92e+07 =  7% of the original kernel matrix.

torch.Size([20839, 2])
We keep 2.82e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([13165, 2])
We keep 1.51e+06/4.17e+07 =  3% of the original kernel matrix.

torch.Size([24775, 2])
We keep 2.88e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([14914, 2])
We keep 2.45e+06/7.07e+07 =  3% of the original kernel matrix.

torch.Size([26211, 2])
We keep 3.49e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([76206, 2])
We keep 5.15e+07/2.47e+09 =  2% of the original kernel matrix.

torch.Size([59622, 2])
We keep 1.42e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([38272, 2])
We keep 2.14e+07/6.89e+08 =  3% of the original kernel matrix.

torch.Size([42636, 2])
We keep 8.51e+06/6.96e+08 =  1% of the original kernel matrix.

torch.Size([138164, 2])
We keep 1.39e+08/7.99e+09 =  1% of the original kernel matrix.

torch.Size([80684, 2])
We keep 2.33e+07/2.37e+09 =  0% of the original kernel matrix.

torch.Size([32289, 2])
We keep 7.64e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([39511, 2])
We keep 6.72e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([7974, 2])
We keep 4.88e+05/1.24e+07 =  3% of the original kernel matrix.

torch.Size([19744, 2])
We keep 1.82e+06/9.36e+07 =  1% of the original kernel matrix.

torch.Size([14394, 2])
We keep 2.10e+06/5.72e+07 =  3% of the original kernel matrix.

torch.Size([25824, 2])
We keep 3.14e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([276858, 2])
We keep 7.78e+08/5.32e+10 =  1% of the original kernel matrix.

torch.Size([116874, 2])
We keep 5.37e+07/6.12e+09 =  0% of the original kernel matrix.

torch.Size([59926, 2])
We keep 1.07e+08/3.08e+09 =  3% of the original kernel matrix.

torch.Size([51181, 2])
We keep 1.57e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([490983, 2])
We keep 9.72e+08/9.06e+10 =  1% of the original kernel matrix.

torch.Size([157652, 2])
We keep 6.77e+07/7.99e+09 =  0% of the original kernel matrix.

torch.Size([65868, 2])
We keep 3.97e+07/1.77e+09 =  2% of the original kernel matrix.

torch.Size([55782, 2])
We keep 1.23e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([17399, 2])
We keep 3.58e+06/1.04e+08 =  3% of the original kernel matrix.

torch.Size([28491, 2])
We keep 3.94e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([9595, 2])
We keep 6.22e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([21456, 2])
We keep 2.03e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([22932, 2])
We keep 1.58e+07/2.31e+08 =  6% of the original kernel matrix.

torch.Size([32101, 2])
We keep 5.49e+06/4.03e+08 =  1% of the original kernel matrix.

torch.Size([1700680, 2])
We keep 1.28e+10/9.16e+11 =  1% of the original kernel matrix.

torch.Size([301553, 2])
We keep 1.92e+08/2.54e+10 =  0% of the original kernel matrix.

torch.Size([21217, 2])
We keep 3.56e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([31656, 2])
We keep 4.42e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([34166, 2])
We keep 8.66e+06/4.19e+08 =  2% of the original kernel matrix.

torch.Size([40661, 2])
We keep 6.93e+06/5.43e+08 =  1% of the original kernel matrix.

torch.Size([143780, 2])
We keep 1.46e+08/8.83e+09 =  1% of the original kernel matrix.

torch.Size([81038, 2])
We keep 2.44e+07/2.49e+09 =  0% of the original kernel matrix.

torch.Size([73530, 2])
We keep 4.93e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([57144, 2])
We keep 1.48e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([11312, 2])
We keep 1.20e+06/2.88e+07 =  4% of the original kernel matrix.

torch.Size([23292, 2])
We keep 2.47e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([7477, 2])
We keep 5.83e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([19086, 2])
We keep 1.81e+06/9.23e+07 =  1% of the original kernel matrix.

torch.Size([135042, 2])
We keep 3.34e+08/1.31e+10 =  2% of the original kernel matrix.

torch.Size([78391, 2])
We keep 2.85e+07/3.04e+09 =  0% of the original kernel matrix.

torch.Size([67227, 2])
We keep 3.99e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([55838, 2])
We keep 1.21e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([31306, 2])
We keep 1.44e+08/7.08e+08 = 20% of the original kernel matrix.

torch.Size([37124, 2])
We keep 8.51e+06/7.06e+08 =  1% of the original kernel matrix.

torch.Size([68105, 2])
We keep 3.23e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([56462, 2])
We keep 1.23e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([245899, 2])
We keep 6.85e+08/3.46e+10 =  1% of the original kernel matrix.

torch.Size([109947, 2])
We keep 4.45e+07/4.94e+09 =  0% of the original kernel matrix.

torch.Size([161537, 2])
We keep 2.35e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([87054, 2])
We keep 2.99e+07/3.14e+09 =  0% of the original kernel matrix.

torch.Size([359232, 2])
We keep 7.27e+08/6.27e+10 =  1% of the original kernel matrix.

torch.Size([132782, 2])
We keep 5.72e+07/6.65e+09 =  0% of the original kernel matrix.

torch.Size([194415, 2])
We keep 1.99e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([96148, 2])
We keep 2.94e+07/3.14e+09 =  0% of the original kernel matrix.

torch.Size([23999, 2])
We keep 2.70e+07/2.50e+08 = 10% of the original kernel matrix.

torch.Size([33590, 2])
We keep 5.70e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([109223, 2])
We keep 8.34e+07/5.03e+09 =  1% of the original kernel matrix.

torch.Size([71013, 2])
We keep 1.87e+07/1.88e+09 =  0% of the original kernel matrix.

torch.Size([64805, 2])
We keep 3.27e+08/3.15e+09 = 10% of the original kernel matrix.

torch.Size([52592, 2])
We keep 1.58e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([24100, 2])
We keep 5.52e+06/1.72e+08 =  3% of the original kernel matrix.

torch.Size([33869, 2])
We keep 4.88e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([29548, 2])
We keep 5.95e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([37166, 2])
We keep 6.25e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([20106, 2])
We keep 2.46e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([30787, 2])
We keep 4.00e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([75174, 2])
We keep 9.93e+07/3.48e+09 =  2% of the original kernel matrix.

torch.Size([58013, 2])
We keep 1.59e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([21384, 2])
We keep 4.54e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([31560, 2])
We keep 4.93e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([5196, 2])
We keep 2.25e+05/4.71e+06 =  4% of the original kernel matrix.

torch.Size([16321, 2])
We keep 1.28e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([25326, 2])
We keep 4.66e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([34493, 2])
We keep 5.33e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([23954, 2])
We keep 6.24e+07/4.64e+08 = 13% of the original kernel matrix.

torch.Size([31722, 2])
We keep 6.93e+06/5.72e+08 =  1% of the original kernel matrix.

torch.Size([76379, 2])
We keep 1.65e+08/3.34e+09 =  4% of the original kernel matrix.

torch.Size([58751, 2])
We keep 1.62e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([21158, 2])
We keep 1.43e+07/1.87e+08 =  7% of the original kernel matrix.

torch.Size([31157, 2])
We keep 4.94e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([38129, 2])
We keep 1.12e+07/5.79e+08 =  1% of the original kernel matrix.

torch.Size([41935, 2])
We keep 7.69e+06/6.39e+08 =  1% of the original kernel matrix.

torch.Size([15342, 2])
We keep 5.97e+06/1.01e+08 =  5% of the original kernel matrix.

torch.Size([26297, 2])
We keep 3.90e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([37707, 2])
We keep 1.23e+07/5.72e+08 =  2% of the original kernel matrix.

torch.Size([40031, 2])
We keep 7.30e+06/6.35e+08 =  1% of the original kernel matrix.

torch.Size([10852, 2])
We keep 9.09e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([22585, 2])
We keep 2.31e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([21579, 2])
We keep 3.09e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([31726, 2])
We keep 4.36e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([38911, 2])
We keep 8.64e+06/5.29e+08 =  1% of the original kernel matrix.

torch.Size([44206, 2])
We keep 7.61e+06/6.10e+08 =  1% of the original kernel matrix.

torch.Size([367291, 2])
We keep 8.92e+08/6.46e+10 =  1% of the original kernel matrix.

torch.Size([134390, 2])
We keep 5.85e+07/6.74e+09 =  0% of the original kernel matrix.

torch.Size([17141, 2])
We keep 2.22e+06/7.69e+07 =  2% of the original kernel matrix.

torch.Size([28402, 2])
We keep 3.60e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([4833, 2])
We keep 1.67e+05/3.56e+06 =  4% of the original kernel matrix.

torch.Size([16358, 2])
We keep 1.18e+06/5.01e+07 =  2% of the original kernel matrix.

torch.Size([9536, 2])
We keep 8.87e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([20998, 2])
We keep 2.17e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([332221, 2])
We keep 9.57e+08/4.78e+10 =  2% of the original kernel matrix.

torch.Size([127553, 2])
We keep 5.06e+07/5.80e+09 =  0% of the original kernel matrix.

torch.Size([388023, 2])
We keep 6.82e+08/6.41e+10 =  1% of the original kernel matrix.

torch.Size([140792, 2])
We keep 5.68e+07/6.72e+09 =  0% of the original kernel matrix.

torch.Size([54529, 2])
We keep 3.02e+07/1.48e+09 =  2% of the original kernel matrix.

torch.Size([48531, 2])
We keep 1.13e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([19973, 2])
We keep 3.48e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([30620, 2])
We keep 4.26e+06/2.90e+08 =  1% of the original kernel matrix.

torch.Size([125186, 2])
We keep 1.39e+08/7.44e+09 =  1% of the original kernel matrix.

torch.Size([75733, 2])
We keep 2.28e+07/2.29e+09 =  0% of the original kernel matrix.

torch.Size([275010, 2])
We keep 2.25e+08/2.91e+10 =  0% of the original kernel matrix.

torch.Size([116884, 2])
We keep 4.05e+07/4.53e+09 =  0% of the original kernel matrix.

torch.Size([6696, 2])
We keep 4.07e+05/8.78e+06 =  4% of the original kernel matrix.

torch.Size([18338, 2])
We keep 1.60e+06/7.86e+07 =  2% of the original kernel matrix.

torch.Size([20637, 2])
We keep 3.22e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([31033, 2])
We keep 4.21e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([48952, 2])
We keep 1.11e+07/8.66e+08 =  1% of the original kernel matrix.

torch.Size([48933, 2])
We keep 9.13e+06/7.81e+08 =  1% of the original kernel matrix.

torch.Size([35574, 2])
We keep 4.59e+07/8.67e+08 =  5% of the original kernel matrix.

torch.Size([40223, 2])
We keep 9.21e+06/7.81e+08 =  1% of the original kernel matrix.

torch.Size([55332, 2])
We keep 3.57e+07/1.32e+09 =  2% of the original kernel matrix.

torch.Size([50355, 2])
We keep 1.07e+07/9.65e+08 =  1% of the original kernel matrix.

torch.Size([214281, 2])
We keep 2.12e+08/1.81e+10 =  1% of the original kernel matrix.

torch.Size([101482, 2])
We keep 3.30e+07/3.57e+09 =  0% of the original kernel matrix.

torch.Size([28318, 2])
We keep 4.35e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([37085, 2])
We keep 5.53e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([10889, 2])
We keep 9.82e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([22917, 2])
We keep 2.30e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([7539, 2])
We keep 3.66e+05/9.71e+06 =  3% of the original kernel matrix.

torch.Size([19363, 2])
We keep 1.65e+06/8.27e+07 =  2% of the original kernel matrix.

torch.Size([44418, 2])
We keep 1.88e+07/7.89e+08 =  2% of the original kernel matrix.

torch.Size([45688, 2])
We keep 8.82e+06/7.45e+08 =  1% of the original kernel matrix.

torch.Size([42129, 2])
We keep 1.34e+07/6.58e+08 =  2% of the original kernel matrix.

torch.Size([45354, 2])
We keep 8.16e+06/6.80e+08 =  1% of the original kernel matrix.

torch.Size([70378, 2])
We keep 4.45e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([56557, 2])
We keep 1.37e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([206104, 2])
We keep 6.48e+08/2.25e+10 =  2% of the original kernel matrix.

torch.Size([98406, 2])
We keep 3.68e+07/3.98e+09 =  0% of the original kernel matrix.

torch.Size([700192, 2])
We keep 1.28e+09/1.79e+11 =  0% of the original kernel matrix.

torch.Size([185861, 2])
We keep 9.15e+07/1.12e+10 =  0% of the original kernel matrix.

torch.Size([17696, 2])
We keep 3.34e+06/8.90e+07 =  3% of the original kernel matrix.

torch.Size([28530, 2])
We keep 3.76e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([6967, 2])
We keep 3.78e+05/9.30e+06 =  4% of the original kernel matrix.

torch.Size([18783, 2])
We keep 1.67e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([21603, 2])
We keep 2.72e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([31890, 2])
We keep 4.29e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([79166, 2])
We keep 3.61e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([60707, 2])
We keep 1.33e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([50681, 2])
We keep 1.46e+07/9.67e+08 =  1% of the original kernel matrix.

torch.Size([48779, 2])
We keep 9.51e+06/8.25e+08 =  1% of the original kernel matrix.

torch.Size([7749, 2])
We keep 7.21e+05/1.24e+07 =  5% of the original kernel matrix.

torch.Size([19510, 2])
We keep 1.76e+06/9.33e+07 =  1% of the original kernel matrix.

torch.Size([10127, 2])
We keep 7.60e+05/2.05e+07 =  3% of the original kernel matrix.

torch.Size([21767, 2])
We keep 2.18e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([81407, 2])
We keep 2.63e+08/2.56e+09 = 10% of the original kernel matrix.

torch.Size([60859, 2])
We keep 1.44e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([7956, 2])
We keep 4.80e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([20067, 2])
We keep 1.75e+06/8.84e+07 =  1% of the original kernel matrix.

torch.Size([93723, 2])
We keep 1.63e+08/6.28e+09 =  2% of the original kernel matrix.

torch.Size([63819, 2])
We keep 2.13e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([13048, 2])
We keep 1.05e+06/3.50e+07 =  2% of the original kernel matrix.

torch.Size([24672, 2])
We keep 2.66e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([30457, 2])
We keep 5.08e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([38800, 2])
We keep 6.11e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([17147, 2])
We keep 3.99e+06/9.74e+07 =  4% of the original kernel matrix.

torch.Size([28312, 2])
We keep 3.90e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([14665, 2])
We keep 1.29e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([25982, 2])
We keep 2.97e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([7547, 2])
We keep 3.72e+05/9.56e+06 =  3% of the original kernel matrix.

torch.Size([19373, 2])
We keep 1.64e+06/8.20e+07 =  2% of the original kernel matrix.

torch.Size([209602, 2])
We keep 1.44e+08/1.70e+10 =  0% of the original kernel matrix.

torch.Size([100423, 2])
We keep 3.19e+07/3.46e+09 =  0% of the original kernel matrix.

torch.Size([598989, 2])
We keep 1.46e+09/1.40e+11 =  1% of the original kernel matrix.

torch.Size([171469, 2])
We keep 8.22e+07/9.94e+09 =  0% of the original kernel matrix.

torch.Size([270802, 2])
We keep 3.58e+08/2.94e+10 =  1% of the original kernel matrix.

torch.Size([114848, 2])
We keep 4.04e+07/4.55e+09 =  0% of the original kernel matrix.

torch.Size([105619, 2])
We keep 1.17e+08/5.22e+09 =  2% of the original kernel matrix.

torch.Size([68412, 2])
We keep 1.91e+07/1.92e+09 =  0% of the original kernel matrix.

torch.Size([53668, 2])
We keep 1.80e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([50693, 2])
We keep 9.80e+06/8.51e+08 =  1% of the original kernel matrix.

torch.Size([15830, 2])
We keep 2.67e+06/6.86e+07 =  3% of the original kernel matrix.

torch.Size([26970, 2])
We keep 3.43e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([26469, 2])
We keep 7.59e+06/2.82e+08 =  2% of the original kernel matrix.

torch.Size([35661, 2])
We keep 5.98e+06/4.45e+08 =  1% of the original kernel matrix.

torch.Size([256513, 2])
We keep 2.31e+08/2.62e+10 =  0% of the original kernel matrix.

torch.Size([111534, 2])
We keep 3.90e+07/4.29e+09 =  0% of the original kernel matrix.

torch.Size([30344, 2])
We keep 1.03e+07/3.62e+08 =  2% of the original kernel matrix.

torch.Size([38879, 2])
We keep 6.57e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([45171, 2])
We keep 3.23e+07/8.24e+08 =  3% of the original kernel matrix.

torch.Size([46399, 2])
We keep 9.11e+06/7.61e+08 =  1% of the original kernel matrix.

torch.Size([17582, 2])
We keep 2.79e+06/9.19e+07 =  3% of the original kernel matrix.

torch.Size([28405, 2])
We keep 3.78e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([698632, 2])
We keep 4.18e+09/3.16e+11 =  1% of the original kernel matrix.

torch.Size([190094, 2])
We keep 1.19e+08/1.49e+10 =  0% of the original kernel matrix.

torch.Size([49203, 2])
We keep 2.55e+07/9.28e+08 =  2% of the original kernel matrix.

torch.Size([48156, 2])
We keep 9.19e+06/8.08e+08 =  1% of the original kernel matrix.

torch.Size([69617, 2])
We keep 4.46e+07/2.01e+09 =  2% of the original kernel matrix.

torch.Size([57555, 2])
We keep 1.32e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([308770, 2])
We keep 7.19e+08/4.14e+10 =  1% of the original kernel matrix.

torch.Size([123995, 2])
We keep 4.79e+07/5.40e+09 =  0% of the original kernel matrix.

torch.Size([83293, 2])
We keep 2.64e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([62163, 2])
We keep 1.36e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([47340, 2])
We keep 2.57e+07/9.17e+08 =  2% of the original kernel matrix.

torch.Size([47491, 2])
We keep 9.39e+06/8.03e+08 =  1% of the original kernel matrix.

torch.Size([248220, 2])
We keep 4.57e+08/3.04e+10 =  1% of the original kernel matrix.

torch.Size([110695, 2])
We keep 4.15e+07/4.62e+09 =  0% of the original kernel matrix.

torch.Size([130578, 2])
We keep 6.06e+07/6.15e+09 =  0% of the original kernel matrix.

torch.Size([77298, 2])
We keep 2.05e+07/2.08e+09 =  0% of the original kernel matrix.

torch.Size([69429, 2])
We keep 3.35e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([56671, 2])
We keep 1.19e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([24655, 2])
We keep 7.22e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([33991, 2])
We keep 5.56e+06/4.18e+08 =  1% of the original kernel matrix.

torch.Size([75265, 2])
We keep 4.12e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([58937, 2])
We keep 1.27e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([23445, 2])
We keep 6.16e+06/1.70e+08 =  3% of the original kernel matrix.

torch.Size([33505, 2])
We keep 4.84e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([172939, 2])
We keep 1.33e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([90418, 2])
We keep 2.68e+07/2.82e+09 =  0% of the original kernel matrix.

torch.Size([1466438, 2])
We keep 4.74e+09/7.14e+11 =  0% of the original kernel matrix.

torch.Size([278332, 2])
We keep 1.72e+08/2.24e+10 =  0% of the original kernel matrix.

torch.Size([201847, 2])
We keep 7.71e+08/2.18e+10 =  3% of the original kernel matrix.

torch.Size([99374, 2])
We keep 3.62e+07/3.91e+09 =  0% of the original kernel matrix.

torch.Size([157372, 2])
We keep 3.68e+08/2.17e+10 =  1% of the original kernel matrix.

torch.Size([82779, 2])
We keep 3.64e+07/3.90e+09 =  0% of the original kernel matrix.

torch.Size([21579, 2])
We keep 4.07e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([31910, 2])
We keep 4.29e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([88489, 2])
We keep 1.62e+08/6.34e+09 =  2% of the original kernel matrix.

torch.Size([62230, 2])
We keep 2.15e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([57797, 2])
We keep 3.91e+07/1.36e+09 =  2% of the original kernel matrix.

torch.Size([52124, 2])
We keep 1.11e+07/9.79e+08 =  1% of the original kernel matrix.

torch.Size([20689, 2])
We keep 3.93e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([30923, 2])
We keep 4.57e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([52346, 2])
We keep 2.77e+07/1.26e+09 =  2% of the original kernel matrix.

torch.Size([50206, 2])
We keep 1.08e+07/9.40e+08 =  1% of the original kernel matrix.

torch.Size([609650, 2])
We keep 8.05e+08/1.31e+11 =  0% of the original kernel matrix.

torch.Size([174197, 2])
We keep 7.90e+07/9.59e+09 =  0% of the original kernel matrix.

torch.Size([30239, 2])
We keep 3.08e+07/7.83e+08 =  3% of the original kernel matrix.

torch.Size([36166, 2])
We keep 8.91e+06/7.42e+08 =  1% of the original kernel matrix.

torch.Size([154613, 2])
We keep 6.20e+08/2.17e+10 =  2% of the original kernel matrix.

torch.Size([81855, 2])
We keep 3.48e+07/3.91e+09 =  0% of the original kernel matrix.

torch.Size([121075, 2])
We keep 2.07e+08/8.11e+09 =  2% of the original kernel matrix.

torch.Size([74419, 2])
We keep 2.36e+07/2.39e+09 =  0% of the original kernel matrix.

torch.Size([70590, 2])
We keep 2.94e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([57201, 2])
We keep 1.24e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([11407, 2])
We keep 1.08e+06/2.74e+07 =  3% of the original kernel matrix.

torch.Size([23142, 2])
We keep 2.35e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([112061, 2])
We keep 1.28e+08/4.72e+09 =  2% of the original kernel matrix.

torch.Size([72211, 2])
We keep 1.85e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([237031, 2])
We keep 1.82e+08/2.10e+10 =  0% of the original kernel matrix.

torch.Size([106827, 2])
We keep 3.48e+07/3.85e+09 =  0% of the original kernel matrix.

torch.Size([8077, 2])
We keep 5.25e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([19892, 2])
We keep 1.85e+06/9.50e+07 =  1% of the original kernel matrix.

torch.Size([1203027, 2])
We keep 3.05e+09/5.17e+11 =  0% of the original kernel matrix.

torch.Size([250991, 2])
We keep 1.47e+08/1.91e+10 =  0% of the original kernel matrix.

torch.Size([71407, 2])
We keep 6.04e+07/2.38e+09 =  2% of the original kernel matrix.

torch.Size([57618, 2])
We keep 1.37e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([96403, 2])
We keep 1.06e+08/4.34e+09 =  2% of the original kernel matrix.

torch.Size([65523, 2])
We keep 1.82e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([17534, 2])
We keep 6.39e+06/8.40e+07 =  7% of the original kernel matrix.

torch.Size([28811, 2])
We keep 3.69e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([23562, 2])
We keep 3.86e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([33372, 2])
We keep 4.81e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([7208, 2])
We keep 4.43e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([18815, 2])
We keep 1.71e+06/8.62e+07 =  1% of the original kernel matrix.

torch.Size([9280, 2])
We keep 1.28e+06/2.09e+07 =  6% of the original kernel matrix.

torch.Size([21090, 2])
We keep 2.24e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([42587, 2])
We keep 2.49e+07/7.57e+08 =  3% of the original kernel matrix.

torch.Size([45105, 2])
We keep 8.91e+06/7.30e+08 =  1% of the original kernel matrix.

torch.Size([91325, 2])
We keep 9.63e+07/3.88e+09 =  2% of the original kernel matrix.

torch.Size([63697, 2])
We keep 1.66e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([3095874, 2])
We keep 5.08e+10/5.76e+12 =  0% of the original kernel matrix.

torch.Size([365805, 2])
We keep 4.58e+08/6.37e+10 =  0% of the original kernel matrix.

torch.Size([15398, 2])
We keep 4.19e+06/7.00e+07 =  5% of the original kernel matrix.

torch.Size([26741, 2])
We keep 3.24e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([125076, 2])
We keep 6.96e+07/5.64e+09 =  1% of the original kernel matrix.

torch.Size([75056, 2])
We keep 2.01e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([15225, 2])
We keep 3.67e+06/6.92e+07 =  5% of the original kernel matrix.

torch.Size([26578, 2])
We keep 3.41e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([33526, 2])
We keep 7.65e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([40171, 2])
We keep 6.77e+06/5.45e+08 =  1% of the original kernel matrix.

torch.Size([20435, 2])
We keep 6.53e+06/1.50e+08 =  4% of the original kernel matrix.

torch.Size([30558, 2])
We keep 4.62e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([278012, 2])
We keep 1.23e+09/5.05e+10 =  2% of the original kernel matrix.

torch.Size([114926, 2])
We keep 5.25e+07/5.96e+09 =  0% of the original kernel matrix.

torch.Size([22496, 2])
We keep 3.72e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([32583, 2])
We keep 4.46e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([1904974, 2])
We keep 5.88e+09/1.14e+12 =  0% of the original kernel matrix.

torch.Size([318922, 2])
We keep 2.13e+08/2.84e+10 =  0% of the original kernel matrix.

torch.Size([12948, 2])
We keep 1.27e+06/3.89e+07 =  3% of the original kernel matrix.

torch.Size([24734, 2])
We keep 2.80e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([269414, 2])
We keep 4.76e+08/3.18e+10 =  1% of the original kernel matrix.

torch.Size([115395, 2])
We keep 4.28e+07/4.73e+09 =  0% of the original kernel matrix.

torch.Size([59067, 2])
We keep 1.12e+08/1.61e+09 =  6% of the original kernel matrix.

torch.Size([52470, 2])
We keep 1.20e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([172909, 2])
We keep 1.60e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([90008, 2])
We keep 2.81e+07/2.97e+09 =  0% of the original kernel matrix.

torch.Size([375596, 2])
We keep 7.30e+08/7.37e+10 =  0% of the original kernel matrix.

torch.Size([135452, 2])
We keep 6.08e+07/7.20e+09 =  0% of the original kernel matrix.

torch.Size([32213, 2])
We keep 6.42e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([39264, 2])
We keep 6.19e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([16024, 2])
We keep 2.05e+06/6.32e+07 =  3% of the original kernel matrix.

torch.Size([27159, 2])
We keep 3.30e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([144942, 2])
We keep 7.37e+07/7.53e+09 =  0% of the original kernel matrix.

torch.Size([81992, 2])
We keep 2.26e+07/2.30e+09 =  0% of the original kernel matrix.

torch.Size([21570, 2])
We keep 3.17e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([31965, 2])
We keep 4.51e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([28341, 2])
We keep 1.16e+07/4.33e+08 =  2% of the original kernel matrix.

torch.Size([35973, 2])
We keep 7.10e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([115828, 2])
We keep 6.93e+07/5.30e+09 =  1% of the original kernel matrix.

torch.Size([71885, 2])
We keep 1.93e+07/1.93e+09 =  0% of the original kernel matrix.

torch.Size([104877, 2])
We keep 7.65e+08/1.05e+10 =  7% of the original kernel matrix.

torch.Size([68738, 2])
We keep 2.43e+07/2.72e+09 =  0% of the original kernel matrix.

torch.Size([83695, 2])
We keep 6.25e+07/3.01e+09 =  2% of the original kernel matrix.

torch.Size([63137, 2])
We keep 1.55e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([41512, 2])
We keep 2.04e+07/8.01e+08 =  2% of the original kernel matrix.

torch.Size([43365, 2])
We keep 8.68e+06/7.51e+08 =  1% of the original kernel matrix.

torch.Size([330371, 2])
We keep 8.09e+08/5.44e+10 =  1% of the original kernel matrix.

torch.Size([126605, 2])
We keep 5.43e+07/6.19e+09 =  0% of the original kernel matrix.

torch.Size([393682, 2])
We keep 1.55e+09/9.91e+10 =  1% of the original kernel matrix.

torch.Size([134967, 2])
We keep 7.14e+07/8.35e+09 =  0% of the original kernel matrix.

torch.Size([54277, 2])
We keep 2.52e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([50904, 2])
We keep 1.11e+07/9.62e+08 =  1% of the original kernel matrix.

torch.Size([15269, 2])
We keep 1.70e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([26768, 2])
We keep 3.21e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([170858, 2])
We keep 4.83e+08/1.79e+10 =  2% of the original kernel matrix.

torch.Size([88419, 2])
We keep 3.21e+07/3.55e+09 =  0% of the original kernel matrix.

torch.Size([55328, 2])
We keep 2.74e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([49697, 2])
We keep 1.08e+07/9.87e+08 =  1% of the original kernel matrix.

torch.Size([85429, 2])
We keep 6.76e+07/2.99e+09 =  2% of the original kernel matrix.

torch.Size([62435, 2])
We keep 1.52e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([55609, 2])
We keep 4.52e+07/1.34e+09 =  3% of the original kernel matrix.

torch.Size([50254, 2])
We keep 1.11e+07/9.72e+08 =  1% of the original kernel matrix.

torch.Size([85155, 2])
We keep 1.24e+08/5.15e+09 =  2% of the original kernel matrix.

torch.Size([60526, 2])
We keep 1.95e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([112210, 2])
We keep 1.73e+08/6.05e+09 =  2% of the original kernel matrix.

torch.Size([71308, 2])
We keep 2.10e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([166733, 2])
We keep 7.95e+08/3.63e+10 =  2% of the original kernel matrix.

torch.Size([80085, 2])
We keep 4.38e+07/5.05e+09 =  0% of the original kernel matrix.

torch.Size([54116, 2])
We keep 3.21e+07/1.32e+09 =  2% of the original kernel matrix.

torch.Size([50364, 2])
We keep 1.06e+07/9.64e+08 =  1% of the original kernel matrix.

torch.Size([30129, 2])
We keep 5.52e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([37992, 2])
We keep 5.98e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([14607, 2])
We keep 1.36e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([25938, 2])
We keep 2.94e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([35487, 2])
We keep 2.31e+07/5.40e+08 =  4% of the original kernel matrix.

torch.Size([41263, 2])
We keep 7.48e+06/6.17e+08 =  1% of the original kernel matrix.

torch.Size([70305, 2])
We keep 9.36e+07/3.20e+09 =  2% of the original kernel matrix.

torch.Size([55246, 2])
We keep 1.59e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([183828, 2])
We keep 2.70e+08/1.61e+10 =  1% of the original kernel matrix.

torch.Size([92374, 2])
We keep 3.16e+07/3.37e+09 =  0% of the original kernel matrix.

torch.Size([216204, 2])
We keep 2.98e+08/1.95e+10 =  1% of the original kernel matrix.

torch.Size([101706, 2])
We keep 3.42e+07/3.70e+09 =  0% of the original kernel matrix.

torch.Size([18455, 2])
We keep 2.49e+06/9.39e+07 =  2% of the original kernel matrix.

torch.Size([29251, 2])
We keep 3.84e+06/2.57e+08 =  1% of the original kernel matrix.

torch.Size([40096, 2])
We keep 8.58e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([42952, 2])
We keep 7.84e+06/6.61e+08 =  1% of the original kernel matrix.

torch.Size([110965, 2])
We keep 1.06e+08/5.15e+09 =  2% of the original kernel matrix.

torch.Size([70876, 2])
We keep 1.93e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([184669, 2])
We keep 3.81e+08/1.20e+10 =  3% of the original kernel matrix.

torch.Size([93428, 2])
We keep 2.68e+07/2.91e+09 =  0% of the original kernel matrix.

torch.Size([106156, 2])
We keep 8.35e+07/4.81e+09 =  1% of the original kernel matrix.

torch.Size([69501, 2])
We keep 1.89e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([129445, 2])
We keep 6.01e+07/5.56e+09 =  1% of the original kernel matrix.

torch.Size([76753, 2])
We keep 1.97e+07/1.98e+09 =  0% of the original kernel matrix.

torch.Size([84755, 2])
We keep 1.38e+09/1.07e+10 = 12% of the original kernel matrix.

torch.Size([60816, 2])
We keep 2.63e+07/2.74e+09 =  0% of the original kernel matrix.

torch.Size([52930, 2])
We keep 2.10e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([50271, 2])
We keep 9.87e+06/8.69e+08 =  1% of the original kernel matrix.

torch.Size([6115, 2])
We keep 4.56e+05/8.19e+06 =  5% of the original kernel matrix.

torch.Size([17950, 2])
We keep 1.61e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([14682, 2])
We keep 1.54e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([26412, 2])
We keep 3.10e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([226068, 2])
We keep 1.97e+08/1.89e+10 =  1% of the original kernel matrix.

torch.Size([104199, 2])
We keep 3.33e+07/3.64e+09 =  0% of the original kernel matrix.

torch.Size([52637, 2])
We keep 2.25e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([48929, 2])
We keep 1.04e+07/9.26e+08 =  1% of the original kernel matrix.

torch.Size([124613, 2])
We keep 5.44e+07/5.24e+09 =  1% of the original kernel matrix.

torch.Size([75122, 2])
We keep 1.93e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([5677, 2])
We keep 2.10e+05/4.77e+06 =  4% of the original kernel matrix.

torch.Size([17582, 2])
We keep 1.30e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([10316, 2])
We keep 7.45e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([22224, 2])
We keep 2.25e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([155983, 2])
We keep 1.72e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([85098, 2])
We keep 2.63e+07/2.77e+09 =  0% of the original kernel matrix.

torch.Size([123718, 2])
We keep 8.59e+07/6.49e+09 =  1% of the original kernel matrix.

torch.Size([74534, 2])
We keep 2.13e+07/2.14e+09 =  0% of the original kernel matrix.

torch.Size([70339, 2])
We keep 2.99e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([57314, 2])
We keep 1.21e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([9785, 2])
We keep 7.16e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([21833, 2])
We keep 2.13e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([10902, 2])
We keep 9.76e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([22547, 2])
We keep 2.39e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([72949, 2])
We keep 4.52e+07/1.93e+09 =  2% of the original kernel matrix.

torch.Size([57982, 2])
We keep 1.27e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([59566, 2])
We keep 3.49e+07/1.46e+09 =  2% of the original kernel matrix.

torch.Size([52597, 2])
We keep 1.14e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([176893, 2])
We keep 3.22e+08/1.46e+10 =  2% of the original kernel matrix.

torch.Size([91011, 2])
We keep 3.03e+07/3.21e+09 =  0% of the original kernel matrix.

torch.Size([14145, 2])
We keep 1.56e+06/4.94e+07 =  3% of the original kernel matrix.

torch.Size([25988, 2])
We keep 2.98e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([33844, 2])
We keep 8.74e+08/1.79e+09 = 48% of the original kernel matrix.

torch.Size([37501, 2])
We keep 1.27e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([25408, 2])
We keep 6.10e+06/2.37e+08 =  2% of the original kernel matrix.

torch.Size([34432, 2])
We keep 5.51e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([25511, 2])
We keep 1.10e+07/2.41e+08 =  4% of the original kernel matrix.

torch.Size([34394, 2])
We keep 5.55e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([569662, 2])
We keep 2.18e+09/1.31e+11 =  1% of the original kernel matrix.

torch.Size([168522, 2])
We keep 7.97e+07/9.59e+09 =  0% of the original kernel matrix.

torch.Size([17895, 2])
We keep 6.30e+06/9.88e+07 =  6% of the original kernel matrix.

torch.Size([28925, 2])
We keep 3.93e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([65473, 2])
We keep 3.01e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([54686, 2])
We keep 1.22e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([196240, 2])
We keep 2.83e+08/1.56e+10 =  1% of the original kernel matrix.

torch.Size([96724, 2])
We keep 3.10e+07/3.32e+09 =  0% of the original kernel matrix.

torch.Size([144246, 2])
We keep 3.55e+08/8.46e+09 =  4% of the original kernel matrix.

torch.Size([81844, 2])
We keep 2.38e+07/2.44e+09 =  0% of the original kernel matrix.

torch.Size([215203, 2])
We keep 4.46e+08/2.13e+10 =  2% of the original kernel matrix.

torch.Size([102775, 2])
We keep 3.56e+07/3.87e+09 =  0% of the original kernel matrix.

torch.Size([291637, 2])
We keep 3.61e+08/3.67e+10 =  0% of the original kernel matrix.

torch.Size([120830, 2])
We keep 4.55e+07/5.08e+09 =  0% of the original kernel matrix.

torch.Size([9133, 2])
We keep 5.03e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([21158, 2])
We keep 1.91e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([80024, 2])
We keep 3.24e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([60624, 2])
We keep 1.39e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([36416, 2])
We keep 7.29e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([42420, 2])
We keep 7.03e+06/5.68e+08 =  1% of the original kernel matrix.

torch.Size([198459, 2])
We keep 2.89e+08/1.53e+10 =  1% of the original kernel matrix.

torch.Size([96899, 2])
We keep 2.96e+07/3.28e+09 =  0% of the original kernel matrix.

torch.Size([188288, 2])
We keep 2.07e+08/1.66e+10 =  1% of the original kernel matrix.

torch.Size([93903, 2])
We keep 3.11e+07/3.41e+09 =  0% of the original kernel matrix.

torch.Size([46669, 2])
We keep 2.80e+07/1.15e+09 =  2% of the original kernel matrix.

torch.Size([45669, 2])
We keep 1.04e+07/9.01e+08 =  1% of the original kernel matrix.

torch.Size([132929, 2])
We keep 9.80e+07/7.14e+09 =  1% of the original kernel matrix.

torch.Size([78174, 2])
We keep 2.24e+07/2.24e+09 =  0% of the original kernel matrix.

torch.Size([20156, 2])
We keep 2.27e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([30724, 2])
We keep 3.93e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([27693, 2])
We keep 1.99e+07/5.52e+08 =  3% of the original kernel matrix.

torch.Size([33618, 2])
We keep 7.45e+06/6.24e+08 =  1% of the original kernel matrix.

torch.Size([13981, 2])
We keep 2.08e+06/4.33e+07 =  4% of the original kernel matrix.

torch.Size([25584, 2])
We keep 2.79e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([25240, 2])
We keep 7.88e+06/2.44e+08 =  3% of the original kernel matrix.

torch.Size([34596, 2])
We keep 5.56e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([265341, 2])
We keep 4.09e+08/2.95e+10 =  1% of the original kernel matrix.

torch.Size([114612, 2])
We keep 4.07e+07/4.56e+09 =  0% of the original kernel matrix.

torch.Size([397213, 2])
We keep 1.54e+09/7.83e+10 =  1% of the original kernel matrix.

torch.Size([141974, 2])
We keep 6.42e+07/7.43e+09 =  0% of the original kernel matrix.

torch.Size([18288, 2])
We keep 7.31e+06/1.72e+08 =  4% of the original kernel matrix.

torch.Size([28920, 2])
We keep 4.94e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([43524, 2])
We keep 1.20e+07/6.61e+08 =  1% of the original kernel matrix.

torch.Size([46462, 2])
We keep 8.23e+06/6.82e+08 =  1% of the original kernel matrix.

torch.Size([30331, 2])
We keep 7.39e+06/3.00e+08 =  2% of the original kernel matrix.

torch.Size([38264, 2])
We keep 6.00e+06/4.60e+08 =  1% of the original kernel matrix.

torch.Size([350608, 2])
We keep 6.12e+08/5.78e+10 =  1% of the original kernel matrix.

torch.Size([131527, 2])
We keep 5.46e+07/6.38e+09 =  0% of the original kernel matrix.

torch.Size([38350, 2])
We keep 7.56e+06/4.84e+08 =  1% of the original kernel matrix.

torch.Size([43568, 2])
We keep 7.26e+06/5.83e+08 =  1% of the original kernel matrix.

torch.Size([22228, 2])
We keep 4.11e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([32366, 2])
We keep 4.53e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([80307, 2])
We keep 6.26e+07/2.85e+09 =  2% of the original kernel matrix.

torch.Size([60816, 2])
We keep 1.49e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([186546, 2])
We keep 1.35e+08/1.36e+10 =  0% of the original kernel matrix.

torch.Size([94492, 2])
We keep 2.92e+07/3.10e+09 =  0% of the original kernel matrix.

torch.Size([18079, 2])
We keep 2.82e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([28703, 2])
We keep 3.95e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([95707, 2])
We keep 9.93e+07/3.80e+09 =  2% of the original kernel matrix.

torch.Size([65864, 2])
We keep 1.70e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([12242, 2])
We keep 1.78e+06/3.26e+07 =  5% of the original kernel matrix.

torch.Size([24097, 2])
We keep 2.47e+06/1.51e+08 =  1% of the original kernel matrix.

time for making ranges is 5.386442184448242
Sorting X and nu_X
time for sorting X is 0.09410214424133301
Sorting Z and nu_Z
time for sorting Z is 0.0002598762512207031
Starting Optim
sum tnu_Z before tensor(45249696., device='cuda:0')
c= tensor(2072.6868, device='cuda:0')
c= tensor(215591.0625, device='cuda:0')
c= tensor(235199.5000, device='cuda:0')
c= tensor(256928.9219, device='cuda:0')
c= tensor(529315.4375, device='cuda:0')
c= tensor(961844.7500, device='cuda:0')
c= tensor(2125489.7500, device='cuda:0')
c= tensor(2710121., device='cuda:0')
c= tensor(2777120.5000, device='cuda:0')
c= tensor(9910996., device='cuda:0')
c= tensor(9982759., device='cuda:0')
c= tensor(16063313., device='cuda:0')
c= tensor(16081136., device='cuda:0')
c= tensor(35229332., device='cuda:0')
c= tensor(35412244., device='cuda:0')
c= tensor(36232520., device='cuda:0')
c= tensor(37341824., device='cuda:0')
c= tensor(38154884., device='cuda:0')
c= tensor(57551520., device='cuda:0')
c= tensor(64697728., device='cuda:0')
c= tensor(64966888., device='cuda:0')
c= tensor(1.1151e+08, device='cuda:0')
c= tensor(1.1158e+08, device='cuda:0')
c= tensor(1.1222e+08, device='cuda:0')
c= tensor(1.1315e+08, device='cuda:0')
c= tensor(1.1482e+08, device='cuda:0')
c= tensor(1.1694e+08, device='cuda:0')
c= tensor(1.1700e+08, device='cuda:0')
c= tensor(1.2146e+08, device='cuda:0')
c= tensor(6.9487e+08, device='cuda:0')
c= tensor(6.9492e+08, device='cuda:0')
c= tensor(8.7841e+08, device='cuda:0')
c= tensor(8.7852e+08, device='cuda:0')
c= tensor(8.7855e+08, device='cuda:0')
c= tensor(8.7860e+08, device='cuda:0')
c= tensor(8.9020e+08, device='cuda:0')
c= tensor(8.9565e+08, device='cuda:0')
c= tensor(8.9565e+08, device='cuda:0')
c= tensor(8.9566e+08, device='cuda:0')
c= tensor(8.9567e+08, device='cuda:0')
c= tensor(8.9569e+08, device='cuda:0')
c= tensor(8.9569e+08, device='cuda:0')
c= tensor(8.9569e+08, device='cuda:0')
c= tensor(8.9570e+08, device='cuda:0')
c= tensor(8.9570e+08, device='cuda:0')
c= tensor(8.9570e+08, device='cuda:0')
c= tensor(8.9575e+08, device='cuda:0')
c= tensor(8.9577e+08, device='cuda:0')
c= tensor(8.9577e+08, device='cuda:0')
c= tensor(8.9583e+08, device='cuda:0')
c= tensor(8.9592e+08, device='cuda:0')
c= tensor(8.9592e+08, device='cuda:0')
c= tensor(8.9594e+08, device='cuda:0')
c= tensor(8.9595e+08, device='cuda:0')
c= tensor(8.9597e+08, device='cuda:0')
c= tensor(8.9599e+08, device='cuda:0')
c= tensor(8.9599e+08, device='cuda:0')
c= tensor(8.9600e+08, device='cuda:0')
c= tensor(8.9601e+08, device='cuda:0')
c= tensor(8.9602e+08, device='cuda:0')
c= tensor(8.9604e+08, device='cuda:0')
c= tensor(8.9604e+08, device='cuda:0')
c= tensor(8.9608e+08, device='cuda:0')
c= tensor(8.9610e+08, device='cuda:0')
c= tensor(8.9611e+08, device='cuda:0')
c= tensor(8.9611e+08, device='cuda:0')
c= tensor(8.9612e+08, device='cuda:0')
c= tensor(8.9614e+08, device='cuda:0')
c= tensor(8.9615e+08, device='cuda:0')
c= tensor(8.9616e+08, device='cuda:0')
c= tensor(8.9616e+08, device='cuda:0')
c= tensor(8.9617e+08, device='cuda:0')
c= tensor(8.9618e+08, device='cuda:0')
c= tensor(8.9618e+08, device='cuda:0')
c= tensor(8.9619e+08, device='cuda:0')
c= tensor(8.9620e+08, device='cuda:0')
c= tensor(8.9621e+08, device='cuda:0')
c= tensor(8.9621e+08, device='cuda:0')
c= tensor(8.9622e+08, device='cuda:0')
c= tensor(8.9626e+08, device='cuda:0')
c= tensor(8.9627e+08, device='cuda:0')
c= tensor(8.9627e+08, device='cuda:0')
c= tensor(8.9630e+08, device='cuda:0')
c= tensor(8.9630e+08, device='cuda:0')
c= tensor(8.9631e+08, device='cuda:0')
c= tensor(8.9631e+08, device='cuda:0')
c= tensor(8.9632e+08, device='cuda:0')
c= tensor(8.9632e+08, device='cuda:0')
c= tensor(8.9633e+08, device='cuda:0')
c= tensor(8.9634e+08, device='cuda:0')
c= tensor(8.9635e+08, device='cuda:0')
c= tensor(8.9636e+08, device='cuda:0')
c= tensor(8.9636e+08, device='cuda:0')
c= tensor(8.9637e+08, device='cuda:0')
c= tensor(8.9638e+08, device='cuda:0')
c= tensor(8.9639e+08, device='cuda:0')
c= tensor(8.9640e+08, device='cuda:0')
c= tensor(8.9643e+08, device='cuda:0')
c= tensor(8.9644e+08, device='cuda:0')
c= tensor(8.9645e+08, device='cuda:0')
c= tensor(8.9652e+08, device='cuda:0')
c= tensor(8.9653e+08, device='cuda:0')
c= tensor(8.9655e+08, device='cuda:0')
c= tensor(8.9655e+08, device='cuda:0')
c= tensor(8.9656e+08, device='cuda:0')
c= tensor(8.9656e+08, device='cuda:0')
c= tensor(8.9659e+08, device='cuda:0')
c= tensor(8.9659e+08, device='cuda:0')
c= tensor(8.9660e+08, device='cuda:0')
c= tensor(8.9660e+08, device='cuda:0')
c= tensor(8.9660e+08, device='cuda:0')
c= tensor(8.9661e+08, device='cuda:0')
c= tensor(8.9661e+08, device='cuda:0')
c= tensor(8.9662e+08, device='cuda:0')
c= tensor(8.9664e+08, device='cuda:0')
c= tensor(8.9664e+08, device='cuda:0')
c= tensor(8.9664e+08, device='cuda:0')
c= tensor(8.9665e+08, device='cuda:0')
c= tensor(8.9667e+08, device='cuda:0')
c= tensor(8.9668e+08, device='cuda:0')
c= tensor(8.9672e+08, device='cuda:0')
c= tensor(8.9673e+08, device='cuda:0')
c= tensor(8.9674e+08, device='cuda:0')
c= tensor(8.9674e+08, device='cuda:0')
c= tensor(8.9675e+08, device='cuda:0')
c= tensor(8.9675e+08, device='cuda:0')
c= tensor(8.9675e+08, device='cuda:0')
c= tensor(8.9676e+08, device='cuda:0')
c= tensor(8.9681e+08, device='cuda:0')
c= tensor(8.9682e+08, device='cuda:0')
c= tensor(8.9684e+08, device='cuda:0')
c= tensor(8.9684e+08, device='cuda:0')
c= tensor(8.9685e+08, device='cuda:0')
c= tensor(8.9686e+08, device='cuda:0')
c= tensor(8.9686e+08, device='cuda:0')
c= tensor(8.9687e+08, device='cuda:0')
c= tensor(8.9687e+08, device='cuda:0')
c= tensor(8.9687e+08, device='cuda:0')
c= tensor(8.9688e+08, device='cuda:0')
c= tensor(8.9689e+08, device='cuda:0')
c= tensor(8.9689e+08, device='cuda:0')
c= tensor(8.9690e+08, device='cuda:0')
c= tensor(8.9695e+08, device='cuda:0')
c= tensor(8.9702e+08, device='cuda:0')
c= tensor(8.9704e+08, device='cuda:0')
c= tensor(8.9704e+08, device='cuda:0')
c= tensor(8.9705e+08, device='cuda:0')
c= tensor(8.9706e+08, device='cuda:0')
c= tensor(8.9706e+08, device='cuda:0')
c= tensor(8.9706e+08, device='cuda:0')
c= tensor(8.9707e+08, device='cuda:0')
c= tensor(8.9708e+08, device='cuda:0')
c= tensor(8.9709e+08, device='cuda:0')
c= tensor(8.9718e+08, device='cuda:0')
c= tensor(8.9719e+08, device='cuda:0')
c= tensor(8.9726e+08, device='cuda:0')
c= tensor(8.9727e+08, device='cuda:0')
c= tensor(8.9728e+08, device='cuda:0')
c= tensor(8.9729e+08, device='cuda:0')
c= tensor(8.9729e+08, device='cuda:0')
c= tensor(8.9738e+08, device='cuda:0')
c= tensor(8.9738e+08, device='cuda:0')
c= tensor(8.9739e+08, device='cuda:0')
c= tensor(8.9740e+08, device='cuda:0')
c= tensor(8.9740e+08, device='cuda:0')
c= tensor(8.9740e+08, device='cuda:0')
c= tensor(8.9742e+08, device='cuda:0')
c= tensor(8.9743e+08, device='cuda:0')
c= tensor(8.9743e+08, device='cuda:0')
c= tensor(8.9744e+08, device='cuda:0')
c= tensor(8.9744e+08, device='cuda:0')
c= tensor(8.9744e+08, device='cuda:0')
c= tensor(8.9746e+08, device='cuda:0')
c= tensor(8.9747e+08, device='cuda:0')
c= tensor(8.9748e+08, device='cuda:0')
c= tensor(8.9750e+08, device='cuda:0')
c= tensor(8.9751e+08, device='cuda:0')
c= tensor(8.9752e+08, device='cuda:0')
c= tensor(8.9754e+08, device='cuda:0')
c= tensor(8.9754e+08, device='cuda:0')
c= tensor(8.9755e+08, device='cuda:0')
c= tensor(8.9755e+08, device='cuda:0')
c= tensor(8.9756e+08, device='cuda:0')
c= tensor(8.9757e+08, device='cuda:0')
c= tensor(8.9759e+08, device='cuda:0')
c= tensor(8.9761e+08, device='cuda:0')
c= tensor(8.9762e+08, device='cuda:0')
c= tensor(8.9763e+08, device='cuda:0')
c= tensor(8.9764e+08, device='cuda:0')
c= tensor(8.9777e+08, device='cuda:0')
c= tensor(8.9777e+08, device='cuda:0')
c= tensor(8.9777e+08, device='cuda:0')
c= tensor(8.9779e+08, device='cuda:0')
c= tensor(8.9779e+08, device='cuda:0')
c= tensor(8.9780e+08, device='cuda:0')
c= tensor(8.9780e+08, device='cuda:0')
c= tensor(8.9781e+08, device='cuda:0')
c= tensor(8.9781e+08, device='cuda:0')
c= tensor(8.9782e+08, device='cuda:0')
c= tensor(8.9782e+08, device='cuda:0')
c= tensor(8.9783e+08, device='cuda:0')
c= tensor(8.9783e+08, device='cuda:0')
c= tensor(8.9785e+08, device='cuda:0')
c= tensor(8.9787e+08, device='cuda:0')
c= tensor(8.9788e+08, device='cuda:0')
c= tensor(8.9788e+08, device='cuda:0')
c= tensor(8.9789e+08, device='cuda:0')
c= tensor(8.9790e+08, device='cuda:0')
c= tensor(8.9791e+08, device='cuda:0')
c= tensor(8.9794e+08, device='cuda:0')
c= tensor(8.9795e+08, device='cuda:0')
c= tensor(8.9795e+08, device='cuda:0')
c= tensor(8.9796e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9798e+08, device='cuda:0')
c= tensor(8.9799e+08, device='cuda:0')
c= tensor(8.9799e+08, device='cuda:0')
c= tensor(8.9800e+08, device='cuda:0')
c= tensor(8.9801e+08, device='cuda:0')
c= tensor(8.9801e+08, device='cuda:0')
c= tensor(8.9802e+08, device='cuda:0')
c= tensor(8.9803e+08, device='cuda:0')
c= tensor(8.9803e+08, device='cuda:0')
c= tensor(8.9804e+08, device='cuda:0')
c= tensor(8.9805e+08, device='cuda:0')
c= tensor(8.9805e+08, device='cuda:0')
c= tensor(8.9805e+08, device='cuda:0')
c= tensor(8.9806e+08, device='cuda:0')
c= tensor(8.9807e+08, device='cuda:0')
c= tensor(8.9808e+08, device='cuda:0')
c= tensor(8.9809e+08, device='cuda:0')
c= tensor(8.9809e+08, device='cuda:0')
c= tensor(8.9810e+08, device='cuda:0')
c= tensor(8.9813e+08, device='cuda:0')
c= tensor(8.9813e+08, device='cuda:0')
c= tensor(8.9819e+08, device='cuda:0')
c= tensor(9.0109e+08, device='cuda:0')
c= tensor(9.0141e+08, device='cuda:0')
c= tensor(9.0142e+08, device='cuda:0')
c= tensor(9.0142e+08, device='cuda:0')
c= tensor(9.0144e+08, device='cuda:0')
c= tensor(9.4216e+08, device='cuda:0')
c= tensor(1.0090e+09, device='cuda:0')
c= tensor(1.0090e+09, device='cuda:0')
c= tensor(1.0128e+09, device='cuda:0')
c= tensor(1.0134e+09, device='cuda:0')
c= tensor(1.0138e+09, device='cuda:0')
c= tensor(1.0367e+09, device='cuda:0')
c= tensor(1.0367e+09, device='cuda:0')
c= tensor(1.0368e+09, device='cuda:0')
c= tensor(1.0534e+09, device='cuda:0')
c= tensor(1.1008e+09, device='cuda:0')
c= tensor(1.1008e+09, device='cuda:0')
c= tensor(1.1011e+09, device='cuda:0')
c= tensor(1.2026e+09, device='cuda:0')
c= tensor(1.2095e+09, device='cuda:0')
c= tensor(1.2122e+09, device='cuda:0')
c= tensor(1.2138e+09, device='cuda:0')
c= tensor(1.2148e+09, device='cuda:0')
c= tensor(1.2149e+09, device='cuda:0')
c= tensor(1.2149e+09, device='cuda:0')
c= tensor(1.3807e+09, device='cuda:0')
c= tensor(1.3807e+09, device='cuda:0')
c= tensor(1.3807e+09, device='cuda:0')
c= tensor(1.3819e+09, device='cuda:0')
c= tensor(1.3826e+09, device='cuda:0')
c= tensor(1.4276e+09, device='cuda:0')
c= tensor(1.4316e+09, device='cuda:0')
c= tensor(1.4316e+09, device='cuda:0')
c= tensor(1.4321e+09, device='cuda:0')
c= tensor(1.4321e+09, device='cuda:0')
c= tensor(1.4334e+09, device='cuda:0')
c= tensor(1.4360e+09, device='cuda:0')
c= tensor(1.4376e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4404e+09, device='cuda:0')
c= tensor(1.4432e+09, device='cuda:0')
c= tensor(1.4438e+09, device='cuda:0')
c= tensor(1.4443e+09, device='cuda:0')
c= tensor(1.4800e+09, device='cuda:0')
c= tensor(1.4800e+09, device='cuda:0')
c= tensor(1.4803e+09, device='cuda:0')
c= tensor(1.4826e+09, device='cuda:0')
c= tensor(1.4826e+09, device='cuda:0')
c= tensor(1.4870e+09, device='cuda:0')
c= tensor(1.5197e+09, device='cuda:0')
c= tensor(1.5567e+09, device='cuda:0')
c= tensor(1.5569e+09, device='cuda:0')
c= tensor(1.5570e+09, device='cuda:0')
c= tensor(1.5571e+09, device='cuda:0')
c= tensor(1.5571e+09, device='cuda:0')
c= tensor(1.5579e+09, device='cuda:0')
c= tensor(1.5579e+09, device='cuda:0')
c= tensor(1.5583e+09, device='cuda:0')
c= tensor(1.6294e+09, device='cuda:0')
c= tensor(1.6321e+09, device='cuda:0')
c= tensor(1.6322e+09, device='cuda:0')
c= tensor(1.6323e+09, device='cuda:0')
c= tensor(1.6411e+09, device='cuda:0')
c= tensor(1.6414e+09, device='cuda:0')
c= tensor(1.6425e+09, device='cuda:0')
c= tensor(1.6426e+09, device='cuda:0')
c= tensor(1.7338e+09, device='cuda:0')
c= tensor(1.7339e+09, device='cuda:0')
c= tensor(1.7438e+09, device='cuda:0')
c= tensor(1.7438e+09, device='cuda:0')
c= tensor(1.7472e+09, device='cuda:0')
c= tensor(1.7489e+09, device='cuda:0')
c= tensor(1.8326e+09, device='cuda:0')
c= tensor(1.8345e+09, device='cuda:0')
c= tensor(1.8345e+09, device='cuda:0')
c= tensor(1.8395e+09, device='cuda:0')
c= tensor(1.8436e+09, device='cuda:0')
c= tensor(1.8436e+09, device='cuda:0')
c= tensor(1.8472e+09, device='cuda:0')
c= tensor(1.8536e+09, device='cuda:0')
c= tensor(1.8751e+09, device='cuda:0')
c= tensor(1.8859e+09, device='cuda:0')
c= tensor(1.8859e+09, device='cuda:0')
c= tensor(1.8860e+09, device='cuda:0')
c= tensor(1.8903e+09, device='cuda:0')
c= tensor(1.9054e+09, device='cuda:0')
c= tensor(1.9057e+09, device='cuda:0')
c= tensor(1.9057e+09, device='cuda:0')
c= tensor(1.9074e+09, device='cuda:0')
c= tensor(1.9153e+09, device='cuda:0')
c= tensor(1.9229e+09, device='cuda:0')
c= tensor(1.9229e+09, device='cuda:0')
c= tensor(1.9231e+09, device='cuda:0')
c= tensor(1.9232e+09, device='cuda:0')
c= tensor(1.9232e+09, device='cuda:0')
c= tensor(1.9233e+09, device='cuda:0')
c= tensor(1.9233e+09, device='cuda:0')
c= tensor(1.9253e+09, device='cuda:0')
c= tensor(1.9261e+09, device='cuda:0')
c= tensor(1.9264e+09, device='cuda:0')
c= tensor(1.9272e+09, device='cuda:0')
c= tensor(1.9272e+09, device='cuda:0')
c= tensor(1.9944e+09, device='cuda:0')
c= tensor(1.9945e+09, device='cuda:0')
c= tensor(1.9985e+09, device='cuda:0')
c= tensor(1.9986e+09, device='cuda:0')
c= tensor(1.9986e+09, device='cuda:0')
c= tensor(1.9986e+09, device='cuda:0')
c= tensor(2.0027e+09, device='cuda:0')
c= tensor(2.0027e+09, device='cuda:0')
c= tensor(2.0183e+09, device='cuda:0')
c= tensor(2.0183e+09, device='cuda:0')
c= tensor(2.0183e+09, device='cuda:0')
c= tensor(2.0344e+09, device='cuda:0')
c= tensor(2.0357e+09, device='cuda:0')
c= tensor(2.0371e+09, device='cuda:0')
c= tensor(2.0535e+09, device='cuda:0')
c= tensor(2.0704e+09, device='cuda:0')
c= tensor(2.0704e+09, device='cuda:0')
c= tensor(2.0704e+09, device='cuda:0')
c= tensor(2.0711e+09, device='cuda:0')
c= tensor(2.0711e+09, device='cuda:0')
c= tensor(2.0711e+09, device='cuda:0')
c= tensor(2.0713e+09, device='cuda:0')
c= tensor(2.0713e+09, device='cuda:0')
c= tensor(2.0713e+09, device='cuda:0')
c= tensor(2.0713e+09, device='cuda:0')
c= tensor(2.0714e+09, device='cuda:0')
c= tensor(2.1179e+09, device='cuda:0')
c= tensor(2.1179e+09, device='cuda:0')
c= tensor(2.1213e+09, device='cuda:0')
c= tensor(2.1215e+09, device='cuda:0')
c= tensor(2.1222e+09, device='cuda:0')
c= tensor(2.1224e+09, device='cuda:0')
c= tensor(2.5550e+09, device='cuda:0')
c= tensor(2.6535e+09, device='cuda:0')
c= tensor(2.6540e+09, device='cuda:0')
c= tensor(2.6622e+09, device='cuda:0')
c= tensor(2.6622e+09, device='cuda:0')
c= tensor(2.6635e+09, device='cuda:0')
c= tensor(2.6665e+09, device='cuda:0')
c= tensor(2.6677e+09, device='cuda:0')
c= tensor(2.6677e+09, device='cuda:0')
c= tensor(2.6708e+09, device='cuda:0')
c= tensor(2.7334e+09, device='cuda:0')
c= tensor(2.7340e+09, device='cuda:0')
c= tensor(2.7340e+09, device='cuda:0')
c= tensor(2.7356e+09, device='cuda:0')
c= tensor(2.7357e+09, device='cuda:0')
c= tensor(2.7357e+09, device='cuda:0')
c= tensor(2.7449e+09, device='cuda:0')
c= tensor(2.7455e+09, device='cuda:0')
c= tensor(2.7455e+09, device='cuda:0')
c= tensor(2.7487e+09, device='cuda:0')
c= tensor(2.7488e+09, device='cuda:0')
c= tensor(2.7488e+09, device='cuda:0')
c= tensor(2.7561e+09, device='cuda:0')
c= tensor(2.7595e+09, device='cuda:0')
c= tensor(2.7639e+09, device='cuda:0')
c= tensor(2.7807e+09, device='cuda:0')
c= tensor(2.7925e+09, device='cuda:0')
c= tensor(2.7925e+09, device='cuda:0')
c= tensor(2.7939e+09, device='cuda:0')
c= tensor(2.7982e+09, device='cuda:0')
c= tensor(2.8062e+09, device='cuda:0')
c= tensor(2.8063e+09, device='cuda:0')
c= tensor(2.9596e+09, device='cuda:0')
c= tensor(3.0897e+09, device='cuda:0')
c= tensor(3.0940e+09, device='cuda:0')
c= tensor(3.0982e+09, device='cuda:0')
c= tensor(3.1000e+09, device='cuda:0')
c= tensor(3.1001e+09, device='cuda:0')
c= tensor(3.1001e+09, device='cuda:0')
c= tensor(3.1001e+09, device='cuda:0')
c= tensor(3.1044e+09, device='cuda:0')
c= tensor(3.1106e+09, device='cuda:0')
c= tensor(3.1552e+09, device='cuda:0')
c= tensor(3.1831e+09, device='cuda:0')
c= tensor(3.1853e+09, device='cuda:0')
c= tensor(3.1856e+09, device='cuda:0')
c= tensor(3.1903e+09, device='cuda:0')
c= tensor(3.1903e+09, device='cuda:0')
c= tensor(3.1903e+09, device='cuda:0')
c= tensor(3.2097e+09, device='cuda:0')
c= tensor(3.2098e+09, device='cuda:0')
c= tensor(3.2098e+09, device='cuda:0')
c= tensor(3.2099e+09, device='cuda:0')
c= tensor(3.2209e+09, device='cuda:0')
c= tensor(3.2214e+09, device='cuda:0')
c= tensor(3.2315e+09, device='cuda:0')
c= tensor(3.2316e+09, device='cuda:0')
c= tensor(3.2319e+09, device='cuda:0')
c= tensor(3.2319e+09, device='cuda:0')
c= tensor(3.2319e+09, device='cuda:0')
c= tensor(3.2330e+09, device='cuda:0')
c= tensor(3.2343e+09, device='cuda:0')
c= tensor(3.2343e+09, device='cuda:0')
c= tensor(3.2408e+09, device='cuda:0')
c= tensor(3.2408e+09, device='cuda:0')
c= tensor(3.2411e+09, device='cuda:0')
c= tensor(3.2412e+09, device='cuda:0')
c= tensor(3.2429e+09, device='cuda:0')
c= tensor(3.2432e+09, device='cuda:0')
c= tensor(3.2444e+09, device='cuda:0')
c= tensor(3.2445e+09, device='cuda:0')
c= tensor(3.2448e+09, device='cuda:0')
c= tensor(3.2465e+09, device='cuda:0')
c= tensor(3.3037e+09, device='cuda:0')
c= tensor(3.3037e+09, device='cuda:0')
c= tensor(3.3039e+09, device='cuda:0')
c= tensor(3.3106e+09, device='cuda:0')
c= tensor(3.3107e+09, device='cuda:0')
c= tensor(3.3979e+09, device='cuda:0')
c= tensor(3.3979e+09, device='cuda:0')
c= tensor(3.4005e+09, device='cuda:0')
c= tensor(3.4120e+09, device='cuda:0')
c= tensor(3.4120e+09, device='cuda:0')
c= tensor(3.4321e+09, device='cuda:0')
c= tensor(3.4340e+09, device='cuda:0')
c= tensor(3.6199e+09, device='cuda:0')
c= tensor(3.6200e+09, device='cuda:0')
c= tensor(3.6214e+09, device='cuda:0')
c= tensor(3.6215e+09, device='cuda:0')
c= tensor(3.6215e+09, device='cuda:0')
c= tensor(3.6216e+09, device='cuda:0')
c= tensor(3.6239e+09, device='cuda:0')
c= tensor(3.6244e+09, device='cuda:0')
c= tensor(3.6276e+09, device='cuda:0')
c= tensor(3.6277e+09, device='cuda:0')
c= tensor(3.6277e+09, device='cuda:0')
c= tensor(3.6278e+09, device='cuda:0')
c= tensor(3.6494e+09, device='cuda:0')
c= tensor(3.6515e+09, device='cuda:0')
c= tensor(3.6779e+09, device='cuda:0')
c= tensor(3.6787e+09, device='cuda:0')
c= tensor(3.6788e+09, device='cuda:0')
c= tensor(3.6788e+09, device='cuda:0')
c= tensor(3.6792e+09, device='cuda:0')
c= tensor(4.4107e+09, device='cuda:0')
c= tensor(4.4108e+09, device='cuda:0')
c= tensor(4.4109e+09, device='cuda:0')
c= tensor(4.4149e+09, device='cuda:0')
c= tensor(4.4172e+09, device='cuda:0')
c= tensor(4.4172e+09, device='cuda:0')
c= tensor(4.4172e+09, device='cuda:0')
c= tensor(4.4329e+09, device='cuda:0')
c= tensor(4.4337e+09, device='cuda:0')
c= tensor(4.4364e+09, device='cuda:0')
c= tensor(4.4369e+09, device='cuda:0')
c= tensor(4.4586e+09, device='cuda:0')
c= tensor(4.4648e+09, device='cuda:0')
c= tensor(4.4876e+09, device='cuda:0')
c= tensor(4.4926e+09, device='cuda:0')
c= tensor(4.4931e+09, device='cuda:0')
c= tensor(4.4955e+09, device='cuda:0')
c= tensor(4.5022e+09, device='cuda:0')
c= tensor(4.5023e+09, device='cuda:0')
c= tensor(4.5024e+09, device='cuda:0')
c= tensor(4.5025e+09, device='cuda:0')
c= tensor(4.5064e+09, device='cuda:0')
c= tensor(4.5065e+09, device='cuda:0')
c= tensor(4.5065e+09, device='cuda:0')
c= tensor(4.5066e+09, device='cuda:0')
c= tensor(4.5086e+09, device='cuda:0')
c= tensor(4.5133e+09, device='cuda:0')
c= tensor(4.5136e+09, device='cuda:0')
c= tensor(4.5139e+09, device='cuda:0')
c= tensor(4.5140e+09, device='cuda:0')
c= tensor(4.5143e+09, device='cuda:0')
c= tensor(4.5143e+09, device='cuda:0')
c= tensor(4.5143e+09, device='cuda:0')
c= tensor(4.5145e+09, device='cuda:0')
c= tensor(4.5418e+09, device='cuda:0')
c= tensor(4.5419e+09, device='cuda:0')
c= tensor(4.5419e+09, device='cuda:0')
c= tensor(4.5419e+09, device='cuda:0')
c= tensor(4.5717e+09, device='cuda:0')
c= tensor(4.5942e+09, device='cuda:0')
c= tensor(4.5948e+09, device='cuda:0')
c= tensor(4.5949e+09, device='cuda:0')
c= tensor(4.5980e+09, device='cuda:0')
c= tensor(4.6031e+09, device='cuda:0')
c= tensor(4.6031e+09, device='cuda:0')
c= tensor(4.6033e+09, device='cuda:0')
c= tensor(4.6034e+09, device='cuda:0')
c= tensor(4.6053e+09, device='cuda:0')
c= tensor(4.6066e+09, device='cuda:0')
c= tensor(4.6118e+09, device='cuda:0')
c= tensor(4.6118e+09, device='cuda:0')
c= tensor(4.6119e+09, device='cuda:0')
c= tensor(4.6119e+09, device='cuda:0')
c= tensor(4.6123e+09, device='cuda:0')
c= tensor(4.6126e+09, device='cuda:0')
c= tensor(4.6146e+09, device='cuda:0')
c= tensor(4.6297e+09, device='cuda:0')
c= tensor(4.6633e+09, device='cuda:0')
c= tensor(4.6633e+09, device='cuda:0')
c= tensor(4.6633e+09, device='cuda:0')
c= tensor(4.6634e+09, device='cuda:0')
c= tensor(4.6643e+09, device='cuda:0')
c= tensor(4.6646e+09, device='cuda:0')
c= tensor(4.6646e+09, device='cuda:0')
c= tensor(4.6646e+09, device='cuda:0')
c= tensor(4.6696e+09, device='cuda:0')
c= tensor(4.6696e+09, device='cuda:0')
c= tensor(4.6736e+09, device='cuda:0')
c= tensor(4.6736e+09, device='cuda:0')
c= tensor(4.6737e+09, device='cuda:0')
c= tensor(4.6738e+09, device='cuda:0')
c= tensor(4.6738e+09, device='cuda:0')
c= tensor(4.6738e+09, device='cuda:0')
c= tensor(4.6780e+09, device='cuda:0')
c= tensor(4.7225e+09, device='cuda:0')
c= tensor(4.7354e+09, device='cuda:0')
c= tensor(4.7384e+09, device='cuda:0')
c= tensor(4.7388e+09, device='cuda:0')
c= tensor(4.7388e+09, device='cuda:0')
c= tensor(4.7390e+09, device='cuda:0')
c= tensor(4.7441e+09, device='cuda:0')
c= tensor(4.7443e+09, device='cuda:0')
c= tensor(4.7449e+09, device='cuda:0')
c= tensor(4.7450e+09, device='cuda:0')
c= tensor(4.8912e+09, device='cuda:0')
c= tensor(4.8917e+09, device='cuda:0')
c= tensor(4.8925e+09, device='cuda:0')
c= tensor(4.9283e+09, device='cuda:0')
c= tensor(4.9287e+09, device='cuda:0')
c= tensor(4.9296e+09, device='cuda:0')
c= tensor(4.9419e+09, device='cuda:0')
c= tensor(4.9438e+09, device='cuda:0')
c= tensor(4.9445e+09, device='cuda:0')
c= tensor(4.9447e+09, device='cuda:0')
c= tensor(4.9456e+09, device='cuda:0')
c= tensor(4.9457e+09, device='cuda:0')
c= tensor(4.9487e+09, device='cuda:0')
c= tensor(5.1038e+09, device='cuda:0')
c= tensor(5.1278e+09, device='cuda:0')
c= tensor(5.1370e+09, device='cuda:0')
c= tensor(5.1371e+09, device='cuda:0')
c= tensor(5.1408e+09, device='cuda:0')
c= tensor(5.1419e+09, device='cuda:0')
c= tensor(5.1420e+09, device='cuda:0')
c= tensor(5.1425e+09, device='cuda:0')
c= tensor(5.1668e+09, device='cuda:0')
c= tensor(5.1673e+09, device='cuda:0')
c= tensor(5.1950e+09, device='cuda:0')
c= tensor(5.2010e+09, device='cuda:0')
c= tensor(5.2018e+09, device='cuda:0')
c= tensor(5.2018e+09, device='cuda:0')
c= tensor(5.2046e+09, device='cuda:0')
c= tensor(5.2089e+09, device='cuda:0')
c= tensor(5.2089e+09, device='cuda:0')
c= tensor(5.3037e+09, device='cuda:0')
c= tensor(5.3062e+09, device='cuda:0')
c= tensor(5.3085e+09, device='cuda:0')
c= tensor(5.3087e+09, device='cuda:0')
c= tensor(5.3087e+09, device='cuda:0')
c= tensor(5.3088e+09, device='cuda:0')
c= tensor(5.3088e+09, device='cuda:0')
c= tensor(5.3093e+09, device='cuda:0')
c= tensor(5.3115e+09, device='cuda:0')
c= tensor(7.4494e+09, device='cuda:0')
c= tensor(7.4496e+09, device='cuda:0')
c= tensor(7.4509e+09, device='cuda:0')
c= tensor(7.4510e+09, device='cuda:0')
c= tensor(7.4511e+09, device='cuda:0')
c= tensor(7.4512e+09, device='cuda:0')
c= tensor(7.4878e+09, device='cuda:0')
c= tensor(7.4879e+09, device='cuda:0')
c= tensor(7.7262e+09, device='cuda:0')
c= tensor(7.7262e+09, device='cuda:0')
c= tensor(7.7393e+09, device='cuda:0')
c= tensor(7.7418e+09, device='cuda:0')
c= tensor(7.7464e+09, device='cuda:0')
c= tensor(7.7730e+09, device='cuda:0')
c= tensor(7.7731e+09, device='cuda:0')
c= tensor(7.7731e+09, device='cuda:0')
c= tensor(7.7747e+09, device='cuda:0')
c= tensor(7.7748e+09, device='cuda:0')
c= tensor(7.7750e+09, device='cuda:0')
c= tensor(7.7764e+09, device='cuda:0')
c= tensor(7.8109e+09, device='cuda:0')
c= tensor(7.8124e+09, device='cuda:0')
c= tensor(7.8128e+09, device='cuda:0')
c= tensor(7.8363e+09, device='cuda:0')
c= tensor(7.8795e+09, device='cuda:0')
c= tensor(7.8800e+09, device='cuda:0')
c= tensor(7.8800e+09, device='cuda:0')
c= tensor(7.9003e+09, device='cuda:0')
c= tensor(7.9009e+09, device='cuda:0')
c= tensor(7.9025e+09, device='cuda:0')
c= tensor(7.9035e+09, device='cuda:0')
c= tensor(7.9067e+09, device='cuda:0')
c= tensor(7.9104e+09, device='cuda:0')
c= tensor(7.9439e+09, device='cuda:0')
c= tensor(7.9447e+09, device='cuda:0')
c= tensor(7.9448e+09, device='cuda:0')
c= tensor(7.9448e+09, device='cuda:0')
c= tensor(7.9454e+09, device='cuda:0')
c= tensor(7.9475e+09, device='cuda:0')
c= tensor(7.9543e+09, device='cuda:0')
c= tensor(7.9618e+09, device='cuda:0')
c= tensor(7.9618e+09, device='cuda:0')
c= tensor(7.9621e+09, device='cuda:0')
c= tensor(7.9650e+09, device='cuda:0')
c= tensor(7.9747e+09, device='cuda:0')
c= tensor(7.9763e+09, device='cuda:0')
c= tensor(7.9779e+09, device='cuda:0')
c= tensor(8.0439e+09, device='cuda:0')
c= tensor(8.0445e+09, device='cuda:0')
c= tensor(8.0445e+09, device='cuda:0')
c= tensor(8.0445e+09, device='cuda:0')
c= tensor(8.0507e+09, device='cuda:0')
c= tensor(8.0517e+09, device='cuda:0')
c= tensor(8.0528e+09, device='cuda:0')
c= tensor(8.0528e+09, device='cuda:0')
c= tensor(8.0528e+09, device='cuda:0')
c= tensor(8.0578e+09, device='cuda:0')
c= tensor(8.0597e+09, device='cuda:0')
c= tensor(8.0602e+09, device='cuda:0')
c= tensor(8.0602e+09, device='cuda:0')
c= tensor(8.0603e+09, device='cuda:0')
c= tensor(8.0611e+09, device='cuda:0')
c= tensor(8.0617e+09, device='cuda:0')
c= tensor(8.0716e+09, device='cuda:0')
c= tensor(8.0716e+09, device='cuda:0')
c= tensor(8.0906e+09, device='cuda:0')
c= tensor(8.0907e+09, device='cuda:0')
c= tensor(8.0909e+09, device='cuda:0')
c= tensor(8.2039e+09, device='cuda:0')
c= tensor(8.2042e+09, device='cuda:0')
c= tensor(8.2049e+09, device='cuda:0')
c= tensor(8.2114e+09, device='cuda:0')
c= tensor(8.2183e+09, device='cuda:0')
c= tensor(8.2290e+09, device='cuda:0')
c= tensor(8.2385e+09, device='cuda:0')
c= tensor(8.2385e+09, device='cuda:0')
c= tensor(8.2392e+09, device='cuda:0')
c= tensor(8.2393e+09, device='cuda:0')
c= tensor(8.2454e+09, device='cuda:0')
c= tensor(8.2515e+09, device='cuda:0')
c= tensor(8.2520e+09, device='cuda:0')
c= tensor(8.2543e+09, device='cuda:0')
c= tensor(8.2544e+09, device='cuda:0')
c= tensor(8.2561e+09, device='cuda:0')
c= tensor(8.2562e+09, device='cuda:0')
c= tensor(8.2563e+09, device='cuda:0')
c= tensor(8.2674e+09, device='cuda:0')
c= tensor(8.3114e+09, device='cuda:0')
c= tensor(8.3115e+09, device='cuda:0')
c= tensor(8.3118e+09, device='cuda:0')
c= tensor(8.3120e+09, device='cuda:0')
c= tensor(8.3317e+09, device='cuda:0')
c= tensor(8.3318e+09, device='cuda:0')
c= tensor(8.3319e+09, device='cuda:0')
c= tensor(8.3331e+09, device='cuda:0')
c= tensor(8.3360e+09, device='cuda:0')
c= tensor(8.3361e+09, device='cuda:0')
c= tensor(8.3382e+09, device='cuda:0')
c= tensor(8.3383e+09, device='cuda:0')
memory (bytes)
5266595840
time for making loss 2 is 11.904524087905884
p0 True
it  0 : 2646331392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 55% |
shape of L is 
torch.Size([])
memory (bytes)
5266862080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 18% |
memory (bytes)
5267496960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  85371440000.0
relative error loss 10.2385
shape of L is 
torch.Size([])
memory (bytes)
5443829760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
5443993600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  85371150000.0
relative error loss 10.238465
shape of L is 
torch.Size([])
memory (bytes)
5448904704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5448990720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  85368570000.0
relative error loss 10.238155
shape of L is 
torch.Size([])
memory (bytes)
5451055104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 19% |
memory (bytes)
5451055104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 19% |
error is  85342850000.0
relative error loss 10.23507
shape of L is 
torch.Size([])
memory (bytes)
5453103104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5453221888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  85213850000.0
relative error loss 10.2196
shape of L is 
torch.Size([])
memory (bytes)
5455265792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5455310848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 19% |
error is  83807320000.0
relative error loss 10.050917
shape of L is 
torch.Size([])
memory (bytes)
5457383424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
5457428480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  77066215000.0
relative error loss 9.242463
shape of L is 
torch.Size([])
memory (bytes)
5459529728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 19% |
memory (bytes)
5459591168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  47896300000.0
relative error loss 5.744149
shape of L is 
torch.Size([])
memory (bytes)
5461696512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 19% |
memory (bytes)
5461725184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  18581492000.0
relative error loss 2.2284572
shape of L is 
torch.Size([])
memory (bytes)
5463814144
| ID | GPU | MEM |
------------------
|  0 |  5% |  0% |
|  1 | 29% | 19% |
memory (bytes)
5463859200
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 19% |
error is  11739303000.0
relative error loss 1.4078813
time to take a step is 199.67728900909424
it  1 : 3168844288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5465853952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 19% |
memory (bytes)
5465997312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  11739303000.0
relative error loss 1.4078813
shape of L is 
torch.Size([])
memory (bytes)
5468135424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
5468135424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  12073363000.0
relative error loss 1.4479448
shape of L is 
torch.Size([])
memory (bytes)
5470158848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5470285824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  9925626000.0
relative error loss 1.190369
shape of L is 
torch.Size([])
memory (bytes)
5472444416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5472444416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 19% |
error is  8257464300.0
relative error loss 0.9903083
shape of L is 
torch.Size([])
memory (bytes)
5474336768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5474471936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  7955222500.0
relative error loss 0.9540608
shape of L is 
torch.Size([])
memory (bytes)
5476704256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5476704256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  7653093000.0
relative error loss 0.9178267
shape of L is 
torch.Size([])
memory (bytes)
5478780928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5478825984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  7455322000.0
relative error loss 0.8941083
shape of L is 
torch.Size([])
memory (bytes)
5480820736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5480935424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  8018223600.0
relative error loss 0.96161646
shape of L is 
torch.Size([])
memory (bytes)
5483032576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 19% |
memory (bytes)
5483032576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  7139989500.0
relative error loss 0.8562908
shape of L is 
torch.Size([])
memory (bytes)
5484974080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5485142016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  6778697000.0
relative error loss 0.81296146
time to take a step is 193.17295217514038
it  2 : 3316854272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5486997504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 19% |
memory (bytes)
5487259648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  6778697000.0
relative error loss 0.81296146
shape of L is 
torch.Size([])
memory (bytes)
5489307648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 19% |
memory (bytes)
5489356800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  6304268000.0
relative error loss 0.75606364
shape of L is 
torch.Size([])
memory (bytes)
5491339264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 19% |
memory (bytes)
5491339264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  5912183000.0
relative error loss 0.70904136
shape of L is 
torch.Size([])
memory (bytes)
5493440512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5493575680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  5626158600.0
relative error loss 0.6747388
shape of L is 
torch.Size([])
memory (bytes)
5495709696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 19% |
memory (bytes)
5495709696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  5192398300.0
relative error loss 0.62271845
shape of L is 
torch.Size([])
memory (bytes)
5497724928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 19% |
memory (bytes)
5497724928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  4829430000.0
relative error loss 0.579188
shape of L is 
torch.Size([])
memory (bytes)
5499740160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5499936768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  4616774700.0
relative error loss 0.55368453
shape of L is 
torch.Size([])
memory (bytes)
5502074880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
5502074880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  4317820000.0
relative error loss 0.5178312
shape of L is 
torch.Size([])
memory (bytes)
5504000000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 19% |
memory (bytes)
5504000000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  4069919200.0
relative error loss 0.4881008
shape of L is 
torch.Size([])
memory (bytes)
5506314240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
5506371584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 19% |
error is  3746552800.0
relative error loss 0.4493198
time to take a step is 193.4346194267273
it  3 : 3316854272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5508489216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5508489216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  3746552800.0
relative error loss 0.4493198
shape of L is 
torch.Size([])
memory (bytes)
5510537216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5510537216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  3434457000.0
relative error loss 0.41189054
shape of L is 
torch.Size([])
memory (bytes)
5512720384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5512765440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  3239269400.0
relative error loss 0.3884819
shape of L is 
torch.Size([])
memory (bytes)
5514899456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5514899456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  3033911800.0
relative error loss 0.3638536
shape of L is 
torch.Size([])
memory (bytes)
5516992512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5517037568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2933807600.0
relative error loss 0.3518482
shape of L is 
torch.Size([])
memory (bytes)
5519167488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
5519187968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2771618800.0
relative error loss 0.3323971
shape of L is 
torch.Size([])
memory (bytes)
5521149952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 19% |
memory (bytes)
5521149952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2698361900.0
relative error loss 0.32361147
shape of L is 
torch.Size([])
memory (bytes)
5523365888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 19% |
memory (bytes)
5523468288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2870454800.0
relative error loss 0.34425038
shape of L is 
torch.Size([])
memory (bytes)
5525553152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5525598208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2468802600.0
relative error loss 0.29608068
shape of L is 
torch.Size([])
memory (bytes)
5527683072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5527728128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  2291943400.0
relative error loss 0.27487016
time to take a step is 194.2971591949463
c= tensor(2072.6868, device='cuda:0')
c= tensor(215591.0625, device='cuda:0')
c= tensor(235199.5000, device='cuda:0')
c= tensor(256928.9219, device='cuda:0')
c= tensor(529315.4375, device='cuda:0')
c= tensor(961844.7500, device='cuda:0')
c= tensor(2125489.7500, device='cuda:0')
c= tensor(2710121., device='cuda:0')
c= tensor(2777120.5000, device='cuda:0')
c= tensor(9910996., device='cuda:0')
c= tensor(9982759., device='cuda:0')
c= tensor(16063313., device='cuda:0')
c= tensor(16081136., device='cuda:0')
c= tensor(35229332., device='cuda:0')
c= tensor(35412244., device='cuda:0')
c= tensor(36232520., device='cuda:0')
c= tensor(37341824., device='cuda:0')
c= tensor(38154884., device='cuda:0')
c= tensor(57551520., device='cuda:0')
c= tensor(64697728., device='cuda:0')
c= tensor(64966888., device='cuda:0')
c= tensor(1.1151e+08, device='cuda:0')
c= tensor(1.1158e+08, device='cuda:0')
c= tensor(1.1222e+08, device='cuda:0')
c= tensor(1.1315e+08, device='cuda:0')
c= tensor(1.1482e+08, device='cuda:0')
c= tensor(1.1694e+08, device='cuda:0')
c= tensor(1.1700e+08, device='cuda:0')
c= tensor(1.2146e+08, device='cuda:0')
c= tensor(6.9487e+08, device='cuda:0')
c= tensor(6.9492e+08, device='cuda:0')
c= tensor(8.7841e+08, device='cuda:0')
c= tensor(8.7852e+08, device='cuda:0')
c= tensor(8.7855e+08, device='cuda:0')
c= tensor(8.7860e+08, device='cuda:0')
c= tensor(8.9020e+08, device='cuda:0')
c= tensor(8.9565e+08, device='cuda:0')
c= tensor(8.9565e+08, device='cuda:0')
c= tensor(8.9566e+08, device='cuda:0')
c= tensor(8.9567e+08, device='cuda:0')
c= tensor(8.9569e+08, device='cuda:0')
c= tensor(8.9569e+08, device='cuda:0')
c= tensor(8.9569e+08, device='cuda:0')
c= tensor(8.9570e+08, device='cuda:0')
c= tensor(8.9570e+08, device='cuda:0')
c= tensor(8.9570e+08, device='cuda:0')
c= tensor(8.9575e+08, device='cuda:0')
c= tensor(8.9577e+08, device='cuda:0')
c= tensor(8.9577e+08, device='cuda:0')
c= tensor(8.9583e+08, device='cuda:0')
c= tensor(8.9592e+08, device='cuda:0')
c= tensor(8.9592e+08, device='cuda:0')
c= tensor(8.9594e+08, device='cuda:0')
c= tensor(8.9595e+08, device='cuda:0')
c= tensor(8.9597e+08, device='cuda:0')
c= tensor(8.9599e+08, device='cuda:0')
c= tensor(8.9599e+08, device='cuda:0')
c= tensor(8.9600e+08, device='cuda:0')
c= tensor(8.9601e+08, device='cuda:0')
c= tensor(8.9602e+08, device='cuda:0')
c= tensor(8.9604e+08, device='cuda:0')
c= tensor(8.9604e+08, device='cuda:0')
c= tensor(8.9608e+08, device='cuda:0')
c= tensor(8.9610e+08, device='cuda:0')
c= tensor(8.9611e+08, device='cuda:0')
c= tensor(8.9611e+08, device='cuda:0')
c= tensor(8.9612e+08, device='cuda:0')
c= tensor(8.9614e+08, device='cuda:0')
c= tensor(8.9615e+08, device='cuda:0')
c= tensor(8.9616e+08, device='cuda:0')
c= tensor(8.9616e+08, device='cuda:0')
c= tensor(8.9617e+08, device='cuda:0')
c= tensor(8.9618e+08, device='cuda:0')
c= tensor(8.9618e+08, device='cuda:0')
c= tensor(8.9619e+08, device='cuda:0')
c= tensor(8.9620e+08, device='cuda:0')
c= tensor(8.9621e+08, device='cuda:0')
c= tensor(8.9621e+08, device='cuda:0')
c= tensor(8.9622e+08, device='cuda:0')
c= tensor(8.9626e+08, device='cuda:0')
c= tensor(8.9627e+08, device='cuda:0')
c= tensor(8.9627e+08, device='cuda:0')
c= tensor(8.9630e+08, device='cuda:0')
c= tensor(8.9630e+08, device='cuda:0')
c= tensor(8.9631e+08, device='cuda:0')
c= tensor(8.9631e+08, device='cuda:0')
c= tensor(8.9632e+08, device='cuda:0')
c= tensor(8.9632e+08, device='cuda:0')
c= tensor(8.9633e+08, device='cuda:0')
c= tensor(8.9634e+08, device='cuda:0')
c= tensor(8.9635e+08, device='cuda:0')
c= tensor(8.9636e+08, device='cuda:0')
c= tensor(8.9636e+08, device='cuda:0')
c= tensor(8.9637e+08, device='cuda:0')
c= tensor(8.9638e+08, device='cuda:0')
c= tensor(8.9639e+08, device='cuda:0')
c= tensor(8.9640e+08, device='cuda:0')
c= tensor(8.9643e+08, device='cuda:0')
c= tensor(8.9644e+08, device='cuda:0')
c= tensor(8.9645e+08, device='cuda:0')
c= tensor(8.9652e+08, device='cuda:0')
c= tensor(8.9653e+08, device='cuda:0')
c= tensor(8.9655e+08, device='cuda:0')
c= tensor(8.9655e+08, device='cuda:0')
c= tensor(8.9656e+08, device='cuda:0')
c= tensor(8.9656e+08, device='cuda:0')
c= tensor(8.9659e+08, device='cuda:0')
c= tensor(8.9659e+08, device='cuda:0')
c= tensor(8.9660e+08, device='cuda:0')
c= tensor(8.9660e+08, device='cuda:0')
c= tensor(8.9660e+08, device='cuda:0')
c= tensor(8.9661e+08, device='cuda:0')
c= tensor(8.9661e+08, device='cuda:0')
c= tensor(8.9662e+08, device='cuda:0')
c= tensor(8.9664e+08, device='cuda:0')
c= tensor(8.9664e+08, device='cuda:0')
c= tensor(8.9664e+08, device='cuda:0')
c= tensor(8.9665e+08, device='cuda:0')
c= tensor(8.9667e+08, device='cuda:0')
c= tensor(8.9668e+08, device='cuda:0')
c= tensor(8.9672e+08, device='cuda:0')
c= tensor(8.9673e+08, device='cuda:0')
c= tensor(8.9674e+08, device='cuda:0')
c= tensor(8.9674e+08, device='cuda:0')
c= tensor(8.9675e+08, device='cuda:0')
c= tensor(8.9675e+08, device='cuda:0')
c= tensor(8.9675e+08, device='cuda:0')
c= tensor(8.9676e+08, device='cuda:0')
c= tensor(8.9681e+08, device='cuda:0')
c= tensor(8.9682e+08, device='cuda:0')
c= tensor(8.9684e+08, device='cuda:0')
c= tensor(8.9684e+08, device='cuda:0')
c= tensor(8.9685e+08, device='cuda:0')
c= tensor(8.9686e+08, device='cuda:0')
c= tensor(8.9686e+08, device='cuda:0')
c= tensor(8.9687e+08, device='cuda:0')
c= tensor(8.9687e+08, device='cuda:0')
c= tensor(8.9687e+08, device='cuda:0')
c= tensor(8.9688e+08, device='cuda:0')
c= tensor(8.9689e+08, device='cuda:0')
c= tensor(8.9689e+08, device='cuda:0')
c= tensor(8.9690e+08, device='cuda:0')
c= tensor(8.9695e+08, device='cuda:0')
c= tensor(8.9702e+08, device='cuda:0')
c= tensor(8.9704e+08, device='cuda:0')
c= tensor(8.9704e+08, device='cuda:0')
c= tensor(8.9705e+08, device='cuda:0')
c= tensor(8.9706e+08, device='cuda:0')
c= tensor(8.9706e+08, device='cuda:0')
c= tensor(8.9706e+08, device='cuda:0')
c= tensor(8.9707e+08, device='cuda:0')
c= tensor(8.9708e+08, device='cuda:0')
c= tensor(8.9709e+08, device='cuda:0')
c= tensor(8.9718e+08, device='cuda:0')
c= tensor(8.9719e+08, device='cuda:0')
c= tensor(8.9726e+08, device='cuda:0')
c= tensor(8.9727e+08, device='cuda:0')
c= tensor(8.9728e+08, device='cuda:0')
c= tensor(8.9729e+08, device='cuda:0')
c= tensor(8.9729e+08, device='cuda:0')
c= tensor(8.9738e+08, device='cuda:0')
c= tensor(8.9738e+08, device='cuda:0')
c= tensor(8.9739e+08, device='cuda:0')
c= tensor(8.9740e+08, device='cuda:0')
c= tensor(8.9740e+08, device='cuda:0')
c= tensor(8.9740e+08, device='cuda:0')
c= tensor(8.9742e+08, device='cuda:0')
c= tensor(8.9743e+08, device='cuda:0')
c= tensor(8.9743e+08, device='cuda:0')
c= tensor(8.9744e+08, device='cuda:0')
c= tensor(8.9744e+08, device='cuda:0')
c= tensor(8.9744e+08, device='cuda:0')
c= tensor(8.9746e+08, device='cuda:0')
c= tensor(8.9747e+08, device='cuda:0')
c= tensor(8.9748e+08, device='cuda:0')
c= tensor(8.9750e+08, device='cuda:0')
c= tensor(8.9751e+08, device='cuda:0')
c= tensor(8.9752e+08, device='cuda:0')
c= tensor(8.9754e+08, device='cuda:0')
c= tensor(8.9754e+08, device='cuda:0')
c= tensor(8.9755e+08, device='cuda:0')
c= tensor(8.9755e+08, device='cuda:0')
c= tensor(8.9756e+08, device='cuda:0')
c= tensor(8.9757e+08, device='cuda:0')
c= tensor(8.9759e+08, device='cuda:0')
c= tensor(8.9761e+08, device='cuda:0')
c= tensor(8.9762e+08, device='cuda:0')
c= tensor(8.9763e+08, device='cuda:0')
c= tensor(8.9764e+08, device='cuda:0')
c= tensor(8.9777e+08, device='cuda:0')
c= tensor(8.9777e+08, device='cuda:0')
c= tensor(8.9777e+08, device='cuda:0')
c= tensor(8.9779e+08, device='cuda:0')
c= tensor(8.9779e+08, device='cuda:0')
c= tensor(8.9780e+08, device='cuda:0')
c= tensor(8.9780e+08, device='cuda:0')
c= tensor(8.9781e+08, device='cuda:0')
c= tensor(8.9781e+08, device='cuda:0')
c= tensor(8.9782e+08, device='cuda:0')
c= tensor(8.9782e+08, device='cuda:0')
c= tensor(8.9783e+08, device='cuda:0')
c= tensor(8.9783e+08, device='cuda:0')
c= tensor(8.9785e+08, device='cuda:0')
c= tensor(8.9787e+08, device='cuda:0')
c= tensor(8.9788e+08, device='cuda:0')
c= tensor(8.9788e+08, device='cuda:0')
c= tensor(8.9789e+08, device='cuda:0')
c= tensor(8.9790e+08, device='cuda:0')
c= tensor(8.9791e+08, device='cuda:0')
c= tensor(8.9794e+08, device='cuda:0')
c= tensor(8.9795e+08, device='cuda:0')
c= tensor(8.9795e+08, device='cuda:0')
c= tensor(8.9796e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9798e+08, device='cuda:0')
c= tensor(8.9799e+08, device='cuda:0')
c= tensor(8.9799e+08, device='cuda:0')
c= tensor(8.9800e+08, device='cuda:0')
c= tensor(8.9801e+08, device='cuda:0')
c= tensor(8.9801e+08, device='cuda:0')
c= tensor(8.9802e+08, device='cuda:0')
c= tensor(8.9803e+08, device='cuda:0')
c= tensor(8.9803e+08, device='cuda:0')
c= tensor(8.9804e+08, device='cuda:0')
c= tensor(8.9805e+08, device='cuda:0')
c= tensor(8.9805e+08, device='cuda:0')
c= tensor(8.9805e+08, device='cuda:0')
c= tensor(8.9806e+08, device='cuda:0')
c= tensor(8.9807e+08, device='cuda:0')
c= tensor(8.9808e+08, device='cuda:0')
c= tensor(8.9809e+08, device='cuda:0')
c= tensor(8.9809e+08, device='cuda:0')
c= tensor(8.9810e+08, device='cuda:0')
c= tensor(8.9813e+08, device='cuda:0')
c= tensor(8.9813e+08, device='cuda:0')
c= tensor(8.9819e+08, device='cuda:0')
c= tensor(9.0109e+08, device='cuda:0')
c= tensor(9.0141e+08, device='cuda:0')
c= tensor(9.0142e+08, device='cuda:0')
c= tensor(9.0142e+08, device='cuda:0')
c= tensor(9.0144e+08, device='cuda:0')
c= tensor(9.4216e+08, device='cuda:0')
c= tensor(1.0090e+09, device='cuda:0')
c= tensor(1.0090e+09, device='cuda:0')
c= tensor(1.0128e+09, device='cuda:0')
c= tensor(1.0134e+09, device='cuda:0')
c= tensor(1.0138e+09, device='cuda:0')
c= tensor(1.0367e+09, device='cuda:0')
c= tensor(1.0367e+09, device='cuda:0')
c= tensor(1.0368e+09, device='cuda:0')
c= tensor(1.0534e+09, device='cuda:0')
c= tensor(1.1008e+09, device='cuda:0')
c= tensor(1.1008e+09, device='cuda:0')
c= tensor(1.1011e+09, device='cuda:0')
c= tensor(1.2026e+09, device='cuda:0')
c= tensor(1.2095e+09, device='cuda:0')
c= tensor(1.2122e+09, device='cuda:0')
c= tensor(1.2138e+09, device='cuda:0')
c= tensor(1.2148e+09, device='cuda:0')
c= tensor(1.2149e+09, device='cuda:0')
c= tensor(1.2149e+09, device='cuda:0')
c= tensor(1.3807e+09, device='cuda:0')
c= tensor(1.3807e+09, device='cuda:0')
c= tensor(1.3807e+09, device='cuda:0')
c= tensor(1.3819e+09, device='cuda:0')
c= tensor(1.3826e+09, device='cuda:0')
c= tensor(1.4276e+09, device='cuda:0')
c= tensor(1.4316e+09, device='cuda:0')
c= tensor(1.4316e+09, device='cuda:0')
c= tensor(1.4321e+09, device='cuda:0')
c= tensor(1.4321e+09, device='cuda:0')
c= tensor(1.4334e+09, device='cuda:0')
c= tensor(1.4360e+09, device='cuda:0')
c= tensor(1.4376e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4404e+09, device='cuda:0')
c= tensor(1.4432e+09, device='cuda:0')
c= tensor(1.4438e+09, device='cuda:0')
c= tensor(1.4443e+09, device='cuda:0')
c= tensor(1.4800e+09, device='cuda:0')
c= tensor(1.4800e+09, device='cuda:0')
c= tensor(1.4803e+09, device='cuda:0')
c= tensor(1.4826e+09, device='cuda:0')
c= tensor(1.4826e+09, device='cuda:0')
c= tensor(1.4870e+09, device='cuda:0')
c= tensor(1.5197e+09, device='cuda:0')
c= tensor(1.5567e+09, device='cuda:0')
c= tensor(1.5569e+09, device='cuda:0')
c= tensor(1.5570e+09, device='cuda:0')
c= tensor(1.5571e+09, device='cuda:0')
c= tensor(1.5571e+09, device='cuda:0')
c= tensor(1.5579e+09, device='cuda:0')
c= tensor(1.5579e+09, device='cuda:0')
c= tensor(1.5583e+09, device='cuda:0')
c= tensor(1.6294e+09, device='cuda:0')
c= tensor(1.6321e+09, device='cuda:0')
c= tensor(1.6322e+09, device='cuda:0')
c= tensor(1.6323e+09, device='cuda:0')
c= tensor(1.6411e+09, device='cuda:0')
c= tensor(1.6414e+09, device='cuda:0')
c= tensor(1.6425e+09, device='cuda:0')
c= tensor(1.6426e+09, device='cuda:0')
c= tensor(1.7338e+09, device='cuda:0')
c= tensor(1.7339e+09, device='cuda:0')
c= tensor(1.7438e+09, device='cuda:0')
c= tensor(1.7438e+09, device='cuda:0')
c= tensor(1.7472e+09, device='cuda:0')
c= tensor(1.7489e+09, device='cuda:0')
c= tensor(1.8326e+09, device='cuda:0')
c= tensor(1.8345e+09, device='cuda:0')
c= tensor(1.8345e+09, device='cuda:0')
c= tensor(1.8395e+09, device='cuda:0')
c= tensor(1.8436e+09, device='cuda:0')
c= tensor(1.8436e+09, device='cuda:0')
c= tensor(1.8472e+09, device='cuda:0')
c= tensor(1.8536e+09, device='cuda:0')
c= tensor(1.8751e+09, device='cuda:0')
c= tensor(1.8859e+09, device='cuda:0')
c= tensor(1.8859e+09, device='cuda:0')
c= tensor(1.8860e+09, device='cuda:0')
c= tensor(1.8903e+09, device='cuda:0')
c= tensor(1.9054e+09, device='cuda:0')
c= tensor(1.9057e+09, device='cuda:0')
c= tensor(1.9057e+09, device='cuda:0')
c= tensor(1.9074e+09, device='cuda:0')
c= tensor(1.9153e+09, device='cuda:0')
c= tensor(1.9229e+09, device='cuda:0')
c= tensor(1.9229e+09, device='cuda:0')
c= tensor(1.9231e+09, device='cuda:0')
c= tensor(1.9232e+09, device='cuda:0')
c= tensor(1.9232e+09, device='cuda:0')
c= tensor(1.9233e+09, device='cuda:0')
c= tensor(1.9233e+09, device='cuda:0')
c= tensor(1.9253e+09, device='cuda:0')
c= tensor(1.9261e+09, device='cuda:0')
c= tensor(1.9264e+09, device='cuda:0')
c= tensor(1.9272e+09, device='cuda:0')
c= tensor(1.9272e+09, device='cuda:0')
c= tensor(1.9944e+09, device='cuda:0')
c= tensor(1.9945e+09, device='cuda:0')
c= tensor(1.9985e+09, device='cuda:0')
c= tensor(1.9986e+09, device='cuda:0')
c= tensor(1.9986e+09, device='cuda:0')
c= tensor(1.9986e+09, device='cuda:0')
c= tensor(2.0027e+09, device='cuda:0')
c= tensor(2.0027e+09, device='cuda:0')
c= tensor(2.0183e+09, device='cuda:0')
c= tensor(2.0183e+09, device='cuda:0')
c= tensor(2.0183e+09, device='cuda:0')
c= tensor(2.0344e+09, device='cuda:0')
c= tensor(2.0357e+09, device='cuda:0')
c= tensor(2.0371e+09, device='cuda:0')
c= tensor(2.0535e+09, device='cuda:0')
c= tensor(2.0704e+09, device='cuda:0')
c= tensor(2.0704e+09, device='cuda:0')
c= tensor(2.0704e+09, device='cuda:0')
c= tensor(2.0711e+09, device='cuda:0')
c= tensor(2.0711e+09, device='cuda:0')
c= tensor(2.0711e+09, device='cuda:0')
c= tensor(2.0713e+09, device='cuda:0')
c= tensor(2.0713e+09, device='cuda:0')
c= tensor(2.0713e+09, device='cuda:0')
c= tensor(2.0713e+09, device='cuda:0')
c= tensor(2.0714e+09, device='cuda:0')
c= tensor(2.1179e+09, device='cuda:0')
c= tensor(2.1179e+09, device='cuda:0')
c= tensor(2.1213e+09, device='cuda:0')
c= tensor(2.1215e+09, device='cuda:0')
c= tensor(2.1222e+09, device='cuda:0')
c= tensor(2.1224e+09, device='cuda:0')
c= tensor(2.5550e+09, device='cuda:0')
c= tensor(2.6535e+09, device='cuda:0')
c= tensor(2.6540e+09, device='cuda:0')
c= tensor(2.6622e+09, device='cuda:0')
c= tensor(2.6622e+09, device='cuda:0')
c= tensor(2.6635e+09, device='cuda:0')
c= tensor(2.6665e+09, device='cuda:0')
c= tensor(2.6677e+09, device='cuda:0')
c= tensor(2.6677e+09, device='cuda:0')
c= tensor(2.6708e+09, device='cuda:0')
c= tensor(2.7334e+09, device='cuda:0')
c= tensor(2.7340e+09, device='cuda:0')
c= tensor(2.7340e+09, device='cuda:0')
c= tensor(2.7356e+09, device='cuda:0')
c= tensor(2.7357e+09, device='cuda:0')
c= tensor(2.7357e+09, device='cuda:0')
c= tensor(2.7449e+09, device='cuda:0')
c= tensor(2.7455e+09, device='cuda:0')
c= tensor(2.7455e+09, device='cuda:0')
c= tensor(2.7487e+09, device='cuda:0')
c= tensor(2.7488e+09, device='cuda:0')
c= tensor(2.7488e+09, device='cuda:0')
c= tensor(2.7561e+09, device='cuda:0')
c= tensor(2.7595e+09, device='cuda:0')
c= tensor(2.7639e+09, device='cuda:0')
c= tensor(2.7807e+09, device='cuda:0')
c= tensor(2.7925e+09, device='cuda:0')
c= tensor(2.7925e+09, device='cuda:0')
c= tensor(2.7939e+09, device='cuda:0')
c= tensor(2.7982e+09, device='cuda:0')
c= tensor(2.8062e+09, device='cuda:0')
c= tensor(2.8063e+09, device='cuda:0')
c= tensor(2.9596e+09, device='cuda:0')
c= tensor(3.0897e+09, device='cuda:0')
c= tensor(3.0940e+09, device='cuda:0')
c= tensor(3.0982e+09, device='cuda:0')
c= tensor(3.1000e+09, device='cuda:0')
c= tensor(3.1001e+09, device='cuda:0')
c= tensor(3.1001e+09, device='cuda:0')
c= tensor(3.1001e+09, device='cuda:0')
c= tensor(3.1044e+09, device='cuda:0')
c= tensor(3.1106e+09, device='cuda:0')
c= tensor(3.1552e+09, device='cuda:0')
c= tensor(3.1831e+09, device='cuda:0')
c= tensor(3.1853e+09, device='cuda:0')
c= tensor(3.1856e+09, device='cuda:0')
c= tensor(3.1903e+09, device='cuda:0')
c= tensor(3.1903e+09, device='cuda:0')
c= tensor(3.1903e+09, device='cuda:0')
c= tensor(3.2097e+09, device='cuda:0')
c= tensor(3.2098e+09, device='cuda:0')
c= tensor(3.2098e+09, device='cuda:0')
c= tensor(3.2099e+09, device='cuda:0')
c= tensor(3.2209e+09, device='cuda:0')
c= tensor(3.2214e+09, device='cuda:0')
c= tensor(3.2315e+09, device='cuda:0')
c= tensor(3.2316e+09, device='cuda:0')
c= tensor(3.2319e+09, device='cuda:0')
c= tensor(3.2319e+09, device='cuda:0')
c= tensor(3.2319e+09, device='cuda:0')
c= tensor(3.2330e+09, device='cuda:0')
c= tensor(3.2343e+09, device='cuda:0')
c= tensor(3.2343e+09, device='cuda:0')
c= tensor(3.2408e+09, device='cuda:0')
c= tensor(3.2408e+09, device='cuda:0')
c= tensor(3.2411e+09, device='cuda:0')
c= tensor(3.2412e+09, device='cuda:0')
c= tensor(3.2429e+09, device='cuda:0')
c= tensor(3.2432e+09, device='cuda:0')
c= tensor(3.2444e+09, device='cuda:0')
c= tensor(3.2445e+09, device='cuda:0')
c= tensor(3.2448e+09, device='cuda:0')
c= tensor(3.2465e+09, device='cuda:0')
c= tensor(3.3037e+09, device='cuda:0')
c= tensor(3.3037e+09, device='cuda:0')
c= tensor(3.3039e+09, device='cuda:0')
c= tensor(3.3106e+09, device='cuda:0')
c= tensor(3.3107e+09, device='cuda:0')
c= tensor(3.3979e+09, device='cuda:0')
c= tensor(3.3979e+09, device='cuda:0')
c= tensor(3.4005e+09, device='cuda:0')
c= tensor(3.4120e+09, device='cuda:0')
c= tensor(3.4120e+09, device='cuda:0')
c= tensor(3.4321e+09, device='cuda:0')
c= tensor(3.4340e+09, device='cuda:0')
c= tensor(3.6199e+09, device='cuda:0')
c= tensor(3.6200e+09, device='cuda:0')
c= tensor(3.6214e+09, device='cuda:0')
c= tensor(3.6215e+09, device='cuda:0')
c= tensor(3.6215e+09, device='cuda:0')
c= tensor(3.6216e+09, device='cuda:0')
c= tensor(3.6239e+09, device='cuda:0')
c= tensor(3.6244e+09, device='cuda:0')
c= tensor(3.6276e+09, device='cuda:0')
c= tensor(3.6277e+09, device='cuda:0')
c= tensor(3.6277e+09, device='cuda:0')
c= tensor(3.6278e+09, device='cuda:0')
c= tensor(3.6494e+09, device='cuda:0')
c= tensor(3.6515e+09, device='cuda:0')
c= tensor(3.6779e+09, device='cuda:0')
c= tensor(3.6787e+09, device='cuda:0')
c= tensor(3.6788e+09, device='cuda:0')
c= tensor(3.6788e+09, device='cuda:0')
c= tensor(3.6792e+09, device='cuda:0')
c= tensor(4.4107e+09, device='cuda:0')
c= tensor(4.4108e+09, device='cuda:0')
c= tensor(4.4109e+09, device='cuda:0')
c= tensor(4.4149e+09, device='cuda:0')
c= tensor(4.4172e+09, device='cuda:0')
c= tensor(4.4172e+09, device='cuda:0')
c= tensor(4.4172e+09, device='cuda:0')
c= tensor(4.4329e+09, device='cuda:0')
c= tensor(4.4337e+09, device='cuda:0')
c= tensor(4.4364e+09, device='cuda:0')
c= tensor(4.4369e+09, device='cuda:0')
c= tensor(4.4586e+09, device='cuda:0')
c= tensor(4.4648e+09, device='cuda:0')
c= tensor(4.4876e+09, device='cuda:0')
c= tensor(4.4926e+09, device='cuda:0')
c= tensor(4.4931e+09, device='cuda:0')
c= tensor(4.4955e+09, device='cuda:0')
c= tensor(4.5022e+09, device='cuda:0')
c= tensor(4.5023e+09, device='cuda:0')
c= tensor(4.5024e+09, device='cuda:0')
c= tensor(4.5025e+09, device='cuda:0')
c= tensor(4.5064e+09, device='cuda:0')
c= tensor(4.5065e+09, device='cuda:0')
c= tensor(4.5065e+09, device='cuda:0')
c= tensor(4.5066e+09, device='cuda:0')
c= tensor(4.5086e+09, device='cuda:0')
c= tensor(4.5133e+09, device='cuda:0')
c= tensor(4.5136e+09, device='cuda:0')
c= tensor(4.5139e+09, device='cuda:0')
c= tensor(4.5140e+09, device='cuda:0')
c= tensor(4.5143e+09, device='cuda:0')
c= tensor(4.5143e+09, device='cuda:0')
c= tensor(4.5143e+09, device='cuda:0')
c= tensor(4.5145e+09, device='cuda:0')
c= tensor(4.5418e+09, device='cuda:0')
c= tensor(4.5419e+09, device='cuda:0')
c= tensor(4.5419e+09, device='cuda:0')
c= tensor(4.5419e+09, device='cuda:0')
c= tensor(4.5717e+09, device='cuda:0')
c= tensor(4.5942e+09, device='cuda:0')
c= tensor(4.5948e+09, device='cuda:0')
c= tensor(4.5949e+09, device='cuda:0')
c= tensor(4.5980e+09, device='cuda:0')
c= tensor(4.6031e+09, device='cuda:0')
c= tensor(4.6031e+09, device='cuda:0')
c= tensor(4.6033e+09, device='cuda:0')
c= tensor(4.6034e+09, device='cuda:0')
c= tensor(4.6053e+09, device='cuda:0')
c= tensor(4.6066e+09, device='cuda:0')
c= tensor(4.6118e+09, device='cuda:0')
c= tensor(4.6118e+09, device='cuda:0')
c= tensor(4.6119e+09, device='cuda:0')
c= tensor(4.6119e+09, device='cuda:0')
c= tensor(4.6123e+09, device='cuda:0')
c= tensor(4.6126e+09, device='cuda:0')
c= tensor(4.6146e+09, device='cuda:0')
c= tensor(4.6297e+09, device='cuda:0')
c= tensor(4.6633e+09, device='cuda:0')
c= tensor(4.6633e+09, device='cuda:0')
c= tensor(4.6633e+09, device='cuda:0')
c= tensor(4.6634e+09, device='cuda:0')
c= tensor(4.6643e+09, device='cuda:0')
c= tensor(4.6646e+09, device='cuda:0')
c= tensor(4.6646e+09, device='cuda:0')
c= tensor(4.6646e+09, device='cuda:0')
c= tensor(4.6696e+09, device='cuda:0')
c= tensor(4.6696e+09, device='cuda:0')
c= tensor(4.6736e+09, device='cuda:0')
c= tensor(4.6736e+09, device='cuda:0')
c= tensor(4.6737e+09, device='cuda:0')
c= tensor(4.6738e+09, device='cuda:0')
c= tensor(4.6738e+09, device='cuda:0')
c= tensor(4.6738e+09, device='cuda:0')
c= tensor(4.6780e+09, device='cuda:0')
c= tensor(4.7225e+09, device='cuda:0')
c= tensor(4.7354e+09, device='cuda:0')
c= tensor(4.7384e+09, device='cuda:0')
c= tensor(4.7388e+09, device='cuda:0')
c= tensor(4.7388e+09, device='cuda:0')
c= tensor(4.7390e+09, device='cuda:0')
c= tensor(4.7441e+09, device='cuda:0')
c= tensor(4.7443e+09, device='cuda:0')
c= tensor(4.7449e+09, device='cuda:0')
c= tensor(4.7450e+09, device='cuda:0')
c= tensor(4.8912e+09, device='cuda:0')
c= tensor(4.8917e+09, device='cuda:0')
c= tensor(4.8925e+09, device='cuda:0')
c= tensor(4.9283e+09, device='cuda:0')
c= tensor(4.9287e+09, device='cuda:0')
c= tensor(4.9296e+09, device='cuda:0')
c= tensor(4.9419e+09, device='cuda:0')
c= tensor(4.9438e+09, device='cuda:0')
c= tensor(4.9445e+09, device='cuda:0')
c= tensor(4.9447e+09, device='cuda:0')
c= tensor(4.9456e+09, device='cuda:0')
c= tensor(4.9457e+09, device='cuda:0')
c= tensor(4.9487e+09, device='cuda:0')
c= tensor(5.1038e+09, device='cuda:0')
c= tensor(5.1278e+09, device='cuda:0')
c= tensor(5.1370e+09, device='cuda:0')
c= tensor(5.1371e+09, device='cuda:0')
c= tensor(5.1408e+09, device='cuda:0')
c= tensor(5.1419e+09, device='cuda:0')
c= tensor(5.1420e+09, device='cuda:0')
c= tensor(5.1425e+09, device='cuda:0')
c= tensor(5.1668e+09, device='cuda:0')
c= tensor(5.1673e+09, device='cuda:0')
c= tensor(5.1950e+09, device='cuda:0')
c= tensor(5.2010e+09, device='cuda:0')
c= tensor(5.2018e+09, device='cuda:0')
c= tensor(5.2018e+09, device='cuda:0')
c= tensor(5.2046e+09, device='cuda:0')
c= tensor(5.2089e+09, device='cuda:0')
c= tensor(5.2089e+09, device='cuda:0')
c= tensor(5.3037e+09, device='cuda:0')
c= tensor(5.3062e+09, device='cuda:0')
c= tensor(5.3085e+09, device='cuda:0')
c= tensor(5.3087e+09, device='cuda:0')
c= tensor(5.3087e+09, device='cuda:0')
c= tensor(5.3088e+09, device='cuda:0')
c= tensor(5.3088e+09, device='cuda:0')
c= tensor(5.3093e+09, device='cuda:0')
c= tensor(5.3115e+09, device='cuda:0')
c= tensor(7.4494e+09, device='cuda:0')
c= tensor(7.4496e+09, device='cuda:0')
c= tensor(7.4509e+09, device='cuda:0')
c= tensor(7.4510e+09, device='cuda:0')
c= tensor(7.4511e+09, device='cuda:0')
c= tensor(7.4512e+09, device='cuda:0')
c= tensor(7.4878e+09, device='cuda:0')
c= tensor(7.4879e+09, device='cuda:0')
c= tensor(7.7262e+09, device='cuda:0')
c= tensor(7.7262e+09, device='cuda:0')
c= tensor(7.7393e+09, device='cuda:0')
c= tensor(7.7418e+09, device='cuda:0')
c= tensor(7.7464e+09, device='cuda:0')
c= tensor(7.7730e+09, device='cuda:0')
c= tensor(7.7731e+09, device='cuda:0')
c= tensor(7.7731e+09, device='cuda:0')
c= tensor(7.7747e+09, device='cuda:0')
c= tensor(7.7748e+09, device='cuda:0')
c= tensor(7.7750e+09, device='cuda:0')
c= tensor(7.7764e+09, device='cuda:0')
c= tensor(7.8109e+09, device='cuda:0')
c= tensor(7.8124e+09, device='cuda:0')
c= tensor(7.8128e+09, device='cuda:0')
c= tensor(7.8363e+09, device='cuda:0')
c= tensor(7.8795e+09, device='cuda:0')
c= tensor(7.8800e+09, device='cuda:0')
c= tensor(7.8800e+09, device='cuda:0')
c= tensor(7.9003e+09, device='cuda:0')
c= tensor(7.9009e+09, device='cuda:0')
c= tensor(7.9025e+09, device='cuda:0')
c= tensor(7.9035e+09, device='cuda:0')
c= tensor(7.9067e+09, device='cuda:0')
c= tensor(7.9104e+09, device='cuda:0')
c= tensor(7.9439e+09, device='cuda:0')
c= tensor(7.9447e+09, device='cuda:0')
c= tensor(7.9448e+09, device='cuda:0')
c= tensor(7.9448e+09, device='cuda:0')
c= tensor(7.9454e+09, device='cuda:0')
c= tensor(7.9475e+09, device='cuda:0')
c= tensor(7.9543e+09, device='cuda:0')
c= tensor(7.9618e+09, device='cuda:0')
c= tensor(7.9618e+09, device='cuda:0')
c= tensor(7.9621e+09, device='cuda:0')
c= tensor(7.9650e+09, device='cuda:0')
c= tensor(7.9747e+09, device='cuda:0')
c= tensor(7.9763e+09, device='cuda:0')
c= tensor(7.9779e+09, device='cuda:0')
c= tensor(8.0439e+09, device='cuda:0')
c= tensor(8.0445e+09, device='cuda:0')
c= tensor(8.0445e+09, device='cuda:0')
c= tensor(8.0445e+09, device='cuda:0')
c= tensor(8.0507e+09, device='cuda:0')
c= tensor(8.0517e+09, device='cuda:0')
c= tensor(8.0528e+09, device='cuda:0')
c= tensor(8.0528e+09, device='cuda:0')
c= tensor(8.0528e+09, device='cuda:0')
c= tensor(8.0578e+09, device='cuda:0')
c= tensor(8.0597e+09, device='cuda:0')
c= tensor(8.0602e+09, device='cuda:0')
c= tensor(8.0602e+09, device='cuda:0')
c= tensor(8.0603e+09, device='cuda:0')
c= tensor(8.0611e+09, device='cuda:0')
c= tensor(8.0617e+09, device='cuda:0')
c= tensor(8.0716e+09, device='cuda:0')
c= tensor(8.0716e+09, device='cuda:0')
c= tensor(8.0906e+09, device='cuda:0')
c= tensor(8.0907e+09, device='cuda:0')
c= tensor(8.0909e+09, device='cuda:0')
c= tensor(8.2039e+09, device='cuda:0')
c= tensor(8.2042e+09, device='cuda:0')
c= tensor(8.2049e+09, device='cuda:0')
c= tensor(8.2114e+09, device='cuda:0')
c= tensor(8.2183e+09, device='cuda:0')
c= tensor(8.2290e+09, device='cuda:0')
c= tensor(8.2385e+09, device='cuda:0')
c= tensor(8.2385e+09, device='cuda:0')
c= tensor(8.2392e+09, device='cuda:0')
c= tensor(8.2393e+09, device='cuda:0')
c= tensor(8.2454e+09, device='cuda:0')
c= tensor(8.2515e+09, device='cuda:0')
c= tensor(8.2520e+09, device='cuda:0')
c= tensor(8.2543e+09, device='cuda:0')
c= tensor(8.2544e+09, device='cuda:0')
c= tensor(8.2561e+09, device='cuda:0')
c= tensor(8.2562e+09, device='cuda:0')
c= tensor(8.2563e+09, device='cuda:0')
c= tensor(8.2674e+09, device='cuda:0')
c= tensor(8.3114e+09, device='cuda:0')
c= tensor(8.3115e+09, device='cuda:0')
c= tensor(8.3118e+09, device='cuda:0')
c= tensor(8.3120e+09, device='cuda:0')
c= tensor(8.3317e+09, device='cuda:0')
c= tensor(8.3318e+09, device='cuda:0')
c= tensor(8.3319e+09, device='cuda:0')
c= tensor(8.3331e+09, device='cuda:0')
c= tensor(8.3360e+09, device='cuda:0')
c= tensor(8.3361e+09, device='cuda:0')
c= tensor(8.3382e+09, device='cuda:0')
c= tensor(8.3383e+09, device='cuda:0')
time to make c is 8.967193365097046
time for making loss is 8.967368602752686
p0 True
it  0 : 2646650368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5529882624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
5530075136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2291943400.0
relative error loss 0.27487016
shape of L is 
torch.Size([])
memory (bytes)
5556150272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5556355072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  2268845600.0
relative error loss 0.27210006
shape of L is 
torch.Size([])
memory (bytes)
5559808000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5559910400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2226090000.0
relative error loss 0.26697245
shape of L is 
torch.Size([])
memory (bytes)
5563064320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5563125760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2200961000.0
relative error loss 0.26395875
shape of L is 
torch.Size([])
memory (bytes)
5566251008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5566316544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2174623700.0
relative error loss 0.26080015
shape of L is 
torch.Size([])
memory (bytes)
5569540096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5569540096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2156079600.0
relative error loss 0.25857618
shape of L is 
torch.Size([])
memory (bytes)
5572497408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5572718592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2143652900.0
relative error loss 0.25708586
shape of L is 
torch.Size([])
memory (bytes)
5575950336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 19% |
memory (bytes)
5575950336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2129246700.0
relative error loss 0.25535813
shape of L is 
torch.Size([])
memory (bytes)
5578985472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5579149312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2121104900.0
relative error loss 0.2543817
shape of L is 
torch.Size([])
memory (bytes)
5582356480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
5582356480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2113470500.0
relative error loss 0.2534661
time to take a step is 254.6193323135376
it  1 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5585481728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5585567744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  2113470500.0
relative error loss 0.2534661
shape of L is 
torch.Size([])
memory (bytes)
5588754432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5588754432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  2105162200.0
relative error loss 0.25246972
shape of L is 
torch.Size([])
memory (bytes)
5591891968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 19% |
memory (bytes)
5591977984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2098907100.0
relative error loss 0.25171953
shape of L is 
torch.Size([])
memory (bytes)
5595021312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 19% |
memory (bytes)
5595185152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2093329900.0
relative error loss 0.25105068
shape of L is 
torch.Size([])
memory (bytes)
5598314496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5598396416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2086124000.0
relative error loss 0.25018647
shape of L is 
torch.Size([])
memory (bytes)
5601374208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5601599488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2082359300.0
relative error loss 0.24973498
shape of L is 
torch.Size([])
memory (bytes)
5604720640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5604806656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2077845500.0
relative error loss 0.24919365
shape of L is 
torch.Size([])
memory (bytes)
5607989248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5608022016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2071434800.0
relative error loss 0.24842481
shape of L is 
torch.Size([])
memory (bytes)
5611155456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5611229184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2068216800.0
relative error loss 0.24803889
shape of L is 
torch.Size([])
memory (bytes)
5614354432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
5614432256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2064420900.0
relative error loss 0.24758364
time to take a step is 250.65912103652954
it  2 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5617557504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5617643520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 19% |
error is  2064420900.0
relative error loss 0.24758364
shape of L is 
torch.Size([])
memory (bytes)
5620793344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
5620854784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2061848600.0
relative error loss 0.24727516
shape of L is 
torch.Size([])
memory (bytes)
5624057856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5624057856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2058611200.0
relative error loss 0.2468869
shape of L is 
torch.Size([])
memory (bytes)
5627174912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5627260928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2056578600.0
relative error loss 0.24664313
shape of L is 
torch.Size([])
memory (bytes)
5630468096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
5630468096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2052442600.0
relative error loss 0.24614711
shape of L is 
torch.Size([])
memory (bytes)
5633597440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5633597440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2050815500.0
relative error loss 0.24595197
shape of L is 
torch.Size([])
memory (bytes)
5636804608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5636890624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2048726500.0
relative error loss 0.24570145
shape of L is 
torch.Size([])
memory (bytes)
5640011776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5640097792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2046538800.0
relative error loss 0.24543907
shape of L is 
torch.Size([])
memory (bytes)
5643300864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5643300864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2044104700.0
relative error loss 0.24514715
shape of L is 
torch.Size([])
memory (bytes)
5646434304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5646516224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2042410000.0
relative error loss 0.2449439
time to take a step is 250.44595384597778
it  3 : 3320357888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5649723392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5649723392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2042410000.0
relative error loss 0.2449439
shape of L is 
torch.Size([])
memory (bytes)
5652848640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5652934656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 19% |
error is  2040672300.0
relative error loss 0.24473551
shape of L is 
torch.Size([])
memory (bytes)
5656141824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5656141824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2039202800.0
relative error loss 0.24455927
shape of L is 
torch.Size([])
memory (bytes)
5659283456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
5659348992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2037443600.0
relative error loss 0.24434829
shape of L is 
torch.Size([])
memory (bytes)
5662547968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5662547968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2036040700.0
relative error loss 0.24418005
shape of L is 
torch.Size([])
memory (bytes)
5665759232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
5665759232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2034876400.0
relative error loss 0.24404041
shape of L is 
torch.Size([])
memory (bytes)
5668773888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5668966400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 19% |
error is  2034026500.0
relative error loss 0.24393849
shape of L is 
torch.Size([])
memory (bytes)
5672173568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5672177664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2031467000.0
relative error loss 0.24363153
shape of L is 
torch.Size([])
memory (bytes)
5675380736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 19% |
memory (bytes)
5675384832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2030773200.0
relative error loss 0.24354832
shape of L is 
torch.Size([])
memory (bytes)
5678501888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5678587904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2029824000.0
relative error loss 0.24343449
time to take a step is 252.112637758255
it  4 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5681745920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5681795072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2029824000.0
relative error loss 0.24343449
shape of L is 
torch.Size([])
memory (bytes)
5684908032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5684989952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 19% |
error is  2028601300.0
relative error loss 0.24328785
shape of L is 
torch.Size([])
memory (bytes)
5688066048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5688209408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2027477000.0
relative error loss 0.243153
shape of L is 
torch.Size([])
memory (bytes)
5691338752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 19% |
memory (bytes)
5691424768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2026751500.0
relative error loss 0.243066
shape of L is 
torch.Size([])
memory (bytes)
5694480384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5694648320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2025594900.0
relative error loss 0.24292728
shape of L is 
torch.Size([])
memory (bytes)
5697765376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5697851392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2024692200.0
relative error loss 0.24281904
shape of L is 
torch.Size([])
memory (bytes)
5700972544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5701054464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 19% |
error is  2023456300.0
relative error loss 0.2426708
shape of L is 
torch.Size([])
memory (bytes)
5704175616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5704257536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2022944300.0
relative error loss 0.2426094
shape of L is 
torch.Size([])
memory (bytes)
5707304960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5707468800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2022321200.0
relative error loss 0.24253468
shape of L is 
torch.Size([])
memory (bytes)
5710680064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5710680064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 19% |
error is  2021192200.0
relative error loss 0.24239928
time to take a step is 253.7008068561554
it  5 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5713817600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 19% |
memory (bytes)
5713891328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2021192200.0
relative error loss 0.24239928
shape of L is 
torch.Size([])
memory (bytes)
5717016576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5717098496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2020996100.0
relative error loss 0.24237576
shape of L is 
torch.Size([])
memory (bytes)
5720240128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5720322048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2019765200.0
relative error loss 0.24222815
shape of L is 
torch.Size([])
memory (bytes)
5723451392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
5723533312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2019353100.0
relative error loss 0.24217872
shape of L is 
torch.Size([])
memory (bytes)
5726670848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5726732288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2018355200.0
relative error loss 0.24205904
shape of L is 
torch.Size([])
memory (bytes)
5729849344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5729935360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2017390600.0
relative error loss 0.24194336
shape of L is 
torch.Size([])
memory (bytes)
5733064704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
5733146624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2016652300.0
relative error loss 0.24185482
shape of L is 
torch.Size([])
memory (bytes)
5736357888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5736357888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2015909900.0
relative error loss 0.24176578
shape of L is 
torch.Size([])
memory (bytes)
5739548672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 19% |
memory (bytes)
5739569152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2015168500.0
relative error loss 0.24167687
shape of L is 
torch.Size([])
memory (bytes)
5742755840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5742772224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2014396900.0
relative error loss 0.24158433
time to take a step is 252.20138597488403
it  6 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5745897472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 19% |
memory (bytes)
5745979392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2014396900.0
relative error loss 0.24158433
shape of L is 
torch.Size([])
memory (bytes)
5749133312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
5749190656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2013618700.0
relative error loss 0.241491
shape of L is 
torch.Size([])
memory (bytes)
5752324096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
5752410112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2012959200.0
relative error loss 0.24141191
shape of L is 
torch.Size([])
memory (bytes)
5755547648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
5755613184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2011761200.0
relative error loss 0.24126823
shape of L is 
torch.Size([])
memory (bytes)
5758746624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5758828544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2011259900.0
relative error loss 0.2412081
shape of L is 
torch.Size([])
memory (bytes)
5761953792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5762039808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 19% |
error is  2010611700.0
relative error loss 0.24113038
shape of L is 
torch.Size([])
memory (bytes)
5765152768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5765238784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2010278400.0
relative error loss 0.2410904
shape of L is 
torch.Size([])
memory (bytes)
5768458240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 19% |
memory (bytes)
5768458240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2009722900.0
relative error loss 0.24102378
shape of L is 
torch.Size([])
memory (bytes)
5771587584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5771673600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2009084900.0
relative error loss 0.24094728
shape of L is 
torch.Size([])
memory (bytes)
5774831616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5774876672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2008429000.0
relative error loss 0.24086861
time to take a step is 251.7312593460083
it  7 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5778022400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5778092032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2008429000.0
relative error loss 0.24086861
shape of L is 
torch.Size([])
memory (bytes)
5781291008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 19% |
memory (bytes)
5781291008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2007844900.0
relative error loss 0.24079855
shape of L is 
torch.Size([])
memory (bytes)
5784412160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
5784502272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2006929900.0
relative error loss 0.24068882
shape of L is 
torch.Size([])
memory (bytes)
5787623424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5787709440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2007866400.0
relative error loss 0.24080113
shape of L is 
torch.Size([])
memory (bytes)
5790765056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 19% |
memory (bytes)
5790912512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2006492700.0
relative error loss 0.24063638
shape of L is 
torch.Size([])
memory (bytes)
5794119680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5794119680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 19% |
error is  2006123000.0
relative error loss 0.24059205
shape of L is 
torch.Size([])
memory (bytes)
5797228544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5797318656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2005717500.0
relative error loss 0.24054343
shape of L is 
torch.Size([])
memory (bytes)
5800525824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5800525824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2005399600.0
relative error loss 0.2405053
shape of L is 
torch.Size([])
memory (bytes)
5803642880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5803642880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2005066200.0
relative error loss 0.24046531
shape of L is 
torch.Size([])
memory (bytes)
5806854144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5806940160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2004787700.0
relative error loss 0.2404319
time to take a step is 252.8650782108307
it  8 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5810147328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5810151424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  2004787700.0
relative error loss 0.2404319
shape of L is 
torch.Size([])
memory (bytes)
5813280768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5813366784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2004503000.0
relative error loss 0.24039777
shape of L is 
torch.Size([])
memory (bytes)
5816487936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5816573952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2004234200.0
relative error loss 0.24036554
shape of L is 
torch.Size([])
memory (bytes)
5819609088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5819772928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2004109300.0
relative error loss 0.24035054
shape of L is 
torch.Size([])
memory (bytes)
5822906368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5822988288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2003881000.0
relative error loss 0.24032317
shape of L is 
torch.Size([])
memory (bytes)
5826068480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 19% |
memory (bytes)
5826195456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2003720200.0
relative error loss 0.24030389
shape of L is 
torch.Size([])
memory (bytes)
5829394432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
5829394432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2003480000.0
relative error loss 0.24027508
shape of L is 
torch.Size([])
memory (bytes)
5832605696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5832605696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2003128300.0
relative error loss 0.2402329
shape of L is 
torch.Size([])
memory (bytes)
5835821056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 19% |
memory (bytes)
5835821056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2002691600.0
relative error loss 0.24018052
shape of L is 
torch.Size([])
memory (bytes)
5839020032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 19% |
memory (bytes)
5839020032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2002405400.0
relative error loss 0.2401462
time to take a step is 252.71836233139038
it  9 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5842149376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5842235392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2002405400.0
relative error loss 0.2401462
shape of L is 
torch.Size([])
memory (bytes)
5845385216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5845434368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2002090000.0
relative error loss 0.24010837
shape of L is 
torch.Size([])
memory (bytes)
5848633344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5848653824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 19% |
error is  2002018800.0
relative error loss 0.24009983
shape of L is 
torch.Size([])
memory (bytes)
5851783168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5851865088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2001607700.0
relative error loss 0.24005054
shape of L is 
torch.Size([])
memory (bytes)
5854986240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 19% |
memory (bytes)
5855080448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2001450500.0
relative error loss 0.24003167
shape of L is 
torch.Size([])
memory (bytes)
5858287616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5858287616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2001261000.0
relative error loss 0.24000897
shape of L is 
torch.Size([])
memory (bytes)
5861453824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 19% |
memory (bytes)
5861494784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2000994300.0
relative error loss 0.23997697
shape of L is 
torch.Size([])
memory (bytes)
5864718336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5864718336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2000955400.0
relative error loss 0.23997231
shape of L is 
torch.Size([])
memory (bytes)
5867827200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5867913216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2000768000.0
relative error loss 0.23994984
shape of L is 
torch.Size([])
memory (bytes)
5871116288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5871116288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2000494100.0
relative error loss 0.23991698
time to take a step is 251.48220324516296
it  10 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5874331648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 19% |
memory (bytes)
5874331648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2000494100.0
relative error loss 0.23991698
shape of L is 
torch.Size([])
memory (bytes)
5877448704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5877534720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  2000226300.0
relative error loss 0.23988487
shape of L is 
torch.Size([])
memory (bytes)
5880745984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5880745984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1999843800.0
relative error loss 0.239839
shape of L is 
torch.Size([])
memory (bytes)
5883953152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5883953152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1999522300.0
relative error loss 0.23980044
shape of L is 
torch.Size([])
memory (bytes)
5887156224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5887156224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1999294000.0
relative error loss 0.23977305
shape of L is 
torch.Size([])
memory (bytes)
5890232320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 19% |
memory (bytes)
5890351104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1999035400.0
relative error loss 0.23974204
shape of L is 
torch.Size([])
memory (bytes)
5893562368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 19% |
memory (bytes)
5893562368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1998703600.0
relative error loss 0.23970225
shape of L is 
torch.Size([])
memory (bytes)
5896749056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 19% |
memory (bytes)
5896749056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1998834700.0
relative error loss 0.23971798
shape of L is 
torch.Size([])
memory (bytes)
5899972608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 19% |
memory (bytes)
5899972608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1998478300.0
relative error loss 0.23967524
shape of L is 
torch.Size([])
memory (bytes)
5903118336
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5903118336
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 95% | 19% |
error is  1998179800.0
relative error loss 0.23963943
time to take a step is 252.04719853401184
it  11 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5906382848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5906382848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1998179800.0
relative error loss 0.23963943
shape of L is 
torch.Size([])
memory (bytes)
5909508096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5909594112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1997981700.0
relative error loss 0.23961568
shape of L is 
torch.Size([])
memory (bytes)
5912752128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5912805376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1997668400.0
relative error loss 0.2395781
shape of L is 
torch.Size([])
memory (bytes)
5916020736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5916020736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1997318700.0
relative error loss 0.23953615
shape of L is 
torch.Size([])
memory (bytes)
5919051776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5919219712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1996985900.0
relative error loss 0.23949625
shape of L is 
torch.Size([])
memory (bytes)
5922381824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5922426880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1996689900.0
relative error loss 0.23946075
shape of L is 
torch.Size([])
memory (bytes)
5925560320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5925634048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 19% |
error is  1996443600.0
relative error loss 0.23943122
shape of L is 
torch.Size([])
memory (bytes)
5928845312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5928845312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 19% |
error is  1996231200.0
relative error loss 0.23940574
shape of L is 
torch.Size([])
memory (bytes)
5931937792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5932052480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1996051500.0
relative error loss 0.23938417
shape of L is 
torch.Size([])
memory (bytes)
5935247360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5935247360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1995886100.0
relative error loss 0.23936434
time to take a step is 252.05459785461426
it  12 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5938315264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5938450432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1995886100.0
relative error loss 0.23936434
shape of L is 
torch.Size([])
memory (bytes)
5941657600
| ID | GPU | MEM |
------------------
|  0 | 20% |  0% |
|  1 | 13% | 19% |
memory (bytes)
5941657600
| ID | GPU  | MEM |
-------------------
|  0 |  17% |  0% |
|  1 | 100% | 19% |
error is  1995827700.0
relative error loss 0.23935735
shape of L is 
torch.Size([])
memory (bytes)
5944864768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
memory (bytes)
5944868864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1995756000.0
relative error loss 0.23934875
shape of L is 
torch.Size([])
memory (bytes)
5947994112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5948076032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 19% |
error is  1995723800.0
relative error loss 0.23934488
shape of L is 
torch.Size([])
memory (bytes)
5951283200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 19% |
memory (bytes)
5951287296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1995673600.0
relative error loss 0.23933886
shape of L is 
torch.Size([])
memory (bytes)
5954490368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5954490368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 19% |
error is  1995602400.0
relative error loss 0.23933034
shape of L is 
torch.Size([])
memory (bytes)
5957607424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5957607424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1995476000.0
relative error loss 0.23931517
shape of L is 
torch.Size([])
memory (bytes)
5960900608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
5960900608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1995435000.0
relative error loss 0.23931025
shape of L is 
torch.Size([])
memory (bytes)
5964107776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
5964107776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1995175400.0
relative error loss 0.23927912
shape of L is 
torch.Size([])
memory (bytes)
5967237120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 19% |
memory (bytes)
5967319040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1995046900.0
relative error loss 0.23926371
time to take a step is 255.53251767158508
it  13 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
5970432000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
5970513920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 19% |
error is  1995046900.0
relative error loss 0.23926371
shape of L is 
torch.Size([])
memory (bytes)
5973594112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
5973725184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1994836000.0
relative error loss 0.23923841
shape of L is 
torch.Size([])
memory (bytes)
5976924160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5976932352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1994636800.0
relative error loss 0.23921452
shape of L is 
torch.Size([])
memory (bytes)
5980065792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5980135424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 19% |
error is  1994356700.0
relative error loss 0.23918094
shape of L is 
torch.Size([])
memory (bytes)
5983346688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 19% |
memory (bytes)
5983346688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1994097200.0
relative error loss 0.23914981
shape of L is 
torch.Size([])
memory (bytes)
5986545664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5986553856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1993881100.0
relative error loss 0.2391239
shape of L is 
torch.Size([])
memory (bytes)
5989765120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
5989765120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1993603100.0
relative error loss 0.23909055
shape of L is 
torch.Size([])
memory (bytes)
5992796160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
5992960000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1993475100.0
relative error loss 0.2390752
shape of L is 
torch.Size([])
memory (bytes)
5996158976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
5996158976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1993298400.0
relative error loss 0.23905401
shape of L is 
torch.Size([])
memory (bytes)
5999202304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 39% | 19% |
memory (bytes)
5999370240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1993089000.0
relative error loss 0.2390289
time to take a step is 237.89578461647034
it  14 : 3320039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
6002577408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 19% |
memory (bytes)
6002577408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1993089000.0
relative error loss 0.2390289
shape of L is 
torch.Size([])
memory (bytes)
6005727232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6005788672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1992957400.0
relative error loss 0.23901312
shape of L is 
torch.Size([])
memory (bytes)
6008860672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6009004032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1992726000.0
relative error loss 0.23898536
shape of L is 
torch.Size([])
memory (bytes)
6012198912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 19% |
memory (bytes)
6012198912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1992557000.0
relative error loss 0.23896511
shape of L is 
torch.Size([])
memory (bytes)
6015287296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 19% |
memory (bytes)
6015414272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1992436700.0
relative error loss 0.23895067
shape of L is 
torch.Size([])
memory (bytes)
6018621440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6018625536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1992305200.0
relative error loss 0.23893489
shape of L is 
torch.Size([])
memory (bytes)
6021771264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 19% |
memory (bytes)
6021832704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1992166400.0
relative error loss 0.23891824
shape of L is 
torch.Size([])
memory (bytes)
6025039872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 19% |
memory (bytes)
6025039872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1991974900.0
relative error loss 0.23889528
shape of L is 
torch.Size([])
memory (bytes)
6028165120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 19% |
memory (bytes)
6028255232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1991771100.0
relative error loss 0.23887084
shape of L is 
torch.Size([])
memory (bytes)
6031454208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 19% |
memory (bytes)
6031454208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 19% |
error is  1991517200.0
relative error loss 0.23884039
time to take a step is 244.7705042362213
sum tnnu_Z after tensor(13294034., device='cuda:0')
shape of features
(6633,)
shape of features
(6633,)
number of orig particles 26532
number of new particles after remove low mass 26517
tnuZ shape should be parts x labs
torch.Size([26532, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  2291864000.0
relative error without small mass is  0.27486065
nnu_Z shape should be number of particles by maxV
(26532, 702)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
shape of features
(26532,)
Thu Feb 2 19:41:51 EST 2023
