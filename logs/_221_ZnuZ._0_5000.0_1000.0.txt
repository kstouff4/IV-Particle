Wed Feb 1 07:15:54 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 4221892
numbers of Z: 18889
shape of features
(18889,)
shape of features
(18889,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01655561142403363	18889	18.889	0.09570003265754985
X	0.01327468998554338	108	0.108	0.49720277513171557
X	0.014241604841398503	2312	2.312	0.18331177690151626
X	0.013851164810521153	303	0.303	0.3575591444454903
X	0.012063114371951818	194	0.194	0.3961739405177914
X	0.015721040896136237	4674	4.674	0.14982956885272627
X	0.013872809738273918	319	0.319	0.351661304131562
X	0.014457547191564274	9989	9.989	0.11311604234629344
X	0.014640833098620822	2261	2.261	0.1863896284005238
X	0.013332209022066605	462	0.462	0.3067279693555384
X	0.01453444338286155	899	0.899	0.2528596723904419
X	0.012936640882318938	1138	1.138	0.22485010089765733
X	0.014161485802907162	28916	28.916	0.0788237093530737
X	0.014375099883743614	927	0.927	0.2493697169253733
X	0.015368436411111934	7436	7.436	0.12737868696226043
X	0.015368723975365487	1262	1.262	0.23006973128944366
X	0.014049174413133974	1061	1.061	0.2365802429206188
X	0.014959256024108355	2787	2.787	0.17508852151422277
X	0.0143978622648407	11717	11.717	0.10710930341879699
X	0.014599291296513436	31065	31.065	0.07774756665672863
X	0.01448354479734288	19255	19.255	0.09094464136109086
X	0.01329767172861292	361	0.361	0.33272809120705277
X	0.01471027170447081	16277	16.277	0.09668270150974992
X	0.014429595426014885	1000	1.0	0.24345463739870704
X	0.015269376412634036	1884	1.884	0.20086925442884715
X	0.013665398407102052	3006	3.006	0.1656575016105291
X	0.014663297738788526	16466	16.466	0.09620873713035251
X	0.014908961420762925	13568	13.568	0.10319148255263588
X	0.014351268836296116	2505	2.505	0.17893454360694058
X	0.014174570844426904	25089	25.089	0.08266890575221007
X	0.01521927508077932	123841	123.841	0.049717563700272395
X	0.014190985834852649	578	0.578	0.29064414331672733
X	0.015063099767262951	83915	83.915	0.05641027945545777
X	0.01472015247608376	790	0.79	0.26511140160209573
X	0.013705008869461353	275	0.275	0.3680008956759627
X	0.014217028201954888	396	0.396	0.32989156635926536
X	0.014311399410377713	5116	5.116	0.14090201013131118
X	0.014431428209271572	9505	9.505	0.11493502572185489
X	0.013679325805481398	324	0.324	0.3482089217999378
X	0.014319145488689372	833	0.833	0.258081792700735
X	0.01242205106144394	377	0.377	0.32059050329177946
X	0.01345242087166099	298	0.298	0.3560639953503634
X	0.01180850029943025	105	0.105	0.4826902005303133
X	0.011752843224495449	71	0.071	0.5490706101947723
X	0.013421686814808527	191	0.191	0.4126589268889994
X	0.01214530045275144	81	0.081	0.5312607707616995
X	0.013067485029998702	102	0.102	0.504116152699937
X	0.01395377289082287	631	0.631	0.28068600783974407
X	0.014056072729584236	438	0.438	0.31778248858029534
X	0.01271656827085788	183	0.183	0.41112410951278366
X	0.013638835405675167	943	0.943	0.24364370695904014
X	0.013927895780842801	803	0.803	0.25885505734731024
X	0.014790316202566465	749	0.749	0.27029118381913725
X	0.013134986396009412	330	0.33	0.3414327917743233
X	0.013491859189068211	362	0.362	0.3340317456157684
X	0.01417233142808792	1050	1.05	0.2380952147274488
X	0.0131678178270111	521	0.521	0.2934666100821146
X	0.013331886854158852	168	0.168	0.42972845550756306
X	0.012859140107463337	223	0.223	0.3863394184369278
X	0.012955788748338725	371	0.371	0.32686115885208444
X	0.012900566597928903	269	0.269	0.36331681783522646
X	0.014981839833899517	1962	1.962	0.1969194960200683
X	0.012825621996170598	122	0.122	0.47196109685606763
X	0.015201927759261208	1976	1.976	0.19741064828783925
X	0.013651896896059718	413	0.413	0.32093295580718506
X	0.014437583959888292	277	0.277	0.37354097777601153
X	0.012912096315761092	359	0.359	0.33009131878999814
X	0.012356679257724584	102	0.102	0.4948047582786182
X	0.013370658145364088	188	0.188	0.41431594288848606
X	0.01375834762471184	207	0.207	0.4050717699611434
X	0.01280644113186269	145	0.145	0.44533498938609634
X	0.01260910623552633	73	0.073	0.5569127334857912
X	0.01394856316137321	322	0.322	0.3512027171157857
X	0.01198506006239009	159	0.159	0.422423136451347
X	0.01330004301147824	129	0.129	0.46890814696856165
X	0.014406480677434986	543	0.543	0.29825382622047114
X	0.014665835294522134	795	0.795	0.2642286412908766
X	0.013013530096117321	257	0.257	0.3699593426860507
X	0.011951079061079376	105	0.105	0.4846251407213544
X	0.012834457549915666	186	0.186	0.41016146712961266
X	0.013240374461011548	444	0.444	0.3101028559160687
X	0.014046226728056117	589	0.589	0.28783799876274724
X	0.011779435889195129	106	0.106	0.4807724161867269
X	0.013626514502925414	800	0.8	0.257295149251617
X	0.012509411549979916	109	0.109	0.4859632870055524
X	0.013885508472301982	458	0.458	0.31181626841362925
X	0.012350879800664804	77	0.077	0.5433367199679882
X	0.013924988333136922	594	0.594	0.2861998916441227
X	0.013358340452115524	154	0.154	0.44266711717832385
X	0.01391401758990539	243	0.243	0.385432878251647
X	0.013609307023597977	533	0.533	0.29446675303927305
X	0.012775175902677112	127	0.127	0.4650729237641779
X	0.012236818449398402	120	0.12	0.46719238661972967
X	0.013276575903637025	131	0.131	0.4662350605157059
X	0.01380060194267389	564	0.564	0.29031733942101445
X	0.013685935494312264	177	0.177	0.42602368126303725
X	0.014074018054510402	558	0.558	0.29326572041360816
X	0.01501610489793455	939	0.939	0.25194006306644007
X	0.014384337016618324	1638	1.638	0.20631239348178262
X	0.01341009863380925	585	0.585	0.28406999831769797
X	0.014333931607403235	323	0.323	0.35404157862391655
X	0.013636624477205338	2042	2.042	0.18831447961377257
X	0.014120303237841962	562	0.562	0.2928886682261438
X	0.013530497920030525	263	0.263	0.3719226921293591
X	0.012667064675611218	132	0.132	0.45782624553023493
X	0.012593070368970125	337	0.337	0.3343237604666036
X	0.012369160989889981	134	0.134	0.4519376065205949
X	0.014192092914540775	623	0.623	0.2834780693985904
X	0.011553280988714517	50	0.05	0.61363733793391
X	0.01389125400012948	391	0.391	0.32874177696547385
X	0.013826942002008073	287	0.287	0.3638715338452403
X	0.013355656754130061	127	0.127	0.4720128725173076
X	0.01217825982251835	68	0.068	0.5636706746213043
X	0.01217023072321344	217	0.217	0.3827774626559463
X	0.013518359885286674	549	0.549	0.29092690483289557
X	0.013528051379841994	483	0.483	0.303689216489254
X	0.012634836257745273	117	0.117	0.476205695596842
X	0.01294532113298413	87	0.087	0.5299048062472075
X	0.012854143342720961	187	0.187	0.4096382580725891
X	0.014962621986096578	733	0.733	0.27329681879712775
X	0.01309656557284065	228	0.228	0.3858402452055389
X	0.014353763048107378	873	0.873	0.25428291203532444
X	0.013775679805183795	288	0.288	0.3630001885768235
X	0.014646256501401313	264	0.264	0.3813943334801497
X	0.015115471934937293	713	0.713	0.2767647191061245
X	0.01404348836784166	202	0.202	0.4111887365785441
X	0.013618840797355532	356	0.356	0.3369483869894669
X	0.012966630443256796	281	0.281	0.3586798156067658
X	0.011533351650569734	51	0.051	0.6092494127442352
X	0.01576993348841024	1718	1.718	0.209380179305588
X	0.013351352022751176	237	0.237	0.38334720419102
X	0.01269997162273701	287	0.287	0.3537042552233944
X	0.0136925871845525	169	0.169	0.4327126617528419
X	0.013186734376507511	197	0.197	0.40602895995672333
X	0.0120483544187433	110	0.11	0.47845931747855963
X	0.014008208599368242	211	0.211	0.4049176568152977
X	0.01316955208408042	277	0.277	0.36226836557612435
X	0.011696130557205954	53	0.053	0.6043039680056942
X	0.012528745781565	141	0.141	0.446234175354638
X	0.0131656589659875	231	0.231	0.38483704726571294
X	0.013336673453789503	412	0.412	0.3187010056525212
X	0.01326188232722143	259	0.259	0.37133720880709525
X	0.01406715765952338	259	0.259	0.37870600963719825
X	0.01502522400796094	585	0.585	0.29504507059468815
X	0.015357688314101163	4074	4.074	0.15563297103193452
X	0.014296969705351242	625	0.625	0.2838711953459738
X	0.013451672736026953	133	0.133	0.46591663565439845
X	0.013436962523122292	211	0.211	0.3993369992643893
X	0.013458246297121442	377	0.377	0.3292676321822171
X	0.012791230766430572	216	0.216	0.3897799788021806
X	0.013672161708379253	180	0.18	0.42350144233446985
X	0.013401226573820403	253	0.253	0.3755559752921829
X	0.01372887748918366	861	0.861	0.25169656737776525
X	0.014280367203055002	490	0.49	0.30773797969707256
X	0.013954706711524436	937	0.937	0.24603319558924225
X	0.014270648296420456	198	0.198	0.4161592040713008
X	0.015239604326848004	1134	1.134	0.2377498284633122
X	0.013207641915696452	155	0.155	0.4400457820896653
X	0.013466962172314691	162	0.162	0.4364336301901778
X	0.014979796133727363	556	0.556	0.29978532559448307
X	0.011325696046212697	34	0.034	0.6932054923194325
X	0.01370688307622968	832	0.832	0.25445156555172077
X	0.012476840687015893	95	0.095	0.5083080740371828
X	0.013738788092680672	244	0.244	0.3832829786045142
X	0.013484918247906598	107	0.107	0.5013659626868555
X	0.012898818118391013	233	0.233	0.3811225679089381
X	0.012763747174739376	326	0.326	0.3395628716153328
X	0.013724948580914689	285	0.285	0.3638217194395377
X	0.012685162442971945	130	0.13	0.4603811784388761
X	0.013843898952876238	342	0.342	0.34335557621488755
X	0.01140975859925954	49	0.049	0.6152148246462117
X	0.011979190943774804	175	0.175	0.40906891567862447
X	0.012876838085814158	265	0.265	0.3649116989958359
X	0.01413747364590799	303	0.303	0.36000598977786763
X	0.013898133164896876	185	0.185	0.4219505720927785
X	0.014324335629088613	672	0.672	0.27726926962168885
X	0.012638070412813712	201	0.201	0.39764443088834506
X	0.01216807347768037	87	0.087	0.5190798737291568
X	0.013926272851490002	1440	1.44	0.21305454467639187
X	0.012770492907670626	316	0.316	0.3431680597131359
X	0.01146147383792166	168	0.168	0.40861132117690097
X	0.01414821361238533	444	0.444	0.31703427193287786
X	0.013250884808612601	163	0.163	0.4331973169076233
X	0.012877786017742478	289	0.289	0.35452575226269706
X	0.013663754931199075	110	0.11	0.49895241187779593
X	0.014150392716761362	758	0.758	0.2652771007709077
X	0.01484520326134224	710	0.71	0.2754921742119389
X	0.01343277861503556	336	0.336	0.3419339264044311
X	0.012114352358879322	197	0.197	0.3947098748928501
X	0.01362600999103981	515	0.515	0.29798002894278025
X	0.014970613908363191	2053	2.053	0.19391744790794163
X	0.011817106282144517	90	0.09	0.5082642270841842
X	0.012805531499084675	95	0.095	0.5127330761845887
X	0.01348331788559088	230	0.23	0.3884689507336814
X	0.014790756619146595	118	0.118	0.5004601032501477
X	0.013196965849287576	232	0.232	0.38458768134186727
X	0.014192101736276063	235	0.235	0.39233731699276064
X	0.012928726237979186	213	0.213	0.39299954299581596
X	0.011719952765850252	99	0.099	0.491017407461754
X	0.0135204664099955	200	0.2	0.4073682955759658
X	0.010991322058780959	38	0.038	0.661335920781422
X	0.014176394104127984	705	0.705	0.2719311047367155
X	0.0135094897959979	111	0.111	0.4955703594985949
X	0.0144826749426248	1103	1.103	0.23591619603102598
X	0.015560586595844528	1142	1.142	0.23884708956869422
X	0.01245254046678054	324	0.324	0.33747192632777356
X	0.01057650604231756	38	0.038	0.6529093306134928
X	0.013715358887232142	418	0.418	0.3201427207331953
X	0.013933241668674674	364	0.364	0.33701580008632115
X	0.013311045869488327	326	0.326	0.3443484982527704
X	0.014138874359378216	720	0.72	0.2697909927947741
X	0.014301741291601442	1933	1.933	0.19485835020807601
X	0.01190963953197993	89	0.089	0.5114888882864719
X	0.01260808317601109	62	0.062	0.5880565954462373
X	0.010434547902409297	56	0.056	0.5711653725464395
X	0.014154476127080384	151	0.151	0.45426117263390053
X	0.0124054056515099	119	0.119	0.47063909785004515
X	0.01414692878864225	145	0.145	0.46036050210220403
X	0.01155224504713752	75	0.075	0.53604556812811
X	0.01196043394000743	603	0.603	0.270692698492219
X	0.013581693493291053	206	0.206	0.40398220467458934
X	0.01163344256122328	109	0.109	0.47434455242411955
X	0.013703717465388723	348	0.348	0.34021468121131354
X	0.012561398404113534	272	0.272	0.3587757068148331
X	0.012970914065370858	175	0.175	0.42005953227363935
X	0.01393321887207834	914	0.914	0.24795245989052048
X	0.01359955502202122	455	0.455	0.31034000819992236
X	0.01201137868974134	93	0.093	0.5054792164685787
X	0.013134713068335203	121	0.121	0.4770293199870357
X	0.012697965718274402	587	0.587	0.2786326569961603
X	0.011860243890588991	93	0.093	0.5033501747892583
X	0.01276360285888929	75	0.075	0.5541627626868079
X	0.011988655354681006	155	0.155	0.42606866206947164
X	0.013013175388370131	335	0.335	0.33867215563413955
X	0.014897492521865197	829	0.829	0.26193058637505673
X	0.013010870902700205	165	0.165	0.42881911991535987
X	0.011053440351632639	58	0.058	0.5754706244404079
X	0.013392342965911682	185	0.185	0.4167685555337735
X	0.01417068439222704	314	0.314	0.35603038111325036
X	0.011384544698512368	55	0.055	0.5915402842622046
X	0.014010495095567342	433	0.433	0.3186558251998582
X	0.013948997520385805	8644	8.644	0.11729407633100099
X	0.013783210461149756	287	0.287	0.3634875138052988
X	0.014778444756654753	834	0.834	0.26070792153723416
X	0.012040161093839064	81	0.081	0.5297233207855033
X	0.013509683790987007	617	0.617	0.27976067897101886
X	0.013989897008444503	1893	1.893	0.19478460744489767
X	0.014272527380688143	8949	8.949	0.1168356714885288
X	0.010353108790328431	103	0.103	0.46495528043147943
X	0.014484737581272016	2002	2.002	0.19341145235801552
X	0.014063532103808209	8440	8.44	0.11855430277691782
X	0.013970291471535	388	0.388	0.33021078010754773
X	0.014269801401727922	17277	17.277	0.09382455902079422
X	0.0116395084076037	95	0.095	0.4966727629429589
X	0.014518861070694953	722	0.722	0.27193500582549734
X	0.014847587225486257	9159	9.159	0.11747242717791181
X	0.01446409772763748	1568	1.568	0.2097241436301237
X	0.013743605259604216	204	0.204	0.4069023223183252
X	0.014108785080390472	1270	1.27	0.22313205522026022
X	0.01409755187905411	3357	3.357	0.1613358147952092
X	0.0142389321527304	7993	7.993	0.12122416533818305
X	0.013440451765381962	1367	1.367	0.21423073806367107
X	0.014179784994493582	3562	3.562	0.15848636702630892
X	0.015285560929050205	3358	3.358	0.16572965625312733
X	0.015385816565896913	3915	3.915	0.15780821034287368
X	0.013337994950894844	430	0.43	0.3142008563484377
X	0.015830222619569397	20092	20.092	0.092360856352209
X	0.01276103889792732	186	0.186	0.4093778706840063
X	0.01366827478198898	277	0.277	0.36678478394784797
X	0.01405306157505307	3806	3.806	0.15456128570947092
X	0.01388539074426084	799	0.799	0.2590222721871961
X	0.015731229437194825	51674	51.674	0.06727135799225967
X	0.01444175238926908	7379	7.379	0.1250856418530598
X	0.01218550862920099	79	0.079	0.5362971924695692
X	0.013723767570699798	533	0.533	0.29528998314210836
X	0.01511037275749482	1443	1.443	0.21877764795752697
X	0.013148032154648933	130	0.13	0.4659140442102603
X	0.015491512842317456	13188	13.188	0.1055127553484857
X	0.015131494671092063	6213	6.213	0.13454270610123495
X	0.013990775405307826	745	0.745	0.26580419901345576
X	0.012562857698616694	48	0.048	0.6396598143713053
X	0.01437280392400608	413	0.413	0.3264854386799075
X	0.013663300458084752	1664	1.664	0.20174406418991023
X	0.014329972598423151	2578	2.578	0.1771417030297023
X	0.01345942643189388	695	0.695	0.26854304937584816
X	0.012723219778226937	292	0.292	0.35188821797473074
X	0.015443049896367222	53614	53.614	0.06604170671677677
X	0.014239812404704554	590	0.59	0.2889908453744211
X	0.014432784777548243	2308	2.308	0.1842347051551132
X	0.015146517433915013	15393	15.393	0.09946337077834588
X	0.01363596897018299	218	0.218	0.3969567307857786
X	0.014046838459761853	1264	1.264	0.22315699503016037
X	0.015444782160058852	22597	22.597	0.088086698699101
X	0.014350637722928399	17854	17.854	0.09297764820168615
X	0.013644151722952101	734	0.734	0.26490093634500467
X	0.012667069604640502	142	0.142	0.44681657051153384
X	0.0129129716172555	274	0.274	0.36120898481657204
X	0.013267708764009444	160	0.16	0.436072449199589
X	0.014331218079604094	844	0.844	0.25702787127294996
X	0.014447388162902345	1386	1.386	0.21844496400300792
X	0.0142881968055053	2760	2.76	0.1729907097436366
X	0.015503563329104273	16551	16.551	0.09784435236483847
X	0.014634675880151011	5043	5.043	0.14263673202218405
X	0.014171085238742194	526	0.526	0.2997821629343426
X	0.014136344002638	866	0.866	0.25367231425785436
X	0.015407350878290295	25467	25.467	0.08457656867875074
X	0.015476017056978247	2045	2.045	0.19633088719973382
X	0.01350773723958195	664	0.664	0.27298461706921545
X	0.015347267303350606	2990	2.99	0.17249901637858342
X	0.0151177700863532	50595	50.595	0.06685389454826221
X	0.013985816680029026	284	0.284	0.3665415092829651
X	0.014548610609631961	4822	4.822	0.1444989766478255
X	0.014232863832206042	374	0.374	0.3363619777614135
X	0.012626231258628794	559	0.559	0.28267494121957315
X	0.015066025245079901	1501	1.501	0.21571118578469303
X	0.0144492015985332	16143	16.143	0.09637251688108188
X	0.015021437901736129	6927	6.927	0.1294361062108171
X	0.013331478802236516	331	0.331	0.3427810285261583
X	0.014843413136811218	9770	9.77	0.1149599070216748
X	0.01544893274758426	11234	11.234	0.11120424204718189
X	0.012963658466507664	364	0.364	0.32900972288151165
X	0.013147525431716642	665	0.665	0.27040047992457206
X	0.0158899509795974	17963	17.963	0.09599484149327177
X	0.016304326740626547	52592	52.592	0.06768011081362317
X	0.014426416549639068	2818	2.818	0.17234783266565692
X	0.012990665750129343	317	0.317	0.34476569721446154
X	0.01357335908459309	265	0.265	0.37137598914367626
X	0.011018252674905844	91	0.091	0.49471731602916946
X	0.01424555982482463	2995	2.995	0.16817479155178536
X	0.013746949922990595	307	0.307	0.35510434943769437
X	0.013052700819771917	241	0.241	0.3783506174373382
X	0.014690287878436506	7126	7.126	0.1272706208950339
X	0.015203562431242955	16531	16.531	0.09724831354616253
X	0.012504180630380295	1398	1.398	0.2075786528353459
X	0.013773284563619967	316	0.316	0.35192502157476174
X	0.01331582900603935	720	0.72	0.26445101467808607
X	0.014240934017955667	1374	1.374	0.21803049820970402
X	0.013924069288065373	1512	1.512	0.2096065234798032
X	0.014157048682903201	1673	1.673	0.20377876608284443
X	0.01379393645307043	189	0.189	0.41790298404938225
X	0.013863418614443342	35657	35.657	0.07298638360974667
X	0.014206250938667403	700	0.7	0.2727682478087209
X	0.014875764403972693	2440	2.44	0.18268230740439595
X	0.014866533028831355	5041	5.041	0.14340501333216163
X	0.012828894609061255	399	0.399	0.31798413112478824
X	0.014544266048087173	6601	6.601	0.13012498500766256
X	0.014371170450905198	943	0.943	0.24792870723973148
X	0.014175061424144613	4849	4.849	0.1429851455473208
X	0.012643638182816901	137	0.137	0.4519084867750943
X	0.013035729329872804	241	0.241	0.37818656559907715
X	0.014853886979814525	512	0.512	0.30727228125148454
X	0.014452736968092015	1099	1.099	0.23603920078211968
X	0.013837153237365738	931	0.931	0.2458663099174228
X	0.014215334485445	2180	2.18	0.18682437145931124
X	0.014031308762700801	365	0.365	0.33749585162708984
X	0.01382895225043267	535	0.535	0.29567311155521925
X	0.015005959926877564	39138	39.138	0.07264767466860847
X	0.013876590891650272	2427	2.427	0.1788151601958369
X	0.014785766810463603	3991	3.991	0.1547345780351059
X	0.015706069456681893	21682	21.682	0.08980959223125184
X	0.014371371238742697	12479	12.479	0.10481887024099809
X	0.01389055121443222	853	0.853	0.2534682013156171
X	0.014050148549097363	317	0.317	0.35389455819564924
X	0.015121889753035874	2426	2.426	0.18403697674997435
X	0.013282499131385803	55	0.055	0.6227384799030135
X	0.013843396747236518	370	0.37	0.3344621526181783
X	0.01338837346355628	913	0.913	0.24476670805986994
X	0.012760692675779037	326	0.326	0.3395357824653024
X	0.013765935696918058	332	0.332	0.34611669729571964
X	0.013087314476250653	169	0.169	0.42624040048661954
X	0.013531976499174097	402	0.402	0.32288291142889686
X	0.01453032016031519	45756	45.756	0.0682248166862739
X	0.01357521734178634	1724	1.724	0.19894659475057241
X	0.01362216768795523	3402	3.402	0.15879517574929952
X	0.01412186359185973	4509	4.509	0.14630884850277318
X	0.01302778665083511	132	0.132	0.4621314965476404
X	0.014608614142344002	354	0.354	0.34556937860460457
X	0.014728804876205101	61170	61.17	0.06221231220112266
X	0.015346634315302678	44013	44.013	0.07038455853361356
X	0.013857518255525869	1459	1.459	0.21177599090861676
X	0.014361150080989921	1603	1.603	0.20769142502698854
X	0.013528273159989836	211	0.211	0.40023952044821287
X	0.013835449351938398	5937	5.937	0.13257920382886132
X	0.014238700794291544	10319	10.319	0.11132966883361568
X	0.014273027896372744	14971	14.971	0.09842114989069788
X	0.014230365714277644	1304	1.304	0.22180916657071745
X	0.014323129986559949	7398	7.398	0.12463533981063961
X	0.014176231672990229	8599	8.599	0.11813292328130937
X	0.01364941451677647	940	0.94	0.24396567275112244
X	0.012999464019620896	310	0.31	0.3474198145701304
X	0.01579452727162964	1367	1.367	0.22607161622219107
X	0.0141556946603949	2294	2.294	0.18341967696652842
X	0.013438561426014899	280	0.28	0.3634111515243391
X	0.015229193163409191	11134	11.134	0.11100483426532468
X	0.01355720179965219	2394	2.394	0.1782441580830082
X	0.0120106159143408	95	0.095	0.5018961811576408
X	0.014363080777219414	982	0.982	0.24455621548429096
X	0.015384882975423104	2184	2.184	0.1916963546843648
X	0.01229090127869431	89	0.089	0.5168897354688752
X	0.014384659987042582	14970	14.97	0.09867927123908252
X	0.015370231125325755	16484	16.484	0.09769505853096314
X	0.01576817016566883	11559	11.559	0.11090565168974995
X	0.014090934420391605	11733	11.733	0.10629436480278955
X	0.013971567216256822	13315	13.315	0.10161737795301469
X	0.01394029301258722	833	0.833	0.2557853304621306
X	0.012825345277789198	888	0.888	0.24352975583986325
X	0.014361864394313084	2528	2.528	0.1784341257665439
X	0.014514858049679216	8507	8.507	0.11949382231736295
X	0.013481629809541044	137	0.137	0.461679520443708
X	0.014673576919889953	3121	3.121	0.16752543003828352
X	0.015485089472673283	30188	30.188	0.08004971546662637
X	0.015270816658103812	2216	2.216	0.19029651533348446
X	0.01532888617308077	2828	2.828	0.1756616932549863
X	0.014176608688071739	18166	18.166	0.09206707534635857
X	0.014967888658631238	820	0.82	0.2632988105448011
X	0.01191322686439488	74	0.074	0.5440004147532707
X	0.01331602215795574	3582	3.582	0.15491125625766478
X	0.014217996460817114	2150	2.15	0.18770102632337374
X	0.013945002312056646	1284	1.284	0.2214545064401594
X	0.014309842731407922	80009	80.009	0.05634196715789444
X	0.015162345143847558	26942	26.942	0.0825617422161406
X	0.014408704398534847	10615	10.615	0.1107222691663105
X	0.014543821946484488	2592	2.592	0.1776974452224549
X	0.014268132910539943	8059	8.059	0.12097491625814047
X	0.014591430739942391	128	0.128	0.4848744531463502
X	0.01229519806483332	318	0.318	0.33814486996777693
X	0.014425390758945356	47263	47.263	0.06732899012390198
X	0.014046665502140641	348	0.348	0.3430293909257976
X	0.012552163127692353	231	0.231	0.37876414878002757
X	0.015172164683649013	1573	1.573	0.2128659881384874
X	0.014442261408622854	6349	6.349	0.13151540443395568
X	0.013975062469491028	4220	4.22	0.14905518994498268
X	0.014970101341195061	6233	6.233	0.1339191048791044
X	0.01296877854915569	420	0.42	0.313725577933131
X	0.01406218393663631	2501	2.501	0.17781962670197068
X	0.01357825765312512	469	0.469	0.30706034547198097
X	0.01478743402266522	2026	2.026	0.1939773966015313
X	0.0149607703952688	3087	3.087	0.16922808609932144
X	0.013258916974551473	1000	1.0	0.23668424527520326
X	0.013793980819090428	161	0.161	0.4408470245437247
X	0.014643661872617793	4316	4.316	0.1502643944135888
X	0.012928191305017251	121	0.121	0.4745159334391406
X	0.015363932449305872	3764	3.764	0.15981501924413594
X	0.013589450131135817	463	0.463	0.3084657603420555
X	0.014402352021305494	840	0.84	0.2578604355446573
X	0.014633443513430844	360	0.36	0.3438333602374864
X	0.014076929021935093	1085	1.085	0.23497738204417756
X	0.01310229041918499	361	0.361	0.3310904645717881
X	0.015556101545033367	3133	3.133	0.17060043132296226
X	0.015371976790401583	13682	13.682	0.10395850961049728
X	0.014003222011200628	5525	5.525	0.13634270226512804
X	0.013867362425863835	780	0.78	0.2609955410773036
X	0.013302801115486604	331	0.331	0.3425350637140235
X	0.014466427633620244	29307	29.307	0.07903084853003438
X	0.013920679064472485	349	0.349	0.34167378970673035
X	0.015495320384427557	84049	84.049	0.056914466579709805
X	0.012289694969790111	49	0.049	0.630640233855723
X	0.013749709801572745	2401	2.401	0.17890966012915005
X	0.014390589899729042	1239	1.239	0.22646569798988325
X	0.013222882137013411	378	0.378	0.3270479583593605
X	0.01342821525556877	778	0.778	0.2584319666070152
X	0.0132623817598193	2857	2.857	0.16681576631777287
X	0.014557193567123082	58014	58.014	0.06307369621816859
X	0.013761939279776777	600	0.6	0.28412500888525943
X	0.013873321112326018	2226	2.226	0.18402850280655955
X	0.013220934136141855	217	0.217	0.39349039512174006
X	0.014206804888129523	617	0.617	0.28449223511801525
X	0.01351149866177359	1146	1.146	0.2276003723954831
X	0.014417985270485215	963	0.963	0.24646736959232118
X	0.012756140533546203	592	0.592	0.27826966348486654
X	0.013917289218620819	1067	1.067	0.2353947347153702
X	0.013545450835413376	2101	2.101	0.18611832582236137
X	0.0130898900025971	95	0.095	0.5165005606421764
X	0.013745985567236796	588	0.588	0.2859342541870893
X	0.014526616062913588	17731	17.731	0.09357153260683157
X	0.01376550370666671	725	0.725	0.2667788321437839
X	0.01446177424031466	6395	6.395	0.13125837084965905
X	0.014373111706498002	12729	12.729	0.10413231053002166
X	0.01324993931121162	760	0.76	0.2592985564090063
X	0.013005284908932288	183	0.183	0.4142122388970937
X	0.01163692166683826	172	0.172	0.4074771137862813
X	0.014576335092977932	19728	19.728	0.0904041626833405
X	0.015105018065967963	1094	1.094	0.23990242742372916
X	0.014786970985712038	1561	1.561	0.21158852247271362
X	0.013914374986935948	2240	2.24	0.18382527667656068
X	0.01571030373886618	10913	10.913	0.11291374421212234
X	0.014281294879409628	396	0.396	0.3303879000244093
X	0.01395997389971568	529	0.529	0.29772112045124083
X	0.014345978260505312	47196	47.196	0.06723699962625548
X	0.01360144440408732	270	0.27	0.36932362434208743
X	0.013498716985973895	597	0.597	0.28277394253219923
X	0.014299725441639098	694	0.694	0.2741507318065323
X	0.014778691617405863	4630	4.63	0.14723746514186195
X	0.01516294642344081	11182	11.182	0.11068481346872037
X	0.013330226554453962	1701	1.701	0.1986298001528397
X	0.01446956281323014	3615	3.615	0.15877509061563633
X	0.014087045809841624	556	0.556	0.29370750572453674
X	0.015313384490403086	9733	9.733	0.11630761616345192
X	0.012984353535910207	518	0.518	0.2926601049788976
X	0.014879769412060331	642	0.642	0.28511521317881416
X	0.013757392149454059	3190	3.19	0.16277279139261683
X	0.01411932315011847	500	0.5	0.30451916100615817
X	0.013705842869883538	6195	6.195	0.13030311938711076
X	0.013476127970402317	212	0.212	0.39909512406287745
X	0.009184518686986	50	0.05	0.5684541821052279
X	0.013257264249934453	858	0.858	0.2490704516949468
X	0.013993141291443743	423	0.423	0.3210147225026323
X	0.014274751036293302	15006	15.006	0.09834852860274283
X	0.014055384810319552	5471	5.471	0.13695945163962053
X	0.014218357778742164	1792	1.792	0.1994514253636338
X	0.012796042034402837	181	0.181	0.41349065077910363
X	0.01444833551120399	3071	3.071	0.16756346554843346
X	0.01320035109179763	203	0.203	0.40212692059770194
X	0.013760638092580019	555	0.555	0.2915962021518516
X	0.014321423979298881	1282	1.282	0.2235455654013886
X	0.014644520922497732	21540	21.54	0.08793123170589837
X	0.01392007681878336	709	0.709	0.26977299010700384
X	0.013585182733145918	97	0.097	0.5193154580213862
X	0.012822317039494652	140	0.14	0.45076081806348706
X	0.01451335223740025	58396	58.396	0.06287261808935819
X	0.015538613058239462	63059	63.059	0.06269338571071045
X	0.012946188988121762	350	0.35	0.3331894943460882
X	0.014131878094882293	695	0.695	0.2729428315156745
X	0.014211065838228001	409	0.409	0.32631305395904997
X	0.01565449665652977	8792	8.792	0.1212040530753164
X	0.01393457946820437	381	0.381	0.33193748336466494
X	0.01472903760681417	1675	1.675	0.20640482875825888
X	0.013425795363584335	207	0.207	0.40178146005493753
X	0.014844992461299	12690	12.69	0.1053673650731608
X	0.012851801067440185	1766	1.766	0.19378690730280396
X	0.014749816486662182	24462	24.462	0.08448213585462427
X	0.013705604167084901	411	0.411	0.32187367180460186
X	0.0138821823689438	844	0.844	0.25431487689844984
X	0.013388690824993804	106	0.106	0.5017384869993486
X	0.014051366274132875	3372	3.372	0.16092011325244185
X	0.014032738555846573	1063	1.063	0.23633954212321
X	0.01419390196950579	7946	7.946	0.12133453038441105
X	0.015225740081775394	22541	22.541	0.08774083118265756
X	0.0152237743631208	70875	70.875	0.05988845217975105
X	0.013140792008429292	621	0.621	0.2765943558663288
X	0.013769892210682444	250	0.25	0.3804785495432106
X	0.0153916377959685	2199	2.199	0.19128747528055257
X	0.014690007771521331	6469	6.469	0.1314402242778337
X	0.01420471867801949	1090	1.09	0.235325349857916
X	0.01341379653838928	106	0.106	0.5020519016962447
X	0.012933348954254938	167	0.167	0.42625044109794824
X	0.014236390996380811	811	0.811	0.25989206368356266
X	0.012908303492639734	109	0.109	0.4910746961473898
X	0.014665068331677451	6394	6.394	0.13187743340464161
X	0.013865618863467462	367	0.367	0.33555037716875646
X	0.014543571714025297	2005	2.005	0.19357632172063569
X	0.01365206236435496	753	0.753	0.2627046514296167
X	0.013354964118753893	246	0.246	0.37864817922361327
X	0.013190819776599505	236	0.236	0.3823431036942908
X	0.015391164219677417	19697	19.697	0.09210653875153067
X	0.014538955426729472	77050	77.05	0.05735705627421968
X	0.014481290612405877	14301	14.301	0.1004184747968222
X	0.014055517590128838	2892	2.892	0.16938808238071412
X	0.015467773652008326	4167	4.167	0.1548346306923087
X	0.014009609765228217	1120	1.12	0.23213253022488162
X	0.015207888625370918	916	0.916	0.25510823804028215
X	0.014448785929044906	5978	5.978	0.13420184359711756
X	0.01637994888976883	3277	3.277	0.17098002134917148
X	0.014815300904660305	5713	5.713	0.13738798400175792
X	0.0152571801976939	2728	2.728	0.17750474618381706
X	0.016048031400460752	33697	33.697	0.07809265276737541
X	0.014426680579701427	699	0.699	0.27430248801730656
X	0.015424158925337633	3823	3.823	0.15919609034030308
X	0.015600128372716427	23294	23.294	0.0874905943849088
X	0.014673281260996865	6018	6.018	0.13459378910593617
X	0.01424821072528376	1318	1.318	0.22111335440010696
X	0.013813705789997853	1782	1.782	0.19790989152478142
X	0.014224557960644433	754	0.754	0.2662088429348308
X	0.013654716631124183	635	0.635	0.27807993645964046
X	0.013697583658007122	549	0.549	0.29220694854380613
X	0.014114481688682134	3008	3.008	0.16741551413097597
X	0.015282507532001707	915	0.915	0.2556178483134099
X	0.014480990676368582	3874	3.874	0.155195652972286
X	0.016133224617813306	97012	97.012	0.05499188088592362
X	0.01414808017899744	14431	14.431	0.09934218209200571
X	0.014148861956864805	12692	12.692	0.10368847377430446
X	0.014398253085815112	1613	1.613	0.2074396695392015
X	0.013287898598206208	1554	1.554	0.20448828390158133
X	0.015594995602053055	5045	5.045	0.14567153458451768
X	0.014038133628091955	2167	2.167	0.18641612963786833
X	0.015673274284714805	14586	14.586	0.10242544077268385
X	0.015254403491870511	19646	19.646	0.09191231326713907
X	0.014314166501519994	524	0.524	0.30116991926982417
X	0.015367156272877523	66050	66.05	0.06150450042635763
X	0.014277593390729041	18391	18.391	0.0919072747246267
X	0.014343997294360419	1409	1.409	0.2167303507506566
X	0.01356139551216956	238	0.238	0.3848065670520763
X	0.014141999458856226	939	0.939	0.24695343688429944
X	0.014688548095991995	6884	6.884	0.12873975233997095
X	0.012677939594912972	316	0.316	0.34233701946946893
X	0.015056483500380817	35083	35.083	0.07542968192497997
X	0.014573483488467815	2773	2.773	0.17386193196521
X	0.014066485247603714	1543	1.543	0.20890069731609004
X	0.014235257340233445	749	0.749	0.2668667714846119
X	0.013037499247927093	141	0.141	0.45219427664304124
X	0.013611305987648184	403	0.403	0.3232448271847514
X	0.013374664920266887	994	0.994	0.2378476374820039
X	0.014200553061283648	247	0.247	0.38595449376517155
X	0.013443024782530793	1003	1.003	0.23753741288228788
X	0.014903709337913936	235268	235.268	0.039863660040781644
X	0.01460748833174099	854	0.854	0.25765538627213036
X	0.01334483299383058	339	0.339	0.34017663563346556
X	0.01354066464122155	311	0.311	0.3517978635691324
X	0.01371602713840022	533	0.533	0.29523445655377273
X	0.013898095893866296	1117	1.117	0.23172205798338433
X	0.013454873303949464	9406	9.406	0.1126743027904995
X	0.015023469773857868	7982	7.982	0.12346755311634541
X	0.014712002669939519	117385	117.385	0.050044120780734835
X	0.013436097709753953	380	0.38	0.32821859767543893
X	0.014631706120692282	15047	15.047	0.09907141127610904
X	0.01509954471908864	890	0.89	0.2569555181648328
X	0.014591778433510184	23688	23.688	0.08508635983379016
X	0.01574041103710318	92753	92.753	0.05536424322760685
X	0.014196522709481709	1173	1.173	0.22959441695830926
X	0.012654522997320181	70	0.07	0.5654348597904667
X	0.015285920495323175	11927	11.927	0.10862258294955435
X	0.015085733562589711	1363	1.363	0.2228555867049513
X	0.013844782491366161	8515	8.515	0.11758912578341633
X	0.013897083038456163	756	0.756	0.26391695128166454
X	0.013222301722767544	354	0.354	0.33427297538201023
X	0.016509426343141422	5413	5.413	0.14502114129368276
X	0.013417872154736924	450	0.45	0.3100918689812756
X	0.01585094663449106	9976	9.976	0.11668980688777479
X	0.014965998886617543	5218	5.218	0.1420803040885703
X	0.014063989444867657	289	0.289	0.36509304774493057
X	0.01387839676733916	517	0.517	0.2994214565924467
X	0.01496910408290832	20926	20.926	0.08943411128313497
X	0.012928511183072814	263	0.263	0.3663230577455138
X	0.01630613393068808	40898	40.898	0.07360087512030086
X	0.01179369305586752	177	0.177	0.4054078789053786
X	0.014401108330820273	9249	9.249	0.1159045217487288
X	0.014197915141476003	3344	3.344	0.1619270618764796
X	0.01365370170744667	5958	5.958	0.1318408427861474
X	0.014316313486396975	6556	6.556	0.12973706981267033
X	0.014152617339834011	1051	1.051	0.23790926179698077
X	0.013908519885877333	555	0.555	0.29263705050285
X	0.014372530303291994	2831	2.831	0.1718690941044679
X	0.013918549322580699	891	0.891	0.24998009162876983
X	0.013555517686105502	12183	12.183	0.10362247344504298
X	0.014688442927694874	10516	10.516	0.11178281982572044
X	0.01386094797742496	1235	1.235	0.22389376112332182
X	0.014678924212785832	3954	3.954	0.15484095970546477
X	0.015085915658193531	5180	5.18	0.14280627965662376
X	0.015153911373747575	11130	11.13	0.11083489807134889
X	0.01470195510000588	9442	9.442	0.11590540762394301
X	0.0154588669996137	9572	9.572	0.11732529555872123
X	0.013848081899530464	6419	6.419	0.1292133588346151
X	0.0151587778764752	2015	2.015	0.19594298188806852
X	0.013566680309178522	298	0.298	0.35706924263641443
X	0.01420434923035147	1371	1.371	0.21800241040708332
X	0.014634859843481163	1838	1.838	0.19968603187691988
X	0.014259170565184975	6794	6.794	0.12803329049914156
X	0.014355341152756815	1276	1.276	0.22407201235569132
X	0.01329795257477632	242	0.242	0.3801804711810667
X	0.0141477821950098	417	0.417	0.3237308618228879
X	0.014248778550931923	9621	9.621	0.11398625207633849
X	0.013679522443878512	698	0.698	0.2696115570634229
X	0.013882608639453915	485	0.485	0.3058978660777809
X	0.012222287961860642	120	0.12	0.4670073924034229
X	0.013173694149150528	425	0.425	0.31412768621287485
X	0.013845142747581318	714	0.714	0.26865797813323145
X	0.01392734784614943	1252	1.252	0.22323109927660273
X	0.014333157994424237	10547	10.547	0.11076544944353686
X	0.01372821523466885	118	0.118	0.4881770485883532
X	0.014554471699143092	10666	10.666	0.11091704451008448
X	0.013408959220773843	130	0.13	0.4689759508065141
X	0.013656164566974723	620	0.62	0.280314577913713
X	0.015411386703344772	41325	41.325	0.07197963143518538
X	0.013793616456449286	2472	2.472	0.17736852719887555
X	0.013891908300757948	1759	1.759	0.19914314254176602
X	0.015344642926721554	25370	25.37	0.08456917745050237
X	0.014324134522725143	4735	4.735	0.14462703046722908
X	0.01611730647098616	24661	24.661	0.08678152153244897
X	0.015224698654336595	28424	28.424	0.08121218032676378
X	0.012944602589897164	281	0.281	0.35847659075421046
X	0.014243713093747574	1958	1.958	0.19376269957362682
X	0.014581567080179037	1846	1.846	0.19915482581474767
X	0.014358154961106004	15718	15.718	0.09702875314570654
X	0.015074613738719557	39357	39.357	0.07262309198056854
X	0.013070861521838985	762	0.762	0.25789905299638893
X	0.014255881069512626	2189	2.189	0.1867451940874378
X	0.01281264466730864	142	0.142	0.44852172030310605
X	0.014141143640684941	569	0.569	0.291826071702419
X	0.013321675906961618	310	0.31	0.35026686609409424
X	0.01414036679591913	1094	1.094	0.2346827224781543
X	0.015244804383436143	59624	59.624	0.06346976874250278
X	0.015940176270470584	70210	70.21	0.06100489943389881
X	0.012910607514513452	781	0.781	0.2547408234095562
X	0.014072308487652448	1880	1.88	0.19561515551898176
X	0.014393804713251312	1872	1.872	0.1973739899651181
X	0.015439055917862148	13682	13.682	0.10410950574507288
X	0.013782851547228001	226	0.226	0.3936197487297393
X	0.012754238762786849	443	0.443	0.3064904444097248
X	0.014368061334210178	1563	1.563	0.2094818489239007
X	0.01526079078451317	10722	10.722	0.11248650204283943
X	0.013324680304115905	383	0.383	0.3264520057634402
X	0.014256074809697138	3140	3.14	0.16558593290505821
X	0.013225936396212001	83	0.083	0.5421458185287665
time for making epsilon is 0.2638230323791504
epsilons are
[0.49720277513171557, 0.18331177690151626, 0.3575591444454903, 0.3961739405177914, 0.14982956885272627, 0.351661304131562, 0.11311604234629344, 0.1863896284005238, 0.3067279693555384, 0.2528596723904419, 0.22485010089765733, 0.0788237093530737, 0.2493697169253733, 0.12737868696226043, 0.23006973128944366, 0.2365802429206188, 0.17508852151422277, 0.10710930341879699, 0.07774756665672863, 0.09094464136109086, 0.33272809120705277, 0.09668270150974992, 0.24345463739870704, 0.20086925442884715, 0.1656575016105291, 0.09620873713035251, 0.10319148255263588, 0.17893454360694058, 0.08266890575221007, 0.049717563700272395, 0.29064414331672733, 0.05641027945545777, 0.26511140160209573, 0.3680008956759627, 0.32989156635926536, 0.14090201013131118, 0.11493502572185489, 0.3482089217999378, 0.258081792700735, 0.32059050329177946, 0.3560639953503634, 0.4826902005303133, 0.5490706101947723, 0.4126589268889994, 0.5312607707616995, 0.504116152699937, 0.28068600783974407, 0.31778248858029534, 0.41112410951278366, 0.24364370695904014, 0.25885505734731024, 0.27029118381913725, 0.3414327917743233, 0.3340317456157684, 0.2380952147274488, 0.2934666100821146, 0.42972845550756306, 0.3863394184369278, 0.32686115885208444, 0.36331681783522646, 0.1969194960200683, 0.47196109685606763, 0.19741064828783925, 0.32093295580718506, 0.37354097777601153, 0.33009131878999814, 0.4948047582786182, 0.41431594288848606, 0.4050717699611434, 0.44533498938609634, 0.5569127334857912, 0.3512027171157857, 0.422423136451347, 0.46890814696856165, 0.29825382622047114, 0.2642286412908766, 0.3699593426860507, 0.4846251407213544, 0.41016146712961266, 0.3101028559160687, 0.28783799876274724, 0.4807724161867269, 0.257295149251617, 0.4859632870055524, 0.31181626841362925, 0.5433367199679882, 0.2861998916441227, 0.44266711717832385, 0.385432878251647, 0.29446675303927305, 0.4650729237641779, 0.46719238661972967, 0.4662350605157059, 0.29031733942101445, 0.42602368126303725, 0.29326572041360816, 0.25194006306644007, 0.20631239348178262, 0.28406999831769797, 0.35404157862391655, 0.18831447961377257, 0.2928886682261438, 0.3719226921293591, 0.45782624553023493, 0.3343237604666036, 0.4519376065205949, 0.2834780693985904, 0.61363733793391, 0.32874177696547385, 0.3638715338452403, 0.4720128725173076, 0.5636706746213043, 0.3827774626559463, 0.29092690483289557, 0.303689216489254, 0.476205695596842, 0.5299048062472075, 0.4096382580725891, 0.27329681879712775, 0.3858402452055389, 0.25428291203532444, 0.3630001885768235, 0.3813943334801497, 0.2767647191061245, 0.4111887365785441, 0.3369483869894669, 0.3586798156067658, 0.6092494127442352, 0.209380179305588, 0.38334720419102, 0.3537042552233944, 0.4327126617528419, 0.40602895995672333, 0.47845931747855963, 0.4049176568152977, 0.36226836557612435, 0.6043039680056942, 0.446234175354638, 0.38483704726571294, 0.3187010056525212, 0.37133720880709525, 0.37870600963719825, 0.29504507059468815, 0.15563297103193452, 0.2838711953459738, 0.46591663565439845, 0.3993369992643893, 0.3292676321822171, 0.3897799788021806, 0.42350144233446985, 0.3755559752921829, 0.25169656737776525, 0.30773797969707256, 0.24603319558924225, 0.4161592040713008, 0.2377498284633122, 0.4400457820896653, 0.4364336301901778, 0.29978532559448307, 0.6932054923194325, 0.25445156555172077, 0.5083080740371828, 0.3832829786045142, 0.5013659626868555, 0.3811225679089381, 0.3395628716153328, 0.3638217194395377, 0.4603811784388761, 0.34335557621488755, 0.6152148246462117, 0.40906891567862447, 0.3649116989958359, 0.36000598977786763, 0.4219505720927785, 0.27726926962168885, 0.39764443088834506, 0.5190798737291568, 0.21305454467639187, 0.3431680597131359, 0.40861132117690097, 0.31703427193287786, 0.4331973169076233, 0.35452575226269706, 0.49895241187779593, 0.2652771007709077, 0.2754921742119389, 0.3419339264044311, 0.3947098748928501, 0.29798002894278025, 0.19391744790794163, 0.5082642270841842, 0.5127330761845887, 0.3884689507336814, 0.5004601032501477, 0.38458768134186727, 0.39233731699276064, 0.39299954299581596, 0.491017407461754, 0.4073682955759658, 0.661335920781422, 0.2719311047367155, 0.4955703594985949, 0.23591619603102598, 0.23884708956869422, 0.33747192632777356, 0.6529093306134928, 0.3201427207331953, 0.33701580008632115, 0.3443484982527704, 0.2697909927947741, 0.19485835020807601, 0.5114888882864719, 0.5880565954462373, 0.5711653725464395, 0.45426117263390053, 0.47063909785004515, 0.46036050210220403, 0.53604556812811, 0.270692698492219, 0.40398220467458934, 0.47434455242411955, 0.34021468121131354, 0.3587757068148331, 0.42005953227363935, 0.24795245989052048, 0.31034000819992236, 0.5054792164685787, 0.4770293199870357, 0.2786326569961603, 0.5033501747892583, 0.5541627626868079, 0.42606866206947164, 0.33867215563413955, 0.26193058637505673, 0.42881911991535987, 0.5754706244404079, 0.4167685555337735, 0.35603038111325036, 0.5915402842622046, 0.3186558251998582, 0.11729407633100099, 0.3634875138052988, 0.26070792153723416, 0.5297233207855033, 0.27976067897101886, 0.19478460744489767, 0.1168356714885288, 0.46495528043147943, 0.19341145235801552, 0.11855430277691782, 0.33021078010754773, 0.09382455902079422, 0.4966727629429589, 0.27193500582549734, 0.11747242717791181, 0.2097241436301237, 0.4069023223183252, 0.22313205522026022, 0.1613358147952092, 0.12122416533818305, 0.21423073806367107, 0.15848636702630892, 0.16572965625312733, 0.15780821034287368, 0.3142008563484377, 0.092360856352209, 0.4093778706840063, 0.36678478394784797, 0.15456128570947092, 0.2590222721871961, 0.06727135799225967, 0.1250856418530598, 0.5362971924695692, 0.29528998314210836, 0.21877764795752697, 0.4659140442102603, 0.1055127553484857, 0.13454270610123495, 0.26580419901345576, 0.6396598143713053, 0.3264854386799075, 0.20174406418991023, 0.1771417030297023, 0.26854304937584816, 0.35188821797473074, 0.06604170671677677, 0.2889908453744211, 0.1842347051551132, 0.09946337077834588, 0.3969567307857786, 0.22315699503016037, 0.088086698699101, 0.09297764820168615, 0.26490093634500467, 0.44681657051153384, 0.36120898481657204, 0.436072449199589, 0.25702787127294996, 0.21844496400300792, 0.1729907097436366, 0.09784435236483847, 0.14263673202218405, 0.2997821629343426, 0.25367231425785436, 0.08457656867875074, 0.19633088719973382, 0.27298461706921545, 0.17249901637858342, 0.06685389454826221, 0.3665415092829651, 0.1444989766478255, 0.3363619777614135, 0.28267494121957315, 0.21571118578469303, 0.09637251688108188, 0.1294361062108171, 0.3427810285261583, 0.1149599070216748, 0.11120424204718189, 0.32900972288151165, 0.27040047992457206, 0.09599484149327177, 0.06768011081362317, 0.17234783266565692, 0.34476569721446154, 0.37137598914367626, 0.49471731602916946, 0.16817479155178536, 0.35510434943769437, 0.3783506174373382, 0.1272706208950339, 0.09724831354616253, 0.2075786528353459, 0.35192502157476174, 0.26445101467808607, 0.21803049820970402, 0.2096065234798032, 0.20377876608284443, 0.41790298404938225, 0.07298638360974667, 0.2727682478087209, 0.18268230740439595, 0.14340501333216163, 0.31798413112478824, 0.13012498500766256, 0.24792870723973148, 0.1429851455473208, 0.4519084867750943, 0.37818656559907715, 0.30727228125148454, 0.23603920078211968, 0.2458663099174228, 0.18682437145931124, 0.33749585162708984, 0.29567311155521925, 0.07264767466860847, 0.1788151601958369, 0.1547345780351059, 0.08980959223125184, 0.10481887024099809, 0.2534682013156171, 0.35389455819564924, 0.18403697674997435, 0.6227384799030135, 0.3344621526181783, 0.24476670805986994, 0.3395357824653024, 0.34611669729571964, 0.42624040048661954, 0.32288291142889686, 0.0682248166862739, 0.19894659475057241, 0.15879517574929952, 0.14630884850277318, 0.4621314965476404, 0.34556937860460457, 0.06221231220112266, 0.07038455853361356, 0.21177599090861676, 0.20769142502698854, 0.40023952044821287, 0.13257920382886132, 0.11132966883361568, 0.09842114989069788, 0.22180916657071745, 0.12463533981063961, 0.11813292328130937, 0.24396567275112244, 0.3474198145701304, 0.22607161622219107, 0.18341967696652842, 0.3634111515243391, 0.11100483426532468, 0.1782441580830082, 0.5018961811576408, 0.24455621548429096, 0.1916963546843648, 0.5168897354688752, 0.09867927123908252, 0.09769505853096314, 0.11090565168974995, 0.10629436480278955, 0.10161737795301469, 0.2557853304621306, 0.24352975583986325, 0.1784341257665439, 0.11949382231736295, 0.461679520443708, 0.16752543003828352, 0.08004971546662637, 0.19029651533348446, 0.1756616932549863, 0.09206707534635857, 0.2632988105448011, 0.5440004147532707, 0.15491125625766478, 0.18770102632337374, 0.2214545064401594, 0.05634196715789444, 0.0825617422161406, 0.1107222691663105, 0.1776974452224549, 0.12097491625814047, 0.4848744531463502, 0.33814486996777693, 0.06732899012390198, 0.3430293909257976, 0.37876414878002757, 0.2128659881384874, 0.13151540443395568, 0.14905518994498268, 0.1339191048791044, 0.313725577933131, 0.17781962670197068, 0.30706034547198097, 0.1939773966015313, 0.16922808609932144, 0.23668424527520326, 0.4408470245437247, 0.1502643944135888, 0.4745159334391406, 0.15981501924413594, 0.3084657603420555, 0.2578604355446573, 0.3438333602374864, 0.23497738204417756, 0.3310904645717881, 0.17060043132296226, 0.10395850961049728, 0.13634270226512804, 0.2609955410773036, 0.3425350637140235, 0.07903084853003438, 0.34167378970673035, 0.056914466579709805, 0.630640233855723, 0.17890966012915005, 0.22646569798988325, 0.3270479583593605, 0.2584319666070152, 0.16681576631777287, 0.06307369621816859, 0.28412500888525943, 0.18402850280655955, 0.39349039512174006, 0.28449223511801525, 0.2276003723954831, 0.24646736959232118, 0.27826966348486654, 0.2353947347153702, 0.18611832582236137, 0.5165005606421764, 0.2859342541870893, 0.09357153260683157, 0.2667788321437839, 0.13125837084965905, 0.10413231053002166, 0.2592985564090063, 0.4142122388970937, 0.4074771137862813, 0.0904041626833405, 0.23990242742372916, 0.21158852247271362, 0.18382527667656068, 0.11291374421212234, 0.3303879000244093, 0.29772112045124083, 0.06723699962625548, 0.36932362434208743, 0.28277394253219923, 0.2741507318065323, 0.14723746514186195, 0.11068481346872037, 0.1986298001528397, 0.15877509061563633, 0.29370750572453674, 0.11630761616345192, 0.2926601049788976, 0.28511521317881416, 0.16277279139261683, 0.30451916100615817, 0.13030311938711076, 0.39909512406287745, 0.5684541821052279, 0.2490704516949468, 0.3210147225026323, 0.09834852860274283, 0.13695945163962053, 0.1994514253636338, 0.41349065077910363, 0.16756346554843346, 0.40212692059770194, 0.2915962021518516, 0.2235455654013886, 0.08793123170589837, 0.26977299010700384, 0.5193154580213862, 0.45076081806348706, 0.06287261808935819, 0.06269338571071045, 0.3331894943460882, 0.2729428315156745, 0.32631305395904997, 0.1212040530753164, 0.33193748336466494, 0.20640482875825888, 0.40178146005493753, 0.1053673650731608, 0.19378690730280396, 0.08448213585462427, 0.32187367180460186, 0.25431487689844984, 0.5017384869993486, 0.16092011325244185, 0.23633954212321, 0.12133453038441105, 0.08774083118265756, 0.05988845217975105, 0.2765943558663288, 0.3804785495432106, 0.19128747528055257, 0.1314402242778337, 0.235325349857916, 0.5020519016962447, 0.42625044109794824, 0.25989206368356266, 0.4910746961473898, 0.13187743340464161, 0.33555037716875646, 0.19357632172063569, 0.2627046514296167, 0.37864817922361327, 0.3823431036942908, 0.09210653875153067, 0.05735705627421968, 0.1004184747968222, 0.16938808238071412, 0.1548346306923087, 0.23213253022488162, 0.25510823804028215, 0.13420184359711756, 0.17098002134917148, 0.13738798400175792, 0.17750474618381706, 0.07809265276737541, 0.27430248801730656, 0.15919609034030308, 0.0874905943849088, 0.13459378910593617, 0.22111335440010696, 0.19790989152478142, 0.2662088429348308, 0.27807993645964046, 0.29220694854380613, 0.16741551413097597, 0.2556178483134099, 0.155195652972286, 0.05499188088592362, 0.09934218209200571, 0.10368847377430446, 0.2074396695392015, 0.20448828390158133, 0.14567153458451768, 0.18641612963786833, 0.10242544077268385, 0.09191231326713907, 0.30116991926982417, 0.06150450042635763, 0.0919072747246267, 0.2167303507506566, 0.3848065670520763, 0.24695343688429944, 0.12873975233997095, 0.34233701946946893, 0.07542968192497997, 0.17386193196521, 0.20890069731609004, 0.2668667714846119, 0.45219427664304124, 0.3232448271847514, 0.2378476374820039, 0.38595449376517155, 0.23753741288228788, 0.039863660040781644, 0.25765538627213036, 0.34017663563346556, 0.3517978635691324, 0.29523445655377273, 0.23172205798338433, 0.1126743027904995, 0.12346755311634541, 0.050044120780734835, 0.32821859767543893, 0.09907141127610904, 0.2569555181648328, 0.08508635983379016, 0.05536424322760685, 0.22959441695830926, 0.5654348597904667, 0.10862258294955435, 0.2228555867049513, 0.11758912578341633, 0.26391695128166454, 0.33427297538201023, 0.14502114129368276, 0.3100918689812756, 0.11668980688777479, 0.1420803040885703, 0.36509304774493057, 0.2994214565924467, 0.08943411128313497, 0.3663230577455138, 0.07360087512030086, 0.4054078789053786, 0.1159045217487288, 0.1619270618764796, 0.1318408427861474, 0.12973706981267033, 0.23790926179698077, 0.29263705050285, 0.1718690941044679, 0.24998009162876983, 0.10362247344504298, 0.11178281982572044, 0.22389376112332182, 0.15484095970546477, 0.14280627965662376, 0.11083489807134889, 0.11590540762394301, 0.11732529555872123, 0.1292133588346151, 0.19594298188806852, 0.35706924263641443, 0.21800241040708332, 0.19968603187691988, 0.12803329049914156, 0.22407201235569132, 0.3801804711810667, 0.3237308618228879, 0.11398625207633849, 0.2696115570634229, 0.3058978660777809, 0.4670073924034229, 0.31412768621287485, 0.26865797813323145, 0.22323109927660273, 0.11076544944353686, 0.4881770485883532, 0.11091704451008448, 0.4689759508065141, 0.280314577913713, 0.07197963143518538, 0.17736852719887555, 0.19914314254176602, 0.08456917745050237, 0.14462703046722908, 0.08678152153244897, 0.08121218032676378, 0.35847659075421046, 0.19376269957362682, 0.19915482581474767, 0.09702875314570654, 0.07262309198056854, 0.25789905299638893, 0.1867451940874378, 0.44852172030310605, 0.291826071702419, 0.35026686609409424, 0.2346827224781543, 0.06346976874250278, 0.06100489943389881, 0.2547408234095562, 0.19561515551898176, 0.1973739899651181, 0.10410950574507288, 0.3936197487297393, 0.3064904444097248, 0.2094818489239007, 0.11248650204283943, 0.3264520057634402, 0.16558593290505821, 0.5421458185287665]
0.09570003265754985
Making ranges
torch.Size([30773, 2])
We keep 5.58e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([426, 2])
We keep 2.28e+03/1.17e+04 = 19% of the original kernel matrix.

torch.Size([5544, 2])
We keep 1.75e+05/2.04e+06 =  8% of the original kernel matrix.

torch.Size([5199, 2])
We keep 4.01e+05/5.35e+06 =  7% of the original kernel matrix.

torch.Size([13104, 2])
We keep 1.29e+06/4.37e+07 =  2% of the original kernel matrix.

torch.Size([881, 2])
We keep 1.22e+04/9.18e+04 = 13% of the original kernel matrix.

torch.Size([6745, 2])
We keep 3.31e+05/5.72e+06 =  5% of the original kernel matrix.

torch.Size([703, 2])
We keep 5.84e+03/3.76e+04 = 15% of the original kernel matrix.

torch.Size([6340, 2])
We keep 2.41e+05/3.66e+06 =  6% of the original kernel matrix.

torch.Size([9644, 2])
We keep 8.26e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([17275, 2])
We keep 2.16e+06/8.83e+07 =  2% of the original kernel matrix.

torch.Size([947, 2])
We keep 1.23e+04/1.02e+05 = 12% of the original kernel matrix.

torch.Size([6897, 2])
We keep 3.39e+05/6.03e+06 =  5% of the original kernel matrix.

torch.Size([18467, 2])
We keep 2.48e+06/9.98e+07 =  2% of the original kernel matrix.

torch.Size([24138, 2])
We keep 3.79e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([5349, 2])
We keep 2.55e+05/5.11e+06 =  4% of the original kernel matrix.

torch.Size([13361, 2])
We keep 1.25e+06/4.27e+07 =  2% of the original kernel matrix.

torch.Size([1267, 2])
We keep 3.23e+04/2.13e+05 = 15% of the original kernel matrix.

torch.Size([7665, 2])
We keep 4.26e+05/8.73e+06 =  4% of the original kernel matrix.

torch.Size([1700, 2])
We keep 9.31e+04/8.08e+05 = 11% of the original kernel matrix.

torch.Size([8143, 2])
We keep 6.35e+05/1.70e+07 =  3% of the original kernel matrix.

torch.Size([2788, 2])
We keep 1.39e+05/1.30e+06 = 10% of the original kernel matrix.

torch.Size([9982, 2])
We keep 7.59e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([44120, 2])
We keep 1.87e+07/8.36e+08 =  2% of the original kernel matrix.

torch.Size([36816, 2])
We keep 8.78e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([2397, 2])
We keep 6.87e+04/8.59e+05 =  7% of the original kernel matrix.

torch.Size([9832, 2])
We keep 6.81e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([13390, 2])
We keep 2.20e+06/5.53e+07 =  3% of the original kernel matrix.

torch.Size([20230, 2])
We keep 3.06e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([2979, 2])
We keep 1.08e+05/1.59e+06 =  6% of the original kernel matrix.

torch.Size([10684, 2])
We keep 8.49e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([2758, 2])
We keep 8.55e+04/1.13e+06 =  7% of the original kernel matrix.

torch.Size([10272, 2])
We keep 7.34e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([6331, 2])
We keep 3.95e+05/7.77e+06 =  5% of the original kernel matrix.

torch.Size([14471, 2])
We keep 1.44e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([19817, 2])
We keep 4.15e+06/1.37e+08 =  3% of the original kernel matrix.

torch.Size([24879, 2])
We keep 4.28e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([50378, 2])
We keep 1.99e+07/9.65e+08 =  2% of the original kernel matrix.

torch.Size([39669, 2])
We keep 9.47e+06/5.87e+08 =  1% of the original kernel matrix.

torch.Size([31705, 2])
We keep 8.38e+06/3.71e+08 =  2% of the original kernel matrix.

torch.Size([31999, 2])
We keep 6.33e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([1015, 2])
We keep 1.45e+04/1.30e+05 = 11% of the original kernel matrix.

torch.Size([7121, 2])
We keep 3.60e+05/6.82e+06 =  5% of the original kernel matrix.

torch.Size([27646, 2])
We keep 5.27e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([30358, 2])
We keep 5.28e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([2567, 2])
We keep 7.16e+04/1.00e+06 =  7% of the original kernel matrix.

torch.Size([10061, 2])
We keep 7.06e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([4491, 2])
We keep 2.10e+05/3.55e+06 =  5% of the original kernel matrix.

torch.Size([12600, 2])
We keep 1.11e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([3628, 2])
We keep 1.37e+06/9.04e+06 = 15% of the original kernel matrix.

torch.Size([9566, 2])
We keep 1.47e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([27936, 2])
We keep 4.95e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([29231, 2])
We keep 5.30e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([20588, 2])
We keep 5.29e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([25134, 2])
We keep 4.92e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([4923, 2])
We keep 7.02e+05/6.28e+06 = 11% of the original kernel matrix.

torch.Size([12655, 2])
We keep 1.30e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([31048, 2])
We keep 2.39e+07/6.29e+08 =  3% of the original kernel matrix.

torch.Size([30187, 2])
We keep 7.79e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([197676, 2])
We keep 1.66e+08/1.53e+10 =  1% of the original kernel matrix.

torch.Size([80790, 2])
We keep 3.03e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([1494, 2])
We keep 3.83e+04/3.34e+05 = 11% of the original kernel matrix.

torch.Size([8132, 2])
We keep 5.01e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([114883, 2])
We keep 2.04e+08/7.04e+09 =  2% of the original kernel matrix.

torch.Size([59257, 2])
We keep 2.19e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([1814, 2])
We keep 7.66e+04/6.24e+05 = 12% of the original kernel matrix.

torch.Size([8623, 2])
We keep 5.91e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([893, 2])
We keep 9.91e+03/7.56e+04 = 13% of the original kernel matrix.

torch.Size([6995, 2])
We keep 3.04e+05/5.19e+06 =  5% of the original kernel matrix.

torch.Size([1047, 2])
We keep 1.63e+04/1.57e+05 = 10% of the original kernel matrix.

torch.Size([7138, 2])
We keep 3.85e+05/7.48e+06 =  5% of the original kernel matrix.

torch.Size([8209, 2])
We keep 2.02e+06/2.62e+07 =  7% of the original kernel matrix.

torch.Size([15417, 2])
We keep 2.27e+06/9.66e+07 =  2% of the original kernel matrix.

torch.Size([17705, 2])
We keep 2.84e+06/9.03e+07 =  3% of the original kernel matrix.

torch.Size([23636, 2])
We keep 3.61e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([1036, 2])
We keep 1.18e+04/1.05e+05 = 11% of the original kernel matrix.

torch.Size([7370, 2])
We keep 3.39e+05/6.12e+06 =  5% of the original kernel matrix.

torch.Size([2042, 2])
We keep 5.85e+04/6.94e+05 =  8% of the original kernel matrix.

torch.Size([9230, 2])
We keep 6.35e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([902, 2])
We keep 2.20e+04/1.42e+05 = 15% of the original kernel matrix.

torch.Size([6444, 2])
We keep 3.60e+05/7.12e+06 =  5% of the original kernel matrix.

torch.Size([922, 2])
We keep 1.13e+04/8.88e+04 = 12% of the original kernel matrix.

torch.Size([6970, 2])
We keep 3.27e+05/5.63e+06 =  5% of the original kernel matrix.

torch.Size([159, 2])
We keep 3.26e+03/1.10e+04 = 29% of the original kernel matrix.

torch.Size([3456, 2])
We keep 1.61e+05/1.98e+06 =  8% of the original kernel matrix.

torch.Size([203, 2])
We keep 1.32e+03/5.04e+03 = 26% of the original kernel matrix.

torch.Size([4088, 2])
We keep 1.35e+05/1.34e+06 = 10% of the original kernel matrix.

torch.Size([536, 2])
We keep 6.41e+03/3.65e+04 = 17% of the original kernel matrix.

torch.Size([5523, 2])
We keep 2.43e+05/3.61e+06 =  6% of the original kernel matrix.

torch.Size([219, 2])
We keep 1.44e+03/6.56e+03 = 21% of the original kernel matrix.

torch.Size([3922, 2])
We keep 1.39e+05/1.53e+06 =  9% of the original kernel matrix.

torch.Size([385, 2])
We keep 2.31e+03/1.04e+04 = 22% of the original kernel matrix.

torch.Size([5197, 2])
We keep 1.70e+05/1.93e+06 =  8% of the original kernel matrix.

torch.Size([1701, 2])
We keep 3.57e+04/3.98e+05 =  8% of the original kernel matrix.

torch.Size([8515, 2])
We keep 5.15e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([1259, 2])
We keep 2.12e+04/1.92e+05 = 11% of the original kernel matrix.

torch.Size([7701, 2])
We keep 4.26e+05/8.27e+06 =  5% of the original kernel matrix.

torch.Size([593, 2])
We keep 4.95e+03/3.35e+04 = 14% of the original kernel matrix.

torch.Size([5928, 2])
We keep 2.31e+05/3.46e+06 =  6% of the original kernel matrix.

torch.Size([1312, 2])
We keep 1.80e+05/8.89e+05 = 20% of the original kernel matrix.

torch.Size([6812, 2])
We keep 6.79e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([1989, 2])
We keep 5.57e+04/6.45e+05 =  8% of the original kernel matrix.

torch.Size([8984, 2])
We keep 6.14e+05/1.52e+07 =  4% of the original kernel matrix.

torch.Size([2040, 2])
We keep 5.76e+04/5.61e+05 = 10% of the original kernel matrix.

torch.Size([9213, 2])
We keep 6.05e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([817, 2])
We keep 1.59e+04/1.09e+05 = 14% of the original kernel matrix.

torch.Size([6330, 2])
We keep 3.35e+05/6.23e+06 =  5% of the original kernel matrix.

torch.Size([1157, 2])
We keep 1.71e+04/1.31e+05 = 13% of the original kernel matrix.

torch.Size([7368, 2])
We keep 3.74e+05/6.84e+06 =  5% of the original kernel matrix.

torch.Size([2962, 2])
We keep 7.65e+04/1.10e+06 =  6% of the original kernel matrix.

torch.Size([10719, 2])
We keep 7.37e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([938, 2])
We keep 5.36e+04/2.71e+05 = 19% of the original kernel matrix.

torch.Size([6117, 2])
We keep 4.48e+05/9.84e+06 =  4% of the original kernel matrix.

torch.Size([555, 2])
We keep 4.37e+03/2.82e+04 = 15% of the original kernel matrix.

torch.Size([5847, 2])
We keep 2.16e+05/3.17e+06 =  6% of the original kernel matrix.

torch.Size([614, 2])
We keep 7.34e+03/4.97e+04 = 14% of the original kernel matrix.

torch.Size([5871, 2])
We keep 2.62e+05/4.21e+06 =  6% of the original kernel matrix.

torch.Size([1198, 2])
We keep 1.43e+04/1.38e+05 = 10% of the original kernel matrix.

torch.Size([7671, 2])
We keep 3.64e+05/7.01e+06 =  5% of the original kernel matrix.

torch.Size([713, 2])
We keep 1.32e+04/7.24e+04 = 18% of the original kernel matrix.

torch.Size([5918, 2])
We keep 2.97e+05/5.08e+06 =  5% of the original kernel matrix.

torch.Size([4464, 2])
We keep 2.05e+05/3.85e+06 =  5% of the original kernel matrix.

torch.Size([12534, 2])
We keep 1.10e+06/3.71e+07 =  2% of the original kernel matrix.

torch.Size([338, 2])
We keep 2.84e+03/1.49e+04 = 19% of the original kernel matrix.

torch.Size([4912, 2])
We keep 1.79e+05/2.30e+06 =  7% of the original kernel matrix.

torch.Size([4351, 2])
We keep 2.56e+05/3.90e+06 =  6% of the original kernel matrix.

torch.Size([12184, 2])
We keep 1.16e+06/3.73e+07 =  3% of the original kernel matrix.

torch.Size([1162, 2])
We keep 1.99e+04/1.71e+05 = 11% of the original kernel matrix.

torch.Size([7362, 2])
We keep 4.08e+05/7.80e+06 =  5% of the original kernel matrix.

torch.Size([871, 2])
We keep 1.05e+04/7.67e+04 = 13% of the original kernel matrix.

torch.Size([6824, 2])
We keep 3.18e+05/5.23e+06 =  6% of the original kernel matrix.

torch.Size([1108, 2])
We keep 1.46e+04/1.29e+05 = 11% of the original kernel matrix.

torch.Size([7381, 2])
We keep 3.56e+05/6.78e+06 =  5% of the original kernel matrix.

torch.Size([296, 2])
We keep 2.51e+03/1.04e+04 = 24% of the original kernel matrix.

torch.Size([4490, 2])
We keep 1.68e+05/1.93e+06 =  8% of the original kernel matrix.

torch.Size([568, 2])
We keep 5.32e+03/3.53e+04 = 15% of the original kernel matrix.

torch.Size([5885, 2])
We keep 2.45e+05/3.55e+06 =  6% of the original kernel matrix.

torch.Size([585, 2])
We keep 8.48e+03/4.28e+04 = 19% of the original kernel matrix.

torch.Size([5836, 2])
We keep 2.56e+05/3.91e+06 =  6% of the original kernel matrix.

torch.Size([450, 2])
We keep 3.89e+03/2.10e+04 = 18% of the original kernel matrix.

torch.Size([5298, 2])
We keep 2.08e+05/2.74e+06 =  7% of the original kernel matrix.

torch.Size([267, 2])
We keep 1.54e+03/5.33e+03 = 28% of the original kernel matrix.

torch.Size([4320, 2])
We keep 1.40e+05/1.38e+06 = 10% of the original kernel matrix.

torch.Size([879, 2])
We keep 1.50e+04/1.04e+05 = 14% of the original kernel matrix.

torch.Size([6651, 2])
We keep 3.49e+05/6.08e+06 =  5% of the original kernel matrix.

torch.Size([446, 2])
We keep 4.32e+03/2.53e+04 = 17% of the original kernel matrix.

torch.Size([5052, 2])
We keep 2.13e+05/3.00e+06 =  7% of the original kernel matrix.

torch.Size([321, 2])
We keep 3.74e+03/1.66e+04 = 22% of the original kernel matrix.

torch.Size([4555, 2])
We keep 1.94e+05/2.44e+06 =  7% of the original kernel matrix.

torch.Size([1358, 2])
We keep 2.86e+04/2.95e+05 =  9% of the original kernel matrix.

torch.Size([7849, 2])
We keep 4.93e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([2120, 2])
We keep 5.28e+04/6.32e+05 =  8% of the original kernel matrix.

torch.Size([9446, 2])
We keep 6.07e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([888, 2])
We keep 8.35e+03/6.60e+04 = 12% of the original kernel matrix.

torch.Size([6976, 2])
We keep 2.87e+05/4.85e+06 =  5% of the original kernel matrix.

torch.Size([322, 2])
We keep 2.49e+03/1.10e+04 = 22% of the original kernel matrix.

torch.Size([4584, 2])
We keep 1.65e+05/1.98e+06 =  8% of the original kernel matrix.

torch.Size([621, 2])
We keep 5.29e+03/3.46e+04 = 15% of the original kernel matrix.

torch.Size([6029, 2])
We keep 2.42e+05/3.51e+06 =  6% of the original kernel matrix.

torch.Size([1231, 2])
We keep 2.04e+04/1.97e+05 = 10% of the original kernel matrix.

torch.Size([7561, 2])
We keep 4.13e+05/8.39e+06 =  4% of the original kernel matrix.

torch.Size([1661, 2])
We keep 3.17e+04/3.47e+05 =  9% of the original kernel matrix.

torch.Size([8599, 2])
We keep 5.11e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([244, 2])
We keep 3.28e+03/1.12e+04 = 29% of the original kernel matrix.

torch.Size([4041, 2])
We keep 1.64e+05/2.00e+06 =  8% of the original kernel matrix.

torch.Size([2027, 2])
We keep 5.83e+04/6.40e+05 =  9% of the original kernel matrix.

torch.Size([9083, 2])
We keep 6.14e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([409, 2])
We keep 2.62e+03/1.19e+04 = 22% of the original kernel matrix.

torch.Size([5265, 2])
We keep 1.78e+05/2.06e+06 =  8% of the original kernel matrix.

torch.Size([1231, 2])
We keep 2.39e+04/2.10e+05 = 11% of the original kernel matrix.

torch.Size([7594, 2])
We keep 4.20e+05/8.65e+06 =  4% of the original kernel matrix.

torch.Size([178, 2])
We keep 1.90e+03/5.93e+03 = 31% of the original kernel matrix.

torch.Size([3671, 2])
We keep 1.43e+05/1.45e+06 =  9% of the original kernel matrix.

torch.Size([1388, 2])
We keep 4.80e+04/3.53e+05 = 13% of the original kernel matrix.

torch.Size([7637, 2])
We keep 5.12e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([535, 2])
We keep 4.27e+03/2.37e+04 = 18% of the original kernel matrix.

torch.Size([5653, 2])
We keep 2.21e+05/2.91e+06 =  7% of the original kernel matrix.

torch.Size([772, 2])
We keep 7.73e+03/5.90e+04 = 13% of the original kernel matrix.

torch.Size([6574, 2])
We keep 2.90e+05/4.59e+06 =  6% of the original kernel matrix.

torch.Size([1423, 2])
We keep 2.73e+04/2.84e+05 =  9% of the original kernel matrix.

torch.Size([7915, 2])
We keep 4.75e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([429, 2])
We keep 2.68e+03/1.61e+04 = 16% of the original kernel matrix.

torch.Size([5376, 2])
We keep 1.88e+05/2.40e+06 =  7% of the original kernel matrix.

torch.Size([362, 2])
We keep 2.78e+03/1.44e+04 = 19% of the original kernel matrix.

torch.Size([4963, 2])
We keep 1.82e+05/2.27e+06 =  8% of the original kernel matrix.

torch.Size([462, 2])
We keep 3.29e+03/1.72e+04 = 19% of the original kernel matrix.

torch.Size([5640, 2])
We keep 1.99e+05/2.47e+06 =  8% of the original kernel matrix.

torch.Size([1618, 2])
We keep 3.12e+04/3.18e+05 =  9% of the original kernel matrix.

torch.Size([8533, 2])
We keep 4.95e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([638, 2])
We keep 4.96e+03/3.13e+04 = 15% of the original kernel matrix.

torch.Size([6243, 2])
We keep 2.35e+05/3.34e+06 =  7% of the original kernel matrix.

torch.Size([1584, 2])
We keep 2.88e+04/3.11e+05 =  9% of the original kernel matrix.

torch.Size([8514, 2])
We keep 4.92e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([2421, 2])
We keep 6.34e+04/8.82e+05 =  7% of the original kernel matrix.

torch.Size([9878, 2])
We keep 6.83e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([3044, 2])
We keep 3.02e+05/2.68e+06 = 11% of the original kernel matrix.

torch.Size([10320, 2])
We keep 1.01e+06/3.09e+07 =  3% of the original kernel matrix.

torch.Size([1646, 2])
We keep 3.54e+04/3.42e+05 = 10% of the original kernel matrix.

torch.Size([8395, 2])
We keep 4.95e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([1001, 2])
We keep 1.24e+04/1.04e+05 = 11% of the original kernel matrix.

torch.Size([7293, 2])
We keep 3.47e+05/6.10e+06 =  5% of the original kernel matrix.

torch.Size([4861, 2])
We keep 2.23e+05/4.17e+06 =  5% of the original kernel matrix.

torch.Size([12650, 2])
We keep 1.15e+06/3.86e+07 =  2% of the original kernel matrix.

torch.Size([1472, 2])
We keep 3.69e+04/3.16e+05 = 11% of the original kernel matrix.

torch.Size([8117, 2])
We keep 4.89e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([786, 2])
We keep 1.02e+04/6.92e+04 = 14% of the original kernel matrix.

torch.Size([6557, 2])
We keep 2.99e+05/4.97e+06 =  6% of the original kernel matrix.

torch.Size([454, 2])
We keep 3.03e+03/1.74e+04 = 17% of the original kernel matrix.

torch.Size([5494, 2])
We keep 1.92e+05/2.49e+06 =  7% of the original kernel matrix.

torch.Size([1006, 2])
We keep 1.34e+04/1.14e+05 = 11% of the original kernel matrix.

torch.Size([6915, 2])
We keep 3.41e+05/6.37e+06 =  5% of the original kernel matrix.

torch.Size([399, 2])
We keep 4.46e+03/1.80e+04 = 24% of the original kernel matrix.

torch.Size([4980, 2])
We keep 1.95e+05/2.53e+06 =  7% of the original kernel matrix.

torch.Size([1805, 2])
We keep 3.38e+04/3.88e+05 =  8% of the original kernel matrix.

torch.Size([8894, 2])
We keep 5.26e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([158, 2])
We keep 6.96e+02/2.50e+03 = 27% of the original kernel matrix.

torch.Size([3578, 2])
We keep 1.02e+05/9.44e+05 = 10% of the original kernel matrix.

torch.Size([1122, 2])
We keep 1.65e+04/1.53e+05 = 10% of the original kernel matrix.

torch.Size([7389, 2])
We keep 3.82e+05/7.39e+06 =  5% of the original kernel matrix.

torch.Size([862, 2])
We keep 9.60e+03/8.24e+04 = 11% of the original kernel matrix.

torch.Size([6862, 2])
We keep 3.03e+05/5.42e+06 =  5% of the original kernel matrix.

torch.Size([360, 2])
We keep 3.26e+03/1.61e+04 = 20% of the original kernel matrix.

torch.Size([4952, 2])
We keep 1.88e+05/2.40e+06 =  7% of the original kernel matrix.

torch.Size([252, 2])
We keep 1.19e+03/4.62e+03 = 25% of the original kernel matrix.

torch.Size([4444, 2])
We keep 1.19e+05/1.28e+06 =  9% of the original kernel matrix.

torch.Size([627, 2])
We keep 8.88e+03/4.71e+04 = 18% of the original kernel matrix.

torch.Size([5743, 2])
We keep 2.62e+05/4.10e+06 =  6% of the original kernel matrix.

torch.Size([1582, 2])
We keep 2.64e+04/3.01e+05 =  8% of the original kernel matrix.

torch.Size([8387, 2])
We keep 4.66e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([1349, 2])
We keep 2.35e+04/2.33e+05 = 10% of the original kernel matrix.

torch.Size([7828, 2])
We keep 4.34e+05/9.12e+06 =  4% of the original kernel matrix.

torch.Size([298, 2])
We keep 3.96e+03/1.37e+04 = 28% of the original kernel matrix.

torch.Size([4261, 2])
We keep 1.82e+05/2.21e+06 =  8% of the original kernel matrix.

torch.Size([266, 2])
We keep 2.06e+03/7.57e+03 = 27% of the original kernel matrix.

torch.Size([4533, 2])
We keep 1.51e+05/1.64e+06 =  9% of the original kernel matrix.

torch.Size([530, 2])
We keep 5.37e+03/3.50e+04 = 15% of the original kernel matrix.

torch.Size([5609, 2])
We keep 2.38e+05/3.53e+06 =  6% of the original kernel matrix.

torch.Size([1697, 2])
We keep 5.05e+04/5.37e+05 =  9% of the original kernel matrix.

torch.Size([8506, 2])
We keep 5.95e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([712, 2])
We keep 7.12e+03/5.20e+04 = 13% of the original kernel matrix.

torch.Size([6284, 2])
We keep 2.79e+05/4.31e+06 =  6% of the original kernel matrix.

torch.Size([2404, 2])
We keep 5.93e+04/7.62e+05 =  7% of the original kernel matrix.

torch.Size([9908, 2])
We keep 6.51e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([957, 2])
We keep 1.00e+04/8.29e+04 = 12% of the original kernel matrix.

torch.Size([7124, 2])
We keep 3.16e+05/5.44e+06 =  5% of the original kernel matrix.

torch.Size([676, 2])
We keep 1.02e+04/6.97e+04 = 14% of the original kernel matrix.

torch.Size([6108, 2])
We keep 3.10e+05/4.99e+06 =  6% of the original kernel matrix.

torch.Size([1851, 2])
We keep 4.52e+04/5.08e+05 =  8% of the original kernel matrix.

torch.Size([8958, 2])
We keep 5.84e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([591, 2])
We keep 6.25e+03/4.08e+04 = 15% of the original kernel matrix.

torch.Size([5989, 2])
We keep 2.60e+05/3.82e+06 =  6% of the original kernel matrix.

torch.Size([1207, 2])
We keep 1.59e+04/1.27e+05 = 12% of the original kernel matrix.

torch.Size([7742, 2])
We keep 3.57e+05/6.72e+06 =  5% of the original kernel matrix.

torch.Size([932, 2])
We keep 1.05e+04/7.90e+04 = 13% of the original kernel matrix.

torch.Size([6987, 2])
We keep 3.20e+05/5.31e+06 =  6% of the original kernel matrix.

torch.Size([252, 2])
We keep 8.17e+02/2.60e+03 = 31% of the original kernel matrix.

torch.Size([4666, 2])
We keep 1.11e+05/9.63e+05 = 11% of the original kernel matrix.

torch.Size([4124, 2])
We keep 1.68e+05/2.95e+06 =  5% of the original kernel matrix.

torch.Size([12188, 2])
We keep 1.05e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([686, 2])
We keep 8.07e+03/5.62e+04 = 14% of the original kernel matrix.

torch.Size([6206, 2])
We keep 2.79e+05/4.48e+06 =  6% of the original kernel matrix.

torch.Size([862, 2])
We keep 1.05e+04/8.24e+04 = 12% of the original kernel matrix.

torch.Size([6658, 2])
We keep 3.10e+05/5.42e+06 =  5% of the original kernel matrix.

torch.Size([530, 2])
We keep 5.10e+03/2.86e+04 = 17% of the original kernel matrix.

torch.Size([5797, 2])
We keep 2.22e+05/3.19e+06 =  6% of the original kernel matrix.

torch.Size([559, 2])
We keep 5.95e+03/3.88e+04 = 15% of the original kernel matrix.

torch.Size([5736, 2])
We keep 2.50e+05/3.72e+06 =  6% of the original kernel matrix.

torch.Size([329, 2])
We keep 2.68e+03/1.21e+04 = 22% of the original kernel matrix.

torch.Size([4605, 2])
We keep 1.65e+05/2.08e+06 =  7% of the original kernel matrix.

torch.Size([677, 2])
We keep 6.52e+03/4.45e+04 = 14% of the original kernel matrix.

torch.Size([6241, 2])
We keep 2.58e+05/3.99e+06 =  6% of the original kernel matrix.

torch.Size([845, 2])
We keep 9.18e+03/7.67e+04 = 11% of the original kernel matrix.

torch.Size([6672, 2])
We keep 2.99e+05/5.23e+06 =  5% of the original kernel matrix.

torch.Size([196, 2])
We keep 6.97e+02/2.81e+03 = 24% of the original kernel matrix.

torch.Size([4155, 2])
We keep 1.06e+05/1.00e+06 = 10% of the original kernel matrix.

torch.Size([425, 2])
We keep 3.45e+03/1.99e+04 = 17% of the original kernel matrix.

torch.Size([5195, 2])
We keep 1.95e+05/2.66e+06 =  7% of the original kernel matrix.

torch.Size([826, 2])
We keep 6.88e+03/5.34e+04 = 12% of the original kernel matrix.

torch.Size([6822, 2])
We keep 2.69e+05/4.36e+06 =  6% of the original kernel matrix.

torch.Size([1318, 2])
We keep 1.80e+04/1.70e+05 = 10% of the original kernel matrix.

torch.Size([8033, 2])
We keep 3.97e+05/7.78e+06 =  5% of the original kernel matrix.

torch.Size([780, 2])
We keep 8.16e+03/6.71e+04 = 12% of the original kernel matrix.

torch.Size([6499, 2])
We keep 2.94e+05/4.89e+06 =  6% of the original kernel matrix.

torch.Size([734, 2])
We keep 1.27e+04/6.71e+04 = 18% of the original kernel matrix.

torch.Size([6338, 2])
We keep 3.09e+05/4.89e+06 =  6% of the original kernel matrix.

torch.Size([1409, 2])
We keep 3.27e+04/3.42e+05 =  9% of the original kernel matrix.

torch.Size([8003, 2])
We keep 5.17e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([8313, 2])
We keep 7.39e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([16053, 2])
We keep 1.92e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([1723, 2])
We keep 3.47e+04/3.91e+05 =  8% of the original kernel matrix.

torch.Size([8643, 2])
We keep 5.18e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([471, 2])
We keep 3.30e+03/1.77e+04 = 18% of the original kernel matrix.

torch.Size([5595, 2])
We keep 1.99e+05/2.51e+06 =  7% of the original kernel matrix.

torch.Size([718, 2])
We keep 6.50e+03/4.45e+04 = 14% of the original kernel matrix.

torch.Size([6441, 2])
We keep 2.59e+05/3.99e+06 =  6% of the original kernel matrix.

torch.Size([1080, 2])
We keep 1.50e+04/1.42e+05 = 10% of the original kernel matrix.

torch.Size([7336, 2])
We keep 3.73e+05/7.12e+06 =  5% of the original kernel matrix.

torch.Size([551, 2])
We keep 9.56e+03/4.67e+04 = 20% of the original kernel matrix.

torch.Size([5400, 2])
We keep 2.62e+05/4.08e+06 =  6% of the original kernel matrix.

torch.Size([604, 2])
We keep 5.20e+03/3.24e+04 = 16% of the original kernel matrix.

torch.Size([6231, 2])
We keep 2.35e+05/3.40e+06 =  6% of the original kernel matrix.

torch.Size([780, 2])
We keep 9.42e+03/6.40e+04 = 14% of the original kernel matrix.

torch.Size([6523, 2])
We keep 2.96e+05/4.78e+06 =  6% of the original kernel matrix.

torch.Size([2347, 2])
We keep 6.27e+04/7.41e+05 =  8% of the original kernel matrix.

torch.Size([9668, 2])
We keep 6.44e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([1401, 2])
We keep 2.19e+04/2.40e+05 =  9% of the original kernel matrix.

torch.Size([8296, 2])
We keep 4.49e+05/9.26e+06 =  4% of the original kernel matrix.

torch.Size([1615, 2])
We keep 1.63e+05/8.78e+05 = 18% of the original kernel matrix.

torch.Size([7862, 2])
We keep 6.89e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([517, 2])
We keep 6.31e+03/3.92e+04 = 16% of the original kernel matrix.

torch.Size([5639, 2])
We keep 2.59e+05/3.74e+06 =  6% of the original kernel matrix.

torch.Size([2945, 2])
We keep 9.36e+04/1.29e+06 =  7% of the original kernel matrix.

torch.Size([10704, 2])
We keep 7.97e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([475, 2])
We keep 4.42e+03/2.40e+04 = 18% of the original kernel matrix.

torch.Size([5390, 2])
We keep 2.16e+05/2.93e+06 =  7% of the original kernel matrix.

torch.Size([551, 2])
We keep 5.66e+03/2.62e+04 = 21% of the original kernel matrix.

torch.Size([5617, 2])
We keep 2.27e+05/3.06e+06 =  7% of the original kernel matrix.

torch.Size([1515, 2])
We keep 2.94e+04/3.09e+05 =  9% of the original kernel matrix.

torch.Size([8374, 2])
We keep 4.93e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([160, 2])
We keep 3.80e+02/1.16e+03 = 32% of the original kernel matrix.

torch.Size([4144, 2])
We keep 8.65e+04/6.42e+05 = 13% of the original kernel matrix.

torch.Size([2047, 2])
We keep 6.34e+04/6.92e+05 =  9% of the original kernel matrix.

torch.Size([9071, 2])
We keep 6.30e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([309, 2])
We keep 2.57e+03/9.02e+03 = 28% of the original kernel matrix.

torch.Size([4695, 2])
We keep 1.58e+05/1.79e+06 =  8% of the original kernel matrix.

torch.Size([748, 2])
We keep 9.60e+03/5.95e+04 = 16% of the original kernel matrix.

torch.Size([6517, 2])
We keep 2.88e+05/4.61e+06 =  6% of the original kernel matrix.

torch.Size([360, 2])
We keep 2.44e+03/1.14e+04 = 21% of the original kernel matrix.

torch.Size([5149, 2])
We keep 1.77e+05/2.02e+06 =  8% of the original kernel matrix.

torch.Size([648, 2])
We keep 8.46e+03/5.43e+04 = 15% of the original kernel matrix.

torch.Size([6016, 2])
We keep 2.80e+05/4.40e+06 =  6% of the original kernel matrix.

torch.Size([952, 2])
We keep 1.22e+04/1.06e+05 = 11% of the original kernel matrix.

torch.Size([6831, 2])
We keep 3.37e+05/6.16e+06 =  5% of the original kernel matrix.

torch.Size([875, 2])
We keep 1.15e+04/8.12e+04 = 14% of the original kernel matrix.

torch.Size([6875, 2])
We keep 3.16e+05/5.38e+06 =  5% of the original kernel matrix.

torch.Size([476, 2])
We keep 3.25e+03/1.69e+04 = 19% of the original kernel matrix.

torch.Size([5501, 2])
We keep 1.96e+05/2.46e+06 =  7% of the original kernel matrix.

torch.Size([972, 2])
We keep 1.55e+04/1.17e+05 = 13% of the original kernel matrix.

torch.Size([7136, 2])
We keep 3.60e+05/6.46e+06 =  5% of the original kernel matrix.

torch.Size([185, 2])
We keep 6.25e+02/2.40e+03 = 26% of the original kernel matrix.

torch.Size([4139, 2])
We keep 1.03e+05/9.26e+05 = 11% of the original kernel matrix.

torch.Size([486, 2])
We keep 5.15e+03/3.06e+04 = 16% of the original kernel matrix.

torch.Size([5378, 2])
We keep 2.31e+05/3.31e+06 =  6% of the original kernel matrix.

torch.Size([807, 2])
We keep 1.00e+04/7.02e+04 = 14% of the original kernel matrix.

torch.Size([6282, 2])
We keep 2.98e+05/5.01e+06 =  5% of the original kernel matrix.

torch.Size([909, 2])
We keep 1.17e+04/9.18e+04 = 12% of the original kernel matrix.

torch.Size([6976, 2])
We keep 3.31e+05/5.72e+06 =  5% of the original kernel matrix.

torch.Size([528, 2])
We keep 6.74e+03/3.42e+04 = 19% of the original kernel matrix.

torch.Size([5658, 2])
We keep 2.38e+05/3.49e+06 =  6% of the original kernel matrix.

torch.Size([1661, 2])
We keep 3.79e+04/4.52e+05 =  8% of the original kernel matrix.

torch.Size([8621, 2])
We keep 5.18e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([721, 2])
We keep 6.54e+03/4.04e+04 = 16% of the original kernel matrix.

torch.Size([6391, 2])
We keep 2.49e+05/3.80e+06 =  6% of the original kernel matrix.

torch.Size([180, 2])
We keep 1.50e+03/7.57e+03 = 19% of the original kernel matrix.

torch.Size([3645, 2])
We keep 1.47e+05/1.64e+06 =  8% of the original kernel matrix.

torch.Size([3792, 2])
We keep 1.34e+05/2.07e+06 =  6% of the original kernel matrix.

torch.Size([11581, 2])
We keep 9.19e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([674, 2])
We keep 2.06e+04/9.99e+04 = 20% of the original kernel matrix.

torch.Size([5724, 2])
We keep 3.26e+05/5.97e+06 =  5% of the original kernel matrix.

torch.Size([511, 2])
We keep 4.80e+03/2.82e+04 = 16% of the original kernel matrix.

torch.Size([5283, 2])
We keep 2.25e+05/3.17e+06 =  7% of the original kernel matrix.

torch.Size([1245, 2])
We keep 1.98e+04/1.97e+05 = 10% of the original kernel matrix.

torch.Size([7765, 2])
We keep 4.17e+05/8.39e+06 =  4% of the original kernel matrix.

torch.Size([590, 2])
We keep 3.93e+03/2.66e+04 = 14% of the original kernel matrix.

torch.Size([6062, 2])
We keep 2.20e+05/3.08e+06 =  7% of the original kernel matrix.

torch.Size([702, 2])
We keep 1.52e+04/8.35e+04 = 18% of the original kernel matrix.

torch.Size([6051, 2])
We keep 3.07e+05/5.46e+06 =  5% of the original kernel matrix.

torch.Size([397, 2])
We keep 2.43e+03/1.21e+04 = 20% of the original kernel matrix.

torch.Size([5332, 2])
We keep 1.73e+05/2.08e+06 =  8% of the original kernel matrix.

torch.Size([2119, 2])
We keep 4.63e+04/5.75e+05 =  8% of the original kernel matrix.

torch.Size([9473, 2])
We keep 5.81e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([1765, 2])
We keep 4.17e+04/5.04e+05 =  8% of the original kernel matrix.

torch.Size([8737, 2])
We keep 5.71e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([1091, 2])
We keep 1.29e+04/1.13e+05 = 11% of the original kernel matrix.

torch.Size([7405, 2])
We keep 3.46e+05/6.35e+06 =  5% of the original kernel matrix.

torch.Size([404, 2])
We keep 9.96e+03/3.88e+04 = 25% of the original kernel matrix.

torch.Size([4468, 2])
We keep 2.37e+05/3.72e+06 =  6% of the original kernel matrix.

torch.Size([1454, 2])
We keep 2.58e+04/2.65e+05 =  9% of the original kernel matrix.

torch.Size([8087, 2])
We keep 4.61e+05/9.73e+06 =  4% of the original kernel matrix.

torch.Size([4701, 2])
We keep 2.59e+05/4.21e+06 =  6% of the original kernel matrix.

torch.Size([12607, 2])
We keep 1.18e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([313, 2])
We keep 2.10e+03/8.10e+03 = 25% of the original kernel matrix.

torch.Size([4646, 2])
We keep 1.49e+05/1.70e+06 =  8% of the original kernel matrix.

torch.Size([287, 2])
We keep 2.17e+03/9.02e+03 = 24% of the original kernel matrix.

torch.Size([4602, 2])
We keep 1.69e+05/1.79e+06 =  9% of the original kernel matrix.

torch.Size([849, 2])
We keep 7.36e+03/5.29e+04 = 13% of the original kernel matrix.

torch.Size([6897, 2])
We keep 2.79e+05/4.34e+06 =  6% of the original kernel matrix.

torch.Size([393, 2])
We keep 2.79e+03/1.39e+04 = 20% of the original kernel matrix.

torch.Size([5326, 2])
We keep 1.89e+05/2.23e+06 =  8% of the original kernel matrix.

torch.Size([516, 2])
We keep 1.31e+04/5.38e+04 = 24% of the original kernel matrix.

torch.Size([5077, 2])
We keep 2.74e+05/4.38e+06 =  6% of the original kernel matrix.

torch.Size([679, 2])
We keep 8.11e+03/5.52e+04 = 14% of the original kernel matrix.

torch.Size([6255, 2])
We keep 2.87e+05/4.44e+06 =  6% of the original kernel matrix.

torch.Size([649, 2])
We keep 6.96e+03/4.54e+04 = 15% of the original kernel matrix.

torch.Size([6082, 2])
We keep 2.67e+05/4.02e+06 =  6% of the original kernel matrix.

torch.Size([236, 2])
We keep 2.70e+03/9.80e+03 = 27% of the original kernel matrix.

torch.Size([3855, 2])
We keep 1.54e+05/1.87e+06 =  8% of the original kernel matrix.

torch.Size([524, 2])
We keep 8.26e+03/4.00e+04 = 20% of the original kernel matrix.

torch.Size([5418, 2])
We keep 2.56e+05/3.78e+06 =  6% of the original kernel matrix.

torch.Size([151, 2])
We keep 4.52e+02/1.44e+03 = 31% of the original kernel matrix.

torch.Size([3939, 2])
We keep 9.09e+04/7.18e+05 = 12% of the original kernel matrix.

torch.Size([1815, 2])
We keep 4.11e+04/4.97e+05 =  8% of the original kernel matrix.

torch.Size([8895, 2])
We keep 5.65e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([324, 2])
We keep 2.06e+03/1.23e+04 = 16% of the original kernel matrix.

torch.Size([4893, 2])
We keep 1.73e+05/2.10e+06 =  8% of the original kernel matrix.

torch.Size([2993, 2])
We keep 9.29e+04/1.22e+06 =  7% of the original kernel matrix.

torch.Size([10706, 2])
We keep 7.58e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([2847, 2])
We keep 9.38e+04/1.30e+06 =  7% of the original kernel matrix.

torch.Size([10671, 2])
We keep 7.88e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([997, 2])
We keep 1.29e+04/1.05e+05 = 12% of the original kernel matrix.

torch.Size([7099, 2])
We keep 3.37e+05/6.12e+06 =  5% of the original kernel matrix.

torch.Size([101, 2])
We keep 4.24e+02/1.44e+03 = 29% of the original kernel matrix.

torch.Size([3235, 2])
We keep 9.19e+04/7.18e+05 = 12% of the original kernel matrix.

torch.Size([1234, 2])
We keep 2.00e+04/1.75e+05 = 11% of the original kernel matrix.

torch.Size([7803, 2])
We keep 4.00e+05/7.90e+06 =  5% of the original kernel matrix.

torch.Size([1188, 2])
We keep 1.50e+04/1.32e+05 = 11% of the original kernel matrix.

torch.Size([7800, 2])
We keep 3.66e+05/6.88e+06 =  5% of the original kernel matrix.

torch.Size([842, 2])
We keep 1.45e+04/1.06e+05 = 13% of the original kernel matrix.

torch.Size([6492, 2])
We keep 3.42e+05/6.16e+06 =  5% of the original kernel matrix.

torch.Size([1997, 2])
We keep 4.57e+04/5.18e+05 =  8% of the original kernel matrix.

torch.Size([9109, 2])
We keep 5.69e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([4679, 2])
We keep 2.41e+05/3.74e+06 =  6% of the original kernel matrix.

torch.Size([12592, 2])
We keep 1.14e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([227, 2])
We keep 2.01e+03/7.92e+03 = 25% of the original kernel matrix.

torch.Size([4028, 2])
We keep 1.47e+05/1.68e+06 =  8% of the original kernel matrix.

torch.Size([202, 2])
We keep 8.72e+02/3.84e+03 = 22% of the original kernel matrix.

torch.Size([4340, 2])
We keep 1.25e+05/1.17e+06 = 10% of the original kernel matrix.

torch.Size([213, 2])
We keep 1.01e+03/3.14e+03 = 32% of the original kernel matrix.

torch.Size([3983, 2])
We keep 1.16e+05/1.06e+06 = 10% of the original kernel matrix.

torch.Size([384, 2])
We keep 3.79e+03/2.28e+04 = 16% of the original kernel matrix.

torch.Size([5159, 2])
We keep 2.10e+05/2.85e+06 =  7% of the original kernel matrix.

torch.Size([164, 2])
We keep 4.61e+03/1.42e+04 = 32% of the original kernel matrix.

torch.Size([3302, 2])
We keep 1.82e+05/2.25e+06 =  8% of the original kernel matrix.

torch.Size([473, 2])
We keep 3.50e+03/2.10e+04 = 16% of the original kernel matrix.

torch.Size([5755, 2])
We keep 2.08e+05/2.74e+06 =  7% of the original kernel matrix.

torch.Size([263, 2])
We keep 1.90e+03/5.62e+03 = 33% of the original kernel matrix.

torch.Size([4435, 2])
We keep 1.36e+05/1.42e+06 =  9% of the original kernel matrix.

torch.Size([1539, 2])
We keep 8.26e+04/3.64e+05 = 22% of the original kernel matrix.

torch.Size([7645, 2])
We keep 4.80e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([617, 2])
We keep 6.65e+03/4.24e+04 = 15% of the original kernel matrix.

torch.Size([5831, 2])
We keep 2.59e+05/3.89e+06 =  6% of the original kernel matrix.

torch.Size([211, 2])
We keep 4.47e+03/1.19e+04 = 37% of the original kernel matrix.

torch.Size([3459, 2])
We keep 1.70e+05/2.06e+06 =  8% of the original kernel matrix.

torch.Size([1111, 2])
We keep 1.58e+04/1.21e+05 = 13% of the original kernel matrix.

torch.Size([7559, 2])
We keep 3.55e+05/6.57e+06 =  5% of the original kernel matrix.

torch.Size([871, 2])
We keep 9.59e+03/7.40e+04 = 12% of the original kernel matrix.

torch.Size([6698, 2])
We keep 2.97e+05/5.14e+06 =  5% of the original kernel matrix.

torch.Size([382, 2])
We keep 8.80e+03/3.06e+04 = 28% of the original kernel matrix.

torch.Size([4662, 2])
We keep 2.26e+05/3.31e+06 =  6% of the original kernel matrix.

torch.Size([2179, 2])
We keep 7.48e+04/8.35e+05 =  8% of the original kernel matrix.

torch.Size([9217, 2])
We keep 6.73e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([1313, 2])
We keep 2.16e+04/2.07e+05 = 10% of the original kernel matrix.

torch.Size([7744, 2])
We keep 4.33e+05/8.59e+06 =  5% of the original kernel matrix.

torch.Size([230, 2])
We keep 2.29e+03/8.65e+03 = 26% of the original kernel matrix.

torch.Size([4243, 2])
We keep 1.58e+05/1.76e+06 =  8% of the original kernel matrix.

torch.Size([332, 2])
We keep 4.51e+03/1.46e+04 = 30% of the original kernel matrix.

torch.Size([4418, 2])
We keep 1.95e+05/2.29e+06 =  8% of the original kernel matrix.

torch.Size([1758, 2])
We keep 3.29e+04/3.45e+05 =  9% of the original kernel matrix.

torch.Size([8606, 2])
We keep 4.93e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([316, 2])
We keep 1.94e+03/8.65e+03 = 22% of the original kernel matrix.

torch.Size([4637, 2])
We keep 1.51e+05/1.76e+06 =  8% of the original kernel matrix.

torch.Size([215, 2])
We keep 1.45e+03/5.62e+03 = 25% of the original kernel matrix.

torch.Size([4137, 2])
We keep 1.39e+05/1.42e+06 =  9% of the original kernel matrix.

torch.Size([392, 2])
We keep 6.43e+03/2.40e+04 = 26% of the original kernel matrix.

torch.Size([4541, 2])
We keep 2.06e+05/2.93e+06 =  7% of the original kernel matrix.

torch.Size([938, 2])
We keep 1.39e+04/1.12e+05 = 12% of the original kernel matrix.

torch.Size([6826, 2])
We keep 3.47e+05/6.33e+06 =  5% of the original kernel matrix.

torch.Size([2101, 2])
We keep 5.80e+04/6.87e+05 =  8% of the original kernel matrix.

torch.Size([9332, 2])
We keep 6.42e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([689, 2])
We keep 4.59e+03/2.72e+04 = 16% of the original kernel matrix.

torch.Size([6529, 2])
We keep 2.29e+05/3.12e+06 =  7% of the original kernel matrix.

torch.Size([226, 2])
We keep 1.05e+03/3.36e+03 = 31% of the original kernel matrix.

torch.Size([4066, 2])
We keep 1.16e+05/1.10e+06 = 10% of the original kernel matrix.

torch.Size([572, 2])
We keep 5.43e+03/3.42e+04 = 15% of the original kernel matrix.

torch.Size([5772, 2])
We keep 2.38e+05/3.49e+06 =  6% of the original kernel matrix.

torch.Size([946, 2])
We keep 1.22e+04/9.86e+04 = 12% of the original kernel matrix.

torch.Size([7140, 2])
We keep 3.25e+05/5.93e+06 =  5% of the original kernel matrix.

torch.Size([152, 2])
We keep 8.83e+02/3.02e+03 = 29% of the original kernel matrix.

torch.Size([3593, 2])
We keep 1.11e+05/1.04e+06 = 10% of the original kernel matrix.

torch.Size([1244, 2])
We keep 2.17e+04/1.87e+05 = 11% of the original kernel matrix.

torch.Size([7828, 2])
We keep 4.17e+05/8.18e+06 =  5% of the original kernel matrix.

torch.Size([9090, 2])
We keep 6.64e+06/7.47e+07 =  8% of the original kernel matrix.

torch.Size([15184, 2])
We keep 3.37e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([905, 2])
We keep 1.04e+04/8.24e+04 = 12% of the original kernel matrix.

torch.Size([6878, 2])
We keep 3.08e+05/5.42e+06 =  5% of the original kernel matrix.

torch.Size([2223, 2])
We keep 5.42e+04/6.96e+05 =  7% of the original kernel matrix.

torch.Size([9728, 2])
We keep 6.38e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([264, 2])
We keep 1.47e+03/6.56e+03 = 22% of the original kernel matrix.

torch.Size([4516, 2])
We keep 1.39e+05/1.53e+06 =  9% of the original kernel matrix.

torch.Size([1826, 2])
We keep 3.41e+04/3.81e+05 =  8% of the original kernel matrix.

torch.Size([8944, 2])
We keep 5.20e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([4402, 2])
We keep 2.46e+05/3.58e+06 =  6% of the original kernel matrix.

torch.Size([12193, 2])
We keep 1.09e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([10153, 2])
We keep 7.14e+06/8.01e+07 =  8% of the original kernel matrix.

torch.Size([16803, 2])
We keep 3.38e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([319, 2])
We keep 2.52e+03/1.06e+04 = 23% of the original kernel matrix.

torch.Size([4412, 2])
We keep 1.60e+05/1.95e+06 =  8% of the original kernel matrix.

torch.Size([4135, 2])
We keep 3.24e+05/4.01e+06 =  8% of the original kernel matrix.

torch.Size([11739, 2])
We keep 1.17e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([13737, 2])
We keep 2.65e+06/7.12e+07 =  3% of the original kernel matrix.

torch.Size([20119, 2])
We keep 3.32e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([1198, 2])
We keep 1.56e+04/1.51e+05 = 10% of the original kernel matrix.

torch.Size([7702, 2])
We keep 3.79e+05/7.33e+06 =  5% of the original kernel matrix.

torch.Size([19967, 2])
We keep 1.21e+07/2.98e+08 =  4% of the original kernel matrix.

torch.Size([23870, 2])
We keep 5.87e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([372, 2])
We keep 1.68e+03/9.02e+03 = 18% of the original kernel matrix.

torch.Size([5218, 2])
We keep 1.47e+05/1.79e+06 =  8% of the original kernel matrix.

torch.Size([1831, 2])
We keep 4.55e+04/5.21e+05 =  8% of the original kernel matrix.

torch.Size([8739, 2])
We keep 5.77e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([13253, 2])
We keep 4.48e+06/8.39e+07 =  5% of the original kernel matrix.

torch.Size([19435, 2])
We keep 3.58e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([3696, 2])
We keep 1.48e+05/2.46e+06 =  6% of the original kernel matrix.

torch.Size([11476, 2])
We keep 9.41e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([461, 2])
We keep 9.16e+03/4.16e+04 = 22% of the original kernel matrix.

torch.Size([5143, 2])
We keep 2.48e+05/3.85e+06 =  6% of the original kernel matrix.

torch.Size([3393, 2])
We keep 1.20e+05/1.61e+06 =  7% of the original kernel matrix.

torch.Size([11130, 2])
We keep 8.26e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([5357, 2])
We keep 7.19e+05/1.13e+07 =  6% of the original kernel matrix.

torch.Size([12532, 2])
We keep 1.66e+06/6.34e+07 =  2% of the original kernel matrix.

torch.Size([10028, 2])
We keep 5.21e+06/6.39e+07 =  8% of the original kernel matrix.

torch.Size([17051, 2])
We keep 3.08e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([2600, 2])
We keep 1.92e+05/1.87e+06 = 10% of the original kernel matrix.

torch.Size([9366, 2])
We keep 8.68e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([6592, 2])
We keep 9.42e+05/1.27e+07 =  7% of the original kernel matrix.

torch.Size([14216, 2])
We keep 1.77e+06/6.73e+07 =  2% of the original kernel matrix.

torch.Size([6257, 2])
We keep 9.84e+05/1.13e+07 =  8% of the original kernel matrix.

torch.Size([14489, 2])
We keep 1.63e+06/6.34e+07 =  2% of the original kernel matrix.

torch.Size([7456, 2])
We keep 1.44e+06/1.53e+07 =  9% of the original kernel matrix.

torch.Size([15598, 2])
We keep 1.90e+06/7.40e+07 =  2% of the original kernel matrix.

torch.Size([883, 2])
We keep 3.25e+04/1.85e+05 = 17% of the original kernel matrix.

torch.Size([6021, 2])
We keep 4.11e+05/8.12e+06 =  5% of the original kernel matrix.

torch.Size([16834, 2])
We keep 3.07e+07/4.04e+08 =  7% of the original kernel matrix.

torch.Size([21611, 2])
We keep 6.65e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([524, 2])
We keep 6.19e+03/3.46e+04 = 17% of the original kernel matrix.

torch.Size([5478, 2])
We keep 2.39e+05/3.51e+06 =  6% of the original kernel matrix.

torch.Size([868, 2])
We keep 1.05e+04/7.67e+04 = 13% of the original kernel matrix.

torch.Size([6807, 2])
We keep 3.13e+05/5.23e+06 =  5% of the original kernel matrix.

torch.Size([7686, 2])
We keep 8.30e+05/1.45e+07 =  5% of the original kernel matrix.

torch.Size([15340, 2])
We keep 1.83e+06/7.19e+07 =  2% of the original kernel matrix.

torch.Size([1701, 2])
We keep 8.32e+04/6.38e+05 = 13% of the original kernel matrix.

torch.Size([8115, 2])
We keep 6.20e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([75664, 2])
We keep 5.45e+07/2.67e+09 =  2% of the original kernel matrix.

torch.Size([48025, 2])
We keep 1.44e+07/9.76e+08 =  1% of the original kernel matrix.

torch.Size([14787, 2])
We keep 1.62e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([21489, 2])
We keep 2.98e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([295, 2])
We keep 1.47e+03/6.24e+03 = 23% of the original kernel matrix.

torch.Size([4763, 2])
We keep 1.42e+05/1.49e+06 =  9% of the original kernel matrix.

torch.Size([1365, 2])
We keep 2.96e+04/2.84e+05 = 10% of the original kernel matrix.

torch.Size([7899, 2])
We keep 4.70e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([3501, 2])
We keep 1.61e+05/2.08e+06 =  7% of the original kernel matrix.

torch.Size([11422, 2])
We keep 9.20e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([431, 2])
We keep 3.00e+03/1.69e+04 = 17% of the original kernel matrix.

torch.Size([5448, 2])
We keep 1.95e+05/2.46e+06 =  7% of the original kernel matrix.

torch.Size([21335, 2])
We keep 3.89e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([26125, 2])
We keep 4.60e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([5310, 2])
We keep 4.55e+06/3.86e+07 = 11% of the original kernel matrix.

torch.Size([11816, 2])
We keep 2.76e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([2143, 2])
We keep 4.48e+04/5.55e+05 =  8% of the original kernel matrix.

torch.Size([9403, 2])
We keep 5.88e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([211, 2])
We keep 7.94e+02/2.30e+03 = 34% of the original kernel matrix.

torch.Size([4373, 2])
We keep 1.05e+05/9.07e+05 = 11% of the original kernel matrix.

torch.Size([1146, 2])
We keep 2.03e+04/1.71e+05 = 11% of the original kernel matrix.

torch.Size([7483, 2])
We keep 4.11e+05/7.80e+06 =  5% of the original kernel matrix.

torch.Size([2521, 2])
We keep 3.81e+05/2.77e+06 = 13% of the original kernel matrix.

torch.Size([8725, 2])
We keep 1.02e+06/3.14e+07 =  3% of the original kernel matrix.

torch.Size([5345, 2])
We keep 4.48e+05/6.65e+06 =  6% of the original kernel matrix.

torch.Size([13065, 2])
We keep 1.40e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([1671, 2])
We keep 5.86e+04/4.83e+05 = 12% of the original kernel matrix.

torch.Size([8139, 2])
We keep 5.55e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([827, 2])
We keep 1.25e+04/8.53e+04 = 14% of the original kernel matrix.

torch.Size([6417, 2])
We keep 3.08e+05/5.52e+06 =  5% of the original kernel matrix.

torch.Size([63995, 2])
We keep 3.80e+07/2.87e+09 =  1% of the original kernel matrix.

torch.Size([44725, 2])
We keep 1.47e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([1797, 2])
We keep 3.06e+04/3.48e+05 =  8% of the original kernel matrix.

torch.Size([9027, 2])
We keep 4.97e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([5511, 2])
We keep 2.61e+05/5.33e+06 =  4% of the original kernel matrix.

torch.Size([13632, 2])
We keep 1.26e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([23119, 2])
We keep 7.00e+06/2.37e+08 =  2% of the original kernel matrix.

torch.Size([27063, 2])
We keep 5.45e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([691, 2])
We keep 6.59e+03/4.75e+04 = 13% of the original kernel matrix.

torch.Size([6302, 2])
We keep 2.70e+05/4.12e+06 =  6% of the original kernel matrix.

torch.Size([3307, 2])
We keep 1.21e+05/1.60e+06 =  7% of the original kernel matrix.

torch.Size([10983, 2])
We keep 8.38e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([29261, 2])
We keep 2.02e+07/5.11e+08 =  3% of the original kernel matrix.

torch.Size([29965, 2])
We keep 7.31e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([25408, 2])
We keep 9.55e+06/3.19e+08 =  2% of the original kernel matrix.

torch.Size([27461, 2])
We keep 6.07e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([1834, 2])
We keep 5.74e+04/5.39e+05 = 10% of the original kernel matrix.

torch.Size([8554, 2])
We keep 5.75e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([373, 2])
We keep 3.12e+03/2.02e+04 = 15% of the original kernel matrix.

torch.Size([4961, 2])
We keep 2.02e+05/2.68e+06 =  7% of the original kernel matrix.

torch.Size([854, 2])
We keep 1.16e+04/7.51e+04 = 15% of the original kernel matrix.

torch.Size([6512, 2])
We keep 3.09e+05/5.18e+06 =  5% of the original kernel matrix.

torch.Size([478, 2])
We keep 5.26e+03/2.56e+04 = 20% of the original kernel matrix.

torch.Size([5301, 2])
We keep 2.16e+05/3.02e+06 =  7% of the original kernel matrix.

torch.Size([2054, 2])
We keep 5.73e+04/7.12e+05 =  8% of the original kernel matrix.

torch.Size([9182, 2])
We keep 6.37e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([3341, 2])
We keep 1.26e+05/1.92e+06 =  6% of the original kernel matrix.

torch.Size([11071, 2])
We keep 8.85e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([5641, 2])
We keep 4.51e+05/7.62e+06 =  5% of the original kernel matrix.

torch.Size([13565, 2])
We keep 1.42e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([15827, 2])
We keep 1.71e+07/2.74e+08 =  6% of the original kernel matrix.

torch.Size([22226, 2])
We keep 4.68e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([8376, 2])
We keep 2.25e+06/2.54e+07 =  8% of the original kernel matrix.

torch.Size([16205, 2])
We keep 2.23e+06/9.53e+07 =  2% of the original kernel matrix.

torch.Size([1207, 2])
We keep 4.60e+04/2.77e+05 = 16% of the original kernel matrix.

torch.Size([7132, 2])
We keep 4.83e+05/9.94e+06 =  4% of the original kernel matrix.

torch.Size([2414, 2])
We keep 5.81e+04/7.50e+05 =  7% of the original kernel matrix.

torch.Size([10019, 2])
We keep 6.21e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([14212, 2])
We keep 3.83e+07/6.49e+08 =  5% of the original kernel matrix.

torch.Size([18526, 2])
We keep 7.99e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([3921, 2])
We keep 2.53e+05/4.18e+06 =  6% of the original kernel matrix.

torch.Size([11696, 2])
We keep 1.14e+06/3.86e+07 =  2% of the original kernel matrix.

torch.Size([1495, 2])
We keep 6.86e+04/4.41e+05 = 15% of the original kernel matrix.

torch.Size([7923, 2])
We keep 5.30e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([6410, 2])
We keep 4.77e+05/8.94e+06 =  5% of the original kernel matrix.

torch.Size([14394, 2])
We keep 1.54e+06/5.65e+07 =  2% of the original kernel matrix.

torch.Size([61791, 2])
We keep 5.87e+07/2.56e+09 =  2% of the original kernel matrix.

torch.Size([41607, 2])
We keep 1.39e+07/9.56e+08 =  1% of the original kernel matrix.

torch.Size([844, 2])
We keep 1.06e+04/8.07e+04 = 13% of the original kernel matrix.

torch.Size([6601, 2])
We keep 3.28e+05/5.36e+06 =  6% of the original kernel matrix.

torch.Size([8130, 2])
We keep 1.74e+06/2.33e+07 =  7% of the original kernel matrix.

torch.Size([15750, 2])
We keep 2.08e+06/9.11e+07 =  2% of the original kernel matrix.

torch.Size([887, 2])
We keep 1.99e+04/1.40e+05 = 14% of the original kernel matrix.

torch.Size([6562, 2])
We keep 3.84e+05/7.06e+06 =  5% of the original kernel matrix.

torch.Size([835, 2])
We keep 7.62e+04/3.12e+05 = 24% of the original kernel matrix.

torch.Size([5043, 2])
We keep 4.91e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([2940, 2])
We keep 1.72e+05/2.25e+06 =  7% of the original kernel matrix.

torch.Size([10449, 2])
We keep 8.84e+05/2.84e+07 =  3% of the original kernel matrix.

torch.Size([13225, 2])
We keep 1.96e+07/2.61e+08 =  7% of the original kernel matrix.

torch.Size([18099, 2])
We keep 5.38e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([12263, 2])
We keep 1.39e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([19655, 2])
We keep 2.85e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([1030, 2])
We keep 1.48e+04/1.10e+05 = 13% of the original kernel matrix.

torch.Size([7001, 2])
We keep 3.42e+05/6.25e+06 =  5% of the original kernel matrix.

torch.Size([17305, 2])
We keep 2.67e+06/9.55e+07 =  2% of the original kernel matrix.

torch.Size([23325, 2])
We keep 3.71e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([19299, 2])
We keep 3.01e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([24863, 2])
We keep 4.11e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([835, 2])
We keep 2.58e+04/1.32e+05 = 19% of the original kernel matrix.

torch.Size([6071, 2])
We keep 3.62e+05/6.88e+06 =  5% of the original kernel matrix.

torch.Size([1632, 2])
We keep 5.44e+04/4.42e+05 = 12% of the original kernel matrix.

torch.Size([8152, 2])
We keep 5.29e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([28783, 2])
We keep 5.90e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([29332, 2])
We keep 5.67e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([80976, 2])
We keep 4.31e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([49650, 2])
We keep 1.47e+07/9.93e+08 =  1% of the original kernel matrix.

torch.Size([4421, 2])
We keep 7.00e+05/7.94e+06 =  8% of the original kernel matrix.

torch.Size([11802, 2])
We keep 1.45e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([561, 2])
We keep 4.09e+04/1.00e+05 = 40% of the original kernel matrix.

torch.Size([4959, 2])
We keep 3.25e+05/5.99e+06 =  5% of the original kernel matrix.

torch.Size([877, 2])
We keep 8.76e+03/7.02e+04 = 12% of the original kernel matrix.

torch.Size([6939, 2])
We keep 3.01e+05/5.01e+06 =  6% of the original kernel matrix.

torch.Size([343, 2])
We keep 1.85e+03/8.28e+03 = 22% of the original kernel matrix.

torch.Size([4858, 2])
We keep 1.54e+05/1.72e+06 =  8% of the original kernel matrix.

torch.Size([6593, 2])
We keep 6.20e+05/8.97e+06 =  6% of the original kernel matrix.

torch.Size([14517, 2])
We keep 1.53e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([939, 2])
We keep 1.17e+04/9.42e+04 = 12% of the original kernel matrix.

torch.Size([7066, 2])
We keep 3.18e+05/5.80e+06 =  5% of the original kernel matrix.

torch.Size([682, 2])
We keep 9.44e+03/5.81e+04 = 16% of the original kernel matrix.

torch.Size([6042, 2])
We keep 2.79e+05/4.55e+06 =  6% of the original kernel matrix.

torch.Size([14093, 2])
We keep 2.00e+06/5.08e+07 =  3% of the original kernel matrix.

torch.Size([20974, 2])
We keep 2.89e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([26363, 2])
We keep 5.72e+06/2.73e+08 =  2% of the original kernel matrix.

torch.Size([29648, 2])
We keep 5.80e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([1847, 2])
We keep 3.09e+05/1.95e+06 = 15% of the original kernel matrix.

torch.Size([7127, 2])
We keep 8.66e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([757, 2])
We keep 1.84e+04/9.99e+04 = 18% of the original kernel matrix.

torch.Size([6154, 2])
We keep 3.38e+05/5.97e+06 =  5% of the original kernel matrix.

torch.Size([1671, 2])
We keep 6.03e+04/5.18e+05 = 11% of the original kernel matrix.

torch.Size([8211, 2])
We keep 5.63e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([3387, 2])
We keep 1.25e+05/1.89e+06 =  6% of the original kernel matrix.

torch.Size([11142, 2])
We keep 8.88e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([3480, 2])
We keep 2.76e+05/2.29e+06 = 12% of the original kernel matrix.

torch.Size([11143, 2])
We keep 9.26e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([4010, 2])
We keep 1.72e+05/2.80e+06 =  6% of the original kernel matrix.

torch.Size([11846, 2])
We keep 1.01e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([611, 2])
We keep 5.37e+03/3.57e+04 = 15% of the original kernel matrix.

torch.Size([6121, 2])
We keep 2.51e+05/3.57e+06 =  7% of the original kernel matrix.

torch.Size([41883, 2])
We keep 3.91e+07/1.27e+09 =  3% of the original kernel matrix.

torch.Size([33872, 2])
We keep 1.06e+07/6.74e+08 =  1% of the original kernel matrix.

torch.Size([1751, 2])
We keep 4.22e+04/4.90e+05 =  8% of the original kernel matrix.

torch.Size([8634, 2])
We keep 5.60e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([5159, 2])
We keep 3.90e+05/5.95e+06 =  6% of the original kernel matrix.

torch.Size([13120, 2])
We keep 1.33e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([6762, 2])
We keep 2.33e+06/2.54e+07 =  9% of the original kernel matrix.

torch.Size([14280, 2])
We keep 2.26e+06/9.52e+07 =  2% of the original kernel matrix.

torch.Size([463, 2])
We keep 4.80e+04/1.59e+05 = 30% of the original kernel matrix.

torch.Size([4029, 2])
We keep 3.82e+05/7.54e+06 =  5% of the original kernel matrix.

torch.Size([12107, 2])
We keep 1.70e+06/4.36e+07 =  3% of the original kernel matrix.

torch.Size([19303, 2])
We keep 2.72e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([2354, 2])
We keep 7.59e+04/8.89e+05 =  8% of the original kernel matrix.

torch.Size([9635, 2])
We keep 6.94e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([10154, 2])
We keep 9.80e+05/2.35e+07 =  4% of the original kernel matrix.

torch.Size([17749, 2])
We keep 2.21e+06/9.16e+07 =  2% of the original kernel matrix.

torch.Size([395, 2])
We keep 3.33e+03/1.88e+04 = 17% of the original kernel matrix.

torch.Size([5008, 2])
We keep 1.88e+05/2.59e+06 =  7% of the original kernel matrix.

torch.Size([714, 2])
We keep 8.34e+03/5.81e+04 = 14% of the original kernel matrix.

torch.Size([6203, 2])
We keep 2.78e+05/4.55e+06 =  6% of the original kernel matrix.

torch.Size([1344, 2])
We keep 2.46e+04/2.62e+05 =  9% of the original kernel matrix.

torch.Size([7887, 2])
We keep 4.69e+05/9.67e+06 =  4% of the original kernel matrix.

torch.Size([2426, 2])
We keep 1.07e+05/1.21e+06 =  8% of the original kernel matrix.

torch.Size([9788, 2])
We keep 6.67e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([1967, 2])
We keep 1.06e+05/8.67e+05 = 12% of the original kernel matrix.

torch.Size([8590, 2])
We keep 6.84e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([3631, 2])
We keep 5.93e+05/4.75e+06 = 12% of the original kernel matrix.

torch.Size([10797, 2])
We keep 1.15e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([1078, 2])
We keep 1.55e+04/1.33e+05 = 11% of the original kernel matrix.

torch.Size([7282, 2])
We keep 3.76e+05/6.89e+06 =  5% of the original kernel matrix.

torch.Size([1404, 2])
We keep 3.41e+04/2.86e+05 = 11% of the original kernel matrix.

torch.Size([7885, 2])
We keep 4.74e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([62454, 2])
We keep 3.47e+07/1.53e+09 =  2% of the original kernel matrix.

torch.Size([43595, 2])
We keep 1.14e+07/7.39e+08 =  1% of the original kernel matrix.

torch.Size([4898, 2])
We keep 4.03e+05/5.89e+06 =  6% of the original kernel matrix.

torch.Size([12605, 2])
We keep 1.30e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([6770, 2])
We keep 1.25e+06/1.59e+07 =  7% of the original kernel matrix.

torch.Size([14516, 2])
We keep 1.91e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([31564, 2])
We keep 1.45e+07/4.70e+08 =  3% of the original kernel matrix.

torch.Size([31780, 2])
We keep 7.09e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([18527, 2])
We keep 4.90e+06/1.56e+08 =  3% of the original kernel matrix.

torch.Size([23098, 2])
We keep 4.50e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([2266, 2])
We keep 5.44e+04/7.28e+05 =  7% of the original kernel matrix.

torch.Size([9529, 2])
We keep 6.35e+05/1.61e+07 =  3% of the original kernel matrix.

torch.Size([834, 2])
We keep 1.41e+04/1.00e+05 = 14% of the original kernel matrix.

torch.Size([6524, 2])
We keep 3.43e+05/5.99e+06 =  5% of the original kernel matrix.

torch.Size([3974, 2])
We keep 7.83e+05/5.89e+06 = 13% of the original kernel matrix.

torch.Size([11372, 2])
We keep 1.36e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([138, 2])
We keep 1.24e+03/3.02e+03 = 41% of the original kernel matrix.

torch.Size([3346, 2])
We keep 1.17e+05/1.04e+06 = 11% of the original kernel matrix.

torch.Size([1065, 2])
We keep 1.56e+04/1.37e+05 = 11% of the original kernel matrix.

torch.Size([7256, 2])
We keep 3.75e+05/6.99e+06 =  5% of the original kernel matrix.

torch.Size([1846, 2])
We keep 8.20e+04/8.34e+05 =  9% of the original kernel matrix.

torch.Size([8224, 2])
We keep 6.60e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([826, 2])
We keep 1.65e+04/1.06e+05 = 15% of the original kernel matrix.

torch.Size([6227, 2])
We keep 3.46e+05/6.16e+06 =  5% of the original kernel matrix.

torch.Size([1074, 2])
We keep 1.40e+04/1.10e+05 = 12% of the original kernel matrix.

torch.Size([7417, 2])
We keep 3.52e+05/6.27e+06 =  5% of the original kernel matrix.

torch.Size([468, 2])
We keep 5.07e+03/2.86e+04 = 17% of the original kernel matrix.

torch.Size([5293, 2])
We keep 2.26e+05/3.19e+06 =  7% of the original kernel matrix.

torch.Size([1188, 2])
We keep 1.85e+04/1.62e+05 = 11% of the original kernel matrix.

torch.Size([7516, 2])
We keep 3.86e+05/7.59e+06 =  5% of the original kernel matrix.

torch.Size([63969, 2])
We keep 4.42e+07/2.09e+09 =  2% of the original kernel matrix.

torch.Size([42531, 2])
We keep 1.31e+07/8.64e+08 =  1% of the original kernel matrix.

torch.Size([2555, 2])
We keep 3.23e+05/2.97e+06 = 10% of the original kernel matrix.

torch.Size([9019, 2])
We keep 9.92e+05/3.26e+07 =  3% of the original kernel matrix.

torch.Size([7174, 2])
We keep 8.37e+05/1.16e+07 =  7% of the original kernel matrix.

torch.Size([14928, 2])
We keep 1.66e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([5113, 2])
We keep 1.63e+06/2.03e+07 =  7% of the original kernel matrix.

torch.Size([11510, 2])
We keep 2.03e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([461, 2])
We keep 3.67e+03/1.74e+04 = 21% of the original kernel matrix.

torch.Size([5623, 2])
We keep 1.89e+05/2.49e+06 =  7% of the original kernel matrix.

torch.Size([1005, 2])
We keep 1.54e+04/1.25e+05 = 12% of the original kernel matrix.

torch.Size([7253, 2])
We keep 3.56e+05/6.69e+06 =  5% of the original kernel matrix.

torch.Size([92690, 2])
We keep 1.13e+08/3.74e+09 =  3% of the original kernel matrix.

torch.Size([53193, 2])
We keep 1.46e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([71169, 2])
We keep 2.76e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([46931, 2])
We keep 1.23e+07/8.31e+08 =  1% of the original kernel matrix.

torch.Size([3380, 2])
We keep 1.95e+05/2.13e+06 =  9% of the original kernel matrix.

torch.Size([11014, 2])
We keep 9.10e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([3462, 2])
We keep 2.05e+05/2.57e+06 =  7% of the original kernel matrix.

torch.Size([10936, 2])
We keep 9.76e+05/3.03e+07 =  3% of the original kernel matrix.

torch.Size([712, 2])
We keep 6.54e+03/4.45e+04 = 14% of the original kernel matrix.

torch.Size([6427, 2])
We keep 2.60e+05/3.99e+06 =  6% of the original kernel matrix.

torch.Size([6616, 2])
We keep 2.73e+06/3.52e+07 =  7% of the original kernel matrix.

torch.Size([12379, 2])
We keep 2.48e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([11130, 2])
We keep 1.40e+07/1.06e+08 = 13% of the original kernel matrix.

torch.Size([17346, 2])
We keep 3.82e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([19594, 2])
We keep 1.04e+07/2.24e+08 =  4% of the original kernel matrix.

torch.Size([24306, 2])
We keep 5.14e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([3128, 2])
We keep 1.57e+05/1.70e+06 =  9% of the original kernel matrix.

torch.Size([10650, 2])
We keep 8.64e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([12731, 2])
We keep 2.11e+06/5.47e+07 =  3% of the original kernel matrix.

torch.Size([19731, 2])
We keep 3.04e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([14091, 2])
We keep 3.77e+06/7.39e+07 =  5% of the original kernel matrix.

torch.Size([20567, 2])
We keep 3.37e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([2238, 2])
We keep 7.32e+04/8.84e+05 =  8% of the original kernel matrix.

torch.Size([9215, 2])
We keep 6.77e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([941, 2])
We keep 1.25e+04/9.61e+04 = 13% of the original kernel matrix.

torch.Size([6942, 2])
We keep 3.27e+05/5.86e+06 =  5% of the original kernel matrix.

torch.Size([3286, 2])
We keep 1.29e+05/1.87e+06 =  6% of the original kernel matrix.

torch.Size([11242, 2])
We keep 9.05e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([5246, 2])
We keep 3.10e+05/5.26e+06 =  5% of the original kernel matrix.

torch.Size([13193, 2])
We keep 1.25e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([863, 2])
We keep 1.09e+04/7.84e+04 = 13% of the original kernel matrix.

torch.Size([6863, 2])
We keep 3.02e+05/5.29e+06 =  5% of the original kernel matrix.

torch.Size([17798, 2])
We keep 4.12e+06/1.24e+08 =  3% of the original kernel matrix.

torch.Size([23586, 2])
We keep 4.20e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([4823, 2])
We keep 4.44e+05/5.73e+06 =  7% of the original kernel matrix.

torch.Size([12201, 2])
We keep 1.31e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([369, 2])
We keep 1.67e+03/9.02e+03 = 18% of the original kernel matrix.

torch.Size([5202, 2])
We keep 1.55e+05/1.79e+06 =  8% of the original kernel matrix.

torch.Size([2355, 2])
We keep 7.20e+04/9.64e+05 =  7% of the original kernel matrix.

torch.Size([9753, 2])
We keep 6.68e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([5026, 2])
We keep 2.65e+05/4.77e+06 =  5% of the original kernel matrix.

torch.Size([13124, 2])
We keep 1.24e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([271, 2])
We keep 2.01e+03/7.92e+03 = 25% of the original kernel matrix.

torch.Size([4464, 2])
We keep 1.60e+05/1.68e+06 =  9% of the original kernel matrix.

torch.Size([19358, 2])
We keep 1.32e+07/2.24e+08 =  5% of the original kernel matrix.

torch.Size([24424, 2])
We keep 5.28e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([25801, 2])
We keep 7.47e+06/2.72e+08 =  2% of the original kernel matrix.

torch.Size([29120, 2])
We keep 5.78e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([19178, 2])
We keep 5.30e+06/1.34e+08 =  3% of the original kernel matrix.

torch.Size([24691, 2])
We keep 4.28e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([18526, 2])
We keep 5.50e+06/1.38e+08 =  3% of the original kernel matrix.

torch.Size([23831, 2])
We keep 4.30e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([21229, 2])
We keep 5.34e+06/1.77e+08 =  3% of the original kernel matrix.

torch.Size([25248, 2])
We keep 4.68e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([1785, 2])
We keep 8.02e+04/6.94e+05 = 11% of the original kernel matrix.

torch.Size([8387, 2])
We keep 6.44e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([2016, 2])
We keep 7.38e+04/7.89e+05 =  9% of the original kernel matrix.

torch.Size([8659, 2])
We keep 6.57e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([5482, 2])
We keep 3.39e+05/6.39e+06 =  5% of the original kernel matrix.

torch.Size([13450, 2])
We keep 1.34e+06/4.78e+07 =  2% of the original kernel matrix.

torch.Size([16569, 2])
We keep 2.10e+06/7.24e+07 =  2% of the original kernel matrix.

torch.Size([22706, 2])
We keep 3.18e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([422, 2])
We keep 3.68e+03/1.88e+04 = 19% of the original kernel matrix.

torch.Size([5277, 2])
We keep 1.98e+05/2.59e+06 =  7% of the original kernel matrix.

torch.Size([5902, 2])
We keep 7.45e+05/9.74e+06 =  7% of the original kernel matrix.

torch.Size([14041, 2])
We keep 1.40e+06/5.90e+07 =  2% of the original kernel matrix.

torch.Size([31797, 2])
We keep 3.20e+07/9.11e+08 =  3% of the original kernel matrix.

torch.Size([30618, 2])
We keep 8.26e+06/5.70e+08 =  1% of the original kernel matrix.

torch.Size([4630, 2])
We keep 3.09e+05/4.91e+06 =  6% of the original kernel matrix.

torch.Size([12653, 2])
We keep 1.23e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([4348, 2])
We keep 6.89e+05/8.00e+06 =  8% of the original kernel matrix.

torch.Size([11665, 2])
We keep 1.40e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([6179, 2])
We keep 5.48e+07/3.30e+08 = 16% of the original kernel matrix.

torch.Size([9115, 2])
We keep 6.24e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([2134, 2])
We keep 5.24e+04/6.72e+05 =  7% of the original kernel matrix.

torch.Size([9498, 2])
We keep 6.39e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([186, 2])
We keep 1.64e+03/5.48e+03 = 29% of the original kernel matrix.

torch.Size([3739, 2])
We keep 1.32e+05/1.40e+06 =  9% of the original kernel matrix.

torch.Size([5404, 2])
We keep 9.92e+05/1.28e+07 =  7% of the original kernel matrix.

torch.Size([12175, 2])
We keep 1.72e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([5173, 2])
We keep 2.39e+05/4.62e+06 =  5% of the original kernel matrix.

torch.Size([13185, 2])
We keep 1.20e+06/4.06e+07 =  2% of the original kernel matrix.

torch.Size([3217, 2])
We keep 1.11e+05/1.65e+06 =  6% of the original kernel matrix.

torch.Size([11030, 2])
We keep 8.38e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([115097, 2])
We keep 1.06e+08/6.40e+09 =  1% of the original kernel matrix.

torch.Size([58488, 2])
We keep 2.06e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([33096, 2])
We keep 3.36e+07/7.26e+08 =  4% of the original kernel matrix.

torch.Size([31222, 2])
We keep 8.42e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([16555, 2])
We keep 4.34e+06/1.13e+08 =  3% of the original kernel matrix.

torch.Size([22528, 2])
We keep 4.01e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([4720, 2])
We keep 3.12e+05/6.72e+06 =  4% of the original kernel matrix.

torch.Size([12725, 2])
We keep 1.38e+06/4.90e+07 =  2% of the original kernel matrix.

torch.Size([13971, 2])
We keep 2.63e+06/6.49e+07 =  4% of the original kernel matrix.

torch.Size([20636, 2])
We keep 3.14e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([374, 2])
We keep 3.07e+03/1.64e+04 = 18% of the original kernel matrix.

torch.Size([5106, 2])
We keep 2.04e+05/2.42e+06 =  8% of the original kernel matrix.

torch.Size([852, 2])
We keep 1.45e+04/1.01e+05 = 14% of the original kernel matrix.

torch.Size([6336, 2])
We keep 3.34e+05/6.01e+06 =  5% of the original kernel matrix.

torch.Size([79472, 2])
We keep 3.76e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([49235, 2])
We keep 1.34e+07/8.93e+08 =  1% of the original kernel matrix.

torch.Size([949, 2])
We keep 1.37e+04/1.21e+05 = 11% of the original kernel matrix.

torch.Size([7033, 2])
We keep 3.57e+05/6.57e+06 =  5% of the original kernel matrix.

torch.Size([654, 2])
We keep 9.16e+03/5.34e+04 = 17% of the original kernel matrix.

torch.Size([5838, 2])
We keep 2.72e+05/4.36e+06 =  6% of the original kernel matrix.

torch.Size([3377, 2])
We keep 1.87e+05/2.47e+06 =  7% of the original kernel matrix.

torch.Size([11010, 2])
We keep 9.79e+05/2.97e+07 =  3% of the original kernel matrix.

torch.Size([9327, 2])
We keep 2.54e+06/4.03e+07 =  6% of the original kernel matrix.

torch.Size([16456, 2])
We keep 2.70e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([7667, 2])
We keep 1.59e+06/1.78e+07 =  8% of the original kernel matrix.

torch.Size([14923, 2])
We keep 1.98e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([11703, 2])
We keep 1.55e+06/3.89e+07 =  3% of the original kernel matrix.

torch.Size([19009, 2])
We keep 2.61e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([859, 2])
We keep 2.29e+04/1.76e+05 = 13% of the original kernel matrix.

torch.Size([6249, 2])
We keep 3.80e+05/7.93e+06 =  4% of the original kernel matrix.

torch.Size([2987, 2])
We keep 8.31e+05/6.26e+06 = 13% of the original kernel matrix.

torch.Size([8859, 2])
We keep 1.38e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([1299, 2])
We keep 2.61e+04/2.20e+05 = 11% of the original kernel matrix.

torch.Size([7682, 2])
We keep 4.33e+05/8.86e+06 =  4% of the original kernel matrix.

torch.Size([3700, 2])
We keep 4.63e+05/4.10e+06 = 11% of the original kernel matrix.

torch.Size([11108, 2])
We keep 1.20e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([6869, 2])
We keep 4.88e+05/9.53e+06 =  5% of the original kernel matrix.

torch.Size([14745, 2])
We keep 1.59e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([1701, 2])
We keep 2.01e+05/1.00e+06 = 20% of the original kernel matrix.

torch.Size([7625, 2])
We keep 7.16e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([444, 2])
We keep 4.41e+03/2.59e+04 = 17% of the original kernel matrix.

torch.Size([5273, 2])
We keep 2.17e+05/3.04e+06 =  7% of the original kernel matrix.

torch.Size([8669, 2])
We keep 9.64e+05/1.86e+07 =  5% of the original kernel matrix.

torch.Size([16378, 2])
We keep 1.98e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([443, 2])
We keep 2.98e+03/1.46e+04 = 20% of the original kernel matrix.

torch.Size([5449, 2])
We keep 1.92e+05/2.29e+06 =  8% of the original kernel matrix.

torch.Size([8013, 2])
We keep 7.20e+05/1.42e+07 =  5% of the original kernel matrix.

torch.Size([15939, 2])
We keep 1.79e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([820, 2])
We keep 3.88e+04/2.14e+05 = 18% of the original kernel matrix.

torch.Size([5738, 2])
We keep 4.31e+05/8.75e+06 =  4% of the original kernel matrix.

torch.Size([2163, 2])
We keep 6.13e+04/7.06e+05 =  8% of the original kernel matrix.

torch.Size([9510, 2])
We keep 6.10e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([908, 2])
We keep 2.07e+04/1.30e+05 = 15% of the original kernel matrix.

torch.Size([6804, 2])
We keep 3.76e+05/6.80e+06 =  5% of the original kernel matrix.

torch.Size([2368, 2])
We keep 1.10e+05/1.18e+06 =  9% of the original kernel matrix.

torch.Size([9407, 2])
We keep 7.55e+05/2.05e+07 =  3% of the original kernel matrix.

torch.Size([994, 2])
We keep 1.67e+04/1.30e+05 = 12% of the original kernel matrix.

torch.Size([7003, 2])
We keep 3.56e+05/6.82e+06 =  5% of the original kernel matrix.

torch.Size([6561, 2])
We keep 4.73e+05/9.82e+06 =  4% of the original kernel matrix.

torch.Size([14567, 2])
We keep 1.59e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([21765, 2])
We keep 5.23e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([26117, 2])
We keep 4.89e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([10319, 2])
We keep 1.39e+06/3.05e+07 =  4% of the original kernel matrix.

torch.Size([17633, 2])
We keep 2.36e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([1892, 2])
We keep 6.97e+04/6.08e+05 = 11% of the original kernel matrix.

torch.Size([8807, 2])
We keep 6.01e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([701, 2])
We keep 1.63e+04/1.10e+05 = 14% of the original kernel matrix.

torch.Size([5543, 2])
We keep 3.56e+05/6.25e+06 =  5% of the original kernel matrix.

torch.Size([41954, 2])
We keep 2.01e+07/8.59e+08 =  2% of the original kernel matrix.

torch.Size([35446, 2])
We keep 9.08e+06/5.54e+08 =  1% of the original kernel matrix.

torch.Size([975, 2])
We keep 1.46e+04/1.22e+05 = 11% of the original kernel matrix.

torch.Size([6952, 2])
We keep 3.58e+05/6.59e+06 =  5% of the original kernel matrix.

torch.Size([130107, 2])
We keep 8.85e+07/7.06e+09 =  1% of the original kernel matrix.

torch.Size([64291, 2])
We keep 2.16e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([145, 2])
We keep 7.27e+02/2.40e+03 = 30% of the original kernel matrix.

torch.Size([3605, 2])
We keep 1.02e+05/9.26e+05 = 11% of the original kernel matrix.

torch.Size([5324, 2])
We keep 3.18e+05/5.76e+06 =  5% of the original kernel matrix.

torch.Size([13086, 2])
We keep 1.31e+06/4.54e+07 =  2% of the original kernel matrix.

torch.Size([3256, 2])
We keep 9.80e+04/1.54e+06 =  6% of the original kernel matrix.

torch.Size([11079, 2])
We keep 8.23e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([1135, 2])
We keep 1.68e+04/1.43e+05 = 11% of the original kernel matrix.

torch.Size([7348, 2])
We keep 3.58e+05/7.14e+06 =  5% of the original kernel matrix.

torch.Size([1717, 2])
We keep 6.18e+04/6.05e+05 = 10% of the original kernel matrix.

torch.Size([8244, 2])
We keep 5.94e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([4154, 2])
We keep 6.45e+05/8.16e+06 =  7% of the original kernel matrix.

torch.Size([10981, 2])
We keep 1.47e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([86702, 2])
We keep 1.09e+08/3.37e+09 =  3% of the original kernel matrix.

torch.Size([50819, 2])
We keep 1.58e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([1732, 2])
We keep 3.14e+04/3.60e+05 =  8% of the original kernel matrix.

torch.Size([8748, 2])
We keep 5.10e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([4279, 2])
We keep 4.25e+05/4.96e+06 =  8% of the original kernel matrix.

torch.Size([11350, 2])
We keep 1.25e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([586, 2])
We keep 9.87e+03/4.71e+04 = 20% of the original kernel matrix.

torch.Size([5636, 2])
We keep 2.68e+05/4.10e+06 =  6% of the original kernel matrix.

torch.Size([1751, 2])
We keep 3.57e+04/3.81e+05 =  9% of the original kernel matrix.

torch.Size([8731, 2])
We keep 5.25e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([2493, 2])
We keep 1.45e+05/1.31e+06 = 11% of the original kernel matrix.

torch.Size([9612, 2])
We keep 7.71e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([2244, 2])
We keep 8.34e+04/9.27e+05 =  8% of the original kernel matrix.

torch.Size([9566, 2])
We keep 6.90e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([1283, 2])
We keep 4.09e+04/3.50e+05 = 11% of the original kernel matrix.

torch.Size([7348, 2])
We keep 5.11e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([2345, 2])
We keep 8.79e+04/1.14e+06 =  7% of the original kernel matrix.

torch.Size([9367, 2])
We keep 7.21e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([5024, 2])
We keep 2.39e+05/4.41e+06 =  5% of the original kernel matrix.

torch.Size([12949, 2])
We keep 1.17e+06/3.97e+07 =  2% of the original kernel matrix.

torch.Size([327, 2])
We keep 1.85e+03/9.02e+03 = 20% of the original kernel matrix.

torch.Size([4970, 2])
We keep 1.63e+05/1.79e+06 =  9% of the original kernel matrix.

torch.Size([1328, 2])
We keep 5.01e+04/3.46e+05 = 14% of the original kernel matrix.

torch.Size([7325, 2])
We keep 4.97e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([12923, 2])
We keep 2.74e+07/3.14e+08 =  8% of the original kernel matrix.

torch.Size([18900, 2])
We keep 6.06e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([1584, 2])
We keep 9.97e+04/5.26e+05 = 18% of the original kernel matrix.

torch.Size([7986, 2])
We keep 5.65e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([13153, 2])
We keep 1.52e+06/4.09e+07 =  3% of the original kernel matrix.

torch.Size([20070, 2])
We keep 2.64e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([16481, 2])
We keep 9.24e+06/1.62e+08 =  5% of the original kernel matrix.

torch.Size([21988, 2])
We keep 4.51e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([1794, 2])
We keep 6.25e+04/5.78e+05 = 10% of the original kernel matrix.

torch.Size([8316, 2])
We keep 5.77e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([595, 2])
We keep 4.97e+03/3.35e+04 = 14% of the original kernel matrix.

torch.Size([5979, 2])
We keep 2.35e+05/3.46e+06 =  6% of the original kernel matrix.

torch.Size([460, 2])
We keep 5.81e+03/2.96e+04 = 19% of the original kernel matrix.

torch.Size([4736, 2])
We keep 2.22e+05/3.25e+06 =  6% of the original kernel matrix.

torch.Size([31351, 2])
We keep 9.20e+06/3.89e+08 =  2% of the original kernel matrix.

torch.Size([31780, 2])
We keep 6.30e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([2687, 2])
We keep 8.15e+04/1.20e+06 =  6% of the original kernel matrix.

torch.Size([10245, 2])
We keep 7.48e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([3493, 2])
We keep 2.00e+05/2.44e+06 =  8% of the original kernel matrix.

torch.Size([11188, 2])
We keep 9.77e+05/2.95e+07 =  3% of the original kernel matrix.

torch.Size([4031, 2])
We keep 3.86e+05/5.02e+06 =  7% of the original kernel matrix.

torch.Size([10921, 2])
We keep 1.24e+06/4.23e+07 =  2% of the original kernel matrix.

torch.Size([12074, 2])
We keep 6.65e+06/1.19e+08 =  5% of the original kernel matrix.

torch.Size([18259, 2])
We keep 4.21e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([1110, 2])
We keep 1.77e+04/1.57e+05 = 11% of the original kernel matrix.

torch.Size([7317, 2])
We keep 3.85e+05/7.48e+06 =  5% of the original kernel matrix.

torch.Size([1402, 2])
We keep 3.13e+04/2.80e+05 = 11% of the original kernel matrix.

torch.Size([7943, 2])
We keep 4.59e+05/9.99e+06 =  4% of the original kernel matrix.

torch.Size([53589, 2])
We keep 1.29e+08/2.23e+09 =  5% of the original kernel matrix.

torch.Size([39046, 2])
We keep 1.33e+07/8.91e+08 =  1% of the original kernel matrix.

torch.Size([771, 2])
We keep 9.86e+03/7.29e+04 = 13% of the original kernel matrix.

torch.Size([6383, 2])
We keep 2.95e+05/5.10e+06 =  5% of the original kernel matrix.

torch.Size([1379, 2])
We keep 4.48e+04/3.56e+05 = 12% of the original kernel matrix.

torch.Size([7523, 2])
We keep 5.18e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([1536, 2])
We keep 4.79e+04/4.82e+05 =  9% of the original kernel matrix.

torch.Size([7999, 2])
We keep 5.68e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([7858, 2])
We keep 4.27e+06/2.14e+07 = 19% of the original kernel matrix.

torch.Size([15850, 2])
We keep 2.09e+06/8.75e+07 =  2% of the original kernel matrix.

torch.Size([17073, 2])
We keep 5.96e+06/1.25e+08 =  4% of the original kernel matrix.

torch.Size([22981, 2])
We keep 4.20e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([2489, 2])
We keep 4.96e+05/2.89e+06 = 17% of the original kernel matrix.

torch.Size([8716, 2])
We keep 1.02e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([8093, 2])
We keep 5.51e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([16015, 2])
We keep 1.73e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([1314, 2])
We keep 3.84e+04/3.09e+05 = 12% of the original kernel matrix.

torch.Size([7711, 2])
We keep 4.86e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([17005, 2])
We keep 2.70e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([22990, 2])
We keep 3.71e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([1099, 2])
We keep 4.24e+04/2.68e+05 = 15% of the original kernel matrix.

torch.Size([6861, 2])
We keep 4.51e+05/9.78e+06 =  4% of the original kernel matrix.

torch.Size([1711, 2])
We keep 3.97e+04/4.12e+05 =  9% of the original kernel matrix.

torch.Size([8641, 2])
We keep 5.52e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([6954, 2])
We keep 5.01e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([14833, 2])
We keep 1.58e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([1272, 2])
We keep 2.59e+04/2.50e+05 = 10% of the original kernel matrix.

torch.Size([7723, 2])
We keep 4.34e+05/9.44e+06 =  4% of the original kernel matrix.

torch.Size([8632, 2])
We keep 2.78e+06/3.84e+07 =  7% of the original kernel matrix.

torch.Size([15749, 2])
We keep 2.67e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([711, 2])
We keep 6.58e+03/4.49e+04 = 14% of the original kernel matrix.

torch.Size([6487, 2])
We keep 2.59e+05/4.00e+06 =  6% of the original kernel matrix.

torch.Size([146, 2])
We keep 8.90e+02/2.50e+03 = 35% of the original kernel matrix.

torch.Size([3076, 2])
We keep 1.02e+05/9.44e+05 = 10% of the original kernel matrix.

torch.Size([2267, 2])
We keep 5.70e+04/7.36e+05 =  7% of the original kernel matrix.

torch.Size([9399, 2])
We keep 6.38e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([620, 2])
We keep 4.01e+04/1.79e+05 = 22% of the original kernel matrix.

torch.Size([4999, 2])
We keep 3.44e+05/7.99e+06 =  4% of the original kernel matrix.

torch.Size([17146, 2])
We keep 8.75e+06/2.25e+08 =  3% of the original kernel matrix.

torch.Size([21460, 2])
We keep 5.33e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([5456, 2])
We keep 3.81e+06/2.99e+07 = 12% of the original kernel matrix.

torch.Size([11589, 2])
We keep 2.26e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([3950, 2])
We keep 1.93e+05/3.21e+06 =  6% of the original kernel matrix.

torch.Size([11503, 2])
We keep 1.02e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([483, 2])
We keep 6.56e+03/3.28e+04 = 20% of the original kernel matrix.

torch.Size([5329, 2])
We keep 2.28e+05/3.42e+06 =  6% of the original kernel matrix.

torch.Size([6135, 2])
We keep 6.07e+05/9.43e+06 =  6% of the original kernel matrix.

torch.Size([13856, 2])
We keep 1.58e+06/5.80e+07 =  2% of the original kernel matrix.

torch.Size([573, 2])
We keep 6.88e+03/4.12e+04 = 16% of the original kernel matrix.

torch.Size([5813, 2])
We keep 2.61e+05/3.83e+06 =  6% of the original kernel matrix.

torch.Size([1487, 2])
We keep 2.79e+04/3.08e+05 =  9% of the original kernel matrix.

torch.Size([8147, 2])
We keep 4.77e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([3234, 2])
We keep 1.15e+05/1.64e+06 =  6% of the original kernel matrix.

torch.Size([10961, 2])
We keep 8.48e+05/2.42e+07 =  3% of the original kernel matrix.

torch.Size([32570, 2])
We keep 1.15e+07/4.64e+08 =  2% of the original kernel matrix.

torch.Size([31887, 2])
We keep 6.94e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([1880, 2])
We keep 4.28e+04/5.03e+05 =  8% of the original kernel matrix.

torch.Size([8856, 2])
We keep 5.70e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([372, 2])
We keep 2.12e+03/9.41e+03 = 22% of the original kernel matrix.

torch.Size([5271, 2])
We keep 1.69e+05/1.83e+06 =  9% of the original kernel matrix.

torch.Size([360, 2])
We keep 5.58e+03/1.96e+04 = 28% of the original kernel matrix.

torch.Size([4517, 2])
We keep 1.94e+05/2.64e+06 =  7% of the original kernel matrix.

torch.Size([84123, 2])
We keep 6.54e+07/3.41e+09 =  1% of the original kernel matrix.

torch.Size([50309, 2])
We keep 1.61e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([99007, 2])
We keep 9.82e+07/3.98e+09 =  2% of the original kernel matrix.

torch.Size([55250, 2])
We keep 1.68e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([963, 2])
We keep 2.35e+04/1.22e+05 = 19% of the original kernel matrix.

torch.Size([6667, 2])
We keep 3.48e+05/6.61e+06 =  5% of the original kernel matrix.

torch.Size([1823, 2])
We keep 4.24e+04/4.83e+05 =  8% of the original kernel matrix.

torch.Size([8819, 2])
We keep 5.62e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([1085, 2])
We keep 2.38e+04/1.67e+05 = 14% of the original kernel matrix.

torch.Size([7136, 2])
We keep 4.01e+05/7.73e+06 =  5% of the original kernel matrix.

torch.Size([15757, 2])
We keep 2.07e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([22346, 2])
We keep 3.46e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([1114, 2])
We keep 1.61e+04/1.45e+05 = 11% of the original kernel matrix.

torch.Size([7318, 2])
We keep 3.88e+05/7.20e+06 =  5% of the original kernel matrix.

torch.Size([3224, 2])
We keep 2.12e+05/2.81e+06 =  7% of the original kernel matrix.

torch.Size([10672, 2])
We keep 1.00e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([691, 2])
We keep 5.99e+03/4.28e+04 = 13% of the original kernel matrix.

torch.Size([6325, 2])
We keep 2.56e+05/3.91e+06 =  6% of the original kernel matrix.

torch.Size([10066, 2])
We keep 2.48e+07/1.61e+08 = 15% of the original kernel matrix.

torch.Size([16457, 2])
We keep 4.58e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([2554, 2])
We keep 8.00e+05/3.12e+06 = 25% of the original kernel matrix.

torch.Size([8972, 2])
We keep 1.03e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([40232, 2])
We keep 1.41e+07/5.98e+08 =  2% of the original kernel matrix.

torch.Size([35851, 2])
We keep 7.71e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([1157, 2])
We keep 2.17e+04/1.69e+05 = 12% of the original kernel matrix.

torch.Size([7455, 2])
We keep 4.05e+05/7.76e+06 =  5% of the original kernel matrix.

torch.Size([2260, 2])
We keep 5.82e+04/7.12e+05 =  8% of the original kernel matrix.

torch.Size([9575, 2])
We keep 6.37e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([355, 2])
We keep 2.32e+03/1.12e+04 = 20% of the original kernel matrix.

torch.Size([5063, 2])
We keep 1.73e+05/2.00e+06 =  8% of the original kernel matrix.

torch.Size([6663, 2])
We keep 7.37e+05/1.14e+07 =  6% of the original kernel matrix.

torch.Size([14325, 2])
We keep 1.67e+06/6.37e+07 =  2% of the original kernel matrix.

torch.Size([2609, 2])
We keep 8.84e+04/1.13e+06 =  7% of the original kernel matrix.

torch.Size([10045, 2])
We keep 7.37e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([10427, 2])
We keep 3.35e+06/6.31e+07 =  5% of the original kernel matrix.

torch.Size([16885, 2])
We keep 3.19e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([33785, 2])
We keep 1.37e+07/5.08e+08 =  2% of the original kernel matrix.

torch.Size([32715, 2])
We keep 7.37e+06/4.26e+08 =  1% of the original kernel matrix.

torch.Size([115748, 2])
We keep 6.66e+07/5.02e+09 =  1% of the original kernel matrix.

torch.Size([60169, 2])
We keep 1.87e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([1819, 2])
We keep 3.82e+04/3.86e+05 =  9% of the original kernel matrix.

torch.Size([8791, 2])
We keep 5.18e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([814, 2])
We keep 8.11e+03/6.25e+04 = 12% of the original kernel matrix.

torch.Size([6800, 2])
We keep 2.84e+05/4.72e+06 =  6% of the original kernel matrix.

torch.Size([4857, 2])
We keep 3.50e+05/4.84e+06 =  7% of the original kernel matrix.

torch.Size([12880, 2])
We keep 1.22e+06/4.15e+07 =  2% of the original kernel matrix.

torch.Size([12297, 2])
We keep 5.43e+06/4.18e+07 = 12% of the original kernel matrix.

torch.Size([19486, 2])
We keep 2.50e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([2848, 2])
We keep 8.66e+04/1.19e+06 =  7% of the original kernel matrix.

torch.Size([10430, 2])
We keep 7.63e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([344, 2])
We keep 2.32e+03/1.12e+04 = 20% of the original kernel matrix.

torch.Size([4950, 2])
We keep 1.74e+05/2.00e+06 =  8% of the original kernel matrix.

torch.Size([546, 2])
We keep 5.24e+03/2.79e+04 = 18% of the original kernel matrix.

torch.Size([5712, 2])
We keep 2.24e+05/3.15e+06 =  7% of the original kernel matrix.

torch.Size([2163, 2])
We keep 5.12e+04/6.58e+05 =  7% of the original kernel matrix.

torch.Size([9504, 2])
We keep 6.07e+05/1.53e+07 =  3% of the original kernel matrix.

torch.Size([342, 2])
We keep 2.65e+03/1.19e+04 = 22% of the original kernel matrix.

torch.Size([4711, 2])
We keep 1.71e+05/2.06e+06 =  8% of the original kernel matrix.

torch.Size([7645, 2])
We keep 6.22e+06/4.09e+07 = 15% of the original kernel matrix.

torch.Size([14480, 2])
We keep 2.73e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([1062, 2])
We keep 1.56e+04/1.35e+05 = 11% of the original kernel matrix.

torch.Size([7256, 2])
We keep 3.69e+05/6.93e+06 =  5% of the original kernel matrix.

torch.Size([5000, 2])
We keep 2.07e+05/4.02e+06 =  5% of the original kernel matrix.

torch.Size([13105, 2])
We keep 1.14e+06/3.79e+07 =  2% of the original kernel matrix.

torch.Size([2093, 2])
We keep 6.26e+04/5.67e+05 = 11% of the original kernel matrix.

torch.Size([9301, 2])
We keep 5.81e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([765, 2])
We keep 8.52e+03/6.05e+04 = 14% of the original kernel matrix.

torch.Size([6427, 2])
We keep 2.91e+05/4.65e+06 =  6% of the original kernel matrix.

torch.Size([704, 2])
We keep 7.97e+03/5.57e+04 = 14% of the original kernel matrix.

torch.Size([6206, 2])
We keep 2.79e+05/4.46e+06 =  6% of the original kernel matrix.

torch.Size([31435, 2])
We keep 7.03e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([32003, 2])
We keep 6.46e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([125050, 2])
We keep 1.10e+08/5.94e+09 =  1% of the original kernel matrix.

torch.Size([62816, 2])
We keep 2.03e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([22347, 2])
We keep 6.62e+06/2.05e+08 =  3% of the original kernel matrix.

torch.Size([26432, 2])
We keep 5.02e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([3597, 2])
We keep 1.53e+06/8.36e+06 = 18% of the original kernel matrix.

torch.Size([9783, 2])
We keep 1.49e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([8629, 2])
We keep 6.34e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([16583, 2])
We keep 1.93e+06/7.87e+07 =  2% of the original kernel matrix.

torch.Size([2564, 2])
We keep 1.20e+05/1.25e+06 =  9% of the original kernel matrix.

torch.Size([9748, 2])
We keep 7.75e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([2340, 2])
We keep 6.81e+04/8.39e+05 =  8% of the original kernel matrix.

torch.Size([9743, 2])
We keep 6.98e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([12285, 2])
We keep 1.41e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([19435, 2])
We keep 2.55e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([6976, 2])
We keep 6.51e+05/1.07e+07 =  6% of the original kernel matrix.

torch.Size([15431, 2])
We keep 1.65e+06/6.19e+07 =  2% of the original kernel matrix.

torch.Size([9232, 2])
We keep 2.97e+06/3.26e+07 =  9% of the original kernel matrix.

torch.Size([16854, 2])
We keep 2.37e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([5280, 2])
We keep 7.17e+05/7.44e+06 =  9% of the original kernel matrix.

torch.Size([13174, 2])
We keep 1.48e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([42816, 2])
We keep 3.62e+07/1.14e+09 =  3% of the original kernel matrix.

torch.Size([36011, 2])
We keep 1.02e+07/6.37e+08 =  1% of the original kernel matrix.

torch.Size([1747, 2])
We keep 5.02e+04/4.89e+05 = 10% of the original kernel matrix.

torch.Size([8608, 2])
We keep 5.80e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([7345, 2])
We keep 8.32e+05/1.46e+07 =  5% of the original kernel matrix.

torch.Size([15178, 2])
We keep 1.84e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([34637, 2])
We keep 1.40e+07/5.43e+08 =  2% of the original kernel matrix.

torch.Size([33229, 2])
We keep 7.50e+06/4.40e+08 =  1% of the original kernel matrix.

torch.Size([12446, 2])
We keep 1.20e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([19466, 2])
We keep 2.54e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([2705, 2])
We keep 2.39e+05/1.74e+06 = 13% of the original kernel matrix.

torch.Size([9765, 2])
We keep 8.01e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([3921, 2])
We keep 2.78e+05/3.18e+06 =  8% of the original kernel matrix.

torch.Size([11563, 2])
We keep 1.04e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([2139, 2])
We keep 5.37e+04/5.69e+05 =  9% of the original kernel matrix.

torch.Size([9491, 2])
We keep 5.93e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([1631, 2])
We keep 3.61e+04/4.03e+05 =  8% of the original kernel matrix.

torch.Size([8458, 2])
We keep 5.16e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([1306, 2])
We keep 3.30e+04/3.01e+05 = 10% of the original kernel matrix.

torch.Size([7467, 2])
We keep 4.82e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([6745, 2])
We keep 5.39e+05/9.05e+06 =  5% of the original kernel matrix.

torch.Size([14635, 2])
We keep 1.53e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([2012, 2])
We keep 1.27e+05/8.37e+05 = 15% of the original kernel matrix.

torch.Size([9076, 2])
We keep 6.72e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([8550, 2])
We keep 6.07e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([16358, 2])
We keep 1.78e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([144513, 2])
We keep 1.28e+08/9.41e+09 =  1% of the original kernel matrix.

torch.Size([68222, 2])
We keep 2.51e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([20551, 2])
We keep 1.57e+07/2.08e+08 =  7% of the original kernel matrix.

torch.Size([25020, 2])
We keep 5.18e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([14549, 2])
We keep 8.10e+06/1.61e+08 =  5% of the original kernel matrix.

torch.Size([19547, 2])
We keep 4.62e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([3237, 2])
We keep 2.69e+05/2.60e+06 = 10% of the original kernel matrix.

torch.Size([10419, 2])
We keep 9.88e+05/3.05e+07 =  3% of the original kernel matrix.

torch.Size([2454, 2])
We keep 4.01e+05/2.41e+06 = 16% of the original kernel matrix.

torch.Size([8898, 2])
We keep 9.06e+05/2.94e+07 =  3% of the original kernel matrix.

torch.Size([10188, 2])
We keep 1.19e+06/2.55e+07 =  4% of the original kernel matrix.

torch.Size([17978, 2])
We keep 2.27e+06/9.53e+07 =  2% of the original kernel matrix.

torch.Size([3641, 2])
We keep 4.11e+05/4.70e+06 =  8% of the original kernel matrix.

torch.Size([10555, 2])
We keep 1.20e+06/4.09e+07 =  2% of the original kernel matrix.

torch.Size([19295, 2])
We keep 1.18e+07/2.13e+08 =  5% of the original kernel matrix.

torch.Size([24107, 2])
We keep 5.26e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([31637, 2])
We keep 7.07e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([32151, 2])
We keep 6.43e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([1097, 2])
We keep 5.10e+04/2.75e+05 = 18% of the original kernel matrix.

torch.Size([6844, 2])
We keep 4.71e+05/9.90e+06 =  4% of the original kernel matrix.

torch.Size([47696, 2])
We keep 3.97e+08/4.36e+09 =  9% of the original kernel matrix.

torch.Size([34698, 2])
We keep 1.74e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([19829, 2])
We keep 3.55e+07/3.38e+08 = 10% of the original kernel matrix.

torch.Size([23968, 2])
We keep 6.26e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([3384, 2])
We keep 1.44e+05/1.99e+06 =  7% of the original kernel matrix.

torch.Size([11032, 2])
We keep 9.10e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([782, 2])
We keep 7.94e+03/5.66e+04 = 14% of the original kernel matrix.

torch.Size([6656, 2])
We keep 2.83e+05/4.50e+06 =  6% of the original kernel matrix.

torch.Size([2427, 2])
We keep 6.68e+04/8.82e+05 =  7% of the original kernel matrix.

torch.Size([9806, 2])
We keep 6.85e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([13785, 2])
We keep 1.46e+06/4.74e+07 =  3% of the original kernel matrix.

torch.Size([20551, 2])
We keep 2.84e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([950, 2])
We keep 1.45e+04/9.99e+04 = 14% of the original kernel matrix.

torch.Size([6768, 2])
We keep 3.24e+05/5.97e+06 =  5% of the original kernel matrix.

torch.Size([58045, 2])
We keep 1.89e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([42588, 2])
We keep 1.03e+07/6.63e+08 =  1% of the original kernel matrix.

torch.Size([4548, 2])
We keep 7.07e+05/7.69e+06 =  9% of the original kernel matrix.

torch.Size([12133, 2])
We keep 1.42e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([3488, 2])
We keep 1.71e+05/2.38e+06 =  7% of the original kernel matrix.

torch.Size([11043, 2])
We keep 9.76e+05/2.91e+07 =  3% of the original kernel matrix.

torch.Size([2039, 2])
We keep 4.35e+04/5.61e+05 =  7% of the original kernel matrix.

torch.Size([9260, 2])
We keep 5.72e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([368, 2])
We keep 4.12e+03/1.99e+04 = 20% of the original kernel matrix.

torch.Size([4873, 2])
We keep 2.04e+05/2.66e+06 =  7% of the original kernel matrix.

torch.Size([1177, 2])
We keep 1.84e+04/1.62e+05 = 11% of the original kernel matrix.

torch.Size([7485, 2])
We keep 3.93e+05/7.61e+06 =  5% of the original kernel matrix.

torch.Size([1826, 2])
We keep 1.37e+05/9.88e+05 = 13% of the original kernel matrix.

torch.Size([8077, 2])
We keep 6.98e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([695, 2])
We keep 1.01e+04/6.10e+04 = 16% of the original kernel matrix.

torch.Size([6249, 2])
We keep 2.95e+05/4.67e+06 =  6% of the original kernel matrix.

torch.Size([2608, 2])
We keep 7.87e+04/1.01e+06 =  7% of the original kernel matrix.

torch.Size([9915, 2])
We keep 7.14e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([138159, 2])
We keep 2.50e+09/5.54e+10 =  4% of the original kernel matrix.

torch.Size([51210, 2])
We keep 5.45e+07/4.44e+09 =  1% of the original kernel matrix.

torch.Size([1957, 2])
We keep 6.85e+04/7.29e+05 =  9% of the original kernel matrix.

torch.Size([8807, 2])
We keep 6.66e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([948, 2])
We keep 1.44e+04/1.15e+05 = 12% of the original kernel matrix.

torch.Size([6900, 2])
We keep 3.42e+05/6.40e+06 =  5% of the original kernel matrix.

torch.Size([698, 2])
We keep 1.82e+04/9.67e+04 = 18% of the original kernel matrix.

torch.Size([6030, 2])
We keep 3.31e+05/5.87e+06 =  5% of the original kernel matrix.

torch.Size([1538, 2])
We keep 2.62e+04/2.84e+05 =  9% of the original kernel matrix.

torch.Size([8450, 2])
We keep 4.68e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([2904, 2])
We keep 8.92e+04/1.25e+06 =  7% of the original kernel matrix.

torch.Size([10451, 2])
We keep 7.76e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([7058, 2])
We keep 1.15e+07/8.85e+07 = 12% of the original kernel matrix.

torch.Size([12648, 2])
We keep 3.59e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([11485, 2])
We keep 5.87e+06/6.37e+07 =  9% of the original kernel matrix.

torch.Size([18363, 2])
We keep 3.21e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([190580, 2])
We keep 1.60e+08/1.38e+10 =  1% of the original kernel matrix.

torch.Size([79093, 2])
We keep 2.91e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([1142, 2])
We keep 1.69e+04/1.44e+05 = 11% of the original kernel matrix.

torch.Size([7479, 2])
We keep 3.79e+05/7.18e+06 =  5% of the original kernel matrix.

torch.Size([25046, 2])
We keep 7.85e+06/2.26e+08 =  3% of the original kernel matrix.

torch.Size([28407, 2])
We keep 5.23e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([1544, 2])
We keep 1.32e+05/7.92e+05 = 16% of the original kernel matrix.

torch.Size([7933, 2])
We keep 6.09e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([35888, 2])
We keep 1.23e+07/5.61e+08 =  2% of the original kernel matrix.

torch.Size([33397, 2])
We keep 7.56e+06/4.47e+08 =  1% of the original kernel matrix.

torch.Size([115443, 2])
We keep 1.27e+08/8.60e+09 =  1% of the original kernel matrix.

torch.Size([59964, 2])
We keep 2.37e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([3002, 2])
We keep 9.12e+04/1.38e+06 =  6% of the original kernel matrix.

torch.Size([10736, 2])
We keep 7.91e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([186, 2])
We keep 1.39e+03/4.90e+03 = 28% of the original kernel matrix.

torch.Size([3823, 2])
We keep 1.32e+05/1.32e+06 =  9% of the original kernel matrix.

torch.Size([20538, 2])
We keep 3.37e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([25741, 2])
We keep 4.36e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([2590, 2])
We keep 1.94e+05/1.86e+06 = 10% of the original kernel matrix.

torch.Size([9655, 2])
We keep 9.19e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([11737, 2])
We keep 3.63e+06/7.25e+07 =  5% of the original kernel matrix.

torch.Size([18148, 2])
We keep 3.35e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([2075, 2])
We keep 5.31e+04/5.72e+05 =  9% of the original kernel matrix.

torch.Size([9083, 2])
We keep 5.95e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([687, 2])
We keep 2.21e+04/1.25e+05 = 17% of the original kernel matrix.

torch.Size([5436, 2])
We keep 3.29e+05/6.69e+06 =  4% of the original kernel matrix.

torch.Size([11566, 2])
We keep 1.28e+06/2.93e+07 =  4% of the original kernel matrix.

torch.Size([19565, 2])
We keep 2.36e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([1295, 2])
We keep 2.20e+04/2.02e+05 = 10% of the original kernel matrix.

torch.Size([7645, 2])
We keep 4.27e+05/8.50e+06 =  5% of the original kernel matrix.

torch.Size([17060, 2])
We keep 3.72e+06/9.95e+07 =  3% of the original kernel matrix.

torch.Size([23264, 2])
We keep 3.84e+06/1.88e+08 =  2% of the original kernel matrix.

torch.Size([7807, 2])
We keep 2.06e+06/2.72e+07 =  7% of the original kernel matrix.

torch.Size([14915, 2])
We keep 2.29e+06/9.86e+07 =  2% of the original kernel matrix.

torch.Size([944, 2])
We keep 1.10e+04/8.35e+04 = 13% of the original kernel matrix.

torch.Size([7036, 2])
We keep 3.20e+05/5.46e+06 =  5% of the original kernel matrix.

torch.Size([1180, 2])
We keep 3.94e+04/2.67e+05 = 14% of the original kernel matrix.

torch.Size([7236, 2])
We keep 4.67e+05/9.77e+06 =  4% of the original kernel matrix.

torch.Size([26746, 2])
We keep 1.28e+07/4.38e+08 =  2% of the original kernel matrix.

torch.Size([27925, 2])
We keep 6.92e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([772, 2])
We keep 8.89e+03/6.92e+04 = 12% of the original kernel matrix.

torch.Size([6343, 2])
We keep 2.96e+05/4.97e+06 =  5% of the original kernel matrix.

torch.Size([54935, 2])
We keep 1.18e+08/1.67e+09 =  7% of the original kernel matrix.

torch.Size([40728, 2])
We keep 1.17e+07/7.73e+08 =  1% of the original kernel matrix.

torch.Size([540, 2])
We keep 6.38e+03/3.13e+04 = 20% of the original kernel matrix.

torch.Size([5421, 2])
We keep 2.28e+05/3.34e+06 =  6% of the original kernel matrix.

torch.Size([12059, 2])
We keep 4.47e+06/8.55e+07 =  5% of the original kernel matrix.

torch.Size([18213, 2])
We keep 3.59e+06/1.75e+08 =  2% of the original kernel matrix.

torch.Size([6132, 2])
We keep 1.01e+06/1.12e+07 =  9% of the original kernel matrix.

torch.Size([13815, 2])
We keep 1.67e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([8808, 2])
We keep 2.71e+06/3.55e+07 =  7% of the original kernel matrix.

torch.Size([15518, 2])
We keep 2.53e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([10321, 2])
We keep 3.34e+06/4.30e+07 =  7% of the original kernel matrix.

torch.Size([17627, 2])
We keep 2.75e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([2829, 2])
We keep 7.65e+04/1.10e+06 =  6% of the original kernel matrix.

torch.Size([10555, 2])
We keep 7.41e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([1371, 2])
We keep 3.48e+04/3.08e+05 = 11% of the original kernel matrix.

torch.Size([7775, 2])
We keep 4.87e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([6376, 2])
We keep 4.03e+05/8.01e+06 =  5% of the original kernel matrix.

torch.Size([14355, 2])
We keep 1.48e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([1202, 2])
We keep 1.57e+05/7.94e+05 = 19% of the original kernel matrix.

torch.Size([6343, 2])
We keep 6.67e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([17170, 2])
We keep 1.22e+07/1.48e+08 =  8% of the original kernel matrix.

torch.Size([22281, 2])
We keep 4.48e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([18858, 2])
We keep 3.20e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([24521, 2])
We keep 3.88e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([3198, 2])
We keep 1.07e+05/1.53e+06 =  7% of the original kernel matrix.

torch.Size([10918, 2])
We keep 8.14e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([8433, 2])
We keep 5.95e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([16308, 2])
We keep 1.85e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([9223, 2])
We keep 1.33e+06/2.68e+07 =  4% of the original kernel matrix.

torch.Size([16710, 2])
We keep 2.35e+06/9.78e+07 =  2% of the original kernel matrix.

torch.Size([18849, 2])
We keep 5.53e+06/1.24e+08 =  4% of the original kernel matrix.

torch.Size([24398, 2])
We keep 3.95e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([16345, 2])
We keep 3.83e+06/8.92e+07 =  4% of the original kernel matrix.

torch.Size([22500, 2])
We keep 3.71e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([16814, 2])
We keep 2.36e+06/9.16e+07 =  2% of the original kernel matrix.

torch.Size([23020, 2])
We keep 3.59e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([8175, 2])
We keep 2.19e+06/4.12e+07 =  5% of the original kernel matrix.

torch.Size([14839, 2])
We keep 2.60e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([4564, 2])
We keep 2.28e+05/4.06e+06 =  5% of the original kernel matrix.

torch.Size([12647, 2])
We keep 1.11e+06/3.81e+07 =  2% of the original kernel matrix.

torch.Size([810, 2])
We keep 1.44e+04/8.88e+04 = 16% of the original kernel matrix.

torch.Size([6480, 2])
We keep 3.27e+05/5.63e+06 =  5% of the original kernel matrix.

torch.Size([3549, 2])
We keep 1.52e+05/1.88e+06 =  8% of the original kernel matrix.

torch.Size([11464, 2])
We keep 8.80e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([4573, 2])
We keep 1.83e+05/3.38e+06 =  5% of the original kernel matrix.

torch.Size([12638, 2])
We keep 1.06e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([11350, 2])
We keep 1.85e+06/4.62e+07 =  4% of the original kernel matrix.

torch.Size([18404, 2])
We keep 2.77e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([3202, 2])
We keep 1.06e+05/1.63e+06 =  6% of the original kernel matrix.

torch.Size([10891, 2])
We keep 8.24e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([712, 2])
We keep 8.31e+03/5.86e+04 = 14% of the original kernel matrix.

torch.Size([6332, 2])
We keep 2.85e+05/4.57e+06 =  6% of the original kernel matrix.

torch.Size([1253, 2])
We keep 1.90e+04/1.74e+05 = 10% of the original kernel matrix.

torch.Size([7837, 2])
We keep 4.11e+05/7.88e+06 =  5% of the original kernel matrix.

torch.Size([15003, 2])
We keep 3.39e+06/9.26e+07 =  3% of the original kernel matrix.

torch.Size([21132, 2])
We keep 3.68e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([1676, 2])
We keep 4.98e+04/4.87e+05 = 10% of the original kernel matrix.

torch.Size([8314, 2])
We keep 5.70e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([1342, 2])
We keep 2.34e+04/2.35e+05 =  9% of the original kernel matrix.

torch.Size([7948, 2])
We keep 4.27e+05/9.16e+06 =  4% of the original kernel matrix.

torch.Size([465, 2])
We keep 2.69e+03/1.44e+04 = 18% of the original kernel matrix.

torch.Size([5639, 2])
We keep 1.79e+05/2.27e+06 =  7% of the original kernel matrix.

torch.Size([808, 2])
We keep 3.94e+04/1.81e+05 = 21% of the original kernel matrix.

torch.Size([5942, 2])
We keep 4.00e+05/8.03e+06 =  4% of the original kernel matrix.

torch.Size([2079, 2])
We keep 4.32e+04/5.10e+05 =  8% of the original kernel matrix.

torch.Size([9289, 2])
We keep 5.66e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([2815, 2])
We keep 1.66e+05/1.57e+06 = 10% of the original kernel matrix.

torch.Size([10079, 2])
We keep 8.27e+05/2.36e+07 =  3% of the original kernel matrix.

torch.Size([15191, 2])
We keep 5.29e+06/1.11e+08 =  4% of the original kernel matrix.

torch.Size([21363, 2])
We keep 3.91e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([329, 2])
We keep 2.98e+03/1.39e+04 = 21% of the original kernel matrix.

torch.Size([4769, 2])
We keep 1.91e+05/2.23e+06 =  8% of the original kernel matrix.

torch.Size([8789, 2])
We keep 1.62e+07/1.14e+08 = 14% of the original kernel matrix.

torch.Size([15325, 2])
We keep 3.80e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([401, 2])
We keep 3.18e+03/1.69e+04 = 18% of the original kernel matrix.

torch.Size([5073, 2])
We keep 1.93e+05/2.46e+06 =  7% of the original kernel matrix.

torch.Size([1694, 2])
We keep 3.70e+04/3.84e+05 =  9% of the original kernel matrix.

torch.Size([8471, 2])
We keep 5.14e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([59487, 2])
We keep 4.43e+07/1.71e+09 =  2% of the original kernel matrix.

torch.Size([42249, 2])
We keep 1.18e+07/7.81e+08 =  1% of the original kernel matrix.

torch.Size([3773, 2])
We keep 6.65e+05/6.11e+06 = 10% of the original kernel matrix.

torch.Size([11027, 2])
We keep 1.27e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([4191, 2])
We keep 1.99e+05/3.09e+06 =  6% of the original kernel matrix.

torch.Size([12017, 2])
We keep 1.03e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([40588, 2])
We keep 1.67e+07/6.44e+08 =  2% of the original kernel matrix.

torch.Size([36312, 2])
We keep 8.06e+06/4.79e+08 =  1% of the original kernel matrix.

torch.Size([10407, 2])
We keep 8.66e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([17992, 2])
We keep 2.13e+06/8.94e+07 =  2% of the original kernel matrix.

torch.Size([36919, 2])
We keep 2.41e+07/6.08e+08 =  3% of the original kernel matrix.

torch.Size([34519, 2])
We keep 7.93e+06/4.66e+08 =  1% of the original kernel matrix.

torch.Size([45462, 2])
We keep 1.36e+07/8.08e+08 =  1% of the original kernel matrix.

torch.Size([38208, 2])
We keep 8.68e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([797, 2])
We keep 1.17e+04/7.90e+04 = 14% of the original kernel matrix.

torch.Size([6360, 2])
We keep 3.06e+05/5.31e+06 =  5% of the original kernel matrix.

torch.Size([4352, 2])
We keep 3.42e+05/3.83e+06 =  8% of the original kernel matrix.

torch.Size([12193, 2])
We keep 1.14e+06/3.70e+07 =  3% of the original kernel matrix.

torch.Size([4573, 2])
We keep 1.84e+05/3.41e+06 =  5% of the original kernel matrix.

torch.Size([12617, 2])
We keep 1.08e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([25179, 2])
We keep 6.52e+06/2.47e+08 =  2% of the original kernel matrix.

torch.Size([28915, 2])
We keep 5.58e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([58725, 2])
We keep 3.12e+07/1.55e+09 =  2% of the original kernel matrix.

torch.Size([42561, 2])
We keep 1.14e+07/7.43e+08 =  1% of the original kernel matrix.

torch.Size([1761, 2])
We keep 6.12e+04/5.81e+05 = 10% of the original kernel matrix.

torch.Size([8233, 2])
We keep 5.86e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([5136, 2])
We keep 2.59e+05/4.79e+06 =  5% of the original kernel matrix.

torch.Size([13085, 2])
We keep 1.22e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([455, 2])
We keep 3.29e+03/2.02e+04 = 16% of the original kernel matrix.

torch.Size([5513, 2])
We keep 2.04e+05/2.68e+06 =  7% of the original kernel matrix.

torch.Size([1405, 2])
We keep 3.98e+04/3.24e+05 = 12% of the original kernel matrix.

torch.Size([7852, 2])
We keep 4.88e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([968, 2])
We keep 1.11e+04/9.61e+04 = 11% of the original kernel matrix.

torch.Size([7002, 2])
We keep 3.21e+05/5.86e+06 =  5% of the original kernel matrix.

torch.Size([2537, 2])
We keep 9.02e+04/1.20e+06 =  7% of the original kernel matrix.

torch.Size([9717, 2])
We keep 7.61e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([94347, 2])
We keep 6.40e+07/3.56e+09 =  1% of the original kernel matrix.

torch.Size([53901, 2])
We keep 1.63e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([97847, 2])
We keep 1.59e+08/4.93e+09 =  3% of the original kernel matrix.

torch.Size([54688, 2])
We keep 1.91e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([1280, 2])
We keep 1.01e+05/6.10e+05 = 16% of the original kernel matrix.

torch.Size([6564, 2])
We keep 5.93e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([4047, 2])
We keep 2.14e+05/3.53e+06 =  6% of the original kernel matrix.

torch.Size([11863, 2])
We keep 1.05e+06/3.55e+07 =  2% of the original kernel matrix.

torch.Size([4264, 2])
We keep 2.34e+05/3.50e+06 =  6% of the original kernel matrix.

torch.Size([12260, 2])
We keep 1.11e+06/3.54e+07 =  3% of the original kernel matrix.

torch.Size([20344, 2])
We keep 5.74e+06/1.87e+08 =  3% of the original kernel matrix.

torch.Size([25094, 2])
We keep 4.95e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([652, 2])
We keep 7.30e+03/5.11e+04 = 14% of the original kernel matrix.

torch.Size([6158, 2])
We keep 2.66e+05/4.27e+06 =  6% of the original kernel matrix.

torch.Size([1355, 2])
We keep 2.27e+04/1.96e+05 = 11% of the original kernel matrix.

torch.Size([7638, 2])
We keep 4.04e+05/8.37e+06 =  4% of the original kernel matrix.

torch.Size([3770, 2])
We keep 1.57e+05/2.44e+06 =  6% of the original kernel matrix.

torch.Size([11517, 2])
We keep 9.83e+05/2.95e+07 =  3% of the original kernel matrix.

torch.Size([18714, 2])
We keep 3.62e+06/1.15e+08 =  3% of the original kernel matrix.

torch.Size([24278, 2])
We keep 3.99e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([1242, 2])
We keep 1.74e+04/1.47e+05 = 11% of the original kernel matrix.

torch.Size([7741, 2])
We keep 3.79e+05/7.23e+06 =  5% of the original kernel matrix.

torch.Size([6857, 2])
We keep 5.34e+05/9.86e+06 =  5% of the original kernel matrix.

torch.Size([14772, 2])
We keep 1.59e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([315, 2])
We keep 1.25e+03/6.89e+03 = 18% of the original kernel matrix.

torch.Size([5110, 2])
We keep 1.38e+05/1.57e+06 =  8% of the original kernel matrix.

time for making ranges is 1.556368350982666
Sorting X and nu_X
time for sorting X is 0.03801107406616211
Sorting Z and nu_Z
time for sorting Z is 0.0002269744873046875
Starting Optim
sum tnu_Z before tensor(4559656., device='cuda:0')
c= tensor(116.4212, device='cuda:0')
c= tensor(4823.7012, device='cuda:0')
c= tensor(5164.6548, device='cuda:0')
c= tensor(5382.8174, device='cuda:0')
c= tensor(16013.9844, device='cuda:0')
c= tensor(16371.5205, device='cuda:0')
c= tensor(53956.0156, device='cuda:0')
c= tensor(57370.5117, device='cuda:0')
c= tensor(58030.6211, device='cuda:0')
c= tensor(69996.6406, device='cuda:0')
c= tensor(71714.3438, device='cuda:0')
c= tensor(572767.7500, device='cuda:0')
c= tensor(573979.9375, device='cuda:0')
c= tensor(604458.2500, device='cuda:0')
c= tensor(606481.3750, device='cuda:0')
c= tensor(607847.1250, device='cuda:0')
c= tensor(613577.1875, device='cuda:0')
c= tensor(706358.6875, device='cuda:0')
c= tensor(1082168.7500, device='cuda:0')
c= tensor(1232267.5000, device='cuda:0')
c= tensor(1232656.7500, device='cuda:0')
c= tensor(1347505.1250, device='cuda:0')
c= tensor(1348841.1250, device='cuda:0')
c= tensor(1352149.7500, device='cuda:0')
c= tensor(1368740.6250, device='cuda:0')
c= tensor(1462973.8750, device='cuda:0')
c= tensor(1553893.5000, device='cuda:0')
c= tensor(1562650., device='cuda:0')
c= tensor(2137494.2500, device='cuda:0')
c= tensor(6245687., device='cuda:0')
c= tensor(6246430., device='cuda:0')
c= tensor(11555976., device='cuda:0')
c= tensor(11557800., device='cuda:0')
c= tensor(11558100., device='cuda:0')
c= tensor(11558547., device='cuda:0')
c= tensor(11578198., device='cuda:0')
c= tensor(11620583., device='cuda:0')
c= tensor(11620952., device='cuda:0')
c= tensor(11622154., device='cuda:0')
c= tensor(11622613., device='cuda:0')
c= tensor(11622939., device='cuda:0')
c= tensor(11623074., device='cuda:0')
c= tensor(11623146., device='cuda:0')
c= tensor(11623357., device='cuda:0')
c= tensor(11623441., device='cuda:0')
c= tensor(11623549., device='cuda:0')
c= tensor(11624314., device='cuda:0')
c= tensor(11624822., device='cuda:0')
c= tensor(11625021., device='cuda:0')
c= tensor(11626928., device='cuda:0')
c= tensor(11627945., device='cuda:0')
c= tensor(11628910., device='cuda:0')
c= tensor(11629291., device='cuda:0')
c= tensor(11629689., device='cuda:0')
c= tensor(11631064., device='cuda:0')
c= tensor(11631846., device='cuda:0')
c= tensor(11632031., device='cuda:0')
c= tensor(11632269., device='cuda:0')
c= tensor(11632691., device='cuda:0')
c= tensor(11633006., device='cuda:0')
c= tensor(11636556., device='cuda:0')
c= tensor(11636687., device='cuda:0')
c= tensor(11639924., device='cuda:0')
c= tensor(11640379., device='cuda:0')
c= tensor(11640693., device='cuda:0')
c= tensor(11641105., device='cuda:0')
c= tensor(11641220., device='cuda:0')
c= tensor(11641427., device='cuda:0')
c= tensor(11641663., device='cuda:0')
c= tensor(11641814., device='cuda:0')
c= tensor(11641889., device='cuda:0')
c= tensor(11642265., device='cuda:0')
c= tensor(11642436., device='cuda:0')
c= tensor(11642576., device='cuda:0')
c= tensor(11643216., device='cuda:0')
c= tensor(11644277., device='cuda:0')
c= tensor(11644557., device='cuda:0')
c= tensor(11644662., device='cuda:0')
c= tensor(11644857., device='cuda:0')
c= tensor(11645359., device='cuda:0')
c= tensor(11646040., device='cuda:0')
c= tensor(11646162., device='cuda:0')
c= tensor(11647202., device='cuda:0')
c= tensor(11647313., device='cuda:0')
c= tensor(11647873., device='cuda:0')
c= tensor(11647953., device='cuda:0')
c= tensor(11648768., device='cuda:0')
c= tensor(11648927., device='cuda:0')
c= tensor(11649184., device='cuda:0')
c= tensor(11649783., device='cuda:0')
c= tensor(11649915., device='cuda:0')
c= tensor(11650041., device='cuda:0')
c= tensor(11650181., device='cuda:0')
c= tensor(11650852., device='cuda:0')
c= tensor(11651040., device='cuda:0')
c= tensor(11651689., device='cuda:0')
c= tensor(11652944., device='cuda:0')
c= tensor(11656859., device='cuda:0')
c= tensor(11657552., device='cuda:0')
c= tensor(11657908., device='cuda:0')
c= tensor(11661780., device='cuda:0')
c= tensor(11662486., device='cuda:0')
c= tensor(11662783., device='cuda:0')
c= tensor(11662921., device='cuda:0')
c= tensor(11663323., device='cuda:0')
c= tensor(11663466., device='cuda:0')
c= tensor(11664192., device='cuda:0')
c= tensor(11664242., device='cuda:0')
c= tensor(11664707., device='cuda:0')
c= tensor(11665029., device='cuda:0')
c= tensor(11665162., device='cuda:0')
c= tensor(11665232., device='cuda:0')
c= tensor(11665489., device='cuda:0')
c= tensor(11666135., device='cuda:0')
c= tensor(11666690., device='cuda:0')
c= tensor(11666824., device='cuda:0')
c= tensor(11666922., device='cuda:0')
c= tensor(11667118., device='cuda:0')
c= tensor(11668108., device='cuda:0')
c= tensor(11668356., device='cuda:0')
c= tensor(11669442., device='cuda:0')
c= tensor(11669750., device='cuda:0')
c= tensor(11670038., device='cuda:0')
c= tensor(11670937., device='cuda:0')
c= tensor(11671153., device='cuda:0')
c= tensor(11671558., device='cuda:0')
c= tensor(11671873., device='cuda:0')
c= tensor(11671924., device='cuda:0')
c= tensor(11674413., device='cuda:0')
c= tensor(11674667., device='cuda:0')
c= tensor(11674989., device='cuda:0')
c= tensor(11675171., device='cuda:0')
c= tensor(11675380., device='cuda:0')
c= tensor(11675502., device='cuda:0')
c= tensor(11675737., device='cuda:0')
c= tensor(11676051., device='cuda:0')
c= tensor(11676108., device='cuda:0')
c= tensor(11676259., device='cuda:0')
c= tensor(11676507., device='cuda:0')
c= tensor(11676979., device='cuda:0')
c= tensor(11677255., device='cuda:0')
c= tensor(11677560., device='cuda:0')
c= tensor(11678273., device='cuda:0')
c= tensor(11688080., device='cuda:0')
c= tensor(11688831., device='cuda:0')
c= tensor(11688965., device='cuda:0')
c= tensor(11689191., device='cuda:0')
c= tensor(11689630., device='cuda:0')
c= tensor(11689892., device='cuda:0')
c= tensor(11690090., device='cuda:0')
c= tensor(11690360., device='cuda:0')
c= tensor(11691553., device='cuda:0')
c= tensor(11692109., device='cuda:0')
c= tensor(11694203., device='cuda:0')
c= tensor(11694412., device='cuda:0')
c= tensor(11695916., device='cuda:0')
c= tensor(11696076., device='cuda:0')
c= tensor(11696252., device='cuda:0')
c= tensor(11696922., device='cuda:0')
c= tensor(11696956., device='cuda:0')
c= tensor(11698109., device='cuda:0')
c= tensor(11698210., device='cuda:0')
c= tensor(11698478., device='cuda:0')
c= tensor(11698587., device='cuda:0')
c= tensor(11698878., device='cuda:0')
c= tensor(11699227., device='cuda:0')
c= tensor(11699556., device='cuda:0')
c= tensor(11699694., device='cuda:0')
c= tensor(11700063., device='cuda:0')
c= tensor(11700114., device='cuda:0')
c= tensor(11700307., device='cuda:0')
c= tensor(11700612., device='cuda:0')
c= tensor(11700943., device='cuda:0')
c= tensor(11701156., device='cuda:0')
c= tensor(11702047., device='cuda:0')
c= tensor(11702258., device='cuda:0')
c= tensor(11702350., device='cuda:0')
c= tensor(11704360., device='cuda:0')
c= tensor(11704800., device='cuda:0')
c= tensor(11704981., device='cuda:0')
c= tensor(11705496., device='cuda:0')
c= tensor(11705665., device='cuda:0')
c= tensor(11706017., device='cuda:0')
c= tensor(11706128., device='cuda:0')
c= tensor(11707082., device='cuda:0')
c= tensor(11707925., device='cuda:0')
c= tensor(11708289., device='cuda:0')
c= tensor(11708524., device='cuda:0')
c= tensor(11709139., device='cuda:0')
c= tensor(11712793., device='cuda:0')
c= tensor(11712890., device='cuda:0')
c= tensor(11712986., device='cuda:0')
c= tensor(11713233., device='cuda:0')
c= tensor(11713430., device='cuda:0')
c= tensor(11713711., device='cuda:0')
c= tensor(11713968., device='cuda:0')
c= tensor(11714194., device='cuda:0')
c= tensor(11714299., device='cuda:0')
c= tensor(11714514., device='cuda:0')
c= tensor(11714552., device='cuda:0')
c= tensor(11715400., device='cuda:0')
c= tensor(11715516., device='cuda:0')
c= tensor(11716929., device='cuda:0')
c= tensor(11718419., device='cuda:0')
c= tensor(11718778., device='cuda:0')
c= tensor(11718816., device='cuda:0')
c= tensor(11719284., device='cuda:0')
c= tensor(11719692., device='cuda:0')
c= tensor(11720065., device='cuda:0')
c= tensor(11720952., device='cuda:0')
c= tensor(11724452., device='cuda:0')
c= tensor(11724543., device='cuda:0')
c= tensor(11724609., device='cuda:0')
c= tensor(11724666., device='cuda:0')
c= tensor(11724827., device='cuda:0')
c= tensor(11724967., device='cuda:0')
c= tensor(11725128., device='cuda:0')
c= tensor(11725209., device='cuda:0')
c= tensor(11726139., device='cuda:0')
c= tensor(11726364., device='cuda:0')
c= tensor(11726487., device='cuda:0')
c= tensor(11726909., device='cuda:0')
c= tensor(11727214., device='cuda:0')
c= tensor(11727433., device='cuda:0')
c= tensor(11728993., device='cuda:0')
c= tensor(11729545., device='cuda:0')
c= tensor(11729644., device='cuda:0')
c= tensor(11729774., device='cuda:0')
c= tensor(11730501., device='cuda:0')
c= tensor(11730596., device='cuda:0')
c= tensor(11730672., device='cuda:0')
c= tensor(11730854., device='cuda:0')
c= tensor(11731237., device='cuda:0')
c= tensor(11732305., device='cuda:0')
c= tensor(11732475., device='cuda:0')
c= tensor(11732534., device='cuda:0')
c= tensor(11732730., device='cuda:0')
c= tensor(11733074., device='cuda:0')
c= tensor(11733129., device='cuda:0')
c= tensor(11733661., device='cuda:0')
c= tensor(11865017., device='cuda:0')
c= tensor(11865362., device='cuda:0')
c= tensor(11866428., device='cuda:0')
c= tensor(11866509., device='cuda:0')
c= tensor(11867232., device='cuda:0')
c= tensor(11872920., device='cuda:0')
c= tensor(12009260., device='cuda:0')
c= tensor(12009371., device='cuda:0')
c= tensor(12013618., device='cuda:0')
c= tensor(12062422., device='cuda:0')
c= tensor(12062861., device='cuda:0')
c= tensor(12325567., device='cuda:0')
c= tensor(12325666., device='cuda:0')
c= tensor(12326611., device='cuda:0')
c= tensor(12382405., device='cuda:0')
c= tensor(12384808., device='cuda:0')
c= tensor(12385051., device='cuda:0')
c= tensor(12386875., device='cuda:0')
c= tensor(12407370., device='cuda:0')
c= tensor(12498600., device='cuda:0')
c= tensor(12501375., device='cuda:0')
c= tensor(12518290., device='cuda:0')
c= tensor(12534225., device='cuda:0')
c= tensor(12555806., device='cuda:0')
c= tensor(12556457., device='cuda:0')
c= tensor(13255791., device='cuda:0')
c= tensor(13255994., device='cuda:0')
c= tensor(13256299., device='cuda:0')
c= tensor(13265883., device='cuda:0')
c= tensor(13267285., device='cuda:0')
c= tensor(14625088., device='cuda:0')
c= tensor(14647079., device='cuda:0')
c= tensor(14647158., device='cuda:0')
c= tensor(14647777., device='cuda:0')
c= tensor(14650087., device='cuda:0')
c= tensor(14650234., device='cuda:0')
c= tensor(14763859., device='cuda:0')
c= tensor(14977509., device='cuda:0')
c= tensor(14978470., device='cuda:0')
c= tensor(14978519., device='cuda:0')
c= tensor(14979008., device='cuda:0')
c= tensor(14983765., device='cuda:0')
c= tensor(14991558., device='cuda:0')
c= tensor(14992593., device='cuda:0')
c= tensor(14992926., device='cuda:0')
c= tensor(17068498., device='cuda:0')
c= tensor(17069200., device='cuda:0')
c= tensor(17073820., device='cuda:0')
c= tensor(17205968., device='cuda:0')
c= tensor(17206192., device='cuda:0')
c= tensor(17208038., device='cuda:0')
c= tensor(17622414., device='cuda:0')
c= tensor(18115280., device='cuda:0')
c= tensor(18116272., device='cuda:0')
c= tensor(18116418., device='cuda:0')
c= tensor(18116722., device='cuda:0')
c= tensor(18116908., device='cuda:0')
c= tensor(18118102., device='cuda:0')
c= tensor(18120798., device='cuda:0')
c= tensor(18134232., device='cuda:0')
c= tensor(19761614., device='cuda:0')
c= tensor(19803406., device='cuda:0')
c= tensor(19804386., device='cuda:0')
c= tensor(19805616., device='cuda:0')
c= tensor(20937852., device='cuda:0')
c= tensor(20945740., device='cuda:0')
c= tensor(20947204., device='cuda:0')
c= tensor(20953700., device='cuda:0')
c= tensor(22518578., device='cuda:0')
c= tensor(22518890., device='cuda:0')
c= tensor(22545316., device='cuda:0')
c= tensor(22545870., device='cuda:0')
c= tensor(22547608., device='cuda:0')
c= tensor(22556010., device='cuda:0')
c= tensor(23024794., device='cuda:0')
c= tensor(23077302., device='cuda:0')
c= tensor(23077684., device='cuda:0')
c= tensor(23127244., device='cuda:0')
c= tensor(23174046., device='cuda:0')
c= tensor(23174504., device='cuda:0')
c= tensor(23175402., device='cuda:0')
c= tensor(23311970., device='cuda:0')
c= tensor(24204828., device='cuda:0')
c= tensor(24231686., device='cuda:0')
c= tensor(24232234., device='cuda:0')
c= tensor(24232508., device='cuda:0')
c= tensor(24232604., device='cuda:0')
c= tensor(24241276., device='cuda:0')
c= tensor(24241608., device='cuda:0')
c= tensor(24241874., device='cuda:0')
c= tensor(24284478., device='cuda:0')
c= tensor(24379574., device='cuda:0')
c= tensor(24383612., device='cuda:0')
c= tensor(24384010., device='cuda:0')
c= tensor(24384982., device='cuda:0')
c= tensor(24387120., device='cuda:0')
c= tensor(24390074., device='cuda:0')
c= tensor(24392658., device='cuda:0')
c= tensor(24392860., device='cuda:0')
c= tensor(26209318., device='cuda:0')
c= tensor(26210178., device='cuda:0')
c= tensor(26216006., device='cuda:0')
c= tensor(26263678., device='cuda:0')
c= tensor(26264346., device='cuda:0')
c= tensor(26290382., device='cuda:0')
c= tensor(26291644., device='cuda:0')
c= tensor(26306214., device='cuda:0')
c= tensor(26306368., device='cuda:0')
c= tensor(26306624., device='cuda:0')
c= tensor(26307228., device='cuda:0')
c= tensor(26311412., device='cuda:0')
c= tensor(26313372., device='cuda:0')
c= tensor(26326258., device='cuda:0')
c= tensor(26326666., device='cuda:0')
c= tensor(26327368., device='cuda:0')
c= tensor(26985498., device='cuda:0')
c= tensor(26993672., device='cuda:0')
c= tensor(27015296., device='cuda:0')
c= tensor(27313232., device='cuda:0')
c= tensor(27382182., device='cuda:0')
c= tensor(27383234., device='cuda:0')
c= tensor(27383592., device='cuda:0')
c= tensor(27393324., device='cuda:0')
c= tensor(27393380., device='cuda:0')
c= tensor(27393792., device='cuda:0')
c= tensor(27395094., device='cuda:0')
c= tensor(27395484., device='cuda:0')
c= tensor(27395842., device='cuda:0')
c= tensor(27396022., device='cuda:0')
c= tensor(27396474., device='cuda:0')
c= tensor(28358224., device='cuda:0')
c= tensor(28365946., device='cuda:0')
c= tensor(28377822., device='cuda:0')
c= tensor(28415386., device='cuda:0')
c= tensor(28415526., device='cuda:0')
c= tensor(28415942., device='cuda:0')
c= tensor(34238176., device='cuda:0')
c= tensor(34875964., device='cuda:0')
c= tensor(34878848., device='cuda:0')
c= tensor(34882016., device='cuda:0')
c= tensor(34882240., device='cuda:0')
c= tensor(34934280., device='cuda:0')
c= tensor(35109404., device='cuda:0')
c= tensor(35280852., device='cuda:0')
c= tensor(35283700., device='cuda:0')
c= tensor(35325824., device='cuda:0')
c= tensor(35386252., device='cuda:0')
c= tensor(35387616., device='cuda:0')
c= tensor(35387960., device='cuda:0')
c= tensor(35390292., device='cuda:0')
c= tensor(35395220., device='cuda:0')
c= tensor(35395560., device='cuda:0')
c= tensor(35466712., device='cuda:0')
c= tensor(35473432., device='cuda:0')
c= tensor(35473528., device='cuda:0')
c= tensor(35474880., device='cuda:0')
c= tensor(35478476., device='cuda:0')
c= tensor(35478564., device='cuda:0')
c= tensor(35678608., device='cuda:0')
c= tensor(35827760., device='cuda:0')
c= tensor(35913204., device='cuda:0')
c= tensor(36001932., device='cuda:0')
c= tensor(36105708., device='cuda:0')
c= tensor(36106920., device='cuda:0')
c= tensor(36108208., device='cuda:0')
c= tensor(36112908., device='cuda:0')
c= tensor(36145000., device='cuda:0')
c= tensor(36145140., device='cuda:0')
c= tensor(36190456., device='cuda:0')
c= tensor(38305000., device='cuda:0')
c= tensor(38314296., device='cuda:0')
c= tensor(38330752., device='cuda:0')
c= tensor(40076008., device='cuda:0')
c= tensor(40077008., device='cuda:0')
c= tensor(40077084., device='cuda:0')
c= tensor(40092972., device='cuda:0')
c= tensor(40096196., device='cuda:0')
c= tensor(40098056., device='cuda:0')
c= tensor(43462808., device='cuda:0')
c= tensor(44245704., device='cuda:0')
c= tensor(44318200., device='cuda:0')
c= tensor(44333772., device='cuda:0')
c= tensor(44365768., device='cuda:0')
c= tensor(44365908., device='cuda:0')
c= tensor(44366280., device='cuda:0')
c= tensor(45219128., device='cuda:0')
c= tensor(45219500., device='cuda:0')
c= tensor(45219752., device='cuda:0')
c= tensor(45224256., device='cuda:0')
c= tensor(45264708., device='cuda:0')
c= tensor(45285676., device='cuda:0')
c= tensor(45307268., device='cuda:0')
c= tensor(45307880., device='cuda:0')
c= tensor(45318540., device='cuda:0')
c= tensor(45319072., device='cuda:0')
c= tensor(45325724., device='cuda:0')
c= tensor(45332076., device='cuda:0')
c= tensor(45334564., device='cuda:0')
c= tensor(45334740., device='cuda:0')
c= tensor(45346452., device='cuda:0')
c= tensor(45346580., device='cuda:0')
c= tensor(45355684., device='cuda:0')
c= tensor(45356432., device='cuda:0')
c= tensor(45357504., device='cuda:0')
c= tensor(45357920., device='cuda:0')
c= tensor(45359756., device='cuda:0')
c= tensor(45360172., device='cuda:0')
c= tensor(45366088., device='cuda:0')
c= tensor(45503444., device='cuda:0')
c= tensor(45527052., device='cuda:0')
c= tensor(45528152., device='cuda:0')
c= tensor(45528624., device='cuda:0')
c= tensor(46049600., device='cuda:0')
c= tensor(46050004., device='cuda:0')
c= tensor(48567516., device='cuda:0')
c= tensor(48567568., device='cuda:0')
c= tensor(48572400., device='cuda:0')
c= tensor(48574028., device='cuda:0')
c= tensor(48574460., device='cuda:0')
c= tensor(48575580., device='cuda:0')
c= tensor(48594072., device='cuda:0')
c= tensor(50843312., device='cuda:0')
c= tensor(50843996., device='cuda:0')
c= tensor(50848624., device='cuda:0')
c= tensor(50848856., device='cuda:0')
c= tensor(50849596., device='cuda:0')
c= tensor(50851876., device='cuda:0')
c= tensor(50854312., device='cuda:0')
c= tensor(50855196., device='cuda:0')
c= tensor(50856780., device='cuda:0')
c= tensor(50860480., device='cuda:0')
c= tensor(50860576., device='cuda:0')
c= tensor(50861484., device='cuda:0')
c= tensor(51402348., device='cuda:0')
c= tensor(51403616., device='cuda:0')
c= tensor(51423288., device='cuda:0')
c= tensor(51557984., device='cuda:0')
c= tensor(51559560., device='cuda:0')
c= tensor(51559748., device='cuda:0')
c= tensor(51559948., device='cuda:0')
c= tensor(51762064., device='cuda:0')
c= tensor(51763684., device='cuda:0')
c= tensor(51766976., device='cuda:0')
c= tensor(51773464., device='cuda:0')
c= tensor(51995380., device='cuda:0')
c= tensor(51995832., device='cuda:0')
c= tensor(51996480., device='cuda:0')
c= tensor(56114784., device='cuda:0')
c= tensor(56115088., device='cuda:0')
c= tensor(56116032., device='cuda:0')
c= tensor(56116904., device='cuda:0')
c= tensor(56188684., device='cuda:0')
c= tensor(56296460., device='cuda:0')
c= tensor(56302324., device='cuda:0')
c= tensor(56309740., device='cuda:0')
c= tensor(56310552., device='cuda:0')
c= tensor(56369636., device='cuda:0')
c= tensor(56370476., device='cuda:0')
c= tensor(56371232., device='cuda:0')
c= tensor(56380456., device='cuda:0')
c= tensor(56381128., device='cuda:0')
c= tensor(56446512., device='cuda:0')
c= tensor(56446736., device='cuda:0')
c= tensor(56446796., device='cuda:0')
c= tensor(56447880., device='cuda:0')
c= tensor(56449672., device='cuda:0')
c= tensor(56652404., device='cuda:0')
c= tensor(56719016., device='cuda:0')
c= tensor(56722620., device='cuda:0')
c= tensor(56722816., device='cuda:0')
c= tensor(56732468., device='cuda:0')
c= tensor(56732680., device='cuda:0')
c= tensor(56733332., device='cuda:0')
c= tensor(56735260., device='cuda:0')
c= tensor(57048188., device='cuda:0')
c= tensor(57049100., device='cuda:0')
c= tensor(57049200., device='cuda:0')
c= tensor(57049356., device='cuda:0')
c= tensor(58874160., device='cuda:0')
c= tensor(60896212., device='cuda:0')
c= tensor(60896668., device='cuda:0')
c= tensor(60897524., device='cuda:0')
c= tensor(60898036., device='cuda:0')
c= tensor(60925868., device='cuda:0')
c= tensor(60926292., device='cuda:0')
c= tensor(60935224., device='cuda:0')
c= tensor(60935440., device='cuda:0')
c= tensor(61418464., device='cuda:0')
c= tensor(61426304., device='cuda:0')
c= tensor(61691300., device='cuda:0')
c= tensor(61691776., device='cuda:0')
c= tensor(61692856., device='cuda:0')
c= tensor(61692964., device='cuda:0')
c= tensor(61701992., device='cuda:0')
c= tensor(61703688., device='cuda:0')
c= tensor(61778524., device='cuda:0')
c= tensor(62050024., device='cuda:0')
c= tensor(63322492., device='cuda:0')
c= tensor(63323272., device='cuda:0')
c= tensor(63323544., device='cuda:0')
c= tensor(63330036., device='cuda:0')
c= tensor(63425312., device='cuda:0')
c= tensor(63426960., device='cuda:0')
c= tensor(63427068., device='cuda:0')
c= tensor(63427252., device='cuda:0')
c= tensor(63428264., device='cuda:0')
c= tensor(63428380., device='cuda:0')
c= tensor(63515528., device='cuda:0')
c= tensor(63515976., device='cuda:0')
c= tensor(63519336., device='cuda:0')
c= tensor(63520344., device='cuda:0')
c= tensor(63520608., device='cuda:0')
c= tensor(63520852., device='cuda:0')
c= tensor(63656120., device='cuda:0')
c= tensor(66646392., device='cuda:0')
c= tensor(66776308., device='cuda:0')
c= tensor(66791468., device='cuda:0')
c= tensor(66804612., device='cuda:0')
c= tensor(66806664., device='cuda:0')
c= tensor(66807796., device='cuda:0')
c= tensor(66824652., device='cuda:0')
c= tensor(66832148., device='cuda:0')
c= tensor(66869064., device='cuda:0')
c= tensor(66878616., device='cuda:0')
c= tensor(67581856., device='cuda:0')
c= tensor(67582736., device='cuda:0')
c= tensor(67593328., device='cuda:0')
c= tensor(68005704., device='cuda:0')
c= tensor(68020936., device='cuda:0')
c= tensor(68023816., device='cuda:0')
c= tensor(68027072., device='cuda:0')
c= tensor(68028080., device='cuda:0')
c= tensor(68028904., device='cuda:0')
c= tensor(68029616., device='cuda:0')
c= tensor(68037488., device='cuda:0')
c= tensor(68039216., device='cuda:0')
c= tensor(68047664., device='cuda:0')
c= tensor(70690584., device='cuda:0')
c= tensor(71140512., device='cuda:0')
c= tensor(71272208., device='cuda:0')
c= tensor(71275136., device='cuda:0')
c= tensor(71281776., device='cuda:0')
c= tensor(71295616., device='cuda:0')
c= tensor(71303152., device='cuda:0')
c= tensor(71467352., device='cuda:0')
c= tensor(71600216., device='cuda:0')
c= tensor(71600992., device='cuda:0')
c= tensor(79394936., device='cuda:0')
c= tensor(80318936., device='cuda:0')
c= tensor(80321480., device='cuda:0')
c= tensor(80321736., device='cuda:0')
c= tensor(80323016., device='cuda:0')
c= tensor(80341432., device='cuda:0')
c= tensor(80341864., device='cuda:0')
c= tensor(80648792., device='cuda:0')
c= tensor(80659848., device='cuda:0')
c= tensor(80662672., device='cuda:0')
c= tensor(80663736., device='cuda:0')
c= tensor(80663888., device='cuda:0')
c= tensor(80664360., device='cuda:0')
c= tensor(80666232., device='cuda:0')
c= tensor(80666520., device='cuda:0')
c= tensor(80667832., device='cuda:0')
c= tensor(1.5545e+08, device='cuda:0')
c= tensor(1.5545e+08, device='cuda:0')
c= tensor(1.5546e+08, device='cuda:0')
c= tensor(1.5546e+08, device='cuda:0')
c= tensor(1.5546e+08, device='cuda:0')
c= tensor(1.5546e+08, device='cuda:0')
c= tensor(1.5571e+08, device='cuda:0')
c= tensor(1.5579e+08, device='cuda:0')
c= tensor(1.5997e+08, device='cuda:0')
c= tensor(1.5997e+08, device='cuda:0')
c= tensor(1.6013e+08, device='cuda:0')
c= tensor(1.6013e+08, device='cuda:0')
c= tensor(1.6040e+08, device='cuda:0')
c= tensor(1.6438e+08, device='cuda:0')
c= tensor(1.6438e+08, device='cuda:0')
c= tensor(1.6438e+08, device='cuda:0')
c= tensor(1.6444e+08, device='cuda:0')
c= tensor(1.6444e+08, device='cuda:0')
c= tensor(1.6449e+08, device='cuda:0')
c= tensor(1.6450e+08, device='cuda:0')
c= tensor(1.6450e+08, device='cuda:0')
c= tensor(1.6451e+08, device='cuda:0')
c= tensor(1.6451e+08, device='cuda:0')
c= tensor(1.6458e+08, device='cuda:0')
c= tensor(1.6460e+08, device='cuda:0')
c= tensor(1.6460e+08, device='cuda:0')
c= tensor(1.6460e+08, device='cuda:0')
c= tensor(1.6493e+08, device='cuda:0')
c= tensor(1.6493e+08, device='cuda:0')
c= tensor(1.6675e+08, device='cuda:0')
c= tensor(1.6675e+08, device='cuda:0')
c= tensor(1.6683e+08, device='cuda:0')
c= tensor(1.6684e+08, device='cuda:0')
c= tensor(1.6690e+08, device='cuda:0')
c= tensor(1.6694e+08, device='cuda:0')
c= tensor(1.6694e+08, device='cuda:0')
c= tensor(1.6694e+08, device='cuda:0')
c= tensor(1.6694e+08, device='cuda:0')
c= tensor(1.6695e+08, device='cuda:0')
c= tensor(1.6720e+08, device='cuda:0')
c= tensor(1.6725e+08, device='cuda:0')
c= tensor(1.6725e+08, device='cuda:0')
c= tensor(1.6727e+08, device='cuda:0')
c= tensor(1.6729e+08, device='cuda:0')
c= tensor(1.6739e+08, device='cuda:0')
c= tensor(1.6745e+08, device='cuda:0')
c= tensor(1.6751e+08, device='cuda:0')
c= tensor(1.6762e+08, device='cuda:0')
c= tensor(1.6762e+08, device='cuda:0')
c= tensor(1.6762e+08, device='cuda:0')
c= tensor(1.6762e+08, device='cuda:0')
c= tensor(1.6763e+08, device='cuda:0')
c= tensor(1.6767e+08, device='cuda:0')
c= tensor(1.6767e+08, device='cuda:0')
c= tensor(1.6767e+08, device='cuda:0')
c= tensor(1.6767e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6782e+08, device='cuda:0')
c= tensor(1.6782e+08, device='cuda:0')
c= tensor(1.6836e+08, device='cuda:0')
c= tensor(1.6836e+08, device='cuda:0')
c= tensor(1.6836e+08, device='cuda:0')
c= tensor(1.6939e+08, device='cuda:0')
c= tensor(1.6944e+08, device='cuda:0')
c= tensor(1.6944e+08, device='cuda:0')
c= tensor(1.6979e+08, device='cuda:0')
c= tensor(1.6980e+08, device='cuda:0')
c= tensor(1.7028e+08, device='cuda:0')
c= tensor(1.7054e+08, device='cuda:0')
c= tensor(1.7054e+08, device='cuda:0')
c= tensor(1.7054e+08, device='cuda:0')
c= tensor(1.7055e+08, device='cuda:0')
c= tensor(1.7064e+08, device='cuda:0')
c= tensor(1.7138e+08, device='cuda:0')
c= tensor(1.7138e+08, device='cuda:0')
c= tensor(1.7138e+08, device='cuda:0')
c= tensor(1.7138e+08, device='cuda:0')
c= tensor(1.7138e+08, device='cuda:0')
c= tensor(1.7139e+08, device='cuda:0')
c= tensor(1.7139e+08, device='cuda:0')
c= tensor(1.7286e+08, device='cuda:0')
c= tensor(1.7626e+08, device='cuda:0')
c= tensor(1.7626e+08, device='cuda:0')
c= tensor(1.7627e+08, device='cuda:0')
c= tensor(1.7627e+08, device='cuda:0')
c= tensor(1.7636e+08, device='cuda:0')
c= tensor(1.7636e+08, device='cuda:0')
c= tensor(1.7636e+08, device='cuda:0')
c= tensor(1.7636e+08, device='cuda:0')
c= tensor(1.7641e+08, device='cuda:0')
c= tensor(1.7641e+08, device='cuda:0')
c= tensor(1.7642e+08, device='cuda:0')
c= tensor(1.7642e+08, device='cuda:0')
memory (bytes)
3145793536
time for making loss 2 is 15.525254249572754
p0 True
it  0 : 430988288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3146002432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  3% |
memory (bytes)
3146498048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  1026386300.0
relative error loss 5.8177886
shape of L is 
torch.Size([])
memory (bytes)
3428941824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  5% |
memory (bytes)
3429163008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  1026374140.0
relative error loss 5.81772
shape of L is 
torch.Size([])
memory (bytes)
3434577920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  5% |
memory (bytes)
3434713088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  6% |
error is  1026307900.0
relative error loss 5.817344
shape of L is 
torch.Size([])
memory (bytes)
3436810240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3436810240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  1025822850.0
relative error loss 5.8145947
shape of L is 
torch.Size([])
memory (bytes)
3438956544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  5% |
memory (bytes)
3438956544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  1023287900.0
relative error loss 5.800226
shape of L is 
torch.Size([])
memory (bytes)
3440984064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  5% |
memory (bytes)
3441037312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  995802700.0
relative error loss 5.644434
shape of L is 
torch.Size([])
memory (bytes)
3443081216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  5% |
memory (bytes)
3443081216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  868650750.0
relative error loss 4.923708
shape of L is 
torch.Size([])
memory (bytes)
3445190656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  6% |
memory (bytes)
3445248000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  6% |
error is  321592480.0
relative error loss 1.8228586
shape of L is 
torch.Size([])
memory (bytes)
3447201792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3447357440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  223528830.0
relative error loss 1.2670118
shape of L is 
torch.Size([])
memory (bytes)
3449458688
| ID | GPU | MEM |
------------------
|  0 | 13% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3449466880
| ID | GPU  | MEM |
-------------------
|  0 |  12% |  0% |
|  1 | 100% |  6% |
error is  152720210.0
relative error loss 0.86565256
time to take a step is 252.70538306236267
it  1 : 908357632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3451613184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3451613184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  152720210.0
relative error loss 0.86565256
shape of L is 
torch.Size([])
memory (bytes)
3453710336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3453710336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  1470553100.0
relative error loss 8.335426
shape of L is 
torch.Size([])
memory (bytes)
3455770624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3455856640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  136191410.0
relative error loss 0.7719636
shape of L is 
torch.Size([])
memory (bytes)
3457961984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  6% |
memory (bytes)
3457978368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  119104140.0
relative error loss 0.6751091
shape of L is 
torch.Size([])
memory (bytes)
3460079616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3460079616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  112012300.0
relative error loss 0.634911
shape of L is 
torch.Size([])
memory (bytes)
3462217728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  6% |
memory (bytes)
3462217728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  6% |
error is  106219410.0
relative error loss 0.6020755
shape of L is 
torch.Size([])
memory (bytes)
3464278016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3464331264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  100403010.0
relative error loss 0.5691069
shape of L is 
torch.Size([])
memory (bytes)
3466280960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3466280960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  92939510.0
relative error loss 0.52680206
shape of L is 
torch.Size([])
memory (bytes)
3468537856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3468537856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  84954780.0
relative error loss 0.48154286
shape of L is 
torch.Size([])
memory (bytes)
3470671872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3470725120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  6% |
error is  77590710.0
relative error loss 0.43980163
time to take a step is 234.83342123031616
it  2 : 908357632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3472637952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3472850944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  77590710.0
relative error loss 0.43980163
shape of L is 
torch.Size([])
memory (bytes)
3474833408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
3474833408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  71909960.0
relative error loss 0.40760186
shape of L is 
torch.Size([])
memory (bytes)
3476901888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3476901888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  66250610.0
relative error loss 0.37552336
shape of L is 
torch.Size([])
memory (bytes)
3479183360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
3479183360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  59058270.0
relative error loss 0.3347556
shape of L is 
torch.Size([])
memory (bytes)
3481145344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  6% |
memory (bytes)
3481145344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  6% |
error is  53673736.0
relative error loss 0.30423483
shape of L is 
torch.Size([])
memory (bytes)
3483250688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3483250688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  48315744.0
relative error loss 0.2738645
shape of L is 
torch.Size([])
memory (bytes)
3485368320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3485368320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  42877470.0
relative error loss 0.24303918
shape of L is 
torch.Size([])
memory (bytes)
3487657984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3487657984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  39198304.0
relative error loss 0.22218482
shape of L is 
torch.Size([])
memory (bytes)
3489583104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  6% |
memory (bytes)
3489796096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  34124944.0
relative error loss 0.19342788
shape of L is 
torch.Size([])
memory (bytes)
3491684352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
3491684352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  31578000.0
relative error loss 0.17899121
time to take a step is 211.12667798995972
it  3 : 908357632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3493744640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
3493744640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  31578000.0
relative error loss 0.17899121
shape of L is 
torch.Size([])
memory (bytes)
3495899136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
3495899136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  29015248.0
relative error loss 0.16446497
shape of L is 
torch.Size([])
memory (bytes)
3498037248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  6% |
memory (bytes)
3498037248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  26022064.0
relative error loss 0.14749892
shape of L is 
torch.Size([])
memory (bytes)
3500412928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3500470272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  6% |
error is  24097104.0
relative error loss 0.13658781
shape of L is 
torch.Size([])
memory (bytes)
3502571520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  6% |
memory (bytes)
3502571520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  22374704.0
relative error loss 0.12682486
shape of L is 
torch.Size([])
memory (bytes)
3504504832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  6% |
memory (bytes)
3504754688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  6% |
error is  20506128.0
relative error loss 0.11623336
shape of L is 
torch.Size([])
memory (bytes)
3506769920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
3506769920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  19045456.0
relative error loss 0.107953936
shape of L is 
torch.Size([])
memory (bytes)
3508879360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
3508879360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  17774432.0
relative error loss 0.100749485
shape of L is 
torch.Size([])
memory (bytes)
3510796288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  6% |
memory (bytes)
3510796288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  6% |
error is  16535392.0
relative error loss 0.09372633
shape of L is 
torch.Size([])
memory (bytes)
3513180160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
3513180160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  15647008.0
relative error loss 0.088690765
time to take a step is 119.23132014274597
c= tensor(116.4212, device='cuda:0')
c= tensor(4823.7012, device='cuda:0')
c= tensor(5164.6548, device='cuda:0')
c= tensor(5382.8174, device='cuda:0')
c= tensor(16013.9844, device='cuda:0')
c= tensor(16371.5205, device='cuda:0')
c= tensor(53956.0156, device='cuda:0')
c= tensor(57370.5117, device='cuda:0')
c= tensor(58030.6211, device='cuda:0')
c= tensor(69996.6406, device='cuda:0')
c= tensor(71714.3438, device='cuda:0')
c= tensor(572767.7500, device='cuda:0')
c= tensor(573979.9375, device='cuda:0')
c= tensor(604458.2500, device='cuda:0')
c= tensor(606481.3750, device='cuda:0')
c= tensor(607847.1250, device='cuda:0')
c= tensor(613577.1875, device='cuda:0')
c= tensor(706358.6875, device='cuda:0')
c= tensor(1082168.7500, device='cuda:0')
c= tensor(1232267.5000, device='cuda:0')
c= tensor(1232656.7500, device='cuda:0')
c= tensor(1347505.1250, device='cuda:0')
c= tensor(1348841.1250, device='cuda:0')
c= tensor(1352149.7500, device='cuda:0')
c= tensor(1368740.6250, device='cuda:0')
c= tensor(1462973.8750, device='cuda:0')
c= tensor(1553893.5000, device='cuda:0')
c= tensor(1562650., device='cuda:0')
c= tensor(2137494.2500, device='cuda:0')
c= tensor(6245687., device='cuda:0')
c= tensor(6246430., device='cuda:0')
c= tensor(11555976., device='cuda:0')
c= tensor(11557800., device='cuda:0')
c= tensor(11558100., device='cuda:0')
c= tensor(11558547., device='cuda:0')
c= tensor(11578198., device='cuda:0')
c= tensor(11620583., device='cuda:0')
c= tensor(11620952., device='cuda:0')
c= tensor(11622154., device='cuda:0')
c= tensor(11622613., device='cuda:0')
c= tensor(11622939., device='cuda:0')
c= tensor(11623074., device='cuda:0')
c= tensor(11623146., device='cuda:0')
c= tensor(11623357., device='cuda:0')
c= tensor(11623441., device='cuda:0')
c= tensor(11623549., device='cuda:0')
c= tensor(11624314., device='cuda:0')
c= tensor(11624822., device='cuda:0')
c= tensor(11625021., device='cuda:0')
c= tensor(11626928., device='cuda:0')
c= tensor(11627945., device='cuda:0')
c= tensor(11628910., device='cuda:0')
c= tensor(11629291., device='cuda:0')
c= tensor(11629689., device='cuda:0')
c= tensor(11631064., device='cuda:0')
c= tensor(11631846., device='cuda:0')
c= tensor(11632031., device='cuda:0')
c= tensor(11632269., device='cuda:0')
c= tensor(11632691., device='cuda:0')
c= tensor(11633006., device='cuda:0')
c= tensor(11636556., device='cuda:0')
c= tensor(11636687., device='cuda:0')
c= tensor(11639924., device='cuda:0')
c= tensor(11640379., device='cuda:0')
c= tensor(11640693., device='cuda:0')
c= tensor(11641105., device='cuda:0')
c= tensor(11641220., device='cuda:0')
c= tensor(11641427., device='cuda:0')
c= tensor(11641663., device='cuda:0')
c= tensor(11641814., device='cuda:0')
c= tensor(11641889., device='cuda:0')
c= tensor(11642265., device='cuda:0')
c= tensor(11642436., device='cuda:0')
c= tensor(11642576., device='cuda:0')
c= tensor(11643216., device='cuda:0')
c= tensor(11644277., device='cuda:0')
c= tensor(11644557., device='cuda:0')
c= tensor(11644662., device='cuda:0')
c= tensor(11644857., device='cuda:0')
c= tensor(11645359., device='cuda:0')
c= tensor(11646040., device='cuda:0')
c= tensor(11646162., device='cuda:0')
c= tensor(11647202., device='cuda:0')
c= tensor(11647313., device='cuda:0')
c= tensor(11647873., device='cuda:0')
c= tensor(11647953., device='cuda:0')
c= tensor(11648768., device='cuda:0')
c= tensor(11648927., device='cuda:0')
c= tensor(11649184., device='cuda:0')
c= tensor(11649783., device='cuda:0')
c= tensor(11649915., device='cuda:0')
c= tensor(11650041., device='cuda:0')
c= tensor(11650181., device='cuda:0')
c= tensor(11650852., device='cuda:0')
c= tensor(11651040., device='cuda:0')
c= tensor(11651689., device='cuda:0')
c= tensor(11652944., device='cuda:0')
c= tensor(11656859., device='cuda:0')
c= tensor(11657552., device='cuda:0')
c= tensor(11657908., device='cuda:0')
c= tensor(11661780., device='cuda:0')
c= tensor(11662486., device='cuda:0')
c= tensor(11662783., device='cuda:0')
c= tensor(11662921., device='cuda:0')
c= tensor(11663323., device='cuda:0')
c= tensor(11663466., device='cuda:0')
c= tensor(11664192., device='cuda:0')
c= tensor(11664242., device='cuda:0')
c= tensor(11664707., device='cuda:0')
c= tensor(11665029., device='cuda:0')
c= tensor(11665162., device='cuda:0')
c= tensor(11665232., device='cuda:0')
c= tensor(11665489., device='cuda:0')
c= tensor(11666135., device='cuda:0')
c= tensor(11666690., device='cuda:0')
c= tensor(11666824., device='cuda:0')
c= tensor(11666922., device='cuda:0')
c= tensor(11667118., device='cuda:0')
c= tensor(11668108., device='cuda:0')
c= tensor(11668356., device='cuda:0')
c= tensor(11669442., device='cuda:0')
c= tensor(11669750., device='cuda:0')
c= tensor(11670038., device='cuda:0')
c= tensor(11670937., device='cuda:0')
c= tensor(11671153., device='cuda:0')
c= tensor(11671558., device='cuda:0')
c= tensor(11671873., device='cuda:0')
c= tensor(11671924., device='cuda:0')
c= tensor(11674413., device='cuda:0')
c= tensor(11674667., device='cuda:0')
c= tensor(11674989., device='cuda:0')
c= tensor(11675171., device='cuda:0')
c= tensor(11675380., device='cuda:0')
c= tensor(11675502., device='cuda:0')
c= tensor(11675737., device='cuda:0')
c= tensor(11676051., device='cuda:0')
c= tensor(11676108., device='cuda:0')
c= tensor(11676259., device='cuda:0')
c= tensor(11676507., device='cuda:0')
c= tensor(11676979., device='cuda:0')
c= tensor(11677255., device='cuda:0')
c= tensor(11677560., device='cuda:0')
c= tensor(11678273., device='cuda:0')
c= tensor(11688080., device='cuda:0')
c= tensor(11688831., device='cuda:0')
c= tensor(11688965., device='cuda:0')
c= tensor(11689191., device='cuda:0')
c= tensor(11689630., device='cuda:0')
c= tensor(11689892., device='cuda:0')
c= tensor(11690090., device='cuda:0')
c= tensor(11690360., device='cuda:0')
c= tensor(11691553., device='cuda:0')
c= tensor(11692109., device='cuda:0')
c= tensor(11694203., device='cuda:0')
c= tensor(11694412., device='cuda:0')
c= tensor(11695916., device='cuda:0')
c= tensor(11696076., device='cuda:0')
c= tensor(11696252., device='cuda:0')
c= tensor(11696922., device='cuda:0')
c= tensor(11696956., device='cuda:0')
c= tensor(11698109., device='cuda:0')
c= tensor(11698210., device='cuda:0')
c= tensor(11698478., device='cuda:0')
c= tensor(11698587., device='cuda:0')
c= tensor(11698878., device='cuda:0')
c= tensor(11699227., device='cuda:0')
c= tensor(11699556., device='cuda:0')
c= tensor(11699694., device='cuda:0')
c= tensor(11700063., device='cuda:0')
c= tensor(11700114., device='cuda:0')
c= tensor(11700307., device='cuda:0')
c= tensor(11700612., device='cuda:0')
c= tensor(11700943., device='cuda:0')
c= tensor(11701156., device='cuda:0')
c= tensor(11702047., device='cuda:0')
c= tensor(11702258., device='cuda:0')
c= tensor(11702350., device='cuda:0')
c= tensor(11704360., device='cuda:0')
c= tensor(11704800., device='cuda:0')
c= tensor(11704981., device='cuda:0')
c= tensor(11705496., device='cuda:0')
c= tensor(11705665., device='cuda:0')
c= tensor(11706017., device='cuda:0')
c= tensor(11706128., device='cuda:0')
c= tensor(11707082., device='cuda:0')
c= tensor(11707925., device='cuda:0')
c= tensor(11708289., device='cuda:0')
c= tensor(11708524., device='cuda:0')
c= tensor(11709139., device='cuda:0')
c= tensor(11712793., device='cuda:0')
c= tensor(11712890., device='cuda:0')
c= tensor(11712986., device='cuda:0')
c= tensor(11713233., device='cuda:0')
c= tensor(11713430., device='cuda:0')
c= tensor(11713711., device='cuda:0')
c= tensor(11713968., device='cuda:0')
c= tensor(11714194., device='cuda:0')
c= tensor(11714299., device='cuda:0')
c= tensor(11714514., device='cuda:0')
c= tensor(11714552., device='cuda:0')
c= tensor(11715400., device='cuda:0')
c= tensor(11715516., device='cuda:0')
c= tensor(11716929., device='cuda:0')
c= tensor(11718419., device='cuda:0')
c= tensor(11718778., device='cuda:0')
c= tensor(11718816., device='cuda:0')
c= tensor(11719284., device='cuda:0')
c= tensor(11719692., device='cuda:0')
c= tensor(11720065., device='cuda:0')
c= tensor(11720952., device='cuda:0')
c= tensor(11724452., device='cuda:0')
c= tensor(11724543., device='cuda:0')
c= tensor(11724609., device='cuda:0')
c= tensor(11724666., device='cuda:0')
c= tensor(11724827., device='cuda:0')
c= tensor(11724967., device='cuda:0')
c= tensor(11725128., device='cuda:0')
c= tensor(11725209., device='cuda:0')
c= tensor(11726139., device='cuda:0')
c= tensor(11726364., device='cuda:0')
c= tensor(11726487., device='cuda:0')
c= tensor(11726909., device='cuda:0')
c= tensor(11727214., device='cuda:0')
c= tensor(11727433., device='cuda:0')
c= tensor(11728993., device='cuda:0')
c= tensor(11729545., device='cuda:0')
c= tensor(11729644., device='cuda:0')
c= tensor(11729774., device='cuda:0')
c= tensor(11730501., device='cuda:0')
c= tensor(11730596., device='cuda:0')
c= tensor(11730672., device='cuda:0')
c= tensor(11730854., device='cuda:0')
c= tensor(11731237., device='cuda:0')
c= tensor(11732305., device='cuda:0')
c= tensor(11732475., device='cuda:0')
c= tensor(11732534., device='cuda:0')
c= tensor(11732730., device='cuda:0')
c= tensor(11733074., device='cuda:0')
c= tensor(11733129., device='cuda:0')
c= tensor(11733661., device='cuda:0')
c= tensor(11865017., device='cuda:0')
c= tensor(11865362., device='cuda:0')
c= tensor(11866428., device='cuda:0')
c= tensor(11866509., device='cuda:0')
c= tensor(11867232., device='cuda:0')
c= tensor(11872920., device='cuda:0')
c= tensor(12009260., device='cuda:0')
c= tensor(12009371., device='cuda:0')
c= tensor(12013618., device='cuda:0')
c= tensor(12062422., device='cuda:0')
c= tensor(12062861., device='cuda:0')
c= tensor(12325567., device='cuda:0')
c= tensor(12325666., device='cuda:0')
c= tensor(12326611., device='cuda:0')
c= tensor(12382405., device='cuda:0')
c= tensor(12384808., device='cuda:0')
c= tensor(12385051., device='cuda:0')
c= tensor(12386875., device='cuda:0')
c= tensor(12407370., device='cuda:0')
c= tensor(12498600., device='cuda:0')
c= tensor(12501375., device='cuda:0')
c= tensor(12518290., device='cuda:0')
c= tensor(12534225., device='cuda:0')
c= tensor(12555806., device='cuda:0')
c= tensor(12556457., device='cuda:0')
c= tensor(13255791., device='cuda:0')
c= tensor(13255994., device='cuda:0')
c= tensor(13256299., device='cuda:0')
c= tensor(13265883., device='cuda:0')
c= tensor(13267285., device='cuda:0')
c= tensor(14625088., device='cuda:0')
c= tensor(14647079., device='cuda:0')
c= tensor(14647158., device='cuda:0')
c= tensor(14647777., device='cuda:0')
c= tensor(14650087., device='cuda:0')
c= tensor(14650234., device='cuda:0')
c= tensor(14763859., device='cuda:0')
c= tensor(14977509., device='cuda:0')
c= tensor(14978470., device='cuda:0')
c= tensor(14978519., device='cuda:0')
c= tensor(14979008., device='cuda:0')
c= tensor(14983765., device='cuda:0')
c= tensor(14991558., device='cuda:0')
c= tensor(14992593., device='cuda:0')
c= tensor(14992926., device='cuda:0')
c= tensor(17068498., device='cuda:0')
c= tensor(17069200., device='cuda:0')
c= tensor(17073820., device='cuda:0')
c= tensor(17205968., device='cuda:0')
c= tensor(17206192., device='cuda:0')
c= tensor(17208038., device='cuda:0')
c= tensor(17622414., device='cuda:0')
c= tensor(18115280., device='cuda:0')
c= tensor(18116272., device='cuda:0')
c= tensor(18116418., device='cuda:0')
c= tensor(18116722., device='cuda:0')
c= tensor(18116908., device='cuda:0')
c= tensor(18118102., device='cuda:0')
c= tensor(18120798., device='cuda:0')
c= tensor(18134232., device='cuda:0')
c= tensor(19761614., device='cuda:0')
c= tensor(19803406., device='cuda:0')
c= tensor(19804386., device='cuda:0')
c= tensor(19805616., device='cuda:0')
c= tensor(20937852., device='cuda:0')
c= tensor(20945740., device='cuda:0')
c= tensor(20947204., device='cuda:0')
c= tensor(20953700., device='cuda:0')
c= tensor(22518578., device='cuda:0')
c= tensor(22518890., device='cuda:0')
c= tensor(22545316., device='cuda:0')
c= tensor(22545870., device='cuda:0')
c= tensor(22547608., device='cuda:0')
c= tensor(22556010., device='cuda:0')
c= tensor(23024794., device='cuda:0')
c= tensor(23077302., device='cuda:0')
c= tensor(23077684., device='cuda:0')
c= tensor(23127244., device='cuda:0')
c= tensor(23174046., device='cuda:0')
c= tensor(23174504., device='cuda:0')
c= tensor(23175402., device='cuda:0')
c= tensor(23311970., device='cuda:0')
c= tensor(24204828., device='cuda:0')
c= tensor(24231686., device='cuda:0')
c= tensor(24232234., device='cuda:0')
c= tensor(24232508., device='cuda:0')
c= tensor(24232604., device='cuda:0')
c= tensor(24241276., device='cuda:0')
c= tensor(24241608., device='cuda:0')
c= tensor(24241874., device='cuda:0')
c= tensor(24284478., device='cuda:0')
c= tensor(24379574., device='cuda:0')
c= tensor(24383612., device='cuda:0')
c= tensor(24384010., device='cuda:0')
c= tensor(24384982., device='cuda:0')
c= tensor(24387120., device='cuda:0')
c= tensor(24390074., device='cuda:0')
c= tensor(24392658., device='cuda:0')
c= tensor(24392860., device='cuda:0')
c= tensor(26209318., device='cuda:0')
c= tensor(26210178., device='cuda:0')
c= tensor(26216006., device='cuda:0')
c= tensor(26263678., device='cuda:0')
c= tensor(26264346., device='cuda:0')
c= tensor(26290382., device='cuda:0')
c= tensor(26291644., device='cuda:0')
c= tensor(26306214., device='cuda:0')
c= tensor(26306368., device='cuda:0')
c= tensor(26306624., device='cuda:0')
c= tensor(26307228., device='cuda:0')
c= tensor(26311412., device='cuda:0')
c= tensor(26313372., device='cuda:0')
c= tensor(26326258., device='cuda:0')
c= tensor(26326666., device='cuda:0')
c= tensor(26327368., device='cuda:0')
c= tensor(26985498., device='cuda:0')
c= tensor(26993672., device='cuda:0')
c= tensor(27015296., device='cuda:0')
c= tensor(27313232., device='cuda:0')
c= tensor(27382182., device='cuda:0')
c= tensor(27383234., device='cuda:0')
c= tensor(27383592., device='cuda:0')
c= tensor(27393324., device='cuda:0')
c= tensor(27393380., device='cuda:0')
c= tensor(27393792., device='cuda:0')
c= tensor(27395094., device='cuda:0')
c= tensor(27395484., device='cuda:0')
c= tensor(27395842., device='cuda:0')
c= tensor(27396022., device='cuda:0')
c= tensor(27396474., device='cuda:0')
c= tensor(28358224., device='cuda:0')
c= tensor(28365946., device='cuda:0')
c= tensor(28377822., device='cuda:0')
c= tensor(28415386., device='cuda:0')
c= tensor(28415526., device='cuda:0')
c= tensor(28415942., device='cuda:0')
c= tensor(34238176., device='cuda:0')
c= tensor(34875964., device='cuda:0')
c= tensor(34878848., device='cuda:0')
c= tensor(34882016., device='cuda:0')
c= tensor(34882240., device='cuda:0')
c= tensor(34934280., device='cuda:0')
c= tensor(35109404., device='cuda:0')
c= tensor(35280852., device='cuda:0')
c= tensor(35283700., device='cuda:0')
c= tensor(35325824., device='cuda:0')
c= tensor(35386252., device='cuda:0')
c= tensor(35387616., device='cuda:0')
c= tensor(35387960., device='cuda:0')
c= tensor(35390292., device='cuda:0')
c= tensor(35395220., device='cuda:0')
c= tensor(35395560., device='cuda:0')
c= tensor(35466712., device='cuda:0')
c= tensor(35473432., device='cuda:0')
c= tensor(35473528., device='cuda:0')
c= tensor(35474880., device='cuda:0')
c= tensor(35478476., device='cuda:0')
c= tensor(35478564., device='cuda:0')
c= tensor(35678608., device='cuda:0')
c= tensor(35827760., device='cuda:0')
c= tensor(35913204., device='cuda:0')
c= tensor(36001932., device='cuda:0')
c= tensor(36105708., device='cuda:0')
c= tensor(36106920., device='cuda:0')
c= tensor(36108208., device='cuda:0')
c= tensor(36112908., device='cuda:0')
c= tensor(36145000., device='cuda:0')
c= tensor(36145140., device='cuda:0')
c= tensor(36190456., device='cuda:0')
c= tensor(38305000., device='cuda:0')
c= tensor(38314296., device='cuda:0')
c= tensor(38330752., device='cuda:0')
c= tensor(40076008., device='cuda:0')
c= tensor(40077008., device='cuda:0')
c= tensor(40077084., device='cuda:0')
c= tensor(40092972., device='cuda:0')
c= tensor(40096196., device='cuda:0')
c= tensor(40098056., device='cuda:0')
c= tensor(43462808., device='cuda:0')
c= tensor(44245704., device='cuda:0')
c= tensor(44318200., device='cuda:0')
c= tensor(44333772., device='cuda:0')
c= tensor(44365768., device='cuda:0')
c= tensor(44365908., device='cuda:0')
c= tensor(44366280., device='cuda:0')
c= tensor(45219128., device='cuda:0')
c= tensor(45219500., device='cuda:0')
c= tensor(45219752., device='cuda:0')
c= tensor(45224256., device='cuda:0')
c= tensor(45264708., device='cuda:0')
c= tensor(45285676., device='cuda:0')
c= tensor(45307268., device='cuda:0')
c= tensor(45307880., device='cuda:0')
c= tensor(45318540., device='cuda:0')
c= tensor(45319072., device='cuda:0')
c= tensor(45325724., device='cuda:0')
c= tensor(45332076., device='cuda:0')
c= tensor(45334564., device='cuda:0')
c= tensor(45334740., device='cuda:0')
c= tensor(45346452., device='cuda:0')
c= tensor(45346580., device='cuda:0')
c= tensor(45355684., device='cuda:0')
c= tensor(45356432., device='cuda:0')
c= tensor(45357504., device='cuda:0')
c= tensor(45357920., device='cuda:0')
c= tensor(45359756., device='cuda:0')
c= tensor(45360172., device='cuda:0')
c= tensor(45366088., device='cuda:0')
c= tensor(45503444., device='cuda:0')
c= tensor(45527052., device='cuda:0')
c= tensor(45528152., device='cuda:0')
c= tensor(45528624., device='cuda:0')
c= tensor(46049600., device='cuda:0')
c= tensor(46050004., device='cuda:0')
c= tensor(48567516., device='cuda:0')
c= tensor(48567568., device='cuda:0')
c= tensor(48572400., device='cuda:0')
c= tensor(48574028., device='cuda:0')
c= tensor(48574460., device='cuda:0')
c= tensor(48575580., device='cuda:0')
c= tensor(48594072., device='cuda:0')
c= tensor(50843312., device='cuda:0')
c= tensor(50843996., device='cuda:0')
c= tensor(50848624., device='cuda:0')
c= tensor(50848856., device='cuda:0')
c= tensor(50849596., device='cuda:0')
c= tensor(50851876., device='cuda:0')
c= tensor(50854312., device='cuda:0')
c= tensor(50855196., device='cuda:0')
c= tensor(50856780., device='cuda:0')
c= tensor(50860480., device='cuda:0')
c= tensor(50860576., device='cuda:0')
c= tensor(50861484., device='cuda:0')
c= tensor(51402348., device='cuda:0')
c= tensor(51403616., device='cuda:0')
c= tensor(51423288., device='cuda:0')
c= tensor(51557984., device='cuda:0')
c= tensor(51559560., device='cuda:0')
c= tensor(51559748., device='cuda:0')
c= tensor(51559948., device='cuda:0')
c= tensor(51762064., device='cuda:0')
c= tensor(51763684., device='cuda:0')
c= tensor(51766976., device='cuda:0')
c= tensor(51773464., device='cuda:0')
c= tensor(51995380., device='cuda:0')
c= tensor(51995832., device='cuda:0')
c= tensor(51996480., device='cuda:0')
c= tensor(56114784., device='cuda:0')
c= tensor(56115088., device='cuda:0')
c= tensor(56116032., device='cuda:0')
c= tensor(56116904., device='cuda:0')
c= tensor(56188684., device='cuda:0')
c= tensor(56296460., device='cuda:0')
c= tensor(56302324., device='cuda:0')
c= tensor(56309740., device='cuda:0')
c= tensor(56310552., device='cuda:0')
c= tensor(56369636., device='cuda:0')
c= tensor(56370476., device='cuda:0')
c= tensor(56371232., device='cuda:0')
c= tensor(56380456., device='cuda:0')
c= tensor(56381128., device='cuda:0')
c= tensor(56446512., device='cuda:0')
c= tensor(56446736., device='cuda:0')
c= tensor(56446796., device='cuda:0')
c= tensor(56447880., device='cuda:0')
c= tensor(56449672., device='cuda:0')
c= tensor(56652404., device='cuda:0')
c= tensor(56719016., device='cuda:0')
c= tensor(56722620., device='cuda:0')
c= tensor(56722816., device='cuda:0')
c= tensor(56732468., device='cuda:0')
c= tensor(56732680., device='cuda:0')
c= tensor(56733332., device='cuda:0')
c= tensor(56735260., device='cuda:0')
c= tensor(57048188., device='cuda:0')
c= tensor(57049100., device='cuda:0')
c= tensor(57049200., device='cuda:0')
c= tensor(57049356., device='cuda:0')
c= tensor(58874160., device='cuda:0')
c= tensor(60896212., device='cuda:0')
c= tensor(60896668., device='cuda:0')
c= tensor(60897524., device='cuda:0')
c= tensor(60898036., device='cuda:0')
c= tensor(60925868., device='cuda:0')
c= tensor(60926292., device='cuda:0')
c= tensor(60935224., device='cuda:0')
c= tensor(60935440., device='cuda:0')
c= tensor(61418464., device='cuda:0')
c= tensor(61426304., device='cuda:0')
c= tensor(61691300., device='cuda:0')
c= tensor(61691776., device='cuda:0')
c= tensor(61692856., device='cuda:0')
c= tensor(61692964., device='cuda:0')
c= tensor(61701992., device='cuda:0')
c= tensor(61703688., device='cuda:0')
c= tensor(61778524., device='cuda:0')
c= tensor(62050024., device='cuda:0')
c= tensor(63322492., device='cuda:0')
c= tensor(63323272., device='cuda:0')
c= tensor(63323544., device='cuda:0')
c= tensor(63330036., device='cuda:0')
c= tensor(63425312., device='cuda:0')
c= tensor(63426960., device='cuda:0')
c= tensor(63427068., device='cuda:0')
c= tensor(63427252., device='cuda:0')
c= tensor(63428264., device='cuda:0')
c= tensor(63428380., device='cuda:0')
c= tensor(63515528., device='cuda:0')
c= tensor(63515976., device='cuda:0')
c= tensor(63519336., device='cuda:0')
c= tensor(63520344., device='cuda:0')
c= tensor(63520608., device='cuda:0')
c= tensor(63520852., device='cuda:0')
c= tensor(63656120., device='cuda:0')
c= tensor(66646392., device='cuda:0')
c= tensor(66776308., device='cuda:0')
c= tensor(66791468., device='cuda:0')
c= tensor(66804612., device='cuda:0')
c= tensor(66806664., device='cuda:0')
c= tensor(66807796., device='cuda:0')
c= tensor(66824652., device='cuda:0')
c= tensor(66832148., device='cuda:0')
c= tensor(66869064., device='cuda:0')
c= tensor(66878616., device='cuda:0')
c= tensor(67581856., device='cuda:0')
c= tensor(67582736., device='cuda:0')
c= tensor(67593328., device='cuda:0')
c= tensor(68005704., device='cuda:0')
c= tensor(68020936., device='cuda:0')
c= tensor(68023816., device='cuda:0')
c= tensor(68027072., device='cuda:0')
c= tensor(68028080., device='cuda:0')
c= tensor(68028904., device='cuda:0')
c= tensor(68029616., device='cuda:0')
c= tensor(68037488., device='cuda:0')
c= tensor(68039216., device='cuda:0')
c= tensor(68047664., device='cuda:0')
c= tensor(70690584., device='cuda:0')
c= tensor(71140512., device='cuda:0')
c= tensor(71272208., device='cuda:0')
c= tensor(71275136., device='cuda:0')
c= tensor(71281776., device='cuda:0')
c= tensor(71295616., device='cuda:0')
c= tensor(71303152., device='cuda:0')
c= tensor(71467352., device='cuda:0')
c= tensor(71600216., device='cuda:0')
c= tensor(71600992., device='cuda:0')
c= tensor(79394936., device='cuda:0')
c= tensor(80318936., device='cuda:0')
c= tensor(80321480., device='cuda:0')
c= tensor(80321736., device='cuda:0')
c= tensor(80323016., device='cuda:0')
c= tensor(80341432., device='cuda:0')
c= tensor(80341864., device='cuda:0')
c= tensor(80648792., device='cuda:0')
c= tensor(80659848., device='cuda:0')
c= tensor(80662672., device='cuda:0')
c= tensor(80663736., device='cuda:0')
c= tensor(80663888., device='cuda:0')
c= tensor(80664360., device='cuda:0')
c= tensor(80666232., device='cuda:0')
c= tensor(80666520., device='cuda:0')
c= tensor(80667832., device='cuda:0')
c= tensor(1.5545e+08, device='cuda:0')
c= tensor(1.5545e+08, device='cuda:0')
c= tensor(1.5546e+08, device='cuda:0')
c= tensor(1.5546e+08, device='cuda:0')
c= tensor(1.5546e+08, device='cuda:0')
c= tensor(1.5546e+08, device='cuda:0')
c= tensor(1.5571e+08, device='cuda:0')
c= tensor(1.5579e+08, device='cuda:0')
c= tensor(1.5997e+08, device='cuda:0')
c= tensor(1.5997e+08, device='cuda:0')
c= tensor(1.6013e+08, device='cuda:0')
c= tensor(1.6013e+08, device='cuda:0')
c= tensor(1.6040e+08, device='cuda:0')
c= tensor(1.6438e+08, device='cuda:0')
c= tensor(1.6438e+08, device='cuda:0')
c= tensor(1.6438e+08, device='cuda:0')
c= tensor(1.6444e+08, device='cuda:0')
c= tensor(1.6444e+08, device='cuda:0')
c= tensor(1.6449e+08, device='cuda:0')
c= tensor(1.6450e+08, device='cuda:0')
c= tensor(1.6450e+08, device='cuda:0')
c= tensor(1.6451e+08, device='cuda:0')
c= tensor(1.6451e+08, device='cuda:0')
c= tensor(1.6458e+08, device='cuda:0')
c= tensor(1.6460e+08, device='cuda:0')
c= tensor(1.6460e+08, device='cuda:0')
c= tensor(1.6460e+08, device='cuda:0')
c= tensor(1.6493e+08, device='cuda:0')
c= tensor(1.6493e+08, device='cuda:0')
c= tensor(1.6675e+08, device='cuda:0')
c= tensor(1.6675e+08, device='cuda:0')
c= tensor(1.6683e+08, device='cuda:0')
c= tensor(1.6684e+08, device='cuda:0')
c= tensor(1.6690e+08, device='cuda:0')
c= tensor(1.6694e+08, device='cuda:0')
c= tensor(1.6694e+08, device='cuda:0')
c= tensor(1.6694e+08, device='cuda:0')
c= tensor(1.6694e+08, device='cuda:0')
c= tensor(1.6695e+08, device='cuda:0')
c= tensor(1.6720e+08, device='cuda:0')
c= tensor(1.6725e+08, device='cuda:0')
c= tensor(1.6725e+08, device='cuda:0')
c= tensor(1.6727e+08, device='cuda:0')
c= tensor(1.6729e+08, device='cuda:0')
c= tensor(1.6739e+08, device='cuda:0')
c= tensor(1.6745e+08, device='cuda:0')
c= tensor(1.6751e+08, device='cuda:0')
c= tensor(1.6762e+08, device='cuda:0')
c= tensor(1.6762e+08, device='cuda:0')
c= tensor(1.6762e+08, device='cuda:0')
c= tensor(1.6762e+08, device='cuda:0')
c= tensor(1.6763e+08, device='cuda:0')
c= tensor(1.6767e+08, device='cuda:0')
c= tensor(1.6767e+08, device='cuda:0')
c= tensor(1.6767e+08, device='cuda:0')
c= tensor(1.6767e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6772e+08, device='cuda:0')
c= tensor(1.6782e+08, device='cuda:0')
c= tensor(1.6782e+08, device='cuda:0')
c= tensor(1.6836e+08, device='cuda:0')
c= tensor(1.6836e+08, device='cuda:0')
c= tensor(1.6836e+08, device='cuda:0')
c= tensor(1.6939e+08, device='cuda:0')
c= tensor(1.6944e+08, device='cuda:0')
c= tensor(1.6944e+08, device='cuda:0')
c= tensor(1.6979e+08, device='cuda:0')
c= tensor(1.6980e+08, device='cuda:0')
c= tensor(1.7028e+08, device='cuda:0')
c= tensor(1.7054e+08, device='cuda:0')
c= tensor(1.7054e+08, device='cuda:0')
c= tensor(1.7054e+08, device='cuda:0')
c= tensor(1.7055e+08, device='cuda:0')
c= tensor(1.7064e+08, device='cuda:0')
c= tensor(1.7138e+08, device='cuda:0')
c= tensor(1.7138e+08, device='cuda:0')
c= tensor(1.7138e+08, device='cuda:0')
c= tensor(1.7138e+08, device='cuda:0')
c= tensor(1.7138e+08, device='cuda:0')
c= tensor(1.7139e+08, device='cuda:0')
c= tensor(1.7139e+08, device='cuda:0')
c= tensor(1.7286e+08, device='cuda:0')
c= tensor(1.7626e+08, device='cuda:0')
c= tensor(1.7626e+08, device='cuda:0')
c= tensor(1.7627e+08, device='cuda:0')
c= tensor(1.7627e+08, device='cuda:0')
c= tensor(1.7636e+08, device='cuda:0')
c= tensor(1.7636e+08, device='cuda:0')
c= tensor(1.7636e+08, device='cuda:0')
c= tensor(1.7636e+08, device='cuda:0')
c= tensor(1.7641e+08, device='cuda:0')
c= tensor(1.7641e+08, device='cuda:0')
c= tensor(1.7642e+08, device='cuda:0')
c= tensor(1.7642e+08, device='cuda:0')
time to make c is 12.110197067260742
time for making loss is 12.110219955444336
p0 True
it  0 : 431215616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
shape of L is 
torch.Size([])
memory (bytes)
3515510784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3515666432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  5% |
error is  15647008.0
relative error loss 0.088690765
shape of L is 
torch.Size([])
memory (bytes)
3541860352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3542118400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  5% |
error is  14983920.0
relative error loss 0.08493223
shape of L is 
torch.Size([])
memory (bytes)
3545735168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3545735168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  14128704.0
relative error loss 0.080084674
shape of L is 
torch.Size([])
memory (bytes)
3548917760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3548917760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  13690944.0
relative error loss 0.077603355
shape of L is 
torch.Size([])
memory (bytes)
3552096256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3552096256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  13275248.0
relative error loss 0.075247094
shape of L is 
torch.Size([])
memory (bytes)
3555405824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3555405824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  12987696.0
relative error loss 0.07361719
shape of L is 
torch.Size([])
memory (bytes)
3558502400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3558502400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  12753984.0
relative error loss 0.072292455
shape of L is 
torch.Size([])
memory (bytes)
3561775104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3561775104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  12565408.0
relative error loss 0.071223564
shape of L is 
torch.Size([])
memory (bytes)
3565060096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  6% |
memory (bytes)
3565060096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  12402320.0
relative error loss 0.07029914
shape of L is 
torch.Size([])
memory (bytes)
3568029696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  6% |
memory (bytes)
3568287744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  12277632.0
relative error loss 0.06959239
time to take a step is 287.98722887039185
it  1 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3571511296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3571511296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  12277632.0
relative error loss 0.06959239
shape of L is 
torch.Size([])
memory (bytes)
3574599680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3574599680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  12182192.0
relative error loss 0.06905141
shape of L is 
torch.Size([])
memory (bytes)
3577827328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  6% |
memory (bytes)
3577962496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  12102384.0
relative error loss 0.06859904
shape of L is 
torch.Size([])
memory (bytes)
3581186048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  6% |
memory (bytes)
3581186048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  12034016.0
relative error loss 0.06821151
shape of L is 
torch.Size([])
memory (bytes)
3584282624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3584417792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11953792.0
relative error loss 0.06775679
shape of L is 
torch.Size([])
memory (bytes)
3587497984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3587649536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11922512.0
relative error loss 0.067579485
shape of L is 
torch.Size([])
memory (bytes)
3590881280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3590885376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11858368.0
relative error loss 0.067215905
shape of L is 
torch.Size([])
memory (bytes)
3593867264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3593867264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11809232.0
relative error loss 0.06693739
shape of L is 
torch.Size([])
memory (bytes)
3597328384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  6% |
memory (bytes)
3597328384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11758592.0
relative error loss 0.066650346
shape of L is 
torch.Size([])
memory (bytes)
3600564224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  6% |
memory (bytes)
3600564224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11716944.0
relative error loss 0.066414274
time to take a step is 291.5674321651459
it  2 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  6% |  0% |
|  1 |  6% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3603800064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3603800064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11716944.0
relative error loss 0.066414274
shape of L is 
torch.Size([])
memory (bytes)
3606982656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3607023616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11623056.0
relative error loss 0.0658821
shape of L is 
torch.Size([])
memory (bytes)
3610218496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3610218496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11581328.0
relative error loss 0.065645576
shape of L is 
torch.Size([])
memory (bytes)
3613417472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3613474816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11540384.0
relative error loss 0.0654135
shape of L is 
torch.Size([])
memory (bytes)
3616694272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3616694272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11481184.0
relative error loss 0.06507794
shape of L is 
torch.Size([])
memory (bytes)
3619885056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3619885056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11459456.0
relative error loss 0.06495478
shape of L is 
torch.Size([])
memory (bytes)
3623124992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3623149568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11429824.0
relative error loss 0.064786814
shape of L is 
torch.Size([])
memory (bytes)
3626373120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  6% |
memory (bytes)
3626373120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11415104.0
relative error loss 0.06470338
shape of L is 
torch.Size([])
memory (bytes)
3629572096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  6% |
memory (bytes)
3629572096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11381824.0
relative error loss 0.06451474
shape of L is 
torch.Size([])
memory (bytes)
3632754688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3632824320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11351296.0
relative error loss 0.0643417
time to take a step is 394.48152446746826
it  3 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3635957760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3635957760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  6% |
error is  11351296.0
relative error loss 0.0643417
shape of L is 
torch.Size([])
memory (bytes)
3639259136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3639259136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11322144.0
relative error loss 0.06417646
shape of L is 
torch.Size([])
memory (bytes)
3642494976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
3642494976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11302912.0
relative error loss 0.06406745
shape of L is 
torch.Size([])
memory (bytes)
3645673472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3645673472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11276896.0
relative error loss 0.063919984
shape of L is 
torch.Size([])
memory (bytes)
3648942080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3648942080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11252016.0
relative error loss 0.06377896
shape of L is 
torch.Size([])
memory (bytes)
3652177920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
3652177920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11232144.0
relative error loss 0.06366632
shape of L is 
torch.Size([])
memory (bytes)
3655397376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3655397376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11222784.0
relative error loss 0.063613266
shape of L is 
torch.Size([])
memory (bytes)
3658629120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3658629120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11212608.0
relative error loss 0.06355559
shape of L is 
torch.Size([])
memory (bytes)
3661860864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3661860864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11194400.0
relative error loss 0.06345238
shape of L is 
torch.Size([])
memory (bytes)
3665080320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3665084416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11195904.0
relative error loss 0.06346091
shape of L is 
torch.Size([])
memory (bytes)
3668312064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  6% |
memory (bytes)
3668312064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  11184384.0
relative error loss 0.063395604
time to take a step is 434.0233874320984
it  4 : 910626816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3671429120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  6% |
memory (bytes)
3671429120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11184384.0
relative error loss 0.063395604
shape of L is 
torch.Size([])
memory (bytes)
3674750976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3674755072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11171184.0
relative error loss 0.063320786
shape of L is 
torch.Size([])
memory (bytes)
3677908992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3677908992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11154720.0
relative error loss 0.06322747
shape of L is 
torch.Size([])
memory (bytes)
3681230848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3681230848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11142208.0
relative error loss 0.063156545
shape of L is 
torch.Size([])
memory (bytes)
3684450304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3684450304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11128400.0
relative error loss 0.06307828
shape of L is 
torch.Size([])
memory (bytes)
3687682048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3687682048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11117408.0
relative error loss 0.063015975
shape of L is 
torch.Size([])
memory (bytes)
3690774528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3690905600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11106576.0
relative error loss 0.062954575
shape of L is 
torch.Size([])
memory (bytes)
3694133248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3694133248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11096112.0
relative error loss 0.06289526
shape of L is 
torch.Size([])
memory (bytes)
3697344512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3697344512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11090448.0
relative error loss 0.062863156
shape of L is 
torch.Size([])
memory (bytes)
3700506624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3700506624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11082256.0
relative error loss 0.062816724
time to take a step is 373.3127009868622
it  5 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3703816192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3703816192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11082256.0
relative error loss 0.062816724
shape of L is 
torch.Size([])
memory (bytes)
3707015168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  6% |
memory (bytes)
3707015168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11076528.0
relative error loss 0.062784255
shape of L is 
torch.Size([])
memory (bytes)
3710267392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3710267392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11071504.0
relative error loss 0.06275578
shape of L is 
torch.Size([])
memory (bytes)
3713380352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  6% |
memory (bytes)
3713380352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11064288.0
relative error loss 0.062714875
shape of L is 
torch.Size([])
memory (bytes)
3716718592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3716718592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11059264.0
relative error loss 0.0626864
shape of L is 
torch.Size([])
memory (bytes)
3719860224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  6% |
memory (bytes)
3719860224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11052496.0
relative error loss 0.062648036
shape of L is 
torch.Size([])
memory (bytes)
3723169792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3723169792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11042800.0
relative error loss 0.06259308
shape of L is 
torch.Size([])
memory (bytes)
3726262272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3726262272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11037920.0
relative error loss 0.062565416
shape of L is 
torch.Size([])
memory (bytes)
3729616896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3729616896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11030832.0
relative error loss 0.06252524
shape of L is 
torch.Size([])
memory (bytes)
3732697088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3732697088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11027600.0
relative error loss 0.06250692
time to take a step is 373.16099286079407
it  6 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3736068096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  6% |
memory (bytes)
3736068096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11027600.0
relative error loss 0.06250692
shape of L is 
torch.Size([])
memory (bytes)
3739156480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3739156480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11018720.0
relative error loss 0.062456585
shape of L is 
torch.Size([])
memory (bytes)
3742511104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3742511104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11014592.0
relative error loss 0.062433187
shape of L is 
torch.Size([])
memory (bytes)
3745619968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3745619968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  6% |
error is  11011040.0
relative error loss 0.062413055
shape of L is 
torch.Size([])
memory (bytes)
3748966400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  6% |
memory (bytes)
3748966400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  11004320.0
relative error loss 0.062374964
shape of L is 
torch.Size([])
memory (bytes)
3752206336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  6% |
memory (bytes)
3752206336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11002880.0
relative error loss 0.062366802
shape of L is 
torch.Size([])
memory (bytes)
3755421696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3755421696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10994592.0
relative error loss 0.062319823
shape of L is 
torch.Size([])
memory (bytes)
3758641152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3758641152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10992176.0
relative error loss 0.06230613
shape of L is 
torch.Size([])
memory (bytes)
3761864704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3761868800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10985504.0
relative error loss 0.062268313
shape of L is 
torch.Size([])
memory (bytes)
3765080064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3765080064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10980144.0
relative error loss 0.06223793
time to take a step is 382.0563774108887
it  7 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3768307712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3768307712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10980144.0
relative error loss 0.06223793
shape of L is 
torch.Size([])
memory (bytes)
3771510784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3771510784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10975328.0
relative error loss 0.06221063
shape of L is 
torch.Size([])
memory (bytes)
3774738432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  6% |
memory (bytes)
3774738432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10971824.0
relative error loss 0.06219077
shape of L is 
torch.Size([])
memory (bytes)
3777830912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3777966080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10967008.0
relative error loss 0.062163472
shape of L is 
torch.Size([])
memory (bytes)
3781107712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3781189632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10961712.0
relative error loss 0.062133454
shape of L is 
torch.Size([])
memory (bytes)
3784413184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  6% |
memory (bytes)
3784417280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  10958800.0
relative error loss 0.062116947
shape of L is 
torch.Size([])
memory (bytes)
3787640832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3787640832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10955504.0
relative error loss 0.062098265
shape of L is 
torch.Size([])
memory (bytes)
3790860288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3790860288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10953632.0
relative error loss 0.062087655
shape of L is 
torch.Size([])
memory (bytes)
3794030592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  6% |
memory (bytes)
3794030592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10950720.0
relative error loss 0.06207115
shape of L is 
torch.Size([])
memory (bytes)
3797319680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3797319680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10949104.0
relative error loss 0.062061988
time to take a step is 384.05197072029114
it  8 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3800526848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3800526848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10949104.0
relative error loss 0.062061988
shape of L is 
torch.Size([])
memory (bytes)
3803762688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3803762688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10944896.0
relative error loss 0.062038135
shape of L is 
torch.Size([])
memory (bytes)
3806998528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3806998528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10942288.0
relative error loss 0.062023353
shape of L is 
torch.Size([])
memory (bytes)
3810222080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  6% |
memory (bytes)
3810222080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10939808.0
relative error loss 0.062009297
shape of L is 
torch.Size([])
memory (bytes)
3813371904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3813371904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10935504.0
relative error loss 0.0619849
shape of L is 
torch.Size([])
memory (bytes)
3816665088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  6% |
memory (bytes)
3816665088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10932192.0
relative error loss 0.06196613
shape of L is 
torch.Size([])
memory (bytes)
3819892736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3819892736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10928784.0
relative error loss 0.06194681
shape of L is 
torch.Size([])
memory (bytes)
3823104000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3823104000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10927216.0
relative error loss 0.06193792
shape of L is 
torch.Size([])
memory (bytes)
3826327552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
3826327552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10922848.0
relative error loss 0.061913162
shape of L is 
torch.Size([])
memory (bytes)
3829551104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  6% |
memory (bytes)
3829551104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10923600.0
relative error loss 0.061917424
shape of L is 
torch.Size([])
memory (bytes)
3832647680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3832782848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10921168.0
relative error loss 0.06190364
time to take a step is 431.24892234802246
it  9 : 910626816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3836022784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  6% |
memory (bytes)
3836022784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10921168.0
relative error loss 0.06190364
shape of L is 
torch.Size([])
memory (bytes)
3839229952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  6% |
memory (bytes)
3839229952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10919632.0
relative error loss 0.061894935
shape of L is 
torch.Size([])
memory (bytes)
3842457600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3842473984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10915648.0
relative error loss 0.061872352
shape of L is 
torch.Size([])
memory (bytes)
3845697536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
3845697536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  10916208.0
relative error loss 0.061875526
shape of L is 
torch.Size([])
memory (bytes)
3848781824
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3848921088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10914192.0
relative error loss 0.0618641
shape of L is 
torch.Size([])
memory (bytes)
3852165120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3852165120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10912896.0
relative error loss 0.061856754
shape of L is 
torch.Size([])
memory (bytes)
3855388672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3855388672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10911584.0
relative error loss 0.061849315
shape of L is 
torch.Size([])
memory (bytes)
3858612224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3858612224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10910496.0
relative error loss 0.06184315
shape of L is 
torch.Size([])
memory (bytes)
3861848064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3861848064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10907952.0
relative error loss 0.06182873
shape of L is 
torch.Size([])
memory (bytes)
3865055232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3865055232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10905952.0
relative error loss 0.061817393
time to take a step is 417.99087357521057
it  10 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3868282880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  6% |
memory (bytes)
3868282880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10905952.0
relative error loss 0.061817393
shape of L is 
torch.Size([])
memory (bytes)
3871494144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3871494144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10903232.0
relative error loss 0.061801974
shape of L is 
torch.Size([])
memory (bytes)
3874648064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  6% |
memory (bytes)
3874648064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10901008.0
relative error loss 0.061789367
shape of L is 
torch.Size([])
memory (bytes)
3877949440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
3877949440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10898880.0
relative error loss 0.06177731
shape of L is 
torch.Size([])
memory (bytes)
3881172992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
3881172992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10899136.0
relative error loss 0.061778758
shape of L is 
torch.Size([])
memory (bytes)
3884392448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
3884392448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10897536.0
relative error loss 0.06176969
shape of L is 
torch.Size([])
memory (bytes)
3887624192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3887624192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10895584.0
relative error loss 0.061758626
shape of L is 
torch.Size([])
memory (bytes)
3890704384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3890843648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10893824.0
relative error loss 0.06174865
shape of L is 
torch.Size([])
memory (bytes)
3894071296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3894071296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10892464.0
relative error loss 0.06174094
shape of L is 
torch.Size([])
memory (bytes)
3897307136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3897307136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10890880.0
relative error loss 0.06173196
time to take a step is 412.22866654396057
it  11 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3900534784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3900534784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10890880.0
relative error loss 0.06173196
shape of L is 
torch.Size([])
memory (bytes)
3903741952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  6% |
memory (bytes)
3903741952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10889472.0
relative error loss 0.06172398
shape of L is 
torch.Size([])
memory (bytes)
3906977792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  6% |
memory (bytes)
3906977792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10888384.0
relative error loss 0.061717812
shape of L is 
torch.Size([])
memory (bytes)
3910197248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3910197248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10887488.0
relative error loss 0.061712734
shape of L is 
torch.Size([])
memory (bytes)
3913433088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  6% |
memory (bytes)
3913433088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10886720.0
relative error loss 0.06170838
shape of L is 
torch.Size([])
memory (bytes)
3916521472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  6% |
memory (bytes)
3916656640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10886240.0
relative error loss 0.06170566
shape of L is 
torch.Size([])
memory (bytes)
3919876096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3919876096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10885632.0
relative error loss 0.061702214
shape of L is 
torch.Size([])
memory (bytes)
3922972672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3923107840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10885392.0
relative error loss 0.061700854
shape of L is 
torch.Size([])
memory (bytes)
3926323200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3926323200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10884448.0
relative error loss 0.061695505
shape of L is 
torch.Size([])
memory (bytes)
3929415680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3929415680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10884208.0
relative error loss 0.06169414
time to take a step is 300.9547231197357
it  12 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3932647424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3932782592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10884208.0
relative error loss 0.06169414
shape of L is 
torch.Size([])
memory (bytes)
3936002048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
3936002048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10883856.0
relative error loss 0.06169215
shape of L is 
torch.Size([])
memory (bytes)
3939160064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3939160064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10883104.0
relative error loss 0.061687887
shape of L is 
torch.Size([])
memory (bytes)
3942453248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3942453248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10882256.0
relative error loss 0.061683077
shape of L is 
torch.Size([])
memory (bytes)
3945680896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3945680896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10881104.0
relative error loss 0.061676547
shape of L is 
torch.Size([])
memory (bytes)
3948777472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  6% |
memory (bytes)
3948908544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10880320.0
relative error loss 0.061672106
shape of L is 
torch.Size([])
memory (bytes)
3952132096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3952132096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  10878000.0
relative error loss 0.061658956
shape of L is 
torch.Size([])
memory (bytes)
3955351552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3955351552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10879952.0
relative error loss 0.06167002
shape of L is 
torch.Size([])
memory (bytes)
3958411264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
3958571008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10877136.0
relative error loss 0.061654057
shape of L is 
torch.Size([])
memory (bytes)
3961798656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3961798656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10875888.0
relative error loss 0.061646983
time to take a step is 288.1886992454529
it  13 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3964899328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3964899328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10875888.0
relative error loss 0.061646983
shape of L is 
torch.Size([])
memory (bytes)
3968102400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3968241664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10873392.0
relative error loss 0.061632834
shape of L is 
torch.Size([])
memory (bytes)
3971489792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3971489792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10872976.0
relative error loss 0.061630476
shape of L is 
torch.Size([])
memory (bytes)
3974598656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3974598656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10871648.0
relative error loss 0.06162295
shape of L is 
torch.Size([])
memory (bytes)
3977924608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3977924608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10870960.0
relative error loss 0.06161905
shape of L is 
torch.Size([])
memory (bytes)
3981152256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  6% |
memory (bytes)
3981152256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10870240.0
relative error loss 0.061614968
shape of L is 
torch.Size([])
memory (bytes)
3984379904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3984379904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10869104.0
relative error loss 0.06160853
shape of L is 
torch.Size([])
memory (bytes)
3987615744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3987615744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10868928.0
relative error loss 0.061607532
shape of L is 
torch.Size([])
memory (bytes)
3990773760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
3990822912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10868176.0
relative error loss 0.06160327
shape of L is 
torch.Size([])
memory (bytes)
3994062848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  6% |
memory (bytes)
3994062848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10867104.0
relative error loss 0.061597195
time to take a step is 283.4471492767334
it  14 : 910626304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3997290496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3997290496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10867104.0
relative error loss 0.061597195
shape of L is 
torch.Size([])
memory (bytes)
4000444416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  6% |
memory (bytes)
4000444416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10866464.0
relative error loss 0.061593566
shape of L is 
torch.Size([])
memory (bytes)
4003696640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  6% |
memory (bytes)
4003696640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10864912.0
relative error loss 0.061584767
shape of L is 
torch.Size([])
memory (bytes)
4006965248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
4006965248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10864176.0
relative error loss 0.0615806
shape of L is 
torch.Size([])
memory (bytes)
4010127360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  6% |
memory (bytes)
4010127360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10863408.0
relative error loss 0.061576243
shape of L is 
torch.Size([])
memory (bytes)
4013285376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
4013420544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10862848.0
relative error loss 0.06157307
shape of L is 
torch.Size([])
memory (bytes)
4016644096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
4016644096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10862160.0
relative error loss 0.06156917
shape of L is 
torch.Size([])
memory (bytes)
4019806208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  6% |
memory (bytes)
4019806208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10860992.0
relative error loss 0.06156255
shape of L is 
torch.Size([])
memory (bytes)
4022964224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
4023099392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10860864.0
relative error loss 0.061561823
shape of L is 
torch.Size([])
memory (bytes)
4026331136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4026331136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10860064.0
relative error loss 0.06155729
time to take a step is 283.84590458869934
sum tnnu_Z after tensor(2902811.5000, device='cuda:0')
shape of features
(4722,)
shape of features
(4722,)
number of orig particles 18889
number of new particles after remove low mass 18889
tnuZ shape should be parts x labs
torch.Size([18889, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  15644769.0
relative error without small mass is  0.08867808
nnu_Z shape should be number of particles by maxV
(18889, 702)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
shape of features
(18889,)
Wed Feb 1 08:59:36 EST 2023
