Tue Jan 31 21:54:04 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 65565205
numbers of Z: 27579
shape of features
(27579,)
shape of features
(27579,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01999255282186396	27579	27.579	0.08983176080489852
X	0.0197320273701535	2176	2.176	0.2085311505392515
X	0.019890361015281844	36665	36.665	0.08155745380704724
X	0.019892640683892515	4459	4.459	0.16462090640777297
X	0.01990949491301463	12459	12.459	0.11691198085557306
X	0.01987610901978873	56712	56.712	0.07050481904363677
X	0.019934144394767365	67736	67.736	0.06651598361730438
X	0.019916198851073763	90925	90.925	0.06028015791441615
X	0.019937645304048204	78517	78.517	0.06332425017194979
X	0.0199011150278677	26427	26.427	0.09097942374829783
X	0.019940572363851227	53431	53.431	0.07199706167927819
X	0.019927622593008645	30451	30.451	0.08681963921338225
X	0.019927029782658225	133347	133.347	0.05306641798248842
X	0.019878496861833864	9452	9.452	0.12812088882340347
X	0.019943661753877646	389694	389.694	0.03712733074492441
X	0.01989344400317659	36913	36.913	0.08137859941051451
X	0.019936036400846305	64455	64.455	0.06762813356844292
X	0.019945724293979596	79394	79.394	0.06309874326953721
X	0.01991721479901841	63827	63.827	0.06782785113045413
X	0.019992933027201256	192172	192.172	0.04703221874311447
X	0.01993847905460533	179543	179.543	0.04806635425654296
X	0.01991043875000063	50098	50.098	0.07352247990048862
X	0.019950739270165607	289897	289.897	0.04097998143404128
X	0.01990672003181717	18297	18.297	0.10285054724751891
X	0.019976544671157583	41733	41.733	0.07822524316114926
X	0.01988696422051428	43625	43.625	0.07696223728936559
X	0.019939358876843796	100461	100.461	0.05833169071149853
X	0.019948699355546634	117768	117.768	0.05533022400909279
X	0.019893557687183233	30802	30.802	0.08643928788476325
X	0.01991505562407436	291826	291.826	0.04086509508642608
X	0.01999247601253878	1780552	1780.552	0.022392571728457922
X	0.019895785209917607	26584	26.584	0.09079186107977681
X	0.019992274716231623	1487884	1487.884	0.023773751148307272
X	0.019904818538406994	25388	25.388	0.09220969333756417
X	0.019926657593478832	25838	25.838	0.09170475496688518
X	0.019908019993720476	26103	26.103	0.09136486460156311
X	0.01996599497466015	520050	520.05	0.03373509614653209
X	0.019946394280964123	106915	106.915	0.05714022152305441
X	0.01971293519351623	2413	2.413	0.2014024073852843
X	0.01990565239628125	5799	5.799	0.15084853694651468
X	0.019806384136050712	5117	5.117	0.15701094479604388
X	0.019873022729420535	5473	5.473	0.15370199116812044
X	0.01943591290269584	3044	3.044	0.18551830400991953
X	0.01962492138221556	1885	1.885	0.2183561979474941
X	0.019867650455865974	6833	6.833	0.14272882700249875
X	0.019748522657150103	1384	1.384	0.24254901064661644
X	0.01983902368256978	2825	2.825	0.19149933987521744
X	0.019571440685569076	5624	5.624	0.15153949232616756
X	0.019887284480173597	9475	9.475	0.1280359974616035
X	0.019863754790913263	3559	3.559	0.17738264199572298
X	0.019856684865419187	15068	15.068	0.10963529191494419
X	0.01989762557440046	22142	22.142	0.09650018277871161
X	0.019750449613579774	2152	2.152	0.20936861166555729
X	0.019852200814827057	7989	7.989	0.13544783869314853
X	0.019773204904334492	7149	7.149	0.14037095564505572
X	0.01989486708902524	8885	8.885	0.13082614266618642
X	0.019826619803852072	6941	6.941	0.14188687099793257
X	0.01987908478998041	3959	3.959	0.17123937266097997
X	0.01985340014386594	8100	8.1	0.13482899343914567
X	0.019929253978078352	5558	5.558	0.1530585165452954
X	0.01979937802408334	4142	4.142	0.16845367279554807
X	0.019854654782451926	5954	5.954	0.14940019694787168
X	0.019893374769807064	2695	2.695	0.19470774704819752
X	0.019860544453618942	8326	8.326	0.13361388170360805
X	0.019861342501337646	20290	20.29	0.09929076372761983
X	0.01986289813612296	5351	5.351	0.1548350296012766
X	0.019899710028686592	4816	4.816	0.16046737807670625
X	0.019887611583274286	6013	6.013	0.1489922945372348
X	0.019754926118903594	7763	7.763	0.13652597117013862
X	0.019884469275989927	8068	8.068	0.13507740378544073
X	0.019870769816149974	5924	5.924	0.14969244461858636
X	0.019883335014158735	5674	5.674	0.15189144002721172
X	0.01988736309592493	5652	5.652	0.15209852996367768
X	0.01985628312576456	6285	6.285	0.14673404557683217
X	0.019782644697338862	4828	4.828	0.16001929886583466
X	0.019780969405085398	6411	6.411	0.14558186034180984
X	0.019868382302528846	16431	16.431	0.10653676472097541
X	0.019793356368008063	1674	1.674	0.22781797999795128
X	0.019811860661534668	2636	2.636	0.1958814392328782
X	0.019836353684037605	5610	5.61	0.15234667987332334
X	0.019897806116502194	26246	26.246	0.09118303079410287
X	0.01979840844002197	4879	4.879	0.15950213003615799
X	0.019770392878341956	3173	3.173	0.18401295908392326
X	0.019843812291416702	5414	5.414	0.15418269040598087
X	0.019575804395800523	2629	2.629	0.195273365019823
X	0.01990274383995923	2905	2.905	0.1899279754594658
X	0.019850986065454174	4011	4.011	0.17041578032339977
X	0.019861303782516163	3630	3.63	0.17621128152561855
X	0.019751736187323173	2264	2.264	0.20586204591755405
X	0.019851756835076274	5672	5.672	0.1518288286772839
X	0.019871535768462744	6588	6.588	0.14448605966803144
X	0.019764649712107337	6016	6.016	0.14865987549141982
X	0.019795928526086637	3715	3.715	0.17466491880912727
X	0.019866463372313774	3374	3.374	0.18057536201973381
X	0.01986756651459084	10034	10.034	0.12557124292461672
X	0.01989835026297435	5735	5.735	0.15138907997788176
X	0.019841016014610555	8298	8.298	0.13372014095687862
X	0.019765606585994643	4151	4.151	0.168236078982222
X	0.019811146926757978	6402	6.402	0.14572407804988022
X	0.019839038710776528	5891	5.891	0.14989156729811967
X	0.019903086292846168	9217	9.217	0.12925388714816027
X	0.01985608591764387	18151	18.151	0.10303806529701619
X	0.01986902367421705	6577	6.577	0.14456047340965358
X	0.019906548556952834	9737	9.737	0.12691811087899316
X	0.01948538219362729	3330	3.33	0.18020004635999912
X	0.01986044225260192	19498	19.498	0.10061582279530318
X	0.019817020512162595	5231	5.231	0.1558898645246689
X	0.019866145808454725	7438	7.438	0.13874557727299405
X	0.019886952133947453	3823	3.823	0.17326917602828995
X	0.01987290733085624	4477	4.477	0.16434560872759738
X	0.019629482620079845	1541	1.541	0.23354401279906428
X	0.019851259792295614	7351	7.351	0.13925598945123188
X	0.01974471687167217	2192	2.192	0.20806711891856766
X	0.019640820196822745	2944	2.944	0.18825243601223807
X	0.01990133334836913	4467	4.467	0.16454653468713834
X	0.019876755795524344	8157	8.157	0.1345669306250549
X	0.01988857673446131	3684	3.684	0.17542629637344653
X	0.019894989331736796	4906	4.906	0.15946745219467343
X	0.019770043592154875	1672	1.672	0.22781926756226192
X	0.019889605114800268	11779	11.779	0.11908011525995806
X	0.01974697122308515	2547	2.547	0.19792037999543008
X	0.019922806878100618	17777	17.777	0.1038717266477727
X	0.019874415648874325	4278	4.278	0.16685959918473742
X	0.01975699734297158	3628	3.628	0.1759345850421037
X	0.01990060951671446	5218	5.218	0.15623827469216608
X	0.019794961019555687	5378	5.378	0.15439904780244632
X	0.019759799114754374	2931	2.931	0.18891027006214522
X	0.019816567508254525	3254	3.254	0.1826152158003647
X	0.019869352513084772	5507	5.507	0.15337557814650155
X	0.019922613147327396	13929	13.929	0.11267012804493601
X	0.0198343673985421	3963	3.963	0.17105328816857993
X	0.019914878426278385	15975	15.975	0.10762477690943441
X	0.01990997042311741	2454	2.454	0.2009395161450359
X	0.01992202847981767	5207	5.207	0.15640428947156337
X	0.0198584787463757	2558	2.558	0.1980075782342562
X	0.019792000260253077	4426	4.426	0.16475025398907103
X	0.019834905302864813	4414	4.414	0.16501848698320845
X	0.0197214554109292	2298	2.298	0.2047369892735932
X	0.019796881471953274	3584	3.584	0.17677042007010133
X	0.019794031361646976	4130	4.13	0.16860148706771852
X	0.01978247052649686	3545	3.545	0.17737323868457838
X	0.019900775347992537	5635	5.635	0.15228554863159854
X	0.019797057758113648	3413	3.413	0.17967519324414905
X	0.01978038707014905	18552	18.552	0.10216010957023937
X	0.0197940101095204	9570	9.57	0.12741110531756958
X	0.019850380071473973	6867	6.867	0.14245157643741266
X	0.019791448012399456	3996	3.996	0.17045799015927576
X	0.01978015743006546	3098	3.098	0.18551660021287517
X	0.019903009192275216	3069	3.069	0.18648358878835777
X	0.019735012975593984	3631	3.631	0.1758208555931329
X	0.01988517936240002	3385	3.385	0.180436175469378
X	0.019799862835739063	4458	4.458	0.1643768697430047
X	0.01983290151763862	4371	4.371	0.16555227365337047
X	0.019888628818066405	5399	5.399	0.15444143860358614
X	0.019899408525967215	8825	8.825	0.1311319394921901
X	0.019898699711843576	3806	3.806	0.1735609311392916
X	0.01969822566788091	17905	17.905	0.10323277324749859
X	0.019687580560322635	6355	6.355	0.14577809291807847
X	0.01986725783414653	5943	5.943	0.14952394009582903
X	0.019863253544041854	4841	4.841	0.16009278798660664
X	0.01957036441109298	1041	1.041	0.26589860630858764
X	0.019953444925255946	13998	13.998	0.11254269199154585
X	0.019479772227032312	2387	2.387	0.201330923273402
X	0.019753125519465932	12554	12.554	0.11631022602902999
X	0.01985598693941878	1589	1.589	0.2320540720526249
X	0.019728894681787297	7896	7.896	0.13569542479626973
X	0.019894310291256493	5247	5.247	0.15593344522336347
X	0.019901031000312125	3348	3.348	0.1811465396310627
X	0.019786997658282037	6178	6.178	0.14740447426058428
X	0.019714928245237674	8434	8.434	0.13271515990991833
X	0.019793506541608375	1613	1.613	0.2306549559742854
X	0.01944821892793727	2190	2.19	0.20708337764577245
X	0.019353938910800422	4107	4.107	0.1676543737267264
X	0.019906805544483975	10317	10.317	0.12449427522977591
X	0.019774267074431258	2434	2.434	0.20102956853421974
X	0.019833867243728043	6855	6.855	0.14249511657052938
X	0.019888495764083428	6495	6.495	0.1452137008920324
X	0.019850623471658765	9282	9.282	0.1288380656772339
X	0.019791745687847122	4110	4.11	0.16886802599733264
X	0.019874921163439696	6174	6.174	0.14765435579966965
X	0.01986859752768122	4572	4.572	0.16318754048817216
X	0.019889182898990228	9062	9.062	0.12995638674387588
X	0.019833006266017684	2818	2.818	0.19163839287546644
X	0.01978994310735063	5064	5.064	0.15751319816663337
X	0.01978976671101045	4704	4.704	0.16143255503625978
X	0.019865120866549398	7601	7.601	0.1377442545639472
X	0.01989065467592666	7938	7.938	0.13582487927167308
X	0.01990082293729833	3975	3.975	0.17107161981098526
X	0.01985016372914792	6237	6.237	0.14709439231249438
X	0.019854493411745885	13156	13.156	0.11470393454882978
X	0.019882500405231606	13190	13.19	0.11465915446300606
X	0.01977104712213938	3717	3.717	0.1745603899314954
X	0.019616444014118974	1491	1.491	0.2360736672451574
X	0.019855094585114644	5454	5.454	0.15383397974654173
X	0.01987991989204942	5985	5.985	0.14920503942902827
X	0.019895750740754833	5393	5.393	0.154517131441588
X	0.019824606298556435	5478	5.478	0.15353032919926043
X	0.01989416370530202	5897	5.897	0.1499793682306128
X	0.019859012052487822	2948	2.948	0.18886147915210705
X	0.01974285744505363	6457	6.457	0.1451419903840152
X	0.01990016297279449	1937	1.937	0.21739138901180832
X	0.019931735544372144	8620	8.62	0.13223485707435234
X	0.019812595335016416	4493	4.493	0.16398406587219905
X	0.019908598795906934	13198	13.198	0.11468611913087143
X	0.019818425207324214	4458	4.458	0.16442822147121464
X	0.019908769340809592	13197	13.197	0.11468934331968379
X	0.019673558103805108	3121	3.121	0.18472671810855468
X	0.019763755169350958	4714	4.714	0.1612476134973463
X	0.019838143677334056	5563	5.563	0.15277911517007567
X	0.019897033902284582	4197	4.197	0.16798988143445648
X	0.01985510700008931	9753	9.753	0.12673930926601953
X	0.01969387401547507	6720	6.72	0.14310472082500567
X	0.01977165381233994	5715	5.715	0.1512431059951062
X	0.019858925712764933	5107	5.107	0.15725216417840737
X	0.01973997045245494	7370	7.37	0.13887572423740854
X	0.019460161446613382	1327	1.327	0.24477017895490571
X	0.019799719105506947	2468	2.468	0.2001879618212376
X	0.01973671938024291	1507	1.507	0.23571500963474126
X	0.019702567096482534	2288	2.288	0.20496935368067368
X	0.019847997626395476	22049	22.049	0.09655525918152058
X	0.019839143158834095	3957	3.957	0.1711534352430468
X	0.01964674578791367	3149	3.149	0.18409386754528104
X	0.019770587574499225	1649	1.649	0.22887567614768192
X	0.019853607041692558	11010	11.01	0.12171684902301452
X	0.01987986667050566	4112	4.112	0.1690908551270033
X	0.019846989542500984	5023	5.023	0.15809221618313968
X	0.01984730145465841	5663	5.663	0.1518978528099621
X	0.019806548282542323	3807	3.807	0.17327742017866052
X	0.01966384537293924	2751	2.751	0.1926308998367555
X	0.019816698190612058	6708	6.708	0.14348706327614516
X	0.019804423645043024	1455	1.455	0.23876265793906878
X	0.01988294357858336	3463	3.463	0.17906445951657998
X	0.019897242988052254	5063	5.063	0.157807749146502
X	0.019859378007378498	4506	4.506	0.16395505768115362
X	0.019910211988630053	7038	7.038	0.1414302168279904
X	0.019716058280398498	5351	5.351	0.15445253739624007
X	0.019884804352132	4644	4.644	0.1623839259341321
X	0.019888010866135827	7309	7.309	0.13960826468636023
X	0.019894788808645942	7632	7.632	0.13762594811172127
X	0.01980275614636239	3080	3.08	0.18594805541882492
X	0.01985441713663841	20173	20.173	0.09947078634569768
X	0.019926669916871205	52387	52.387	0.07245534010027986
X	0.01993919973810775	10046	10.046	0.1256719006469703
X	0.01986196375486591	6453	6.453	0.14546332372443557
X	0.019708271763188102	3084	3.084	0.18557154817162647
X	0.019886134498113023	16608	16.608	0.10618854710865715
X	0.019847090786329524	15754	15.754	0.1080028804639349
X	0.019914928971226445	102805	102.805	0.057861298381015225
X	0.019888679732853205	7359	7.359	0.13929292252726394
X	0.019921049285450805	217493	217.493	0.04507706614013495
X	0.019933517268446544	124930	124.93	0.05423825795459934
X	0.019820187675782086	64803	64.803	0.0673758437550099
X	0.01999079746031505	141119	141.119	0.05212925795623491
X	0.019802956755033758	3344	3.344	0.18092056004035018
X	0.01990552494839362	9197	9.197	0.12935279472067698
X	0.019913461843276446	494211	494.211	0.03428294481087225
X	0.019933622861299446	563053	563.053	0.03283565269821316
X	0.019891349759733612	6947	6.947	0.1420002094613521
X	0.019894880421354788	46253	46.253	0.07548612678707076
X	0.019917150700375884	61298	61.298	0.06874803692076552
X	0.01992758198132245	31029	31.029	0.08627711379902804
X	0.01993824953238963	68683	68.683	0.06621340524975665
X	0.019910591661124046	51168	51.168	0.07300656378064645
X	0.01991226305315118	25781	25.781	0.09175018614274284
X	0.019923262623223484	19739	19.739	0.10031020182955985
X	0.019782693243843495	7665	7.665	0.1371695622019234
X	0.019964540655588496	137307	137.307	0.05258423352824656
X	0.019904099408214488	11070	11.07	0.12159945740480789
X	0.019860549034462063	6100	6.1	0.1482132868082585
X	0.01992182767028011	42458	42.458	0.0777063569565318
X	0.019858256014249672	53642	53.642	0.07180346149565238
X	0.019989134626849644	389218	389.218	0.03717066738530836
X	0.019941940927697854	108300	108.3	0.05689136093632402
X	0.019841029793574577	3547	3.547	0.17751470712646564
X	0.019880635272116132	50656	50.656	0.07321495305210951
X	0.019827673457457475	9168	9.168	0.12932000649550499
X	0.01995243261564342	31737	31.737	0.08566629681972879
X	0.019945601926788677	81285	81.285	0.06260546501392757
X	0.019972190161751863	54880	54.88	0.07139544912142888
X	0.01990670927271813	116726	116.726	0.05545541464080413
X	0.019369839657276313	1659	1.659	0.2268610197272696
X	0.019875169990988834	9862	9.862	0.1263131651709706
X	0.019912996599600885	102367	102.367	0.05794183093124304
X	0.019915681880039563	87257	87.257	0.061112722912867616
X	0.019925216934513018	74346	74.346	0.06447358463821234
X	0.019854450786584665	5461	5.461	0.15376656067499778
X	0.019982745736833927	362881	362.881	0.038044946487156533
X	0.019925842288250728	23793	23.793	0.09425892765491856
X	0.01989892720118957	37542	37.542	0.08092898357917135
X	0.01994553085350999	124735	124.735	0.05427740669048328
X	0.01988267052571081	3960	3.96	0.17123525177205076
X	0.019934505211956402	89646	89.646	0.060584033157442506
X	0.019990340959722033	207454	207.454	0.045845790221675474
X	0.019888647111724122	917959	917.959	0.027877938303864407
X	0.0198914797437961	42259	42.259	0.07778860059400784
X	0.019908492282497654	44945	44.945	0.07622878461132589
X	0.019841313873130064	14869	14.869	0.11009380672518669
X	0.019893671781930057	9134	9.134	0.12962376959677527
X	0.01993385132129216	191849	191.849	0.04701219818494845
X	0.019930912122396713	12364	12.364	0.117252665951163
X	0.01991649799909863	36388	36.388	0.0817996785435779
X	0.01998514923070243	68217	68.217	0.06641582826291018
X	0.019938899623894342	49497	49.497	0.07385401380751956
X	0.01991752612016092	16650	16.65	0.10615498317936709
X	0.019915343024451117	9978	9.978	0.1259064842807848
X	0.019948759553977163	246250	246.25	0.043269301611378354
X	0.01994488740355958	20728	20.728	0.0987244515242895
X	0.01991686196427719	17004	17.004	0.1054119722571062
X	0.019868670870342026	11432	11.432	0.12023074364194104
X	0.019947255740091357	164401	164.401	0.04950619714271137
X	0.019909584247593634	22521	22.521	0.09597501165505545
X	0.019846582251332207	119830	119.83	0.05491698968124191
X	0.01992239430990265	18808	18.808	0.10193726373621385
X	0.01988082587686647	31779	31.779	0.08552598110732615
X	0.01999187624882397	21746	21.746	0.09723547133149715
X	0.01996666458960586	376467	376.467	0.03757158671025072
X	0.019941614243336698	66745	66.745	0.0668519158482146
X	0.019920149727213326	17001	17.001	0.10542397252471226
X	0.019937823270856476	193267	193.267	0.04690005430527255
X	0.01994025230859275	228215	228.215	0.04437402472719083
X	0.019919841227108676	6474	6.474	0.1454468742759775
X	0.019935425058778922	137300	137.3	0.05255955193673116
X	0.019940521336577557	202585	202.585	0.04617175766845652
X	0.01997093098494015	579532	579.532	0.03254170445551911
X	0.019933162662706172	22218	22.218	0.09644737591828847
X	0.01990168653004632	5666	5.666	0.15200963064052586
X	0.019933931185191645	20214	20.214	0.0995360107963607
X	0.019937564320920937	20580	20.58	0.0989484306021245
X	0.01991523310591975	42551	42.551	0.07764113479876486
X	0.01991828989696401	42931	42.931	0.07741533808195196
X	0.019798825172442	3501	3.501	0.17816228333453363
X	0.01999010636253339	74054	74.054	0.06462821995520437
X	0.01993172407428311	227674	227.674	0.04440281216899363
X	0.01997345979902759	145443	145.443	0.051592529986851494
X	0.019894849928031325	11012	11.012	0.12179369926654784
X	0.019916774214128567	91983	91.983	0.060048727694532796
X	0.019917592777705804	19169	19.169	0.10128515502263263
X	0.019901888148199615	27002	27.002	0.0903301606728988
X	0.019887703866595798	18485	18.485	0.10246803231288765
X	0.019942735441484897	7252	7.252	0.14010134701159854
X	0.01992151826783174	211412	211.412	0.04550553937608863
X	0.019925244369344064	58269	58.269	0.0699286853833414
X	0.019969377967010836	24442	24.442	0.09348514352038059
X	0.019882746618499095	65227	65.227	0.06730019530353117
X	0.019890023282416302	10347	10.347	0.12433887896564247
X	0.01994977678236715	734107	734.107	0.03006488579683396
X	0.019847022190214208	16910	16.91	0.10548335048026825
X	0.019918451150850634	170170	170.17	0.04891674177211523
X	0.019915705119626297	3618	3.618	0.17656679813882267
X	0.019883860630638635	6310	6.31	0.1466078136041476
X	0.019917977666346398	5556	5.556	0.15304800327817739
X	0.019941426878116342	19495	19.495	0.10075756505088661
X	0.019859672172976323	9956	9.956	0.12588164100132596
X	0.01986931696109086	48575	48.575	0.07423180875722443
X	0.01987267763307501	6972	6.972	0.14178588654842683
X	0.01985072679628356	7109	7.109	0.14081728476447297
X	0.019999972961470637	341713	341.713	0.03882599870858264
X	0.019917072675266344	61121	61.121	0.06881424547045252
X	0.01990555368299996	43996	43.996	0.07676920141939607
X	0.019948100681136973	152030	152.03	0.05081486217725388
X	0.019971854691986052	433563	433.563	0.03584721288403222
X	0.01991408192252083	6667	6.667	0.14401573484552374
X	0.01993246525755234	10891	10.891	0.12232007891258284
X	0.01991945775536335	24993	24.993	0.09271564862512872
X	0.019888598037553818	3777	3.777	0.1739745526893164
X	0.019844726775448657	10478	10.478	0.1237244627325973
X	0.019907866899085122	23724	23.724	0.09432184147367785
X	0.019877397051922544	10250	10.25	0.1247034731086323
X	0.01989227956027348	10011	10.011	0.12571941868871447
X	0.019882626975589103	15559	15.559	0.1085168956267617
X	0.019875197723476866	18171	18.171	0.10303328417715213
X	0.019948217861229668	966474	966.474	0.027430768554569617
X	0.0198800249890454	13433	13.433	0.11395882702396745
X	0.019920845628081014	94869	94.869	0.059437584141815075
X	0.019913975620928462	21527	21.527	0.09743720554579474
X	0.01998923694526078	5016	5.016	0.1585426924742572
X	0.019918451554254812	98606	98.606	0.058674681923369866
X	0.019955414897166235	337629	337.629	0.038952948775035044
X	0.01995870164269857	700267	700.267	0.030546130894336707
X	0.019980626987093884	64263	64.263	0.06774585143791892
X	0.019934620110677793	52539	52.539	0.07239502467602858
X	0.019910127046154875	4074	4.074	0.16970097422318656
X	0.019929231023877014	82830	82.83	0.062196740675802754
X	0.0199321611788499	1560829	1560.829	0.0233739990253249
X	0.0199724968001346	80497	80.497	0.06283730565269645
X	0.01990896363821563	15662	15.662	0.1083262754677375
X	0.01993971338433531	99176	99.176	0.05858288757221725
X	0.019918249620930598	568020	568.02	0.032731244210048
X	0.01991462856073587	98145	98.145	0.05876264606715514
X	0.01980771806986962	15669	15.669	0.10812622857159977
X	0.01987529110983946	12467	12.467	0.116819994012353
X	0.01990563068947413	15738	15.738	0.1081455865455853
X	0.019905326342715142	9154	9.154	0.129554588139337
X	0.01999194725722966	343800	343.8	0.03874209261588032
X	0.01996445103293599	24729	24.729	0.09311441935156228
X	0.019881845813850767	3169	3.169	0.18443562881995804
X	0.019924257713702282	69153	69.153	0.0660476000927758
X	0.01991362628372548	27275	27.275	0.09004546899788671
X	0.019875119556898517	3290	3.29	0.18212572094959104
X	0.019928307324183096	123052	123.052	0.05450804156435443
X	0.019948602757659552	111889	111.889	0.056282713820773496
X	0.01993718468932299	154367	154.367	0.05054790058165608
X	0.019911207940488312	213083	213.083	0.045378444173609506
X	0.01992111741663841	248755	248.755	0.04310364118565294
X	0.0198892441818446	34794	34.794	0.0829923330545187
X	0.019862172709047797	66939	66.939	0.06669846583256651
X	0.01994108304181391	149134	149.134	0.051135678781305385
X	0.019951022201508444	362510	362.51	0.03803777047761046
X	0.01990349240196946	13368	13.368	0.11418812720227585
X	0.019958724852070585	108348	108.348	0.056898912255559944
X	0.019997482768394418	155334	155.334	0.05049359222386138
X	0.01995614739026548	126329	126.329	0.05405773954851317
X	0.01994363865635557	83480	83.48	0.06204983802795615
X	0.01982863851650755	41724	41.724	0.07803731521771931
X	0.01991656785719706	19899	19.899	0.10002941971924273
X	0.019900686857412184	11295	11.295	0.12077969917715912
X	0.019884260120292006	79101	79.101	0.063111594689802
X	0.019970928668418396	213407	213.407	0.04540076804988221
X	0.01994535092788234	170529	170.529	0.0489043861724588
X	0.019929979668848612	261448	261.448	0.042400779805188304
X	0.019951384356124053	171115	171.115	0.048853421404867425
X	0.01993719301925355	114246	114.246	0.055882308261423264
X	0.01987159035322648	23632	23.632	0.09438668122518819
X	0.019965269172330092	367727	367.727	0.037866039507775444
X	0.019863903540942467	5671	5.671	0.15186871447278957
X	0.019838161567333448	19864	19.864	0.09995662229153499
X	0.019942522713182826	307552	307.552	0.040174811394737
X	0.019909207271457105	49560	49.56	0.07378604347837055
X	0.019842802892294578	7399	7.399	0.1389344662651572
X	0.01994266482483205	33143	33.143	0.08442358516072773
X	0.019925413142174043	323788	323.788	0.039480459193904506
X	0.0199457978906667	41541	41.541	0.07830535973325088
X	0.019924826880385656	114340	114.34	0.05585543729303365
X	0.019845396735269236	7084	7.084	0.14097012266413614
X	0.019909610110475297	13702	13.702	0.11326427011889764
X	0.01990267622106379	8921	8.921	0.13066701784793758
X	0.019843124250611337	49315	49.315	0.07382616994939921
X	0.019924332015429275	48036	48.036	0.0745771263159596
X	0.019927792828357598	93401	93.401	0.05975430844126585
X	0.019899197898492274	5958	5.958	0.14947837138097372
X	0.01993575008114906	166508	166.508	0.049287012228789714
X	0.019785072430862145	12733	12.733	0.11582499811565407
X	0.01995382981984711	70517	70.517	0.06565142322093595
X	0.019922970737007474	15328	15.328	0.10913302711836469
X	0.019928865039772276	128065	128.065	0.05378783171247526
X	0.019789306137442477	6035	6.035	0.14856542996772815
X	0.019852483755176783	42469	42.469	0.07760939026878488
X	0.019911807278562847	40692	40.692	0.07880139059066299
X	0.019878794727053978	31827	31.827	0.08548005278234717
X	0.019979701470419692	75375	75.375	0.06423729182052283
X	0.019940774759843474	552888	552.888	0.03303961322474836
X	0.019899935188714666	11431	11.431	0.12029728128502143
X	0.019779039692803826	14342	14.342	0.11130939646402123
X	0.01993711148904166	122593	122.593	0.054584020518365216
X	0.01984069464953324	30216	30.216	0.08691738143567701
X	0.0199486958522246	700794	700.794	0.030533367951740473
X	0.019798105703903515	5859	5.859	0.15006061207860774
X	0.0199274418790636	171200	171.2	0.048825788280528505
X	0.019939854147271823	423315	423.315	0.03611487420712962
X	0.019882263391907608	7825	7.825	0.13645637361620672
X	0.019937674925959954	241525	241.525	0.04354157555771474
X	0.019914033907310415	23089	23.089	0.09518854887724691
X	0.019972623556676308	448307	448.307	0.03545029597495763
X	0.019909094150453725	22879	22.879	0.09547100347446837
X	0.019890976269280894	49197	49.197	0.07394449141205321
X	0.019820086123610046	6264	6.264	0.1468085207597855
X	0.019860552521493154	7425	7.425	0.13881347388239582
X	0.019889238303160497	8738	8.738	0.1315432911930615
X	0.019951572012990065	57115	57.115	0.07042750659632765
X	0.019911311519060674	31277	31.277	0.08602504840483971
X	0.019919888417960857	181073	181.073	0.04791568870859098
X	0.01992700106575944	27067	27.067	0.09029574253165605
X	0.019806526351006857	6403	6.403	0.14570516199480707
X	0.019933079947369377	27943	27.943	0.08935121435452494
X	0.019866787948124843	131741	131.741	0.053227438504036884
X	0.01995032302504231	49957	49.957	0.07364069193426111
X	0.019989085089854558	288439	288.439	0.0410751960671979
X	0.019932218359998373	73212	73.212	0.06481235295452238
X	0.01992909053966754	9190	9.19	0.1294366674857703
X	0.019852375942782993	5060	5.06	0.1577202020109279
X	0.01991143569960848	8487	8.487	0.13287689091062108
X	0.019954369531307987	628961	628.961	0.03165712209419645
X	0.019901394160715135	11283	11.283	0.12082393369260097
X	0.019920097740639624	32917	32.917	0.08458442587678958
X	0.01994161454721565	88667	88.667	0.06081342091393041
X	0.019942399055316713	28894	28.894	0.08837374199702712
X	0.019893734979271972	8046	8.046	0.13522140162789678
X	0.019835277406257557	8967	8.967	0.13029578586929472
X	0.019958366474482278	213270	213.27	0.045400964030676935
X	0.01992250414577267	56709	56.709	0.0705608784529433
X	0.01990682073211272	53380	53.38	0.07197932649583133
X	0.019934632199383105	20378	20.378	0.09926943645497821
X	0.019928501835690022	91199	91.199	0.06023212585082013
X	0.01991584350328999	95849	95.849	0.05922935995498135
X	0.019936981483997716	228194	228.194	0.04437295943821948
X	0.01994470286442943	141406	141.406	0.052053896520416795
X	0.01990411499323673	8174	8.174	0.13453524673927125
X	0.01996570753409402	107115	107.115	0.05712306092976915
X	0.019927150198145907	24578	24.578	0.09324657411919823
X	0.019880727040253333	37775	37.775	0.08073761754663861
X	0.01991740934140193	23850	23.85	0.09417048819801804
X	0.01988002936207687	8222	8.222	0.13421874788944035
X	0.01994464167220101	32947	32.947	0.08459345976380067
X	0.019856366311074482	37504	37.504	0.08089854835665244
X	0.019812348654391087	8422	8.422	0.13299650907389635
X	0.019824933912533777	30796	30.796	0.0863453880759454
X	0.019942944909096177	9789	9.789	0.12677014498195666
X	0.019976527635965212	134794	134.794	0.05291959109306639
X	0.019871340759850097	5981	5.981	0.14921682312257803
X	0.019916858378493107	17871	17.871	0.1036789666830362
X	0.019941686631293348	20001	20.001	0.09990105145216423
X	0.01990037855628198	33117	33.117	0.084385944686161
X	0.01989600372747929	6847	6.847	0.14269929973601286
X	0.019890258441521994	19549	19.549	0.1005785321403867
X	0.019938188504329067	36346	36.346	0.08186087048186794
X	0.019936132509157496	164494	164.494	0.04948766354988893
X	0.019895776927726632	9159	9.159	0.12951029162920036
X	0.0196972033650128	2957	2.957	0.18815585973921772
X	0.019735394621493374	7127	7.127	0.1404256206389565
X	0.019976987075746667	266031	266.031	0.042189001529923165
X	0.02000556587375838	1082105	1082.105	0.026441970244076088
X	0.019869704810750993	42071	42.071	0.07787586077896175
X	0.01970638669863881	9524	9.524	0.1274273166825254
X	0.019971675976684443	159767	159.767	0.050000668445070104
X	0.019950196871213497	194438	194.438	0.04681539371791524
X	0.019809995763880157	4403	4.403	0.16508662883302302
X	0.019938109384117587	15478	15.478	0.10880688503284534
X	0.01993818668712577	52897	52.897	0.07223564227984268
X	0.019977238938744684	267943	267.943	0.04208858735298518
X	0.01993195895148758	807782	807.782	0.02911288674451306
X	0.019946406873582054	249075	249.075	0.04310339829932644
X	0.01992490519953202	32767	32.767	0.08472011322867026
X	0.019842148768492485	6110	6.11	0.1480866233144189
X	0.019816878584818877	5211	5.211	0.1560886742017851
X	0.019853365578626582	132149	132.149	0.05316062597841498
X	0.01992788825437023	23271	23.271	0.0949617576422689
X	0.01992585008822038	56182	56.182	0.0707847801820909
X	0.019995393907515258	230897	230.897	0.044242251948360296
X	0.0199887641242161	666063	666.063	0.031075887794483944
X	0.019911017658551043	14018	14.018	0.11240935795977855
X	0.019835293897031866	4114	4.114	0.16893700251204172
X	0.019897810356047475	40454	40.454	0.07893711912269934
X	0.01996138998527515	195810	195.81	0.046714528599970995
X	0.019949249597588637	52701	52.701	0.07233845618841207
X	0.019819040670619306	11565	11.565	0.11966826576159567
X	0.019842897107846397	5048	5.048	0.15781995361270987
X	0.01991067844180469	34908	34.908	0.08293166099946399
X	0.019857916069309225	4474	4.474	0.16434098929946722
X	0.019927557211005555	39709	39.709	0.07946728424203929
X	0.019917198226915935	8393	8.393	0.13338398066350107
X	0.019900302905542115	20026	20.026	0.09979033755649933
X	0.01990625233137214	16515	16.515	0.10642336226910051
X	0.019884641721690525	10919	10.919	0.12211761153174408
X	0.019858392716178935	6283	6.283	0.14675481020409778
X	0.019994047497824755	217318	217.318	0.045144170065103614
X	0.01994549177856473	485321	485.321	0.034509491104849466
X	0.019945371433575262	116984	116.984	0.055450468579270606
X	0.019954115062771394	78499	78.499	0.06334652308178441
X	0.019946151831222537	29147	29.147	0.08812282677861585
X	0.01984134813371946	12111	12.111	0.11788640513089475
X	0.019932948922509165	26171	26.171	0.0913237518908809
X	0.0199441795261085	220179	220.179	0.04491038312079575
X	0.01987197829535755	19624	19.624	0.10041945414234836
X	0.019929870914855624	77481	77.481	0.0635969697270472
X	0.019894508209460128	20679	20.679	0.09871910853581826
X	0.020004864063916085	1313769	1313.769	0.02478594784794456
X	0.019914473298026352	24773	24.773	0.09298154123504704
X	0.01990894488872064	94491	94.491	0.05950488219661573
X	0.01996428532963046	360462	360.462	0.03811811594400863
X	0.019931874632364323	76859	76.859	0.06377020392045686
X	0.019938608100305556	38265	38.265	0.08046945127499891
X	0.01997312651077651	850372	850.372	0.028638198853015526
X	0.01993215936429478	163140	163.14	0.049620899305931075
X	0.019925572734427296	59813	59.813	0.06932210513164123
X	0.019869921144330224	15528	15.528	0.10856592585484291
X	0.01993564879431595	44593	44.593	0.07646356829092661
X	0.019935341317101105	24338	24.338	0.09356489394248237
X	0.01993496575730291	174363	174.363	0.04853485320745397
X	0.01996738825696326	970488	970.488	0.02740166994898378
X	0.01997232966637901	124548	124.548	0.054328867981643506
X	0.01994497012770618	209564	209.564	0.045656810612946304
X	0.019927280342593445	47286	47.286	0.07497304517458374
X	0.019891484757037826	27089	27.089	0.09021762902237722
X	0.019899778766322023	39894	39.894	0.07930737127207337
X	0.01959142702719643	21045	21.045	0.0976425351951883
X	0.019970429551244413	510208	510.208	0.03395314807214242
X	0.01995028410362531	527186	527.186	0.03357338046282029
X	0.01992899582548497	42936	42.936	0.07742619992630233
X	0.01995070372235441	1835285	1835.285	0.022152269928159666
X	0.0199268087993872	137151	137.151	0.05257100244547571
X	0.01991611122949519	64263	64.263	0.06767285760628745
X	0.019801016226404598	7131	7.131	0.14055479985554617
X	0.019941607516039298	75187	75.187	0.06424990195256715
X	0.01993461592296708	198318	198.318	0.04649596291195657
X	0.019882205929461563	6908	6.908	0.142245132097662
X	0.019986914093403106	994439	994.439	0.02718874735884868
X	0.019940404190486084	38052	38.052	0.08062173803051408
X	0.019911493682552444	34644	34.644	0.08314291945719382
X	0.019917540775247853	16710	16.71	0.10602780117341118
X	0.0199023379700196	22173	22.173	0.09646280356482685
X	0.01981342444680504	3513	3.513	0.17800292184229785
X	0.019887235129617046	22228	22.228	0.0963587905112108
X	0.019913565472415472	29477	29.477	0.08774490638356916
X	0.01992133998318495	58320	58.32	0.06990372904709528
X	0.01997207064676355	1739763	1739.763	0.02255854065555804
X	0.019879560761090855	31670	31.67	0.08562217197106906
X	0.019935566477815737	272548	272.548	0.041821075167765315
X	0.019826228197558853	8925	8.925	0.13048000317152678
X	0.019920564629106127	34873	34.873	0.08297312453474576
X	0.019833667604021686	11117	11.117	0.12128445517149732
X	0.01991158161470203	144795	144.795	0.051615966557923255
X	0.019899837160740088	138258	138.258	0.05240665221603075
X	0.01999568428747261	1528605	1528.605	0.023562082108723518
X	0.01992705855347419	14683	14.683	0.11071576521889187
X	0.019946089363983614	237699	237.699	0.04378010531396793
X	0.019938456466119137	29766	29.766	0.08749643380435333
X	0.019907392505926103	180351	180.351	0.04796950918581223
X	0.019925290014389845	274494	274.494	0.04171484156655269
X	0.019923588106638607	37399	37.399	0.08106546065679451
X	0.019907521566202617	14156	14.156	0.11203633160061549
X	0.019922400895925185	112603	112.603	0.056138900763848226
X	0.019941928891053354	50649	50.649	0.07329349463846059
X	0.019918253017216295	113340	113.34	0.05601306601398309
X	0.01996102754552841	446309	446.309	0.03549624525797397
X	0.01993527612778371	16618	16.618	0.1062546227353027
X	0.019967877049253444	70513	70.513	0.06566806718855704
X	0.019827705280467022	38977	38.977	0.07982789583295756
X	0.019959911317928444	192605	192.605	0.0469710588122481
X	0.019923619074025765	315802	315.802	0.039809291760940516
X	0.019916965819083757	51307	51.307	0.07294835767529378
X	0.01983035386260047	12319	12.319	0.1171974969370706
X	0.0199892210390488	350564	350.564	0.0384895515967043
X	0.019949201465596814	126589	126.589	0.05401443641593388
X	0.019957925272365905	879549	879.549	0.02831077670214884
X	0.019939847898848694	106392	106.392	0.05722743613728462
X	0.01992569557133569	99865	99.865	0.05843414961012654
X	0.019912123790712777	41841	41.841	0.0780737745014274
X	0.019852596876791407	151053	151.053	0.05084278397286541
X	0.019928699504433132	299619	299.619	0.04051693047276649
X	0.019876431605801714	65730	65.73	0.06712097587906747
X	0.019802817180085166	9615	9.615	0.12723088941975833
X	0.019907108637048084	22284	22.284	0.09631006565046611
X	0.01993337315888311	58481	58.481	0.06985358004209259
X	0.01993828763947181	127390	127.39	0.05389115685466115
X	0.01993751558356989	88973	88.973	0.06073946120534239
X	0.019907433615472616	12258	12.258	0.11754348236379475
X	0.019919779868056667	34355	34.355	0.08338697012052222
X	0.019937951438230708	101625	101.625	0.058106759775898124
X	0.019941439032165383	78281	78.281	0.06339184277444361
X	0.01998911058792382	41076	41.076	0.078656588911993
X	0.019945188064586755	75529	75.529	0.06415661871373604
X	0.019907900508980836	39733	39.733	0.0794251483733327
X	0.019947054865310888	31882	31.882	0.08552854322592875
X	0.019803421411924302	4172	4.172	0.1680603678658219
X	0.019895647016344376	13329	13.329	0.11428436855687603
X	0.01994658507471335	220916	220.916	0.04486218901811367
X	0.019912477064244396	32953	32.953	0.08454282909704862
X	0.019977771629971153	108083	108.083	0.05696348497899618
X	0.019889712624814856	3148	3.148	0.1848692140486229
X	0.019821120400066346	8056	8.056	0.13500077055065646
X	0.01992134581156808	139842	139.842	0.05222683195400956
X	0.019906943558403795	118464	118.464	0.055183096200791634
X	0.019932356092364607	40793	40.793	0.07876337741169928
X	0.01987603960288733	6958	6.958	0.1418889178049727
X	0.019789991789626503	7260	7.26	0.13969139824097768
X	0.01991845125444363	39479	39.479	0.0796091782847336
X	0.019924980191768575	32886	32.886	0.08461790758010038
X	0.019919028763512963	141692	141.692	0.051996519731116464
X	0.019892995756484465	10493	10.493	0.12376566292230783
X	0.019884795991314517	22643	22.643	0.09576255491106714
X	0.01988735534916101	39653	39.653	0.07945117565516086
X	0.01986287965460217	14433	14.433	0.11123168935875674
X	0.019948907436503006	242195	242.195	0.04350955558685172
X	0.019908134064632977	19619	19.619	0.10048885470610654
X	0.019934106140676665	119620	119.62	0.05502976473384356
X	0.01999497826801535	198969	198.969	0.046492029682856366
X	0.019943739946434863	45349	45.349	0.07604656979636099
X	0.019934968564209378	109610	109.61	0.05665720233892228
X	0.019948883666693302	162480	162.48	0.04970188907653939
X	0.019870128274514568	4226	4.226	0.16752915296402107
X	0.019939467050873867	66491	66.491	0.06693453138147434
X	0.019940392374534124	31932	31.932	0.08547436041304605
X	0.01992370319142012	319063	319.063	0.0396732585879442
X	0.019938566671909047	142023	142.023	0.05197307558008791
X	0.019764726685107994	57363	57.363	0.07010564440094373
X	0.019976503968256246	120373	120.373	0.05495368253402847
X	0.019898585743365622	34419	34.419	0.08330568775138228
X	0.019898959555907966	23493	23.493	0.09461586487021321
X	0.01986977122427433	7058	7.058	0.1412007719145697
X	0.019862702222022372	17456	17.456	0.10439936717905962
X	0.019992638457718696	361414	361.414	0.03810263819377083
X	0.019990056833090405	848711	848.711	0.028664963906743553
X	0.019934474187546767	55834	55.834	0.07094176905859033
X	0.019934185608450474	21156	21.156	0.0980366162545935
X	0.019948724995600995	43702	43.702	0.07699655191600567
X	0.019969417644349133	522729	522.729	0.03367929059422711
X	0.019923723250835718	40371	40.371	0.07902545368659719
X	0.019823379688036352	20628	20.628	0.09868251111660457
X	0.019941415311925517	89213	89.213	0.06068890172506719
X	0.019937083025228546	141152	141.152	0.05207846724864611
X	0.01984777110087026	15042	15.042	0.10968200634035753
X	0.019899162743022322	33045	33.045	0.08444546844762446
X	0.019940484130524735	63792	63.792	0.06786666266035683
time for making epsilon is 3.471850633621216
epsilons are
[0.2085311505392515, 0.08155745380704724, 0.16462090640777297, 0.11691198085557306, 0.07050481904363677, 0.06651598361730438, 0.06028015791441615, 0.06332425017194979, 0.09097942374829783, 0.07199706167927819, 0.08681963921338225, 0.05306641798248842, 0.12812088882340347, 0.03712733074492441, 0.08137859941051451, 0.06762813356844292, 0.06309874326953721, 0.06782785113045413, 0.04703221874311447, 0.04806635425654296, 0.07352247990048862, 0.04097998143404128, 0.10285054724751891, 0.07822524316114926, 0.07696223728936559, 0.05833169071149853, 0.05533022400909279, 0.08643928788476325, 0.04086509508642608, 0.022392571728457922, 0.09079186107977681, 0.023773751148307272, 0.09220969333756417, 0.09170475496688518, 0.09136486460156311, 0.03373509614653209, 0.05714022152305441, 0.2014024073852843, 0.15084853694651468, 0.15701094479604388, 0.15370199116812044, 0.18551830400991953, 0.2183561979474941, 0.14272882700249875, 0.24254901064661644, 0.19149933987521744, 0.15153949232616756, 0.1280359974616035, 0.17738264199572298, 0.10963529191494419, 0.09650018277871161, 0.20936861166555729, 0.13544783869314853, 0.14037095564505572, 0.13082614266618642, 0.14188687099793257, 0.17123937266097997, 0.13482899343914567, 0.1530585165452954, 0.16845367279554807, 0.14940019694787168, 0.19470774704819752, 0.13361388170360805, 0.09929076372761983, 0.1548350296012766, 0.16046737807670625, 0.1489922945372348, 0.13652597117013862, 0.13507740378544073, 0.14969244461858636, 0.15189144002721172, 0.15209852996367768, 0.14673404557683217, 0.16001929886583466, 0.14558186034180984, 0.10653676472097541, 0.22781797999795128, 0.1958814392328782, 0.15234667987332334, 0.09118303079410287, 0.15950213003615799, 0.18401295908392326, 0.15418269040598087, 0.195273365019823, 0.1899279754594658, 0.17041578032339977, 0.17621128152561855, 0.20586204591755405, 0.1518288286772839, 0.14448605966803144, 0.14865987549141982, 0.17466491880912727, 0.18057536201973381, 0.12557124292461672, 0.15138907997788176, 0.13372014095687862, 0.168236078982222, 0.14572407804988022, 0.14989156729811967, 0.12925388714816027, 0.10303806529701619, 0.14456047340965358, 0.12691811087899316, 0.18020004635999912, 0.10061582279530318, 0.1558898645246689, 0.13874557727299405, 0.17326917602828995, 0.16434560872759738, 0.23354401279906428, 0.13925598945123188, 0.20806711891856766, 0.18825243601223807, 0.16454653468713834, 0.1345669306250549, 0.17542629637344653, 0.15946745219467343, 0.22781926756226192, 0.11908011525995806, 0.19792037999543008, 0.1038717266477727, 0.16685959918473742, 0.1759345850421037, 0.15623827469216608, 0.15439904780244632, 0.18891027006214522, 0.1826152158003647, 0.15337557814650155, 0.11267012804493601, 0.17105328816857993, 0.10762477690943441, 0.2009395161450359, 0.15640428947156337, 0.1980075782342562, 0.16475025398907103, 0.16501848698320845, 0.2047369892735932, 0.17677042007010133, 0.16860148706771852, 0.17737323868457838, 0.15228554863159854, 0.17967519324414905, 0.10216010957023937, 0.12741110531756958, 0.14245157643741266, 0.17045799015927576, 0.18551660021287517, 0.18648358878835777, 0.1758208555931329, 0.180436175469378, 0.1643768697430047, 0.16555227365337047, 0.15444143860358614, 0.1311319394921901, 0.1735609311392916, 0.10323277324749859, 0.14577809291807847, 0.14952394009582903, 0.16009278798660664, 0.26589860630858764, 0.11254269199154585, 0.201330923273402, 0.11631022602902999, 0.2320540720526249, 0.13569542479626973, 0.15593344522336347, 0.1811465396310627, 0.14740447426058428, 0.13271515990991833, 0.2306549559742854, 0.20708337764577245, 0.1676543737267264, 0.12449427522977591, 0.20102956853421974, 0.14249511657052938, 0.1452137008920324, 0.1288380656772339, 0.16886802599733264, 0.14765435579966965, 0.16318754048817216, 0.12995638674387588, 0.19163839287546644, 0.15751319816663337, 0.16143255503625978, 0.1377442545639472, 0.13582487927167308, 0.17107161981098526, 0.14709439231249438, 0.11470393454882978, 0.11465915446300606, 0.1745603899314954, 0.2360736672451574, 0.15383397974654173, 0.14920503942902827, 0.154517131441588, 0.15353032919926043, 0.1499793682306128, 0.18886147915210705, 0.1451419903840152, 0.21739138901180832, 0.13223485707435234, 0.16398406587219905, 0.11468611913087143, 0.16442822147121464, 0.11468934331968379, 0.18472671810855468, 0.1612476134973463, 0.15277911517007567, 0.16798988143445648, 0.12673930926601953, 0.14310472082500567, 0.1512431059951062, 0.15725216417840737, 0.13887572423740854, 0.24477017895490571, 0.2001879618212376, 0.23571500963474126, 0.20496935368067368, 0.09655525918152058, 0.1711534352430468, 0.18409386754528104, 0.22887567614768192, 0.12171684902301452, 0.1690908551270033, 0.15809221618313968, 0.1518978528099621, 0.17327742017866052, 0.1926308998367555, 0.14348706327614516, 0.23876265793906878, 0.17906445951657998, 0.157807749146502, 0.16395505768115362, 0.1414302168279904, 0.15445253739624007, 0.1623839259341321, 0.13960826468636023, 0.13762594811172127, 0.18594805541882492, 0.09947078634569768, 0.07245534010027986, 0.1256719006469703, 0.14546332372443557, 0.18557154817162647, 0.10618854710865715, 0.1080028804639349, 0.057861298381015225, 0.13929292252726394, 0.04507706614013495, 0.05423825795459934, 0.0673758437550099, 0.05212925795623491, 0.18092056004035018, 0.12935279472067698, 0.03428294481087225, 0.03283565269821316, 0.1420002094613521, 0.07548612678707076, 0.06874803692076552, 0.08627711379902804, 0.06621340524975665, 0.07300656378064645, 0.09175018614274284, 0.10031020182955985, 0.1371695622019234, 0.05258423352824656, 0.12159945740480789, 0.1482132868082585, 0.0777063569565318, 0.07180346149565238, 0.03717066738530836, 0.05689136093632402, 0.17751470712646564, 0.07321495305210951, 0.12932000649550499, 0.08566629681972879, 0.06260546501392757, 0.07139544912142888, 0.05545541464080413, 0.2268610197272696, 0.1263131651709706, 0.05794183093124304, 0.061112722912867616, 0.06447358463821234, 0.15376656067499778, 0.038044946487156533, 0.09425892765491856, 0.08092898357917135, 0.05427740669048328, 0.17123525177205076, 0.060584033157442506, 0.045845790221675474, 0.027877938303864407, 0.07778860059400784, 0.07622878461132589, 0.11009380672518669, 0.12962376959677527, 0.04701219818494845, 0.117252665951163, 0.0817996785435779, 0.06641582826291018, 0.07385401380751956, 0.10615498317936709, 0.1259064842807848, 0.043269301611378354, 0.0987244515242895, 0.1054119722571062, 0.12023074364194104, 0.04950619714271137, 0.09597501165505545, 0.05491698968124191, 0.10193726373621385, 0.08552598110732615, 0.09723547133149715, 0.03757158671025072, 0.0668519158482146, 0.10542397252471226, 0.04690005430527255, 0.04437402472719083, 0.1454468742759775, 0.05255955193673116, 0.04617175766845652, 0.03254170445551911, 0.09644737591828847, 0.15200963064052586, 0.0995360107963607, 0.0989484306021245, 0.07764113479876486, 0.07741533808195196, 0.17816228333453363, 0.06462821995520437, 0.04440281216899363, 0.051592529986851494, 0.12179369926654784, 0.060048727694532796, 0.10128515502263263, 0.0903301606728988, 0.10246803231288765, 0.14010134701159854, 0.04550553937608863, 0.0699286853833414, 0.09348514352038059, 0.06730019530353117, 0.12433887896564247, 0.03006488579683396, 0.10548335048026825, 0.04891674177211523, 0.17656679813882267, 0.1466078136041476, 0.15304800327817739, 0.10075756505088661, 0.12588164100132596, 0.07423180875722443, 0.14178588654842683, 0.14081728476447297, 0.03882599870858264, 0.06881424547045252, 0.07676920141939607, 0.05081486217725388, 0.03584721288403222, 0.14401573484552374, 0.12232007891258284, 0.09271564862512872, 0.1739745526893164, 0.1237244627325973, 0.09432184147367785, 0.1247034731086323, 0.12571941868871447, 0.1085168956267617, 0.10303328417715213, 0.027430768554569617, 0.11395882702396745, 0.059437584141815075, 0.09743720554579474, 0.1585426924742572, 0.058674681923369866, 0.038952948775035044, 0.030546130894336707, 0.06774585143791892, 0.07239502467602858, 0.16970097422318656, 0.062196740675802754, 0.0233739990253249, 0.06283730565269645, 0.1083262754677375, 0.05858288757221725, 0.032731244210048, 0.05876264606715514, 0.10812622857159977, 0.116819994012353, 0.1081455865455853, 0.129554588139337, 0.03874209261588032, 0.09311441935156228, 0.18443562881995804, 0.0660476000927758, 0.09004546899788671, 0.18212572094959104, 0.05450804156435443, 0.056282713820773496, 0.05054790058165608, 0.045378444173609506, 0.04310364118565294, 0.0829923330545187, 0.06669846583256651, 0.051135678781305385, 0.03803777047761046, 0.11418812720227585, 0.056898912255559944, 0.05049359222386138, 0.05405773954851317, 0.06204983802795615, 0.07803731521771931, 0.10002941971924273, 0.12077969917715912, 0.063111594689802, 0.04540076804988221, 0.0489043861724588, 0.042400779805188304, 0.048853421404867425, 0.055882308261423264, 0.09438668122518819, 0.037866039507775444, 0.15186871447278957, 0.09995662229153499, 0.040174811394737, 0.07378604347837055, 0.1389344662651572, 0.08442358516072773, 0.039480459193904506, 0.07830535973325088, 0.05585543729303365, 0.14097012266413614, 0.11326427011889764, 0.13066701784793758, 0.07382616994939921, 0.0745771263159596, 0.05975430844126585, 0.14947837138097372, 0.049287012228789714, 0.11582499811565407, 0.06565142322093595, 0.10913302711836469, 0.05378783171247526, 0.14856542996772815, 0.07760939026878488, 0.07880139059066299, 0.08548005278234717, 0.06423729182052283, 0.03303961322474836, 0.12029728128502143, 0.11130939646402123, 0.054584020518365216, 0.08691738143567701, 0.030533367951740473, 0.15006061207860774, 0.048825788280528505, 0.03611487420712962, 0.13645637361620672, 0.04354157555771474, 0.09518854887724691, 0.03545029597495763, 0.09547100347446837, 0.07394449141205321, 0.1468085207597855, 0.13881347388239582, 0.1315432911930615, 0.07042750659632765, 0.08602504840483971, 0.04791568870859098, 0.09029574253165605, 0.14570516199480707, 0.08935121435452494, 0.053227438504036884, 0.07364069193426111, 0.0410751960671979, 0.06481235295452238, 0.1294366674857703, 0.1577202020109279, 0.13287689091062108, 0.03165712209419645, 0.12082393369260097, 0.08458442587678958, 0.06081342091393041, 0.08837374199702712, 0.13522140162789678, 0.13029578586929472, 0.045400964030676935, 0.0705608784529433, 0.07197932649583133, 0.09926943645497821, 0.06023212585082013, 0.05922935995498135, 0.04437295943821948, 0.052053896520416795, 0.13453524673927125, 0.05712306092976915, 0.09324657411919823, 0.08073761754663861, 0.09417048819801804, 0.13421874788944035, 0.08459345976380067, 0.08089854835665244, 0.13299650907389635, 0.0863453880759454, 0.12677014498195666, 0.05291959109306639, 0.14921682312257803, 0.1036789666830362, 0.09990105145216423, 0.084385944686161, 0.14269929973601286, 0.1005785321403867, 0.08186087048186794, 0.04948766354988893, 0.12951029162920036, 0.18815585973921772, 0.1404256206389565, 0.042189001529923165, 0.026441970244076088, 0.07787586077896175, 0.1274273166825254, 0.050000668445070104, 0.04681539371791524, 0.16508662883302302, 0.10880688503284534, 0.07223564227984268, 0.04208858735298518, 0.02911288674451306, 0.04310339829932644, 0.08472011322867026, 0.1480866233144189, 0.1560886742017851, 0.05316062597841498, 0.0949617576422689, 0.0707847801820909, 0.044242251948360296, 0.031075887794483944, 0.11240935795977855, 0.16893700251204172, 0.07893711912269934, 0.046714528599970995, 0.07233845618841207, 0.11966826576159567, 0.15781995361270987, 0.08293166099946399, 0.16434098929946722, 0.07946728424203929, 0.13338398066350107, 0.09979033755649933, 0.10642336226910051, 0.12211761153174408, 0.14675481020409778, 0.045144170065103614, 0.034509491104849466, 0.055450468579270606, 0.06334652308178441, 0.08812282677861585, 0.11788640513089475, 0.0913237518908809, 0.04491038312079575, 0.10041945414234836, 0.0635969697270472, 0.09871910853581826, 0.02478594784794456, 0.09298154123504704, 0.05950488219661573, 0.03811811594400863, 0.06377020392045686, 0.08046945127499891, 0.028638198853015526, 0.049620899305931075, 0.06932210513164123, 0.10856592585484291, 0.07646356829092661, 0.09356489394248237, 0.04853485320745397, 0.02740166994898378, 0.054328867981643506, 0.045656810612946304, 0.07497304517458374, 0.09021762902237722, 0.07930737127207337, 0.0976425351951883, 0.03395314807214242, 0.03357338046282029, 0.07742619992630233, 0.022152269928159666, 0.05257100244547571, 0.06767285760628745, 0.14055479985554617, 0.06424990195256715, 0.04649596291195657, 0.142245132097662, 0.02718874735884868, 0.08062173803051408, 0.08314291945719382, 0.10602780117341118, 0.09646280356482685, 0.17800292184229785, 0.0963587905112108, 0.08774490638356916, 0.06990372904709528, 0.02255854065555804, 0.08562217197106906, 0.041821075167765315, 0.13048000317152678, 0.08297312453474576, 0.12128445517149732, 0.051615966557923255, 0.05240665221603075, 0.023562082108723518, 0.11071576521889187, 0.04378010531396793, 0.08749643380435333, 0.04796950918581223, 0.04171484156655269, 0.08106546065679451, 0.11203633160061549, 0.056138900763848226, 0.07329349463846059, 0.05601306601398309, 0.03549624525797397, 0.1062546227353027, 0.06566806718855704, 0.07982789583295756, 0.0469710588122481, 0.039809291760940516, 0.07294835767529378, 0.1171974969370706, 0.0384895515967043, 0.05401443641593388, 0.02831077670214884, 0.05722743613728462, 0.05843414961012654, 0.0780737745014274, 0.05084278397286541, 0.04051693047276649, 0.06712097587906747, 0.12723088941975833, 0.09631006565046611, 0.06985358004209259, 0.05389115685466115, 0.06073946120534239, 0.11754348236379475, 0.08338697012052222, 0.058106759775898124, 0.06339184277444361, 0.078656588911993, 0.06415661871373604, 0.0794251483733327, 0.08552854322592875, 0.1680603678658219, 0.11428436855687603, 0.04486218901811367, 0.08454282909704862, 0.05696348497899618, 0.1848692140486229, 0.13500077055065646, 0.05222683195400956, 0.055183096200791634, 0.07876337741169928, 0.1418889178049727, 0.13969139824097768, 0.0796091782847336, 0.08461790758010038, 0.051996519731116464, 0.12376566292230783, 0.09576255491106714, 0.07945117565516086, 0.11123168935875674, 0.04350955558685172, 0.10048885470610654, 0.05502976473384356, 0.046492029682856366, 0.07604656979636099, 0.05665720233892228, 0.04970188907653939, 0.16752915296402107, 0.06693453138147434, 0.08547436041304605, 0.0396732585879442, 0.05197307558008791, 0.07010564440094373, 0.05495368253402847, 0.08330568775138228, 0.09461586487021321, 0.1412007719145697, 0.10439936717905962, 0.03810263819377083, 0.028664963906743553, 0.07094176905859033, 0.0980366162545935, 0.07699655191600567, 0.03367929059422711, 0.07902545368659719, 0.09868251111660457, 0.06068890172506719, 0.05207846724864611, 0.10968200634035753, 0.08444546844762446, 0.06786666266035683]
0.08983176080489852
Making ranges
torch.Size([49576, 2])
We keep 7.94e+06/7.61e+08 =  1% of the original kernel matrix.

torch.Size([5547, 2])
We keep 1.98e+05/4.73e+06 =  4% of the original kernel matrix.

torch.Size([18302, 2])
We keep 1.31e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([65564, 2])
We keep 1.58e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([59240, 2])
We keep 1.10e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([10692, 2])
We keep 6.00e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([23988, 2])
We keep 2.17e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([22712, 2])
We keep 9.69e+06/1.55e+08 =  6% of the original kernel matrix.

torch.Size([34465, 2])
We keep 4.52e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([98531, 2])
We keep 7.92e+07/3.22e+09 =  2% of the original kernel matrix.

torch.Size([71266, 2])
We keep 1.56e+07/1.56e+09 =  0% of the original kernel matrix.

torch.Size([124343, 2])
We keep 4.39e+07/4.59e+09 =  0% of the original kernel matrix.

torch.Size([79659, 2])
We keep 1.82e+07/1.87e+09 =  0% of the original kernel matrix.

torch.Size([163392, 2])
We keep 7.05e+07/8.27e+09 =  0% of the original kernel matrix.

torch.Size([92215, 2])
We keep 2.34e+07/2.51e+09 =  0% of the original kernel matrix.

torch.Size([145361, 2])
We keep 5.35e+07/6.16e+09 =  0% of the original kernel matrix.

torch.Size([86235, 2])
We keep 2.06e+07/2.17e+09 =  0% of the original kernel matrix.

torch.Size([44926, 2])
We keep 1.97e+07/6.98e+08 =  2% of the original kernel matrix.

torch.Size([49761, 2])
We keep 8.63e+06/7.29e+08 =  1% of the original kernel matrix.

torch.Size([80848, 2])
We keep 1.38e+08/2.85e+09 =  4% of the original kernel matrix.

torch.Size([64856, 2])
We keep 1.46e+07/1.47e+09 =  0% of the original kernel matrix.

torch.Size([53230, 2])
We keep 1.42e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([53827, 2])
We keep 9.47e+06/8.40e+08 =  1% of the original kernel matrix.

torch.Size([217488, 2])
We keep 2.21e+08/1.78e+10 =  1% of the original kernel matrix.

torch.Size([107921, 2])
We keep 3.27e+07/3.68e+09 =  0% of the original kernel matrix.

torch.Size([20023, 2])
We keep 1.84e+06/8.93e+07 =  2% of the original kernel matrix.

torch.Size([32252, 2])
We keep 3.78e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([740535, 2])
We keep 1.18e+09/1.52e+11 =  0% of the original kernel matrix.

torch.Size([203460, 2])
We keep 8.33e+07/1.07e+10 =  0% of the original kernel matrix.

torch.Size([65404, 2])
We keep 1.49e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([58877, 2])
We keep 1.10e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([120058, 2])
We keep 4.77e+07/4.15e+09 =  1% of the original kernel matrix.

torch.Size([78011, 2])
We keep 1.73e+07/1.78e+09 =  0% of the original kernel matrix.

torch.Size([145665, 2])
We keep 6.56e+07/6.30e+09 =  1% of the original kernel matrix.

torch.Size([86870, 2])
We keep 2.07e+07/2.19e+09 =  0% of the original kernel matrix.

torch.Size([110887, 2])
We keep 5.02e+07/4.07e+09 =  1% of the original kernel matrix.

torch.Size([75974, 2])
We keep 1.75e+07/1.76e+09 =  0% of the original kernel matrix.

torch.Size([329965, 2])
We keep 3.42e+08/3.69e+10 =  0% of the original kernel matrix.

torch.Size([136308, 2])
We keep 4.48e+07/5.30e+09 =  0% of the original kernel matrix.

torch.Size([311967, 2])
We keep 2.41e+08/3.22e+10 =  0% of the original kernel matrix.

torch.Size([131916, 2])
We keep 4.20e+07/4.95e+09 =  0% of the original kernel matrix.

torch.Size([85961, 2])
We keep 4.54e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([66552, 2])
We keep 1.42e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([532411, 2])
We keep 5.01e+08/8.40e+10 =  0% of the original kernel matrix.

torch.Size([173472, 2])
We keep 6.27e+07/8.00e+09 =  0% of the original kernel matrix.

torch.Size([34215, 2])
We keep 5.17e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([43207, 2])
We keep 6.25e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([72500, 2])
We keep 5.60e+07/1.74e+09 =  3% of the original kernel matrix.

torch.Size([61854, 2])
We keep 1.20e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([42549, 2])
We keep 9.66e+07/1.90e+09 =  5% of the original kernel matrix.

torch.Size([45366, 2])
We keep 1.23e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([180516, 2])
We keep 7.98e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([97721, 2])
We keep 2.52e+07/2.77e+09 =  0% of the original kernel matrix.

torch.Size([209141, 2])
We keep 1.28e+08/1.39e+10 =  0% of the original kernel matrix.

torch.Size([106113, 2])
We keep 2.91e+07/3.25e+09 =  0% of the original kernel matrix.

torch.Size([53427, 2])
We keep 2.55e+07/9.49e+08 =  2% of the original kernel matrix.

torch.Size([53965, 2])
We keep 9.41e+06/8.49e+08 =  1% of the original kernel matrix.

torch.Size([404554, 2])
We keep 8.89e+08/8.52e+10 =  1% of the original kernel matrix.

torch.Size([147211, 2])
We keep 6.54e+07/8.05e+09 =  0% of the original kernel matrix.

torch.Size([3896051, 2])
We keep 1.40e+10/3.17e+12 =  0% of the original kernel matrix.

torch.Size([491495, 2])
We keep 3.35e+08/4.91e+10 =  0% of the original kernel matrix.

torch.Size([43421, 2])
We keep 1.15e+07/7.07e+08 =  1% of the original kernel matrix.

torch.Size([48593, 2])
We keep 8.39e+06/7.33e+08 =  1% of the original kernel matrix.

torch.Size([2974966, 2])
We keep 1.22e+10/2.21e+12 =  0% of the original kernel matrix.

torch.Size([430241, 2])
We keep 2.88e+08/4.10e+10 =  0% of the original kernel matrix.

torch.Size([44624, 2])
We keep 1.47e+07/6.45e+08 =  2% of the original kernel matrix.

torch.Size([48941, 2])
We keep 7.77e+06/7.00e+08 =  1% of the original kernel matrix.

torch.Size([45446, 2])
We keep 1.08e+07/6.68e+08 =  1% of the original kernel matrix.

torch.Size([50427, 2])
We keep 8.39e+06/7.13e+08 =  1% of the original kernel matrix.

torch.Size([45209, 2])
We keep 1.01e+07/6.81e+08 =  1% of the original kernel matrix.

torch.Size([50449, 2])
We keep 8.50e+06/7.20e+08 =  1% of the original kernel matrix.

torch.Size([917521, 2])
We keep 2.08e+09/2.70e+11 =  0% of the original kernel matrix.

torch.Size([230923, 2])
We keep 1.10e+08/1.43e+10 =  0% of the original kernel matrix.

torch.Size([191042, 2])
We keep 1.19e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([101306, 2])
We keep 2.68e+07/2.95e+09 =  0% of the original kernel matrix.

torch.Size([6232, 2])
We keep 2.39e+05/5.82e+06 =  4% of the original kernel matrix.

torch.Size([19222, 2])
We keep 1.43e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([13653, 2])
We keep 8.97e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([26691, 2])
We keep 2.61e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([12118, 2])
We keep 7.67e+05/2.62e+07 =  2% of the original kernel matrix.

torch.Size([25329, 2])
We keep 2.42e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([13044, 2])
We keep 7.82e+05/3.00e+07 =  2% of the original kernel matrix.

torch.Size([26158, 2])
We keep 2.50e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([6473, 2])
We keep 6.33e+05/9.27e+06 =  6% of the original kernel matrix.

torch.Size([19083, 2])
We keep 1.65e+06/8.40e+07 =  1% of the original kernel matrix.

torch.Size([4846, 2])
We keep 1.67e+05/3.55e+06 =  4% of the original kernel matrix.

torch.Size([17344, 2])
We keep 1.21e+06/5.20e+07 =  2% of the original kernel matrix.

torch.Size([15248, 2])
We keep 1.24e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([27975, 2])
We keep 2.98e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([3714, 2])
We keep 9.64e+04/1.92e+06 =  5% of the original kernel matrix.

torch.Size([15654, 2])
We keep 9.65e+05/3.82e+07 =  2% of the original kernel matrix.

torch.Size([7283, 2])
We keep 2.92e+05/7.98e+06 =  3% of the original kernel matrix.

torch.Size([20468, 2])
We keep 1.58e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([12887, 2])
We keep 9.24e+05/3.16e+07 =  2% of the original kernel matrix.

torch.Size([25961, 2])
We keep 2.57e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([19697, 2])
We keep 1.94e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([31970, 2])
We keep 3.79e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([9061, 2])
We keep 4.21e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([22406, 2])
We keep 1.85e+06/9.82e+07 =  1% of the original kernel matrix.

torch.Size([28315, 2])
We keep 5.67e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([39007, 2])
We keep 5.40e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([38700, 2])
We keep 7.08e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([46249, 2])
We keep 7.35e+06/6.11e+08 =  1% of the original kernel matrix.

torch.Size([5540, 2])
We keep 1.90e+05/4.63e+06 =  4% of the original kernel matrix.

torch.Size([18370, 2])
We keep 1.32e+06/5.94e+07 =  2% of the original kernel matrix.

torch.Size([17431, 2])
We keep 1.66e+06/6.38e+07 =  2% of the original kernel matrix.

torch.Size([29938, 2])
We keep 3.32e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([15521, 2])
We keep 1.53e+06/5.11e+07 =  2% of the original kernel matrix.

torch.Size([28287, 2])
We keep 3.11e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([19650, 2])
We keep 1.70e+06/7.89e+07 =  2% of the original kernel matrix.

torch.Size([31764, 2])
We keep 3.58e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([15441, 2])
We keep 1.44e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([28253, 2])
We keep 3.00e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([9824, 2])
We keep 4.76e+05/1.57e+07 =  3% of the original kernel matrix.

torch.Size([23074, 2])
We keep 1.98e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([17887, 2])
We keep 1.49e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([30311, 2])
We keep 3.35e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([13012, 2])
We keep 8.69e+05/3.09e+07 =  2% of the original kernel matrix.

torch.Size([26068, 2])
We keep 2.56e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([10158, 2])
We keep 5.94e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([23350, 2])
We keep 2.09e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([14312, 2])
We keep 9.35e+05/3.55e+07 =  2% of the original kernel matrix.

torch.Size([27370, 2])
We keep 2.68e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([6950, 2])
We keep 2.66e+05/7.26e+06 =  3% of the original kernel matrix.

torch.Size([20221, 2])
We keep 1.53e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([18603, 2])
We keep 1.67e+06/6.93e+07 =  2% of the original kernel matrix.

torch.Size([31173, 2])
We keep 3.45e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([36931, 2])
We keep 5.73e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([44899, 2])
We keep 6.83e+06/5.60e+08 =  1% of the original kernel matrix.

torch.Size([12601, 2])
We keep 8.32e+05/2.86e+07 =  2% of the original kernel matrix.

torch.Size([25768, 2])
We keep 2.49e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([11592, 2])
We keep 6.90e+05/2.32e+07 =  2% of the original kernel matrix.

torch.Size([24849, 2])
We keep 2.32e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([14302, 2])
We keep 9.36e+05/3.62e+07 =  2% of the original kernel matrix.

torch.Size([27350, 2])
We keep 2.70e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([16495, 2])
We keep 1.51e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([29121, 2])
We keep 3.26e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([17953, 2])
We keep 1.43e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([30370, 2])
We keep 3.31e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([14071, 2])
We keep 9.81e+05/3.51e+07 =  2% of the original kernel matrix.

torch.Size([27084, 2])
We keep 2.70e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([12329, 2])
We keep 1.25e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([25540, 2])
We keep 2.61e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([13049, 2])
We keep 8.99e+05/3.19e+07 =  2% of the original kernel matrix.

torch.Size([26068, 2])
We keep 2.61e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([14106, 2])
We keep 1.05e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([26958, 2])
We keep 2.77e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([11000, 2])
We keep 8.23e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([24172, 2])
We keep 2.32e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([14362, 2])
We keep 1.11e+06/4.11e+07 =  2% of the original kernel matrix.

torch.Size([27197, 2])
We keep 2.86e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([29373, 2])
We keep 5.96e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([39751, 2])
We keep 5.86e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([4386, 2])
We keep 1.29e+05/2.80e+06 =  4% of the original kernel matrix.

torch.Size([16946, 2])
We keep 1.13e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([6910, 2])
We keep 2.58e+05/6.95e+06 =  3% of the original kernel matrix.

torch.Size([20103, 2])
We keep 1.51e+06/7.27e+07 =  2% of the original kernel matrix.

torch.Size([12977, 2])
We keep 8.58e+05/3.15e+07 =  2% of the original kernel matrix.

torch.Size([26047, 2])
We keep 2.52e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([45740, 2])
We keep 8.79e+06/6.89e+08 =  1% of the original kernel matrix.

torch.Size([50997, 2])
We keep 8.59e+06/7.24e+08 =  1% of the original kernel matrix.

torch.Size([11841, 2])
We keep 6.97e+05/2.38e+07 =  2% of the original kernel matrix.

torch.Size([24923, 2])
We keep 2.33e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([7667, 2])
We keep 3.95e+05/1.01e+07 =  3% of the original kernel matrix.

torch.Size([20908, 2])
We keep 1.71e+06/8.75e+07 =  1% of the original kernel matrix.

torch.Size([12714, 2])
We keep 9.94e+05/2.93e+07 =  3% of the original kernel matrix.

torch.Size([25938, 2])
We keep 2.50e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([7008, 2])
We keep 2.92e+05/6.91e+06 =  4% of the original kernel matrix.

torch.Size([20033, 2])
We keep 1.50e+06/7.25e+07 =  2% of the original kernel matrix.

torch.Size([7277, 2])
We keep 2.99e+05/8.44e+06 =  3% of the original kernel matrix.

torch.Size([20397, 2])
We keep 1.61e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([10167, 2])
We keep 5.04e+05/1.61e+07 =  3% of the original kernel matrix.

torch.Size([23424, 2])
We keep 2.02e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([9133, 2])
We keep 4.59e+05/1.32e+07 =  3% of the original kernel matrix.

torch.Size([22367, 2])
We keep 1.89e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([6119, 2])
We keep 2.11e+05/5.13e+06 =  4% of the original kernel matrix.

torch.Size([19170, 2])
We keep 1.36e+06/6.24e+07 =  2% of the original kernel matrix.

torch.Size([13076, 2])
We keep 8.79e+05/3.22e+07 =  2% of the original kernel matrix.

torch.Size([26034, 2])
We keep 2.57e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([15320, 2])
We keep 1.11e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([28144, 2])
We keep 2.89e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([14140, 2])
We keep 9.87e+05/3.62e+07 =  2% of the original kernel matrix.

torch.Size([27125, 2])
We keep 2.70e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([9462, 2])
We keep 4.34e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([22804, 2])
We keep 1.91e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([8245, 2])
We keep 4.04e+05/1.14e+07 =  3% of the original kernel matrix.

torch.Size([21534, 2])
We keep 1.79e+06/9.31e+07 =  1% of the original kernel matrix.

torch.Size([20004, 2])
We keep 2.37e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([32160, 2])
We keep 4.00e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([12976, 2])
We keep 9.64e+05/3.29e+07 =  2% of the original kernel matrix.

torch.Size([26094, 2])
We keep 2.61e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([18346, 2])
We keep 1.55e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([30963, 2])
We keep 3.40e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([10286, 2])
We keep 5.44e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([23657, 2])
We keep 2.08e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([13669, 2])
We keep 1.72e+06/4.10e+07 =  4% of the original kernel matrix.

torch.Size([26821, 2])
We keep 2.83e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([13990, 2])
We keep 9.58e+05/3.47e+07 =  2% of the original kernel matrix.

torch.Size([26915, 2])
We keep 2.69e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([20066, 2])
We keep 1.74e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([32224, 2])
We keep 3.69e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([33546, 2])
We keep 5.05e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([42523, 2])
We keep 6.27e+06/5.01e+08 =  1% of the original kernel matrix.

torch.Size([15323, 2])
We keep 1.18e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([28147, 2])
We keep 2.91e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([21115, 2])
We keep 1.89e+06/9.48e+07 =  1% of the original kernel matrix.

torch.Size([33019, 2])
We keep 3.84e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([8058, 2])
We keep 4.00e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([21195, 2])
We keep 1.80e+06/9.18e+07 =  1% of the original kernel matrix.

torch.Size([33878, 2])
We keep 8.19e+06/3.80e+08 =  2% of the original kernel matrix.

torch.Size([42590, 2])
We keep 6.70e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([12037, 2])
We keep 1.07e+06/2.74e+07 =  3% of the original kernel matrix.

torch.Size([25220, 2])
We keep 2.47e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([16888, 2])
We keep 1.32e+06/5.53e+07 =  2% of the original kernel matrix.

torch.Size([29402, 2])
We keep 3.15e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([9138, 2])
We keep 5.91e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([22455, 2])
We keep 1.96e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([10819, 2])
We keep 6.10e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([24088, 2])
We keep 2.19e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([4138, 2])
We keep 1.17e+05/2.37e+06 =  4% of the original kernel matrix.

torch.Size([16518, 2])
We keep 1.03e+06/4.25e+07 =  2% of the original kernel matrix.

torch.Size([16104, 2])
We keep 1.47e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([28840, 2])
We keep 3.17e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([5514, 2])
We keep 2.09e+05/4.80e+06 =  4% of the original kernel matrix.

torch.Size([18283, 2])
We keep 1.32e+06/6.05e+07 =  2% of the original kernel matrix.

torch.Size([7373, 2])
We keep 3.51e+05/8.67e+06 =  4% of the original kernel matrix.

torch.Size([20537, 2])
We keep 1.64e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([10315, 2])
We keep 6.80e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([23594, 2])
We keep 2.15e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([17804, 2])
We keep 1.63e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([30447, 2])
We keep 3.41e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([8716, 2])
We keep 5.58e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([21909, 2])
We keep 1.91e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([11991, 2])
We keep 6.93e+05/2.41e+07 =  2% of the original kernel matrix.

torch.Size([25208, 2])
We keep 2.31e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([4484, 2])
We keep 1.24e+05/2.80e+06 =  4% of the original kernel matrix.

torch.Size([16942, 2])
We keep 1.10e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([23711, 2])
We keep 2.68e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([35316, 2])
We keep 4.47e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([6405, 2])
We keep 2.57e+05/6.49e+06 =  3% of the original kernel matrix.

torch.Size([19275, 2])
We keep 1.48e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([32677, 2])
We keep 4.81e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([42269, 2])
We keep 6.14e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([10245, 2])
We keep 6.19e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([23423, 2])
We keep 2.11e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([8749, 2])
We keep 4.75e+05/1.32e+07 =  3% of the original kernel matrix.

torch.Size([21948, 2])
We keep 1.89e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([12035, 2])
We keep 7.75e+05/2.72e+07 =  2% of the original kernel matrix.

torch.Size([25345, 2])
We keep 2.45e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([12305, 2])
We keep 9.62e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([25364, 2])
We keep 2.53e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([7512, 2])
We keep 3.28e+05/8.59e+06 =  3% of the original kernel matrix.

torch.Size([20770, 2])
We keep 1.63e+06/8.08e+07 =  2% of the original kernel matrix.

torch.Size([8373, 2])
We keep 3.72e+05/1.06e+07 =  3% of the original kernel matrix.

torch.Size([21601, 2])
We keep 1.74e+06/8.97e+07 =  1% of the original kernel matrix.

torch.Size([13002, 2])
We keep 8.24e+05/3.03e+07 =  2% of the original kernel matrix.

torch.Size([26117, 2])
We keep 2.52e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([27005, 2])
We keep 3.18e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([37993, 2])
We keep 5.07e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([9743, 2])
We keep 4.89e+05/1.57e+07 =  3% of the original kernel matrix.

torch.Size([22900, 2])
We keep 1.97e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([30119, 2])
We keep 3.96e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([40251, 2])
We keep 5.64e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([6175, 2])
We keep 2.60e+05/6.02e+06 =  4% of the original kernel matrix.

torch.Size([19135, 2])
We keep 1.42e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([12221, 2])
We keep 8.89e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([25332, 2])
We keep 2.42e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([6201, 2])
We keep 2.87e+05/6.54e+06 =  4% of the original kernel matrix.

torch.Size([19119, 2])
We keep 1.48e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([9957, 2])
We keep 7.60e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([23102, 2])
We keep 2.14e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([10718, 2])
We keep 6.12e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([23934, 2])
We keep 2.14e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([6242, 2])
We keep 2.20e+05/5.28e+06 =  4% of the original kernel matrix.

torch.Size([19386, 2])
We keep 1.36e+06/6.34e+07 =  2% of the original kernel matrix.

torch.Size([8902, 2])
We keep 4.36e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([22118, 2])
We keep 1.87e+06/9.88e+07 =  1% of the original kernel matrix.

torch.Size([10093, 2])
We keep 5.48e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([23419, 2])
We keep 2.05e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([8732, 2])
We keep 4.39e+05/1.26e+07 =  3% of the original kernel matrix.

torch.Size([21873, 2])
We keep 1.86e+06/9.78e+07 =  1% of the original kernel matrix.

torch.Size([12906, 2])
We keep 8.98e+05/3.18e+07 =  2% of the original kernel matrix.

torch.Size([25880, 2])
We keep 2.60e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([7719, 2])
We keep 4.89e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([20745, 2])
We keep 1.84e+06/9.41e+07 =  1% of the original kernel matrix.

torch.Size([33198, 2])
We keep 5.18e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([42217, 2])
We keep 6.34e+06/5.12e+08 =  1% of the original kernel matrix.

torch.Size([20192, 2])
We keep 1.88e+06/9.16e+07 =  2% of the original kernel matrix.

torch.Size([32404, 2])
We keep 3.84e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([15491, 2])
We keep 1.17e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([28311, 2])
We keep 2.98e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([9881, 2])
We keep 5.11e+05/1.60e+07 =  3% of the original kernel matrix.

torch.Size([23155, 2])
We keep 2.02e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([7871, 2])
We keep 3.49e+05/9.60e+06 =  3% of the original kernel matrix.

torch.Size([21069, 2])
We keep 1.69e+06/8.54e+07 =  1% of the original kernel matrix.

torch.Size([7504, 2])
We keep 3.67e+05/9.42e+06 =  3% of the original kernel matrix.

torch.Size([20693, 2])
We keep 1.67e+06/8.46e+07 =  1% of the original kernel matrix.

torch.Size([9104, 2])
We keep 4.84e+05/1.32e+07 =  3% of the original kernel matrix.

torch.Size([22377, 2])
We keep 1.89e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([8339, 2])
We keep 4.10e+05/1.15e+07 =  3% of the original kernel matrix.

torch.Size([21659, 2])
We keep 1.79e+06/9.34e+07 =  1% of the original kernel matrix.

torch.Size([10521, 2])
We keep 6.48e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([23654, 2])
We keep 2.18e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([10579, 2])
We keep 5.71e+05/1.91e+07 =  2% of the original kernel matrix.

torch.Size([23745, 2])
We keep 2.14e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([12997, 2])
We keep 8.23e+05/2.91e+07 =  2% of the original kernel matrix.

torch.Size([26132, 2])
We keep 2.50e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([17278, 2])
We keep 3.36e+06/7.79e+07 =  4% of the original kernel matrix.

torch.Size([30058, 2])
We keep 3.57e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([9261, 2])
We keep 4.94e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([22546, 2])
We keep 1.94e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([32940, 2])
We keep 6.36e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([42083, 2])
We keep 6.17e+06/4.94e+08 =  1% of the original kernel matrix.

torch.Size([14804, 2])
We keep 1.04e+06/4.04e+07 =  2% of the original kernel matrix.

torch.Size([27639, 2])
We keep 2.80e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([13811, 2])
We keep 1.09e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([26979, 2])
We keep 2.70e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([11819, 2])
We keep 6.68e+05/2.34e+07 =  2% of the original kernel matrix.

torch.Size([24921, 2])
We keep 2.31e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([2879, 2])
We keep 6.19e+04/1.08e+06 =  5% of the original kernel matrix.

torch.Size([14242, 2])
We keep 8.04e+05/2.87e+07 =  2% of the original kernel matrix.

torch.Size([27057, 2])
We keep 9.89e+06/1.96e+08 =  5% of the original kernel matrix.

torch.Size([38163, 2])
We keep 5.12e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([6080, 2])
We keep 2.67e+05/5.70e+06 =  4% of the original kernel matrix.

torch.Size([18937, 2])
We keep 1.39e+06/6.58e+07 =  2% of the original kernel matrix.

torch.Size([22941, 2])
We keep 3.97e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([34706, 2])
We keep 4.72e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([4225, 2])
We keep 1.27e+05/2.52e+06 =  5% of the original kernel matrix.

torch.Size([16611, 2])
We keep 1.07e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([16470, 2])
We keep 2.04e+06/6.23e+07 =  3% of the original kernel matrix.

torch.Size([29277, 2])
We keep 3.33e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([12281, 2])
We keep 8.59e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([25451, 2])
We keep 2.48e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([8239, 2])
We keep 3.73e+05/1.12e+07 =  3% of the original kernel matrix.

torch.Size([21531, 2])
We keep 1.76e+06/9.23e+07 =  1% of the original kernel matrix.

torch.Size([14272, 2])
We keep 1.06e+06/3.82e+07 =  2% of the original kernel matrix.

torch.Size([27270, 2])
We keep 2.76e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([18239, 2])
We keep 1.74e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([30881, 2])
We keep 3.49e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([4296, 2])
We keep 1.19e+05/2.60e+06 =  4% of the original kernel matrix.

torch.Size([16760, 2])
We keep 1.08e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([5437, 2])
We keep 2.01e+05/4.80e+06 =  4% of the original kernel matrix.

torch.Size([18248, 2])
We keep 1.34e+06/6.04e+07 =  2% of the original kernel matrix.

torch.Size([9384, 2])
We keep 7.82e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([22282, 2])
We keep 2.08e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([21505, 2])
We keep 2.18e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([33475, 2])
We keep 3.99e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([6062, 2])
We keep 2.67e+05/5.92e+06 =  4% of the original kernel matrix.

torch.Size([19094, 2])
We keep 1.42e+06/6.71e+07 =  2% of the original kernel matrix.

torch.Size([15068, 2])
We keep 1.25e+06/4.70e+07 =  2% of the original kernel matrix.

torch.Size([27977, 2])
We keep 3.00e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([14236, 2])
We keep 4.35e+06/4.22e+07 = 10% of the original kernel matrix.

torch.Size([27454, 2])
We keep 2.60e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([19708, 2])
We keep 1.88e+06/8.62e+07 =  2% of the original kernel matrix.

torch.Size([31907, 2])
We keep 3.72e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([10017, 2])
We keep 5.54e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([23207, 2])
We keep 2.06e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([13826, 2])
We keep 1.40e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([27029, 2])
We keep 2.74e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([11026, 2])
We keep 6.26e+05/2.09e+07 =  2% of the original kernel matrix.

torch.Size([24212, 2])
We keep 2.23e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([19740, 2])
We keep 1.79e+06/8.21e+07 =  2% of the original kernel matrix.

torch.Size([32053, 2])
We keep 3.68e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([7196, 2])
We keep 3.26e+05/7.94e+06 =  4% of the original kernel matrix.

torch.Size([20382, 2])
We keep 1.57e+06/7.77e+07 =  2% of the original kernel matrix.

torch.Size([11412, 2])
We keep 1.16e+06/2.56e+07 =  4% of the original kernel matrix.

torch.Size([24642, 2])
We keep 2.37e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([11326, 2])
We keep 7.29e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([24447, 2])
We keep 2.27e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([17244, 2])
We keep 1.32e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([29821, 2])
We keep 3.22e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([17495, 2])
We keep 1.40e+06/6.30e+07 =  2% of the original kernel matrix.

torch.Size([30051, 2])
We keep 3.28e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([9638, 2])
We keep 5.07e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([22715, 2])
We keep 2.01e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([14027, 2])
We keep 1.18e+06/3.89e+07 =  3% of the original kernel matrix.

torch.Size([27144, 2])
We keep 2.79e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([26348, 2])
We keep 3.00e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([37383, 2])
We keep 4.89e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([25906, 2])
We keep 3.14e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([36958, 2])
We keep 4.90e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([9285, 2])
We keep 4.59e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([22532, 2])
We keep 1.92e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([4055, 2])
We keep 1.07e+05/2.22e+06 =  4% of the original kernel matrix.

torch.Size([16305, 2])
We keep 1.03e+06/4.11e+07 =  2% of the original kernel matrix.

torch.Size([12994, 2])
We keep 8.05e+05/2.97e+07 =  2% of the original kernel matrix.

torch.Size([26208, 2])
We keep 2.53e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([13973, 2])
We keep 9.85e+05/3.58e+07 =  2% of the original kernel matrix.

torch.Size([27191, 2])
We keep 2.70e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([12482, 2])
We keep 1.12e+06/2.91e+07 =  3% of the original kernel matrix.

torch.Size([25743, 2])
We keep 2.49e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([12974, 2])
We keep 8.33e+05/3.00e+07 =  2% of the original kernel matrix.

torch.Size([26100, 2])
We keep 2.52e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([13787, 2])
We keep 9.87e+05/3.48e+07 =  2% of the original kernel matrix.

torch.Size([26848, 2])
We keep 2.69e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([7482, 2])
We keep 3.10e+05/8.69e+06 =  3% of the original kernel matrix.

torch.Size([20750, 2])
We keep 1.63e+06/8.13e+07 =  2% of the original kernel matrix.

torch.Size([14374, 2])
We keep 1.54e+06/4.17e+07 =  3% of the original kernel matrix.

torch.Size([27258, 2])
We keep 2.89e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([5285, 2])
We keep 1.53e+05/3.75e+06 =  4% of the original kernel matrix.

torch.Size([18124, 2])
We keep 1.22e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([18846, 2])
We keep 1.60e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([31338, 2])
We keep 3.54e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([11027, 2])
We keep 5.87e+05/2.02e+07 =  2% of the original kernel matrix.

torch.Size([24193, 2])
We keep 2.18e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([26546, 2])
We keep 2.89e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([37619, 2])
We keep 4.88e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([10757, 2])
We keep 6.04e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([23868, 2])
We keep 2.19e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([26031, 2])
We keep 3.50e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([37100, 2])
We keep 4.86e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([8078, 2])
We keep 3.52e+05/9.74e+06 =  3% of the original kernel matrix.

torch.Size([21212, 2])
We keep 1.70e+06/8.61e+07 =  1% of the original kernel matrix.

torch.Size([11020, 2])
We keep 8.17e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([24185, 2])
We keep 2.26e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([12976, 2])
We keep 9.04e+05/3.09e+07 =  2% of the original kernel matrix.

torch.Size([25952, 2])
We keep 2.57e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([10454, 2])
We keep 5.59e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([23708, 2])
We keep 2.08e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([20807, 2])
We keep 1.95e+06/9.51e+07 =  2% of the original kernel matrix.

torch.Size([32787, 2])
We keep 3.83e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([15068, 2])
We keep 1.19e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([28041, 2])
We keep 2.94e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([12890, 2])
We keep 1.28e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([25974, 2])
We keep 2.59e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([12038, 2])
We keep 7.30e+05/2.61e+07 =  2% of the original kernel matrix.

torch.Size([25119, 2])
We keep 2.40e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([16543, 2])
We keep 1.32e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([29253, 2])
We keep 3.16e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([3722, 2])
We keep 9.44e+04/1.76e+06 =  5% of the original kernel matrix.

torch.Size([15750, 2])
We keep 9.47e+05/3.66e+07 =  2% of the original kernel matrix.

torch.Size([6118, 2])
We keep 2.99e+05/6.09e+06 =  4% of the original kernel matrix.

torch.Size([19022, 2])
We keep 1.43e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([4208, 2])
We keep 1.09e+05/2.27e+06 =  4% of the original kernel matrix.

torch.Size([16529, 2])
We keep 1.03e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([6120, 2])
We keep 2.16e+05/5.23e+06 =  4% of the original kernel matrix.

torch.Size([19229, 2])
We keep 1.37e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([35788, 2])
We keep 1.07e+07/4.86e+08 =  2% of the original kernel matrix.

torch.Size([43774, 2])
We keep 7.40e+06/6.08e+08 =  1% of the original kernel matrix.

torch.Size([9987, 2])
We keep 4.87e+05/1.57e+07 =  3% of the original kernel matrix.

torch.Size([23309, 2])
We keep 1.99e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([7636, 2])
We keep 4.73e+05/9.92e+06 =  4% of the original kernel matrix.

torch.Size([20880, 2])
We keep 1.70e+06/8.68e+07 =  1% of the original kernel matrix.

torch.Size([4451, 2])
We keep 1.29e+05/2.72e+06 =  4% of the original kernel matrix.

torch.Size([16876, 2])
We keep 1.09e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([22110, 2])
We keep 2.68e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([34045, 2])
We keep 4.28e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([9255, 2])
We keep 8.02e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([22402, 2])
We keep 2.04e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([11405, 2])
We keep 7.46e+05/2.52e+07 =  2% of the original kernel matrix.

torch.Size([24511, 2])
We keep 2.39e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([12811, 2])
We keep 9.00e+05/3.21e+07 =  2% of the original kernel matrix.

torch.Size([25858, 2])
We keep 2.57e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([9229, 2])
We keep 4.63e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([22441, 2])
We keep 1.95e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([6832, 2])
We keep 3.07e+05/7.57e+06 =  4% of the original kernel matrix.

torch.Size([19782, 2])
We keep 1.55e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([15331, 2])
We keep 1.16e+06/4.50e+07 =  2% of the original kernel matrix.

torch.Size([28131, 2])
We keep 2.95e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([3990, 2])
We keep 1.04e+05/2.12e+06 =  4% of the original kernel matrix.

torch.Size([16163, 2])
We keep 1.02e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([8396, 2])
We keep 4.38e+05/1.20e+07 =  3% of the original kernel matrix.

torch.Size([21674, 2])
We keep 1.83e+06/9.55e+07 =  1% of the original kernel matrix.

torch.Size([11860, 2])
We keep 8.11e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([25012, 2])
We keep 2.38e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([11037, 2])
We keep 6.35e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([24290, 2])
We keep 2.20e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([15840, 2])
We keep 1.34e+06/4.95e+07 =  2% of the original kernel matrix.

torch.Size([28751, 2])
We keep 3.03e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([12383, 2])
We keep 9.39e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([25457, 2])
We keep 2.51e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([11336, 2])
We keep 6.88e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([24559, 2])
We keep 2.23e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([16770, 2])
We keep 1.31e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([29470, 2])
We keep 3.13e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([17272, 2])
We keep 1.38e+06/5.82e+07 =  2% of the original kernel matrix.

torch.Size([29951, 2])
We keep 3.17e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([7957, 2])
We keep 3.40e+05/9.49e+06 =  3% of the original kernel matrix.

torch.Size([21204, 2])
We keep 1.66e+06/8.49e+07 =  1% of the original kernel matrix.

torch.Size([36427, 2])
We keep 1.20e+07/4.07e+08 =  2% of the original kernel matrix.

torch.Size([43974, 2])
We keep 6.77e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([74896, 2])
We keep 1.15e+08/2.74e+09 =  4% of the original kernel matrix.

torch.Size([62258, 2])
We keep 1.46e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([20858, 2])
We keep 2.62e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([33118, 2])
We keep 3.83e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([14089, 2])
We keep 1.35e+06/4.16e+07 =  3% of the original kernel matrix.

torch.Size([27200, 2])
We keep 2.82e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([7253, 2])
We keep 4.05e+05/9.51e+06 =  4% of the original kernel matrix.

torch.Size([20372, 2])
We keep 1.70e+06/8.51e+07 =  1% of the original kernel matrix.

torch.Size([30699, 2])
We keep 8.54e+06/2.76e+08 =  3% of the original kernel matrix.

torch.Size([40473, 2])
We keep 5.78e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([30175, 2])
We keep 5.40e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([40301, 2])
We keep 5.65e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([146449, 2])
We keep 4.23e+08/1.06e+10 =  4% of the original kernel matrix.

torch.Size([87677, 2])
We keep 2.58e+07/2.84e+09 =  0% of the original kernel matrix.

torch.Size([16602, 2])
We keep 1.27e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([29314, 2])
We keep 3.13e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([360507, 2])
We keep 3.78e+08/4.73e+10 =  0% of the original kernel matrix.

torch.Size([142839, 2])
We keep 5.04e+07/6.00e+09 =  0% of the original kernel matrix.

torch.Size([173774, 2])
We keep 3.66e+08/1.56e+10 =  2% of the original kernel matrix.

torch.Size([95386, 2])
We keep 2.94e+07/3.45e+09 =  0% of the original kernel matrix.

torch.Size([111090, 2])
We keep 5.74e+07/4.20e+09 =  1% of the original kernel matrix.

torch.Size([75000, 2])
We keep 1.75e+07/1.79e+09 =  0% of the original kernel matrix.

torch.Size([205372, 2])
We keep 4.79e+08/1.99e+10 =  2% of the original kernel matrix.

torch.Size([105856, 2])
We keep 3.33e+07/3.89e+09 =  0% of the original kernel matrix.

torch.Size([8451, 2])
We keep 3.79e+05/1.12e+07 =  3% of the original kernel matrix.

torch.Size([21858, 2])
We keep 1.79e+06/9.22e+07 =  1% of the original kernel matrix.

torch.Size([17163, 2])
We keep 4.12e+06/8.46e+07 =  4% of the original kernel matrix.

torch.Size([29880, 2])
We keep 3.69e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([901063, 2])
We keep 1.72e+09/2.44e+11 =  0% of the original kernel matrix.

torch.Size([227306, 2])
We keep 1.05e+08/1.36e+10 =  0% of the original kernel matrix.

torch.Size([1054695, 2])
We keep 2.23e+09/3.17e+11 =  0% of the original kernel matrix.

torch.Size([250321, 2])
We keep 1.17e+08/1.55e+10 =  0% of the original kernel matrix.

torch.Size([15385, 2])
We keep 1.64e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([28371, 2])
We keep 3.02e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([84326, 2])
We keep 2.68e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([66560, 2])
We keep 1.32e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([95113, 2])
We keep 1.03e+08/3.76e+09 =  2% of the original kernel matrix.

torch.Size([69931, 2])
We keep 1.70e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([53616, 2])
We keep 2.06e+07/9.63e+08 =  2% of the original kernel matrix.

torch.Size([53868, 2])
We keep 9.57e+06/8.56e+08 =  1% of the original kernel matrix.

torch.Size([96662, 2])
We keep 1.27e+08/4.72e+09 =  2% of the original kernel matrix.

torch.Size([69828, 2])
We keep 1.82e+07/1.89e+09 =  0% of the original kernel matrix.

torch.Size([62249, 2])
We keep 8.78e+07/2.62e+09 =  3% of the original kernel matrix.

torch.Size([54745, 2])
We keep 1.44e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([44847, 2])
We keep 1.75e+07/6.65e+08 =  2% of the original kernel matrix.

torch.Size([49389, 2])
We keep 8.05e+06/7.11e+08 =  1% of the original kernel matrix.

torch.Size([36225, 2])
We keep 8.49e+06/3.90e+08 =  2% of the original kernel matrix.

torch.Size([44279, 2])
We keep 6.69e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([15848, 2])
We keep 2.81e+06/5.88e+07 =  4% of the original kernel matrix.

torch.Size([28906, 2])
We keep 3.25e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([145588, 2])
We keep 6.22e+08/1.89e+10 =  3% of the original kernel matrix.

torch.Size([85200, 2])
We keep 3.27e+07/3.79e+09 =  0% of the original kernel matrix.

torch.Size([21926, 2])
We keep 5.01e+06/1.23e+08 =  4% of the original kernel matrix.

torch.Size([33838, 2])
We keep 4.31e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([13958, 2])
We keep 1.05e+06/3.72e+07 =  2% of the original kernel matrix.

torch.Size([27127, 2])
We keep 2.76e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([73203, 2])
We keep 3.50e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([62086, 2])
We keep 1.23e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([85309, 2])
We keep 8.07e+07/2.88e+09 =  2% of the original kernel matrix.

torch.Size([65953, 2])
We keep 1.48e+07/1.48e+09 =  0% of the original kernel matrix.

torch.Size([705209, 2])
We keep 1.14e+09/1.51e+11 =  0% of the original kernel matrix.

torch.Size([198875, 2])
We keep 8.39e+07/1.07e+10 =  0% of the original kernel matrix.

torch.Size([192624, 2])
We keep 1.03e+08/1.17e+10 =  0% of the original kernel matrix.

torch.Size([101252, 2])
We keep 2.71e+07/2.99e+09 =  0% of the original kernel matrix.

torch.Size([8864, 2])
We keep 4.15e+05/1.26e+07 =  3% of the original kernel matrix.

torch.Size([22161, 2])
We keep 1.85e+06/9.78e+07 =  1% of the original kernel matrix.

torch.Size([85321, 2])
We keep 3.78e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([66351, 2])
We keep 1.44e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([19557, 2])
We keep 1.93e+06/8.41e+07 =  2% of the original kernel matrix.

torch.Size([31859, 2])
We keep 3.72e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([51419, 2])
We keep 2.13e+07/1.01e+09 =  2% of the original kernel matrix.

torch.Size([52297, 2])
We keep 9.67e+06/8.75e+08 =  1% of the original kernel matrix.

torch.Size([144356, 2])
We keep 6.29e+07/6.61e+09 =  0% of the original kernel matrix.

torch.Size([86895, 2])
We keep 2.09e+07/2.24e+09 =  0% of the original kernel matrix.

torch.Size([83074, 2])
We keep 8.20e+07/3.01e+09 =  2% of the original kernel matrix.

torch.Size([65977, 2])
We keep 1.55e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([202547, 2])
We keep 1.35e+08/1.36e+10 =  0% of the original kernel matrix.

torch.Size([103989, 2])
We keep 2.89e+07/3.22e+09 =  0% of the original kernel matrix.

torch.Size([4588, 2])
We keep 1.26e+05/2.75e+06 =  4% of the original kernel matrix.

torch.Size([17087, 2])
We keep 1.11e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([21264, 2])
We keep 2.09e+06/9.73e+07 =  2% of the original kernel matrix.

torch.Size([33182, 2])
We keep 3.86e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([164443, 2])
We keep 1.36e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([92717, 2])
We keep 2.58e+07/2.82e+09 =  0% of the original kernel matrix.

torch.Size([133017, 2])
We keep 1.26e+08/7.61e+09 =  1% of the original kernel matrix.

torch.Size([83074, 2])
We keep 2.25e+07/2.41e+09 =  0% of the original kernel matrix.

torch.Size([125552, 2])
We keep 8.83e+07/5.53e+09 =  1% of the original kernel matrix.

torch.Size([80002, 2])
We keep 1.94e+07/2.05e+09 =  0% of the original kernel matrix.

torch.Size([12894, 2])
We keep 8.10e+05/2.98e+07 =  2% of the original kernel matrix.

torch.Size([25955, 2])
We keep 2.51e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([625336, 2])
We keep 7.30e+08/1.32e+11 =  0% of the original kernel matrix.

torch.Size([188301, 2])
We keep 7.79e+07/1.00e+10 =  0% of the original kernel matrix.

torch.Size([41893, 2])
We keep 6.83e+06/5.66e+08 =  1% of the original kernel matrix.

torch.Size([48270, 2])
We keep 7.67e+06/6.56e+08 =  1% of the original kernel matrix.

torch.Size([66180, 2])
We keep 1.87e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([59410, 2])
We keep 1.11e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([213906, 2])
We keep 1.40e+08/1.56e+10 =  0% of the original kernel matrix.

torch.Size([107141, 2])
We keep 3.07e+07/3.44e+09 =  0% of the original kernel matrix.

torch.Size([9490, 2])
We keep 5.03e+05/1.57e+07 =  3% of the original kernel matrix.

torch.Size([22779, 2])
We keep 2.00e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([159320, 2])
We keep 1.23e+08/8.04e+09 =  1% of the original kernel matrix.

torch.Size([91064, 2])
We keep 2.31e+07/2.47e+09 =  0% of the original kernel matrix.

torch.Size([333990, 2])
We keep 7.20e+08/4.30e+10 =  1% of the original kernel matrix.

torch.Size([136440, 2])
We keep 4.79e+07/5.72e+09 =  0% of the original kernel matrix.

torch.Size([1704142, 2])
We keep 8.81e+09/8.43e+11 =  1% of the original kernel matrix.

torch.Size([317064, 2])
We keep 1.85e+08/2.53e+10 =  0% of the original kernel matrix.

torch.Size([73667, 2])
We keep 2.26e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([62330, 2])
We keep 1.23e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([78721, 2])
We keep 2.35e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([64242, 2])
We keep 1.30e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([25837, 2])
We keep 6.05e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([36793, 2])
We keep 5.35e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([19879, 2])
We keep 1.72e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([32111, 2])
We keep 3.70e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([268955, 2])
We keep 5.40e+08/3.68e+10 =  1% of the original kernel matrix.

torch.Size([120163, 2])
We keep 4.46e+07/5.29e+09 =  0% of the original kernel matrix.

torch.Size([24834, 2])
We keep 2.70e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([36334, 2])
We keep 4.63e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([58677, 2])
We keep 2.40e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([55466, 2])
We keep 1.06e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([108057, 2])
We keep 1.38e+08/4.65e+09 =  2% of the original kernel matrix.

torch.Size([75119, 2])
We keep 1.62e+07/1.88e+09 =  0% of the original kernel matrix.

torch.Size([83934, 2])
We keep 8.86e+07/2.45e+09 =  3% of the original kernel matrix.

torch.Size([66220, 2])
We keep 1.33e+07/1.37e+09 =  0% of the original kernel matrix.

torch.Size([26958, 2])
We keep 1.24e+07/2.77e+08 =  4% of the original kernel matrix.

torch.Size([37555, 2])
We keep 5.89e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([21516, 2])
We keep 2.21e+06/9.96e+07 =  2% of the original kernel matrix.

torch.Size([33477, 2])
We keep 3.91e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([241350, 2])
We keep 9.51e+08/6.06e+10 =  1% of the original kernel matrix.

torch.Size([109809, 2])
We keep 5.63e+07/6.79e+09 =  0% of the original kernel matrix.

torch.Size([35828, 2])
We keep 2.02e+07/4.30e+08 =  4% of the original kernel matrix.

torch.Size([43840, 2])
We keep 6.56e+06/5.72e+08 =  1% of the original kernel matrix.

torch.Size([25835, 2])
We keep 3.03e+07/2.89e+08 = 10% of the original kernel matrix.

torch.Size([36632, 2])
We keep 5.87e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([23282, 2])
We keep 2.46e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([35124, 2])
We keep 4.35e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([232169, 2])
We keep 4.14e+08/2.70e+10 =  1% of the original kernel matrix.

torch.Size([112085, 2])
We keep 3.88e+07/4.53e+09 =  0% of the original kernel matrix.

torch.Size([40248, 2])
We keep 7.26e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([47167, 2])
We keep 7.39e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([164662, 2])
We keep 4.08e+08/1.44e+10 =  2% of the original kernel matrix.

torch.Size([92247, 2])
We keep 2.93e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([29991, 2])
We keep 1.40e+07/3.54e+08 =  3% of the original kernel matrix.

torch.Size([39603, 2])
We keep 6.32e+06/5.19e+08 =  1% of the original kernel matrix.

torch.Size([44251, 2])
We keep 3.07e+07/1.01e+09 =  3% of the original kernel matrix.

torch.Size([47322, 2])
We keep 9.88e+06/8.76e+08 =  1% of the original kernel matrix.

torch.Size([38330, 2])
We keep 1.20e+07/4.73e+08 =  2% of the original kernel matrix.

torch.Size([45537, 2])
We keep 7.07e+06/6.00e+08 =  1% of the original kernel matrix.

torch.Size([523204, 2])
We keep 2.03e+09/1.42e+11 =  1% of the original kernel matrix.

torch.Size([165708, 2])
We keep 8.30e+07/1.04e+10 =  0% of the original kernel matrix.

torch.Size([95867, 2])
We keep 4.30e+07/4.45e+09 =  0% of the original kernel matrix.

torch.Size([70818, 2])
We keep 1.79e+07/1.84e+09 =  0% of the original kernel matrix.

torch.Size([32588, 2])
We keep 4.52e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([41885, 2])
We keep 5.86e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([340816, 2])
We keep 2.40e+08/3.74e+10 =  0% of the original kernel matrix.

torch.Size([138240, 2])
We keep 4.49e+07/5.33e+09 =  0% of the original kernel matrix.

torch.Size([399080, 2])
We keep 3.14e+08/5.21e+10 =  0% of the original kernel matrix.

torch.Size([151122, 2])
We keep 5.17e+07/6.29e+09 =  0% of the original kernel matrix.

torch.Size([14572, 2])
We keep 1.50e+06/4.19e+07 =  3% of the original kernel matrix.

torch.Size([27852, 2])
We keep 2.76e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([235242, 2])
We keep 3.49e+08/1.89e+10 =  1% of the original kernel matrix.

torch.Size([112778, 2])
We keep 3.26e+07/3.79e+09 =  0% of the original kernel matrix.

torch.Size([344244, 2])
We keep 2.70e+08/4.10e+10 =  0% of the original kernel matrix.

torch.Size([138204, 2])
We keep 4.65e+07/5.59e+09 =  0% of the original kernel matrix.

torch.Size([1089047, 2])
We keep 2.35e+09/3.36e+11 =  0% of the original kernel matrix.

torch.Size([258053, 2])
We keep 1.18e+08/1.60e+10 =  0% of the original kernel matrix.

torch.Size([38950, 2])
We keep 1.47e+07/4.94e+08 =  2% of the original kernel matrix.

torch.Size([45987, 2])
We keep 7.28e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([11928, 2])
We keep 1.45e+06/3.21e+07 =  4% of the original kernel matrix.

torch.Size([25217, 2])
We keep 2.57e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([36943, 2])
We keep 5.54e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([44958, 2])
We keep 6.74e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([35166, 2])
We keep 9.81e+06/4.24e+08 =  2% of the original kernel matrix.

torch.Size([43431, 2])
We keep 6.82e+06/5.68e+08 =  1% of the original kernel matrix.

torch.Size([74768, 2])
We keep 2.42e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([62886, 2])
We keep 1.24e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([73940, 2])
We keep 4.68e+07/1.84e+09 =  2% of the original kernel matrix.

torch.Size([62510, 2])
We keep 1.25e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([8669, 2])
We keep 4.04e+05/1.23e+07 =  3% of the original kernel matrix.

torch.Size([21825, 2])
We keep 1.82e+06/9.66e+07 =  1% of the original kernel matrix.

torch.Size([134384, 2])
We keep 8.23e+07/5.48e+09 =  1% of the original kernel matrix.

torch.Size([83258, 2])
We keep 1.98e+07/2.04e+09 =  0% of the original kernel matrix.

torch.Size([395010, 2])
We keep 3.36e+08/5.18e+10 =  0% of the original kernel matrix.

torch.Size([149523, 2])
We keep 5.17e+07/6.28e+09 =  0% of the original kernel matrix.

torch.Size([110800, 2])
We keep 6.33e+08/2.12e+10 =  2% of the original kernel matrix.

torch.Size([68205, 2])
We keep 3.47e+07/4.01e+09 =  0% of the original kernel matrix.

torch.Size([21571, 2])
We keep 3.67e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([33597, 2])
We keep 4.15e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([160656, 2])
We keep 1.14e+08/8.46e+09 =  1% of the original kernel matrix.

torch.Size([91683, 2])
We keep 2.34e+07/2.54e+09 =  0% of the original kernel matrix.

torch.Size([34618, 2])
We keep 6.08e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([43527, 2])
We keep 6.53e+06/5.29e+08 =  1% of the original kernel matrix.

torch.Size([44816, 2])
We keep 1.50e+07/7.29e+08 =  2% of the original kernel matrix.

torch.Size([47635, 2])
We keep 8.31e+06/7.45e+08 =  1% of the original kernel matrix.

torch.Size([34167, 2])
We keep 5.07e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([43040, 2])
We keep 6.36e+06/5.10e+08 =  1% of the original kernel matrix.

torch.Size([16785, 2])
We keep 1.23e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([29554, 2])
We keep 3.03e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([307801, 2])
We keep 7.38e+08/4.47e+10 =  1% of the original kernel matrix.

torch.Size([129649, 2])
We keep 4.85e+07/5.83e+09 =  0% of the original kernel matrix.

torch.Size([107372, 2])
We keep 3.33e+07/3.40e+09 =  0% of the original kernel matrix.

torch.Size([74599, 2])
We keep 1.59e+07/1.61e+09 =  0% of the original kernel matrix.

torch.Size([43496, 2])
We keep 1.21e+07/5.97e+08 =  2% of the original kernel matrix.

torch.Size([49088, 2])
We keep 7.80e+06/6.74e+08 =  1% of the original kernel matrix.

torch.Size([100677, 2])
We keep 6.99e+07/4.25e+09 =  1% of the original kernel matrix.

torch.Size([71837, 2])
We keep 1.77e+07/1.80e+09 =  0% of the original kernel matrix.

torch.Size([20869, 2])
We keep 2.99e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([32965, 2])
We keep 4.06e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([1374731, 2])
We keep 5.22e+09/5.39e+11 =  0% of the original kernel matrix.

torch.Size([286431, 2])
We keep 1.47e+08/2.02e+10 =  0% of the original kernel matrix.

torch.Size([31976, 2])
We keep 4.40e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([41445, 2])
We keep 5.91e+06/4.66e+08 =  1% of the original kernel matrix.

torch.Size([288373, 2])
We keep 2.26e+08/2.90e+10 =  0% of the original kernel matrix.

torch.Size([126129, 2])
We keep 4.01e+07/4.69e+09 =  0% of the original kernel matrix.

torch.Size([9046, 2])
We keep 4.16e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([22304, 2])
We keep 1.87e+06/9.98e+07 =  1% of the original kernel matrix.

torch.Size([14420, 2])
We keep 1.03e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([27288, 2])
We keep 2.80e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([13113, 2])
We keep 8.33e+05/3.09e+07 =  2% of the original kernel matrix.

torch.Size([26196, 2])
We keep 2.56e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([36196, 2])
We keep 7.73e+06/3.80e+08 =  2% of the original kernel matrix.

torch.Size([44477, 2])
We keep 6.44e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([20549, 2])
We keep 3.30e+06/9.91e+07 =  3% of the original kernel matrix.

torch.Size([32795, 2])
We keep 3.98e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([82538, 2])
We keep 9.14e+07/2.36e+09 =  3% of the original kernel matrix.

torch.Size([65332, 2])
We keep 1.38e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([15886, 2])
We keep 1.19e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([28669, 2])
We keep 3.02e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([15952, 2])
We keep 1.22e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([28513, 2])
We keep 3.05e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([633517, 2])
We keep 8.44e+08/1.17e+11 =  0% of the original kernel matrix.

torch.Size([188391, 2])
We keep 7.44e+07/9.42e+09 =  0% of the original kernel matrix.

torch.Size([105733, 2])
We keep 6.17e+07/3.74e+09 =  1% of the original kernel matrix.

torch.Size([74078, 2])
We keep 1.67e+07/1.69e+09 =  0% of the original kernel matrix.

torch.Size([74671, 2])
We keep 2.69e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([62623, 2])
We keep 1.27e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([265505, 2])
We keep 1.94e+08/2.31e+10 =  0% of the original kernel matrix.

torch.Size([120922, 2])
We keep 3.65e+07/4.19e+09 =  0% of the original kernel matrix.

torch.Size([819844, 2])
We keep 1.14e+09/1.88e+11 =  0% of the original kernel matrix.

torch.Size([215912, 2])
We keep 9.22e+07/1.20e+10 =  0% of the original kernel matrix.

torch.Size([15158, 2])
We keep 1.34e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([28058, 2])
We keep 2.93e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([22830, 2])
We keep 2.27e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([34467, 2])
We keep 4.23e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([39397, 2])
We keep 1.97e+07/6.25e+08 =  3% of the original kernel matrix.

torch.Size([45894, 2])
We keep 7.81e+06/6.89e+08 =  1% of the original kernel matrix.

torch.Size([8841, 2])
We keep 5.53e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([22072, 2])
We keep 1.95e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([21855, 2])
We keep 2.16e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([33723, 2])
We keep 4.10e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([40301, 2])
We keep 1.03e+07/5.63e+08 =  1% of the original kernel matrix.

torch.Size([46689, 2])
We keep 7.70e+06/6.54e+08 =  1% of the original kernel matrix.

torch.Size([19936, 2])
We keep 2.73e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([32141, 2])
We keep 4.09e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([21347, 2])
We keep 1.93e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([33314, 2])
We keep 3.93e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([29312, 2])
We keep 5.33e+06/2.42e+08 =  2% of the original kernel matrix.

torch.Size([39583, 2])
We keep 5.47e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([33481, 2])
We keep 5.51e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([42503, 2])
We keep 6.24e+06/5.01e+08 =  1% of the original kernel matrix.

torch.Size([1878315, 2])
We keep 5.63e+09/9.34e+11 =  0% of the original kernel matrix.

torch.Size([334141, 2])
We keep 1.91e+08/2.67e+10 =  0% of the original kernel matrix.

torch.Size([22554, 2])
We keep 5.79e+06/1.80e+08 =  3% of the original kernel matrix.

torch.Size([34440, 2])
We keep 4.90e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([162847, 2])
We keep 1.27e+08/9.00e+09 =  1% of the original kernel matrix.

torch.Size([92340, 2])
We keep 2.46e+07/2.62e+09 =  0% of the original kernel matrix.

torch.Size([34759, 2])
We keep 1.36e+07/4.63e+08 =  2% of the original kernel matrix.

torch.Size([42594, 2])
We keep 7.11e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([11393, 2])
We keep 8.70e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([24612, 2])
We keep 2.35e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([166240, 2])
We keep 1.16e+08/9.72e+09 =  1% of the original kernel matrix.

torch.Size([93160, 2])
We keep 2.50e+07/2.72e+09 =  0% of the original kernel matrix.

torch.Size([567245, 2])
We keep 1.81e+09/1.14e+11 =  1% of the original kernel matrix.

torch.Size([177864, 2])
We keep 6.88e+07/9.31e+09 =  0% of the original kernel matrix.

torch.Size([1353064, 2])
We keep 2.51e+09/4.90e+11 =  0% of the original kernel matrix.

torch.Size([284278, 2])
We keep 1.41e+08/1.93e+10 =  0% of the original kernel matrix.

torch.Size([111913, 2])
We keep 1.28e+08/4.13e+09 =  3% of the original kernel matrix.

torch.Size([76307, 2])
We keep 1.68e+07/1.77e+09 =  0% of the original kernel matrix.

torch.Size([93415, 2])
We keep 4.57e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([69788, 2])
We keep 1.45e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([10206, 2])
We keep 4.98e+05/1.66e+07 =  3% of the original kernel matrix.

torch.Size([23554, 2])
We keep 2.03e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([107004, 2])
We keep 1.95e+08/6.86e+09 =  2% of the original kernel matrix.

torch.Size([73235, 2])
We keep 2.17e+07/2.28e+09 =  0% of the original kernel matrix.

torch.Size([2408051, 2])
We keep 2.05e+10/2.44e+12 =  0% of the original kernel matrix.

torch.Size([368837, 2])
We keep 3.03e+08/4.30e+10 =  0% of the original kernel matrix.

torch.Size([128113, 2])
We keep 1.84e+08/6.48e+09 =  2% of the original kernel matrix.

torch.Size([82087, 2])
We keep 2.08e+07/2.22e+09 =  0% of the original kernel matrix.

torch.Size([30062, 2])
We keep 4.65e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([40056, 2])
We keep 5.56e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([149205, 2])
We keep 1.46e+08/9.84e+09 =  1% of the original kernel matrix.

torch.Size([88314, 2])
We keep 2.50e+07/2.74e+09 =  0% of the original kernel matrix.

torch.Size([991670, 2])
We keep 2.52e+09/3.23e+11 =  0% of the original kernel matrix.

torch.Size([240623, 2])
We keep 1.18e+08/1.57e+10 =  0% of the original kernel matrix.

torch.Size([165862, 2])
We keep 9.27e+07/9.63e+09 =  0% of the original kernel matrix.

torch.Size([92739, 2])
We keep 2.49e+07/2.71e+09 =  0% of the original kernel matrix.

torch.Size([29077, 2])
We keep 4.56e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([39148, 2])
We keep 5.51e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([24440, 2])
We keep 4.53e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([36185, 2])
We keep 4.63e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([30255, 2])
We keep 3.84e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([40287, 2])
We keep 5.63e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([19562, 2])
We keep 2.85e+06/8.38e+07 =  3% of the original kernel matrix.

torch.Size([32056, 2])
We keep 3.67e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([617738, 2])
We keep 1.22e+09/1.18e+11 =  1% of the original kernel matrix.

torch.Size([186117, 2])
We keep 7.48e+07/9.48e+09 =  0% of the original kernel matrix.

torch.Size([43507, 2])
We keep 1.01e+07/6.12e+08 =  1% of the original kernel matrix.

torch.Size([48600, 2])
We keep 8.06e+06/6.82e+08 =  1% of the original kernel matrix.

torch.Size([8135, 2])
We keep 3.67e+05/1.00e+07 =  3% of the original kernel matrix.

torch.Size([21461, 2])
We keep 1.71e+06/8.74e+07 =  1% of the original kernel matrix.

torch.Size([126657, 2])
We keep 1.02e+08/4.78e+09 =  2% of the original kernel matrix.

torch.Size([80380, 2])
We keep 1.75e+07/1.91e+09 =  0% of the original kernel matrix.

torch.Size([48281, 2])
We keep 1.02e+07/7.44e+08 =  1% of the original kernel matrix.

torch.Size([48688, 2])
We keep 8.10e+06/7.52e+08 =  1% of the original kernel matrix.

torch.Size([8231, 2])
We keep 4.09e+05/1.08e+07 =  3% of the original kernel matrix.

torch.Size([21394, 2])
We keep 1.78e+06/9.07e+07 =  1% of the original kernel matrix.

torch.Size([200638, 2])
We keep 1.85e+08/1.51e+10 =  1% of the original kernel matrix.

torch.Size([103442, 2])
We keep 3.06e+07/3.39e+09 =  0% of the original kernel matrix.

torch.Size([196913, 2])
We keep 1.40e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([102642, 2])
We keep 2.78e+07/3.09e+09 =  0% of the original kernel matrix.

torch.Size([241250, 2])
We keep 5.49e+08/2.38e+10 =  2% of the original kernel matrix.

torch.Size([114370, 2])
We keep 3.66e+07/4.26e+09 =  0% of the original kernel matrix.

torch.Size([356866, 2])
We keep 5.05e+08/4.54e+10 =  1% of the original kernel matrix.

torch.Size([141662, 2])
We keep 4.88e+07/5.88e+09 =  0% of the original kernel matrix.

torch.Size([418033, 2])
We keep 5.47e+08/6.19e+10 =  0% of the original kernel matrix.

torch.Size([152551, 2])
We keep 5.57e+07/6.86e+09 =  0% of the original kernel matrix.

torch.Size([58653, 2])
We keep 2.12e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([55788, 2])
We keep 1.05e+07/9.60e+08 =  1% of the original kernel matrix.

torch.Size([116813, 2])
We keep 6.24e+07/4.48e+09 =  1% of the original kernel matrix.

torch.Size([77283, 2])
We keep 1.80e+07/1.85e+09 =  0% of the original kernel matrix.

torch.Size([267394, 2])
We keep 1.76e+08/2.22e+10 =  0% of the original kernel matrix.

torch.Size([120557, 2])
We keep 3.53e+07/4.11e+09 =  0% of the original kernel matrix.

torch.Size([684630, 2])
We keep 9.57e+08/1.31e+11 =  0% of the original kernel matrix.

torch.Size([195159, 2])
We keep 7.81e+07/1.00e+10 =  0% of the original kernel matrix.

torch.Size([26120, 2])
We keep 3.53e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([37265, 2])
We keep 4.85e+06/3.69e+08 =  1% of the original kernel matrix.

torch.Size([158435, 2])
We keep 2.93e+08/1.17e+10 =  2% of the original kernel matrix.

torch.Size([92036, 2])
We keep 2.53e+07/2.99e+09 =  0% of the original kernel matrix.

torch.Size([241110, 2])
We keep 4.48e+08/2.41e+10 =  1% of the original kernel matrix.

torch.Size([114287, 2])
We keep 3.43e+07/4.28e+09 =  0% of the original kernel matrix.

torch.Size([207841, 2])
We keep 2.90e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([105676, 2])
We keep 3.07e+07/3.48e+09 =  0% of the original kernel matrix.

torch.Size([128293, 2])
We keep 1.49e+08/6.97e+09 =  2% of the original kernel matrix.

torch.Size([80811, 2])
We keep 2.12e+07/2.30e+09 =  0% of the original kernel matrix.

torch.Size([54972, 2])
We keep 1.19e+08/1.74e+09 =  6% of the original kernel matrix.

torch.Size([52443, 2])
We keep 1.18e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([35515, 2])
We keep 6.38e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([43595, 2])
We keep 6.79e+06/5.49e+08 =  1% of the original kernel matrix.

torch.Size([23158, 2])
We keep 2.38e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([34876, 2])
We keep 4.32e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([125503, 2])
We keep 1.09e+08/6.26e+09 =  1% of the original kernel matrix.

torch.Size([80396, 2])
We keep 2.06e+07/2.18e+09 =  0% of the original kernel matrix.

torch.Size([367593, 2])
We keep 3.46e+08/4.55e+10 =  0% of the original kernel matrix.

torch.Size([144792, 2])
We keep 4.87e+07/5.89e+09 =  0% of the original kernel matrix.

torch.Size([298776, 2])
We keep 1.95e+08/2.91e+10 =  0% of the original kernel matrix.

torch.Size([128689, 2])
We keep 4.00e+07/4.70e+09 =  0% of the original kernel matrix.

torch.Size([407313, 2])
We keep 7.82e+08/6.84e+10 =  1% of the original kernel matrix.

torch.Size([150673, 2])
We keep 5.87e+07/7.21e+09 =  0% of the original kernel matrix.

torch.Size([279994, 2])
We keep 3.12e+08/2.93e+10 =  1% of the original kernel matrix.

torch.Size([124123, 2])
We keep 4.04e+07/4.72e+09 =  0% of the original kernel matrix.

torch.Size([191599, 2])
We keep 1.63e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([101194, 2])
We keep 2.78e+07/3.15e+09 =  0% of the original kernel matrix.

torch.Size([39246, 2])
We keep 7.64e+06/5.58e+08 =  1% of the original kernel matrix.

torch.Size([45526, 2])
We keep 7.61e+06/6.52e+08 =  1% of the original kernel matrix.

torch.Size([692681, 2])
We keep 1.11e+09/1.35e+11 =  0% of the original kernel matrix.

torch.Size([196283, 2])
We keep 8.01e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([12414, 2])
We keep 1.43e+06/3.22e+07 =  4% of the original kernel matrix.

torch.Size([25417, 2])
We keep 2.59e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([35196, 2])
We keep 6.59e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([43582, 2])
We keep 6.73e+06/5.48e+08 =  1% of the original kernel matrix.

torch.Size([561941, 2])
We keep 6.26e+08/9.46e+10 =  0% of the original kernel matrix.

torch.Size([177203, 2])
We keep 6.81e+07/8.48e+09 =  0% of the original kernel matrix.

torch.Size([88958, 2])
We keep 3.13e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([68084, 2])
We keep 1.39e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([16115, 2])
We keep 1.61e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([28704, 2])
We keep 3.17e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([56141, 2])
We keep 1.74e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([54415, 2])
We keep 9.89e+06/9.14e+08 =  1% of the original kernel matrix.

torch.Size([484526, 2])
We keep 1.61e+09/1.05e+11 =  1% of the original kernel matrix.

torch.Size([160929, 2])
We keep 6.98e+07/8.93e+09 =  0% of the original kernel matrix.

torch.Size([72703, 2])
We keep 2.66e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([61954, 2])
We keep 1.23e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([200046, 2])
We keep 1.42e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([103443, 2])
We keep 2.83e+07/3.15e+09 =  0% of the original kernel matrix.

torch.Size([14623, 2])
We keep 2.24e+06/5.02e+07 =  4% of the original kernel matrix.

torch.Size([27526, 2])
We keep 3.00e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([25758, 2])
We keep 7.08e+06/1.88e+08 =  3% of the original kernel matrix.

torch.Size([36916, 2])
We keep 4.96e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([18980, 2])
We keep 1.83e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([31340, 2])
We keep 3.64e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([68309, 2])
We keep 7.98e+07/2.43e+09 =  3% of the original kernel matrix.

torch.Size([58937, 2])
We keep 1.41e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([85443, 2])
We keep 3.53e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([67208, 2])
We keep 1.35e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([160094, 2])
We keep 1.32e+08/8.72e+09 =  1% of the original kernel matrix.

torch.Size([91281, 2])
We keep 2.42e+07/2.58e+09 =  0% of the original kernel matrix.

torch.Size([13842, 2])
We keep 1.03e+06/3.55e+07 =  2% of the original kernel matrix.

torch.Size([26881, 2])
We keep 2.63e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([284604, 2])
We keep 2.90e+08/2.77e+10 =  1% of the original kernel matrix.

torch.Size([125363, 2])
We keep 3.96e+07/4.59e+09 =  0% of the original kernel matrix.

torch.Size([22879, 2])
We keep 4.29e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([34638, 2])
We keep 4.80e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([126602, 2])
We keep 7.09e+07/4.97e+09 =  1% of the original kernel matrix.

torch.Size([80754, 2])
We keep 1.89e+07/1.94e+09 =  0% of the original kernel matrix.

torch.Size([26993, 2])
We keep 1.03e+07/2.35e+08 =  4% of the original kernel matrix.

torch.Size([37761, 2])
We keep 5.52e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([227461, 2])
We keep 1.45e+08/1.64e+10 =  0% of the original kernel matrix.

torch.Size([110774, 2])
We keep 3.08e+07/3.53e+09 =  0% of the original kernel matrix.

torch.Size([13034, 2])
We keep 1.84e+06/3.64e+07 =  5% of the original kernel matrix.

torch.Size([26351, 2])
We keep 2.75e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([74554, 2])
We keep 3.39e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([62728, 2])
We keep 1.24e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([72585, 2])
We keep 2.10e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([62299, 2])
We keep 1.19e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([54040, 2])
We keep 2.25e+07/1.01e+09 =  2% of the original kernel matrix.

torch.Size([53713, 2])
We keep 9.91e+06/8.78e+08 =  1% of the original kernel matrix.

torch.Size([129847, 2])
We keep 7.51e+07/5.68e+09 =  1% of the original kernel matrix.

torch.Size([82426, 2])
We keep 1.99e+07/2.08e+09 =  0% of the original kernel matrix.

torch.Size([967606, 2])
We keep 2.79e+09/3.06e+11 =  0% of the original kernel matrix.

torch.Size([237187, 2])
We keep 1.16e+08/1.52e+10 =  0% of the original kernel matrix.

torch.Size([23553, 2])
We keep 2.39e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([35247, 2])
We keep 4.36e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([25861, 2])
We keep 4.88e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([36896, 2])
We keep 5.16e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([207045, 2])
We keep 1.99e+08/1.50e+10 =  1% of the original kernel matrix.

torch.Size([105395, 2])
We keep 3.02e+07/3.38e+09 =  0% of the original kernel matrix.

torch.Size([52962, 2])
We keep 1.21e+07/9.13e+08 =  1% of the original kernel matrix.

torch.Size([53731, 2])
We keep 9.43e+06/8.33e+08 =  1% of the original kernel matrix.

torch.Size([1356414, 2])
We keep 2.39e+09/4.91e+11 =  0% of the original kernel matrix.

torch.Size([284688, 2])
We keep 1.41e+08/1.93e+10 =  0% of the original kernel matrix.

torch.Size([13490, 2])
We keep 1.08e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([26570, 2])
We keep 2.66e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([290509, 2])
We keep 2.21e+08/2.93e+10 =  0% of the original kernel matrix.

torch.Size([126324, 2])
We keep 4.03e+07/4.72e+09 =  0% of the original kernel matrix.

torch.Size([821100, 2])
We keep 9.53e+08/1.79e+11 =  0% of the original kernel matrix.

torch.Size([215284, 2])
We keep 8.94e+07/1.17e+10 =  0% of the original kernel matrix.

torch.Size([16389, 2])
We keep 1.78e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([29040, 2])
We keep 3.31e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([365559, 2])
We keep 6.68e+08/5.83e+10 =  1% of the original kernel matrix.

torch.Size([141392, 2])
We keep 5.51e+07/6.66e+09 =  0% of the original kernel matrix.

torch.Size([33287, 2])
We keep 1.09e+07/5.33e+08 =  2% of the original kernel matrix.

torch.Size([41508, 2])
We keep 7.63e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([841008, 2])
We keep 2.29e+09/2.01e+11 =  1% of the original kernel matrix.

torch.Size([218305, 2])
We keep 9.49e+07/1.24e+10 =  0% of the original kernel matrix.

torch.Size([41358, 2])
We keep 6.81e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([47725, 2])
We keep 7.50e+06/6.31e+08 =  1% of the original kernel matrix.

torch.Size([83962, 2])
We keep 4.63e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([66078, 2])
We keep 1.39e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([11510, 2])
We keep 2.87e+06/3.92e+07 =  7% of the original kernel matrix.

torch.Size([24467, 2])
We keep 2.74e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([16603, 2])
We keep 1.45e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([29390, 2])
We keep 3.14e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([17802, 2])
We keep 2.39e+06/7.64e+07 =  3% of the original kernel matrix.

torch.Size([30420, 2])
We keep 3.54e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([98217, 2])
We keep 8.50e+07/3.26e+09 =  2% of the original kernel matrix.

torch.Size([71741, 2])
We keep 1.48e+07/1.58e+09 =  0% of the original kernel matrix.

torch.Size([50471, 2])
We keep 2.94e+07/9.78e+08 =  3% of the original kernel matrix.

torch.Size([51754, 2])
We keep 9.69e+06/8.63e+08 =  1% of the original kernel matrix.

torch.Size([313799, 2])
We keep 3.13e+08/3.28e+10 =  0% of the original kernel matrix.

torch.Size([132460, 2])
We keep 4.21e+07/4.99e+09 =  0% of the original kernel matrix.

torch.Size([47757, 2])
We keep 9.28e+06/7.33e+08 =  1% of the original kernel matrix.

torch.Size([49066, 2])
We keep 8.19e+06/7.46e+08 =  1% of the original kernel matrix.

torch.Size([14813, 2])
We keep 1.15e+06/4.10e+07 =  2% of the original kernel matrix.

torch.Size([27784, 2])
We keep 2.82e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([46617, 2])
We keep 1.27e+07/7.81e+08 =  1% of the original kernel matrix.

torch.Size([48421, 2])
We keep 8.42e+06/7.71e+08 =  1% of the original kernel matrix.

torch.Size([174738, 2])
We keep 4.73e+08/1.74e+10 =  2% of the original kernel matrix.

torch.Size([95898, 2])
We keep 3.19e+07/3.63e+09 =  0% of the original kernel matrix.

torch.Size([55385, 2])
We keep 1.26e+08/2.50e+09 =  5% of the original kernel matrix.

torch.Size([52431, 2])
We keep 1.41e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([523941, 2])
We keep 6.36e+08/8.32e+10 =  0% of the original kernel matrix.

torch.Size([172074, 2])
We keep 6.32e+07/7.95e+09 =  0% of the original kernel matrix.

torch.Size([120556, 2])
We keep 1.33e+08/5.36e+09 =  2% of the original kernel matrix.

torch.Size([79105, 2])
We keep 1.90e+07/2.02e+09 =  0% of the original kernel matrix.

torch.Size([17607, 2])
We keep 2.46e+06/8.45e+07 =  2% of the original kernel matrix.

torch.Size([30525, 2])
We keep 3.70e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([11909, 2])
We keep 7.76e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([24947, 2])
We keep 2.37e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([16803, 2])
We keep 2.17e+06/7.20e+07 =  3% of the original kernel matrix.

torch.Size([29756, 2])
We keep 3.53e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([1123216, 2])
We keep 4.06e+09/3.96e+11 =  1% of the original kernel matrix.

torch.Size([262154, 2])
We keep 1.28e+08/1.73e+10 =  0% of the original kernel matrix.

torch.Size([23200, 2])
We keep 2.58e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([35002, 2])
We keep 4.33e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([56874, 2])
We keep 1.65e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([55552, 2])
We keep 9.97e+06/9.08e+08 =  1% of the original kernel matrix.

torch.Size([146886, 2])
We keep 9.59e+07/7.86e+09 =  1% of the original kernel matrix.

torch.Size([87646, 2])
We keep 2.30e+07/2.45e+09 =  0% of the original kernel matrix.

torch.Size([41273, 2])
We keep 2.70e+07/8.35e+08 =  3% of the original kernel matrix.

torch.Size([46628, 2])
We keep 9.16e+06/7.97e+08 =  1% of the original kernel matrix.

torch.Size([17135, 2])
We keep 2.23e+06/6.47e+07 =  3% of the original kernel matrix.

torch.Size([29940, 2])
We keep 3.33e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([18833, 2])
We keep 1.95e+06/8.04e+07 =  2% of the original kernel matrix.

torch.Size([31142, 2])
We keep 3.68e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([326097, 2])
We keep 7.14e+08/4.55e+10 =  1% of the original kernel matrix.

torch.Size([133596, 2])
We keep 4.89e+07/5.88e+09 =  0% of the original kernel matrix.

torch.Size([100379, 2])
We keep 6.44e+07/3.22e+09 =  2% of the original kernel matrix.

torch.Size([72166, 2])
We keep 1.56e+07/1.56e+09 =  0% of the original kernel matrix.

torch.Size([74820, 2])
We keep 6.48e+07/2.85e+09 =  2% of the original kernel matrix.

torch.Size([61737, 2])
We keep 1.50e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([36900, 2])
We keep 6.84e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([44540, 2])
We keep 6.69e+06/5.62e+08 =  1% of the original kernel matrix.

torch.Size([124672, 2])
We keep 5.11e+08/8.32e+09 =  6% of the original kernel matrix.

torch.Size([80310, 2])
We keep 2.37e+07/2.52e+09 =  0% of the original kernel matrix.

torch.Size([168262, 2])
We keep 1.06e+08/9.19e+09 =  1% of the original kernel matrix.

torch.Size([94154, 2])
We keep 2.45e+07/2.64e+09 =  0% of the original kernel matrix.

torch.Size([264934, 2])
We keep 1.08e+09/5.21e+10 =  2% of the original kernel matrix.

torch.Size([118447, 2])
We keep 5.16e+07/6.29e+09 =  0% of the original kernel matrix.

torch.Size([247397, 2])
We keep 1.79e+08/2.00e+10 =  0% of the original kernel matrix.

torch.Size([116107, 2])
We keep 3.38e+07/3.90e+09 =  0% of the original kernel matrix.

torch.Size([17273, 2])
We keep 2.70e+06/6.68e+07 =  4% of the original kernel matrix.

torch.Size([30140, 2])
We keep 3.24e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([175883, 2])
We keep 1.34e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([96931, 2])
We keep 2.69e+07/2.95e+09 =  0% of the original kernel matrix.

torch.Size([37647, 2])
We keep 3.20e+07/6.04e+08 =  5% of the original kernel matrix.

torch.Size([44845, 2])
We keep 7.72e+06/6.78e+08 =  1% of the original kernel matrix.

torch.Size([64006, 2])
We keep 3.45e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([58448, 2])
We keep 1.13e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([36516, 2])
We keep 1.20e+07/5.69e+08 =  2% of the original kernel matrix.

torch.Size([43972, 2])
We keep 7.67e+06/6.58e+08 =  1% of the original kernel matrix.

torch.Size([18250, 2])
We keep 1.53e+06/6.76e+07 =  2% of the original kernel matrix.

torch.Size([30911, 2])
We keep 3.39e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([55767, 2])
We keep 3.12e+07/1.09e+09 =  2% of the original kernel matrix.

torch.Size([54508, 2])
We keep 1.01e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([62996, 2])
We keep 2.22e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([57843, 2])
We keep 1.13e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([17392, 2])
We keep 2.16e+06/7.09e+07 =  3% of the original kernel matrix.

torch.Size([29933, 2])
We keep 3.53e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([53109, 2])
We keep 1.35e+07/9.48e+08 =  1% of the original kernel matrix.

torch.Size([53775, 2])
We keep 9.60e+06/8.49e+08 =  1% of the original kernel matrix.

torch.Size([17632, 2])
We keep 4.81e+06/9.58e+07 =  5% of the original kernel matrix.

torch.Size([30294, 2])
We keep 3.71e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([173040, 2])
We keep 5.62e+08/1.82e+10 =  3% of the original kernel matrix.

torch.Size([94752, 2])
We keep 3.27e+07/3.72e+09 =  0% of the original kernel matrix.

torch.Size([12981, 2])
We keep 1.62e+06/3.58e+07 =  4% of the original kernel matrix.

torch.Size([26245, 2])
We keep 2.69e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([26991, 2])
We keep 1.31e+07/3.19e+08 =  4% of the original kernel matrix.

torch.Size([37178, 2])
We keep 6.07e+06/4.93e+08 =  1% of the original kernel matrix.

torch.Size([27251, 2])
We keep 1.73e+07/4.00e+08 =  4% of the original kernel matrix.

torch.Size([37378, 2])
We keep 6.52e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([54552, 2])
We keep 2.21e+07/1.10e+09 =  2% of the original kernel matrix.

torch.Size([53716, 2])
We keep 9.92e+06/9.13e+08 =  1% of the original kernel matrix.

torch.Size([15135, 2])
We keep 1.34e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([27970, 2])
We keep 2.96e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([35796, 2])
We keep 5.39e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([44200, 2])
We keep 6.65e+06/5.39e+08 =  1% of the original kernel matrix.

torch.Size([64281, 2])
We keep 1.60e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([58887, 2])
We keep 1.08e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([271938, 2])
We keep 3.06e+08/2.71e+10 =  1% of the original kernel matrix.

torch.Size([121965, 2])
We keep 3.90e+07/4.54e+09 =  0% of the original kernel matrix.

torch.Size([19948, 2])
We keep 1.91e+06/8.39e+07 =  2% of the original kernel matrix.

torch.Size([32352, 2])
We keep 3.68e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([7421, 2])
We keep 3.11e+05/8.74e+06 =  3% of the original kernel matrix.

torch.Size([20599, 2])
We keep 1.64e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([15675, 2])
We keep 1.61e+06/5.08e+07 =  3% of the original kernel matrix.

torch.Size([28559, 2])
We keep 3.03e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([399048, 2])
We keep 8.58e+08/7.08e+10 =  1% of the original kernel matrix.

torch.Size([148341, 2])
We keep 5.87e+07/7.34e+09 =  0% of the original kernel matrix.

torch.Size([2131040, 2])
We keep 8.88e+09/1.17e+12 =  0% of the original kernel matrix.

torch.Size([360384, 2])
We keep 2.14e+08/2.98e+10 =  0% of the original kernel matrix.

torch.Size([70776, 2])
We keep 2.96e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([60827, 2])
We keep 1.23e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([19983, 2])
We keep 2.02e+06/9.07e+07 =  2% of the original kernel matrix.

torch.Size([32234, 2])
We keep 3.81e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([233935, 2])
We keep 3.53e+08/2.55e+10 =  1% of the original kernel matrix.

torch.Size([112095, 2])
We keep 3.83e+07/4.41e+09 =  0% of the original kernel matrix.

torch.Size([339820, 2])
We keep 2.44e+08/3.78e+10 =  0% of the original kernel matrix.

torch.Size([139007, 2])
We keep 4.47e+07/5.36e+09 =  0% of the original kernel matrix.

torch.Size([10830, 2])
We keep 5.96e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([24062, 2])
We keep 2.17e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([28264, 2])
We keep 5.11e+06/2.40e+08 =  2% of the original kernel matrix.

torch.Size([38755, 2])
We keep 5.45e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([97562, 2])
We keep 2.70e+07/2.80e+09 =  0% of the original kernel matrix.

torch.Size([71078, 2])
We keep 1.47e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([310889, 2])
We keep 1.38e+09/7.18e+10 =  1% of the original kernel matrix.

torch.Size([126655, 2])
We keep 6.08e+07/7.39e+09 =  0% of the original kernel matrix.

torch.Size([1167071, 2])
We keep 6.23e+09/6.53e+11 =  0% of the original kernel matrix.

torch.Size([253908, 2])
We keep 1.65e+08/2.23e+10 =  0% of the original kernel matrix.

torch.Size([437457, 2])
We keep 4.22e+08/6.20e+10 =  0% of the original kernel matrix.

torch.Size([156966, 2])
We keep 5.62e+07/6.87e+09 =  0% of the original kernel matrix.

torch.Size([57605, 2])
We keep 1.83e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([55942, 2])
We keep 9.76e+06/9.04e+08 =  1% of the original kernel matrix.

torch.Size([14194, 2])
We keep 9.90e+05/3.73e+07 =  2% of the original kernel matrix.

torch.Size([27215, 2])
We keep 2.72e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([12355, 2])
We keep 8.48e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([25362, 2])
We keep 2.45e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([217236, 2])
We keep 2.23e+08/1.75e+10 =  1% of the original kernel matrix.

torch.Size([107625, 2])
We keep 3.19e+07/3.64e+09 =  0% of the original kernel matrix.

torch.Size([41910, 2])
We keep 8.68e+06/5.42e+08 =  1% of the original kernel matrix.

torch.Size([47795, 2])
We keep 7.54e+06/6.42e+08 =  1% of the original kernel matrix.

torch.Size([70691, 2])
We keep 8.39e+07/3.16e+09 =  2% of the original kernel matrix.

torch.Size([59024, 2])
We keep 1.51e+07/1.55e+09 =  0% of the original kernel matrix.

torch.Size([382245, 2])
We keep 5.00e+08/5.33e+10 =  0% of the original kernel matrix.

torch.Size([146912, 2])
We keep 5.35e+07/6.37e+09 =  0% of the original kernel matrix.

torch.Size([1239532, 2])
We keep 2.11e+09/4.44e+11 =  0% of the original kernel matrix.

torch.Size([276134, 2])
We keep 1.36e+08/1.84e+10 =  0% of the original kernel matrix.

torch.Size([26562, 2])
We keep 4.33e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([37482, 2])
We keep 5.16e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([9886, 2])
We keep 7.29e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([23184, 2])
We keep 2.03e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([69349, 2])
We keep 4.20e+07/1.64e+09 =  2% of the original kernel matrix.

torch.Size([60498, 2])
We keep 1.15e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([319719, 2])
We keep 1.17e+09/3.83e+10 =  3% of the original kernel matrix.

torch.Size([133576, 2])
We keep 4.31e+07/5.40e+09 =  0% of the original kernel matrix.

torch.Size([93409, 2])
We keep 3.63e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([69560, 2])
We keep 1.48e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([17826, 2])
We keep 7.21e+06/1.34e+08 =  5% of the original kernel matrix.

torch.Size([30362, 2])
We keep 4.23e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([11669, 2])
We keep 1.02e+06/2.55e+07 =  3% of the original kernel matrix.

torch.Size([24832, 2])
We keep 2.38e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([60078, 2])
We keep 1.96e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([56581, 2])
We keep 1.05e+07/9.63e+08 =  1% of the original kernel matrix.

torch.Size([10475, 2])
We keep 7.06e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([23782, 2])
We keep 2.16e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([55545, 2])
We keep 9.78e+07/1.58e+09 =  6% of the original kernel matrix.

torch.Size([53618, 2])
We keep 1.19e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([18402, 2])
We keep 1.83e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([31052, 2])
We keep 3.38e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([36557, 2])
We keep 5.24e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([44713, 2])
We keep 6.67e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([30354, 2])
We keep 5.05e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([40368, 2])
We keep 5.85e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([22857, 2])
We keep 2.16e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([34573, 2])
We keep 4.21e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([13834, 2])
We keep 1.23e+06/3.95e+07 =  3% of the original kernel matrix.

torch.Size([26881, 2])
We keep 2.77e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([382638, 2])
We keep 3.08e+08/4.72e+10 =  0% of the original kernel matrix.

torch.Size([146657, 2])
We keep 4.96e+07/5.99e+09 =  0% of the original kernel matrix.

torch.Size([918623, 2])
We keep 1.55e+09/2.36e+11 =  0% of the original kernel matrix.

torch.Size([230042, 2])
We keep 1.02e+08/1.34e+10 =  0% of the original kernel matrix.

torch.Size([189415, 2])
We keep 2.61e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([100612, 2])
We keep 2.91e+07/3.23e+09 =  0% of the original kernel matrix.

torch.Size([124177, 2])
We keep 2.11e+08/6.16e+09 =  3% of the original kernel matrix.

torch.Size([79604, 2])
We keep 2.06e+07/2.16e+09 =  0% of the original kernel matrix.

torch.Size([52059, 2])
We keep 1.05e+07/8.50e+08 =  1% of the original kernel matrix.

torch.Size([53727, 2])
We keep 9.10e+06/8.04e+08 =  1% of the original kernel matrix.

torch.Size([23562, 2])
We keep 4.26e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([35178, 2])
We keep 4.47e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([46641, 2])
We keep 1.13e+07/6.85e+08 =  1% of the original kernel matrix.

torch.Size([51343, 2])
We keep 8.49e+06/7.22e+08 =  1% of the original kernel matrix.

torch.Size([380779, 2])
We keep 3.02e+08/4.85e+10 =  0% of the original kernel matrix.

torch.Size([151022, 2])
We keep 5.13e+07/6.07e+09 =  0% of the original kernel matrix.

torch.Size([34930, 2])
We keep 9.73e+06/3.85e+08 =  2% of the original kernel matrix.

torch.Size([43277, 2])
We keep 6.57e+06/5.41e+08 =  1% of the original kernel matrix.

torch.Size([125972, 2])
We keep 3.04e+08/6.00e+09 =  5% of the original kernel matrix.

torch.Size([80909, 2])
We keep 1.91e+07/2.14e+09 =  0% of the original kernel matrix.

torch.Size([36890, 2])
We keep 7.27e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([44809, 2])
We keep 6.92e+06/5.70e+08 =  1% of the original kernel matrix.

torch.Size([2595205, 2])
We keep 9.80e+09/1.73e+12 =  0% of the original kernel matrix.

torch.Size([402278, 2])
We keep 2.56e+08/3.62e+10 =  0% of the original kernel matrix.

torch.Size([44081, 2])
We keep 1.09e+07/6.14e+08 =  1% of the original kernel matrix.

torch.Size([49090, 2])
We keep 8.05e+06/6.83e+08 =  1% of the original kernel matrix.

torch.Size([164497, 2])
We keep 8.67e+07/8.93e+09 =  0% of the original kernel matrix.

torch.Size([92782, 2])
We keep 2.41e+07/2.61e+09 =  0% of the original kernel matrix.

torch.Size([618884, 2])
We keep 1.05e+09/1.30e+11 =  0% of the original kernel matrix.

torch.Size([186228, 2])
We keep 7.86e+07/9.94e+09 =  0% of the original kernel matrix.

torch.Size([140902, 2])
We keep 6.63e+07/5.91e+09 =  1% of the original kernel matrix.

torch.Size([85419, 2])
We keep 2.01e+07/2.12e+09 =  0% of the original kernel matrix.

torch.Size([63596, 2])
We keep 2.42e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([58125, 2])
We keep 1.13e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([1577411, 2])
We keep 6.22e+09/7.23e+11 =  0% of the original kernel matrix.

torch.Size([305131, 2])
We keep 1.72e+08/2.35e+10 =  0% of the original kernel matrix.

torch.Size([284468, 2])
We keep 1.81e+08/2.66e+10 =  0% of the original kernel matrix.

torch.Size([125235, 2])
We keep 3.86e+07/4.50e+09 =  0% of the original kernel matrix.

torch.Size([107584, 2])
We keep 6.86e+07/3.58e+09 =  1% of the original kernel matrix.

torch.Size([74492, 2])
We keep 1.60e+07/1.65e+09 =  0% of the original kernel matrix.

torch.Size([28434, 2])
We keep 6.32e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([38791, 2])
We keep 5.49e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([77873, 2])
We keep 3.28e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([63906, 2])
We keep 1.28e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([41140, 2])
We keep 1.29e+07/5.92e+08 =  2% of the original kernel matrix.

torch.Size([47143, 2])
We keep 7.94e+06/6.71e+08 =  1% of the original kernel matrix.

torch.Size([306301, 2])
We keep 5.54e+08/3.04e+10 =  1% of the original kernel matrix.

torch.Size([130407, 2])
We keep 4.06e+07/4.81e+09 =  0% of the original kernel matrix.

torch.Size([1989646, 2])
We keep 5.42e+09/9.42e+11 =  0% of the original kernel matrix.

torch.Size([344678, 2])
We keep 1.92e+08/2.68e+10 =  0% of the original kernel matrix.

torch.Size([201166, 2])
We keep 4.04e+08/1.55e+10 =  2% of the original kernel matrix.

torch.Size([104290, 2])
We keep 3.08e+07/3.43e+09 =  0% of the original kernel matrix.

torch.Size([327239, 2])
We keep 4.12e+08/4.39e+10 =  0% of the original kernel matrix.

torch.Size([134811, 2])
We keep 4.85e+07/5.78e+09 =  0% of the original kernel matrix.

torch.Size([82290, 2])
We keep 8.93e+07/2.24e+09 =  3% of the original kernel matrix.

torch.Size([65541, 2])
We keep 1.32e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([43694, 2])
We keep 3.20e+07/7.34e+08 =  4% of the original kernel matrix.

torch.Size([46614, 2])
We keep 8.21e+06/7.47e+08 =  1% of the original kernel matrix.

torch.Size([67859, 2])
We keep 2.29e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([59878, 2])
We keep 1.19e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([35377, 2])
We keep 1.34e+07/4.43e+08 =  3% of the original kernel matrix.

torch.Size([43088, 2])
We keep 7.02e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([726247, 2])
We keep 4.05e+09/2.60e+11 =  1% of the original kernel matrix.

torch.Size([200788, 2])
We keep 1.08e+08/1.41e+10 =  0% of the original kernel matrix.

torch.Size([993874, 2])
We keep 1.33e+09/2.78e+11 =  0% of the original kernel matrix.

torch.Size([242159, 2])
We keep 1.09e+08/1.45e+10 =  0% of the original kernel matrix.

torch.Size([41912, 2])
We keep 8.37e+07/1.84e+09 =  4% of the original kernel matrix.

torch.Size([45638, 2])
We keep 1.24e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([2677874, 2])
We keep 3.05e+10/3.37e+12 =  0% of the original kernel matrix.

torch.Size([382024, 2])
We keep 3.53e+08/5.06e+10 =  0% of the original kernel matrix.

torch.Size([209979, 2])
We keep 6.11e+08/1.88e+10 =  3% of the original kernel matrix.

torch.Size([106348, 2])
We keep 3.34e+07/3.78e+09 =  0% of the original kernel matrix.

torch.Size([114393, 2])
We keep 4.89e+07/4.13e+09 =  1% of the original kernel matrix.

torch.Size([77011, 2])
We keep 1.74e+07/1.77e+09 =  0% of the original kernel matrix.

torch.Size([15327, 2])
We keep 1.50e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([28122, 2])
We keep 3.09e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([132926, 2])
We keep 7.58e+07/5.65e+09 =  1% of the original kernel matrix.

torch.Size([83272, 2])
We keep 1.96e+07/2.07e+09 =  0% of the original kernel matrix.

torch.Size([345513, 2])
We keep 2.85e+08/3.93e+10 =  0% of the original kernel matrix.

torch.Size([139758, 2])
We keep 4.56e+07/5.47e+09 =  0% of the original kernel matrix.

torch.Size([14943, 2])
We keep 1.75e+06/4.77e+07 =  3% of the original kernel matrix.

torch.Size([27979, 2])
We keep 2.95e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([2047073, 2])
We keep 4.50e+09/9.89e+11 =  0% of the original kernel matrix.

torch.Size([348835, 2])
We keep 1.96e+08/2.74e+10 =  0% of the original kernel matrix.

torch.Size([63259, 2])
We keep 2.27e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([57882, 2])
We keep 1.08e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([55602, 2])
We keep 2.60e+07/1.20e+09 =  2% of the original kernel matrix.

torch.Size([54196, 2])
We keep 1.04e+07/9.55e+08 =  1% of the original kernel matrix.

torch.Size([30455, 2])
We keep 6.68e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([40219, 2])
We keep 5.84e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([38819, 2])
We keep 6.80e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([46364, 2])
We keep 7.38e+06/6.12e+08 =  1% of the original kernel matrix.

torch.Size([8626, 2])
We keep 4.25e+05/1.23e+07 =  3% of the original kernel matrix.

torch.Size([21928, 2])
We keep 1.85e+06/9.69e+07 =  1% of the original kernel matrix.

torch.Size([36509, 2])
We keep 8.59e+06/4.94e+08 =  1% of the original kernel matrix.

torch.Size([44511, 2])
We keep 7.42e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([50619, 2])
We keep 1.91e+07/8.69e+08 =  2% of the original kernel matrix.

torch.Size([52264, 2])
We keep 9.32e+06/8.13e+08 =  1% of the original kernel matrix.

torch.Size([103305, 2])
We keep 6.19e+07/3.40e+09 =  1% of the original kernel matrix.

torch.Size([72949, 2])
We keep 1.60e+07/1.61e+09 =  0% of the original kernel matrix.

torch.Size([1561230, 2])
We keep 4.91e+10/3.03e+12 =  1% of the original kernel matrix.

torch.Size([271353, 2])
We keep 3.29e+08/4.80e+10 =  0% of the original kernel matrix.

torch.Size([46128, 2])
We keep 3.19e+07/1.00e+09 =  3% of the original kernel matrix.

torch.Size([48733, 2])
We keep 9.90e+06/8.73e+08 =  1% of the original kernel matrix.

torch.Size([494294, 2])
We keep 4.87e+08/7.43e+10 =  0% of the original kernel matrix.

torch.Size([167007, 2])
We keep 6.02e+07/7.52e+09 =  0% of the original kernel matrix.

torch.Size([19894, 2])
We keep 1.80e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([32067, 2])
We keep 3.60e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([61425, 2])
We keep 1.45e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([57730, 2])
We keep 1.06e+07/9.62e+08 =  1% of the original kernel matrix.

torch.Size([21817, 2])
We keep 3.20e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([33621, 2])
We keep 4.28e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([156423, 2])
We keep 7.13e+08/2.10e+10 =  3% of the original kernel matrix.

torch.Size([89987, 2])
We keep 3.46e+07/3.99e+09 =  0% of the original kernel matrix.

torch.Size([195009, 2])
We keep 2.96e+08/1.91e+10 =  1% of the original kernel matrix.

torch.Size([101331, 2])
We keep 3.40e+07/3.81e+09 =  0% of the original kernel matrix.

torch.Size([3237952, 2])
We keep 1.08e+10/2.34e+12 =  0% of the original kernel matrix.

torch.Size([449901, 2])
We keep 2.92e+08/4.22e+10 =  0% of the original kernel matrix.

torch.Size([27621, 2])
We keep 6.55e+06/2.16e+08 =  3% of the original kernel matrix.

torch.Size([38379, 2])
We keep 5.28e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([417339, 2])
We keep 4.24e+08/5.65e+10 =  0% of the original kernel matrix.

torch.Size([154251, 2])
We keep 5.37e+07/6.56e+09 =  0% of the original kernel matrix.

torch.Size([50324, 2])
We keep 4.43e+07/8.86e+08 =  4% of the original kernel matrix.

torch.Size([51844, 2])
We keep 9.17e+06/8.21e+08 =  1% of the original kernel matrix.

torch.Size([304419, 2])
We keep 2.81e+08/3.25e+10 =  0% of the original kernel matrix.

torch.Size([129563, 2])
We keep 4.21e+07/4.97e+09 =  0% of the original kernel matrix.

torch.Size([442515, 2])
We keep 6.62e+08/7.53e+10 =  0% of the original kernel matrix.

torch.Size([157598, 2])
We keep 6.15e+07/7.57e+09 =  0% of the original kernel matrix.

torch.Size([65416, 2])
We keep 1.88e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([58997, 2])
We keep 1.11e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([27065, 2])
We keep 4.63e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([37933, 2])
We keep 5.08e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([201457, 2])
We keep 1.17e+08/1.27e+10 =  0% of the original kernel matrix.

torch.Size([103751, 2])
We keep 2.80e+07/3.11e+09 =  0% of the original kernel matrix.

torch.Size([77162, 2])
We keep 1.62e+08/2.57e+09 =  6% of the original kernel matrix.

torch.Size([63117, 2])
We keep 1.34e+07/1.40e+09 =  0% of the original kernel matrix.

torch.Size([181969, 2])
We keep 1.83e+08/1.28e+10 =  1% of the original kernel matrix.

torch.Size([97786, 2])
We keep 2.79e+07/3.13e+09 =  0% of the original kernel matrix.

torch.Size([814859, 2])
We keep 1.59e+09/1.99e+11 =  0% of the original kernel matrix.

torch.Size([214001, 2])
We keep 9.57e+07/1.23e+10 =  0% of the original kernel matrix.

torch.Size([28207, 2])
We keep 6.85e+06/2.76e+08 =  2% of the original kernel matrix.

torch.Size([38730, 2])
We keep 5.79e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([123201, 2])
We keep 1.20e+08/4.97e+09 =  2% of the original kernel matrix.

torch.Size([79962, 2])
We keep 1.90e+07/1.94e+09 =  0% of the original kernel matrix.

torch.Size([65439, 2])
We keep 2.51e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([58490, 2])
We keep 1.15e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([324172, 2])
We keep 2.88e+08/3.71e+10 =  0% of the original kernel matrix.

torch.Size([134456, 2])
We keep 4.49e+07/5.31e+09 =  0% of the original kernel matrix.

torch.Size([560390, 2])
We keep 8.63e+08/9.97e+10 =  0% of the original kernel matrix.

torch.Size([176039, 2])
We keep 6.86e+07/8.71e+09 =  0% of the original kernel matrix.

torch.Size([92449, 2])
We keep 3.61e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([69239, 2])
We keep 1.44e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([24589, 2])
We keep 2.92e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([36093, 2])
We keep 4.61e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([558511, 2])
We keep 1.12e+09/1.23e+11 =  0% of the original kernel matrix.

torch.Size([174803, 2])
We keep 7.65e+07/9.67e+09 =  0% of the original kernel matrix.

torch.Size([211608, 2])
We keep 1.55e+08/1.60e+10 =  0% of the original kernel matrix.

torch.Size([106368, 2])
We keep 3.11e+07/3.49e+09 =  0% of the original kernel matrix.

torch.Size([1273947, 2])
We keep 9.44e+09/7.74e+11 =  1% of the original kernel matrix.

torch.Size([263964, 2])
We keep 1.78e+08/2.43e+10 =  0% of the original kernel matrix.

torch.Size([179924, 2])
We keep 1.34e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([97279, 2])
We keep 2.71e+07/2.93e+09 =  0% of the original kernel matrix.

torch.Size([153527, 2])
We keep 1.62e+08/9.97e+09 =  1% of the original kernel matrix.

torch.Size([89345, 2])
We keep 2.57e+07/2.75e+09 =  0% of the original kernel matrix.

torch.Size([63925, 2])
We keep 4.41e+07/1.75e+09 =  2% of the original kernel matrix.

torch.Size([57746, 2])
We keep 1.20e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([215965, 2])
We keep 3.45e+08/2.28e+10 =  1% of the original kernel matrix.

torch.Size([105649, 2])
We keep 3.64e+07/4.17e+09 =  0% of the original kernel matrix.

torch.Size([446842, 2])
We keep 9.79e+08/8.98e+10 =  1% of the original kernel matrix.

torch.Size([155564, 2])
We keep 6.70e+07/8.26e+09 =  0% of the original kernel matrix.

torch.Size([119221, 2])
We keep 5.42e+07/4.32e+09 =  1% of the original kernel matrix.

torch.Size([77583, 2])
We keep 1.80e+07/1.81e+09 =  0% of the original kernel matrix.

torch.Size([19890, 2])
We keep 2.08e+06/9.24e+07 =  2% of the original kernel matrix.

torch.Size([32130, 2])
We keep 3.84e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([38639, 2])
We keep 1.14e+07/4.97e+08 =  2% of the original kernel matrix.

torch.Size([45708, 2])
We keep 7.41e+06/6.15e+08 =  1% of the original kernel matrix.

torch.Size([98285, 2])
We keep 1.03e+08/3.42e+09 =  3% of the original kernel matrix.

torch.Size([70920, 2])
We keep 1.58e+07/1.61e+09 =  0% of the original kernel matrix.

torch.Size([207396, 2])
We keep 2.96e+08/1.62e+10 =  1% of the original kernel matrix.

torch.Size([105365, 2])
We keep 3.12e+07/3.51e+09 =  0% of the original kernel matrix.

torch.Size([161260, 2])
We keep 7.44e+07/7.92e+09 =  0% of the original kernel matrix.

torch.Size([91704, 2])
We keep 2.28e+07/2.45e+09 =  0% of the original kernel matrix.

torch.Size([24749, 2])
We keep 2.82e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([36174, 2])
We keep 4.61e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([60035, 2])
We keep 1.30e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([56307, 2])
We keep 1.03e+07/9.47e+08 =  1% of the original kernel matrix.

torch.Size([165539, 2])
We keep 1.95e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([93259, 2])
We keep 2.52e+07/2.80e+09 =  0% of the original kernel matrix.

torch.Size([143500, 2])
We keep 6.35e+07/6.13e+09 =  1% of the original kernel matrix.

torch.Size([86156, 2])
We keep 2.05e+07/2.16e+09 =  0% of the original kernel matrix.

torch.Size([72655, 2])
We keep 2.48e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([62248, 2])
We keep 1.22e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([138800, 2])
We keep 5.76e+07/5.70e+09 =  1% of the original kernel matrix.

torch.Size([84441, 2])
We keep 1.98e+07/2.08e+09 =  0% of the original kernel matrix.

torch.Size([59946, 2])
We keep 3.24e+07/1.58e+09 =  2% of the original kernel matrix.

torch.Size([55753, 2])
We keep 1.20e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([56433, 2])
We keep 1.46e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([55231, 2])
We keep 9.55e+06/8.79e+08 =  1% of the original kernel matrix.

torch.Size([9702, 2])
We keep 5.96e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([22889, 2])
We keep 2.09e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([26193, 2])
We keep 3.24e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([37413, 2])
We keep 4.96e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([385461, 2])
We keep 2.98e+08/4.88e+10 =  0% of the original kernel matrix.

torch.Size([150667, 2])
We keep 5.08e+07/6.09e+09 =  0% of the original kernel matrix.

torch.Size([56002, 2])
We keep 1.94e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([54613, 2])
We keep 1.01e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([194472, 2])
We keep 9.69e+07/1.17e+10 =  0% of the original kernel matrix.

torch.Size([101918, 2])
We keep 2.67e+07/2.98e+09 =  0% of the original kernel matrix.

torch.Size([7983, 2])
We keep 4.02e+05/9.91e+06 =  4% of the original kernel matrix.

torch.Size([21243, 2])
We keep 1.70e+06/8.68e+07 =  1% of the original kernel matrix.

torch.Size([17190, 2])
We keep 2.92e+06/6.49e+07 =  4% of the original kernel matrix.

torch.Size([29882, 2])
We keep 3.31e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([210893, 2])
We keep 3.68e+08/1.96e+10 =  1% of the original kernel matrix.

torch.Size([106196, 2])
We keep 3.33e+07/3.86e+09 =  0% of the original kernel matrix.

torch.Size([207313, 2])
We keep 1.31e+08/1.40e+10 =  0% of the original kernel matrix.

torch.Size([105125, 2])
We keep 2.91e+07/3.27e+09 =  0% of the original kernel matrix.

torch.Size([72653, 2])
We keep 2.30e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([62276, 2])
We keep 1.19e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([15739, 2])
We keep 1.25e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([28459, 2])
We keep 3.03e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([16176, 2])
We keep 1.75e+06/5.27e+07 =  3% of the original kernel matrix.

torch.Size([29033, 2])
We keep 3.09e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([69481, 2])
We keep 2.49e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([60935, 2])
We keep 1.16e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([57152, 2])
We keep 1.95e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([55584, 2])
We keep 1.00e+07/9.07e+08 =  1% of the original kernel matrix.

torch.Size([228969, 2])
We keep 3.14e+08/2.01e+10 =  1% of the original kernel matrix.

torch.Size([111229, 2])
We keep 3.44e+07/3.91e+09 =  0% of the original kernel matrix.

torch.Size([20390, 2])
We keep 2.74e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([32734, 2])
We keep 4.08e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([36390, 2])
We keep 1.08e+07/5.13e+08 =  2% of the original kernel matrix.

torch.Size([43687, 2])
We keep 7.51e+06/6.24e+08 =  1% of the original kernel matrix.

torch.Size([56115, 2])
We keep 3.22e+07/1.57e+09 =  2% of the original kernel matrix.

torch.Size([53569, 2])
We keep 1.15e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([27098, 2])
We keep 6.06e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([37803, 2])
We keep 5.19e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([382765, 2])
We keep 6.62e+08/5.87e+10 =  1% of the original kernel matrix.

torch.Size([146594, 2])
We keep 5.37e+07/6.68e+09 =  0% of the original kernel matrix.

torch.Size([32513, 2])
We keep 2.43e+07/3.85e+08 =  6% of the original kernel matrix.

torch.Size([41579, 2])
We keep 6.60e+06/5.41e+08 =  1% of the original kernel matrix.

torch.Size([191240, 2])
We keep 1.76e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([100779, 2])
We keep 2.93e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([345140, 2])
We keep 3.50e+08/3.96e+10 =  0% of the original kernel matrix.

torch.Size([139942, 2])
We keep 4.60e+07/5.49e+09 =  0% of the original kernel matrix.

torch.Size([80379, 2])
We keep 2.56e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([65216, 2])
We keep 1.31e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([188686, 2])
We keep 2.02e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([100669, 2])
We keep 2.73e+07/3.02e+09 =  0% of the original kernel matrix.

torch.Size([289176, 2])
We keep 1.97e+08/2.64e+10 =  0% of the original kernel matrix.

torch.Size([126475, 2])
We keep 3.85e+07/4.48e+09 =  0% of the original kernel matrix.

torch.Size([10303, 2])
We keep 5.79e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([23604, 2])
We keep 2.08e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([120563, 2])
We keep 4.89e+07/4.42e+09 =  1% of the original kernel matrix.

torch.Size([78512, 2])
We keep 1.80e+07/1.83e+09 =  0% of the original kernel matrix.

torch.Size([57093, 2])
We keep 1.44e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([55803, 2])
We keep 9.75e+06/8.81e+08 =  1% of the original kernel matrix.

torch.Size([579159, 2])
We keep 1.00e+09/1.02e+11 =  0% of the original kernel matrix.

torch.Size([179136, 2])
We keep 7.12e+07/8.80e+09 =  0% of the original kernel matrix.

torch.Size([226871, 2])
We keep 2.06e+08/2.02e+10 =  1% of the original kernel matrix.

torch.Size([110749, 2])
We keep 3.43e+07/3.92e+09 =  0% of the original kernel matrix.

torch.Size([88696, 2])
We keep 6.35e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([66534, 2])
We keep 1.56e+07/1.58e+09 =  0% of the original kernel matrix.

torch.Size([211631, 2])
We keep 1.49e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([106662, 2])
We keep 2.94e+07/3.32e+09 =  0% of the original kernel matrix.

torch.Size([59480, 2])
We keep 1.74e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([56155, 2])
We keep 1.04e+07/9.49e+08 =  1% of the original kernel matrix.

torch.Size([28059, 2])
We keep 3.36e+07/5.52e+08 =  6% of the original kernel matrix.

torch.Size([37694, 2])
We keep 7.17e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([15723, 2])
We keep 1.27e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([28573, 2])
We keep 3.04e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([31191, 2])
We keep 5.65e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([40978, 2])
We keep 6.08e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([645512, 2])
We keep 2.34e+09/1.31e+11 =  1% of the original kernel matrix.

torch.Size([190171, 2])
We keep 7.57e+07/9.97e+09 =  0% of the original kernel matrix.

torch.Size([1617632, 2])
We keep 5.30e+09/7.20e+11 =  0% of the original kernel matrix.

torch.Size([309720, 2])
We keep 1.68e+08/2.34e+10 =  0% of the original kernel matrix.

torch.Size([81577, 2])
We keep 7.29e+07/3.12e+09 =  2% of the original kernel matrix.

torch.Size([63615, 2])
We keep 1.56e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([38084, 2])
We keep 6.78e+06/4.48e+08 =  1% of the original kernel matrix.

torch.Size([45424, 2])
We keep 6.83e+06/5.83e+08 =  1% of the original kernel matrix.

torch.Size([76709, 2])
We keep 2.61e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([63671, 2])
We keep 1.27e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([910949, 2])
We keep 2.55e+09/2.73e+11 =  0% of the original kernel matrix.

torch.Size([231163, 2])
We keep 1.09e+08/1.44e+10 =  0% of the original kernel matrix.

torch.Size([71417, 2])
We keep 2.04e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([61760, 2])
We keep 1.18e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([36813, 2])
We keep 6.66e+06/4.26e+08 =  1% of the original kernel matrix.

torch.Size([44770, 2])
We keep 6.94e+06/5.69e+08 =  1% of the original kernel matrix.

torch.Size([159589, 2])
We keep 8.06e+07/7.96e+09 =  1% of the original kernel matrix.

torch.Size([91300, 2])
We keep 2.28e+07/2.46e+09 =  0% of the original kernel matrix.

torch.Size([249074, 2])
We keep 1.97e+08/1.99e+10 =  0% of the original kernel matrix.

torch.Size([116336, 2])
We keep 3.37e+07/3.89e+09 =  0% of the original kernel matrix.

torch.Size([26736, 2])
We keep 6.26e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([37694, 2])
We keep 5.40e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([57219, 2])
We keep 2.97e+07/1.09e+09 =  2% of the original kernel matrix.

torch.Size([55503, 2])
We keep 9.99e+06/9.11e+08 =  1% of the original kernel matrix.

torch.Size([92535, 2])
We keep 1.49e+08/4.07e+09 =  3% of the original kernel matrix.

torch.Size([68802, 2])
We keep 1.66e+07/1.76e+09 =  0% of the original kernel matrix.

time for making ranges is 8.976473808288574
Sorting X and nu_X
time for sorting X is 0.10648632049560547
Sorting Z and nu_Z
time for sorting Z is 0.0002849102020263672
Starting Optim
sum tnu_Z before tensor(66488108., device='cuda:0')
c= tensor(3138.6401, device='cuda:0')
c= tensor(294290.4062, device='cuda:0')
c= tensor(302727.8750, device='cuda:0')
c= tensor(419534.6250, device='cuda:0')
c= tensor(1878808.5000, device='cuda:0')
c= tensor(2702185.2500, device='cuda:0')
c= tensor(4186347.7500, device='cuda:0')
c= tensor(5178750., device='cuda:0')
c= tensor(5760474., device='cuda:0')
c= tensor(23928566., device='cuda:0')
c= tensor(24206828., device='cuda:0')
c= tensor(32934284., device='cuda:0')
c= tensor(32960936., device='cuda:0')
c= tensor(69932832., device='cuda:0')
c= tensor(70333176., device='cuda:0')
c= tensor(71133792., device='cuda:0')
c= tensor(72523512., device='cuda:0')
c= tensor(74090256., device='cuda:0')
c= tensor(84474720., device='cuda:0')
c= tensor(90697888., device='cuda:0')
c= tensor(91522008., device='cuda:0')
c= tensor(1.0901e+08, device='cuda:0')
c= tensor(1.0910e+08, device='cuda:0')
c= tensor(1.1021e+08, device='cuda:0')
c= tensor(1.1282e+08, device='cuda:0')
c= tensor(1.1480e+08, device='cuda:0')
c= tensor(1.1835e+08, device='cuda:0')
c= tensor(1.1884e+08, device='cuda:0')
c= tensor(1.4849e+08, device='cuda:0')
c= tensor(6.8855e+08, device='cuda:0')
c= tensor(6.8882e+08, device='cuda:0')
c= tensor(1.2026e+09, device='cuda:0')
c= tensor(1.2029e+09, device='cuda:0')
c= tensor(1.2032e+09, device='cuda:0')
c= tensor(1.2034e+09, device='cuda:0')
c= tensor(1.2687e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2720e+09, device='cuda:0')
c= tensor(1.2721e+09, device='cuda:0')
c= tensor(1.2721e+09, device='cuda:0')
c= tensor(1.2721e+09, device='cuda:0')
c= tensor(1.2721e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2723e+09, device='cuda:0')
c= tensor(1.2723e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2725e+09, device='cuda:0')
c= tensor(1.2725e+09, device='cuda:0')
c= tensor(1.2725e+09, device='cuda:0')
c= tensor(1.2725e+09, device='cuda:0')
c= tensor(1.2726e+09, device='cuda:0')
c= tensor(1.2726e+09, device='cuda:0')
c= tensor(1.2726e+09, device='cuda:0')
c= tensor(1.2726e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2730e+09, device='cuda:0')
c= tensor(1.2730e+09, device='cuda:0')
c= tensor(1.2730e+09, device='cuda:0')
c= tensor(1.2731e+09, device='cuda:0')
c= tensor(1.2731e+09, device='cuda:0')
c= tensor(1.2731e+09, device='cuda:0')
c= tensor(1.2731e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2736e+09, device='cuda:0')
c= tensor(1.2736e+09, device='cuda:0')
c= tensor(1.2736e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2738e+09, device='cuda:0')
c= tensor(1.2738e+09, device='cuda:0')
c= tensor(1.2738e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2740e+09, device='cuda:0')
c= tensor(1.2740e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2748e+09, device='cuda:0')
c= tensor(1.2748e+09, device='cuda:0')
c= tensor(1.2748e+09, device='cuda:0')
c= tensor(1.2748e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2750e+09, device='cuda:0')
c= tensor(1.2750e+09, device='cuda:0')
c= tensor(1.2750e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2756e+09, device='cuda:0')
c= tensor(1.2756e+09, device='cuda:0')
c= tensor(1.2756e+09, device='cuda:0')
c= tensor(1.2756e+09, device='cuda:0')
c= tensor(1.2756e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2764e+09, device='cuda:0')
c= tensor(1.2764e+09, device='cuda:0')
c= tensor(1.2764e+09, device='cuda:0')
c= tensor(1.2766e+09, device='cuda:0')
c= tensor(1.2794e+09, device='cuda:0')
c= tensor(1.2795e+09, device='cuda:0')
c= tensor(1.2796e+09, device='cuda:0')
c= tensor(1.2796e+09, device='cuda:0')
c= tensor(1.2797e+09, device='cuda:0')
c= tensor(1.2799e+09, device='cuda:0')
c= tensor(1.2912e+09, device='cuda:0')
c= tensor(1.2912e+09, device='cuda:0')
c= tensor(1.3021e+09, device='cuda:0')
c= tensor(1.3120e+09, device='cuda:0')
c= tensor(1.3131e+09, device='cuda:0')
c= tensor(1.3331e+09, device='cuda:0')
c= tensor(1.3331e+09, device='cuda:0')
c= tensor(1.3332e+09, device='cuda:0')
c= tensor(1.3838e+09, device='cuda:0')
c= tensor(1.4733e+09, device='cuda:0')
c= tensor(1.4733e+09, device='cuda:0')
c= tensor(1.4738e+09, device='cuda:0')
c= tensor(1.4773e+09, device='cuda:0')
c= tensor(1.4779e+09, device='cuda:0')
c= tensor(1.4809e+09, device='cuda:0')
c= tensor(1.4830e+09, device='cuda:0')
c= tensor(1.4835e+09, device='cuda:0')
c= tensor(1.4837e+09, device='cuda:0')
c= tensor(1.4838e+09, device='cuda:0')
c= tensor(1.5058e+09, device='cuda:0')
c= tensor(1.5059e+09, device='cuda:0')
c= tensor(1.5059e+09, device='cuda:0')
c= tensor(1.5066e+09, device='cuda:0')
c= tensor(1.5080e+09, device='cuda:0')
c= tensor(1.5505e+09, device='cuda:0')
c= tensor(1.5530e+09, device='cuda:0')
c= tensor(1.5530e+09, device='cuda:0')
c= tensor(1.5537e+09, device='cuda:0')
c= tensor(1.5537e+09, device='cuda:0')
c= tensor(1.5545e+09, device='cuda:0')
c= tensor(1.5571e+09, device='cuda:0')
c= tensor(1.5651e+09, device='cuda:0')
c= tensor(1.5688e+09, device='cuda:0')
c= tensor(1.5688e+09, device='cuda:0')
c= tensor(1.5688e+09, device='cuda:0')
c= tensor(1.5721e+09, device='cuda:0')
c= tensor(1.5759e+09, device='cuda:0')
c= tensor(1.5778e+09, device='cuda:0')
c= tensor(1.5779e+09, device='cuda:0')
c= tensor(1.6248e+09, device='cuda:0')
c= tensor(1.6249e+09, device='cuda:0')
c= tensor(1.6254e+09, device='cuda:0')
c= tensor(1.6292e+09, device='cuda:0')
c= tensor(1.6292e+09, device='cuda:0')
c= tensor(1.6319e+09, device='cuda:0')
c= tensor(1.6523e+09, device='cuda:0')
c= tensor(1.9814e+09, device='cuda:0')
c= tensor(1.9818e+09, device='cuda:0')
c= tensor(1.9823e+09, device='cuda:0')
c= tensor(1.9824e+09, device='cuda:0')
c= tensor(1.9824e+09, device='cuda:0')
c= tensor(1.9975e+09, device='cuda:0')
c= tensor(1.9976e+09, device='cuda:0')
c= tensor(1.9986e+09, device='cuda:0')
c= tensor(2.0111e+09, device='cuda:0')
c= tensor(2.0134e+09, device='cuda:0')
c= tensor(2.0137e+09, device='cuda:0')
c= tensor(2.0137e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0503e+09, device='cuda:0')
c= tensor(2.0515e+09, device='cuda:0')
c= tensor(2.0515e+09, device='cuda:0')
c= tensor(2.0668e+09, device='cuda:0')
c= tensor(2.0669e+09, device='cuda:0')
c= tensor(2.0762e+09, device='cuda:0')
c= tensor(2.0768e+09, device='cuda:0')
c= tensor(2.0789e+09, device='cuda:0')
c= tensor(2.0798e+09, device='cuda:0')
c= tensor(2.1378e+09, device='cuda:0')
c= tensor(2.1403e+09, device='cuda:0')
c= tensor(2.1404e+09, device='cuda:0')
c= tensor(2.1486e+09, device='cuda:0')
c= tensor(2.1571e+09, device='cuda:0')
c= tensor(2.1571e+09, device='cuda:0')
c= tensor(2.1650e+09, device='cuda:0')
c= tensor(2.1800e+09, device='cuda:0')
c= tensor(2.2845e+09, device='cuda:0')
c= tensor(2.2850e+09, device='cuda:0')
c= tensor(2.2851e+09, device='cuda:0')
c= tensor(2.2852e+09, device='cuda:0')
c= tensor(2.2858e+09, device='cuda:0')
c= tensor(2.2864e+09, device='cuda:0')
c= tensor(2.2873e+09, device='cuda:0')
c= tensor(2.2873e+09, device='cuda:0')
c= tensor(2.2891e+09, device='cuda:0')
c= tensor(2.2982e+09, device='cuda:0')
c= tensor(2.3181e+09, device='cuda:0')
c= tensor(2.3181e+09, device='cuda:0')
c= tensor(2.3223e+09, device='cuda:0')
c= tensor(2.3224e+09, device='cuda:0')
c= tensor(2.3227e+09, device='cuda:0')
c= tensor(2.3228e+09, device='cuda:0')
c= tensor(2.3228e+09, device='cuda:0')
c= tensor(2.3689e+09, device='cuda:0')
c= tensor(2.3699e+09, device='cuda:0')
c= tensor(2.3701e+09, device='cuda:0')
c= tensor(2.3721e+09, device='cuda:0')
c= tensor(2.3721e+09, device='cuda:0')
c= tensor(2.6245e+09, device='cuda:0')
c= tensor(2.6246e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6332e+09, device='cuda:0')
c= tensor(2.6332e+09, device='cuda:0')
c= tensor(2.6352e+09, device='cuda:0')
c= tensor(2.6352e+09, device='cuda:0')
c= tensor(2.6352e+09, device='cuda:0')
c= tensor(2.6587e+09, device='cuda:0')
c= tensor(2.6603e+09, device='cuda:0')
c= tensor(2.6611e+09, device='cuda:0')
c= tensor(2.6664e+09, device='cuda:0')
c= tensor(2.6990e+09, device='cuda:0')
c= tensor(2.6990e+09, device='cuda:0')
c= tensor(2.6991e+09, device='cuda:0')
c= tensor(2.6995e+09, device='cuda:0')
c= tensor(2.6995e+09, device='cuda:0')
c= tensor(2.6995e+09, device='cuda:0')
c= tensor(2.6997e+09, device='cuda:0')
c= tensor(2.6998e+09, device='cuda:0')
c= tensor(2.6998e+09, device='cuda:0')
c= tensor(2.6999e+09, device='cuda:0')
c= tensor(2.7000e+09, device='cuda:0')
c= tensor(2.9194e+09, device='cuda:0')
c= tensor(2.9195e+09, device='cuda:0')
c= tensor(2.9228e+09, device='cuda:0')
c= tensor(2.9233e+09, device='cuda:0')
c= tensor(2.9233e+09, device='cuda:0')
c= tensor(2.9259e+09, device='cuda:0')
c= tensor(3.0246e+09, device='cuda:0')
c= tensor(3.1226e+09, device='cuda:0')
c= tensor(3.1258e+09, device='cuda:0')
c= tensor(3.1269e+09, device='cuda:0')
c= tensor(3.1269e+09, device='cuda:0')
c= tensor(3.1324e+09, device='cuda:0')
c= tensor(3.9370e+09, device='cuda:0')
c= tensor(3.9408e+09, device='cuda:0')
c= tensor(3.9409e+09, device='cuda:0')
c= tensor(3.9457e+09, device='cuda:0')
c= tensor(4.0325e+09, device='cuda:0')
c= tensor(4.0347e+09, device='cuda:0')
c= tensor(4.0348e+09, device='cuda:0')
c= tensor(4.0349e+09, device='cuda:0')
c= tensor(4.0349e+09, device='cuda:0')
c= tensor(4.0350e+09, device='cuda:0')
c= tensor(4.0812e+09, device='cuda:0')
c= tensor(4.0815e+09, device='cuda:0')
c= tensor(4.0815e+09, device='cuda:0')
c= tensor(4.0833e+09, device='cuda:0')
c= tensor(4.0835e+09, device='cuda:0')
c= tensor(4.0835e+09, device='cuda:0')
c= tensor(4.0876e+09, device='cuda:0')
c= tensor(4.0915e+09, device='cuda:0')
c= tensor(4.1054e+09, device='cuda:0')
c= tensor(4.1193e+09, device='cuda:0')
c= tensor(4.1347e+09, device='cuda:0')
c= tensor(4.1351e+09, device='cuda:0')
c= tensor(4.1365e+09, device='cuda:0')
c= tensor(4.1406e+09, device='cuda:0')
c= tensor(4.1673e+09, device='cuda:0')
c= tensor(4.1673e+09, device='cuda:0')
c= tensor(4.1981e+09, device='cuda:0')
c= tensor(4.2307e+09, device='cuda:0')
c= tensor(4.2407e+09, device='cuda:0')
c= tensor(4.2444e+09, device='cuda:0')
c= tensor(4.2486e+09, device='cuda:0')
c= tensor(4.2488e+09, device='cuda:0')
c= tensor(4.2488e+09, device='cuda:0')
c= tensor(4.2530e+09, device='cuda:0')
c= tensor(4.2639e+09, device='cuda:0')
c= tensor(4.2706e+09, device='cuda:0')
c= tensor(4.3001e+09, device='cuda:0')
c= tensor(4.3094e+09, device='cuda:0')
c= tensor(4.3140e+09, device='cuda:0')
c= tensor(4.3145e+09, device='cuda:0')
c= tensor(4.3443e+09, device='cuda:0')
c= tensor(4.3443e+09, device='cuda:0')
c= tensor(4.3444e+09, device='cuda:0')
c= tensor(4.3636e+09, device='cuda:0')
c= tensor(4.3642e+09, device='cuda:0')
c= tensor(4.3642e+09, device='cuda:0')
c= tensor(4.3647e+09, device='cuda:0')
c= tensor(4.4167e+09, device='cuda:0')
c= tensor(4.4173e+09, device='cuda:0')
c= tensor(4.4207e+09, device='cuda:0')
c= tensor(4.4207e+09, device='cuda:0')
c= tensor(4.4208e+09, device='cuda:0')
c= tensor(4.4209e+09, device='cuda:0')
c= tensor(4.4230e+09, device='cuda:0')
c= tensor(4.4237e+09, device='cuda:0')
c= tensor(4.4273e+09, device='cuda:0')
c= tensor(4.4273e+09, device='cuda:0')
c= tensor(4.4356e+09, device='cuda:0')
c= tensor(4.4357e+09, device='cuda:0')
c= tensor(4.4372e+09, device='cuda:0')
c= tensor(4.4376e+09, device='cuda:0')
c= tensor(4.4406e+09, device='cuda:0')
c= tensor(4.4407e+09, device='cuda:0')
c= tensor(4.4414e+09, device='cuda:0')
c= tensor(4.4418e+09, device='cuda:0')
c= tensor(4.4423e+09, device='cuda:0')
c= tensor(4.4447e+09, device='cuda:0')
c= tensor(4.5703e+09, device='cuda:0')
c= tensor(4.5703e+09, device='cuda:0')
c= tensor(4.5704e+09, device='cuda:0')
c= tensor(4.5780e+09, device='cuda:0')
c= tensor(4.5783e+09, device='cuda:0')
c= tensor(4.6715e+09, device='cuda:0')
c= tensor(4.6715e+09, device='cuda:0')
c= tensor(4.6780e+09, device='cuda:0')
c= tensor(4.7036e+09, device='cuda:0')
c= tensor(4.7036e+09, device='cuda:0')
c= tensor(4.7306e+09, device='cuda:0')
c= tensor(4.7313e+09, device='cuda:0')
c= tensor(4.7986e+09, device='cuda:0')
c= tensor(4.7987e+09, device='cuda:0')
c= tensor(4.7997e+09, device='cuda:0')
c= tensor(4.7997e+09, device='cuda:0')
c= tensor(4.7997e+09, device='cuda:0')
c= tensor(4.7998e+09, device='cuda:0')
c= tensor(4.8026e+09, device='cuda:0')
c= tensor(4.8033e+09, device='cuda:0')
c= tensor(4.8148e+09, device='cuda:0')
c= tensor(4.8150e+09, device='cuda:0')
c= tensor(4.8150e+09, device='cuda:0')
c= tensor(4.8152e+09, device='cuda:0')
c= tensor(4.8258e+09, device='cuda:0')
c= tensor(4.8280e+09, device='cuda:0')
c= tensor(4.8454e+09, device='cuda:0')
c= tensor(4.8484e+09, device='cuda:0')
c= tensor(4.8485e+09, device='cuda:0')
c= tensor(4.8485e+09, device='cuda:0')
c= tensor(4.8485e+09, device='cuda:0')
c= tensor(5.0200e+09, device='cuda:0')
c= tensor(5.0200e+09, device='cuda:0')
c= tensor(5.0203e+09, device='cuda:0')
c= tensor(5.0277e+09, device='cuda:0')
c= tensor(5.0292e+09, device='cuda:0')
c= tensor(5.0292e+09, device='cuda:0')
c= tensor(5.0292e+09, device='cuda:0')
c= tensor(5.0595e+09, device='cuda:0')
c= tensor(5.0620e+09, device='cuda:0')
c= tensor(5.0641e+09, device='cuda:0')
c= tensor(5.0643e+09, device='cuda:0')
c= tensor(5.0805e+09, device='cuda:0')
c= tensor(5.0834e+09, device='cuda:0')
c= tensor(5.1153e+09, device='cuda:0')
c= tensor(5.1196e+09, device='cuda:0')
c= tensor(5.1196e+09, device='cuda:0')
c= tensor(5.1235e+09, device='cuda:0')
c= tensor(5.1241e+09, device='cuda:0')
c= tensor(5.1249e+09, device='cuda:0')
c= tensor(5.1252e+09, device='cuda:0')
c= tensor(5.1252e+09, device='cuda:0')
c= tensor(5.1268e+09, device='cuda:0')
c= tensor(5.1272e+09, device='cuda:0')
c= tensor(5.1273e+09, device='cuda:0')
c= tensor(5.1275e+09, device='cuda:0')
c= tensor(5.1278e+09, device='cuda:0')
c= tensor(5.1506e+09, device='cuda:0')
c= tensor(5.1506e+09, device='cuda:0')
c= tensor(5.1508e+09, device='cuda:0')
c= tensor(5.1512e+09, device='cuda:0')
c= tensor(5.1518e+09, device='cuda:0')
c= tensor(5.1518e+09, device='cuda:0')
c= tensor(5.1519e+09, device='cuda:0')
c= tensor(5.1522e+09, device='cuda:0')
c= tensor(5.1643e+09, device='cuda:0')
c= tensor(5.1643e+09, device='cuda:0')
c= tensor(5.1643e+09, device='cuda:0')
c= tensor(5.1644e+09, device='cuda:0')
c= tensor(5.1966e+09, device='cuda:0')
c= tensor(5.5289e+09, device='cuda:0')
c= tensor(5.5295e+09, device='cuda:0')
c= tensor(5.5295e+09, device='cuda:0')
c= tensor(5.5396e+09, device='cuda:0')
c= tensor(5.5454e+09, device='cuda:0')
c= tensor(5.5454e+09, device='cuda:0')
c= tensor(5.5457e+09, device='cuda:0')
c= tensor(5.5462e+09, device='cuda:0')
c= tensor(5.6025e+09, device='cuda:0')
c= tensor(5.8719e+09, device='cuda:0')
c= tensor(5.8838e+09, device='cuda:0')
c= tensor(5.8842e+09, device='cuda:0')
c= tensor(5.8842e+09, device='cuda:0')
c= tensor(5.8842e+09, device='cuda:0')
c= tensor(5.8903e+09, device='cuda:0')
c= tensor(5.8904e+09, device='cuda:0')
c= tensor(5.8926e+09, device='cuda:0')
c= tensor(5.9061e+09, device='cuda:0')
c= tensor(5.9733e+09, device='cuda:0')
c= tensor(5.9733e+09, device='cuda:0')
c= tensor(5.9733e+09, device='cuda:0')
c= tensor(5.9743e+09, device='cuda:0')
c= tensor(6.0083e+09, device='cuda:0')
c= tensor(6.0094e+09, device='cuda:0')
c= tensor(6.0095e+09, device='cuda:0')
c= tensor(6.0095e+09, device='cuda:0')
c= tensor(6.0099e+09, device='cuda:0')
c= tensor(6.0100e+09, device='cuda:0')
c= tensor(6.0121e+09, device='cuda:0')
c= tensor(6.0122e+09, device='cuda:0')
c= tensor(6.0122e+09, device='cuda:0')
c= tensor(6.0124e+09, device='cuda:0')
c= tensor(6.0124e+09, device='cuda:0')
c= tensor(6.0124e+09, device='cuda:0')
c= tensor(6.0219e+09, device='cuda:0')
c= tensor(6.0769e+09, device='cuda:0')
c= tensor(6.0845e+09, device='cuda:0')
c= tensor(6.0901e+09, device='cuda:0')
c= tensor(6.0904e+09, device='cuda:0')
c= tensor(6.0905e+09, device='cuda:0')
c= tensor(6.0907e+09, device='cuda:0')
c= tensor(6.0982e+09, device='cuda:0')
c= tensor(6.0984e+09, device='cuda:0')
c= tensor(6.1049e+09, device='cuda:0')
c= tensor(6.1051e+09, device='cuda:0')
c= tensor(6.4980e+09, device='cuda:0')
c= tensor(6.4982e+09, device='cuda:0')
c= tensor(6.5002e+09, device='cuda:0')
c= tensor(6.5379e+09, device='cuda:0')
c= tensor(6.5398e+09, device='cuda:0')
c= tensor(6.5404e+09, device='cuda:0')
c= tensor(6.7506e+09, device='cuda:0')
c= tensor(6.7585e+09, device='cuda:0')
c= tensor(6.7598e+09, device='cuda:0')
c= tensor(6.7599e+09, device='cuda:0')
c= tensor(6.7606e+09, device='cuda:0')
c= tensor(6.7608e+09, device='cuda:0')
c= tensor(6.7745e+09, device='cuda:0')
c= tensor(6.9528e+09, device='cuda:0')
c= tensor(6.9671e+09, device='cuda:0')
c= tensor(6.9785e+09, device='cuda:0')
c= tensor(6.9802e+09, device='cuda:0')
c= tensor(6.9808e+09, device='cuda:0')
c= tensor(6.9813e+09, device='cuda:0')
c= tensor(6.9817e+09, device='cuda:0')
c= tensor(7.0994e+09, device='cuda:0')
c= tensor(7.1457e+09, device='cuda:0')
c= tensor(7.1471e+09, device='cuda:0')
c= tensor(8.3769e+09, device='cuda:0')
c= tensor(8.3968e+09, device='cuda:0')
c= tensor(8.3981e+09, device='cuda:0')
c= tensor(8.3981e+09, device='cuda:0')
c= tensor(8.4001e+09, device='cuda:0')
c= tensor(8.4075e+09, device='cuda:0')
c= tensor(8.4075e+09, device='cuda:0')
c= tensor(8.5576e+09, device='cuda:0')
c= tensor(8.5585e+09, device='cuda:0')
c= tensor(8.5592e+09, device='cuda:0')
c= tensor(8.5593e+09, device='cuda:0')
c= tensor(8.5594e+09, device='cuda:0')
c= tensor(8.5594e+09, device='cuda:0')
c= tensor(8.5596e+09, device='cuda:0')
c= tensor(8.5600e+09, device='cuda:0')
c= tensor(8.5616e+09, device='cuda:0')
c= tensor(1.0513e+10, device='cuda:0')
c= tensor(1.0516e+10, device='cuda:0')
c= tensor(1.0529e+10, device='cuda:0')
c= tensor(1.0529e+10, device='cuda:0')
c= tensor(1.0529e+10, device='cuda:0')
c= tensor(1.0529e+10, device='cuda:0')
c= tensor(1.0552e+10, device='cuda:0')
c= tensor(1.0561e+10, device='cuda:0')
c= tensor(1.0994e+10, device='cuda:0')
c= tensor(1.0994e+10, device='cuda:0')
c= tensor(1.1006e+10, device='cuda:0')
c= tensor(1.1007e+10, device='cuda:0')
c= tensor(1.1016e+10, device='cuda:0')
c= tensor(1.1042e+10, device='cuda:0')
c= tensor(1.1043e+10, device='cuda:0')
c= tensor(1.1043e+10, device='cuda:0')
c= tensor(1.1046e+10, device='cuda:0')
c= tensor(1.1049e+10, device='cuda:0')
c= tensor(1.1053e+10, device='cuda:0')
c= tensor(1.1099e+10, device='cuda:0')
c= tensor(1.1100e+10, device='cuda:0')
c= tensor(1.1103e+10, device='cuda:0')
c= tensor(1.1103e+10, device='cuda:0')
c= tensor(1.1112e+10, device='cuda:0')
c= tensor(1.1137e+10, device='cuda:0')
c= tensor(1.1138e+10, device='cuda:0')
c= tensor(1.1138e+10, device='cuda:0')
c= tensor(1.1183e+10, device='cuda:0')
c= tensor(1.1187e+10, device='cuda:0')
c= tensor(1.1475e+10, device='cuda:0')
c= tensor(1.1478e+10, device='cuda:0')
c= tensor(1.1482e+10, device='cuda:0')
c= tensor(1.1483e+10, device='cuda:0')
c= tensor(1.1500e+10, device='cuda:0')
c= tensor(1.1536e+10, device='cuda:0')
c= tensor(1.1537e+10, device='cuda:0')
c= tensor(1.1537e+10, device='cuda:0')
c= tensor(1.1537e+10, device='cuda:0')
c= tensor(1.1540e+10, device='cuda:0')
c= tensor(1.1548e+10, device='cuda:0')
c= tensor(1.1550e+10, device='cuda:0')
c= tensor(1.1550e+10, device='cuda:0')
c= tensor(1.1550e+10, device='cuda:0')
c= tensor(1.1556e+10, device='cuda:0')
c= tensor(1.1558e+10, device='cuda:0')
c= tensor(1.1559e+10, device='cuda:0')
c= tensor(1.1561e+10, device='cuda:0')
c= tensor(1.1563e+10, device='cuda:0')
c= tensor(1.1563e+10, device='cuda:0')
c= tensor(1.1563e+10, device='cuda:0')
c= tensor(1.1563e+10, device='cuda:0')
c= tensor(1.1573e+10, device='cuda:0')
c= tensor(1.1574e+10, device='cuda:0')
c= tensor(1.1576e+10, device='cuda:0')
c= tensor(1.1576e+10, device='cuda:0')
c= tensor(1.1576e+10, device='cuda:0')
c= tensor(1.1586e+10, device='cuda:0')
c= tensor(1.1589e+10, device='cuda:0')
c= tensor(1.1589e+10, device='cuda:0')
c= tensor(1.1589e+10, device='cuda:0')
c= tensor(1.1589e+10, device='cuda:0')
c= tensor(1.1590e+10, device='cuda:0')
c= tensor(1.1590e+10, device='cuda:0')
c= tensor(1.1600e+10, device='cuda:0')
c= tensor(1.1600e+10, device='cuda:0')
c= tensor(1.1601e+10, device='cuda:0')
c= tensor(1.1601e+10, device='cuda:0')
c= tensor(1.1602e+10, device='cuda:0')
c= tensor(1.1629e+10, device='cuda:0')
c= tensor(1.1630e+10, device='cuda:0')
c= tensor(1.1642e+10, device='cuda:0')
c= tensor(1.1654e+10, device='cuda:0')
c= tensor(1.1654e+10, device='cuda:0')
c= tensor(1.1659e+10, device='cuda:0')
c= tensor(1.1664e+10, device='cuda:0')
c= tensor(1.1664e+10, device='cuda:0')
c= tensor(1.1665e+10, device='cuda:0')
c= tensor(1.1665e+10, device='cuda:0')
c= tensor(1.1690e+10, device='cuda:0')
c= tensor(1.1696e+10, device='cuda:0')
c= tensor(1.1698e+10, device='cuda:0')
c= tensor(1.1703e+10, device='cuda:0')
c= tensor(1.1703e+10, device='cuda:0')
c= tensor(1.1706e+10, device='cuda:0')
c= tensor(1.1706e+10, device='cuda:0')
c= tensor(1.1706e+10, device='cuda:0')
c= tensor(1.1778e+10, device='cuda:0')
c= tensor(1.1970e+10, device='cuda:0')
c= tensor(1.1972e+10, device='cuda:0')
c= tensor(1.1972e+10, device='cuda:0')
c= tensor(1.1973e+10, device='cuda:0')
c= tensor(1.2058e+10, device='cuda:0')
c= tensor(1.2059e+10, device='cuda:0')
c= tensor(1.2059e+10, device='cuda:0')
c= tensor(1.2060e+10, device='cuda:0')
c= tensor(1.2065e+10, device='cuda:0')
c= tensor(1.2065e+10, device='cuda:0')
c= tensor(1.2066e+10, device='cuda:0')
c= tensor(1.2068e+10, device='cuda:0')
memory (bytes)
6354984960
time for making loss 2 is 9.32244610786438
p0 True
it  0 : 3944014336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 70% |
shape of L is 
torch.Size([])
memory (bytes)
6355120128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 41% | 44% |
memory (bytes)
6355734528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  172996590000.0
relative error loss 14.334927
shape of L is 
torch.Size([])
memory (bytes)
6426226688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 45% |
memory (bytes)
6426230784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  172996120000.0
relative error loss 14.3348875
shape of L is 
torch.Size([])
memory (bytes)
6428368896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 45% |
memory (bytes)
6428504064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  172993950000.0
relative error loss 14.334708
shape of L is 
torch.Size([])
memory (bytes)
6429544448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
memory (bytes)
6429544448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  172982080000.0
relative error loss 14.333724
shape of L is 
torch.Size([])
memory (bytes)
6430638080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 45% |
memory (bytes)
6430638080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  172916470000.0
relative error loss 14.328288
shape of L is 
torch.Size([])
memory (bytes)
6431764480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
memory (bytes)
6431764480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 45% |
error is  172555600000.0
relative error loss 14.298385
shape of L is 
torch.Size([])
memory (bytes)
6432833536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6432833536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  168629710000.0
relative error loss 13.973077
shape of L is 
torch.Size([])
memory (bytes)
6433894400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6433894400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  150081620000.0
relative error loss 12.436135
shape of L is 
torch.Size([])
memory (bytes)
6435872768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 45% |
memory (bytes)
6435872768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  40856916000.0
relative error loss 3.3855054
shape of L is 
torch.Size([])
memory (bytes)
6438019072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6438023168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  22733201000.0
relative error loss 1.8837295
time to take a step is 178.29859900474548
it  1 : 4486263808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6440148992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 45% |
memory (bytes)
6440148992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  22733201000.0
relative error loss 1.8837295
shape of L is 
torch.Size([])
memory (bytes)
6442266624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
memory (bytes)
6442266624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  32095121000.0
relative error loss 2.6594813
shape of L is 
torch.Size([])
memory (bytes)
6444441600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 45% |
memory (bytes)
6444441600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  17317407000.0
relative error loss 1.4349632
shape of L is 
torch.Size([])
memory (bytes)
6446571520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
memory (bytes)
6446571520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  13510903000.0
relative error loss 1.1195469
shape of L is 
torch.Size([])
memory (bytes)
6448697344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 45% |
memory (bytes)
6448697344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  12489617000.0
relative error loss 1.0349207
shape of L is 
torch.Size([])
memory (bytes)
6450774016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 45% |
memory (bytes)
6450774016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  12336176000.0
relative error loss 1.0222062
shape of L is 
torch.Size([])
memory (bytes)
6452916224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
memory (bytes)
6452916224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  12048650000.0
relative error loss 0.998381
shape of L is 
torch.Size([])
memory (bytes)
6455025664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
memory (bytes)
6455025664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  11919483000.0
relative error loss 0.98767793
shape of L is 
torch.Size([])
memory (bytes)
6457098240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
memory (bytes)
6457102336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% | 45% |
error is  11741026000.0
relative error loss 0.97289056
shape of L is 
torch.Size([])
memory (bytes)
6459232256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 45% |
memory (bytes)
6459232256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  11408990000.0
relative error loss 0.94537723
time to take a step is 177.21046924591064
it  2 : 4641148928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6461345792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 45% |
memory (bytes)
6461345792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  11408990000.0
relative error loss 0.94537723
shape of L is 
torch.Size([])
memory (bytes)
6463467520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6463467520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  11160883000.0
relative error loss 0.92481846
shape of L is 
torch.Size([])
memory (bytes)
6465585152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6465585152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  12412558000.0
relative error loss 1.0285354
shape of L is 
torch.Size([])
memory (bytes)
6467698688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 45% |
memory (bytes)
6467698688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  10908535000.0
relative error loss 0.90390825
shape of L is 
torch.Size([])
memory (bytes)
6469820416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6469820416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  10527229000.0
relative error loss 0.8723123
shape of L is 
torch.Size([])
memory (bytes)
6471942144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6471942144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  9823855000.0
relative error loss 0.814029
shape of L is 
torch.Size([])
memory (bytes)
6473854976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
memory (bytes)
6474059776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  9120231000.0
relative error loss 0.75572497
shape of L is 
torch.Size([])
memory (bytes)
6476189696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6476193792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  8608159000.0
relative error loss 0.7132934
shape of L is 
torch.Size([])
memory (bytes)
6478327808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 45% |
memory (bytes)
6478327808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  8036343000.0
relative error loss 0.6659113
shape of L is 
torch.Size([])
memory (bytes)
6480470016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6480470016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  7743894000.0
relative error loss 0.6416783
time to take a step is 170.09122681617737
it  3 : 4640996352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6482456576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
memory (bytes)
6482608128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 45% |
error is  7743894000.0
relative error loss 0.6416783
shape of L is 
torch.Size([])
memory (bytes)
6484516864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
memory (bytes)
6484729856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  7069475300.0
relative error loss 0.58579427
shape of L is 
torch.Size([])
memory (bytes)
6486855680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6486855680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  6576674000.0
relative error loss 0.5449595
shape of L is 
torch.Size([])
memory (bytes)
6488784896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
memory (bytes)
6488784896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 45% |
error is  6070141400.0
relative error loss 0.50298697
shape of L is 
torch.Size([])
memory (bytes)
6491107328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6491107328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  5541605000.0
relative error loss 0.4591911
shape of L is 
torch.Size([])
memory (bytes)
6493253632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 45% |
memory (bytes)
6493257728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  5215255600.0
relative error loss 0.43214902
shape of L is 
torch.Size([])
memory (bytes)
6495203328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6495379456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  4776613400.0
relative error loss 0.39580202
shape of L is 
torch.Size([])
memory (bytes)
6497378304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6497525760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  4239509000.0
relative error loss 0.35129622
shape of L is 
torch.Size([])
memory (bytes)
6499651584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 45% |
memory (bytes)
6499655680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3852576800.0
relative error loss 0.31923407
shape of L is 
torch.Size([])
memory (bytes)
6501691392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 45% |
memory (bytes)
6501691392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3515510300.0
relative error loss 0.2913039
time to take a step is 171.3365023136139
c= tensor(3138.6401, device='cuda:0')
c= tensor(294290.4062, device='cuda:0')
c= tensor(302727.8750, device='cuda:0')
c= tensor(419534.6250, device='cuda:0')
c= tensor(1878808.5000, device='cuda:0')
c= tensor(2702185.2500, device='cuda:0')
c= tensor(4186347.7500, device='cuda:0')
c= tensor(5178750., device='cuda:0')
c= tensor(5760474., device='cuda:0')
c= tensor(23928566., device='cuda:0')
c= tensor(24206828., device='cuda:0')
c= tensor(32934284., device='cuda:0')
c= tensor(32960936., device='cuda:0')
c= tensor(69932832., device='cuda:0')
c= tensor(70333176., device='cuda:0')
c= tensor(71133792., device='cuda:0')
c= tensor(72523512., device='cuda:0')
c= tensor(74090256., device='cuda:0')
c= tensor(84474720., device='cuda:0')
c= tensor(90697888., device='cuda:0')
c= tensor(91522008., device='cuda:0')
c= tensor(1.0901e+08, device='cuda:0')
c= tensor(1.0910e+08, device='cuda:0')
c= tensor(1.1021e+08, device='cuda:0')
c= tensor(1.1282e+08, device='cuda:0')
c= tensor(1.1480e+08, device='cuda:0')
c= tensor(1.1835e+08, device='cuda:0')
c= tensor(1.1884e+08, device='cuda:0')
c= tensor(1.4849e+08, device='cuda:0')
c= tensor(6.8855e+08, device='cuda:0')
c= tensor(6.8882e+08, device='cuda:0')
c= tensor(1.2026e+09, device='cuda:0')
c= tensor(1.2029e+09, device='cuda:0')
c= tensor(1.2032e+09, device='cuda:0')
c= tensor(1.2034e+09, device='cuda:0')
c= tensor(1.2687e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2718e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2719e+09, device='cuda:0')
c= tensor(1.2720e+09, device='cuda:0')
c= tensor(1.2721e+09, device='cuda:0')
c= tensor(1.2721e+09, device='cuda:0')
c= tensor(1.2721e+09, device='cuda:0')
c= tensor(1.2721e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2723e+09, device='cuda:0')
c= tensor(1.2723e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.2725e+09, device='cuda:0')
c= tensor(1.2725e+09, device='cuda:0')
c= tensor(1.2725e+09, device='cuda:0')
c= tensor(1.2725e+09, device='cuda:0')
c= tensor(1.2726e+09, device='cuda:0')
c= tensor(1.2726e+09, device='cuda:0')
c= tensor(1.2726e+09, device='cuda:0')
c= tensor(1.2726e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2729e+09, device='cuda:0')
c= tensor(1.2730e+09, device='cuda:0')
c= tensor(1.2730e+09, device='cuda:0')
c= tensor(1.2730e+09, device='cuda:0')
c= tensor(1.2731e+09, device='cuda:0')
c= tensor(1.2731e+09, device='cuda:0')
c= tensor(1.2731e+09, device='cuda:0')
c= tensor(1.2731e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2733e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2734e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2735e+09, device='cuda:0')
c= tensor(1.2736e+09, device='cuda:0')
c= tensor(1.2736e+09, device='cuda:0')
c= tensor(1.2736e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2738e+09, device='cuda:0')
c= tensor(1.2738e+09, device='cuda:0')
c= tensor(1.2738e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2739e+09, device='cuda:0')
c= tensor(1.2740e+09, device='cuda:0')
c= tensor(1.2740e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2741e+09, device='cuda:0')
c= tensor(1.2748e+09, device='cuda:0')
c= tensor(1.2748e+09, device='cuda:0')
c= tensor(1.2748e+09, device='cuda:0')
c= tensor(1.2748e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2749e+09, device='cuda:0')
c= tensor(1.2750e+09, device='cuda:0')
c= tensor(1.2750e+09, device='cuda:0')
c= tensor(1.2750e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2754e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2755e+09, device='cuda:0')
c= tensor(1.2756e+09, device='cuda:0')
c= tensor(1.2756e+09, device='cuda:0')
c= tensor(1.2756e+09, device='cuda:0')
c= tensor(1.2756e+09, device='cuda:0')
c= tensor(1.2756e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2757e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2759e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2760e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2762e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2763e+09, device='cuda:0')
c= tensor(1.2764e+09, device='cuda:0')
c= tensor(1.2764e+09, device='cuda:0')
c= tensor(1.2764e+09, device='cuda:0')
c= tensor(1.2766e+09, device='cuda:0')
c= tensor(1.2794e+09, device='cuda:0')
c= tensor(1.2795e+09, device='cuda:0')
c= tensor(1.2796e+09, device='cuda:0')
c= tensor(1.2796e+09, device='cuda:0')
c= tensor(1.2797e+09, device='cuda:0')
c= tensor(1.2799e+09, device='cuda:0')
c= tensor(1.2912e+09, device='cuda:0')
c= tensor(1.2912e+09, device='cuda:0')
c= tensor(1.3021e+09, device='cuda:0')
c= tensor(1.3120e+09, device='cuda:0')
c= tensor(1.3131e+09, device='cuda:0')
c= tensor(1.3331e+09, device='cuda:0')
c= tensor(1.3331e+09, device='cuda:0')
c= tensor(1.3332e+09, device='cuda:0')
c= tensor(1.3838e+09, device='cuda:0')
c= tensor(1.4733e+09, device='cuda:0')
c= tensor(1.4733e+09, device='cuda:0')
c= tensor(1.4738e+09, device='cuda:0')
c= tensor(1.4773e+09, device='cuda:0')
c= tensor(1.4779e+09, device='cuda:0')
c= tensor(1.4809e+09, device='cuda:0')
c= tensor(1.4830e+09, device='cuda:0')
c= tensor(1.4835e+09, device='cuda:0')
c= tensor(1.4837e+09, device='cuda:0')
c= tensor(1.4838e+09, device='cuda:0')
c= tensor(1.5058e+09, device='cuda:0')
c= tensor(1.5059e+09, device='cuda:0')
c= tensor(1.5059e+09, device='cuda:0')
c= tensor(1.5066e+09, device='cuda:0')
c= tensor(1.5080e+09, device='cuda:0')
c= tensor(1.5505e+09, device='cuda:0')
c= tensor(1.5530e+09, device='cuda:0')
c= tensor(1.5530e+09, device='cuda:0')
c= tensor(1.5537e+09, device='cuda:0')
c= tensor(1.5537e+09, device='cuda:0')
c= tensor(1.5545e+09, device='cuda:0')
c= tensor(1.5571e+09, device='cuda:0')
c= tensor(1.5651e+09, device='cuda:0')
c= tensor(1.5688e+09, device='cuda:0')
c= tensor(1.5688e+09, device='cuda:0')
c= tensor(1.5688e+09, device='cuda:0')
c= tensor(1.5721e+09, device='cuda:0')
c= tensor(1.5759e+09, device='cuda:0')
c= tensor(1.5778e+09, device='cuda:0')
c= tensor(1.5779e+09, device='cuda:0')
c= tensor(1.6248e+09, device='cuda:0')
c= tensor(1.6249e+09, device='cuda:0')
c= tensor(1.6254e+09, device='cuda:0')
c= tensor(1.6292e+09, device='cuda:0')
c= tensor(1.6292e+09, device='cuda:0')
c= tensor(1.6319e+09, device='cuda:0')
c= tensor(1.6523e+09, device='cuda:0')
c= tensor(1.9814e+09, device='cuda:0')
c= tensor(1.9818e+09, device='cuda:0')
c= tensor(1.9823e+09, device='cuda:0')
c= tensor(1.9824e+09, device='cuda:0')
c= tensor(1.9824e+09, device='cuda:0')
c= tensor(1.9975e+09, device='cuda:0')
c= tensor(1.9976e+09, device='cuda:0')
c= tensor(1.9986e+09, device='cuda:0')
c= tensor(2.0111e+09, device='cuda:0')
c= tensor(2.0134e+09, device='cuda:0')
c= tensor(2.0137e+09, device='cuda:0')
c= tensor(2.0137e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0503e+09, device='cuda:0')
c= tensor(2.0515e+09, device='cuda:0')
c= tensor(2.0515e+09, device='cuda:0')
c= tensor(2.0668e+09, device='cuda:0')
c= tensor(2.0669e+09, device='cuda:0')
c= tensor(2.0762e+09, device='cuda:0')
c= tensor(2.0768e+09, device='cuda:0')
c= tensor(2.0789e+09, device='cuda:0')
c= tensor(2.0798e+09, device='cuda:0')
c= tensor(2.1378e+09, device='cuda:0')
c= tensor(2.1403e+09, device='cuda:0')
c= tensor(2.1404e+09, device='cuda:0')
c= tensor(2.1486e+09, device='cuda:0')
c= tensor(2.1571e+09, device='cuda:0')
c= tensor(2.1571e+09, device='cuda:0')
c= tensor(2.1650e+09, device='cuda:0')
c= tensor(2.1800e+09, device='cuda:0')
c= tensor(2.2845e+09, device='cuda:0')
c= tensor(2.2850e+09, device='cuda:0')
c= tensor(2.2851e+09, device='cuda:0')
c= tensor(2.2852e+09, device='cuda:0')
c= tensor(2.2858e+09, device='cuda:0')
c= tensor(2.2864e+09, device='cuda:0')
c= tensor(2.2873e+09, device='cuda:0')
c= tensor(2.2873e+09, device='cuda:0')
c= tensor(2.2891e+09, device='cuda:0')
c= tensor(2.2982e+09, device='cuda:0')
c= tensor(2.3181e+09, device='cuda:0')
c= tensor(2.3181e+09, device='cuda:0')
c= tensor(2.3223e+09, device='cuda:0')
c= tensor(2.3224e+09, device='cuda:0')
c= tensor(2.3227e+09, device='cuda:0')
c= tensor(2.3228e+09, device='cuda:0')
c= tensor(2.3228e+09, device='cuda:0')
c= tensor(2.3689e+09, device='cuda:0')
c= tensor(2.3699e+09, device='cuda:0')
c= tensor(2.3701e+09, device='cuda:0')
c= tensor(2.3721e+09, device='cuda:0')
c= tensor(2.3721e+09, device='cuda:0')
c= tensor(2.6245e+09, device='cuda:0')
c= tensor(2.6246e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6332e+09, device='cuda:0')
c= tensor(2.6332e+09, device='cuda:0')
c= tensor(2.6352e+09, device='cuda:0')
c= tensor(2.6352e+09, device='cuda:0')
c= tensor(2.6352e+09, device='cuda:0')
c= tensor(2.6587e+09, device='cuda:0')
c= tensor(2.6603e+09, device='cuda:0')
c= tensor(2.6611e+09, device='cuda:0')
c= tensor(2.6664e+09, device='cuda:0')
c= tensor(2.6990e+09, device='cuda:0')
c= tensor(2.6990e+09, device='cuda:0')
c= tensor(2.6991e+09, device='cuda:0')
c= tensor(2.6995e+09, device='cuda:0')
c= tensor(2.6995e+09, device='cuda:0')
c= tensor(2.6995e+09, device='cuda:0')
c= tensor(2.6997e+09, device='cuda:0')
c= tensor(2.6998e+09, device='cuda:0')
c= tensor(2.6998e+09, device='cuda:0')
c= tensor(2.6999e+09, device='cuda:0')
c= tensor(2.7000e+09, device='cuda:0')
c= tensor(2.9194e+09, device='cuda:0')
c= tensor(2.9195e+09, device='cuda:0')
c= tensor(2.9228e+09, device='cuda:0')
c= tensor(2.9233e+09, device='cuda:0')
c= tensor(2.9233e+09, device='cuda:0')
c= tensor(2.9259e+09, device='cuda:0')
c= tensor(3.0246e+09, device='cuda:0')
c= tensor(3.1226e+09, device='cuda:0')
c= tensor(3.1258e+09, device='cuda:0')
c= tensor(3.1269e+09, device='cuda:0')
c= tensor(3.1269e+09, device='cuda:0')
c= tensor(3.1324e+09, device='cuda:0')
c= tensor(3.9370e+09, device='cuda:0')
c= tensor(3.9408e+09, device='cuda:0')
c= tensor(3.9409e+09, device='cuda:0')
c= tensor(3.9457e+09, device='cuda:0')
c= tensor(4.0325e+09, device='cuda:0')
c= tensor(4.0347e+09, device='cuda:0')
c= tensor(4.0348e+09, device='cuda:0')
c= tensor(4.0349e+09, device='cuda:0')
c= tensor(4.0349e+09, device='cuda:0')
c= tensor(4.0350e+09, device='cuda:0')
c= tensor(4.0812e+09, device='cuda:0')
c= tensor(4.0815e+09, device='cuda:0')
c= tensor(4.0815e+09, device='cuda:0')
c= tensor(4.0833e+09, device='cuda:0')
c= tensor(4.0835e+09, device='cuda:0')
c= tensor(4.0835e+09, device='cuda:0')
c= tensor(4.0876e+09, device='cuda:0')
c= tensor(4.0915e+09, device='cuda:0')
c= tensor(4.1054e+09, device='cuda:0')
c= tensor(4.1193e+09, device='cuda:0')
c= tensor(4.1347e+09, device='cuda:0')
c= tensor(4.1351e+09, device='cuda:0')
c= tensor(4.1365e+09, device='cuda:0')
c= tensor(4.1406e+09, device='cuda:0')
c= tensor(4.1673e+09, device='cuda:0')
c= tensor(4.1673e+09, device='cuda:0')
c= tensor(4.1981e+09, device='cuda:0')
c= tensor(4.2307e+09, device='cuda:0')
c= tensor(4.2407e+09, device='cuda:0')
c= tensor(4.2444e+09, device='cuda:0')
c= tensor(4.2486e+09, device='cuda:0')
c= tensor(4.2488e+09, device='cuda:0')
c= tensor(4.2488e+09, device='cuda:0')
c= tensor(4.2530e+09, device='cuda:0')
c= tensor(4.2639e+09, device='cuda:0')
c= tensor(4.2706e+09, device='cuda:0')
c= tensor(4.3001e+09, device='cuda:0')
c= tensor(4.3094e+09, device='cuda:0')
c= tensor(4.3140e+09, device='cuda:0')
c= tensor(4.3145e+09, device='cuda:0')
c= tensor(4.3443e+09, device='cuda:0')
c= tensor(4.3443e+09, device='cuda:0')
c= tensor(4.3444e+09, device='cuda:0')
c= tensor(4.3636e+09, device='cuda:0')
c= tensor(4.3642e+09, device='cuda:0')
c= tensor(4.3642e+09, device='cuda:0')
c= tensor(4.3647e+09, device='cuda:0')
c= tensor(4.4167e+09, device='cuda:0')
c= tensor(4.4173e+09, device='cuda:0')
c= tensor(4.4207e+09, device='cuda:0')
c= tensor(4.4207e+09, device='cuda:0')
c= tensor(4.4208e+09, device='cuda:0')
c= tensor(4.4209e+09, device='cuda:0')
c= tensor(4.4230e+09, device='cuda:0')
c= tensor(4.4237e+09, device='cuda:0')
c= tensor(4.4273e+09, device='cuda:0')
c= tensor(4.4273e+09, device='cuda:0')
c= tensor(4.4356e+09, device='cuda:0')
c= tensor(4.4357e+09, device='cuda:0')
c= tensor(4.4372e+09, device='cuda:0')
c= tensor(4.4376e+09, device='cuda:0')
c= tensor(4.4406e+09, device='cuda:0')
c= tensor(4.4407e+09, device='cuda:0')
c= tensor(4.4414e+09, device='cuda:0')
c= tensor(4.4418e+09, device='cuda:0')
c= tensor(4.4423e+09, device='cuda:0')
c= tensor(4.4447e+09, device='cuda:0')
c= tensor(4.5703e+09, device='cuda:0')
c= tensor(4.5703e+09, device='cuda:0')
c= tensor(4.5704e+09, device='cuda:0')
c= tensor(4.5780e+09, device='cuda:0')
c= tensor(4.5783e+09, device='cuda:0')
c= tensor(4.6715e+09, device='cuda:0')
c= tensor(4.6715e+09, device='cuda:0')
c= tensor(4.6780e+09, device='cuda:0')
c= tensor(4.7036e+09, device='cuda:0')
c= tensor(4.7036e+09, device='cuda:0')
c= tensor(4.7306e+09, device='cuda:0')
c= tensor(4.7313e+09, device='cuda:0')
c= tensor(4.7986e+09, device='cuda:0')
c= tensor(4.7987e+09, device='cuda:0')
c= tensor(4.7997e+09, device='cuda:0')
c= tensor(4.7997e+09, device='cuda:0')
c= tensor(4.7997e+09, device='cuda:0')
c= tensor(4.7998e+09, device='cuda:0')
c= tensor(4.8026e+09, device='cuda:0')
c= tensor(4.8033e+09, device='cuda:0')
c= tensor(4.8148e+09, device='cuda:0')
c= tensor(4.8150e+09, device='cuda:0')
c= tensor(4.8150e+09, device='cuda:0')
c= tensor(4.8152e+09, device='cuda:0')
c= tensor(4.8258e+09, device='cuda:0')
c= tensor(4.8280e+09, device='cuda:0')
c= tensor(4.8454e+09, device='cuda:0')
c= tensor(4.8484e+09, device='cuda:0')
c= tensor(4.8485e+09, device='cuda:0')
c= tensor(4.8485e+09, device='cuda:0')
c= tensor(4.8485e+09, device='cuda:0')
c= tensor(5.0200e+09, device='cuda:0')
c= tensor(5.0200e+09, device='cuda:0')
c= tensor(5.0203e+09, device='cuda:0')
c= tensor(5.0277e+09, device='cuda:0')
c= tensor(5.0292e+09, device='cuda:0')
c= tensor(5.0292e+09, device='cuda:0')
c= tensor(5.0292e+09, device='cuda:0')
c= tensor(5.0595e+09, device='cuda:0')
c= tensor(5.0620e+09, device='cuda:0')
c= tensor(5.0641e+09, device='cuda:0')
c= tensor(5.0643e+09, device='cuda:0')
c= tensor(5.0805e+09, device='cuda:0')
c= tensor(5.0834e+09, device='cuda:0')
c= tensor(5.1153e+09, device='cuda:0')
c= tensor(5.1196e+09, device='cuda:0')
c= tensor(5.1196e+09, device='cuda:0')
c= tensor(5.1235e+09, device='cuda:0')
c= tensor(5.1241e+09, device='cuda:0')
c= tensor(5.1249e+09, device='cuda:0')
c= tensor(5.1252e+09, device='cuda:0')
c= tensor(5.1252e+09, device='cuda:0')
c= tensor(5.1268e+09, device='cuda:0')
c= tensor(5.1272e+09, device='cuda:0')
c= tensor(5.1273e+09, device='cuda:0')
c= tensor(5.1275e+09, device='cuda:0')
c= tensor(5.1278e+09, device='cuda:0')
c= tensor(5.1506e+09, device='cuda:0')
c= tensor(5.1506e+09, device='cuda:0')
c= tensor(5.1508e+09, device='cuda:0')
c= tensor(5.1512e+09, device='cuda:0')
c= tensor(5.1518e+09, device='cuda:0')
c= tensor(5.1518e+09, device='cuda:0')
c= tensor(5.1519e+09, device='cuda:0')
c= tensor(5.1522e+09, device='cuda:0')
c= tensor(5.1643e+09, device='cuda:0')
c= tensor(5.1643e+09, device='cuda:0')
c= tensor(5.1643e+09, device='cuda:0')
c= tensor(5.1644e+09, device='cuda:0')
c= tensor(5.1966e+09, device='cuda:0')
c= tensor(5.5289e+09, device='cuda:0')
c= tensor(5.5295e+09, device='cuda:0')
c= tensor(5.5295e+09, device='cuda:0')
c= tensor(5.5396e+09, device='cuda:0')
c= tensor(5.5454e+09, device='cuda:0')
c= tensor(5.5454e+09, device='cuda:0')
c= tensor(5.5457e+09, device='cuda:0')
c= tensor(5.5462e+09, device='cuda:0')
c= tensor(5.6025e+09, device='cuda:0')
c= tensor(5.8719e+09, device='cuda:0')
c= tensor(5.8838e+09, device='cuda:0')
c= tensor(5.8842e+09, device='cuda:0')
c= tensor(5.8842e+09, device='cuda:0')
c= tensor(5.8842e+09, device='cuda:0')
c= tensor(5.8903e+09, device='cuda:0')
c= tensor(5.8904e+09, device='cuda:0')
c= tensor(5.8926e+09, device='cuda:0')
c= tensor(5.9061e+09, device='cuda:0')
c= tensor(5.9733e+09, device='cuda:0')
c= tensor(5.9733e+09, device='cuda:0')
c= tensor(5.9733e+09, device='cuda:0')
c= tensor(5.9743e+09, device='cuda:0')
c= tensor(6.0083e+09, device='cuda:0')
c= tensor(6.0094e+09, device='cuda:0')
c= tensor(6.0095e+09, device='cuda:0')
c= tensor(6.0095e+09, device='cuda:0')
c= tensor(6.0099e+09, device='cuda:0')
c= tensor(6.0100e+09, device='cuda:0')
c= tensor(6.0121e+09, device='cuda:0')
c= tensor(6.0122e+09, device='cuda:0')
c= tensor(6.0122e+09, device='cuda:0')
c= tensor(6.0124e+09, device='cuda:0')
c= tensor(6.0124e+09, device='cuda:0')
c= tensor(6.0124e+09, device='cuda:0')
c= tensor(6.0219e+09, device='cuda:0')
c= tensor(6.0769e+09, device='cuda:0')
c= tensor(6.0845e+09, device='cuda:0')
c= tensor(6.0901e+09, device='cuda:0')
c= tensor(6.0904e+09, device='cuda:0')
c= tensor(6.0905e+09, device='cuda:0')
c= tensor(6.0907e+09, device='cuda:0')
c= tensor(6.0982e+09, device='cuda:0')
c= tensor(6.0984e+09, device='cuda:0')
c= tensor(6.1049e+09, device='cuda:0')
c= tensor(6.1051e+09, device='cuda:0')
c= tensor(6.4980e+09, device='cuda:0')
c= tensor(6.4982e+09, device='cuda:0')
c= tensor(6.5002e+09, device='cuda:0')
c= tensor(6.5379e+09, device='cuda:0')
c= tensor(6.5398e+09, device='cuda:0')
c= tensor(6.5404e+09, device='cuda:0')
c= tensor(6.7506e+09, device='cuda:0')
c= tensor(6.7585e+09, device='cuda:0')
c= tensor(6.7598e+09, device='cuda:0')
c= tensor(6.7599e+09, device='cuda:0')
c= tensor(6.7606e+09, device='cuda:0')
c= tensor(6.7608e+09, device='cuda:0')
c= tensor(6.7745e+09, device='cuda:0')
c= tensor(6.9528e+09, device='cuda:0')
c= tensor(6.9671e+09, device='cuda:0')
c= tensor(6.9785e+09, device='cuda:0')
c= tensor(6.9802e+09, device='cuda:0')
c= tensor(6.9808e+09, device='cuda:0')
c= tensor(6.9813e+09, device='cuda:0')
c= tensor(6.9817e+09, device='cuda:0')
c= tensor(7.0994e+09, device='cuda:0')
c= tensor(7.1457e+09, device='cuda:0')
c= tensor(7.1471e+09, device='cuda:0')
c= tensor(8.3769e+09, device='cuda:0')
c= tensor(8.3968e+09, device='cuda:0')
c= tensor(8.3981e+09, device='cuda:0')
c= tensor(8.3981e+09, device='cuda:0')
c= tensor(8.4001e+09, device='cuda:0')
c= tensor(8.4075e+09, device='cuda:0')
c= tensor(8.4075e+09, device='cuda:0')
c= tensor(8.5576e+09, device='cuda:0')
c= tensor(8.5585e+09, device='cuda:0')
c= tensor(8.5592e+09, device='cuda:0')
c= tensor(8.5593e+09, device='cuda:0')
c= tensor(8.5594e+09, device='cuda:0')
c= tensor(8.5594e+09, device='cuda:0')
c= tensor(8.5596e+09, device='cuda:0')
c= tensor(8.5600e+09, device='cuda:0')
c= tensor(8.5616e+09, device='cuda:0')
c= tensor(1.0513e+10, device='cuda:0')
c= tensor(1.0516e+10, device='cuda:0')
c= tensor(1.0529e+10, device='cuda:0')
c= tensor(1.0529e+10, device='cuda:0')
c= tensor(1.0529e+10, device='cuda:0')
c= tensor(1.0529e+10, device='cuda:0')
c= tensor(1.0552e+10, device='cuda:0')
c= tensor(1.0561e+10, device='cuda:0')
c= tensor(1.0994e+10, device='cuda:0')
c= tensor(1.0994e+10, device='cuda:0')
c= tensor(1.1006e+10, device='cuda:0')
c= tensor(1.1007e+10, device='cuda:0')
c= tensor(1.1016e+10, device='cuda:0')
c= tensor(1.1042e+10, device='cuda:0')
c= tensor(1.1043e+10, device='cuda:0')
c= tensor(1.1043e+10, device='cuda:0')
c= tensor(1.1046e+10, device='cuda:0')
c= tensor(1.1049e+10, device='cuda:0')
c= tensor(1.1053e+10, device='cuda:0')
c= tensor(1.1099e+10, device='cuda:0')
c= tensor(1.1100e+10, device='cuda:0')
c= tensor(1.1103e+10, device='cuda:0')
c= tensor(1.1103e+10, device='cuda:0')
c= tensor(1.1112e+10, device='cuda:0')
c= tensor(1.1137e+10, device='cuda:0')
c= tensor(1.1138e+10, device='cuda:0')
c= tensor(1.1138e+10, device='cuda:0')
c= tensor(1.1183e+10, device='cuda:0')
c= tensor(1.1187e+10, device='cuda:0')
c= tensor(1.1475e+10, device='cuda:0')
c= tensor(1.1478e+10, device='cuda:0')
c= tensor(1.1482e+10, device='cuda:0')
c= tensor(1.1483e+10, device='cuda:0')
c= tensor(1.1500e+10, device='cuda:0')
c= tensor(1.1536e+10, device='cuda:0')
c= tensor(1.1537e+10, device='cuda:0')
c= tensor(1.1537e+10, device='cuda:0')
c= tensor(1.1537e+10, device='cuda:0')
c= tensor(1.1540e+10, device='cuda:0')
c= tensor(1.1548e+10, device='cuda:0')
c= tensor(1.1550e+10, device='cuda:0')
c= tensor(1.1550e+10, device='cuda:0')
c= tensor(1.1550e+10, device='cuda:0')
c= tensor(1.1556e+10, device='cuda:0')
c= tensor(1.1558e+10, device='cuda:0')
c= tensor(1.1559e+10, device='cuda:0')
c= tensor(1.1561e+10, device='cuda:0')
c= tensor(1.1563e+10, device='cuda:0')
c= tensor(1.1563e+10, device='cuda:0')
c= tensor(1.1563e+10, device='cuda:0')
c= tensor(1.1563e+10, device='cuda:0')
c= tensor(1.1573e+10, device='cuda:0')
c= tensor(1.1574e+10, device='cuda:0')
c= tensor(1.1576e+10, device='cuda:0')
c= tensor(1.1576e+10, device='cuda:0')
c= tensor(1.1576e+10, device='cuda:0')
c= tensor(1.1586e+10, device='cuda:0')
c= tensor(1.1589e+10, device='cuda:0')
c= tensor(1.1589e+10, device='cuda:0')
c= tensor(1.1589e+10, device='cuda:0')
c= tensor(1.1589e+10, device='cuda:0')
c= tensor(1.1590e+10, device='cuda:0')
c= tensor(1.1590e+10, device='cuda:0')
c= tensor(1.1600e+10, device='cuda:0')
c= tensor(1.1600e+10, device='cuda:0')
c= tensor(1.1601e+10, device='cuda:0')
c= tensor(1.1601e+10, device='cuda:0')
c= tensor(1.1602e+10, device='cuda:0')
c= tensor(1.1629e+10, device='cuda:0')
c= tensor(1.1630e+10, device='cuda:0')
c= tensor(1.1642e+10, device='cuda:0')
c= tensor(1.1654e+10, device='cuda:0')
c= tensor(1.1654e+10, device='cuda:0')
c= tensor(1.1659e+10, device='cuda:0')
c= tensor(1.1664e+10, device='cuda:0')
c= tensor(1.1664e+10, device='cuda:0')
c= tensor(1.1665e+10, device='cuda:0')
c= tensor(1.1665e+10, device='cuda:0')
c= tensor(1.1690e+10, device='cuda:0')
c= tensor(1.1696e+10, device='cuda:0')
c= tensor(1.1698e+10, device='cuda:0')
c= tensor(1.1703e+10, device='cuda:0')
c= tensor(1.1703e+10, device='cuda:0')
c= tensor(1.1706e+10, device='cuda:0')
c= tensor(1.1706e+10, device='cuda:0')
c= tensor(1.1706e+10, device='cuda:0')
c= tensor(1.1778e+10, device='cuda:0')
c= tensor(1.1970e+10, device='cuda:0')
c= tensor(1.1972e+10, device='cuda:0')
c= tensor(1.1972e+10, device='cuda:0')
c= tensor(1.1973e+10, device='cuda:0')
c= tensor(1.2058e+10, device='cuda:0')
c= tensor(1.2059e+10, device='cuda:0')
c= tensor(1.2059e+10, device='cuda:0')
c= tensor(1.2060e+10, device='cuda:0')
c= tensor(1.2065e+10, device='cuda:0')
c= tensor(1.2065e+10, device='cuda:0')
c= tensor(1.2066e+10, device='cuda:0')
c= tensor(1.2068e+10, device='cuda:0')
time to make c is 7.166171312332153
time for making loss is 7.166236162185669
p0 True
it  0 : 3944345600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6503968768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 45% |
memory (bytes)
6504124416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3515510300.0
relative error loss 0.2913039
shape of L is 
torch.Size([])
memory (bytes)
6529253376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 45% |
memory (bytes)
6529253376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3499761700.0
relative error loss 0.28999892
shape of L is 
torch.Size([])
memory (bytes)
6533181440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6533181440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3406652400.0
relative error loss 0.28228366
shape of L is 
torch.Size([])
memory (bytes)
6536368128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6536368128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3358098400.0
relative error loss 0.27826035
shape of L is 
torch.Size([])
memory (bytes)
6539563008
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 35% | 45% |
memory (bytes)
6539563008
| ID | GPU  | MEM |
-------------------
|  0 |  15% |  0% |
|  1 | 100% | 45% |
error is  3323388000.0
relative error loss 0.27538416
shape of L is 
torch.Size([])
memory (bytes)
6542766080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
memory (bytes)
6542766080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3298499600.0
relative error loss 0.27332187
shape of L is 
torch.Size([])
memory (bytes)
6545977344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 45% |
memory (bytes)
6545977344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3275038700.0
relative error loss 0.27137783
shape of L is 
torch.Size([])
memory (bytes)
6549168128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6549168128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3254245400.0
relative error loss 0.26965484
shape of L is 
torch.Size([])
memory (bytes)
6552276992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 45% |
memory (bytes)
6552367104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3242771500.0
relative error loss 0.2687041
shape of L is 
torch.Size([])
memory (bytes)
6555590656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6555590656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3232956400.0
relative error loss 0.26789078
time to take a step is 225.59267711639404
it  1 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6558765056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6558765056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3232956400.0
relative error loss 0.26789078
shape of L is 
torch.Size([])
memory (bytes)
6561992704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6561992704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3223391200.0
relative error loss 0.2670982
shape of L is 
torch.Size([])
memory (bytes)
6565203968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6565203968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3214936000.0
relative error loss 0.26639757
shape of L is 
torch.Size([])
memory (bytes)
6568411136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 45% |
memory (bytes)
6568427520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3206783000.0
relative error loss 0.26572198
shape of L is 
torch.Size([])
memory (bytes)
6571638784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6571638784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3198072800.0
relative error loss 0.26500025
shape of L is 
torch.Size([])
memory (bytes)
6574829568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
memory (bytes)
6574829568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3191203800.0
relative error loss 0.26443106
shape of L is 
torch.Size([])
memory (bytes)
6578040832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6578040832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3186459600.0
relative error loss 0.26403794
shape of L is 
torch.Size([])
memory (bytes)
6581268480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 45% |
memory (bytes)
6581268480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3180086300.0
relative error loss 0.26350984
shape of L is 
torch.Size([])
memory (bytes)
6584475648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 45% |
memory (bytes)
6584475648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3174879200.0
relative error loss 0.26307836
shape of L is 
torch.Size([])
memory (bytes)
6587695104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 45% |
memory (bytes)
6587695104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 45% |
error is  3170490400.0
relative error loss 0.26271468
time to take a step is 223.20862197875977
it  2 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6590840832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6590889984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3170490400.0
relative error loss 0.26271468
shape of L is 
torch.Size([])
memory (bytes)
6594097152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6594097152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3166091300.0
relative error loss 0.26235017
shape of L is 
torch.Size([])
memory (bytes)
6597304320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6597304320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3162074000.0
relative error loss 0.2620173
shape of L is 
torch.Size([])
memory (bytes)
6600511488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 45% |
memory (bytes)
6600511488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3159116800.0
relative error loss 0.26177225
shape of L is 
torch.Size([])
memory (bytes)
6603710464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 45% |
memory (bytes)
6603710464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3156555800.0
relative error loss 0.26156005
shape of L is 
torch.Size([])
memory (bytes)
6606925824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 45% |
memory (bytes)
6606925824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3153912800.0
relative error loss 0.26134104
shape of L is 
torch.Size([])
memory (bytes)
6610128896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6610128896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3152729000.0
relative error loss 0.26124296
shape of L is 
torch.Size([])
memory (bytes)
6613331968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 45% |
memory (bytes)
6613331968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3149852700.0
relative error loss 0.2610046
shape of L is 
torch.Size([])
memory (bytes)
6616539136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6616539136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3148372000.0
relative error loss 0.2608819
shape of L is 
torch.Size([])
memory (bytes)
6619729920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6619729920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3146891300.0
relative error loss 0.2607592
time to take a step is 223.37195801734924
it  3 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6622887936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 45% |
memory (bytes)
6622961664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3146891300.0
relative error loss 0.2607592
shape of L is 
torch.Size([])
memory (bytes)
6626156544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 45% |
memory (bytes)
6626156544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3143458800.0
relative error loss 0.2604748
shape of L is 
torch.Size([])
memory (bytes)
6629371904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6629371904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3143291000.0
relative error loss 0.26046088
shape of L is 
torch.Size([])
memory (bytes)
6632579072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
memory (bytes)
6632579072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3141585000.0
relative error loss 0.2603195
shape of L is 
torch.Size([])
memory (bytes)
6635786240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 45% |
memory (bytes)
6635786240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3138564000.0
relative error loss 0.2600692
shape of L is 
torch.Size([])
memory (bytes)
6638997504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 45% |
memory (bytes)
6638997504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3135710200.0
relative error loss 0.2598327
shape of L is 
torch.Size([])
memory (bytes)
6642208768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 45% |
memory (bytes)
6642208768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3132549000.0
relative error loss 0.25957078
shape of L is 
torch.Size([])
memory (bytes)
6645420032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 45% |
memory (bytes)
6645420032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3130005500.0
relative error loss 0.25936002
shape of L is 
torch.Size([])
memory (bytes)
6648627200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 45% |
memory (bytes)
6648627200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3128286200.0
relative error loss 0.25921756
shape of L is 
torch.Size([])
memory (bytes)
6651834368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 45% |
memory (bytes)
6651834368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3126100000.0
relative error loss 0.2590364
time to take a step is 225.24539518356323
it  4 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6655045632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 45% |
memory (bytes)
6655045632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 45% |
error is  3126100000.0
relative error loss 0.2590364
shape of L is 
torch.Size([])
memory (bytes)
6658248704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 45% |
memory (bytes)
6658248704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3123913700.0
relative error loss 0.25885525
shape of L is 
torch.Size([])
memory (bytes)
6661464064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
memory (bytes)
6661464064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3122030600.0
relative error loss 0.2586992
shape of L is 
torch.Size([])
memory (bytes)
6664658944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 45% |
memory (bytes)
6664658944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3121195000.0
relative error loss 0.25862995
shape of L is 
torch.Size([])
memory (bytes)
6667874304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 45% |
memory (bytes)
6667874304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3120255000.0
relative error loss 0.25855207
shape of L is 
torch.Size([])
memory (bytes)
6671081472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
memory (bytes)
6671081472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 45% |
error is  3118873600.0
relative error loss 0.2584376
shape of L is 
torch.Size([])
memory (bytes)
6674239488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6674239488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3117979600.0
relative error loss 0.25836352
shape of L is 
torch.Size([])
memory (bytes)
6677499904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
memory (bytes)
6677508096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 45% |
error is  3116491800.0
relative error loss 0.25824022
shape of L is 
torch.Size([])
memory (bytes)
6680707072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 45% |
memory (bytes)
6680707072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3115478000.0
relative error loss 0.25815624
shape of L is 
torch.Size([])
memory (bytes)
6683918336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6683918336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3114306600.0
relative error loss 0.25805917
time to take a step is 218.1533441543579
it  5 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6686986240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 45% |
memory (bytes)
6687121408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3114306600.0
relative error loss 0.25805917
shape of L is 
torch.Size([])
memory (bytes)
6690328576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 45% |
memory (bytes)
6690328576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3113944000.0
relative error loss 0.25802913
shape of L is 
torch.Size([])
memory (bytes)
6693548032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6693548032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3111424000.0
relative error loss 0.2578203
shape of L is 
torch.Size([])
memory (bytes)
6696755200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 45% |
memory (bytes)
6696755200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3110520800.0
relative error loss 0.25774547
shape of L is 
torch.Size([])
memory (bytes)
6699962368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 45% |
memory (bytes)
6699962368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3109344300.0
relative error loss 0.25764796
shape of L is 
torch.Size([])
memory (bytes)
6703165440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 45% |
memory (bytes)
6703165440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3107627000.0
relative error loss 0.25750569
shape of L is 
torch.Size([])
memory (bytes)
6706368512
| ID | GPU | MEM |
------------------
|  0 |  8% |  0% |
|  1 | 17% | 45% |
memory (bytes)
6706368512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3108241400.0
relative error loss 0.2575566
shape of L is 
torch.Size([])
memory (bytes)
6709575680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 45% |
memory (bytes)
6709575680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3106594800.0
relative error loss 0.25742015
shape of L is 
torch.Size([])
memory (bytes)
6712799232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
memory (bytes)
6712799232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3104896000.0
relative error loss 0.2572794
shape of L is 
torch.Size([])
memory (bytes)
6716006400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 45% |
memory (bytes)
6716006400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3103650800.0
relative error loss 0.2571762
time to take a step is 219.42760968208313
it  6 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6719156224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6719205376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3103650800.0
relative error loss 0.2571762
shape of L is 
torch.Size([])
memory (bytes)
6722412544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6722412544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3102331000.0
relative error loss 0.25706682
shape of L is 
torch.Size([])
memory (bytes)
6725476352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6725632000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 45% |
error is  3100995600.0
relative error loss 0.2569562
shape of L is 
torch.Size([])
memory (bytes)
6728847360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 45% |
memory (bytes)
6728847360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3100387300.0
relative error loss 0.2569058
shape of L is 
torch.Size([])
memory (bytes)
6732050432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 45% |
memory (bytes)
6732050432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3099526100.0
relative error loss 0.25683442
shape of L is 
torch.Size([])
memory (bytes)
6735265792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6735265792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3098826800.0
relative error loss 0.25677648
shape of L is 
torch.Size([])
memory (bytes)
6738419712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 45% |
memory (bytes)
6738477056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3097950200.0
relative error loss 0.25670382
shape of L is 
torch.Size([])
memory (bytes)
6741676032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 45% |
memory (bytes)
6741676032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3097434000.0
relative error loss 0.25666106
shape of L is 
torch.Size([])
memory (bytes)
6744731648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
memory (bytes)
6744883200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3096921000.0
relative error loss 0.25661856
shape of L is 
torch.Size([])
memory (bytes)
6748094464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 45% |
memory (bytes)
6748094464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3096450000.0
relative error loss 0.25657952
time to take a step is 220.25820636749268
it  7 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6751252480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 45% |
memory (bytes)
6751301632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3096450000.0
relative error loss 0.25657952
shape of L is 
torch.Size([])
memory (bytes)
6754504704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 45% |
memory (bytes)
6754504704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3095459800.0
relative error loss 0.25649747
shape of L is 
torch.Size([])
memory (bytes)
6757670912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 45% |
memory (bytes)
6757720064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3095045000.0
relative error loss 0.2564631
shape of L is 
torch.Size([])
memory (bytes)
6760931328
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 26% | 45% |
memory (bytes)
6760931328
| ID | GPU  | MEM |
-------------------
|  0 |   1% |  0% |
|  1 | 100% | 45% |
error is  3094552600.0
relative error loss 0.2564223
shape of L is 
torch.Size([])
memory (bytes)
6764093440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6764093440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3093694500.0
relative error loss 0.2563512
shape of L is 
torch.Size([])
memory (bytes)
6767337472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6767337472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3092978700.0
relative error loss 0.2562919
shape of L is 
torch.Size([])
memory (bytes)
6770544640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
memory (bytes)
6770544640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3092455400.0
relative error loss 0.25624853
shape of L is 
torch.Size([])
memory (bytes)
6773706752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6773755904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 45% |
error is  3092033500.0
relative error loss 0.25621358
shape of L is 
torch.Size([])
memory (bytes)
6776958976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 45% |
memory (bytes)
6776958976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3090590700.0
relative error loss 0.256094
shape of L is 
torch.Size([])
memory (bytes)
6780174336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6780174336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 45% |
error is  3090743300.0
relative error loss 0.25610664
shape of L is 
torch.Size([])
memory (bytes)
6783377408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 45% |
memory (bytes)
6783377408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3090017300.0
relative error loss 0.2560465
time to take a step is 249.2496738433838
it  8 : 4644304896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6786592768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 45% |
memory (bytes)
6786592768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3090017300.0
relative error loss 0.2560465
shape of L is 
torch.Size([])
memory (bytes)
6789791744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 45% |
memory (bytes)
6789791744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 45% |
error is  3089492000.0
relative error loss 0.25600296
shape of L is 
torch.Size([])
memory (bytes)
6793003008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6793003008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3088015400.0
relative error loss 0.25588062
shape of L is 
torch.Size([])
memory (bytes)
6796218368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6796218368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3087287300.0
relative error loss 0.25582027
shape of L is 
torch.Size([])
memory (bytes)
6799417344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 45% |
memory (bytes)
6799417344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3086467000.0
relative error loss 0.25575233
shape of L is 
torch.Size([])
memory (bytes)
6802620416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 45% |
memory (bytes)
6802620416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3085746200.0
relative error loss 0.25569257
shape of L is 
torch.Size([])
memory (bytes)
6805827584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
memory (bytes)
6805827584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3085269000.0
relative error loss 0.25565305
shape of L is 
torch.Size([])
memory (bytes)
6809026560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6809026560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3084514300.0
relative error loss 0.2555905
shape of L is 
torch.Size([])
memory (bytes)
6812237824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
memory (bytes)
6812237824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3083863000.0
relative error loss 0.25553653
shape of L is 
torch.Size([])
memory (bytes)
6815449088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 45% |
memory (bytes)
6815449088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3082881000.0
relative error loss 0.25545517
time to take a step is 226.6655216217041
it  9 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6818590720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6818639872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3082881000.0
relative error loss 0.25545517
shape of L is 
torch.Size([])
memory (bytes)
6821842944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 45% |
memory (bytes)
6821842944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3082282000.0
relative error loss 0.25540552
shape of L is 
torch.Size([])
memory (bytes)
6825054208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6825054208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3081635800.0
relative error loss 0.255352
shape of L is 
torch.Size([])
memory (bytes)
6828265472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6828265472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 45% |
error is  3081230300.0
relative error loss 0.25531837
shape of L is 
torch.Size([])
memory (bytes)
6831431680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6831431680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3080612900.0
relative error loss 0.25526723
shape of L is 
torch.Size([])
memory (bytes)
6834679808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6834679808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3080201200.0
relative error loss 0.2552331
shape of L is 
torch.Size([])
memory (bytes)
6837837824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 45% |
memory (bytes)
6837886976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3079867400.0
relative error loss 0.25520545
shape of L is 
torch.Size([])
memory (bytes)
6841102336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 45% |
memory (bytes)
6841102336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3079131100.0
relative error loss 0.25514445
shape of L is 
torch.Size([])
memory (bytes)
6844313600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6844313600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 45% |
error is  3078623200.0
relative error loss 0.25510237
shape of L is 
torch.Size([])
memory (bytes)
6847352832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6847512576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3078295600.0
relative error loss 0.25507522
time to take a step is 227.19102573394775
it  10 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6850719744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 45% |
memory (bytes)
6850719744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3078295600.0
relative error loss 0.25507522
shape of L is 
torch.Size([])
memory (bytes)
6853881856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 45% |
memory (bytes)
6853931008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3077926000.0
relative error loss 0.25504458
shape of L is 
torch.Size([])
memory (bytes)
6857134080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
memory (bytes)
6857134080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3077186600.0
relative error loss 0.2549833
shape of L is 
torch.Size([])
memory (bytes)
6860337152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 45% |
memory (bytes)
6860337152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3076868000.0
relative error loss 0.25495693
shape of L is 
torch.Size([])
memory (bytes)
6863552512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6863552512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3075844000.0
relative error loss 0.25487208
shape of L is 
torch.Size([])
memory (bytes)
6866747392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 45% |
memory (bytes)
6866747392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3075462100.0
relative error loss 0.25484043
shape of L is 
torch.Size([])
memory (bytes)
6869962752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
memory (bytes)
6869962752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3075012600.0
relative error loss 0.25480318
shape of L is 
torch.Size([])
memory (bytes)
6873178112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
memory (bytes)
6873178112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3074435000.0
relative error loss 0.25475532
shape of L is 
torch.Size([])
memory (bytes)
6876381184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
memory (bytes)
6876381184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3073785900.0
relative error loss 0.25470152
shape of L is 
torch.Size([])
memory (bytes)
6879588352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 45% |
memory (bytes)
6879588352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3073393700.0
relative error loss 0.254669
time to take a step is 232.41390323638916
it  11 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6882799616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6882799616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3073393700.0
relative error loss 0.254669
shape of L is 
torch.Size([])
memory (bytes)
6885949440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6885949440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3072944000.0
relative error loss 0.25463176
shape of L is 
torch.Size([])
memory (bytes)
6889209856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 45% |
memory (bytes)
6889209856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3072518100.0
relative error loss 0.25459647
shape of L is 
torch.Size([])
memory (bytes)
6892277760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 45% |
memory (bytes)
6892412928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3072015400.0
relative error loss 0.2545548
shape of L is 
torch.Size([])
memory (bytes)
6895616000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
memory (bytes)
6895616000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3071681500.0
relative error loss 0.25452715
shape of L is 
torch.Size([])
memory (bytes)
6898782208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6898782208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3071441000.0
relative error loss 0.2545072
shape of L is 
torch.Size([])
memory (bytes)
6901956608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6902013952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3071072300.0
relative error loss 0.25447667
shape of L is 
torch.Size([])
memory (bytes)
6905225216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6905225216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3070546000.0
relative error loss 0.25443304
shape of L is 
torch.Size([])
memory (bytes)
6908358656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
memory (bytes)
6908424192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3070352400.0
relative error loss 0.254417
shape of L is 
torch.Size([])
memory (bytes)
6911631360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 45% |
memory (bytes)
6911631360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3070079000.0
relative error loss 0.25439435
time to take a step is 229.65152597427368
it  12 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 45% |
shape of L is 
torch.Size([])
memory (bytes)
6914715648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 45% |
memory (bytes)
6914834432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3070079000.0
relative error loss 0.25439435
shape of L is 
torch.Size([])
memory (bytes)
6918045696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 45% |
memory (bytes)
6918045696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3069645800.0
relative error loss 0.25435847
shape of L is 
torch.Size([])
memory (bytes)
6921170944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 45% |
memory (bytes)
6921256960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3069445000.0
relative error loss 0.25434184
shape of L is 
torch.Size([])
memory (bytes)
6924464128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 45% |
memory (bytes)
6924464128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 45% |
error is  3069171700.0
relative error loss 0.2543192
shape of L is 
torch.Size([])
memory (bytes)
6927683584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 46% |
memory (bytes)
6927683584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3069023200.0
relative error loss 0.25430688
shape of L is 
torch.Size([])
memory (bytes)
6930866176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 46% |
memory (bytes)
6930866176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3068644400.0
relative error loss 0.25427547
shape of L is 
torch.Size([])
memory (bytes)
6934089728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 46% |
memory (bytes)
6934089728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3068398600.0
relative error loss 0.25425512
shape of L is 
torch.Size([])
memory (bytes)
6937214976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 46% |
memory (bytes)
6937292800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3068047400.0
relative error loss 0.254226
shape of L is 
torch.Size([])
memory (bytes)
6940499968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 46% |
memory (bytes)
6940499968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3067680800.0
relative error loss 0.25419563
shape of L is 
torch.Size([])
memory (bytes)
6943498240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 46% |
memory (bytes)
6943707136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3067372500.0
relative error loss 0.2541701
time to take a step is 225.55757975578308
it  13 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 46% |
shape of L is 
torch.Size([])
memory (bytes)
6946922496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 46% |
memory (bytes)
6946922496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3067372500.0
relative error loss 0.2541701
shape of L is 
torch.Size([])
memory (bytes)
6950121472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 46% |
memory (bytes)
6950121472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3066802200.0
relative error loss 0.25412282
shape of L is 
torch.Size([])
memory (bytes)
6953148416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 46% |
memory (bytes)
6953332736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3066294300.0
relative error loss 0.25408074
shape of L is 
torch.Size([])
memory (bytes)
6956544000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 46% |
memory (bytes)
6956544000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3065887700.0
relative error loss 0.25404707
shape of L is 
torch.Size([])
memory (bytes)
6959648768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 46% |
memory (bytes)
6959751168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3065609200.0
relative error loss 0.25402397
shape of L is 
torch.Size([])
memory (bytes)
6962954240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 46% |
memory (bytes)
6962954240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3065117700.0
relative error loss 0.25398326
shape of L is 
torch.Size([])
memory (bytes)
6966173696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 46% |
memory (bytes)
6966173696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3065141200.0
relative error loss 0.2539852
shape of L is 
torch.Size([])
memory (bytes)
6969364480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 46% |
memory (bytes)
6969364480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3064879000.0
relative error loss 0.25396347
shape of L is 
torch.Size([])
memory (bytes)
6972588032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 46% |
memory (bytes)
6972588032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3064488000.0
relative error loss 0.25393108
shape of L is 
torch.Size([])
memory (bytes)
6975791104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 46% |
memory (bytes)
6975791104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3064074200.0
relative error loss 0.2538968
time to take a step is 226.60988450050354
it  14 : 4644304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 46% |
shape of L is 
torch.Size([])
memory (bytes)
6978961408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 46% |
memory (bytes)
6978961408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 46% |
error is  3064074200.0
relative error loss 0.2538968
shape of L is 
torch.Size([])
memory (bytes)
6982201344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 46% |
memory (bytes)
6982201344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3063644200.0
relative error loss 0.25386116
shape of L is 
torch.Size([])
memory (bytes)
6985408512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 46% |
memory (bytes)
6985424896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3063289900.0
relative error loss 0.2538318
shape of L is 
torch.Size([])
memory (bytes)
6988619776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 46% |
memory (bytes)
6988627968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3063019500.0
relative error loss 0.2538094
shape of L is 
torch.Size([])
memory (bytes)
6991839232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 46% |
memory (bytes)
6991839232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3062781000.0
relative error loss 0.25378963
shape of L is 
torch.Size([])
memory (bytes)
6994993152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 46% |
memory (bytes)
6994993152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3062439000.0
relative error loss 0.2537613
shape of L is 
torch.Size([])
memory (bytes)
6998249472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 46% |
memory (bytes)
6998249472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 46% |
error is  3062965200.0
relative error loss 0.2538049
shape of L is 
torch.Size([])
memory (bytes)
7001403392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 46% |
memory (bytes)
7001452544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3062157300.0
relative error loss 0.25373796
shape of L is 
torch.Size([])
memory (bytes)
7004651520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 46% |
memory (bytes)
7004651520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3061670000.0
relative error loss 0.25369757
shape of L is 
torch.Size([])
memory (bytes)
7007789056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 46% |
memory (bytes)
7007858688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 46% |
error is  3061293000.0
relative error loss 0.25366634
time to take a step is 223.6081554889679
sum tnnu_Z after tensor(18680624., device='cuda:0')
shape of features
(6895,)
shape of features
(6895,)
number of orig particles 27579
number of new particles after remove low mass 25983
tnuZ shape should be parts x labs
torch.Size([27579, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  3515361000.0
relative error without small mass is  0.29129153
nnu_Z shape should be number of particles by maxV
(27579, 702)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
shape of features
(27579,)
Tue Jan 31 23:05:43 EST 2023
