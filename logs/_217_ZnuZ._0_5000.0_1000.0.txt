Wed Feb 1 02:44:16 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 37825699
numbers of Z: 20528
shape of features
(20528,)
shape of features
(20528,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01999884628246009	20528	20.528	0.0991332705499087
X	0.017671090536007066	1605	1.605	0.22246555208702845
X	0.0177857874551649	19262	19.262	0.0973771904116057
X	0.01752290755106762	1834	1.834	0.21219522791902262
X	0.017790113823892874	15446	15.446	0.10482243211592975
X	0.018733208000822895	11971	11.971	0.11609866917055398
X	0.018412097560940706	45882	45.882	0.07375989825157737
X	0.017774313309038843	36756	36.756	0.07849130133442524
X	0.01780254455232192	37218	37.218	0.07820652996090202
X	0.01762654483622493	11627	11.627	0.11487702301978034
X	0.018451769235608487	30909	30.909	0.08420108478383274
X	0.01751947267457352	8565	8.565	0.12693982275487012
X	0.017686425313644224	128716	128.716	0.05160231392328651
X	0.01818146024905376	6634	6.634	0.13994297360452498
X	0.01781237564821857	348492	348.492	0.03711159991072803
X	0.017738765130774804	21706	21.706	0.09349345193191116
X	0.01772716820752163	50233	50.233	0.07066701548932058
X	0.0177942422024905	52984	52.984	0.06950962067671596
X	0.017741971832045643	15917	15.917	0.10368443283498331
X	0.017773053125087798	128334	128.334	0.05173765844101151
X	0.018003750954609724	74966	74.966	0.06215836075723377
X	0.017728332171740278	67975	67.975	0.06389098074280847
X	0.01795959318088307	210806	210.806	0.04400187426638568
X	0.0176040749622215	8914	8.914	0.12546238136310417
X	0.018461538130995568	42263	42.263	0.07587566666642645
X	0.017411231266821342	3268	3.268	0.17465558541399082
X	0.017786379409119493	69870	69.87	0.0633770863881998
X	0.01778127725541636	61265	61.265	0.06620897105815604
X	0.017898041254126862	9686	9.686	0.12271199404279007
X	0.017581883567465692	39533	39.533	0.07633116235002389
X	0.01783189159480149	1128653	1128.653	0.025092599695982296
X	0.0173760179618003	15217	15.217	0.10452184617164254
X	0.017812621839633606	584553	584.553	0.031234506706003858
X	0.01759179512387913	28318	28.318	0.08532622402222763
X	0.017735412581635652	10447	10.447	0.11929344132269036
X	0.017603500678225144	31718	31.718	0.08217967426541352
X	0.017782701767493175	131055	131.055	0.051386383067876545
X	0.01779933776449397	76258	76.258	0.061570518535326996
X	0.017014292004426154	1356	1.356	0.23237250022580558
X	0.01749923149134875	2862	2.862	0.18285904570529607
X	0.017667630644928004	1783	1.783	0.21478749323185506
X	0.017708146585038498	2276	2.276	0.1981528319487067
X	0.01642906898324034	3428	3.428	0.16859992611450403
X	0.01747989453729088	993	0.993	0.26013364111738996
X	0.01718893621149916	3284	3.284	0.1736262093881433
X	0.01765332765702829	1046	1.046	0.25650645532325206
X	0.01769077607679671	2026	2.026	0.2059218737937829
X	0.0174639655418482	2826	2.826	0.18350880315680276
X	0.017576362290263818	3921	3.921	0.16488357093055286
X	0.017604750533020602	2462	2.462	0.19265514377599216
X	0.017706937509589904	17010	17.01	0.10134750159601282
X	0.01872907800710636	9276	9.276	0.12639169929871716
X	0.017004509472782692	1284	1.284	0.23659180353315046
X	0.01755176781315437	18289	18.289	0.0986378585523562
X	0.017183086976085872	2319	2.319	0.19495459685230726
X	0.017585112595399324	3641	3.641	0.16903429430187555
X	0.017074506272355232	7406	7.406	0.13210567737010814
X	0.017516482231919043	2015	2.015	0.20561617733496437
X	0.0174417297359013	3549	3.549	0.1700178491698889
X	0.01768590397454452	3650	3.65	0.16921731486982688
X	0.017742150545258028	2973	2.973	0.18138600832409055
X	0.0175538201428744	4074	4.074	0.16272347364490844
X	0.017196935403581914	1638	1.638	0.21896719456503191
X	0.01771464055077203	7443	7.443	0.13351440002012196
X	0.017724837766236817	8758	8.758	0.12649084951649794
X	0.017679903385726923	2952	2.952	0.18160223038317874
X	0.01757438364214885	2509	2.509	0.19133443889620294
X	0.017735673871734383	5635	5.635	0.14654958444439656
X	0.01715564252920872	5536	5.536	0.14579309949180638
X	0.017647181383350297	5519	5.519	0.14732337240989618
X	0.017670277934745127	2393	2.393	0.19473031243110533
X	0.01743774978573544	5452	5.452	0.14733690120103385
X	0.017658717751659708	3123	3.123	0.1781540611865744
X	0.017749971143377754	4073	4.073	0.16334070049611837
X	0.016707920807760825	1934	1.934	0.20518958485181818
X	0.017364302058748483	3699	3.699	0.16743940300356958
X	0.017799294439404863	7097	7.097	0.13586554254026234
X	0.017461492165485776	750	0.75	0.2855442132721422
X	0.01762170752965068	2246	2.246	0.19870680740601263
X	0.017571100698555338	3672	3.672	0.16851249094895165
X	0.01732739627457799	14489	14.489	0.10614469621671183
X	0.017107444299863987	2783	2.783	0.18318530957644139
X	0.017431500202059948	2004	2.004	0.2056580353585358
X	0.017507360266409606	3040	3.04	0.1792458165489997
X	0.01731920531541726	1893	1.893	0.2091506003779586
X	0.01651078726361055	1387	1.387	0.22833041895786577
X	0.017928886102998566	2097	2.097	0.2044803717041604
X	0.018503426152565356	3864	3.864	0.16855372100408467
X	0.017494531736264803	895	0.895	0.26937653508126463
X	0.017776268247367073	3245	3.245	0.17628226880246692
X	0.01744616566232128	3816	3.816	0.1659703622944125
X	0.01774267254949781	6449	6.449	0.14012282575966833
X	0.01728446308947559	1718	1.718	0.2158792553461824
X	0.01757419352660186	3312	3.312	0.1744194952964184
X	0.017556685600597737	2137	2.137	0.20178056632730268
X	0.017552849464579766	3257	3.257	0.17532477487044665
X	0.017555688155402285	3293	3.293	0.1746929487825775
X	0.01745459677812282	2147	2.147	0.20107554694723792
X	0.017640945900364663	4702	4.702	0.15538642721376716
X	0.017234314381569135	3549	3.549	0.16934121463768725
X	0.017619253047343613	3757	3.757	0.16738457449368901
X	0.017720792434157206	14719	14.719	0.10638203108836569
X	0.017777775069166467	5477	5.477	0.14806243544199466
X	0.017617807852063048	6384	6.384	0.14026618881805925
X	0.01693340378234934	1440	1.44	0.22740181365182702
X	0.017687603921091553	5408	5.408	0.14843766265395955
X	0.017379040205109077	3137	3.137	0.17694450888973912
X	0.017457989226635433	6292	6.292	0.14051903836808796
X	0.017323408009982902	1557	1.557	0.22324491419527137
X	0.017691451285010935	4111	4.111	0.16265671411103436
X	0.017453396113617518	1589	1.589	0.22228936371099542
X	0.0174107553888184	2385	2.385	0.19398868996722482
X	0.01721655456711184	1455	1.455	0.22787381156766814
X	0.017248106210449116	2620	2.62	0.18741889607779166
X	0.01733816803557974	2608	2.608	0.1880320468021942
X	0.01761870336417028	5444	5.444	0.14791717677020838
X	0.01730315656210183	5255	5.255	0.14877074540030977
X	0.0176968754845454	4291	4.291	0.16036614652054174
X	0.01728320390937314	939	0.939	0.264029763797816
X	0.01792278692681618	5539	5.539	0.14790790722575595
X	0.016486989445047145	1418	1.418	0.22654529293612122
X	0.01777815384637125	15271	15.271	0.1051977362471105
X	0.017717730654906616	2871	2.871	0.18342490419849178
X	0.017240169891208995	1991	1.991	0.2053478186018083
X	0.01767692050174024	2169	2.169	0.20124063803396086
X	0.017244806518152373	2085	2.085	0.2022324091700907
X	0.017210894176145335	2136	2.136	0.20047831123540188
X	0.017391297805791815	1316	1.316	0.23642425773449166
X	0.016930986869981318	3391	3.391	0.17091682935729194
X	0.0174107113962861	6480	6.48	0.13902103055392923
X	0.016683106564980617	2150	2.15	0.19797612582284
X	0.017608391628887516	8108	8.108	0.12949966638529728
X	0.017097111263184705	1511	1.511	0.22450159173962145
X	0.01684326570786422	2531	2.531	0.1880953466603442
X	0.017284646875041703	1640	1.64	0.21924963940533196
X	0.017181306162853002	3663	3.663	0.1673939481403365
X	0.017757138997293122	3036	3.036	0.18017329003276428
X	0.01750320027574601	1653	1.653	0.21959116840209786
X	0.017716530697431898	2467	2.467	0.19293151614068615
X	0.01756023904470066	2233	2.233	0.1988599077017204
X	0.0173320966125867	1335	1.335	0.23502998805213612
X	0.01751882201869087	2135	2.135	0.20169834763161573
X	0.017192600169211803	1130	1.13	0.24779224284002777
X	0.017387336830259142	10755	10.755	0.11736561646107178
X	0.01783317699970132	6466	6.466	0.14023755777184668
X	0.017916192981060005	2567	2.567	0.1911055446596794
X	0.017713530555919073	3369	3.369	0.17388737040262428
X	0.01769609148695506	2321	2.321	0.19681917398157947
X	0.01860240856624287	1663	1.663	0.22364488840804836
X	0.017172073935538917	2104	2.104	0.20133796346458643
X	0.01736181728017313	2382	2.382	0.19388809400561619
X	0.017235657937346736	2108	2.108	0.20145857296129255
X	0.01748499358653322	2247	2.247	0.19816219383719905
X	0.0173457044812944	2523	2.523	0.19014791346973217
X	0.01774566766847905	9951	9.951	0.12126678745635136
X	0.01761745143307349	2948	2.948	0.18147015127032448
X	0.017312625646949603	13886	13.886	0.10762882337056442
X	0.0177383259440712	3006	3.006	0.18070682336645588
X	0.017572172458694	3886	3.886	0.16536396876243
X	0.017707641566145283	3300	3.3	0.1750715570194384
X	0.017598364843095383	767	0.767	0.2841574091596577
X	0.018205057495940733	12573	12.573	0.11313171734191409
X	0.01745107870810477	1982	1.982	0.206493416105383
X	0.01710678270133397	2640	2.64	0.1864324344405146
X	0.017191417819184533	1115	1.115	0.24889276708985322
X	0.017724703568067407	6075	6.075	0.1428929819848044
X	0.017629538951610724	2231	2.231	0.19918064213851194
X	0.017448287424695536	1679	1.679	0.21822309126596534
X	0.0173687627259471	4634	4.634	0.15533558272033535
X	0.017258177051256166	3300	3.3	0.17357759234407147
X	0.017116576850905837	926	0.926	0.2644043783936947
X	0.016830884769706723	628	0.628	0.29926029587053143
X	0.017060594203863026	1133	1.133	0.2469380879670127
X	0.018669608236289787	7955	7.955	0.1328918478431232
X	0.0174014791599636	2051	2.051	0.20395772940168802
X	0.018347247307065565	4150	4.15	0.1641248237619324
X	0.01748244981779541	8507	8.507	0.12713797381192457
X	0.017268842635602806	4729	4.729	0.15399193374699882
X	0.017734339519899004	2181	2.181	0.20108813721440436
X	0.01729056071276485	5141	5.141	0.14982599059201787
X	0.017485925772582962	2653	2.653	0.18749245404961687
X	0.01771758941914425	3490	3.49	0.17186722656265155
X	0.0169961396298532	3492	3.492	0.16946968352318925
X	0.017696324750214005	3410	3.41	0.17313155651981993
X	0.01723307466807597	1669	1.669	0.21775533851378798
X	0.01757359226670658	3500	3.5	0.17123695660733113
X	0.0174462516151976	5594	5.594	0.1461032544301397
X	0.016970506356892248	1800	1.8	0.21125521170788786
X	0.017167262168797586	3209	3.209	0.17489488469331205
X	0.017656944001225878	6420	6.42	0.14010710857749306
X	0.017503757879488862	6315	6.315	0.1404707757468347
X	0.017704339862800417	2728	2.728	0.1865284683044994
X	0.017154570219751992	669	0.669	0.2948840220217094
X	0.01766886281368692	3625	3.625	0.16955093768810292
X	0.017481704230966455	8386	8.386	0.1277447233309248
X	0.016963440397901507	6339	6.339	0.1388346048344038
X	0.01770565130563675	3161	3.161	0.1775943467552402
X	0.017649954835305393	2821	2.821	0.18426669117770883
X	0.01722929460743012	1326	1.326	0.2350938803855052
X	0.017538020131138227	2356	2.356	0.1952548181285199
X	0.017583210567164803	1071	1.071	0.2541574836858203
X	0.017789605802960864	3139	3.139	0.1782891863485245
X	0.016958215996138726	2112	2.112	0.20024518539365987
X	0.017608411926334458	5170	5.17	0.15045612837647282
X	0.017341127080497863	2064	2.064	0.2032930551573307
X	0.017728294871084488	7760	7.76	0.13170484405982047
X	0.017754016976961922	2252	2.252	0.1990258167765021
X	0.017113227521242842	3445	3.445	0.17062692576516783
X	0.01763525090493238	2819	2.819	0.1842590618760814
X	0.01773202987352566	3413	3.413	0.1731971422963236
X	0.018223690432228858	5124	5.124	0.15264259340475542
X	0.017414000759142036	5306	5.306	0.14860854124830314
X	0.017229969680372447	7539	7.539	0.13172157753421182
X	0.01771827753858021	3282	3.282	0.1754261401471815
X	0.017322458574337996	3258	3.258	0.17453645487603114
X	0.017589475702809606	1243	1.243	0.2418766371639047
X	0.017049148331917837	2301	2.301	0.19495244676132148
X	0.0175706059136276	1078	1.078	0.25354554990286926
X	0.01718403213076637	1421	1.421	0.22953222471084025
X	0.017424323911795433	5161	5.161	0.1500170745970228
X	0.01740061841209161	2346	2.346	0.1950198831197253
X	0.0174671756028244	2194	2.194	0.1996772963756458
X	0.01679787887385666	1060	1.06	0.2511787298696685
X	0.017406506963403002	6353	6.353	0.1399300290450199
X	0.01746635468804801	3135	3.135	0.17727802714049345
X	0.01829209348919301	2370	2.37	0.19762339482673347
X	0.01934391521001165	3195	3.195	0.18225991880458345
X	0.01745864091205956	1427	1.427	0.23042458808679486
X	0.017748501754564813	2010	2.01	0.20669114929518098
X	0.01730414544277296	3230	3.23	0.17497763600491192
X	0.017122601347246	937	0.937	0.2633965313788555
X	0.017402342847315264	2860	2.86	0.18256347587478408
X	0.017431162504271503	3452	3.452	0.17156098125182573
X	0.01776382856948996	4102	4.102	0.16299726069021273
X	0.01771155773713477	3908	3.908	0.16548834067714494
X	0.01750398400500047	1807	1.807	0.2131700817483601
X	0.0173886737418306	3765	3.765	0.16653307175716994
X	0.017801636196641036	4880	4.88	0.15393830482276494
X	0.0175792839283527	3889	3.889	0.16534373576551792
X	0.017064000372573915	2839	2.839	0.1818186741367434
X	0.01775690729763376	20107	20.107	0.0959415395408218
X	0.01782103847102434	41505	41.505	0.07544159524638622
X	0.017767958940315347	4957	4.957	0.15304044071711834
X	0.017450148228734474	7061	7.061	0.1352002940918297
X	0.017639490766594983	743	0.743	0.28740814822633654
X	0.017643276245570386	7256	7.256	0.13447059297744673
X	0.017561111978707345	12017	12.017	0.11347982315831386
X	0.017653849075658253	129236	129.236	0.05150135244540183
X	0.01776002298926799	2406	2.406	0.19470747891339305
X	0.017734336501487598	137397	137.397	0.0505373615828136
X	0.017468589284643926	25782	25.782	0.08783084815808247
X	0.017362894168485455	34003	34.003	0.07992849352403848
X	0.017892907575987205	38625	38.625	0.07737562001495714
X	0.01765368094259385	3015	3.015	0.18023922815610607
X	0.01755505671643011	4925	4.925	0.15275613708799837
X	0.0177813980307536	152221	152.221	0.048883667074634735
X	0.018478606001898653	577527	577.527	0.03174672799211602
X	0.017355287934710025	3427	3.427	0.17172725961977667
X	0.017695189234862796	39208	39.208	0.0767055592444692
X	0.017335255968737306	15629	15.629	0.10351414258595959
X	0.0175688826292748	26849	26.849	0.08681711348090587
X	0.01769300535000271	126892	126.892	0.0518548189292533
X	0.017721646512855765	55162	55.162	0.0684890835894289
X	0.01846780959292352	34034	34.034	0.08156442856668497
X	0.019584011386386223	15713	15.713	0.10761699164386437
X	0.01718920491372674	2386	2.386	0.19313535543542665
X	0.017760920481666092	125416	125.416	0.05212397264348432
X	0.01767233014916988	11177	11.177	0.11649917640755297
X	0.017223971872246568	2323	2.323	0.19499704612920152
X	0.017654717856282153	22665	22.665	0.09200999759205915
X	0.017685840534462597	65609	65.609	0.0645982513784506
X	0.01935581944859743	111699	111.699	0.0557511836671684
X	0.017735965137928227	83935	83.935	0.059562178971855444
X	0.017485553040008903	3718	3.718	0.1675418586935026
X	0.0175544547957042	27378	27.378	0.08623069283532825
X	0.01750897690687079	5754	5.754	0.14490953764971554
X	0.017761713153236908	42732	42.732	0.07462950208755495
X	0.017813404576155253	61057	61.057	0.06632396630544202
X	0.01770817402822538	6116	6.116	0.1425286270046467
X	0.01766095755522334	95419	95.419	0.056989274555725285
X	0.0171499753247582	736	0.736	0.28562434954630284
X	0.017625829428828496	6753	6.753	0.1376842524980855
X	0.017612504612128516	74321	74.321	0.06188278611066712
X	0.017388983626158275	40619	40.619	0.07536707438789603
X	0.01748118266560815	75825	75.825	0.06131773873450999
X	0.017258970340379425	2832	2.832	0.18265877657105992
X	0.0177930068835868	210320	210.32	0.04389916482060589
X	0.017782605067985513	17754	17.754	0.10005367751678208
X	0.017741834013280736	26983	26.983	0.08695664184136756
X	0.01782123158635688	66297	66.297	0.06453786769898698
X	0.017658332004658602	2030	2.03	0.2056606543940364
X	0.017647826659854485	106301	106.301	0.054960578218132765
X	0.017734510139560976	164540	164.54	0.04759002153536107
X	0.01812082996224285	263420	263.42	0.040974133670982314
X	0.01737719776279203	58992	58.992	0.06653684559298599
X	0.017561384249574936	30708	30.708	0.08300464653677112
X	0.01754142178788376	7711	7.711	0.13151784863935542
X	0.017786483583019257	6315	6.315	0.1412230479123632
X	0.017835458300772877	99498	99.498	0.05638409846066499
X	0.017564006625906736	9075	9.075	0.12462131210630159
X	0.017700780481495457	23950	23.95	0.0904126015992512
X	0.017832223283806826	46327	46.327	0.07274287116130518
X	0.017784417414891285	18015	18.015	0.09957151740271382
X	0.017539012399013922	14843	14.843	0.10572097244623749
X	0.01775041417573256	4666	4.666	0.15610659543166835
X	0.01778885406513487	78396	78.396	0.06099365927484563
X	0.017760291137027946	82683	82.683	0.05988866308625871
X	0.018467590166312737	22509	22.509	0.09361627193726757
X	0.01768927276893734	6887	6.887	0.13694934683563162
X	0.017735563545056194	44793	44.793	0.07343080357218673
X	0.01769141791583687	17746	17.746	0.09989737003628388
X	0.01758262461948978	167086	167.086	0.047211508875222415
X	0.017715559875386207	9404	9.404	0.12350392330717579
X	0.017319909952158503	32879	32.879	0.08076236160724747
X	0.017785168306140818	10880	10.88	0.11779938632703706
X	0.017729387979171563	73857	73.857	0.0621489830981677
X	0.017807995122518835	36588	36.588	0.0786608773208385
X	0.017767625152501014	19755	19.755	0.0965274388071528
X	0.017741395532045377	103340	103.34	0.05557845122544695
X	0.017800451912177058	164447	164.447	0.04765791346563235
X	0.017606879218241712	5372	5.372	0.14854183809133364
X	0.017963212245446548	147356	147.356	0.04958367654009863
X	0.017817359511016352	113831	113.831	0.05389240903159083
X	0.019539630649603554	270953	270.953	0.04162371079828256
X	0.017743957023346094	17101	17.101	0.10123786784626249
X	0.017345808218763362	2517	2.517	0.1902992638501917
X	0.017716826758725393	15471	15.471	0.10462187943564731
X	0.017802299525034832	21552	21.552	0.09382736251155772
X	0.017712247211828874	26318	26.318	0.08763419349405042
X	0.017739479969602758	64580	64.58	0.06500512751916424
X	0.01741875569023735	3463	3.463	0.17133846776220474
X	0.01772286734814802	61741	61.741	0.06596599373448414
X	0.01837906090196013	126553	126.553	0.05256342115113971
X	0.017302063543604754	5591	5.591	0.14572569552243403
X	0.017249155863464934	6047	6.047	0.14182168009827748
X	0.017494427315781227	44836	44.836	0.0730731153696862
X	0.01753781245059374	12017	12.017	0.11342961379927234
X	0.01778880620376116	6527	6.527	0.13968327118371912
X	0.01778406817019432	14958	14.958	0.1059381851126287
X	0.017687386291513285	3752	3.752	0.16767447056687354
X	0.017598967285288928	15034	15.034	0.10539116680450533
X	0.0177810711371307	32339	32.339	0.08192386709984718
X	0.0188474986051466	20000	20.0	0.0980410396715724
X	0.01765015786855493	30220	30.22	0.08358943275237847
X	0.017293303779465117	6207	6.207	0.1407123042828393
X	0.01842047872006271	908559	908.559	0.027267747219156074
X	0.017742020527821683	13365	13.365	0.1099034615843708
X	0.0176136133718641	97246	97.246	0.05657947380640363
X	0.01753804508709	2013	2.013	0.20576861564893106
X	0.01773069204904	7123	7.123	0.13552546754001038
X	0.01761796904920197	5326	5.326	0.14899952829907195
X	0.01780823732826694	13270	13.27	0.11030198734246141
X	0.019351197975734532	3002	3.002	0.1861082917001459
X	0.017582919645789903	77571	77.571	0.060972013026092974
X	0.0175247848543414	4325	4.325	0.15942467037969113
X	0.017603366594374534	7410	7.41	0.13343174789624765
X	0.019262790727661162	202429	202.429	0.045654349473307335
X	0.017688347066716653	54221	54.221	0.06883984904803113
X	0.017610585896445766	30876	30.876	0.08293113150765177
X	0.017789301809163768	101511	101.511	0.05596054460972161
X	0.018544536560100522	224524	224.524	0.04354962348254101
X	0.017534172586092918	5272	5.272	0.14926911517412716
X	0.017510006939278365	5145	5.145	0.1504181742675172
X	0.017534035429225603	20925	20.925	0.09427693850000528
X	0.0176320294580612	1998	1.998	0.20665009358144673
X	0.01783670658526239	7725	7.725	0.13217177631204036
X	0.01772814712722956	11185	11.185	0.11659388841367582
X	0.017567701143488896	2789	2.789	0.18468096712069315
X	0.017757437537601325	11736	11.736	0.11480304261643251
X	0.01770520540716089	8746	8.746	0.1265019340991939
X	0.01775591044202541	5613	5.613	0.14679659019891803
X	0.017613708564673484	170828	170.828	0.04689183735652031
X	0.01736053601176136	9635	9.635	0.12168502537638955
X	0.017796221680317716	59493	59.493	0.0668786214541992
X	0.017627469073442453	2162	2.162	0.20126955695011095
X	0.017704984481607487	2901	2.901	0.1827465866274437
X	0.017662820596726535	100324	100.324	0.056046918456144734
X	0.01958476382746032	486936	486.936	0.034262218862102206
X	0.017792241822168302	397167	397.167	0.03551560131033522
X	0.017807244666216332	12800	12.8	0.11163376840117349
X	0.017803658938354183	42889	42.889	0.0745969575767511
X	0.017718723701522675	3314	3.314	0.17486114209664258
X	0.01744365700106454	3712	3.712	0.16749808918049358
X	0.01943522296102063	223278	223.278	0.04431810661912549
X	0.017801577834166516	15946	15.946	0.10373745262132111
X	0.01732905067720464	5939	5.939	0.14289613494989245
X	0.017732705595986387	44768	44.768	0.07344052458073023
X	0.018568377969557008	460295	460.295	0.0342961921327198
X	0.017373650771936942	28295	28.295	0.08499508184171245
X	0.017543192596402028	12635	12.635	0.11156067372276225
X	0.019069713159961473	13778	13.778	0.11144296547455233
X	0.017683943750802882	12223	12.223	0.11310117175848149
X	0.017564690859982503	5007	5.007	0.15194542902364527
X	0.017796467335269746	138286	138.286	0.05048765456353667
X	0.017787512546745828	24070	24.07	0.0904092867494601
X	0.01699844535987793	2718	2.718	0.1842411279301627
X	0.01778121308990451	84114	84.114	0.05957047094173065
X	0.01795425331898205	13512	13.512	0.10993835573976718
X	0.017487274519960124	1734	1.734	0.21605169517794567
X	0.017701715026776074	37746	37.746	0.07769292457377769
X	0.017798339474979556	94880	94.88	0.057244654252912294
X	0.01790599950530762	174427	174.427	0.04682325691801419
X	0.01954884910347999	149531	149.531	0.05075318703623343
X	0.01760355720604462	157198	157.198	0.048200455221079
X	0.017586879014350087	22465	22.465	0.09216388733130937
X	0.01776218974784591	31954	31.954	0.08222245215286261
X	0.017791167803916	131764	131.764	0.05130218886656364
X	0.01958748457824714	178479	178.479	0.047877395223464944
X	0.017666977327637886	15559	15.559	0.10432623238150876
X	0.01783794273223775	235250	235.25	0.04232578722754358
X	0.01833025473617741	375446	375.446	0.03654884632662189
X	0.01944407722036098	165589	165.589	0.04896870487156666
X	0.017746139663846378	116828	116.828	0.05335629328055244
X	0.01734615135255526	46125	46.125	0.07218089300892233
X	0.017661959063930917	11077	11.077	0.11682583829893078
X	0.01773155476228781	5943	5.943	0.14396172077554306
X	0.017458435171431058	5967	5.967	0.1430264718171191
X	0.01779579416395984	113616	113.616	0.05390461635959733
X	0.017807603734094665	100567	100.567	0.05615434027311045
X	0.0177305955315051	221505	221.505	0.04309695599013889
X	0.017744030092683924	71426	71.426	0.06286351275013677
X	0.017754195092720946	51925	51.925	0.0699264521752132
X	0.01753638496403762	12855	12.855	0.11090624375286366
X	0.01779291150086114	119191	119.191	0.0530478579205675
X	0.01769136948670427	7529	7.529	0.13294582964870091
X	0.01761637346891449	10771	10.771	0.11782030996998523
X	0.019188121327655492	98043	98.043	0.05805932130092083
X	0.017789836862390385	48573	48.573	0.07154715118456216
X	0.018487220681941766	3181	3.181	0.17979156748938263
X	0.01779308665367599	14736	14.736	0.10648552105314577
X	0.01946679280558711	496939	496.939	0.033962301121227216
X	0.01774603297975149	14429	14.429	0.10714083259300002
X	0.019403514660909713	100111	100.111	0.057871694816476256
X	0.016971935708866286	3278	3.278	0.1729978913395886
X	0.017496259755765183	4885	4.885	0.15300074654678614
X	0.017270074109876236	5493	5.493	0.14649682106414533
X	0.01764950158748578	3470	3.47	0.17197592614980842
X	0.017758623371642718	15871	15.871	0.10381696656038455
X	0.01749041567024507	44814	44.814	0.07307948424571423
X	0.01756110294705373	10314	10.314	0.11941023524313805
X	0.01780009566431869	164763	164.763	0.047627108430853764
X	0.01777565495429169	7012	7.012	0.13635191277563058
X	0.017976911501892325	61433	61.433	0.06639027264026572
X	0.01742643561113067	11398	11.398	0.11520198733357871
X	0.017594640565623	99575	99.575	0.056114706366789176
X	0.01711450937674047	5512	5.512	0.1458876118148331
X	0.017649413108874048	15622	15.622	0.1041512640019201
X	0.01771644362500558	28135	28.135	0.08571231183037799
X	0.01778445433958381	25889	25.889	0.08823517602336495
X	0.017757024559239207	55205	55.205	0.06851682979351637
X	0.01778257078026244	629376	629.376	0.030457537623418414
X	0.017639423590692656	12470	12.47	0.11225507352225834
X	0.017315225184887362	4792	4.792	0.15345125506833526
X	0.017780944300936883	26876	26.876	0.08713582144099573
X	0.017437518068585428	20132	20.132	0.09532333969418245
X	0.018892206789069683	373268	373.268	0.03699025372702072
X	0.0175513340642577	6107	6.107	0.1421763642337241
X	0.018435505589374342	88286	88.286	0.05932727541804054
X	0.019815276623797704	210482	210.482	0.04549130469175842
X	0.017582537786939433	3942	3.942	0.16460953186829477
X	0.01741733030400121	266452	266.452	0.04028291294855251
X	0.017480522422797344	15904	15.904	0.10320070883425471
X	0.018392302385706074	426207	426.207	0.035075605221246975
X	0.017519849215356386	10844	10.844	0.11734023447983687
X	0.01762162805846016	19591	19.591	0.09653018575092338
X	0.01721659541952098	9591	9.591	0.12153306786885391
X	0.017435559910606267	4208	4.208	0.1606152294444468
X	0.01730795742396934	7636	7.636	0.13135904008782567
X	0.017794265754908053	30283	30.283	0.08375814658243566
X	0.017674470257159798	3887	3.887	0.16567003160265786
X	0.017778105167873686	106718	106.718	0.055023633338773345
X	0.01761179513608991	12129	12.129	0.11323817887644509
X	0.017594365993542003	6609	6.609	0.13859448743331856
X	0.01771626091906921	18168	18.168	0.09916421589320068
X	0.01775817438964551	159368	159.368	0.04812075116178904
X	0.01848341824244476	118018	118.018	0.053902802656405235
X	0.017816178741976087	256254	256.254	0.041119513649323244
X	0.01776392623820732	10693	10.693	0.11843492738627895
X	0.01776898865672576	7208	7.208	0.13508775285516153
X	0.017578372786770182	3179	3.179	0.1768327554058894
X	0.01760448878361594	3206	3.206	0.1764222270421918
X	0.017920939276895188	150920	150.92	0.04915163744751835
X	0.017513305853371408	5952	5.952	0.14329633072777057
X	0.0176248395351488	21330	21.33	0.09383786224634923
X	0.01771733800415538	19738	19.738	0.09646396493103788
X	0.017577417818700994	20796	20.796	0.09454932276726934
X	0.017756560382159055	27955	27.955	0.08596066804916438
X	0.01775259519241199	4504	4.504	0.1579626785572131
X	0.01805883666520221	142513	142.513	0.05022790866692825
X	0.018588125349859597	48687	48.687	0.0725450068156015
X	0.017629238884867338	5712	5.712	0.1455956684414782
X	0.01778414918627082	18033	18.033	0.09953787618301074
X	0.017739339289566818	59396	59.396	0.06684363830785647
X	0.018503780367356646	90133	90.133	0.058991876738511556
X	0.01837975886047086	376869	376.869	0.036535618572620894
X	0.017812142594773223	101906	101.906	0.05591205689349219
X	0.017767013236373592	4419	4.419	0.1590120883924279
X	0.017801411879497617	29961	29.961	0.08406838698733254
X	0.0175529695609432	51189	51.189	0.06999354414995276
X	0.017755121038756344	8119	8.119	0.12979970296646753
X	0.0175478247000924	9911	9.911	0.12097675665114867
X	0.017622089862503264	5611	5.611	0.14644426914557085
X	0.01779886085897975	19018	19.018	0.09781582487316143
X	0.01752734950321984	36202	36.202	0.0785225800408748
X	0.01708421683591761	3471	3.471	0.17010359301016612
X	0.01759786188768439	12280	12.28	0.11274237501694459
X	0.017819094194450564	7564	7.564	0.13305913662696434
X	0.0178003792068825	14303	14.303	0.10756410597048179
X	0.017354545118408587	3915	3.915	0.16427084437333503
X	0.01761073774946871	23057	23.057	0.09140955262237564
X	0.017480870440805457	21725	21.725	0.0930110234077448
X	0.017575690689031233	9506	9.506	0.12273587704167595
X	0.01748849084909242	6322	6.322	0.14037807442403835
X	0.018356953538101482	9108	9.108	0.12631624170107233
X	0.018418760594668283	22828	22.828	0.0930959625005423
X	0.017822566146749128	118423	118.423	0.053191804710246364
X	0.017711713078690624	6466	6.466	0.13991844051186816
X	0.01713124727226612	1867	1.867	0.20935414083409565
X	0.01756234174832093	4391	4.391	0.15873510938473337
X	0.017721472791035045	70572	70.572	0.0630893126148858
X	0.018741764868100058	273769	273.769	0.04090804626588571
X	0.01765470731072684	79159	79.159	0.06064384665723323
X	0.017801294104125943	7810	7.81	0.1316033205105085
X	0.01779463937098673	172591	172.591	0.0468910922068302
X	0.01782235879572778	140600	140.6	0.05023348250752402
X	0.017323238602719653	2618	2.618	0.18773841558777024
X	0.01946110547451653	7263	7.263	0.13889410728013066
X	0.017787620832983044	35508	35.508	0.07942012215287421
X	0.017750518093389418	68455	68.455	0.06376787619151089
X	0.01766951760831245	107413	107.413	0.05479268915285613
X	0.019299654273932185	148172	148.172	0.050690642446690325
X	0.01779152578206001	29670	29.67	0.08432672151511733
X	0.01760643199123386	4599	4.599	0.15643574014850925
X	0.017182474428404204	2922	2.922	0.18049638083647557
X	0.01759916148512852	35440	35.44	0.0791892240890723
X	0.017553107261349492	12773	12.773	0.1111783859559341
X	0.017733576463971806	15798	15.798	0.10392772185792136
X	0.017794007383570638	96821	96.821	0.05685492193058005
X	0.017781967995458354	449134	449.134	0.03408275011422597
X	0.017543425602020643	7690	7.69	0.13164246925510553
X	0.016849511448604637	1782	1.782	0.21145917708615286
X	0.017755431014639986	30162	30.162	0.08380894258904337
X	0.019592062144853584	51324	51.324	0.07254150326109016
X	0.017610275740142584	52565	52.565	0.06945281120486624
X	0.017575630647057503	11380	11.38	0.1155906976875529
X	0.017447445462345227	3948	3.948	0.1641036478847747
X	0.017792060240120904	36091	36.091	0.07899672887908207
X	0.017567297880977672	4684	4.684	0.15536839988649048
X	0.01810221158582089	37582	37.582	0.07838818029826088
X	0.017727158329195313	6272	6.272	0.1413875030098262
X	0.01778470824675841	13440	13.44	0.10978655531738916
X	0.017881705179228432	7415	7.415	0.13410117761216142
X	0.017622434381320003	8821	8.821	0.1259455111538501
X	0.01740873634616359	8607	8.607	0.12646542708334207
X	0.017841964467492723	127129	127.129	0.05196760231397504
X	0.019562181234891642	202269	202.269	0.045901757782793394
X	0.01781451426497591	75081	75.081	0.0619081717791518
X	0.017903750014750013	115146	115.146	0.05377309699153366
X	0.01856548546623914	19978	19.978	0.09758538212518199
X	0.017361973336849783	11448	11.448	0.11489200842358152
X	0.017629687670217838	15215	15.215	0.10503262522794174
X	0.017818046710961596	106334	106.334	0.05513101403976977
X	0.017630034441759587	31873	31.873	0.0820874451928502
X	0.017800750474377156	22805	22.805	0.09207380237900582
X	0.017547462779843915	10331	10.331	0.11931379407637789
X	0.019537073096587233	832269	832.269	0.02863289501925491
X	0.01772718719522575	18557	18.557	0.09848664752069805
X	0.01777122440891425	54241	54.241	0.06893872088984909
X	0.01777506260825613	102219	102.219	0.055816144852576494
X	0.017733188006355306	45152	45.152	0.07323240138753699
X	0.01775771978124135	22003	22.003	0.09310402002197674
X	0.019914473339555715	586085	586.085	0.03238938782431897
X	0.017771696477923062	134374	134.374	0.050949260011390385
X	0.017753381804440038	73566	73.566	0.06225888187052613
X	0.017565712174632094	10590	10.59	0.11837407564579816
X	0.017790812288847328	34381	34.381	0.08028340020996107
X	0.017792127411324854	7236	7.236	0.13497182098560603
X	0.01772754363504167	141490	141.49	0.0500388828792376
X	0.019643586222355926	814095	814.095	0.02889672090243295
X	0.01789507630430788	66699	66.699	0.06449678657102849
X	0.017920737981627678	135504	135.504	0.05094887414461277
X	0.017790719771251306	85566	85.566	0.05924214338066166
X	0.01762546247681804	23758	23.758	0.09052674000726303
X	0.017633186680935938	22889	22.889	0.09167156847560291
X	0.01735686623930752	10886	10.886	0.11682460703878673
X	0.018778588002002772	40105	40.105	0.07765238593780933
X	0.017885432606927452	291642	291.642	0.03943511640443837
X	0.017631687124223142	79688	79.688	0.06048304505416361
X	0.017791201938966272	393456	393.456	0.03562621672890857
X	0.017736741480515326	123161	123.161	0.05241636217299737
X	0.01769219039969404	40325	40.325	0.07598637783972
X	0.017485809692315817	13434	13.434	0.10918428501064766
X	0.01777351327129584	53757	53.757	0.06914796755412529
X	0.017801608891388317	118506	118.506	0.05315853111370647
X	0.0173693412852254	10698	10.698	0.1175331164041212
X	0.019404546832607977	413707	413.707	0.036063694202827075
X	0.017829103279415944	21904	21.904	0.0933688529174817
X	0.017337321564707728	28106	28.106	0.08512575709362209
X	0.01775885869999569	5508	5.508	0.14773170383944054
X	0.017528214061214364	10093	10.093	0.12020036829946111
X	0.017116075492651775	1807	1.807	0.21158360555641548
X	0.017550737936825597	3647	3.647	0.16883139388765803
X	0.01752903353007997	16716	16.716	0.1015956710184631
X	0.01768545333925925	40954	40.954	0.07558575525857014
X	0.019376846143487884	2202174	2202.174	0.020644790813815237
X	0.017693588704129765	8893	8.893	0.12577351889369062
X	0.017830675140966014	161286	161.286	0.04799437849362846
X	0.01791544414272114	8666	8.666	0.1273902817614482
X	0.01775807605350191	21783	21.783	0.0934170338079854
X	0.01725043953154944	7015	7.015	0.13497629303697098
X	0.0176887997949876	155496	155.496	0.0484536370274219
X	0.017742379048479563	23645	23.645	0.0908707637682805
X	0.01849361158572169	524102	524.102	0.03279960686903975
X	0.017584663020506625	4820	4.82	0.1539438477492125
X	0.017837781752282966	136511	136.511	0.05074474400877046
X	0.01764186456370316	18278	18.278	0.09882616368601439
X	0.0177175116162215	106133	106.133	0.05506184530377324
X	0.017688137564373884	239199	239.199	0.04197340957504887
X	0.01759783173132296	27402	27.402	0.0862764564767038
X	0.018858876355643085	12346	12.346	0.11516747543243049
X	0.018160035681923474	73709	73.709	0.0626900658603212
X	0.017753172794616006	12605	12.605	0.11209279803168792
X	0.017508006056732226	19748	19.748	0.09606632781275676
X	0.017527030371569337	144886	144.886	0.04945694721392298
X	0.017804977414133605	8350	8.35	0.12871178617829734
X	0.017785947702818508	55812	55.812	0.068304577364861
X	0.017382990948392758	32180	32.18	0.08144157328442594
X	0.0184495961574225	93163	93.163	0.05828826563391022
X	0.017815204787057	151445	151.445	0.0489980308128014
X	0.017587214484892977	50237	50.237	0.07047868425426194
X	0.01777506940522477	12993	12.993	0.1110113256658636
X	0.017625503351841453	82402	82.402	0.059804600269793735
X	0.017743357962452435	80358	80.358	0.06044154608928999
X	0.018834238308548912	86732	86.732	0.060106702103632885
X	0.017757267268922447	34235	34.235	0.08034680279174254
X	0.017597871677722494	24800	24.8	0.08919410566148031
X	0.01775439631329544	25612	25.612	0.08850221474440788
X	0.017539770083293833	133319	133.319	0.05086010856999329
X	0.017711529506604404	71765	71.765	0.06272602872474556
X	0.01766175285739563	19012	19.012	0.09757427489232585
X	0.017804900536231665	8714	8.714	0.12689387665334723
X	0.017235486480945375	4342	4.342	0.15833535823526732
X	0.017559055910980986	46014	46.014	0.07253323672292797
X	0.01774596763368319	87834	87.834	0.05867850061349033
X	0.01779902136014121	72869	72.869	0.06251023115475636
X	0.017632490955420216	8267	8.267	0.12872273064321405
X	0.017792039599292844	21650	21.65	0.0936675754021028
X	0.017560418549382534	25823	25.823	0.08793789305159762
X	0.019601774887258738	49505	49.505	0.07343145115288555
X	0.017638218819835072	27265	27.265	0.0864867826370087
X	0.017792470776713663	54848	54.848	0.06871082373706859
X	0.017464445757418942	9396	9.396	0.12295247248635524
X	0.01780078625372333	28619	28.619	0.08536140936063011
X	0.0171477417528031	2822	2.822	0.1824805753801605
X	0.017272568888598683	5502	5.502	0.14642394912360088
X	0.017834727339796835	171297	171.297	0.04704414418571562
X	0.017737556613086614	15287	15.287	0.10508091432928877
X	0.01782620617289513	88014	88.014	0.05872671685223823
X	0.01776722180865618	3487	3.487	0.17207688100218996
X	0.017488108536618488	6287	6.287	0.14063706461674075
X	0.017674233769672112	23258	23.258	0.0912548753582586
X	0.017683249636103703	56256	56.256	0.06799303225284804
X	0.017767311590235594	34409	34.409	0.08022626169222907
X	0.017712118836300198	4586	4.586	0.15689610263263198
X	0.017546955453603356	10225	10.225	0.11972352235458761
X	0.01847251116907646	39171	39.171	0.07783717994810574
X	0.01772405740946694	20288	20.288	0.09559634853054685
X	0.017752623258739626	75060	75.06	0.06184216125602273
X	0.017798012736275307	5184	5.184	0.15085817172233254
X	0.017784889782473858	6569	6.569	0.13937470850091996
X	0.01761780908149038	58361	58.361	0.06708257499653669
X	0.017574517384647614	5855	5.855	0.14425100118197873
X	0.017835451935411206	117015	117.015	0.05341716812958929
X	0.01756405668239443	17101	17.101	0.10089456548537659
X	0.01776323987706676	61107	61.107	0.06624357149808528
X	0.018492961666090654	102764	102.764	0.05645752337177729
X	0.017732177513926764	66263	66.263	0.06444120587218367
X	0.017786610000605253	104369	104.369	0.055442212541668166
X	0.018183558367444456	97985	97.985	0.05703915625714304
X	0.01772741782616667	5000	5.0	0.15248430998995233
X	0.01760306127960895	43659	43.659	0.07387624375634086
X	0.017778160468530934	24545	24.545	0.08980653254554248
X	0.017653078400917406	50861	50.861	0.07027677956800708
X	0.01773345980515032	116286	116.286	0.05342633065390936
X	0.017600310448363625	29966	29.966	0.083745957240191
X	0.01781071888614624	56511	56.511	0.06805334637016952
X	0.01777168228235783	17247	17.247	0.10100394243073338
X	0.017226541366377735	32088	32.088	0.08127403359405588
X	0.017501585194914973	4087	4.087	0.1623893647502282
X	0.017918364354220284	10075	10.075	0.12115772302756275
X	0.017767554203483162	123194	123.194	0.052442014243121726
X	0.018327978999272448	232322	232.322	0.04288855493566434
X	0.01758330370744576	15116	15.116	0.10516902913405096
X	0.017800348998741264	13926	13.926	0.10852606143987277
X	0.017777306924921178	22671	22.671	0.09221433340866723
X	0.01771105941085673	57284	57.284	0.06761925806477079
X	0.017798956477555035	50744	50.744	0.07052394762655127
X	0.01776411772290801	5359	5.359	0.14910308474177666
X	0.017801431119925448	35502	35.502	0.07944514575741485
X	0.017783530322751616	139535	139.535	0.05032436161043104
X	0.017524813161823253	7599	7.599	0.13211911340448243
X	0.017693108433555507	45570	45.57	0.07295275604245105
X	0.017797698727600283	16302	16.302	0.10296927479376002
time for making epsilon is 1.9961018562316895
epsilons are
[0.22246555208702845, 0.0973771904116057, 0.21219522791902262, 0.10482243211592975, 0.11609866917055398, 0.07375989825157737, 0.07849130133442524, 0.07820652996090202, 0.11487702301978034, 0.08420108478383274, 0.12693982275487012, 0.05160231392328651, 0.13994297360452498, 0.03711159991072803, 0.09349345193191116, 0.07066701548932058, 0.06950962067671596, 0.10368443283498331, 0.05173765844101151, 0.06215836075723377, 0.06389098074280847, 0.04400187426638568, 0.12546238136310417, 0.07587566666642645, 0.17465558541399082, 0.0633770863881998, 0.06620897105815604, 0.12271199404279007, 0.07633116235002389, 0.025092599695982296, 0.10452184617164254, 0.031234506706003858, 0.08532622402222763, 0.11929344132269036, 0.08217967426541352, 0.051386383067876545, 0.061570518535326996, 0.23237250022580558, 0.18285904570529607, 0.21478749323185506, 0.1981528319487067, 0.16859992611450403, 0.26013364111738996, 0.1736262093881433, 0.25650645532325206, 0.2059218737937829, 0.18350880315680276, 0.16488357093055286, 0.19265514377599216, 0.10134750159601282, 0.12639169929871716, 0.23659180353315046, 0.0986378585523562, 0.19495459685230726, 0.16903429430187555, 0.13210567737010814, 0.20561617733496437, 0.1700178491698889, 0.16921731486982688, 0.18138600832409055, 0.16272347364490844, 0.21896719456503191, 0.13351440002012196, 0.12649084951649794, 0.18160223038317874, 0.19133443889620294, 0.14654958444439656, 0.14579309949180638, 0.14732337240989618, 0.19473031243110533, 0.14733690120103385, 0.1781540611865744, 0.16334070049611837, 0.20518958485181818, 0.16743940300356958, 0.13586554254026234, 0.2855442132721422, 0.19870680740601263, 0.16851249094895165, 0.10614469621671183, 0.18318530957644139, 0.2056580353585358, 0.1792458165489997, 0.2091506003779586, 0.22833041895786577, 0.2044803717041604, 0.16855372100408467, 0.26937653508126463, 0.17628226880246692, 0.1659703622944125, 0.14012282575966833, 0.2158792553461824, 0.1744194952964184, 0.20178056632730268, 0.17532477487044665, 0.1746929487825775, 0.20107554694723792, 0.15538642721376716, 0.16934121463768725, 0.16738457449368901, 0.10638203108836569, 0.14806243544199466, 0.14026618881805925, 0.22740181365182702, 0.14843766265395955, 0.17694450888973912, 0.14051903836808796, 0.22324491419527137, 0.16265671411103436, 0.22228936371099542, 0.19398868996722482, 0.22787381156766814, 0.18741889607779166, 0.1880320468021942, 0.14791717677020838, 0.14877074540030977, 0.16036614652054174, 0.264029763797816, 0.14790790722575595, 0.22654529293612122, 0.1051977362471105, 0.18342490419849178, 0.2053478186018083, 0.20124063803396086, 0.2022324091700907, 0.20047831123540188, 0.23642425773449166, 0.17091682935729194, 0.13902103055392923, 0.19797612582284, 0.12949966638529728, 0.22450159173962145, 0.1880953466603442, 0.21924963940533196, 0.1673939481403365, 0.18017329003276428, 0.21959116840209786, 0.19293151614068615, 0.1988599077017204, 0.23502998805213612, 0.20169834763161573, 0.24779224284002777, 0.11736561646107178, 0.14023755777184668, 0.1911055446596794, 0.17388737040262428, 0.19681917398157947, 0.22364488840804836, 0.20133796346458643, 0.19388809400561619, 0.20145857296129255, 0.19816219383719905, 0.19014791346973217, 0.12126678745635136, 0.18147015127032448, 0.10762882337056442, 0.18070682336645588, 0.16536396876243, 0.1750715570194384, 0.2841574091596577, 0.11313171734191409, 0.206493416105383, 0.1864324344405146, 0.24889276708985322, 0.1428929819848044, 0.19918064213851194, 0.21822309126596534, 0.15533558272033535, 0.17357759234407147, 0.2644043783936947, 0.29926029587053143, 0.2469380879670127, 0.1328918478431232, 0.20395772940168802, 0.1641248237619324, 0.12713797381192457, 0.15399193374699882, 0.20108813721440436, 0.14982599059201787, 0.18749245404961687, 0.17186722656265155, 0.16946968352318925, 0.17313155651981993, 0.21775533851378798, 0.17123695660733113, 0.1461032544301397, 0.21125521170788786, 0.17489488469331205, 0.14010710857749306, 0.1404707757468347, 0.1865284683044994, 0.2948840220217094, 0.16955093768810292, 0.1277447233309248, 0.1388346048344038, 0.1775943467552402, 0.18426669117770883, 0.2350938803855052, 0.1952548181285199, 0.2541574836858203, 0.1782891863485245, 0.20024518539365987, 0.15045612837647282, 0.2032930551573307, 0.13170484405982047, 0.1990258167765021, 0.17062692576516783, 0.1842590618760814, 0.1731971422963236, 0.15264259340475542, 0.14860854124830314, 0.13172157753421182, 0.1754261401471815, 0.17453645487603114, 0.2418766371639047, 0.19495244676132148, 0.25354554990286926, 0.22953222471084025, 0.1500170745970228, 0.1950198831197253, 0.1996772963756458, 0.2511787298696685, 0.1399300290450199, 0.17727802714049345, 0.19762339482673347, 0.18225991880458345, 0.23042458808679486, 0.20669114929518098, 0.17497763600491192, 0.2633965313788555, 0.18256347587478408, 0.17156098125182573, 0.16299726069021273, 0.16548834067714494, 0.2131700817483601, 0.16653307175716994, 0.15393830482276494, 0.16534373576551792, 0.1818186741367434, 0.0959415395408218, 0.07544159524638622, 0.15304044071711834, 0.1352002940918297, 0.28740814822633654, 0.13447059297744673, 0.11347982315831386, 0.05150135244540183, 0.19470747891339305, 0.0505373615828136, 0.08783084815808247, 0.07992849352403848, 0.07737562001495714, 0.18023922815610607, 0.15275613708799837, 0.048883667074634735, 0.03174672799211602, 0.17172725961977667, 0.0767055592444692, 0.10351414258595959, 0.08681711348090587, 0.0518548189292533, 0.0684890835894289, 0.08156442856668497, 0.10761699164386437, 0.19313535543542665, 0.05212397264348432, 0.11649917640755297, 0.19499704612920152, 0.09200999759205915, 0.0645982513784506, 0.0557511836671684, 0.059562178971855444, 0.1675418586935026, 0.08623069283532825, 0.14490953764971554, 0.07462950208755495, 0.06632396630544202, 0.1425286270046467, 0.056989274555725285, 0.28562434954630284, 0.1376842524980855, 0.06188278611066712, 0.07536707438789603, 0.06131773873450999, 0.18265877657105992, 0.04389916482060589, 0.10005367751678208, 0.08695664184136756, 0.06453786769898698, 0.2056606543940364, 0.054960578218132765, 0.04759002153536107, 0.040974133670982314, 0.06653684559298599, 0.08300464653677112, 0.13151784863935542, 0.1412230479123632, 0.05638409846066499, 0.12462131210630159, 0.0904126015992512, 0.07274287116130518, 0.09957151740271382, 0.10572097244623749, 0.15610659543166835, 0.06099365927484563, 0.05988866308625871, 0.09361627193726757, 0.13694934683563162, 0.07343080357218673, 0.09989737003628388, 0.047211508875222415, 0.12350392330717579, 0.08076236160724747, 0.11779938632703706, 0.0621489830981677, 0.0786608773208385, 0.0965274388071528, 0.05557845122544695, 0.04765791346563235, 0.14854183809133364, 0.04958367654009863, 0.05389240903159083, 0.04162371079828256, 0.10123786784626249, 0.1902992638501917, 0.10462187943564731, 0.09382736251155772, 0.08763419349405042, 0.06500512751916424, 0.17133846776220474, 0.06596599373448414, 0.05256342115113971, 0.14572569552243403, 0.14182168009827748, 0.0730731153696862, 0.11342961379927234, 0.13968327118371912, 0.1059381851126287, 0.16767447056687354, 0.10539116680450533, 0.08192386709984718, 0.0980410396715724, 0.08358943275237847, 0.1407123042828393, 0.027267747219156074, 0.1099034615843708, 0.05657947380640363, 0.20576861564893106, 0.13552546754001038, 0.14899952829907195, 0.11030198734246141, 0.1861082917001459, 0.060972013026092974, 0.15942467037969113, 0.13343174789624765, 0.045654349473307335, 0.06883984904803113, 0.08293113150765177, 0.05596054460972161, 0.04354962348254101, 0.14926911517412716, 0.1504181742675172, 0.09427693850000528, 0.20665009358144673, 0.13217177631204036, 0.11659388841367582, 0.18468096712069315, 0.11480304261643251, 0.1265019340991939, 0.14679659019891803, 0.04689183735652031, 0.12168502537638955, 0.0668786214541992, 0.20126955695011095, 0.1827465866274437, 0.056046918456144734, 0.034262218862102206, 0.03551560131033522, 0.11163376840117349, 0.0745969575767511, 0.17486114209664258, 0.16749808918049358, 0.04431810661912549, 0.10373745262132111, 0.14289613494989245, 0.07344052458073023, 0.0342961921327198, 0.08499508184171245, 0.11156067372276225, 0.11144296547455233, 0.11310117175848149, 0.15194542902364527, 0.05048765456353667, 0.0904092867494601, 0.1842411279301627, 0.05957047094173065, 0.10993835573976718, 0.21605169517794567, 0.07769292457377769, 0.057244654252912294, 0.04682325691801419, 0.05075318703623343, 0.048200455221079, 0.09216388733130937, 0.08222245215286261, 0.05130218886656364, 0.047877395223464944, 0.10432623238150876, 0.04232578722754358, 0.03654884632662189, 0.04896870487156666, 0.05335629328055244, 0.07218089300892233, 0.11682583829893078, 0.14396172077554306, 0.1430264718171191, 0.05390461635959733, 0.05615434027311045, 0.04309695599013889, 0.06286351275013677, 0.0699264521752132, 0.11090624375286366, 0.0530478579205675, 0.13294582964870091, 0.11782030996998523, 0.05805932130092083, 0.07154715118456216, 0.17979156748938263, 0.10648552105314577, 0.033962301121227216, 0.10714083259300002, 0.057871694816476256, 0.1729978913395886, 0.15300074654678614, 0.14649682106414533, 0.17197592614980842, 0.10381696656038455, 0.07307948424571423, 0.11941023524313805, 0.047627108430853764, 0.13635191277563058, 0.06639027264026572, 0.11520198733357871, 0.056114706366789176, 0.1458876118148331, 0.1041512640019201, 0.08571231183037799, 0.08823517602336495, 0.06851682979351637, 0.030457537623418414, 0.11225507352225834, 0.15345125506833526, 0.08713582144099573, 0.09532333969418245, 0.03699025372702072, 0.1421763642337241, 0.05932727541804054, 0.04549130469175842, 0.16460953186829477, 0.04028291294855251, 0.10320070883425471, 0.035075605221246975, 0.11734023447983687, 0.09653018575092338, 0.12153306786885391, 0.1606152294444468, 0.13135904008782567, 0.08375814658243566, 0.16567003160265786, 0.055023633338773345, 0.11323817887644509, 0.13859448743331856, 0.09916421589320068, 0.04812075116178904, 0.053902802656405235, 0.041119513649323244, 0.11843492738627895, 0.13508775285516153, 0.1768327554058894, 0.1764222270421918, 0.04915163744751835, 0.14329633072777057, 0.09383786224634923, 0.09646396493103788, 0.09454932276726934, 0.08596066804916438, 0.1579626785572131, 0.05022790866692825, 0.0725450068156015, 0.1455956684414782, 0.09953787618301074, 0.06684363830785647, 0.058991876738511556, 0.036535618572620894, 0.05591205689349219, 0.1590120883924279, 0.08406838698733254, 0.06999354414995276, 0.12979970296646753, 0.12097675665114867, 0.14644426914557085, 0.09781582487316143, 0.0785225800408748, 0.17010359301016612, 0.11274237501694459, 0.13305913662696434, 0.10756410597048179, 0.16427084437333503, 0.09140955262237564, 0.0930110234077448, 0.12273587704167595, 0.14037807442403835, 0.12631624170107233, 0.0930959625005423, 0.053191804710246364, 0.13991844051186816, 0.20935414083409565, 0.15873510938473337, 0.0630893126148858, 0.04090804626588571, 0.06064384665723323, 0.1316033205105085, 0.0468910922068302, 0.05023348250752402, 0.18773841558777024, 0.13889410728013066, 0.07942012215287421, 0.06376787619151089, 0.05479268915285613, 0.050690642446690325, 0.08432672151511733, 0.15643574014850925, 0.18049638083647557, 0.0791892240890723, 0.1111783859559341, 0.10392772185792136, 0.05685492193058005, 0.03408275011422597, 0.13164246925510553, 0.21145917708615286, 0.08380894258904337, 0.07254150326109016, 0.06945281120486624, 0.1155906976875529, 0.1641036478847747, 0.07899672887908207, 0.15536839988649048, 0.07838818029826088, 0.1413875030098262, 0.10978655531738916, 0.13410117761216142, 0.1259455111538501, 0.12646542708334207, 0.05196760231397504, 0.045901757782793394, 0.0619081717791518, 0.05377309699153366, 0.09758538212518199, 0.11489200842358152, 0.10503262522794174, 0.05513101403976977, 0.0820874451928502, 0.09207380237900582, 0.11931379407637789, 0.02863289501925491, 0.09848664752069805, 0.06893872088984909, 0.055816144852576494, 0.07323240138753699, 0.09310402002197674, 0.03238938782431897, 0.050949260011390385, 0.06225888187052613, 0.11837407564579816, 0.08028340020996107, 0.13497182098560603, 0.0500388828792376, 0.02889672090243295, 0.06449678657102849, 0.05094887414461277, 0.05924214338066166, 0.09052674000726303, 0.09167156847560291, 0.11682460703878673, 0.07765238593780933, 0.03943511640443837, 0.06048304505416361, 0.03562621672890857, 0.05241636217299737, 0.07598637783972, 0.10918428501064766, 0.06914796755412529, 0.05315853111370647, 0.1175331164041212, 0.036063694202827075, 0.0933688529174817, 0.08512575709362209, 0.14773170383944054, 0.12020036829946111, 0.21158360555641548, 0.16883139388765803, 0.1015956710184631, 0.07558575525857014, 0.020644790813815237, 0.12577351889369062, 0.04799437849362846, 0.1273902817614482, 0.0934170338079854, 0.13497629303697098, 0.0484536370274219, 0.0908707637682805, 0.03279960686903975, 0.1539438477492125, 0.05074474400877046, 0.09882616368601439, 0.05506184530377324, 0.04197340957504887, 0.0862764564767038, 0.11516747543243049, 0.0626900658603212, 0.11209279803168792, 0.09606632781275676, 0.04945694721392298, 0.12871178617829734, 0.068304577364861, 0.08144157328442594, 0.05828826563391022, 0.0489980308128014, 0.07047868425426194, 0.1110113256658636, 0.059804600269793735, 0.06044154608928999, 0.060106702103632885, 0.08034680279174254, 0.08919410566148031, 0.08850221474440788, 0.05086010856999329, 0.06272602872474556, 0.09757427489232585, 0.12689387665334723, 0.15833535823526732, 0.07253323672292797, 0.05867850061349033, 0.06251023115475636, 0.12872273064321405, 0.0936675754021028, 0.08793789305159762, 0.07343145115288555, 0.0864867826370087, 0.06871082373706859, 0.12295247248635524, 0.08536140936063011, 0.1824805753801605, 0.14642394912360088, 0.04704414418571562, 0.10508091432928877, 0.05872671685223823, 0.17207688100218996, 0.14063706461674075, 0.0912548753582586, 0.06799303225284804, 0.08022626169222907, 0.15689610263263198, 0.11972352235458761, 0.07783717994810574, 0.09559634853054685, 0.06184216125602273, 0.15085817172233254, 0.13937470850091996, 0.06708257499653669, 0.14425100118197873, 0.05341716812958929, 0.10089456548537659, 0.06624357149808528, 0.05645752337177729, 0.06444120587218367, 0.055442212541668166, 0.05703915625714304, 0.15248430998995233, 0.07387624375634086, 0.08980653254554248, 0.07027677956800708, 0.05342633065390936, 0.083745957240191, 0.06805334637016952, 0.10100394243073338, 0.08127403359405588, 0.1623893647502282, 0.12115772302756275, 0.052442014243121726, 0.04288855493566434, 0.10516902913405096, 0.10852606143987277, 0.09221433340866723, 0.06761925806477079, 0.07052394762655127, 0.14910308474177666, 0.07944514575741485, 0.05032436161043104, 0.13211911340448243, 0.07295275604245105, 0.10296927479376002]
0.0991332705499087
Making ranges
torch.Size([30407, 2])
We keep 6.35e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([3569, 2])
We keep 1.72e+05/2.58e+06 =  6% of the original kernel matrix.

torch.Size([11459, 2])
We keep 1.06e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([28211, 2])
We keep 8.29e+06/3.71e+08 =  2% of the original kernel matrix.

torch.Size([30648, 2])
We keep 6.83e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([4015, 2])
We keep 1.98e+05/3.36e+06 =  5% of the original kernel matrix.

torch.Size([12020, 2])
We keep 1.14e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([19812, 2])
We keep 8.14e+06/2.39e+08 =  3% of the original kernel matrix.

torch.Size([24790, 2])
We keep 5.69e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([18935, 2])
We keep 7.68e+06/1.43e+08 =  5% of the original kernel matrix.

torch.Size([24407, 2])
We keep 4.40e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([66759, 2])
We keep 2.96e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([45615, 2])
We keep 1.37e+07/9.42e+08 =  1% of the original kernel matrix.

torch.Size([53956, 2])
We keep 2.05e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([41355, 2])
We keep 1.13e+07/7.55e+08 =  1% of the original kernel matrix.

torch.Size([56035, 2])
We keep 1.88e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([42257, 2])
We keep 1.13e+07/7.64e+08 =  1% of the original kernel matrix.

torch.Size([17052, 2])
We keep 4.36e+06/1.35e+08 =  3% of the original kernel matrix.

torch.Size([22975, 2])
We keep 4.45e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([35409, 2])
We keep 5.21e+07/9.55e+08 =  5% of the original kernel matrix.

torch.Size([33149, 2])
We keep 9.10e+06/6.34e+08 =  1% of the original kernel matrix.

torch.Size([14249, 2])
We keep 2.54e+06/7.34e+07 =  3% of the original kernel matrix.

torch.Size([20981, 2])
We keep 3.52e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([169336, 2])
We keep 3.15e+08/1.66e+10 =  1% of the original kernel matrix.

torch.Size([73256, 2])
We keep 3.31e+07/2.64e+09 =  1% of the original kernel matrix.

torch.Size([12572, 2])
We keep 1.42e+06/4.40e+07 =  3% of the original kernel matrix.

torch.Size([19690, 2])
We keep 2.93e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([526850, 2])
We keep 1.19e+09/1.21e+11 =  0% of the original kernel matrix.

torch.Size([133283, 2])
We keep 8.08e+07/7.15e+09 =  1% of the original kernel matrix.

torch.Size([31454, 2])
We keep 8.28e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([31746, 2])
We keep 7.24e+06/4.46e+08 =  1% of the original kernel matrix.

torch.Size([75437, 2])
We keep 3.93e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([48129, 2])
We keep 1.47e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([80458, 2])
We keep 4.67e+07/2.81e+09 =  1% of the original kernel matrix.

torch.Size([50010, 2])
We keep 1.53e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([23807, 2])
We keep 5.36e+06/2.53e+08 =  2% of the original kernel matrix.

torch.Size([27313, 2])
We keep 5.68e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([169747, 2])
We keep 2.50e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([73983, 2])
We keep 3.34e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([112004, 2])
We keep 9.29e+07/5.62e+09 =  1% of the original kernel matrix.

torch.Size([59086, 2])
We keep 2.04e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([84832, 2])
We keep 8.65e+07/4.62e+09 =  1% of the original kernel matrix.

torch.Size([49790, 2])
We keep 1.93e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([301562, 2])
We keep 4.33e+08/4.44e+10 =  0% of the original kernel matrix.

torch.Size([101782, 2])
We keep 4.94e+07/4.33e+09 =  1% of the original kernel matrix.

torch.Size([15460, 2])
We keep 2.17e+06/7.95e+07 =  2% of the original kernel matrix.

torch.Size([21857, 2])
We keep 3.62e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([57836, 2])
We keep 5.65e+07/1.79e+09 =  3% of the original kernel matrix.

torch.Size([42640, 2])
We keep 1.29e+07/8.68e+08 =  1% of the original kernel matrix.

torch.Size([5425, 2])
We keep 2.17e+06/1.07e+07 = 20% of the original kernel matrix.

torch.Size([13137, 2])
We keep 1.51e+06/6.71e+07 =  2% of the original kernel matrix.

torch.Size([107865, 2])
We keep 5.80e+07/4.88e+09 =  1% of the original kernel matrix.

torch.Size([58028, 2])
We keep 1.91e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([88426, 2])
We keep 5.34e+07/3.75e+09 =  1% of the original kernel matrix.

torch.Size([51876, 2])
We keep 1.74e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([16516, 2])
We keep 3.02e+06/9.38e+07 =  3% of the original kernel matrix.

torch.Size([22751, 2])
We keep 3.78e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([47980, 2])
We keep 9.00e+07/1.56e+09 =  5% of the original kernel matrix.

torch.Size([38448, 2])
We keep 1.15e+07/8.12e+08 =  1% of the original kernel matrix.

torch.Size([1892264, 2])
We keep 7.18e+09/1.27e+12 =  0% of the original kernel matrix.

torch.Size([266357, 2])
We keep 2.34e+08/2.32e+10 =  1% of the original kernel matrix.

torch.Size([21043, 2])
We keep 7.16e+06/2.32e+08 =  3% of the original kernel matrix.

torch.Size([25523, 2])
We keep 5.51e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([859279, 2])
We keep 2.86e+09/3.42e+11 =  0% of the original kernel matrix.

torch.Size([175992, 2])
We keep 1.28e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([40051, 2])
We keep 2.20e+07/8.02e+08 =  2% of the original kernel matrix.

torch.Size([35631, 2])
We keep 9.02e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([17991, 2])
We keep 3.00e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([23779, 2])
We keep 4.04e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([40117, 2])
We keep 1.92e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([35066, 2])
We keep 1.02e+07/6.51e+08 =  1% of the original kernel matrix.

torch.Size([175030, 2])
We keep 3.31e+08/1.72e+10 =  1% of the original kernel matrix.

torch.Size([74978, 2])
We keep 3.33e+07/2.69e+09 =  1% of the original kernel matrix.

torch.Size([117072, 2])
We keep 8.13e+07/5.82e+09 =  1% of the original kernel matrix.

torch.Size([60599, 2])
We keep 2.10e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([3209, 2])
We keep 1.21e+05/1.84e+06 =  6% of the original kernel matrix.

torch.Size([10903, 2])
We keep 9.36e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([5797, 2])
We keep 4.19e+05/8.19e+06 =  5% of the original kernel matrix.

torch.Size([13836, 2])
We keep 1.57e+06/5.88e+07 =  2% of the original kernel matrix.

torch.Size([3857, 2])
We keep 1.90e+05/3.18e+06 =  5% of the original kernel matrix.

torch.Size([11724, 2])
We keep 1.13e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([5049, 2])
We keep 2.52e+05/5.18e+06 =  4% of the original kernel matrix.

torch.Size([13219, 2])
We keep 1.34e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([5167, 2])
We keep 8.26e+05/1.18e+07 =  7% of the original kernel matrix.

torch.Size([11675, 2])
We keep 1.75e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([2449, 2])
We keep 8.08e+04/9.86e+05 =  8% of the original kernel matrix.

torch.Size([10001, 2])
We keep 7.63e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([6782, 2])
We keep 4.90e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([14627, 2])
We keep 1.76e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([2579, 2])
We keep 7.58e+04/1.09e+06 =  6% of the original kernel matrix.

torch.Size([10283, 2])
We keep 7.81e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([4548, 2])
We keep 2.94e+05/4.10e+06 =  7% of the original kernel matrix.

torch.Size([12735, 2])
We keep 1.18e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([6000, 2])
We keep 4.45e+05/7.99e+06 =  5% of the original kernel matrix.

torch.Size([14086, 2])
We keep 1.53e+06/5.80e+07 =  2% of the original kernel matrix.

torch.Size([8422, 2])
We keep 6.56e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([16328, 2])
We keep 1.98e+06/8.05e+07 =  2% of the original kernel matrix.

torch.Size([4851, 2])
We keep 3.58e+05/6.06e+06 =  5% of the original kernel matrix.

torch.Size([12796, 2])
We keep 1.44e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([24073, 2])
We keep 9.28e+06/2.89e+08 =  3% of the original kernel matrix.

torch.Size([27979, 2])
We keep 5.94e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([15075, 2])
We keep 2.52e+06/8.60e+07 =  2% of the original kernel matrix.

torch.Size([21616, 2])
We keep 3.82e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([2926, 2])
We keep 1.21e+05/1.65e+06 =  7% of the original kernel matrix.

torch.Size([10638, 2])
We keep 9.01e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([22548, 2])
We keep 1.18e+07/3.34e+08 =  3% of the original kernel matrix.

torch.Size([25300, 2])
We keep 6.08e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([5148, 2])
We keep 2.82e+05/5.38e+06 =  5% of the original kernel matrix.

torch.Size([13256, 2])
We keep 1.35e+06/4.76e+07 =  2% of the original kernel matrix.

torch.Size([7576, 2])
We keep 5.71e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([15404, 2])
We keep 1.88e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([11958, 2])
We keep 2.23e+06/5.48e+07 =  4% of the original kernel matrix.

torch.Size([18882, 2])
We keep 3.12e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([4328, 2])
We keep 2.49e+05/4.06e+06 =  6% of the original kernel matrix.

torch.Size([12372, 2])
We keep 1.23e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([7701, 2])
We keep 5.47e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([15652, 2])
We keep 1.85e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([7444, 2])
We keep 6.39e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([15379, 2])
We keep 1.86e+06/7.49e+07 =  2% of the original kernel matrix.

torch.Size([6156, 2])
We keep 4.97e+05/8.84e+06 =  5% of the original kernel matrix.

torch.Size([14205, 2])
We keep 1.62e+06/6.10e+07 =  2% of the original kernel matrix.

torch.Size([8425, 2])
We keep 6.89e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([16256, 2])
We keep 2.00e+06/8.36e+07 =  2% of the original kernel matrix.

torch.Size([3239, 2])
We keep 1.76e+05/2.68e+06 =  6% of the original kernel matrix.

torch.Size([10853, 2])
We keep 1.06e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([12378, 2])
We keep 2.16e+06/5.54e+07 =  3% of the original kernel matrix.

torch.Size([19340, 2])
We keep 3.17e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([15575, 2])
We keep 2.15e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([21968, 2])
We keep 3.63e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([6129, 2])
We keep 4.49e+05/8.71e+06 =  5% of the original kernel matrix.

torch.Size([14179, 2])
We keep 1.61e+06/6.06e+07 =  2% of the original kernel matrix.

torch.Size([5060, 2])
We keep 3.86e+05/6.30e+06 =  6% of the original kernel matrix.

torch.Size([12962, 2])
We keep 1.43e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([10728, 2])
We keep 1.66e+06/3.18e+07 =  5% of the original kernel matrix.

torch.Size([18349, 2])
We keep 2.52e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([10849, 2])
We keep 1.13e+06/3.06e+07 =  3% of the original kernel matrix.

torch.Size([18208, 2])
We keep 2.54e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([10601, 2])
We keep 1.66e+06/3.05e+07 =  5% of the original kernel matrix.

torch.Size([18174, 2])
We keep 2.52e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([4977, 2])
We keep 3.36e+05/5.73e+06 =  5% of the original kernel matrix.

torch.Size([13014, 2])
We keep 1.39e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([9707, 2])
We keep 1.73e+06/2.97e+07 =  5% of the original kernel matrix.

torch.Size([17273, 2])
We keep 2.52e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([6790, 2])
We keep 4.36e+05/9.75e+06 =  4% of the original kernel matrix.

torch.Size([14861, 2])
We keep 1.69e+06/6.41e+07 =  2% of the original kernel matrix.

torch.Size([8605, 2])
We keep 6.67e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([16514, 2])
We keep 2.02e+06/8.36e+07 =  2% of the original kernel matrix.

torch.Size([4497, 2])
We keep 2.08e+05/3.74e+06 =  5% of the original kernel matrix.

torch.Size([12437, 2])
We keep 1.19e+06/3.97e+07 =  2% of the original kernel matrix.

torch.Size([6812, 2])
We keep 6.76e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([14486, 2])
We keep 1.90e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([11968, 2])
We keep 2.03e+06/5.04e+07 =  4% of the original kernel matrix.

torch.Size([19208, 2])
We keep 3.03e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([2035, 2])
We keep 4.64e+04/5.62e+05 =  8% of the original kernel matrix.

torch.Size([9436, 2])
We keep 6.34e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([4407, 2])
We keep 3.23e+05/5.04e+06 =  6% of the original kernel matrix.

torch.Size([12408, 2])
We keep 1.32e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([7708, 2])
We keep 5.76e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([15671, 2])
We keep 1.86e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([22948, 2])
We keep 5.00e+06/2.10e+08 =  2% of the original kernel matrix.

torch.Size([26978, 2])
We keep 5.33e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([5816, 2])
We keep 4.45e+05/7.75e+06 =  5% of the original kernel matrix.

torch.Size([13764, 2])
We keep 1.54e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([4093, 2])
We keep 2.63e+05/4.02e+06 =  6% of the original kernel matrix.

torch.Size([11809, 2])
We keep 1.24e+06/4.11e+07 =  3% of the original kernel matrix.

torch.Size([5677, 2])
We keep 5.30e+05/9.24e+06 =  5% of the original kernel matrix.

torch.Size([13384, 2])
We keep 1.67e+06/6.24e+07 =  2% of the original kernel matrix.

torch.Size([4109, 2])
We keep 2.05e+05/3.58e+06 =  5% of the original kernel matrix.

torch.Size([12092, 2])
We keep 1.19e+06/3.89e+07 =  3% of the original kernel matrix.

torch.Size([2927, 2])
We keep 1.38e+05/1.92e+06 =  7% of the original kernel matrix.

torch.Size([10334, 2])
We keep 9.46e+05/2.85e+07 =  3% of the original kernel matrix.

torch.Size([4435, 2])
We keep 2.73e+05/4.40e+06 =  6% of the original kernel matrix.

torch.Size([12439, 2])
We keep 1.27e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([7241, 2])
We keep 8.34e+05/1.49e+07 =  5% of the original kernel matrix.

torch.Size([15234, 2])
We keep 1.95e+06/7.93e+07 =  2% of the original kernel matrix.

torch.Size([1987, 2])
We keep 6.46e+04/8.01e+05 =  8% of the original kernel matrix.

torch.Size([9006, 2])
We keep 7.09e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([6694, 2])
We keep 5.19e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([14619, 2])
We keep 1.73e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([7439, 2])
We keep 7.88e+05/1.46e+07 =  5% of the original kernel matrix.

torch.Size([15341, 2])
We keep 1.96e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([12203, 2])
We keep 1.84e+06/4.16e+07 =  4% of the original kernel matrix.

torch.Size([19440, 2])
We keep 2.79e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([3693, 2])
We keep 1.95e+05/2.95e+06 =  6% of the original kernel matrix.

torch.Size([11510, 2])
We keep 1.11e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([6478, 2])
We keep 5.70e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([14380, 2])
We keep 1.73e+06/6.80e+07 =  2% of the original kernel matrix.

torch.Size([4370, 2])
We keep 2.70e+05/4.57e+06 =  5% of the original kernel matrix.

torch.Size([12324, 2])
We keep 1.28e+06/4.39e+07 =  2% of the original kernel matrix.

torch.Size([6574, 2])
We keep 5.06e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([14544, 2])
We keep 1.74e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([6789, 2])
We keep 5.23e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([14719, 2])
We keep 1.75e+06/6.76e+07 =  2% of the original kernel matrix.

torch.Size([4663, 2])
We keep 2.54e+05/4.61e+06 =  5% of the original kernel matrix.

torch.Size([12705, 2])
We keep 1.30e+06/4.41e+07 =  2% of the original kernel matrix.

torch.Size([7742, 2])
We keep 1.12e+06/2.21e+07 =  5% of the original kernel matrix.

torch.Size([15084, 2])
We keep 2.31e+06/9.65e+07 =  2% of the original kernel matrix.

torch.Size([7245, 2])
We keep 5.93e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([15153, 2])
We keep 1.84e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([7354, 2])
We keep 7.32e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([15192, 2])
We keep 1.92e+06/7.71e+07 =  2% of the original kernel matrix.

torch.Size([21818, 2])
We keep 6.27e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([26416, 2])
We keep 5.32e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([10413, 2])
We keep 1.41e+06/3.00e+07 =  4% of the original kernel matrix.

torch.Size([17937, 2])
We keep 2.47e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([12239, 2])
We keep 1.58e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([19498, 2])
We keep 2.80e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([3053, 2])
We keep 1.52e+05/2.07e+06 =  7% of the original kernel matrix.

torch.Size([10452, 2])
We keep 9.92e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([9484, 2])
We keep 1.27e+06/2.92e+07 =  4% of the original kernel matrix.

torch.Size([16895, 2])
We keep 2.50e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([6187, 2])
We keep 7.17e+05/9.84e+06 =  7% of the original kernel matrix.

torch.Size([14131, 2])
We keep 1.66e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([12104, 2])
We keep 1.50e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([19337, 2])
We keep 2.77e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([3217, 2])
We keep 1.53e+05/2.42e+06 =  6% of the original kernel matrix.

torch.Size([10851, 2])
We keep 1.01e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([7977, 2])
We keep 9.24e+05/1.69e+07 =  5% of the original kernel matrix.

torch.Size([15812, 2])
We keep 2.00e+06/8.44e+07 =  2% of the original kernel matrix.

torch.Size([3399, 2])
We keep 1.64e+05/2.52e+06 =  6% of the original kernel matrix.

torch.Size([11145, 2])
We keep 1.04e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([4957, 2])
We keep 3.15e+05/5.69e+06 =  5% of the original kernel matrix.

torch.Size([12848, 2])
We keep 1.38e+06/4.90e+07 =  2% of the original kernel matrix.

torch.Size([2850, 2])
We keep 1.73e+05/2.12e+06 =  8% of the original kernel matrix.

torch.Size([10190, 2])
We keep 9.91e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([4726, 2])
We keep 5.45e+05/6.86e+06 =  7% of the original kernel matrix.

torch.Size([12297, 2])
We keep 1.47e+06/5.38e+07 =  2% of the original kernel matrix.

torch.Size([5529, 2])
We keep 3.84e+05/6.80e+06 =  5% of the original kernel matrix.

torch.Size([13609, 2])
We keep 1.47e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([9780, 2])
We keep 1.29e+06/2.96e+07 =  4% of the original kernel matrix.

torch.Size([17269, 2])
We keep 2.53e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([8385, 2])
We keep 1.76e+06/2.76e+07 =  6% of the original kernel matrix.

torch.Size([15582, 2])
We keep 2.40e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([7942, 2])
We keep 8.39e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([15751, 2])
We keep 2.09e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([2173, 2])
We keep 6.33e+04/8.82e+05 =  7% of the original kernel matrix.

torch.Size([9565, 2])
We keep 7.26e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([10397, 2])
We keep 1.23e+06/3.07e+07 =  4% of the original kernel matrix.

torch.Size([17801, 2])
We keep 2.57e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([3327, 2])
We keep 1.36e+05/2.01e+06 =  6% of the original kernel matrix.

torch.Size([11131, 2])
We keep 9.57e+05/2.91e+07 =  3% of the original kernel matrix.

torch.Size([23816, 2])
We keep 5.61e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([27831, 2])
We keep 5.52e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([5645, 2])
We keep 5.10e+05/8.24e+06 =  6% of the original kernel matrix.

torch.Size([13624, 2])
We keep 1.58e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([3762, 2])
We keep 2.70e+05/3.96e+06 =  6% of the original kernel matrix.

torch.Size([11210, 2])
We keep 1.22e+06/4.09e+07 =  2% of the original kernel matrix.

torch.Size([4404, 2])
We keep 2.62e+05/4.70e+06 =  5% of the original kernel matrix.

torch.Size([12318, 2])
We keep 1.29e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([4411, 2])
We keep 2.39e+05/4.35e+06 =  5% of the original kernel matrix.

torch.Size([12401, 2])
We keep 1.24e+06/4.28e+07 =  2% of the original kernel matrix.

torch.Size([4450, 2])
We keep 2.63e+05/4.56e+06 =  5% of the original kernel matrix.

torch.Size([12389, 2])
We keep 1.27e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([2922, 2])
We keep 1.19e+05/1.73e+06 =  6% of the original kernel matrix.

torch.Size([10702, 2])
We keep 9.33e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([7340, 2])
We keep 9.75e+05/1.15e+07 =  8% of the original kernel matrix.

torch.Size([15460, 2])
We keep 1.74e+06/6.96e+07 =  2% of the original kernel matrix.

torch.Size([11807, 2])
We keep 1.53e+06/4.20e+07 =  3% of the original kernel matrix.

torch.Size([18913, 2])
We keep 2.87e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([4870, 2])
We keep 2.53e+05/4.62e+06 =  5% of the original kernel matrix.

torch.Size([12882, 2])
We keep 1.26e+06/4.41e+07 =  2% of the original kernel matrix.

torch.Size([14255, 2])
We keep 1.92e+06/6.57e+07 =  2% of the original kernel matrix.

torch.Size([20882, 2])
We keep 3.38e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([3229, 2])
We keep 1.54e+05/2.28e+06 =  6% of the original kernel matrix.

torch.Size([10919, 2])
We keep 9.90e+05/3.10e+07 =  3% of the original kernel matrix.

torch.Size([5484, 2])
We keep 3.81e+05/6.41e+06 =  5% of the original kernel matrix.

torch.Size([13471, 2])
We keep 1.43e+06/5.20e+07 =  2% of the original kernel matrix.

torch.Size([3275, 2])
We keep 1.92e+05/2.69e+06 =  7% of the original kernel matrix.

torch.Size([10873, 2])
We keep 1.08e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([7336, 2])
We keep 7.30e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([15349, 2])
We keep 1.83e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([6127, 2])
We keep 5.44e+05/9.22e+06 =  5% of the original kernel matrix.

torch.Size([14142, 2])
We keep 1.62e+06/6.23e+07 =  2% of the original kernel matrix.

torch.Size([3714, 2])
We keep 1.76e+05/2.73e+06 =  6% of the original kernel matrix.

torch.Size([11748, 2])
We keep 1.08e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([4998, 2])
We keep 3.49e+05/6.09e+06 =  5% of the original kernel matrix.

torch.Size([12899, 2])
We keep 1.43e+06/5.06e+07 =  2% of the original kernel matrix.

torch.Size([4949, 2])
We keep 2.80e+05/4.99e+06 =  5% of the original kernel matrix.

torch.Size([13117, 2])
We keep 1.32e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([3095, 2])
We keep 1.13e+05/1.78e+06 =  6% of the original kernel matrix.

torch.Size([10809, 2])
We keep 9.13e+05/2.74e+07 =  3% of the original kernel matrix.

torch.Size([4266, 2])
We keep 2.70e+05/4.56e+06 =  5% of the original kernel matrix.

torch.Size([12136, 2])
We keep 1.28e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([2651, 2])
We keep 8.89e+04/1.28e+06 =  6% of the original kernel matrix.

torch.Size([10185, 2])
We keep 8.19e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([16756, 2])
We keep 3.28e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([22689, 2])
We keep 4.19e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([11228, 2])
We keep 1.55e+06/4.18e+07 =  3% of the original kernel matrix.

torch.Size([18591, 2])
We keep 2.85e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([5245, 2])
We keep 3.78e+05/6.59e+06 =  5% of the original kernel matrix.

torch.Size([13198, 2])
We keep 1.48e+06/5.27e+07 =  2% of the original kernel matrix.

torch.Size([6901, 2])
We keep 6.09e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([14876, 2])
We keep 1.74e+06/6.92e+07 =  2% of the original kernel matrix.

torch.Size([4704, 2])
We keep 3.38e+05/5.39e+06 =  6% of the original kernel matrix.

torch.Size([12706, 2])
We keep 1.36e+06/4.76e+07 =  2% of the original kernel matrix.

torch.Size([3410, 2])
We keep 1.81e+05/2.77e+06 =  6% of the original kernel matrix.

torch.Size([11290, 2])
We keep 1.11e+06/3.41e+07 =  3% of the original kernel matrix.

torch.Size([4337, 2])
We keep 3.13e+05/4.43e+06 =  7% of the original kernel matrix.

torch.Size([12181, 2])
We keep 1.25e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([4241, 2])
We keep 3.95e+05/5.67e+06 =  6% of the original kernel matrix.

torch.Size([11720, 2])
We keep 1.39e+06/4.89e+07 =  2% of the original kernel matrix.

torch.Size([4241, 2])
We keep 2.79e+05/4.44e+06 =  6% of the original kernel matrix.

torch.Size([12023, 2])
We keep 1.27e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([4874, 2])
We keep 2.95e+05/5.05e+06 =  5% of the original kernel matrix.

torch.Size([12955, 2])
We keep 1.33e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([5485, 2])
We keep 3.31e+05/6.37e+06 =  5% of the original kernel matrix.

torch.Size([13598, 2])
We keep 1.45e+06/5.18e+07 =  2% of the original kernel matrix.

torch.Size([14559, 2])
We keep 3.52e+06/9.90e+07 =  3% of the original kernel matrix.

torch.Size([20917, 2])
We keep 4.00e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([5839, 2])
We keep 4.63e+05/8.69e+06 =  5% of the original kernel matrix.

torch.Size([13812, 2])
We keep 1.62e+06/6.05e+07 =  2% of the original kernel matrix.

torch.Size([18813, 2])
We keep 5.40e+06/1.93e+08 =  2% of the original kernel matrix.

torch.Size([24014, 2])
We keep 5.20e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([6596, 2])
We keep 4.24e+05/9.04e+06 =  4% of the original kernel matrix.

torch.Size([14770, 2])
We keep 1.64e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([7606, 2])
We keep 7.53e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([15449, 2])
We keep 1.98e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([6737, 2])
We keep 5.74e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([14854, 2])
We keep 1.73e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([1661, 2])
We keep 5.49e+04/5.88e+05 =  9% of the original kernel matrix.

torch.Size([8502, 2])
We keep 6.38e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([20282, 2])
We keep 4.35e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([25566, 2])
We keep 4.81e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([4269, 2])
We keep 2.28e+05/3.93e+06 =  5% of the original kernel matrix.

torch.Size([12186, 2])
We keep 1.23e+06/4.07e+07 =  3% of the original kernel matrix.

torch.Size([5543, 2])
We keep 4.27e+05/6.97e+06 =  6% of the original kernel matrix.

torch.Size([13468, 2])
We keep 1.47e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([2494, 2])
We keep 9.93e+04/1.24e+06 =  7% of the original kernel matrix.

torch.Size([9828, 2])
We keep 8.38e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([10084, 2])
We keep 1.57e+06/3.69e+07 =  4% of the original kernel matrix.

torch.Size([17502, 2])
We keep 2.71e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([4795, 2])
We keep 2.95e+05/4.98e+06 =  5% of the original kernel matrix.

torch.Size([12815, 2])
We keep 1.31e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([3688, 2])
We keep 1.58e+05/2.82e+06 =  5% of the original kernel matrix.

torch.Size([11695, 2])
We keep 1.06e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([9012, 2])
We keep 9.61e+05/2.15e+07 =  4% of the original kernel matrix.

torch.Size([16661, 2])
We keep 2.23e+06/9.51e+07 =  2% of the original kernel matrix.

torch.Size([6956, 2])
We keep 4.67e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([14975, 2])
We keep 1.73e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([2275, 2])
We keep 6.41e+04/8.57e+05 =  7% of the original kernel matrix.

torch.Size([9703, 2])
We keep 7.16e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([1550, 2])
We keep 3.73e+04/3.94e+05 =  9% of the original kernel matrix.

torch.Size([8282, 2])
We keep 5.55e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([2735, 2])
We keep 9.05e+04/1.28e+06 =  7% of the original kernel matrix.

torch.Size([10477, 2])
We keep 8.22e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([13380, 2])
We keep 2.27e+06/6.33e+07 =  3% of the original kernel matrix.

torch.Size([20342, 2])
We keep 3.40e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([4323, 2])
We keep 2.68e+05/4.21e+06 =  6% of the original kernel matrix.

torch.Size([12158, 2])
We keep 1.23e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([8063, 2])
We keep 7.93e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([15934, 2])
We keep 2.05e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([11859, 2])
We keep 1.55e+07/7.24e+07 = 21% of the original kernel matrix.

torch.Size([18894, 2])
We keep 3.29e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([9431, 2])
We keep 9.87e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([17123, 2])
We keep 2.26e+06/9.71e+07 =  2% of the original kernel matrix.

torch.Size([4393, 2])
We keep 2.75e+05/4.76e+06 =  5% of the original kernel matrix.

torch.Size([12275, 2])
We keep 1.30e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([8992, 2])
We keep 1.48e+06/2.64e+07 =  5% of the original kernel matrix.

torch.Size([16483, 2])
We keep 2.41e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([5846, 2])
We keep 3.50e+05/7.04e+06 =  4% of the original kernel matrix.

torch.Size([13910, 2])
We keep 1.49e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([7030, 2])
We keep 5.58e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([14977, 2])
We keep 1.82e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([6627, 2])
We keep 7.47e+05/1.22e+07 =  6% of the original kernel matrix.

torch.Size([14331, 2])
We keep 1.80e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([6418, 2])
We keep 6.27e+05/1.16e+07 =  5% of the original kernel matrix.

torch.Size([14193, 2])
We keep 1.81e+06/7.00e+07 =  2% of the original kernel matrix.

torch.Size([3885, 2])
We keep 1.61e+05/2.79e+06 =  5% of the original kernel matrix.

torch.Size([11927, 2])
We keep 1.08e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([6841, 2])
We keep 7.12e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([14706, 2])
We keep 1.83e+06/7.18e+07 =  2% of the original kernel matrix.

torch.Size([11102, 2])
We keep 1.50e+06/3.13e+07 =  4% of the original kernel matrix.

torch.Size([18632, 2])
We keep 2.51e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([3885, 2])
We keep 1.96e+05/3.24e+06 =  6% of the original kernel matrix.

torch.Size([11686, 2])
We keep 1.14e+06/3.70e+07 =  3% of the original kernel matrix.

torch.Size([6500, 2])
We keep 5.03e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([14422, 2])
We keep 1.72e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([11894, 2])
We keep 1.60e+06/4.12e+07 =  3% of the original kernel matrix.

torch.Size([19123, 2])
We keep 2.85e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([11686, 2])
We keep 1.54e+06/3.99e+07 =  3% of the original kernel matrix.

torch.Size([18923, 2])
We keep 2.79e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([5906, 2])
We keep 3.79e+05/7.44e+06 =  5% of the original kernel matrix.

torch.Size([14058, 2])
We keep 1.51e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([1460, 2])
We keep 4.49e+04/4.48e+05 = 10% of the original kernel matrix.

torch.Size([8016, 2])
We keep 5.92e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([7495, 2])
We keep 7.34e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([15663, 2])
We keep 1.86e+06/7.44e+07 =  2% of the original kernel matrix.

torch.Size([13289, 2])
We keep 2.92e+06/7.03e+07 =  4% of the original kernel matrix.

torch.Size([20098, 2])
We keep 3.39e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([11037, 2])
We keep 1.82e+06/4.02e+07 =  4% of the original kernel matrix.

torch.Size([18288, 2])
We keep 2.80e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([6433, 2])
We keep 5.47e+05/9.99e+06 =  5% of the original kernel matrix.

torch.Size([14503, 2])
We keep 1.67e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([5967, 2])
We keep 4.07e+05/7.96e+06 =  5% of the original kernel matrix.

torch.Size([13982, 2])
We keep 1.55e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([3007, 2])
We keep 1.05e+05/1.76e+06 =  5% of the original kernel matrix.

torch.Size([10825, 2])
We keep 9.12e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([5222, 2])
We keep 3.15e+05/5.55e+06 =  5% of the original kernel matrix.

torch.Size([13414, 2])
We keep 1.35e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([2636, 2])
We keep 7.32e+04/1.15e+06 =  6% of the original kernel matrix.

torch.Size([10367, 2])
We keep 7.75e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([6751, 2])
We keep 4.26e+05/9.85e+06 =  4% of the original kernel matrix.

torch.Size([14898, 2])
We keep 1.68e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([4439, 2])
We keep 2.52e+05/4.46e+06 =  5% of the original kernel matrix.

torch.Size([12370, 2])
We keep 1.26e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([10423, 2])
We keep 9.46e+05/2.67e+07 =  3% of the original kernel matrix.

torch.Size([17901, 2])
We keep 2.42e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([4508, 2])
We keep 2.34e+05/4.26e+06 =  5% of the original kernel matrix.

torch.Size([12560, 2])
We keep 1.26e+06/4.24e+07 =  2% of the original kernel matrix.

torch.Size([13852, 2])
We keep 2.08e+06/6.02e+07 =  3% of the original kernel matrix.

torch.Size([20687, 2])
We keep 3.26e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([4692, 2])
We keep 3.80e+05/5.07e+06 =  7% of the original kernel matrix.

torch.Size([12717, 2])
We keep 1.30e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([6253, 2])
We keep 6.89e+05/1.19e+07 =  5% of the original kernel matrix.

torch.Size([13895, 2])
We keep 1.81e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([5751, 2])
We keep 3.93e+05/7.95e+06 =  4% of the original kernel matrix.

torch.Size([13838, 2])
We keep 1.54e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([6843, 2])
We keep 6.76e+05/1.16e+07 =  5% of the original kernel matrix.

torch.Size([14817, 2])
We keep 1.73e+06/7.01e+07 =  2% of the original kernel matrix.

torch.Size([10036, 2])
We keep 9.73e+05/2.63e+07 =  3% of the original kernel matrix.

torch.Size([17786, 2])
We keep 2.41e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([9870, 2])
We keep 1.12e+06/2.82e+07 =  3% of the original kernel matrix.

torch.Size([17336, 2])
We keep 2.47e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([11192, 2])
We keep 3.82e+06/5.68e+07 =  6% of the original kernel matrix.

torch.Size([18196, 2])
We keep 3.16e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([6932, 2])
We keep 4.90e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([14899, 2])
We keep 1.75e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([6833, 2])
We keep 4.83e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([14743, 2])
We keep 1.72e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([2613, 2])
We keep 1.27e+05/1.55e+06 =  8% of the original kernel matrix.

torch.Size([9932, 2])
We keep 8.90e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([4811, 2])
We keep 3.15e+05/5.29e+06 =  5% of the original kernel matrix.

torch.Size([12687, 2])
We keep 1.34e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([2657, 2])
We keep 8.04e+04/1.16e+06 =  6% of the original kernel matrix.

torch.Size([10337, 2])
We keep 8.07e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([3035, 2])
We keep 1.46e+05/2.02e+06 =  7% of the original kernel matrix.

torch.Size([10606, 2])
We keep 9.63e+05/2.92e+07 =  3% of the original kernel matrix.

torch.Size([9198, 2])
We keep 1.20e+06/2.66e+07 =  4% of the original kernel matrix.

torch.Size([16716, 2])
We keep 2.43e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([5406, 2])
We keep 2.85e+05/5.50e+06 =  5% of the original kernel matrix.

torch.Size([13578, 2])
We keep 1.36e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([4751, 2])
We keep 2.53e+05/4.81e+06 =  5% of the original kernel matrix.

torch.Size([12724, 2])
We keep 1.31e+06/4.50e+07 =  2% of the original kernel matrix.

torch.Size([2448, 2])
We keep 8.37e+04/1.12e+06 =  7% of the original kernel matrix.

torch.Size([9892, 2])
We keep 7.80e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([11784, 2])
We keep 1.50e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([18974, 2])
We keep 2.82e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([6078, 2])
We keep 6.67e+05/9.83e+06 =  6% of the original kernel matrix.

torch.Size([13987, 2])
We keep 1.67e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([4517, 2])
We keep 3.35e+05/5.62e+06 =  5% of the original kernel matrix.

torch.Size([12294, 2])
We keep 1.40e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([5503, 2])
We keep 6.01e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([13336, 2])
We keep 1.77e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([3173, 2])
We keep 1.34e+05/2.04e+06 =  6% of the original kernel matrix.

torch.Size([10916, 2])
We keep 9.76e+05/2.93e+07 =  3% of the original kernel matrix.

torch.Size([4465, 2])
We keep 2.29e+05/4.04e+06 =  5% of the original kernel matrix.

torch.Size([12587, 2])
We keep 1.23e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([6488, 2])
We keep 5.31e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([14374, 2])
We keep 1.71e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([2165, 2])
We keep 7.11e+04/8.78e+05 =  8% of the original kernel matrix.

torch.Size([9496, 2])
We keep 7.37e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([5226, 2])
We keep 6.36e+05/8.18e+06 =  7% of the original kernel matrix.

torch.Size([13095, 2])
We keep 1.57e+06/5.87e+07 =  2% of the original kernel matrix.

torch.Size([6952, 2])
We keep 5.66e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([14833, 2])
We keep 1.78e+06/7.09e+07 =  2% of the original kernel matrix.

torch.Size([7790, 2])
We keep 9.44e+05/1.68e+07 =  5% of the original kernel matrix.

torch.Size([15631, 2])
We keep 1.99e+06/8.42e+07 =  2% of the original kernel matrix.

torch.Size([7105, 2])
We keep 8.18e+05/1.53e+07 =  5% of the original kernel matrix.

torch.Size([14907, 2])
We keep 1.97e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([4011, 2])
We keep 2.01e+05/3.27e+06 =  6% of the original kernel matrix.

torch.Size([12048, 2])
We keep 1.14e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([6928, 2])
We keep 8.65e+05/1.42e+07 =  6% of the original kernel matrix.

torch.Size([14656, 2])
We keep 1.89e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([9368, 2])
We keep 9.92e+05/2.38e+07 =  4% of the original kernel matrix.

torch.Size([17088, 2])
We keep 2.33e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([8112, 2])
We keep 5.93e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([16152, 2])
We keep 1.93e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([5826, 2])
We keep 4.37e+05/8.06e+06 =  5% of the original kernel matrix.

torch.Size([13831, 2])
We keep 1.54e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([29206, 2])
We keep 9.84e+06/4.04e+08 =  2% of the original kernel matrix.

torch.Size([30616, 2])
We keep 6.95e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([47685, 2])
We keep 5.02e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([37728, 2])
We keep 1.26e+07/8.52e+08 =  1% of the original kernel matrix.

torch.Size([9553, 2])
We keep 1.26e+06/2.46e+07 =  5% of the original kernel matrix.

torch.Size([17517, 2])
We keep 2.13e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([11649, 2])
We keep 2.28e+06/4.99e+07 =  4% of the original kernel matrix.

torch.Size([18867, 2])
We keep 3.12e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([1828, 2])
We keep 4.65e+04/5.52e+05 =  8% of the original kernel matrix.

torch.Size([8971, 2])
We keep 6.33e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([13147, 2])
We keep 1.78e+06/5.26e+07 =  3% of the original kernel matrix.

torch.Size([20089, 2])
We keep 3.10e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([19095, 2])
We keep 1.01e+07/1.44e+08 =  7% of the original kernel matrix.

torch.Size([24338, 2])
We keep 4.43e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([157422, 2])
We keep 3.06e+08/1.67e+10 =  1% of the original kernel matrix.

torch.Size([70182, 2])
We keep 3.34e+07/2.65e+09 =  1% of the original kernel matrix.

torch.Size([5199, 2])
We keep 3.02e+05/5.79e+06 =  5% of the original kernel matrix.

torch.Size([13377, 2])
We keep 1.37e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([168878, 2])
We keep 2.64e+08/1.89e+10 =  1% of the original kernel matrix.

torch.Size([72779, 2])
We keep 3.52e+07/2.82e+09 =  1% of the original kernel matrix.

torch.Size([33910, 2])
We keep 4.29e+07/6.65e+08 =  6% of the original kernel matrix.

torch.Size([32282, 2])
We keep 8.25e+06/5.29e+08 =  1% of the original kernel matrix.

torch.Size([45134, 2])
We keep 3.63e+07/1.16e+09 =  3% of the original kernel matrix.

torch.Size([37285, 2])
We keep 1.06e+07/6.98e+08 =  1% of the original kernel matrix.

torch.Size([49347, 2])
We keep 1.78e+08/1.49e+09 = 11% of the original kernel matrix.

torch.Size([39094, 2])
We keep 1.15e+07/7.93e+08 =  1% of the original kernel matrix.

torch.Size([6182, 2])
We keep 6.63e+05/9.09e+06 =  7% of the original kernel matrix.

torch.Size([14322, 2])
We keep 1.60e+06/6.19e+07 =  2% of the original kernel matrix.

torch.Size([9654, 2])
We keep 9.77e+05/2.43e+07 =  4% of the original kernel matrix.

torch.Size([17318, 2])
We keep 2.34e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([194165, 2])
We keep 3.12e+08/2.32e+10 =  1% of the original kernel matrix.

torch.Size([78835, 2])
We keep 3.82e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([862345, 2])
We keep 3.52e+09/3.34e+11 =  1% of the original kernel matrix.

torch.Size([177105, 2])
We keep 1.27e+08/1.19e+10 =  1% of the original kernel matrix.

torch.Size([7210, 2])
We keep 5.46e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([15201, 2])
We keep 1.74e+06/7.03e+07 =  2% of the original kernel matrix.

torch.Size([57003, 2])
We keep 2.84e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([42395, 2])
We keep 1.20e+07/8.05e+08 =  1% of the original kernel matrix.

torch.Size([22681, 2])
We keep 2.07e+07/2.44e+08 =  8% of the original kernel matrix.

torch.Size([26793, 2])
We keep 5.87e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([38758, 2])
We keep 3.38e+07/7.21e+08 =  4% of the original kernel matrix.

torch.Size([35150, 2])
We keep 8.80e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([137437, 2])
We keep 2.74e+08/1.61e+10 =  1% of the original kernel matrix.

torch.Size([64016, 2])
We keep 3.30e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([53518, 2])
We keep 1.34e+08/3.04e+09 =  4% of the original kernel matrix.

torch.Size([37817, 2])
We keep 1.63e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([48441, 2])
We keep 3.53e+07/1.16e+09 =  3% of the original kernel matrix.

torch.Size([39315, 2])
We keep 1.04e+07/6.99e+08 =  1% of the original kernel matrix.

torch.Size([23060, 2])
We keep 7.23e+06/2.47e+08 =  2% of the original kernel matrix.

torch.Size([27046, 2])
We keep 5.58e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([5094, 2])
We keep 2.87e+05/5.69e+06 =  5% of the original kernel matrix.

torch.Size([13160, 2])
We keep 1.36e+06/4.90e+07 =  2% of the original kernel matrix.

torch.Size([134314, 2])
We keep 2.61e+08/1.57e+10 =  1% of the original kernel matrix.

torch.Size([63057, 2])
We keep 3.21e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([17002, 2])
We keep 8.86e+06/1.25e+08 =  7% of the original kernel matrix.

torch.Size([22984, 2])
We keep 4.08e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([4911, 2])
We keep 2.76e+05/5.40e+06 =  5% of the original kernel matrix.

torch.Size([12965, 2])
We keep 1.34e+06/4.77e+07 =  2% of the original kernel matrix.

torch.Size([32359, 2])
We keep 1.21e+07/5.14e+08 =  2% of the original kernel matrix.

torch.Size([32353, 2])
We keep 7.68e+06/4.65e+08 =  1% of the original kernel matrix.

torch.Size([75465, 2])
We keep 1.06e+08/4.30e+09 =  2% of the original kernel matrix.

torch.Size([45717, 2])
We keep 1.89e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([143599, 2])
We keep 1.64e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([67952, 2])
We keep 2.94e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([125313, 2])
We keep 8.42e+07/7.05e+09 =  1% of the original kernel matrix.

torch.Size([62755, 2])
We keep 2.29e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([7007, 2])
We keep 8.02e+05/1.38e+07 =  5% of the original kernel matrix.

torch.Size([14926, 2])
We keep 1.87e+06/7.63e+07 =  2% of the original kernel matrix.

torch.Size([36593, 2])
We keep 2.37e+07/7.50e+08 =  3% of the original kernel matrix.

torch.Size([33760, 2])
We keep 8.84e+06/5.62e+08 =  1% of the original kernel matrix.

torch.Size([10444, 2])
We keep 1.39e+06/3.31e+07 =  4% of the original kernel matrix.

torch.Size([17941, 2])
We keep 2.60e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([55865, 2])
We keep 3.50e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([41528, 2])
We keep 1.28e+07/8.77e+08 =  1% of the original kernel matrix.

torch.Size([88626, 2])
We keep 1.06e+08/3.73e+09 =  2% of the original kernel matrix.

torch.Size([52801, 2])
We keep 1.64e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([10234, 2])
We keep 1.79e+06/3.74e+07 =  4% of the original kernel matrix.

torch.Size([17697, 2])
We keep 2.75e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([139274, 2])
We keep 1.29e+08/9.10e+09 =  1% of the original kernel matrix.

torch.Size([66640, 2])
We keep 2.55e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([1868, 2])
We keep 4.63e+04/5.42e+05 =  8% of the original kernel matrix.

torch.Size([8930, 2])
We keep 6.23e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([12318, 2])
We keep 1.91e+06/4.56e+07 =  4% of the original kernel matrix.

torch.Size([19463, 2])
We keep 2.93e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([89784, 2])
We keep 9.41e+07/5.52e+09 =  1% of the original kernel matrix.

torch.Size([51057, 2])
We keep 2.07e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([50374, 2])
We keep 3.88e+07/1.65e+09 =  2% of the original kernel matrix.

torch.Size([38816, 2])
We keep 1.25e+07/8.34e+08 =  1% of the original kernel matrix.

torch.Size([88809, 2])
We keep 1.02e+08/5.75e+09 =  1% of the original kernel matrix.

torch.Size([50521, 2])
We keep 2.11e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([6133, 2])
We keep 4.38e+05/8.02e+06 =  5% of the original kernel matrix.

torch.Size([14278, 2])
We keep 1.55e+06/5.81e+07 =  2% of the original kernel matrix.

torch.Size([264490, 2])
We keep 4.05e+08/4.42e+10 =  0% of the original kernel matrix.

torch.Size([93213, 2])
We keep 5.00e+07/4.32e+09 =  1% of the original kernel matrix.

torch.Size([27826, 2])
We keep 6.34e+06/3.15e+08 =  2% of the original kernel matrix.

torch.Size([30239, 2])
We keep 6.09e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([39411, 2])
We keep 1.31e+07/7.28e+08 =  1% of the original kernel matrix.

torch.Size([35560, 2])
We keep 8.71e+06/5.54e+08 =  1% of the original kernel matrix.

torch.Size([92142, 2])
We keep 7.50e+07/4.40e+09 =  1% of the original kernel matrix.

torch.Size([53411, 2])
We keep 1.86e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([4441, 2])
We keep 2.30e+05/4.12e+06 =  5% of the original kernel matrix.

torch.Size([12456, 2])
We keep 1.23e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([140699, 2])
We keep 1.83e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([66535, 2])
We keep 2.84e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([207011, 2])
We keep 6.21e+08/2.71e+10 =  2% of the original kernel matrix.

torch.Size([82575, 2])
We keep 4.17e+07/3.38e+09 =  1% of the original kernel matrix.

torch.Size([366025, 2])
We keep 8.13e+08/6.94e+10 =  1% of the original kernel matrix.

torch.Size([110671, 2])
We keep 6.17e+07/5.41e+09 =  1% of the original kernel matrix.

torch.Size([75088, 2])
We keep 6.45e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([46635, 2])
We keep 1.71e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([41226, 2])
We keep 2.13e+07/9.43e+08 =  2% of the original kernel matrix.

torch.Size([35862, 2])
We keep 9.87e+06/6.30e+08 =  1% of the original kernel matrix.

torch.Size([10985, 2])
We keep 2.72e+06/5.95e+07 =  4% of the original kernel matrix.

torch.Size([17615, 2])
We keep 3.32e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([12122, 2])
We keep 1.38e+06/3.99e+07 =  3% of the original kernel matrix.

torch.Size([19278, 2])
We keep 2.79e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([115208, 2])
We keep 1.75e+08/9.90e+09 =  1% of the original kernel matrix.

torch.Size([59304, 2])
We keep 2.58e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([14527, 2])
We keep 2.67e+06/8.24e+07 =  3% of the original kernel matrix.

torch.Size([21125, 2])
We keep 3.62e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([32944, 2])
We keep 1.34e+07/5.74e+08 =  2% of the original kernel matrix.

torch.Size([32265, 2])
We keep 7.92e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([53379, 2])
We keep 8.30e+07/2.15e+09 =  3% of the original kernel matrix.

torch.Size([40771, 2])
We keep 1.15e+07/9.51e+08 =  1% of the original kernel matrix.

torch.Size([27551, 2])
We keep 9.79e+06/3.25e+08 =  3% of the original kernel matrix.

torch.Size([28879, 2])
We keep 5.55e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([18321, 2])
We keep 9.45e+06/2.20e+08 =  4% of the original kernel matrix.

torch.Size([23214, 2])
We keep 5.50e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([9436, 2])
We keep 8.31e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([17109, 2])
We keep 2.18e+06/9.58e+07 =  2% of the original kernel matrix.

torch.Size([52932, 2])
We keep 1.91e+08/6.15e+09 =  3% of the original kernel matrix.

torch.Size([37566, 2])
We keep 2.13e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([71166, 2])
We keep 9.34e+08/6.84e+09 = 13% of the original kernel matrix.

torch.Size([45867, 2])
We keep 2.09e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([27335, 2])
We keep 4.01e+07/5.07e+08 =  7% of the original kernel matrix.

torch.Size([29050, 2])
We keep 7.73e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([12368, 2])
We keep 1.89e+06/4.74e+07 =  3% of the original kernel matrix.

torch.Size([19618, 2])
We keep 2.98e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([59972, 2])
We keep 4.43e+07/2.01e+09 =  2% of the original kernel matrix.

torch.Size([43209, 2])
We keep 1.32e+07/9.20e+08 =  1% of the original kernel matrix.

torch.Size([27619, 2])
We keep 6.59e+06/3.15e+08 =  2% of the original kernel matrix.

torch.Size([29576, 2])
We keep 6.13e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([193698, 2])
We keep 5.42e+08/2.79e+10 =  1% of the original kernel matrix.

torch.Size([78398, 2])
We keep 4.23e+07/3.43e+09 =  1% of the original kernel matrix.

torch.Size([13500, 2])
We keep 3.25e+06/8.84e+07 =  3% of the original kernel matrix.

torch.Size([20359, 2])
We keep 3.81e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([35986, 2])
We keep 2.40e+07/1.08e+09 =  2% of the original kernel matrix.

torch.Size([31826, 2])
We keep 1.05e+07/6.75e+08 =  1% of the original kernel matrix.

torch.Size([17904, 2])
We keep 3.29e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([23728, 2])
We keep 4.06e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([82915, 2])
We keep 1.67e+08/5.45e+09 =  3% of the original kernel matrix.

torch.Size([49133, 2])
We keep 2.03e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([48480, 2])
We keep 1.91e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([38959, 2])
We keep 1.12e+07/7.51e+08 =  1% of the original kernel matrix.

torch.Size([29580, 2])
We keep 1.34e+07/3.90e+08 =  3% of the original kernel matrix.

torch.Size([31004, 2])
We keep 6.53e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([152262, 2])
We keep 1.16e+08/1.07e+10 =  1% of the original kernel matrix.

torch.Size([70025, 2])
We keep 2.69e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([242730, 2])
We keep 3.53e+08/2.70e+10 =  1% of the original kernel matrix.

torch.Size([90790, 2])
We keep 4.00e+07/3.38e+09 =  1% of the original kernel matrix.

torch.Size([9575, 2])
We keep 3.26e+06/2.89e+07 = 11% of the original kernel matrix.

torch.Size([17360, 2])
We keep 2.41e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([201371, 2])
We keep 5.25e+08/2.17e+10 =  2% of the original kernel matrix.

torch.Size([81805, 2])
We keep 3.75e+07/3.02e+09 =  1% of the original kernel matrix.

torch.Size([152208, 2])
We keep 1.34e+08/1.30e+10 =  1% of the original kernel matrix.

torch.Size([70108, 2])
We keep 2.95e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([362309, 2])
We keep 7.24e+08/7.34e+10 =  0% of the original kernel matrix.

torch.Size([110955, 2])
We keep 6.40e+07/5.56e+09 =  1% of the original kernel matrix.

torch.Size([25930, 2])
We keep 9.02e+06/2.92e+08 =  3% of the original kernel matrix.

torch.Size([28917, 2])
We keep 5.85e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([5345, 2])
We keep 3.29e+05/6.34e+06 =  5% of the original kernel matrix.

torch.Size([13402, 2])
We keep 1.44e+06/5.17e+07 =  2% of the original kernel matrix.

torch.Size([22532, 2])
We keep 5.26e+06/2.39e+08 =  2% of the original kernel matrix.

torch.Size([26764, 2])
We keep 5.65e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([23740, 2])
We keep 2.80e+07/4.64e+08 =  6% of the original kernel matrix.

torch.Size([26967, 2])
We keep 6.44e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([38310, 2])
We keep 1.56e+07/6.93e+08 =  2% of the original kernel matrix.

torch.Size([34807, 2])
We keep 8.59e+06/5.40e+08 =  1% of the original kernel matrix.

torch.Size([93437, 2])
We keep 1.16e+08/4.17e+09 =  2% of the original kernel matrix.

torch.Size([53753, 2])
We keep 1.84e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([7102, 2])
We keep 7.56e+05/1.20e+07 =  6% of the original kernel matrix.

torch.Size([15144, 2])
We keep 1.76e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([93274, 2])
We keep 6.28e+07/3.81e+09 =  1% of the original kernel matrix.

torch.Size([53281, 2])
We keep 1.75e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([174188, 2])
We keep 1.87e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([75311, 2])
We keep 3.28e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([8059, 2])
We keep 1.30e+07/3.13e+07 = 41% of the original kernel matrix.

torch.Size([15874, 2])
We keep 2.06e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([11669, 2])
We keep 1.33e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([18956, 2])
We keep 2.72e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([62269, 2])
We keep 3.46e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([43756, 2])
We keep 1.33e+07/9.20e+08 =  1% of the original kernel matrix.

torch.Size([19896, 2])
We keep 4.93e+06/1.44e+08 =  3% of the original kernel matrix.

torch.Size([25074, 2])
We keep 4.40e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([11782, 2])
We keep 1.60e+06/4.26e+07 =  3% of the original kernel matrix.

torch.Size([19188, 2])
We keep 2.89e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([22732, 2])
We keep 4.65e+06/2.24e+08 =  2% of the original kernel matrix.

torch.Size([26977, 2])
We keep 5.33e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([7445, 2])
We keep 7.29e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([15414, 2])
We keep 1.87e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([20513, 2])
We keep 2.13e+07/2.26e+08 =  9% of the original kernel matrix.

torch.Size([25124, 2])
We keep 4.99e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([47413, 2])
We keep 1.60e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([38915, 2])
We keep 1.01e+07/6.64e+08 =  1% of the original kernel matrix.

torch.Size([28451, 2])
We keep 8.39e+06/4.00e+08 =  2% of the original kernel matrix.

torch.Size([30729, 2])
We keep 6.97e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([39124, 2])
We keep 2.28e+07/9.13e+08 =  2% of the original kernel matrix.

torch.Size([34718, 2])
We keep 9.53e+06/6.20e+08 =  1% of the original kernel matrix.

torch.Size([11750, 2])
We keep 1.43e+06/3.85e+07 =  3% of the original kernel matrix.

torch.Size([18960, 2])
We keep 2.78e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([1436156, 2])
We keep 1.14e+10/8.25e+11 =  1% of the original kernel matrix.

torch.Size([228626, 2])
We keep 1.86e+08/1.87e+10 =  0% of the original kernel matrix.

torch.Size([20617, 2])
We keep 4.59e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([25693, 2])
We keep 5.04e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([142544, 2])
We keep 1.06e+08/9.46e+09 =  1% of the original kernel matrix.

torch.Size([67474, 2])
We keep 2.59e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([4366, 2])
We keep 2.38e+05/4.05e+06 =  5% of the original kernel matrix.

torch.Size([12376, 2])
We keep 1.24e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([13063, 2])
We keep 1.96e+06/5.07e+07 =  3% of the original kernel matrix.

torch.Size([20116, 2])
We keep 3.07e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([10509, 2])
We keep 1.06e+06/2.84e+07 =  3% of the original kernel matrix.

torch.Size([17982, 2])
We keep 2.47e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([21949, 2])
We keep 4.29e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([26535, 2])
We keep 4.68e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([5951, 2])
We keep 4.09e+05/9.01e+06 =  4% of the original kernel matrix.

torch.Size([14113, 2])
We keep 1.64e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([103571, 2])
We keep 1.45e+08/6.02e+09 =  2% of the original kernel matrix.

torch.Size([56164, 2])
We keep 2.15e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([9113, 2])
We keep 6.92e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([16887, 2])
We keep 2.13e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([12719, 2])
We keep 2.24e+06/5.49e+07 =  4% of the original kernel matrix.

torch.Size([19878, 2])
We keep 3.12e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([238797, 2])
We keep 4.93e+08/4.10e+10 =  1% of the original kernel matrix.

torch.Size([89464, 2])
We keep 5.03e+07/4.16e+09 =  1% of the original kernel matrix.

torch.Size([75650, 2])
We keep 5.46e+07/2.94e+09 =  1% of the original kernel matrix.

torch.Size([47879, 2])
We keep 1.59e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([41471, 2])
We keep 1.90e+07/9.53e+08 =  1% of the original kernel matrix.

torch.Size([35951, 2])
We keep 9.89e+06/6.34e+08 =  1% of the original kernel matrix.

torch.Size([139484, 2])
We keep 1.46e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([67043, 2])
We keep 2.70e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([305944, 2])
We keep 4.80e+08/5.04e+10 =  0% of the original kernel matrix.

torch.Size([102340, 2])
We keep 5.43e+07/4.61e+09 =  1% of the original kernel matrix.

torch.Size([10170, 2])
We keep 1.03e+06/2.78e+07 =  3% of the original kernel matrix.

torch.Size([17586, 2])
We keep 2.46e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([10190, 2])
We keep 9.28e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([17700, 2])
We keep 2.43e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([28323, 2])
We keep 1.64e+07/4.38e+08 =  3% of the original kernel matrix.

torch.Size([29868, 2])
We keep 7.11e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([4349, 2])
We keep 2.25e+05/3.99e+06 =  5% of the original kernel matrix.

torch.Size([12374, 2])
We keep 1.22e+06/4.10e+07 =  2% of the original kernel matrix.

torch.Size([13808, 2])
We keep 1.99e+06/5.97e+07 =  3% of the original kernel matrix.

torch.Size([20598, 2])
We keep 3.21e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([17503, 2])
We keep 3.42e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([23382, 2])
We keep 4.36e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([5765, 2])
We keep 4.20e+05/7.78e+06 =  5% of the original kernel matrix.

torch.Size([13859, 2])
We keep 1.55e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([18614, 2])
We keep 4.14e+06/1.38e+08 =  3% of the original kernel matrix.

torch.Size([24290, 2])
We keep 4.52e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([14955, 2])
We keep 2.24e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([21470, 2])
We keep 3.60e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([10519, 2])
We keep 1.20e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([18093, 2])
We keep 2.56e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([232686, 2])
We keep 3.47e+08/2.92e+10 =  1% of the original kernel matrix.

torch.Size([87809, 2])
We keep 4.18e+07/3.51e+09 =  1% of the original kernel matrix.

torch.Size([13531, 2])
We keep 3.65e+06/9.28e+07 =  3% of the original kernel matrix.

torch.Size([20083, 2])
We keep 3.81e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([77180, 2])
We keep 6.81e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([48445, 2])
We keep 1.71e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([4416, 2])
We keep 2.57e+05/4.67e+06 =  5% of the original kernel matrix.

torch.Size([12485, 2])
We keep 1.29e+06/4.44e+07 =  2% of the original kernel matrix.

torch.Size([5769, 2])
We keep 4.57e+05/8.42e+06 =  5% of the original kernel matrix.

torch.Size([13756, 2])
We keep 1.60e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([130786, 2])
We keep 1.41e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([63975, 2])
We keep 2.68e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([633580, 2])
We keep 3.62e+09/2.37e+11 =  1% of the original kernel matrix.

torch.Size([148268, 2])
We keep 1.04e+08/1.00e+10 =  1% of the original kernel matrix.

torch.Size([609730, 2])
We keep 1.12e+09/1.58e+11 =  0% of the original kernel matrix.

torch.Size([145157, 2])
We keep 8.81e+07/8.15e+09 =  1% of the original kernel matrix.

torch.Size([20060, 2])
We keep 3.86e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([25266, 2])
We keep 4.68e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([63106, 2])
We keep 3.62e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([44592, 2])
We keep 1.26e+07/8.80e+08 =  1% of the original kernel matrix.

torch.Size([6821, 2])
We keep 4.89e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([14817, 2])
We keep 1.73e+06/6.80e+07 =  2% of the original kernel matrix.

torch.Size([6659, 2])
We keep 1.80e+06/1.38e+07 = 13% of the original kernel matrix.

torch.Size([14703, 2])
We keep 1.66e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([208182, 2])
We keep 2.30e+09/4.99e+10 =  4% of the original kernel matrix.

torch.Size([81192, 2])
We keep 5.27e+07/4.58e+09 =  1% of the original kernel matrix.

torch.Size([24890, 2])
We keep 7.05e+06/2.54e+08 =  2% of the original kernel matrix.

torch.Size([28426, 2])
We keep 5.53e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([11216, 2])
We keep 1.24e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([18501, 2])
We keep 2.68e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([62649, 2])
We keep 4.13e+07/2.00e+09 =  2% of the original kernel matrix.

torch.Size([43793, 2])
We keep 1.32e+07/9.19e+08 =  1% of the original kernel matrix.

torch.Size([535276, 2])
We keep 2.18e+09/2.12e+11 =  1% of the original kernel matrix.

torch.Size([131838, 2])
We keep 1.05e+08/9.45e+09 =  1% of the original kernel matrix.

torch.Size([34828, 2])
We keep 1.55e+07/8.01e+08 =  1% of the original kernel matrix.

torch.Size([32312, 2])
We keep 9.09e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([17269, 2])
We keep 4.51e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([23059, 2])
We keep 4.83e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([17248, 2])
We keep 6.49e+06/1.90e+08 =  3% of the original kernel matrix.

torch.Size([23088, 2])
We keep 5.20e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([20252, 2])
We keep 3.58e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([25320, 2])
We keep 4.64e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([9671, 2])
We keep 1.53e+06/2.51e+07 =  6% of the original kernel matrix.

torch.Size([17362, 2])
We keep 2.33e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([190499, 2])
We keep 2.86e+08/1.91e+10 =  1% of the original kernel matrix.

torch.Size([79112, 2])
We keep 3.51e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([31706, 2])
We keep 1.49e+07/5.79e+08 =  2% of the original kernel matrix.

torch.Size([31894, 2])
We keep 7.89e+06/4.94e+08 =  1% of the original kernel matrix.

torch.Size([5235, 2])
We keep 4.84e+05/7.39e+06 =  6% of the original kernel matrix.

torch.Size([13032, 2])
We keep 1.53e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([128060, 2])
We keep 9.21e+07/7.08e+09 =  1% of the original kernel matrix.

torch.Size([63690, 2])
We keep 2.24e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([21101, 2])
We keep 5.05e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([26004, 2])
We keep 4.96e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([3572, 2])
We keep 2.03e+05/3.01e+06 =  6% of the original kernel matrix.

torch.Size([11375, 2])
We keep 1.07e+06/3.56e+07 =  2% of the original kernel matrix.

torch.Size([53271, 2])
We keep 2.67e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([40746, 2])
We keep 1.14e+07/7.75e+08 =  1% of the original kernel matrix.

torch.Size([139601, 2])
We keep 1.07e+08/9.00e+09 =  1% of the original kernel matrix.

torch.Size([66768, 2])
We keep 2.54e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([222133, 2])
We keep 5.55e+08/3.04e+10 =  1% of the original kernel matrix.

torch.Size([85834, 2])
We keep 4.35e+07/3.58e+09 =  1% of the original kernel matrix.

torch.Size([170550, 2])
We keep 5.02e+08/2.24e+10 =  2% of the original kernel matrix.

torch.Size([73335, 2])
We keep 3.83e+07/3.07e+09 =  1% of the original kernel matrix.

torch.Size([186956, 2])
We keep 3.90e+08/2.47e+10 =  1% of the original kernel matrix.

torch.Size([76785, 2])
We keep 3.97e+07/3.23e+09 =  1% of the original kernel matrix.

torch.Size([27775, 2])
We keep 1.48e+07/5.05e+08 =  2% of the original kernel matrix.

torch.Size([29097, 2])
We keep 7.66e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([42248, 2])
We keep 4.05e+07/1.02e+09 =  3% of the original kernel matrix.

torch.Size([36200, 2])
We keep 9.72e+06/6.56e+08 =  1% of the original kernel matrix.

torch.Size([195206, 2])
We keep 1.75e+08/1.74e+10 =  1% of the original kernel matrix.

torch.Size([80087, 2])
We keep 3.35e+07/2.70e+09 =  1% of the original kernel matrix.

torch.Size([235070, 2])
We keep 3.38e+08/3.19e+10 =  1% of the original kernel matrix.

torch.Size([89126, 2])
We keep 4.28e+07/3.66e+09 =  1% of the original kernel matrix.

torch.Size([22698, 2])
We keep 6.90e+06/2.42e+08 =  2% of the original kernel matrix.

torch.Size([26949, 2])
We keep 5.63e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([180719, 2])
We keep 4.80e+09/5.53e+10 =  8% of the original kernel matrix.

torch.Size([77072, 2])
We keep 5.17e+07/4.83e+09 =  1% of the original kernel matrix.

torch.Size([444787, 2])
We keep 1.13e+10/1.41e+11 =  8% of the original kernel matrix.

torch.Size([121498, 2])
We keep 8.06e+07/7.71e+09 =  1% of the original kernel matrix.

torch.Size([206196, 2])
We keep 4.15e+08/2.74e+10 =  1% of the original kernel matrix.

torch.Size([82816, 2])
We keep 4.13e+07/3.40e+09 =  1% of the original kernel matrix.

torch.Size([150972, 2])
We keep 2.03e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([68983, 2])
We keep 3.04e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([48777, 2])
We keep 7.64e+07/2.13e+09 =  3% of the original kernel matrix.

torch.Size([36685, 2])
We keep 1.37e+07/9.47e+08 =  1% of the original kernel matrix.

torch.Size([15730, 2])
We keep 4.46e+06/1.23e+08 =  3% of the original kernel matrix.

torch.Size([21887, 2])
We keep 4.41e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([11177, 2])
We keep 1.11e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([18644, 2])
We keep 2.66e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([10268, 2])
We keep 3.20e+06/3.56e+07 =  8% of the original kernel matrix.

torch.Size([17687, 2])
We keep 2.59e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([167610, 2])
We keep 1.32e+08/1.29e+10 =  1% of the original kernel matrix.

torch.Size([73873, 2])
We keep 2.91e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([143489, 2])
We keep 1.15e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([68037, 2])
We keep 2.65e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([266441, 2])
We keep 6.15e+08/4.91e+10 =  1% of the original kernel matrix.

torch.Size([93267, 2])
We keep 5.41e+07/4.55e+09 =  1% of the original kernel matrix.

torch.Size([98401, 2])
We keep 1.11e+08/5.10e+09 =  2% of the original kernel matrix.

torch.Size([55309, 2])
We keep 1.98e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([72745, 2])
We keep 4.56e+07/2.70e+09 =  1% of the original kernel matrix.

torch.Size([47281, 2])
We keep 1.52e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([18538, 2])
We keep 3.74e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([23977, 2])
We keep 4.78e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([170637, 2])
We keep 1.86e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([74021, 2])
We keep 3.06e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([10503, 2])
We keep 2.58e+06/5.67e+07 =  4% of the original kernel matrix.

torch.Size([17220, 2])
We keep 3.28e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([16791, 2])
We keep 3.53e+06/1.16e+08 =  3% of the original kernel matrix.

torch.Size([22807, 2])
We keep 4.25e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([135108, 2])
We keep 1.27e+08/9.61e+09 =  1% of the original kernel matrix.

torch.Size([65645, 2])
We keep 2.60e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([69675, 2])
We keep 4.76e+07/2.36e+09 =  2% of the original kernel matrix.

torch.Size([46292, 2])
We keep 1.43e+07/9.97e+08 =  1% of the original kernel matrix.

torch.Size([6270, 2])
We keep 5.04e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([14344, 2])
We keep 1.74e+06/6.53e+07 =  2% of the original kernel matrix.

torch.Size([22502, 2])
We keep 4.46e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([26760, 2])
We keep 5.23e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([541221, 2])
We keep 3.39e+09/2.47e+11 =  1% of the original kernel matrix.

torch.Size([131052, 2])
We keep 1.14e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([21245, 2])
We keep 5.30e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([25925, 2])
We keep 5.38e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([131957, 2])
We keep 1.52e+08/1.00e+10 =  1% of the original kernel matrix.

torch.Size([64867, 2])
We keep 2.69e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([6531, 2])
We keep 1.31e+06/1.07e+07 = 12% of the original kernel matrix.

torch.Size([14607, 2])
We keep 1.59e+06/6.73e+07 =  2% of the original kernel matrix.

torch.Size([8815, 2])
We keep 1.50e+06/2.39e+07 =  6% of the original kernel matrix.

torch.Size([16542, 2])
We keep 2.16e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([9505, 2])
We keep 1.21e+06/3.02e+07 =  4% of the original kernel matrix.

torch.Size([16850, 2])
We keep 2.52e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([6055, 2])
We keep 6.45e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([13774, 2])
We keep 1.82e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([24580, 2])
We keep 5.34e+06/2.52e+08 =  2% of the original kernel matrix.

torch.Size([28273, 2])
We keep 5.67e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([60014, 2])
We keep 4.65e+07/2.01e+09 =  2% of the original kernel matrix.

torch.Size([42589, 2])
We keep 1.35e+07/9.20e+08 =  1% of the original kernel matrix.

torch.Size([16041, 2])
We keep 7.27e+06/1.06e+08 =  6% of the original kernel matrix.

torch.Size([22386, 2])
We keep 3.82e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([236472, 2])
We keep 4.10e+08/2.71e+10 =  1% of the original kernel matrix.

torch.Size([89369, 2])
We keep 4.06e+07/3.38e+09 =  1% of the original kernel matrix.

torch.Size([11947, 2])
We keep 2.06e+06/4.92e+07 =  4% of the original kernel matrix.

torch.Size([19232, 2])
We keep 2.99e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([91538, 2])
We keep 1.30e+08/3.77e+09 =  3% of the original kernel matrix.

torch.Size([53240, 2])
We keep 1.67e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([16850, 2])
We keep 4.64e+06/1.30e+08 =  3% of the original kernel matrix.

torch.Size([22857, 2])
We keep 4.37e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([141150, 2])
We keep 1.33e+08/9.92e+09 =  1% of the original kernel matrix.

torch.Size([67046, 2])
We keep 2.64e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([9745, 2])
We keep 1.52e+06/3.04e+07 =  5% of the original kernel matrix.

torch.Size([17241, 2])
We keep 2.57e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([23920, 2])
We keep 7.08e+06/2.44e+08 =  2% of the original kernel matrix.

torch.Size([27508, 2])
We keep 5.65e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([41091, 2])
We keep 1.41e+07/7.92e+08 =  1% of the original kernel matrix.

torch.Size([36543, 2])
We keep 9.05e+06/5.78e+08 =  1% of the original kernel matrix.

torch.Size([34572, 2])
We keep 1.69e+07/6.70e+08 =  2% of the original kernel matrix.

torch.Size([33186, 2])
We keep 8.59e+06/5.31e+08 =  1% of the original kernel matrix.

torch.Size([80789, 2])
We keep 4.55e+07/3.05e+09 =  1% of the original kernel matrix.

torch.Size([50175, 2])
We keep 1.59e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([875581, 2])
We keep 5.40e+09/3.96e+11 =  1% of the original kernel matrix.

torch.Size([175406, 2])
We keep 1.38e+08/1.29e+10 =  1% of the original kernel matrix.

torch.Size([20597, 2])
We keep 3.35e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([25649, 2])
We keep 4.65e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([9327, 2])
We keep 9.22e+05/2.30e+07 =  4% of the original kernel matrix.

torch.Size([17065, 2])
We keep 2.26e+06/9.84e+07 =  2% of the original kernel matrix.

torch.Size([37649, 2])
We keep 1.57e+07/7.22e+08 =  2% of the original kernel matrix.

torch.Size([34632, 2])
We keep 8.63e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([28623, 2])
We keep 1.01e+07/4.05e+08 =  2% of the original kernel matrix.

torch.Size([30329, 2])
We keep 6.95e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([543375, 2])
We keep 1.06e+09/1.39e+11 =  0% of the original kernel matrix.

torch.Size([136216, 2])
We keep 8.46e+07/7.66e+09 =  1% of the original kernel matrix.

torch.Size([10274, 2])
We keep 1.86e+06/3.73e+07 =  4% of the original kernel matrix.

torch.Size([17471, 2])
We keep 2.74e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([109418, 2])
We keep 1.19e+08/7.79e+09 =  1% of the original kernel matrix.

torch.Size([57678, 2])
We keep 2.41e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([271225, 2])
We keep 3.50e+08/4.43e+10 =  0% of the original kernel matrix.

torch.Size([97019, 2])
We keep 5.08e+07/4.32e+09 =  1% of the original kernel matrix.

torch.Size([7612, 2])
We keep 7.67e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([15429, 2])
We keep 1.98e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([307722, 2])
We keep 1.23e+09/7.10e+10 =  1% of the original kernel matrix.

torch.Size([98836, 2])
We keep 6.28e+07/5.47e+09 =  1% of the original kernel matrix.

torch.Size([19177, 2])
We keep 9.83e+06/2.53e+08 =  3% of the original kernel matrix.

torch.Size([24069, 2])
We keep 5.56e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([579264, 2])
We keep 1.83e+09/1.82e+11 =  1% of the original kernel matrix.

torch.Size([139150, 2])
We keep 9.72e+07/8.75e+09 =  1% of the original kernel matrix.

torch.Size([18227, 2])
We keep 3.07e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([23879, 2])
We keep 4.27e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([28777, 2])
We keep 1.48e+07/3.84e+08 =  3% of the original kernel matrix.

torch.Size([30611, 2])
We keep 6.46e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([11920, 2])
We keep 3.67e+06/9.20e+07 =  3% of the original kernel matrix.

torch.Size([18510, 2])
We keep 3.94e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([7539, 2])
We keep 1.69e+06/1.77e+07 =  9% of the original kernel matrix.

torch.Size([15455, 2])
We keep 1.96e+06/8.64e+07 =  2% of the original kernel matrix.

torch.Size([12294, 2])
We keep 2.08e+06/5.83e+07 =  3% of the original kernel matrix.

torch.Size([19209, 2])
We keep 3.30e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([43965, 2])
We keep 2.23e+07/9.17e+08 =  2% of the original kernel matrix.

torch.Size([37469, 2])
We keep 9.34e+06/6.22e+08 =  1% of the original kernel matrix.

torch.Size([8005, 2])
We keep 6.79e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([16050, 2])
We keep 1.89e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([145477, 2])
We keep 1.58e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([68520, 2])
We keep 2.81e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([18889, 2])
We keep 3.63e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([24388, 2])
We keep 4.64e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([12163, 2])
We keep 1.81e+06/4.37e+07 =  4% of the original kernel matrix.

torch.Size([19333, 2])
We keep 2.94e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([25677, 2])
We keep 7.26e+06/3.30e+08 =  2% of the original kernel matrix.

torch.Size([27114, 2])
We keep 5.90e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([188326, 2])
We keep 3.37e+08/2.54e+10 =  1% of the original kernel matrix.

torch.Size([77245, 2])
We keep 4.03e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([132338, 2])
We keep 2.32e+08/1.39e+10 =  1% of the original kernel matrix.

torch.Size([63644, 2])
We keep 3.13e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([386315, 2])
We keep 5.32e+08/6.57e+10 =  0% of the original kernel matrix.

torch.Size([114626, 2])
We keep 6.03e+07/5.26e+09 =  1% of the original kernel matrix.

torch.Size([18044, 2])
We keep 5.43e+06/1.14e+08 =  4% of the original kernel matrix.

torch.Size([23739, 2])
We keep 3.91e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([11612, 2])
We keep 1.79e+06/5.20e+07 =  3% of the original kernel matrix.

torch.Size([19013, 2])
We keep 3.13e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([6806, 2])
We keep 5.39e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([14888, 2])
We keep 1.65e+06/6.53e+07 =  2% of the original kernel matrix.

torch.Size([6126, 2])
We keep 5.77e+05/1.03e+07 =  5% of the original kernel matrix.

torch.Size([14199, 2])
We keep 1.74e+06/6.58e+07 =  2% of the original kernel matrix.

torch.Size([214938, 2])
We keep 7.61e+08/2.28e+10 =  3% of the original kernel matrix.

torch.Size([84877, 2])
We keep 3.39e+07/3.10e+09 =  1% of the original kernel matrix.

torch.Size([10900, 2])
We keep 1.35e+06/3.54e+07 =  3% of the original kernel matrix.

torch.Size([18160, 2])
We keep 2.68e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([30854, 2])
We keep 1.07e+07/4.55e+08 =  2% of the original kernel matrix.

torch.Size([31509, 2])
We keep 7.32e+06/4.38e+08 =  1% of the original kernel matrix.

torch.Size([26199, 2])
We keep 1.18e+07/3.90e+08 =  3% of the original kernel matrix.

torch.Size([28611, 2])
We keep 6.54e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([25108, 2])
We keep 1.15e+07/4.32e+08 =  2% of the original kernel matrix.

torch.Size([27105, 2])
We keep 7.09e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([34310, 2])
We keep 2.60e+07/7.81e+08 =  3% of the original kernel matrix.

torch.Size([32577, 2])
We keep 9.21e+06/5.74e+08 =  1% of the original kernel matrix.

torch.Size([7499, 2])
We keep 1.22e+06/2.03e+07 =  5% of the original kernel matrix.

torch.Size([15036, 2])
We keep 2.24e+06/9.25e+07 =  2% of the original kernel matrix.

torch.Size([165213, 2])
We keep 2.81e+08/2.03e+10 =  1% of the original kernel matrix.

torch.Size([71848, 2])
We keep 3.63e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([61677, 2])
We keep 4.88e+07/2.37e+09 =  2% of the original kernel matrix.

torch.Size([43274, 2])
We keep 1.45e+07/9.99e+08 =  1% of the original kernel matrix.

torch.Size([9666, 2])
We keep 1.40e+06/3.26e+07 =  4% of the original kernel matrix.

torch.Size([17488, 2])
We keep 2.42e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([26835, 2])
We keep 1.20e+07/3.25e+08 =  3% of the original kernel matrix.

torch.Size([28375, 2])
We keep 5.73e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([82929, 2])
We keep 1.05e+08/3.53e+09 =  2% of the original kernel matrix.

torch.Size([50322, 2])
We keep 1.69e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([116208, 2])
We keep 1.23e+08/8.12e+09 =  1% of the original kernel matrix.

torch.Size([60340, 2])
We keep 2.44e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([504703, 2])
We keep 1.61e+09/1.42e+11 =  1% of the original kernel matrix.

torch.Size([129410, 2])
We keep 8.75e+07/7.74e+09 =  1% of the original kernel matrix.

torch.Size([145185, 2])
We keep 1.53e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([68430, 2])
We keep 2.64e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([8389, 2])
We keep 7.96e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([16361, 2])
We keep 2.12e+06/9.07e+07 =  2% of the original kernel matrix.

torch.Size([43885, 2])
We keep 1.32e+07/8.98e+08 =  1% of the original kernel matrix.

torch.Size([37420, 2])
We keep 9.28e+06/6.15e+08 =  1% of the original kernel matrix.

torch.Size([54503, 2])
We keep 6.71e+07/2.62e+09 =  2% of the original kernel matrix.

torch.Size([37898, 2])
We keep 1.51e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([14021, 2])
We keep 2.34e+06/6.59e+07 =  3% of the original kernel matrix.

torch.Size([20829, 2])
We keep 3.36e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([16257, 2])
We keep 2.73e+06/9.82e+07 =  2% of the original kernel matrix.

torch.Size([22341, 2])
We keep 4.00e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([11265, 2])
We keep 1.03e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([18650, 2])
We keep 2.57e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([27015, 2])
We keep 3.02e+07/3.62e+08 =  8% of the original kernel matrix.

torch.Size([29497, 2])
We keep 6.45e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([41884, 2])
We keep 3.05e+07/1.31e+09 =  2% of the original kernel matrix.

torch.Size([34855, 2])
We keep 1.14e+07/7.43e+08 =  1% of the original kernel matrix.

torch.Size([7080, 2])
We keep 6.77e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([14971, 2])
We keep 1.77e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([19707, 2])
We keep 3.64e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([25008, 2])
We keep 4.69e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([11577, 2])
We keep 2.11e+06/5.72e+07 =  3% of the original kernel matrix.

torch.Size([18616, 2])
We keep 3.04e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([17443, 2])
We keep 1.05e+07/2.05e+08 =  5% of the original kernel matrix.

torch.Size([23173, 2])
We keep 4.98e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([7937, 2])
We keep 7.38e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([15804, 2])
We keep 2.00e+06/8.04e+07 =  2% of the original kernel matrix.

torch.Size([33119, 2])
We keep 1.30e+07/5.32e+08 =  2% of the original kernel matrix.

torch.Size([32573, 2])
We keep 7.60e+06/4.73e+08 =  1% of the original kernel matrix.

torch.Size([19463, 2])
We keep 5.29e+07/4.72e+08 = 11% of the original kernel matrix.

torch.Size([23713, 2])
We keep 6.53e+06/4.46e+08 =  1% of the original kernel matrix.

torch.Size([15172, 2])
We keep 2.74e+06/9.04e+07 =  3% of the original kernel matrix.

torch.Size([21786, 2])
We keep 3.82e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([11015, 2])
We keep 1.62e+06/4.00e+07 =  4% of the original kernel matrix.

torch.Size([18361, 2])
We keep 2.82e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([15288, 2])
We keep 2.46e+06/8.30e+07 =  2% of the original kernel matrix.

torch.Size([21653, 2])
We keep 3.79e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([33385, 2])
We keep 1.36e+07/5.21e+08 =  2% of the original kernel matrix.

torch.Size([33133, 2])
We keep 7.35e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([168514, 2])
We keep 2.44e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([74063, 2])
We keep 3.10e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([12163, 2])
We keep 1.47e+06/4.18e+07 =  3% of the original kernel matrix.

torch.Size([19358, 2])
We keep 2.84e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([4093, 2])
We keep 1.96e+05/3.49e+06 =  5% of the original kernel matrix.

torch.Size([11925, 2])
We keep 1.18e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([8733, 2])
We keep 9.62e+05/1.93e+07 =  4% of the original kernel matrix.

torch.Size([16490, 2])
We keep 2.18e+06/9.01e+07 =  2% of the original kernel matrix.

torch.Size([94026, 2])
We keep 9.46e+07/4.98e+09 =  1% of the original kernel matrix.

torch.Size([53775, 2])
We keep 1.96e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([392348, 2])
We keep 1.17e+09/7.49e+10 =  1% of the original kernel matrix.

torch.Size([115792, 2])
We keep 6.24e+07/5.62e+09 =  1% of the original kernel matrix.

torch.Size([87076, 2])
We keep 1.08e+08/6.27e+09 =  1% of the original kernel matrix.

torch.Size([49196, 2])
We keep 2.20e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([13119, 2])
We keep 2.32e+06/6.10e+07 =  3% of the original kernel matrix.

torch.Size([20015, 2])
We keep 3.26e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([207039, 2])
We keep 4.49e+08/2.98e+10 =  1% of the original kernel matrix.

torch.Size([82198, 2])
We keep 4.33e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([206207, 2])
We keep 1.84e+08/1.98e+10 =  0% of the original kernel matrix.

torch.Size([82287, 2])
We keep 3.54e+07/2.89e+09 =  1% of the original kernel matrix.

torch.Size([5788, 2])
We keep 3.29e+05/6.85e+06 =  4% of the original kernel matrix.

torch.Size([13864, 2])
We keep 1.48e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([11310, 2])
We keep 2.16e+06/5.28e+07 =  4% of the original kernel matrix.

torch.Size([18764, 2])
We keep 3.13e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([53863, 2])
We keep 1.75e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([41618, 2])
We keep 1.07e+07/7.29e+08 =  1% of the original kernel matrix.

torch.Size([71111, 2])
We keep 2.07e+08/4.69e+09 =  4% of the original kernel matrix.

torch.Size([45666, 2])
We keep 1.84e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([105605, 2])
We keep 7.53e+08/1.15e+10 =  6% of the original kernel matrix.

torch.Size([56570, 2])
We keep 2.70e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([191469, 2])
We keep 2.52e+08/2.20e+10 =  1% of the original kernel matrix.

torch.Size([79386, 2])
We keep 3.78e+07/3.04e+09 =  1% of the original kernel matrix.

torch.Size([44587, 2])
We keep 3.14e+07/8.80e+08 =  3% of the original kernel matrix.

torch.Size([38221, 2])
We keep 9.19e+06/6.09e+08 =  1% of the original kernel matrix.

torch.Size([9302, 2])
We keep 9.11e+05/2.12e+07 =  4% of the original kernel matrix.

torch.Size([17007, 2])
We keep 2.20e+06/9.44e+07 =  2% of the original kernel matrix.

torch.Size([6342, 2])
We keep 3.91e+05/8.54e+06 =  4% of the original kernel matrix.

torch.Size([14332, 2])
We keep 1.61e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([48970, 2])
We keep 3.14e+07/1.26e+09 =  2% of the original kernel matrix.

torch.Size([39035, 2])
We keep 1.09e+07/7.28e+08 =  1% of the original kernel matrix.

torch.Size([20746, 2])
We keep 3.92e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([25607, 2])
We keep 4.80e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([21425, 2])
We keep 6.58e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([25593, 2])
We keep 5.71e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([136006, 2])
We keep 1.39e+08/9.37e+09 =  1% of the original kernel matrix.

torch.Size([64923, 2])
We keep 2.60e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([686616, 2])
We keep 1.49e+09/2.02e+11 =  0% of the original kernel matrix.

torch.Size([154687, 2])
We keep 1.01e+08/9.22e+09 =  1% of the original kernel matrix.

torch.Size([12348, 2])
We keep 2.83e+06/5.91e+07 =  4% of the original kernel matrix.

torch.Size([19212, 2])
We keep 3.32e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([4022, 2])
We keep 1.84e+05/3.18e+06 =  5% of the original kernel matrix.

torch.Size([11877, 2])
We keep 1.12e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([41616, 2])
We keep 2.21e+07/9.10e+08 =  2% of the original kernel matrix.

torch.Size([36540, 2])
We keep 9.43e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([69555, 2])
We keep 8.92e+07/2.63e+09 =  3% of the original kernel matrix.

torch.Size([46718, 2])
We keep 1.47e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([77946, 2])
We keep 7.97e+07/2.76e+09 =  2% of the original kernel matrix.

torch.Size([49169, 2])
We keep 1.49e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([14400, 2])
We keep 1.74e+07/1.30e+08 = 13% of the original kernel matrix.

torch.Size([20603, 2])
We keep 4.03e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([7900, 2])
We keep 7.61e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([15765, 2])
We keep 1.97e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([52140, 2])
We keep 2.80e+07/1.30e+09 =  2% of the original kernel matrix.

torch.Size([40496, 2])
We keep 1.11e+07/7.41e+08 =  1% of the original kernel matrix.

torch.Size([8494, 2])
We keep 1.27e+06/2.19e+07 =  5% of the original kernel matrix.

torch.Size([16186, 2])
We keep 2.29e+06/9.62e+07 =  2% of the original kernel matrix.

torch.Size([41404, 2])
We keep 3.79e+07/1.41e+09 =  2% of the original kernel matrix.

torch.Size([34838, 2])
We keep 1.18e+07/7.71e+08 =  1% of the original kernel matrix.

torch.Size([11537, 2])
We keep 1.41e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([18937, 2])
We keep 2.79e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([21866, 2])
We keep 3.58e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([26603, 2])
We keep 4.93e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([11865, 2])
We keep 2.15e+06/5.50e+07 =  3% of the original kernel matrix.

torch.Size([19005, 2])
We keep 3.23e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([15465, 2])
We keep 2.13e+06/7.78e+07 =  2% of the original kernel matrix.

torch.Size([21968, 2])
We keep 3.55e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([15031, 2])
We keep 2.19e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([21419, 2])
We keep 3.58e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([187204, 2])
We keep 1.75e+08/1.62e+10 =  1% of the original kernel matrix.

torch.Size([78634, 2])
We keep 3.24e+07/2.61e+09 =  1% of the original kernel matrix.

torch.Size([262425, 2])
We keep 4.21e+08/4.09e+10 =  1% of the original kernel matrix.

torch.Size([94448, 2])
We keep 4.98e+07/4.15e+09 =  1% of the original kernel matrix.

torch.Size([97506, 2])
We keep 1.17e+08/5.64e+09 =  2% of the original kernel matrix.

torch.Size([55102, 2])
We keep 2.04e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([146765, 2])
We keep 2.63e+08/1.33e+10 =  1% of the original kernel matrix.

torch.Size([68161, 2])
We keep 3.06e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([29740, 2])
We keep 7.24e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([31452, 2])
We keep 6.90e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([17457, 2])
We keep 6.18e+06/1.31e+08 =  4% of the original kernel matrix.

torch.Size([23156, 2])
We keep 4.42e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([22223, 2])
We keep 5.57e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([26487, 2])
We keep 5.59e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([152094, 2])
We keep 1.36e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([70392, 2])
We keep 2.79e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([39507, 2])
We keep 2.66e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([34558, 2])
We keep 1.01e+07/6.54e+08 =  1% of the original kernel matrix.

torch.Size([32832, 2])
We keep 1.92e+07/5.20e+08 =  3% of the original kernel matrix.

torch.Size([32284, 2])
We keep 7.64e+06/4.68e+08 =  1% of the original kernel matrix.

torch.Size([16654, 2])
We keep 3.08e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([22845, 2])
We keep 4.11e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([1170338, 2])
We keep 5.59e+09/6.93e+11 =  0% of the original kernel matrix.

torch.Size([203731, 2])
We keep 1.80e+08/1.71e+10 =  1% of the original kernel matrix.

torch.Size([28159, 2])
We keep 7.78e+06/3.44e+08 =  2% of the original kernel matrix.

torch.Size([29635, 2])
We keep 6.24e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([75432, 2])
We keep 4.87e+07/2.94e+09 =  1% of the original kernel matrix.

torch.Size([48047, 2])
We keep 1.57e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([132155, 2])
We keep 1.61e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([64702, 2])
We keep 2.60e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([68923, 2])
We keep 4.39e+07/2.04e+09 =  2% of the original kernel matrix.

torch.Size([46478, 2])
We keep 1.28e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([32342, 2])
We keep 1.07e+07/4.84e+08 =  2% of the original kernel matrix.

torch.Size([32291, 2])
We keep 7.41e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([729316, 2])
We keep 3.98e+09/3.43e+11 =  1% of the original kernel matrix.

torch.Size([161370, 2])
We keep 1.31e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([193166, 2])
We keep 1.72e+08/1.81e+10 =  0% of the original kernel matrix.

torch.Size([79650, 2])
We keep 3.39e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([102081, 2])
We keep 9.82e+07/5.41e+09 =  1% of the original kernel matrix.

torch.Size([55989, 2])
We keep 2.05e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([16643, 2])
We keep 7.18e+06/1.12e+08 =  6% of the original kernel matrix.

torch.Size([22667, 2])
We keep 3.87e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([47768, 2])
We keep 2.73e+07/1.18e+09 =  2% of the original kernel matrix.

torch.Size([38927, 2])
We keep 1.07e+07/7.06e+08 =  1% of the original kernel matrix.

torch.Size([12463, 2])
We keep 2.33e+06/5.24e+07 =  4% of the original kernel matrix.

torch.Size([19772, 2])
We keep 2.92e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([201341, 2])
We keep 6.24e+08/2.00e+10 =  3% of the original kernel matrix.

torch.Size([81265, 2])
We keep 3.62e+07/2.90e+09 =  1% of the original kernel matrix.

torch.Size([1150501, 2])
We keep 5.56e+09/6.63e+11 =  0% of the original kernel matrix.

torch.Size([199183, 2])
We keep 1.77e+08/1.67e+10 =  1% of the original kernel matrix.

torch.Size([90977, 2])
We keep 1.96e+08/4.45e+09 =  4% of the original kernel matrix.

torch.Size([52900, 2])
We keep 1.89e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([171459, 2])
We keep 2.40e+08/1.84e+10 =  1% of the original kernel matrix.

torch.Size([73626, 2])
We keep 3.49e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([106835, 2])
We keep 2.70e+08/7.32e+09 =  3% of the original kernel matrix.

torch.Size([57585, 2])
We keep 2.37e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([33021, 2])
We keep 1.62e+07/5.64e+08 =  2% of the original kernel matrix.

torch.Size([32428, 2])
We keep 7.96e+06/4.88e+08 =  1% of the original kernel matrix.

torch.Size([29141, 2])
We keep 1.74e+07/5.24e+08 =  3% of the original kernel matrix.

torch.Size([30005, 2])
We keep 7.30e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([15523, 2])
We keep 4.12e+06/1.19e+08 =  3% of the original kernel matrix.

torch.Size([21712, 2])
We keep 4.25e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([43510, 2])
We keep 1.99e+08/1.61e+09 = 12% of the original kernel matrix.

torch.Size([36542, 2])
We keep 1.18e+07/8.23e+08 =  1% of the original kernel matrix.

torch.Size([439290, 2])
We keep 6.60e+08/8.51e+10 =  0% of the original kernel matrix.

torch.Size([122041, 2])
We keep 6.76e+07/5.99e+09 =  1% of the original kernel matrix.

torch.Size([93969, 2])
We keep 1.15e+08/6.35e+09 =  1% of the original kernel matrix.

torch.Size([52555, 2])
We keep 2.22e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([466033, 2])
We keep 3.32e+09/1.55e+11 =  2% of the original kernel matrix.

torch.Size([122175, 2])
We keep 8.83e+07/8.08e+09 =  1% of the original kernel matrix.

torch.Size([151876, 2])
We keep 2.76e+08/1.52e+10 =  1% of the original kernel matrix.

torch.Size([69365, 2])
We keep 3.23e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([57900, 2])
We keep 2.63e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([42734, 2])
We keep 1.22e+07/8.28e+08 =  1% of the original kernel matrix.

torch.Size([18735, 2])
We keep 6.78e+06/1.80e+08 =  3% of the original kernel matrix.

torch.Size([24057, 2])
We keep 5.08e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([79783, 2])
We keep 5.82e+07/2.89e+09 =  2% of the original kernel matrix.

torch.Size([49905, 2])
We keep 1.56e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([176346, 2])
We keep 1.55e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([75782, 2])
We keep 3.02e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([14948, 2])
We keep 4.30e+06/1.14e+08 =  3% of the original kernel matrix.

torch.Size([21172, 2])
We keep 4.24e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([598263, 2])
We keep 1.33e+09/1.71e+11 =  0% of the original kernel matrix.

torch.Size([142657, 2])
We keep 9.34e+07/8.49e+09 =  1% of the original kernel matrix.

torch.Size([30658, 2])
We keep 1.46e+07/4.80e+08 =  3% of the original kernel matrix.

torch.Size([31274, 2])
We keep 6.53e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([38124, 2])
We keep 1.84e+07/7.90e+08 =  2% of the original kernel matrix.

torch.Size([34400, 2])
We keep 9.17e+06/5.77e+08 =  1% of the original kernel matrix.

torch.Size([9662, 2])
We keep 3.39e+06/3.03e+07 = 11% of the original kernel matrix.

torch.Size([17512, 2])
We keep 2.19e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([16658, 2])
We keep 2.80e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([22759, 2])
We keep 4.02e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([3700, 2])
We keep 2.32e+05/3.27e+06 =  7% of the original kernel matrix.

torch.Size([11354, 2])
We keep 1.14e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([6898, 2])
We keep 1.03e+06/1.33e+07 =  7% of the original kernel matrix.

torch.Size([14869, 2])
We keep 1.74e+06/7.49e+07 =  2% of the original kernel matrix.

torch.Size([25993, 2])
We keep 7.62e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([29012, 2])
We keep 6.05e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([54468, 2])
We keep 4.16e+07/1.68e+09 =  2% of the original kernel matrix.

torch.Size([40830, 2])
We keep 1.25e+07/8.41e+08 =  1% of the original kernel matrix.

torch.Size([2993477, 2])
We keep 3.74e+10/4.85e+12 =  0% of the original kernel matrix.

torch.Size([323743, 2])
We keep 4.49e+08/4.52e+10 =  0% of the original kernel matrix.

torch.Size([12394, 2])
We keep 3.41e+06/7.91e+07 =  4% of the original kernel matrix.

torch.Size([19415, 2])
We keep 3.66e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([229995, 2])
We keep 2.34e+08/2.60e+10 =  0% of the original kernel matrix.

torch.Size([87896, 2])
We keep 3.98e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([14443, 2])
We keep 2.74e+06/7.51e+07 =  3% of the original kernel matrix.

torch.Size([21186, 2])
We keep 3.58e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([31652, 2])
We keep 1.04e+07/4.74e+08 =  2% of the original kernel matrix.

torch.Size([31923, 2])
We keep 7.27e+06/4.47e+08 =  1% of the original kernel matrix.

torch.Size([11065, 2])
We keep 2.31e+06/4.92e+07 =  4% of the original kernel matrix.

torch.Size([18111, 2])
We keep 3.07e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([173855, 2])
We keep 3.73e+08/2.42e+10 =  1% of the original kernel matrix.

torch.Size([73548, 2])
We keep 3.95e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([30319, 2])
We keep 5.19e+07/5.59e+08 =  9% of the original kernel matrix.

torch.Size([30769, 2])
We keep 7.60e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([763170, 2])
We keep 2.07e+09/2.75e+11 =  0% of the original kernel matrix.

torch.Size([165271, 2])
We keep 1.16e+08/1.08e+10 =  1% of the original kernel matrix.

torch.Size([9388, 2])
We keep 9.97e+05/2.32e+07 =  4% of the original kernel matrix.

torch.Size([17019, 2])
We keep 2.28e+06/9.89e+07 =  2% of the original kernel matrix.

torch.Size([196688, 2])
We keep 2.24e+08/1.86e+10 =  1% of the original kernel matrix.

torch.Size([80546, 2])
We keep 3.46e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([27043, 2])
We keep 1.10e+07/3.34e+08 =  3% of the original kernel matrix.

torch.Size([28357, 2])
We keep 5.99e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([142166, 2])
We keep 1.77e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([67391, 2])
We keep 2.82e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([299129, 2])
We keep 8.55e+08/5.72e+10 =  1% of the original kernel matrix.

torch.Size([99584, 2])
We keep 5.71e+07/4.91e+09 =  1% of the original kernel matrix.

torch.Size([40787, 2])
We keep 1.62e+07/7.51e+08 =  2% of the original kernel matrix.

torch.Size([35998, 2])
We keep 8.83e+06/5.63e+08 =  1% of the original kernel matrix.

torch.Size([18756, 2])
We keep 4.29e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([24373, 2])
We keep 4.80e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([108118, 2])
We keep 6.40e+07/5.43e+09 =  1% of the original kernel matrix.

torch.Size([58186, 2])
We keep 2.01e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([19868, 2])
We keep 4.65e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([25088, 2])
We keep 4.76e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([28132, 2])
We keep 1.32e+07/3.90e+08 =  3% of the original kernel matrix.

torch.Size([29596, 2])
We keep 6.75e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([202449, 2])
We keep 3.03e+08/2.10e+10 =  1% of the original kernel matrix.

torch.Size([80842, 2])
We keep 3.64e+07/2.97e+09 =  1% of the original kernel matrix.

torch.Size([12790, 2])
We keep 2.22e+06/6.97e+07 =  3% of the original kernel matrix.

torch.Size([20051, 2])
We keep 3.25e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([69100, 2])
We keep 7.41e+07/3.11e+09 =  2% of the original kernel matrix.

torch.Size([45061, 2])
We keep 1.63e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([42932, 2])
We keep 2.96e+07/1.04e+09 =  2% of the original kernel matrix.

torch.Size([36166, 2])
We keep 1.01e+07/6.61e+08 =  1% of the original kernel matrix.

torch.Size([121228, 2])
We keep 1.45e+08/8.68e+09 =  1% of the original kernel matrix.

torch.Size([61650, 2])
We keep 2.52e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([184846, 2])
We keep 3.69e+08/2.29e+10 =  1% of the original kernel matrix.

torch.Size([76713, 2])
We keep 3.78e+07/3.11e+09 =  1% of the original kernel matrix.

torch.Size([75937, 2])
We keep 4.44e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([48304, 2])
We keep 1.48e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([20680, 2])
We keep 7.75e+06/1.69e+08 =  4% of the original kernel matrix.

torch.Size([25547, 2])
We keep 4.75e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([100216, 2])
We keep 1.54e+08/6.79e+09 =  2% of the original kernel matrix.

torch.Size([55090, 2])
We keep 2.23e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([100155, 2])
We keep 1.19e+08/6.46e+09 =  1% of the original kernel matrix.

torch.Size([54577, 2])
We keep 2.23e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([76538, 2])
We keep 7.43e+08/7.52e+09 =  9% of the original kernel matrix.

torch.Size([47932, 2])
We keep 2.27e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([48139, 2])
We keep 2.77e+07/1.17e+09 =  2% of the original kernel matrix.

torch.Size([38806, 2])
We keep 1.05e+07/7.03e+08 =  1% of the original kernel matrix.

torch.Size([30586, 2])
We keep 2.00e+07/6.15e+08 =  3% of the original kernel matrix.

torch.Size([30608, 2])
We keep 7.72e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([35666, 2])
We keep 2.00e+07/6.56e+08 =  3% of the original kernel matrix.

torch.Size([33645, 2])
We keep 8.36e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([122268, 2])
We keep 3.97e+08/1.78e+10 =  2% of the original kernel matrix.

torch.Size([58202, 2])
We keep 3.48e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([80725, 2])
We keep 1.87e+08/5.15e+09 =  3% of the original kernel matrix.

torch.Size([49498, 2])
We keep 1.97e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([28940, 2])
We keep 7.40e+06/3.61e+08 =  2% of the original kernel matrix.

torch.Size([31141, 2])
We keep 6.68e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([13893, 2])
We keep 3.16e+06/7.59e+07 =  4% of the original kernel matrix.

torch.Size([20636, 2])
We keep 3.57e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([8706, 2])
We keep 8.16e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([16508, 2])
We keep 2.11e+06/8.91e+07 =  2% of the original kernel matrix.

torch.Size([50593, 2])
We keep 5.75e+07/2.12e+09 =  2% of the original kernel matrix.

torch.Size([38493, 2])
We keep 1.38e+07/9.45e+08 =  1% of the original kernel matrix.

torch.Size([106892, 2])
We keep 2.04e+08/7.71e+09 =  2% of the original kernel matrix.

torch.Size([56870, 2])
We keep 2.41e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([111964, 2])
We keep 7.18e+07/5.31e+09 =  1% of the original kernel matrix.

torch.Size([59034, 2])
We keep 2.01e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([14450, 2])
We keep 2.26e+06/6.83e+07 =  3% of the original kernel matrix.

torch.Size([21141, 2])
We keep 3.49e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([30927, 2])
We keep 8.47e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([31431, 2])
We keep 7.23e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([37450, 2])
We keep 1.54e+07/6.67e+08 =  2% of the original kernel matrix.

torch.Size([34574, 2])
We keep 8.48e+06/5.30e+08 =  1% of the original kernel matrix.

torch.Size([68959, 2])
We keep 3.55e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([46390, 2])
We keep 1.43e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([39701, 2])
We keep 1.89e+07/7.43e+08 =  2% of the original kernel matrix.

torch.Size([35804, 2])
We keep 8.84e+06/5.60e+08 =  1% of the original kernel matrix.

torch.Size([83710, 2])
We keep 3.93e+07/3.01e+09 =  1% of the original kernel matrix.

torch.Size([50577, 2])
We keep 1.57e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([14426, 2])
We keep 4.09e+06/8.83e+07 =  4% of the original kernel matrix.

torch.Size([21242, 2])
We keep 3.73e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([40803, 2])
We keep 1.80e+07/8.19e+08 =  2% of the original kernel matrix.

torch.Size([36233, 2])
We keep 8.84e+06/5.87e+08 =  1% of the original kernel matrix.

torch.Size([5618, 2])
We keep 4.91e+05/7.96e+06 =  6% of the original kernel matrix.

torch.Size([13534, 2])
We keep 1.55e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([10215, 2])
We keep 1.28e+06/3.03e+07 =  4% of the original kernel matrix.

torch.Size([17670, 2])
We keep 2.55e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([248078, 2])
We keep 4.84e+08/2.93e+10 =  1% of the original kernel matrix.

torch.Size([92147, 2])
We keep 4.04e+07/3.52e+09 =  1% of the original kernel matrix.

torch.Size([21723, 2])
We keep 6.45e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([25972, 2])
We keep 5.54e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([133175, 2])
We keep 1.55e+08/7.75e+09 =  1% of the original kernel matrix.

torch.Size([65104, 2])
We keep 2.28e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([6807, 2])
We keep 6.54e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([14822, 2])
We keep 1.83e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([9927, 2])
We keep 3.41e+06/3.95e+07 =  8% of the original kernel matrix.

torch.Size([17387, 2])
We keep 2.85e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([32441, 2])
We keep 2.06e+07/5.41e+08 =  3% of the original kernel matrix.

torch.Size([31898, 2])
We keep 7.41e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([78759, 2])
We keep 4.90e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([48712, 2])
We keep 1.63e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([49480, 2])
We keep 2.19e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([39778, 2])
We keep 1.07e+07/7.06e+08 =  1% of the original kernel matrix.

torch.Size([8602, 2])
We keep 1.36e+06/2.10e+07 =  6% of the original kernel matrix.

torch.Size([16355, 2])
We keep 2.18e+06/9.41e+07 =  2% of the original kernel matrix.

torch.Size([16765, 2])
We keep 3.29e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([22916, 2])
We keep 4.08e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([54186, 2])
We keep 3.67e+07/1.53e+09 =  2% of the original kernel matrix.

torch.Size([41398, 2])
We keep 1.21e+07/8.04e+08 =  1% of the original kernel matrix.

torch.Size([29749, 2])
We keep 8.04e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([31126, 2])
We keep 7.00e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([99292, 2])
We keep 1.62e+08/5.63e+09 =  2% of the original kernel matrix.

torch.Size([55470, 2])
We keep 2.05e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([9523, 2])
We keep 1.01e+06/2.69e+07 =  3% of the original kernel matrix.

torch.Size([17354, 2])
We keep 2.35e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([10031, 2])
We keep 3.57e+06/4.32e+07 =  8% of the original kernel matrix.

torch.Size([17451, 2])
We keep 2.71e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([70967, 2])
We keep 7.15e+07/3.41e+09 =  2% of the original kernel matrix.

torch.Size([45217, 2])
We keep 1.70e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([10180, 2])
We keep 1.44e+06/3.43e+07 =  4% of the original kernel matrix.

torch.Size([17705, 2])
We keep 2.67e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([156179, 2])
We keep 2.10e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([71281, 2])
We keep 2.92e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([22527, 2])
We keep 2.94e+07/2.92e+08 = 10% of the original kernel matrix.

torch.Size([26717, 2])
We keep 5.57e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([78751, 2])
We keep 7.19e+07/3.73e+09 =  1% of the original kernel matrix.

torch.Size([48902, 2])
We keep 1.75e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([146835, 2])
We keep 1.37e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([69103, 2])
We keep 2.72e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([90486, 2])
We keep 1.79e+08/4.39e+09 =  4% of the original kernel matrix.

torch.Size([53026, 2])
We keep 1.88e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([145653, 2])
We keep 1.66e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([68397, 2])
We keep 2.78e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([142742, 2])
We keep 1.13e+08/9.60e+09 =  1% of the original kernel matrix.

torch.Size([67808, 2])
We keep 2.60e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([9530, 2])
We keep 1.40e+06/2.50e+07 =  5% of the original kernel matrix.

torch.Size([17351, 2])
We keep 2.26e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([66117, 2])
We keep 3.01e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([45271, 2])
We keep 1.31e+07/8.96e+08 =  1% of the original kernel matrix.

torch.Size([36447, 2])
We keep 1.31e+07/6.02e+08 =  2% of the original kernel matrix.

torch.Size([34602, 2])
We keep 7.85e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([75349, 2])
We keep 5.93e+07/2.59e+09 =  2% of the original kernel matrix.

torch.Size([48135, 2])
We keep 1.46e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([152231, 2])
We keep 2.31e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([69617, 2])
We keep 3.01e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([35123, 2])
We keep 3.30e+07/8.98e+08 =  3% of the original kernel matrix.

torch.Size([32326, 2])
We keep 9.69e+06/6.15e+08 =  1% of the original kernel matrix.

torch.Size([84928, 2])
We keep 4.22e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([51213, 2])
We keep 1.61e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([26021, 2])
We keep 6.66e+06/2.97e+08 =  2% of the original kernel matrix.

torch.Size([28982, 2])
We keep 6.12e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([30282, 2])
We keep 3.17e+07/1.03e+09 =  3% of the original kernel matrix.

torch.Size([28816, 2])
We keep 1.02e+07/6.59e+08 =  1% of the original kernel matrix.

torch.Size([8409, 2])
We keep 6.91e+05/1.67e+07 =  4% of the original kernel matrix.

torch.Size([16395, 2])
We keep 2.03e+06/8.39e+07 =  2% of the original kernel matrix.

torch.Size([15725, 2])
We keep 3.45e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([22094, 2])
We keep 4.01e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([178069, 2])
We keep 3.56e+08/1.52e+10 =  2% of the original kernel matrix.

torch.Size([76233, 2])
We keep 3.18e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([315203, 2])
We keep 6.61e+08/5.40e+10 =  1% of the original kernel matrix.

torch.Size([103486, 2])
We keep 5.54e+07/4.77e+09 =  1% of the original kernel matrix.

torch.Size([20393, 2])
We keep 5.90e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([25086, 2])
We keep 5.56e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([21283, 2])
We keep 4.49e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([25948, 2])
We keep 4.87e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([33301, 2])
We keep 1.01e+07/5.14e+08 =  1% of the original kernel matrix.

torch.Size([32986, 2])
We keep 7.64e+06/4.65e+08 =  1% of the original kernel matrix.

torch.Size([84051, 2])
We keep 7.30e+07/3.28e+09 =  2% of the original kernel matrix.

torch.Size([51268, 2])
We keep 1.58e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([77405, 2])
We keep 3.75e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([48791, 2])
We keep 1.49e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([10128, 2])
We keep 9.41e+05/2.87e+07 =  3% of the original kernel matrix.

torch.Size([17867, 2])
We keep 2.43e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([52181, 2])
We keep 2.48e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([40778, 2])
We keep 1.06e+07/7.29e+08 =  1% of the original kernel matrix.

torch.Size([204691, 2])
We keep 2.16e+08/1.95e+10 =  1% of the original kernel matrix.

torch.Size([82074, 2])
We keep 3.52e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([12586, 2])
We keep 2.40e+06/5.77e+07 =  4% of the original kernel matrix.

torch.Size([19621, 2])
We keep 3.27e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([64536, 2])
We keep 5.40e+07/2.08e+09 =  2% of the original kernel matrix.

torch.Size([44642, 2])
We keep 1.37e+07/9.35e+08 =  1% of the original kernel matrix.

torch.Size([23192, 2])
We keep 1.44e+07/2.66e+08 =  5% of the original kernel matrix.

torch.Size([27197, 2])
We keep 5.54e+06/3.35e+08 =  1% of the original kernel matrix.

time for making ranges is 3.8095240592956543
Sorting X and nu_X
time for sorting X is 0.08903264999389648
Sorting Z and nu_Z
time for sorting Z is 0.0002682209014892578
Starting Optim
sum tnu_Z before tensor(31980836., device='cuda:0')
c= tensor(2264.3940, device='cuda:0')
c= tensor(125199.4688, device='cuda:0')
c= tensor(128085.1250, device='cuda:0')
c= tensor(243265.0312, device='cuda:0')
c= tensor(521018.9062, device='cuda:0')
c= tensor(1032163.2500, device='cuda:0')
c= tensor(1387147.2500, device='cuda:0')
c= tensor(1708529., device='cuda:0')
c= tensor(1778446.8750, device='cuda:0')
c= tensor(9618371., device='cuda:0')
c= tensor(9650902., device='cuda:0')
c= tensor(19163618., device='cuda:0')
c= tensor(19180342., device='cuda:0')
c= tensor(51839752., device='cuda:0')
c= tensor(52060316., device='cuda:0')
c= tensor(52696448., device='cuda:0')
c= tensor(53598436., device='cuda:0')
c= tensor(53760944., device='cuda:0')
c= tensor(60781892., device='cuda:0')
c= tensor(63049140., device='cuda:0')
c= tensor(64668592., device='cuda:0')
c= tensor(81720928., device='cuda:0')
c= tensor(81747784., device='cuda:0')
c= tensor(82746344., device='cuda:0')
c= tensor(82844584., device='cuda:0')
c= tensor(84226272., device='cuda:0')
c= tensor(85899896., device='cuda:0')
c= tensor(85948904., device='cuda:0')
c= tensor(88100824., device='cuda:0')
c= tensor(3.5968e+08, device='cuda:0')
c= tensor(3.5980e+08, device='cuda:0')
c= tensor(4.6267e+08, device='cuda:0')
c= tensor(4.6314e+08, device='cuda:0')
c= tensor(4.6319e+08, device='cuda:0')
c= tensor(4.6353e+08, device='cuda:0')
c= tensor(4.7151e+08, device='cuda:0')
c= tensor(4.7351e+08, device='cuda:0')
c= tensor(4.7351e+08, device='cuda:0')
c= tensor(4.7351e+08, device='cuda:0')
c= tensor(4.7352e+08, device='cuda:0')
c= tensor(4.7352e+08, device='cuda:0')
c= tensor(4.7353e+08, device='cuda:0')
c= tensor(4.7353e+08, device='cuda:0')
c= tensor(4.7354e+08, device='cuda:0')
c= tensor(4.7354e+08, device='cuda:0')
c= tensor(4.7354e+08, device='cuda:0')
c= tensor(4.7355e+08, device='cuda:0')
c= tensor(4.7355e+08, device='cuda:0')
c= tensor(4.7356e+08, device='cuda:0')
c= tensor(4.7368e+08, device='cuda:0')
c= tensor(4.7372e+08, device='cuda:0')
c= tensor(4.7372e+08, device='cuda:0')
c= tensor(4.7390e+08, device='cuda:0')
c= tensor(4.7391e+08, device='cuda:0')
c= tensor(4.7392e+08, device='cuda:0')
c= tensor(4.7394e+08, device='cuda:0')
c= tensor(4.7394e+08, device='cuda:0')
c= tensor(4.7395e+08, device='cuda:0')
c= tensor(4.7396e+08, device='cuda:0')
c= tensor(4.7396e+08, device='cuda:0')
c= tensor(4.7397e+08, device='cuda:0')
c= tensor(4.7397e+08, device='cuda:0')
c= tensor(4.7400e+08, device='cuda:0')
c= tensor(4.7402e+08, device='cuda:0')
c= tensor(4.7403e+08, device='cuda:0')
c= tensor(4.7403e+08, device='cuda:0')
c= tensor(4.7405e+08, device='cuda:0')
c= tensor(4.7406e+08, device='cuda:0')
c= tensor(4.7409e+08, device='cuda:0')
c= tensor(4.7410e+08, device='cuda:0')
c= tensor(4.7412e+08, device='cuda:0')
c= tensor(4.7412e+08, device='cuda:0')
c= tensor(4.7413e+08, device='cuda:0')
c= tensor(4.7413e+08, device='cuda:0')
c= tensor(4.7414e+08, device='cuda:0')
c= tensor(4.7416e+08, device='cuda:0')
c= tensor(4.7416e+08, device='cuda:0')
c= tensor(4.7417e+08, device='cuda:0')
c= tensor(4.7418e+08, device='cuda:0')
c= tensor(4.7426e+08, device='cuda:0')
c= tensor(4.7426e+08, device='cuda:0')
c= tensor(4.7426e+08, device='cuda:0')
c= tensor(4.7427e+08, device='cuda:0')
c= tensor(4.7427e+08, device='cuda:0')
c= tensor(4.7428e+08, device='cuda:0')
c= tensor(4.7428e+08, device='cuda:0')
c= tensor(4.7429e+08, device='cuda:0')
c= tensor(4.7429e+08, device='cuda:0')
c= tensor(4.7429e+08, device='cuda:0')
c= tensor(4.7430e+08, device='cuda:0')
c= tensor(4.7433e+08, device='cuda:0')
c= tensor(4.7434e+08, device='cuda:0')
c= tensor(4.7434e+08, device='cuda:0')
c= tensor(4.7435e+08, device='cuda:0')
c= tensor(4.7435e+08, device='cuda:0')
c= tensor(4.7436e+08, device='cuda:0')
c= tensor(4.7436e+08, device='cuda:0')
c= tensor(4.7438e+08, device='cuda:0')
c= tensor(4.7438e+08, device='cuda:0')
c= tensor(4.7439e+08, device='cuda:0')
c= tensor(4.7447e+08, device='cuda:0')
c= tensor(4.7449e+08, device='cuda:0')
c= tensor(4.7451e+08, device='cuda:0')
c= tensor(4.7451e+08, device='cuda:0')
c= tensor(4.7452e+08, device='cuda:0')
c= tensor(4.7453e+08, device='cuda:0')
c= tensor(4.7455e+08, device='cuda:0')
c= tensor(4.7455e+08, device='cuda:0')
c= tensor(4.7456e+08, device='cuda:0')
c= tensor(4.7456e+08, device='cuda:0')
c= tensor(4.7457e+08, device='cuda:0')
c= tensor(4.7457e+08, device='cuda:0')
c= tensor(4.7457e+08, device='cuda:0')
c= tensor(4.7458e+08, device='cuda:0')
c= tensor(4.7459e+08, device='cuda:0')
c= tensor(4.7461e+08, device='cuda:0')
c= tensor(4.7462e+08, device='cuda:0')
c= tensor(4.7462e+08, device='cuda:0')
c= tensor(4.7463e+08, device='cuda:0')
c= tensor(4.7464e+08, device='cuda:0')
c= tensor(4.7471e+08, device='cuda:0')
c= tensor(4.7472e+08, device='cuda:0')
c= tensor(4.7472e+08, device='cuda:0')
c= tensor(4.7472e+08, device='cuda:0')
c= tensor(4.7473e+08, device='cuda:0')
c= tensor(4.7473e+08, device='cuda:0')
c= tensor(4.7473e+08, device='cuda:0')
c= tensor(4.7475e+08, device='cuda:0')
c= tensor(4.7476e+08, device='cuda:0')
c= tensor(4.7477e+08, device='cuda:0')
c= tensor(4.7479e+08, device='cuda:0')
c= tensor(4.7480e+08, device='cuda:0')
c= tensor(4.7480e+08, device='cuda:0')
c= tensor(4.7480e+08, device='cuda:0')
c= tensor(4.7482e+08, device='cuda:0')
c= tensor(4.7482e+08, device='cuda:0')
c= tensor(4.7483e+08, device='cuda:0')
c= tensor(4.7483e+08, device='cuda:0')
c= tensor(4.7483e+08, device='cuda:0')
c= tensor(4.7484e+08, device='cuda:0')
c= tensor(4.7484e+08, device='cuda:0')
c= tensor(4.7484e+08, device='cuda:0')
c= tensor(4.7489e+08, device='cuda:0')
c= tensor(4.7491e+08, device='cuda:0')
c= tensor(4.7491e+08, device='cuda:0')
c= tensor(4.7492e+08, device='cuda:0')
c= tensor(4.7492e+08, device='cuda:0')
c= tensor(4.7492e+08, device='cuda:0')
c= tensor(4.7493e+08, device='cuda:0')
c= tensor(4.7493e+08, device='cuda:0')
c= tensor(4.7494e+08, device='cuda:0')
c= tensor(4.7494e+08, device='cuda:0')
c= tensor(4.7494e+08, device='cuda:0')
c= tensor(4.7499e+08, device='cuda:0')
c= tensor(4.7499e+08, device='cuda:0')
c= tensor(4.7507e+08, device='cuda:0')
c= tensor(4.7507e+08, device='cuda:0')
c= tensor(4.7508e+08, device='cuda:0')
c= tensor(4.7509e+08, device='cuda:0')
c= tensor(4.7509e+08, device='cuda:0')
c= tensor(4.7514e+08, device='cuda:0')
c= tensor(4.7515e+08, device='cuda:0')
c= tensor(4.7515e+08, device='cuda:0')
c= tensor(4.7515e+08, device='cuda:0')
c= tensor(4.7518e+08, device='cuda:0')
c= tensor(4.7518e+08, device='cuda:0')
c= tensor(4.7519e+08, device='cuda:0')
c= tensor(4.7520e+08, device='cuda:0')
c= tensor(4.7520e+08, device='cuda:0')
c= tensor(4.7520e+08, device='cuda:0')
c= tensor(4.7520e+08, device='cuda:0')
c= tensor(4.7521e+08, device='cuda:0')
c= tensor(4.7523e+08, device='cuda:0')
c= tensor(4.7523e+08, device='cuda:0')
c= tensor(4.7524e+08, device='cuda:0')
c= tensor(4.7564e+08, device='cuda:0')
c= tensor(4.7565e+08, device='cuda:0')
c= tensor(4.7566e+08, device='cuda:0')
c= tensor(4.7567e+08, device='cuda:0')
c= tensor(4.7568e+08, device='cuda:0')
c= tensor(4.7568e+08, device='cuda:0')
c= tensor(4.7569e+08, device='cuda:0')
c= tensor(4.7570e+08, device='cuda:0')
c= tensor(4.7570e+08, device='cuda:0')
c= tensor(4.7571e+08, device='cuda:0')
c= tensor(4.7572e+08, device='cuda:0')
c= tensor(4.7573e+08, device='cuda:0')
c= tensor(4.7573e+08, device='cuda:0')
c= tensor(4.7575e+08, device='cuda:0')
c= tensor(4.7577e+08, device='cuda:0')
c= tensor(4.7577e+08, device='cuda:0')
c= tensor(4.7577e+08, device='cuda:0')
c= tensor(4.7578e+08, device='cuda:0')
c= tensor(4.7583e+08, device='cuda:0')
c= tensor(4.7585e+08, device='cuda:0')
c= tensor(4.7586e+08, device='cuda:0')
c= tensor(4.7586e+08, device='cuda:0')
c= tensor(4.7586e+08, device='cuda:0')
c= tensor(4.7587e+08, device='cuda:0')
c= tensor(4.7587e+08, device='cuda:0')
c= tensor(4.7588e+08, device='cuda:0')
c= tensor(4.7588e+08, device='cuda:0')
c= tensor(4.7589e+08, device='cuda:0')
c= tensor(4.7589e+08, device='cuda:0')
c= tensor(4.7592e+08, device='cuda:0')
c= tensor(4.7593e+08, device='cuda:0')
c= tensor(4.7593e+08, device='cuda:0')
c= tensor(4.7594e+08, device='cuda:0')
c= tensor(4.7595e+08, device='cuda:0')
c= tensor(4.7596e+08, device='cuda:0')
c= tensor(4.7597e+08, device='cuda:0')
c= tensor(4.7601e+08, device='cuda:0')
c= tensor(4.7602e+08, device='cuda:0')
c= tensor(4.7603e+08, device='cuda:0')
c= tensor(4.7603e+08, device='cuda:0')
c= tensor(4.7603e+08, device='cuda:0')
c= tensor(4.7603e+08, device='cuda:0')
c= tensor(4.7603e+08, device='cuda:0')
c= tensor(4.7605e+08, device='cuda:0')
c= tensor(4.7605e+08, device='cuda:0')
c= tensor(4.7606e+08, device='cuda:0')
c= tensor(4.7606e+08, device='cuda:0')
c= tensor(4.7608e+08, device='cuda:0')
c= tensor(4.7608e+08, device='cuda:0')
c= tensor(4.7609e+08, device='cuda:0')
c= tensor(4.7609e+08, device='cuda:0')
c= tensor(4.7610e+08, device='cuda:0')
c= tensor(4.7610e+08, device='cuda:0')
c= tensor(4.7610e+08, device='cuda:0')
c= tensor(4.7611e+08, device='cuda:0')
c= tensor(4.7612e+08, device='cuda:0')
c= tensor(4.7612e+08, device='cuda:0')
c= tensor(4.7613e+08, device='cuda:0')
c= tensor(4.7614e+08, device='cuda:0')
c= tensor(4.7614e+08, device='cuda:0')
c= tensor(4.7615e+08, device='cuda:0')
c= tensor(4.7617e+08, device='cuda:0')
c= tensor(4.7617e+08, device='cuda:0')
c= tensor(4.7618e+08, device='cuda:0')
c= tensor(4.7639e+08, device='cuda:0')
c= tensor(4.7762e+08, device='cuda:0')
c= tensor(4.7766e+08, device='cuda:0')
c= tensor(4.7768e+08, device='cuda:0')
c= tensor(4.7769e+08, device='cuda:0')
c= tensor(4.7771e+08, device='cuda:0')
c= tensor(4.7814e+08, device='cuda:0')
c= tensor(4.8660e+08, device='cuda:0')
c= tensor(4.8660e+08, device='cuda:0')
c= tensor(4.9319e+08, device='cuda:0')
c= tensor(4.9465e+08, device='cuda:0')
c= tensor(4.9534e+08, device='cuda:0')
c= tensor(5.0506e+08, device='cuda:0')
c= tensor(5.0507e+08, device='cuda:0')
c= tensor(5.0509e+08, device='cuda:0')
c= tensor(5.1298e+08, device='cuda:0')
c= tensor(6.4717e+08, device='cuda:0')
c= tensor(6.4718e+08, device='cuda:0')
c= tensor(6.4761e+08, device='cuda:0')
c= tensor(6.4812e+08, device='cuda:0')
c= tensor(6.4947e+08, device='cuda:0')
c= tensor(6.5679e+08, device='cuda:0')
c= tensor(6.5953e+08, device='cuda:0')
c= tensor(6.6055e+08, device='cuda:0')
c= tensor(6.6076e+08, device='cuda:0')
c= tensor(6.6076e+08, device='cuda:0')
c= tensor(6.7003e+08, device='cuda:0')
c= tensor(6.7033e+08, device='cuda:0')
c= tensor(6.7033e+08, device='cuda:0')
c= tensor(6.7054e+08, device='cuda:0')
c= tensor(6.7246e+08, device='cuda:0')
c= tensor(6.7793e+08, device='cuda:0')
c= tensor(6.7980e+08, device='cuda:0')
c= tensor(6.7981e+08, device='cuda:0')
c= tensor(6.8022e+08, device='cuda:0')
c= tensor(6.8024e+08, device='cuda:0')
c= tensor(6.8150e+08, device='cuda:0')
c= tensor(6.8619e+08, device='cuda:0')
c= tensor(6.8635e+08, device='cuda:0')
c= tensor(6.8950e+08, device='cuda:0')
c= tensor(6.8950e+08, device='cuda:0')
c= tensor(6.8952e+08, device='cuda:0')
c= tensor(6.9138e+08, device='cuda:0')
c= tensor(6.9233e+08, device='cuda:0')
c= tensor(6.9449e+08, device='cuda:0')
c= tensor(6.9450e+08, device='cuda:0')
c= tensor(7.1938e+08, device='cuda:0')
c= tensor(7.1950e+08, device='cuda:0')
c= tensor(7.1985e+08, device='cuda:0')
c= tensor(7.2140e+08, device='cuda:0')
c= tensor(7.2141e+08, device='cuda:0')
c= tensor(7.2539e+08, device='cuda:0')
c= tensor(7.4131e+08, device='cuda:0')
c= tensor(7.7465e+08, device='cuda:0')
c= tensor(7.7589e+08, device='cuda:0')
c= tensor(7.7630e+08, device='cuda:0')
c= tensor(7.7634e+08, device='cuda:0')
c= tensor(7.7636e+08, device='cuda:0')
c= tensor(7.8304e+08, device='cuda:0')
c= tensor(7.8310e+08, device='cuda:0')
c= tensor(7.8370e+08, device='cuda:0')
c= tensor(7.9036e+08, device='cuda:0')
c= tensor(7.9086e+08, device='cuda:0')
c= tensor(7.9109e+08, device='cuda:0')
c= tensor(7.9110e+08, device='cuda:0')
c= tensor(7.9619e+08, device='cuda:0')
c= tensor(8.3392e+08, device='cuda:0')
c= tensor(8.3551e+08, device='cuda:0')
c= tensor(8.3553e+08, device='cuda:0')
c= tensor(8.3745e+08, device='cuda:0')
c= tensor(8.3755e+08, device='cuda:0')
c= tensor(8.5139e+08, device='cuda:0')
c= tensor(8.5153e+08, device='cuda:0')
c= tensor(8.5452e+08, device='cuda:0')
c= tensor(8.5468e+08, device='cuda:0')
c= tensor(8.5878e+08, device='cuda:0')
c= tensor(8.5979e+08, device='cuda:0')
c= tensor(8.6014e+08, device='cuda:0')
c= tensor(8.6438e+08, device='cuda:0')
c= tensor(8.7670e+08, device='cuda:0')
c= tensor(8.7681e+08, device='cuda:0')
c= tensor(8.8847e+08, device='cuda:0')
c= tensor(8.9496e+08, device='cuda:0')
c= tensor(9.1748e+08, device='cuda:0')
c= tensor(9.1781e+08, device='cuda:0')
c= tensor(9.1782e+08, device='cuda:0')
c= tensor(9.1789e+08, device='cuda:0')
c= tensor(9.2005e+08, device='cuda:0')
c= tensor(9.2042e+08, device='cuda:0')
c= tensor(9.2271e+08, device='cuda:0')
c= tensor(9.2271e+08, device='cuda:0')
c= tensor(9.2413e+08, device='cuda:0')
c= tensor(9.2840e+08, device='cuda:0')
c= tensor(9.2900e+08, device='cuda:0')
c= tensor(9.2903e+08, device='cuda:0')
c= tensor(9.3096e+08, device='cuda:0')
c= tensor(9.3113e+08, device='cuda:0')
c= tensor(9.3116e+08, device='cuda:0')
c= tensor(9.3124e+08, device='cuda:0')
c= tensor(9.3126e+08, device='cuda:0')
c= tensor(9.3278e+08, device='cuda:0')
c= tensor(9.3315e+08, device='cuda:0')
c= tensor(9.3334e+08, device='cuda:0')
c= tensor(9.3396e+08, device='cuda:0')
c= tensor(9.3398e+08, device='cuda:0')
c= tensor(1.5268e+09, device='cuda:0')
c= tensor(1.5268e+09, device='cuda:0')
c= tensor(1.5312e+09, device='cuda:0')
c= tensor(1.5312e+09, device='cuda:0')
c= tensor(1.5312e+09, device='cuda:0')
c= tensor(1.5313e+09, device='cuda:0')
c= tensor(1.5314e+09, device='cuda:0')
c= tensor(1.5314e+09, device='cuda:0')
c= tensor(1.5346e+09, device='cuda:0')
c= tensor(1.5346e+09, device='cuda:0')
c= tensor(1.5346e+09, device='cuda:0')
c= tensor(1.5469e+09, device='cuda:0')
c= tensor(1.5480e+09, device='cuda:0')
c= tensor(1.5484e+09, device='cuda:0')
c= tensor(1.5518e+09, device='cuda:0')
c= tensor(1.5644e+09, device='cuda:0')
c= tensor(1.5644e+09, device='cuda:0')
c= tensor(1.5644e+09, device='cuda:0')
c= tensor(1.5647e+09, device='cuda:0')
c= tensor(1.5647e+09, device='cuda:0')
c= tensor(1.5647e+09, device='cuda:0')
c= tensor(1.5648e+09, device='cuda:0')
c= tensor(1.5648e+09, device='cuda:0')
c= tensor(1.5649e+09, device='cuda:0')
c= tensor(1.5649e+09, device='cuda:0')
c= tensor(1.5649e+09, device='cuda:0')
c= tensor(1.5796e+09, device='cuda:0')
c= tensor(1.5797e+09, device='cuda:0')
c= tensor(1.5813e+09, device='cuda:0')
c= tensor(1.5813e+09, device='cuda:0')
c= tensor(1.5813e+09, device='cuda:0')
c= tensor(1.5844e+09, device='cuda:0')
c= tensor(1.7331e+09, device='cuda:0')
c= tensor(1.7780e+09, device='cuda:0')
c= tensor(1.7781e+09, device='cuda:0')
c= tensor(1.7790e+09, device='cuda:0')
c= tensor(1.7790e+09, device='cuda:0')
c= tensor(1.7791e+09, device='cuda:0')
c= tensor(1.8476e+09, device='cuda:0')
c= tensor(1.8478e+09, device='cuda:0')
c= tensor(1.8478e+09, device='cuda:0')
c= tensor(1.8493e+09, device='cuda:0')
c= tensor(1.9168e+09, device='cuda:0')
c= tensor(1.9172e+09, device='cuda:0')
c= tensor(1.9173e+09, device='cuda:0')
c= tensor(1.9174e+09, device='cuda:0')
c= tensor(1.9174e+09, device='cuda:0')
c= tensor(1.9175e+09, device='cuda:0')
c= tensor(1.9252e+09, device='cuda:0')
c= tensor(1.9254e+09, device='cuda:0')
c= tensor(1.9254e+09, device='cuda:0')
c= tensor(1.9274e+09, device='cuda:0')
c= tensor(1.9275e+09, device='cuda:0')
c= tensor(1.9275e+09, device='cuda:0')
c= tensor(1.9280e+09, device='cuda:0')
c= tensor(1.9308e+09, device='cuda:0')
c= tensor(1.9453e+09, device='cuda:0')
c= tensor(1.9571e+09, device='cuda:0')
c= tensor(1.9668e+09, device='cuda:0')
c= tensor(1.9670e+09, device='cuda:0')
c= tensor(1.9683e+09, device='cuda:0')
c= tensor(1.9723e+09, device='cuda:0')
c= tensor(1.9826e+09, device='cuda:0')
c= tensor(1.9827e+09, device='cuda:0')
c= tensor(2.2261e+09, device='cuda:0')
c= tensor(2.7950e+09, device='cuda:0')
c= tensor(2.8096e+09, device='cuda:0')
c= tensor(2.8145e+09, device='cuda:0')
c= tensor(2.8166e+09, device='cuda:0')
c= tensor(2.8166e+09, device='cuda:0')
c= tensor(2.8167e+09, device='cuda:0')
c= tensor(2.8167e+09, device='cuda:0')
c= tensor(2.8199e+09, device='cuda:0')
c= tensor(2.8240e+09, device='cuda:0')
c= tensor(2.8477e+09, device='cuda:0')
c= tensor(2.8508e+09, device='cuda:0')
c= tensor(2.8522e+09, device='cuda:0')
c= tensor(2.8524e+09, device='cuda:0')
c= tensor(2.8566e+09, device='cuda:0')
c= tensor(2.8566e+09, device='cuda:0')
c= tensor(2.8567e+09, device='cuda:0')
c= tensor(2.8600e+09, device='cuda:0')
c= tensor(2.8609e+09, device='cuda:0')
c= tensor(2.8609e+09, device='cuda:0')
c= tensor(2.8610e+09, device='cuda:0')
c= tensor(2.9820e+09, device='cuda:0')
c= tensor(2.9821e+09, device='cuda:0')
c= tensor(2.9856e+09, device='cuda:0')
c= tensor(2.9856e+09, device='cuda:0')
c= tensor(2.9857e+09, device='cuda:0')
c= tensor(2.9857e+09, device='cuda:0')
c= tensor(2.9857e+09, device='cuda:0')
c= tensor(2.9858e+09, device='cuda:0')
c= tensor(2.9865e+09, device='cuda:0')
c= tensor(2.9867e+09, device='cuda:0')
c= tensor(2.9970e+09, device='cuda:0')
c= tensor(2.9971e+09, device='cuda:0')
c= tensor(3.0010e+09, device='cuda:0')
c= tensor(3.0011e+09, device='cuda:0')
c= tensor(3.0037e+09, device='cuda:0')
c= tensor(3.0037e+09, device='cuda:0')
c= tensor(3.0038e+09, device='cuda:0')
c= tensor(3.0041e+09, device='cuda:0')
c= tensor(3.0044e+09, device='cuda:0')
c= tensor(3.0057e+09, device='cuda:0')
c= tensor(3.2190e+09, device='cuda:0')
c= tensor(3.2191e+09, device='cuda:0')
c= tensor(3.2191e+09, device='cuda:0')
c= tensor(3.2195e+09, device='cuda:0')
c= tensor(3.2197e+09, device='cuda:0')
c= tensor(3.2561e+09, device='cuda:0')
c= tensor(3.2561e+09, device='cuda:0')
c= tensor(3.2586e+09, device='cuda:0')
c= tensor(3.2675e+09, device='cuda:0')
c= tensor(3.2675e+09, device='cuda:0')
c= tensor(3.3089e+09, device='cuda:0')
c= tensor(3.3094e+09, device='cuda:0')
c= tensor(3.3617e+09, device='cuda:0')
c= tensor(3.3617e+09, device='cuda:0')
c= tensor(3.3622e+09, device='cuda:0')
c= tensor(3.3623e+09, device='cuda:0')
c= tensor(3.3623e+09, device='cuda:0')
c= tensor(3.3623e+09, device='cuda:0')
c= tensor(3.3633e+09, device='cuda:0')
c= tensor(3.3633e+09, device='cuda:0')
c= tensor(3.3672e+09, device='cuda:0')
c= tensor(3.3672e+09, device='cuda:0')
c= tensor(3.3673e+09, device='cuda:0')
c= tensor(3.3674e+09, device='cuda:0')
c= tensor(3.3754e+09, device='cuda:0')
c= tensor(3.3815e+09, device='cuda:0')
c= tensor(3.3964e+09, device='cuda:0')
c= tensor(3.3966e+09, device='cuda:0')
c= tensor(3.3966e+09, device='cuda:0')
c= tensor(3.3966e+09, device='cuda:0')
c= tensor(3.3967e+09, device='cuda:0')
c= tensor(3.4413e+09, device='cuda:0')
c= tensor(3.4413e+09, device='cuda:0')
c= tensor(3.4415e+09, device='cuda:0')
c= tensor(3.4427e+09, device='cuda:0')
c= tensor(3.4433e+09, device='cuda:0')
c= tensor(3.4438e+09, device='cuda:0')
c= tensor(3.4438e+09, device='cuda:0')
c= tensor(3.4539e+09, device='cuda:0')
c= tensor(3.4548e+09, device='cuda:0')
c= tensor(3.4548e+09, device='cuda:0')
c= tensor(3.4552e+09, device='cuda:0')
c= tensor(3.4590e+09, device='cuda:0')
c= tensor(3.4618e+09, device='cuda:0')
c= tensor(3.5074e+09, device='cuda:0')
c= tensor(3.5120e+09, device='cuda:0')
c= tensor(3.5120e+09, device='cuda:0')
c= tensor(3.5125e+09, device='cuda:0')
c= tensor(3.5138e+09, device='cuda:0')
c= tensor(3.5138e+09, device='cuda:0')
c= tensor(3.5139e+09, device='cuda:0')
c= tensor(3.5139e+09, device='cuda:0')
c= tensor(3.5156e+09, device='cuda:0')
c= tensor(3.5162e+09, device='cuda:0')
c= tensor(3.5162e+09, device='cuda:0')
c= tensor(3.5163e+09, device='cuda:0')
c= tensor(3.5163e+09, device='cuda:0')
c= tensor(3.5167e+09, device='cuda:0')
c= tensor(3.5167e+09, device='cuda:0')
c= tensor(3.5169e+09, device='cuda:0')
c= tensor(3.5183e+09, device='cuda:0')
c= tensor(3.5184e+09, device='cuda:0')
c= tensor(3.5184e+09, device='cuda:0')
c= tensor(3.5184e+09, device='cuda:0')
c= tensor(3.5187e+09, device='cuda:0')
c= tensor(3.5272e+09, device='cuda:0')
c= tensor(3.5272e+09, device='cuda:0')
c= tensor(3.5272e+09, device='cuda:0')
c= tensor(3.5272e+09, device='cuda:0')
c= tensor(3.5301e+09, device='cuda:0')
c= tensor(3.5741e+09, device='cuda:0')
c= tensor(3.5765e+09, device='cuda:0')
c= tensor(3.5765e+09, device='cuda:0')
c= tensor(3.5884e+09, device='cuda:0')
c= tensor(3.5926e+09, device='cuda:0')
c= tensor(3.5927e+09, device='cuda:0')
c= tensor(3.5927e+09, device='cuda:0')
c= tensor(3.5931e+09, device='cuda:0')
c= tensor(3.6009e+09, device='cuda:0')
c= tensor(3.6248e+09, device='cuda:0')
c= tensor(3.6309e+09, device='cuda:0')
c= tensor(3.6318e+09, device='cuda:0')
c= tensor(3.6319e+09, device='cuda:0')
c= tensor(3.6319e+09, device='cuda:0')
c= tensor(3.6326e+09, device='cuda:0')
c= tensor(3.6327e+09, device='cuda:0')
c= tensor(3.6330e+09, device='cuda:0')
c= tensor(3.6364e+09, device='cuda:0')
c= tensor(3.6776e+09, device='cuda:0')
c= tensor(3.6777e+09, device='cuda:0')
c= tensor(3.6777e+09, device='cuda:0')
c= tensor(3.6781e+09, device='cuda:0')
c= tensor(3.6810e+09, device='cuda:0')
c= tensor(3.6832e+09, device='cuda:0')
c= tensor(3.6837e+09, device='cuda:0')
c= tensor(3.6838e+09, device='cuda:0')
c= tensor(3.6843e+09, device='cuda:0')
c= tensor(3.6843e+09, device='cuda:0')
c= tensor(3.6852e+09, device='cuda:0')
c= tensor(3.6852e+09, device='cuda:0')
c= tensor(3.6853e+09, device='cuda:0')
c= tensor(3.6853e+09, device='cuda:0')
c= tensor(3.6853e+09, device='cuda:0')
c= tensor(3.6854e+09, device='cuda:0')
c= tensor(3.6903e+09, device='cuda:0')
c= tensor(3.7021e+09, device='cuda:0')
c= tensor(3.7050e+09, device='cuda:0')
c= tensor(3.7144e+09, device='cuda:0')
c= tensor(3.7146e+09, device='cuda:0')
c= tensor(3.7147e+09, device='cuda:0')
c= tensor(3.7148e+09, device='cuda:0')
c= tensor(3.7175e+09, device='cuda:0')
c= tensor(3.7181e+09, device='cuda:0')
c= tensor(3.7186e+09, device='cuda:0')
c= tensor(3.7186e+09, device='cuda:0')
c= tensor(3.9095e+09, device='cuda:0')
c= tensor(3.9097e+09, device='cuda:0')
c= tensor(3.9107e+09, device='cuda:0')
c= tensor(3.9187e+09, device='cuda:0')
c= tensor(3.9198e+09, device='cuda:0')
c= tensor(3.9201e+09, device='cuda:0')
c= tensor(4.0522e+09, device='cuda:0')
c= tensor(4.0597e+09, device='cuda:0')
c= tensor(4.0615e+09, device='cuda:0')
c= tensor(4.0618e+09, device='cuda:0')
c= tensor(4.0623e+09, device='cuda:0')
c= tensor(4.0624e+09, device='cuda:0')
c= tensor(4.0768e+09, device='cuda:0')
c= tensor(4.2464e+09, device='cuda:0')
c= tensor(4.2509e+09, device='cuda:0')
c= tensor(4.2563e+09, device='cuda:0')
c= tensor(4.2620e+09, device='cuda:0')
c= tensor(4.2623e+09, device='cuda:0')
c= tensor(4.2630e+09, device='cuda:0')
c= tensor(4.2631e+09, device='cuda:0')
c= tensor(4.2668e+09, device='cuda:0')
c= tensor(4.2869e+09, device='cuda:0')
c= tensor(4.2892e+09, device='cuda:0')
c= tensor(4.4065e+09, device='cuda:0')
c= tensor(4.4138e+09, device='cuda:0')
c= tensor(4.4144e+09, device='cuda:0')
c= tensor(4.4145e+09, device='cuda:0')
c= tensor(4.4159e+09, device='cuda:0')
c= tensor(4.4199e+09, device='cuda:0')
c= tensor(4.4200e+09, device='cuda:0')
c= tensor(4.4567e+09, device='cuda:0')
c= tensor(4.4576e+09, device='cuda:0')
c= tensor(4.4580e+09, device='cuda:0')
c= tensor(4.4582e+09, device='cuda:0')
c= tensor(4.4582e+09, device='cuda:0')
c= tensor(4.4582e+09, device='cuda:0')
c= tensor(4.4582e+09, device='cuda:0')
c= tensor(4.4584e+09, device='cuda:0')
c= tensor(4.4594e+09, device='cuda:0')
c= tensor(5.9340e+09, device='cuda:0')
c= tensor(5.9343e+09, device='cuda:0')
c= tensor(5.9400e+09, device='cuda:0')
c= tensor(5.9400e+09, device='cuda:0')
c= tensor(5.9402e+09, device='cuda:0')
c= tensor(5.9403e+09, device='cuda:0')
c= tensor(5.9502e+09, device='cuda:0')
c= tensor(5.9513e+09, device='cuda:0')
c= tensor(6.0313e+09, device='cuda:0')
c= tensor(6.0313e+09, device='cuda:0')
c= tensor(6.0370e+09, device='cuda:0')
c= tensor(6.0372e+09, device='cuda:0')
c= tensor(6.0419e+09, device='cuda:0')
c= tensor(6.0702e+09, device='cuda:0')
c= tensor(6.0705e+09, device='cuda:0')
c= tensor(6.0705e+09, device='cuda:0')
c= tensor(6.0722e+09, device='cuda:0')
c= tensor(6.0724e+09, device='cuda:0')
c= tensor(6.0728e+09, device='cuda:0')
c= tensor(6.0808e+09, device='cuda:0')
c= tensor(6.0810e+09, device='cuda:0')
c= tensor(6.0826e+09, device='cuda:0')
c= tensor(6.0831e+09, device='cuda:0')
c= tensor(6.0864e+09, device='cuda:0')
c= tensor(6.0963e+09, device='cuda:0')
c= tensor(6.0971e+09, device='cuda:0')
c= tensor(6.0973e+09, device='cuda:0')
c= tensor(6.1023e+09, device='cuda:0')
c= tensor(6.1045e+09, device='cuda:0')
c= tensor(6.1192e+09, device='cuda:0')
c= tensor(6.1199e+09, device='cuda:0')
c= tensor(6.1207e+09, device='cuda:0')
c= tensor(6.1211e+09, device='cuda:0')
c= tensor(6.1425e+09, device='cuda:0')
c= tensor(6.1467e+09, device='cuda:0')
c= tensor(6.1468e+09, device='cuda:0')
c= tensor(6.1468e+09, device='cuda:0')
c= tensor(6.1469e+09, device='cuda:0')
c= tensor(6.1484e+09, device='cuda:0')
c= tensor(6.1528e+09, device='cuda:0')
c= tensor(6.1542e+09, device='cuda:0')
c= tensor(6.1542e+09, device='cuda:0')
c= tensor(6.1545e+09, device='cuda:0')
c= tensor(6.1548e+09, device='cuda:0')
c= tensor(6.1560e+09, device='cuda:0')
c= tensor(6.1563e+09, device='cuda:0')
c= tensor(6.1573e+09, device='cuda:0')
c= tensor(6.1578e+09, device='cuda:0')
c= tensor(6.1584e+09, device='cuda:0')
c= tensor(6.1584e+09, device='cuda:0')
c= tensor(6.1584e+09, device='cuda:0')
c= tensor(6.1794e+09, device='cuda:0')
c= tensor(6.1796e+09, device='cuda:0')
c= tensor(6.1848e+09, device='cuda:0')
c= tensor(6.1848e+09, device='cuda:0')
c= tensor(6.1849e+09, device='cuda:0')
c= tensor(6.1856e+09, device='cuda:0')
c= tensor(6.1865e+09, device='cuda:0')
c= tensor(6.1869e+09, device='cuda:0')
c= tensor(6.1869e+09, device='cuda:0')
c= tensor(6.1869e+09, device='cuda:0')
c= tensor(6.1876e+09, device='cuda:0')
c= tensor(6.1878e+09, device='cuda:0')
c= tensor(6.1920e+09, device='cuda:0')
c= tensor(6.1920e+09, device='cuda:0')
c= tensor(6.1923e+09, device='cuda:0')
c= tensor(6.1936e+09, device='cuda:0')
c= tensor(6.1937e+09, device='cuda:0')
c= tensor(6.2020e+09, device='cuda:0')
c= tensor(6.2028e+09, device='cuda:0')
c= tensor(6.2046e+09, device='cuda:0')
c= tensor(6.2074e+09, device='cuda:0')
c= tensor(6.2106e+09, device='cuda:0')
c= tensor(6.2141e+09, device='cuda:0')
c= tensor(6.2168e+09, device='cuda:0')
c= tensor(6.2168e+09, device='cuda:0')
c= tensor(6.2175e+09, device='cuda:0')
c= tensor(6.2178e+09, device='cuda:0')
c= tensor(6.2188e+09, device='cuda:0')
c= tensor(6.2250e+09, device='cuda:0')
c= tensor(6.2255e+09, device='cuda:0')
c= tensor(6.2267e+09, device='cuda:0')
c= tensor(6.2269e+09, device='cuda:0')
c= tensor(6.2301e+09, device='cuda:0')
c= tensor(6.2301e+09, device='cuda:0')
c= tensor(6.2302e+09, device='cuda:0')
c= tensor(6.2399e+09, device='cuda:0')
c= tensor(6.2642e+09, device='cuda:0')
c= tensor(6.2643e+09, device='cuda:0')
c= tensor(6.2644e+09, device='cuda:0')
c= tensor(6.2646e+09, device='cuda:0')
c= tensor(6.2672e+09, device='cuda:0')
c= tensor(6.2679e+09, device='cuda:0')
c= tensor(6.2679e+09, device='cuda:0')
c= tensor(6.2684e+09, device='cuda:0')
c= tensor(6.2737e+09, device='cuda:0')
c= tensor(6.2737e+09, device='cuda:0')
c= tensor(6.2747e+09, device='cuda:0')
c= tensor(6.2750e+09, device='cuda:0')
memory (bytes)
4876353536
time for making loss 2 is 14.726258277893066
p0 True
it  0 : 2045434880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 58% |
shape of L is 
torch.Size([])
memory (bytes)
4876623872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4877119488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  55637130000.0
relative error loss 8.866415
shape of L is 
torch.Size([])
memory (bytes)
5055909888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 10% |
memory (bytes)
5055909888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 10% |
error is  55636877000.0
relative error loss 8.866375
shape of L is 
torch.Size([])
memory (bytes)
5057933312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5057933312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  55636290000.0
relative error loss 8.8662815
shape of L is 
torch.Size([])
memory (bytes)
5058969600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
5058969600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  55632520000.0
relative error loss 8.865681
shape of L is 
torch.Size([])
memory (bytes)
5060956160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5060956160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  55611700000.0
relative error loss 8.862363
shape of L is 
torch.Size([])
memory (bytes)
5063081984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
5063081984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  55383224000.0
relative error loss 8.825953
shape of L is 
torch.Size([])
memory (bytes)
5065072640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
5065072640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  54252073000.0
relative error loss 8.64569
shape of L is 
torch.Size([])
memory (bytes)
5067227136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5067321344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  48385090000.0
relative error loss 7.710719
shape of L is 
torch.Size([])
memory (bytes)
5069422592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5069422592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  14956062000.0
relative error loss 2.38342
shape of L is 
torch.Size([])
memory (bytes)
5071613952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5071618048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  12332440000.0
relative error loss 1.9653158
shape of L is 
torch.Size([])
memory (bytes)
5073674240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
5073674240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  10781070000.0
relative error loss 1.7180873
time to take a step is 255.5803279876709
it  1 : 2448939008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5075853312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
5075853312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  10781070000.0
relative error loss 1.7180873
shape of L is 
torch.Size([])
memory (bytes)
5078003712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5078003712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  7513743400.0
relative error loss 1.1974013
shape of L is 
torch.Size([])
memory (bytes)
5080064000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
5080064000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  6543525000.0
relative error loss 1.0427858
shape of L is 
torch.Size([])
memory (bytes)
5082320896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5082337280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  6155001000.0
relative error loss 0.98087
shape of L is 
torch.Size([])
memory (bytes)
5084336128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5084336128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  5963785000.0
relative error loss 0.9503976
shape of L is 
torch.Size([])
memory (bytes)
5086547968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5086547968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  5785075000.0
relative error loss 0.9219181
shape of L is 
torch.Size([])
memory (bytes)
5088653312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
5088653312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  5718798000.0
relative error loss 0.91135603
shape of L is 
torch.Size([])
memory (bytes)
5090664448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
5090664448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  5632147500.0
relative error loss 0.8975473
shape of L is 
torch.Size([])
memory (bytes)
5092896768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5092896768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  5449646000.0
relative error loss 0.86846364
shape of L is 
torch.Size([])
memory (bytes)
5094871040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
5094871040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  5310551600.0
relative error loss 0.8462973
time to take a step is 224.52405667304993
it  2 : 2564225536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5097070592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
5097070592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  5310551600.0
relative error loss 0.8462973
shape of L is 
torch.Size([])
memory (bytes)
5099233280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5099233280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  5226895000.0
relative error loss 0.8329657
shape of L is 
torch.Size([])
memory (bytes)
5101195264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5101195264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  5076319700.0
relative error loss 0.8089698
shape of L is 
torch.Size([])
memory (bytes)
5103288320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5103288320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  4913523700.0
relative error loss 0.7830264
shape of L is 
torch.Size([])
memory (bytes)
5105598464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
5105598464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  12123340000.0
relative error loss 1.9319934
shape of L is 
torch.Size([])
memory (bytes)
5107687424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
5107687424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  4642425000.0
relative error loss 0.7398237
shape of L is 
torch.Size([])
memory (bytes)
5109850112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5109850112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  4393083000.0
relative error loss 0.70008814
shape of L is 
torch.Size([])
memory (bytes)
5111889920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
5111889920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  4349398500.0
relative error loss 0.69312656
shape of L is 
torch.Size([])
memory (bytes)
5113999360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
5113999360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  4010039000.0
relative error loss 0.6390457
shape of L is 
torch.Size([])
memory (bytes)
5116272640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5116272640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  3662303500.0
relative error loss 0.5836301
time to take a step is 223.3331503868103
it  3 : 2564225536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5118173184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5118173184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3662303500.0
relative error loss 0.5836301
shape of L is 
torch.Size([])
memory (bytes)
5120405504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5120405504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  3437465300.0
relative error loss 0.5477995
shape of L is 
torch.Size([])
memory (bytes)
5122703360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5122703360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3146785000.0
relative error loss 0.5014763
shape of L is 
torch.Size([])
memory (bytes)
5124837376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5124837376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3519812000.0
relative error loss 0.56092244
shape of L is 
torch.Size([])
memory (bytes)
5126762496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5126762496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2992414000.0
relative error loss 0.4768755
shape of L is 
torch.Size([])
memory (bytes)
5129097216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5129097216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2801263900.0
relative error loss 0.44641355
shape of L is 
torch.Size([])
memory (bytes)
5131255808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5131255808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2651533800.0
relative error loss 0.42255235
shape of L is 
torch.Size([])
memory (bytes)
5133217792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5133217792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2444964900.0
relative error loss 0.3896332
shape of L is 
torch.Size([])
memory (bytes)
5135474688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5135474688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2305255400.0
relative error loss 0.3673689
shape of L is 
torch.Size([])
memory (bytes)
5137674240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 10% |
memory (bytes)
5137674240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2121615100.0
relative error loss 0.3381037
time to take a step is 209.02755498886108
c= tensor(2264.3940, device='cuda:0')
c= tensor(125199.4688, device='cuda:0')
c= tensor(128085.1250, device='cuda:0')
c= tensor(243265.0312, device='cuda:0')
c= tensor(521018.9062, device='cuda:0')
c= tensor(1032163.2500, device='cuda:0')
c= tensor(1387147.2500, device='cuda:0')
c= tensor(1708529., device='cuda:0')
c= tensor(1778446.8750, device='cuda:0')
c= tensor(9618371., device='cuda:0')
c= tensor(9650902., device='cuda:0')
c= tensor(19163618., device='cuda:0')
c= tensor(19180342., device='cuda:0')
c= tensor(51839752., device='cuda:0')
c= tensor(52060316., device='cuda:0')
c= tensor(52696448., device='cuda:0')
c= tensor(53598436., device='cuda:0')
c= tensor(53760944., device='cuda:0')
c= tensor(60781892., device='cuda:0')
c= tensor(63049140., device='cuda:0')
c= tensor(64668592., device='cuda:0')
c= tensor(81720928., device='cuda:0')
c= tensor(81747784., device='cuda:0')
c= tensor(82746344., device='cuda:0')
c= tensor(82844584., device='cuda:0')
c= tensor(84226272., device='cuda:0')
c= tensor(85899896., device='cuda:0')
c= tensor(85948904., device='cuda:0')
c= tensor(88100824., device='cuda:0')
c= tensor(3.5968e+08, device='cuda:0')
c= tensor(3.5980e+08, device='cuda:0')
c= tensor(4.6267e+08, device='cuda:0')
c= tensor(4.6314e+08, device='cuda:0')
c= tensor(4.6319e+08, device='cuda:0')
c= tensor(4.6353e+08, device='cuda:0')
c= tensor(4.7151e+08, device='cuda:0')
c= tensor(4.7351e+08, device='cuda:0')
c= tensor(4.7351e+08, device='cuda:0')
c= tensor(4.7351e+08, device='cuda:0')
c= tensor(4.7352e+08, device='cuda:0')
c= tensor(4.7352e+08, device='cuda:0')
c= tensor(4.7353e+08, device='cuda:0')
c= tensor(4.7353e+08, device='cuda:0')
c= tensor(4.7354e+08, device='cuda:0')
c= tensor(4.7354e+08, device='cuda:0')
c= tensor(4.7354e+08, device='cuda:0')
c= tensor(4.7355e+08, device='cuda:0')
c= tensor(4.7355e+08, device='cuda:0')
c= tensor(4.7356e+08, device='cuda:0')
c= tensor(4.7368e+08, device='cuda:0')
c= tensor(4.7372e+08, device='cuda:0')
c= tensor(4.7372e+08, device='cuda:0')
c= tensor(4.7390e+08, device='cuda:0')
c= tensor(4.7391e+08, device='cuda:0')
c= tensor(4.7392e+08, device='cuda:0')
c= tensor(4.7394e+08, device='cuda:0')
c= tensor(4.7394e+08, device='cuda:0')
c= tensor(4.7395e+08, device='cuda:0')
c= tensor(4.7396e+08, device='cuda:0')
c= tensor(4.7396e+08, device='cuda:0')
c= tensor(4.7397e+08, device='cuda:0')
c= tensor(4.7397e+08, device='cuda:0')
c= tensor(4.7400e+08, device='cuda:0')
c= tensor(4.7402e+08, device='cuda:0')
c= tensor(4.7403e+08, device='cuda:0')
c= tensor(4.7403e+08, device='cuda:0')
c= tensor(4.7405e+08, device='cuda:0')
c= tensor(4.7406e+08, device='cuda:0')
c= tensor(4.7409e+08, device='cuda:0')
c= tensor(4.7410e+08, device='cuda:0')
c= tensor(4.7412e+08, device='cuda:0')
c= tensor(4.7412e+08, device='cuda:0')
c= tensor(4.7413e+08, device='cuda:0')
c= tensor(4.7413e+08, device='cuda:0')
c= tensor(4.7414e+08, device='cuda:0')
c= tensor(4.7416e+08, device='cuda:0')
c= tensor(4.7416e+08, device='cuda:0')
c= tensor(4.7417e+08, device='cuda:0')
c= tensor(4.7418e+08, device='cuda:0')
c= tensor(4.7426e+08, device='cuda:0')
c= tensor(4.7426e+08, device='cuda:0')
c= tensor(4.7426e+08, device='cuda:0')
c= tensor(4.7427e+08, device='cuda:0')
c= tensor(4.7427e+08, device='cuda:0')
c= tensor(4.7428e+08, device='cuda:0')
c= tensor(4.7428e+08, device='cuda:0')
c= tensor(4.7429e+08, device='cuda:0')
c= tensor(4.7429e+08, device='cuda:0')
c= tensor(4.7429e+08, device='cuda:0')
c= tensor(4.7430e+08, device='cuda:0')
c= tensor(4.7433e+08, device='cuda:0')
c= tensor(4.7434e+08, device='cuda:0')
c= tensor(4.7434e+08, device='cuda:0')
c= tensor(4.7435e+08, device='cuda:0')
c= tensor(4.7435e+08, device='cuda:0')
c= tensor(4.7436e+08, device='cuda:0')
c= tensor(4.7436e+08, device='cuda:0')
c= tensor(4.7438e+08, device='cuda:0')
c= tensor(4.7438e+08, device='cuda:0')
c= tensor(4.7439e+08, device='cuda:0')
c= tensor(4.7447e+08, device='cuda:0')
c= tensor(4.7449e+08, device='cuda:0')
c= tensor(4.7451e+08, device='cuda:0')
c= tensor(4.7451e+08, device='cuda:0')
c= tensor(4.7452e+08, device='cuda:0')
c= tensor(4.7453e+08, device='cuda:0')
c= tensor(4.7455e+08, device='cuda:0')
c= tensor(4.7455e+08, device='cuda:0')
c= tensor(4.7456e+08, device='cuda:0')
c= tensor(4.7456e+08, device='cuda:0')
c= tensor(4.7457e+08, device='cuda:0')
c= tensor(4.7457e+08, device='cuda:0')
c= tensor(4.7457e+08, device='cuda:0')
c= tensor(4.7458e+08, device='cuda:0')
c= tensor(4.7459e+08, device='cuda:0')
c= tensor(4.7461e+08, device='cuda:0')
c= tensor(4.7462e+08, device='cuda:0')
c= tensor(4.7462e+08, device='cuda:0')
c= tensor(4.7463e+08, device='cuda:0')
c= tensor(4.7464e+08, device='cuda:0')
c= tensor(4.7471e+08, device='cuda:0')
c= tensor(4.7472e+08, device='cuda:0')
c= tensor(4.7472e+08, device='cuda:0')
c= tensor(4.7472e+08, device='cuda:0')
c= tensor(4.7473e+08, device='cuda:0')
c= tensor(4.7473e+08, device='cuda:0')
c= tensor(4.7473e+08, device='cuda:0')
c= tensor(4.7475e+08, device='cuda:0')
c= tensor(4.7476e+08, device='cuda:0')
c= tensor(4.7477e+08, device='cuda:0')
c= tensor(4.7479e+08, device='cuda:0')
c= tensor(4.7480e+08, device='cuda:0')
c= tensor(4.7480e+08, device='cuda:0')
c= tensor(4.7480e+08, device='cuda:0')
c= tensor(4.7482e+08, device='cuda:0')
c= tensor(4.7482e+08, device='cuda:0')
c= tensor(4.7483e+08, device='cuda:0')
c= tensor(4.7483e+08, device='cuda:0')
c= tensor(4.7483e+08, device='cuda:0')
c= tensor(4.7484e+08, device='cuda:0')
c= tensor(4.7484e+08, device='cuda:0')
c= tensor(4.7484e+08, device='cuda:0')
c= tensor(4.7489e+08, device='cuda:0')
c= tensor(4.7491e+08, device='cuda:0')
c= tensor(4.7491e+08, device='cuda:0')
c= tensor(4.7492e+08, device='cuda:0')
c= tensor(4.7492e+08, device='cuda:0')
c= tensor(4.7492e+08, device='cuda:0')
c= tensor(4.7493e+08, device='cuda:0')
c= tensor(4.7493e+08, device='cuda:0')
c= tensor(4.7494e+08, device='cuda:0')
c= tensor(4.7494e+08, device='cuda:0')
c= tensor(4.7494e+08, device='cuda:0')
c= tensor(4.7499e+08, device='cuda:0')
c= tensor(4.7499e+08, device='cuda:0')
c= tensor(4.7507e+08, device='cuda:0')
c= tensor(4.7507e+08, device='cuda:0')
c= tensor(4.7508e+08, device='cuda:0')
c= tensor(4.7509e+08, device='cuda:0')
c= tensor(4.7509e+08, device='cuda:0')
c= tensor(4.7514e+08, device='cuda:0')
c= tensor(4.7515e+08, device='cuda:0')
c= tensor(4.7515e+08, device='cuda:0')
c= tensor(4.7515e+08, device='cuda:0')
c= tensor(4.7518e+08, device='cuda:0')
c= tensor(4.7518e+08, device='cuda:0')
c= tensor(4.7519e+08, device='cuda:0')
c= tensor(4.7520e+08, device='cuda:0')
c= tensor(4.7520e+08, device='cuda:0')
c= tensor(4.7520e+08, device='cuda:0')
c= tensor(4.7520e+08, device='cuda:0')
c= tensor(4.7521e+08, device='cuda:0')
c= tensor(4.7523e+08, device='cuda:0')
c= tensor(4.7523e+08, device='cuda:0')
c= tensor(4.7524e+08, device='cuda:0')
c= tensor(4.7564e+08, device='cuda:0')
c= tensor(4.7565e+08, device='cuda:0')
c= tensor(4.7566e+08, device='cuda:0')
c= tensor(4.7567e+08, device='cuda:0')
c= tensor(4.7568e+08, device='cuda:0')
c= tensor(4.7568e+08, device='cuda:0')
c= tensor(4.7569e+08, device='cuda:0')
c= tensor(4.7570e+08, device='cuda:0')
c= tensor(4.7570e+08, device='cuda:0')
c= tensor(4.7571e+08, device='cuda:0')
c= tensor(4.7572e+08, device='cuda:0')
c= tensor(4.7573e+08, device='cuda:0')
c= tensor(4.7573e+08, device='cuda:0')
c= tensor(4.7575e+08, device='cuda:0')
c= tensor(4.7577e+08, device='cuda:0')
c= tensor(4.7577e+08, device='cuda:0')
c= tensor(4.7577e+08, device='cuda:0')
c= tensor(4.7578e+08, device='cuda:0')
c= tensor(4.7583e+08, device='cuda:0')
c= tensor(4.7585e+08, device='cuda:0')
c= tensor(4.7586e+08, device='cuda:0')
c= tensor(4.7586e+08, device='cuda:0')
c= tensor(4.7586e+08, device='cuda:0')
c= tensor(4.7587e+08, device='cuda:0')
c= tensor(4.7587e+08, device='cuda:0')
c= tensor(4.7588e+08, device='cuda:0')
c= tensor(4.7588e+08, device='cuda:0')
c= tensor(4.7589e+08, device='cuda:0')
c= tensor(4.7589e+08, device='cuda:0')
c= tensor(4.7592e+08, device='cuda:0')
c= tensor(4.7593e+08, device='cuda:0')
c= tensor(4.7593e+08, device='cuda:0')
c= tensor(4.7594e+08, device='cuda:0')
c= tensor(4.7595e+08, device='cuda:0')
c= tensor(4.7596e+08, device='cuda:0')
c= tensor(4.7597e+08, device='cuda:0')
c= tensor(4.7601e+08, device='cuda:0')
c= tensor(4.7602e+08, device='cuda:0')
c= tensor(4.7603e+08, device='cuda:0')
c= tensor(4.7603e+08, device='cuda:0')
c= tensor(4.7603e+08, device='cuda:0')
c= tensor(4.7603e+08, device='cuda:0')
c= tensor(4.7603e+08, device='cuda:0')
c= tensor(4.7605e+08, device='cuda:0')
c= tensor(4.7605e+08, device='cuda:0')
c= tensor(4.7606e+08, device='cuda:0')
c= tensor(4.7606e+08, device='cuda:0')
c= tensor(4.7608e+08, device='cuda:0')
c= tensor(4.7608e+08, device='cuda:0')
c= tensor(4.7609e+08, device='cuda:0')
c= tensor(4.7609e+08, device='cuda:0')
c= tensor(4.7610e+08, device='cuda:0')
c= tensor(4.7610e+08, device='cuda:0')
c= tensor(4.7610e+08, device='cuda:0')
c= tensor(4.7611e+08, device='cuda:0')
c= tensor(4.7612e+08, device='cuda:0')
c= tensor(4.7612e+08, device='cuda:0')
c= tensor(4.7613e+08, device='cuda:0')
c= tensor(4.7614e+08, device='cuda:0')
c= tensor(4.7614e+08, device='cuda:0')
c= tensor(4.7615e+08, device='cuda:0')
c= tensor(4.7617e+08, device='cuda:0')
c= tensor(4.7617e+08, device='cuda:0')
c= tensor(4.7618e+08, device='cuda:0')
c= tensor(4.7639e+08, device='cuda:0')
c= tensor(4.7762e+08, device='cuda:0')
c= tensor(4.7766e+08, device='cuda:0')
c= tensor(4.7768e+08, device='cuda:0')
c= tensor(4.7769e+08, device='cuda:0')
c= tensor(4.7771e+08, device='cuda:0')
c= tensor(4.7814e+08, device='cuda:0')
c= tensor(4.8660e+08, device='cuda:0')
c= tensor(4.8660e+08, device='cuda:0')
c= tensor(4.9319e+08, device='cuda:0')
c= tensor(4.9465e+08, device='cuda:0')
c= tensor(4.9534e+08, device='cuda:0')
c= tensor(5.0506e+08, device='cuda:0')
c= tensor(5.0507e+08, device='cuda:0')
c= tensor(5.0509e+08, device='cuda:0')
c= tensor(5.1298e+08, device='cuda:0')
c= tensor(6.4717e+08, device='cuda:0')
c= tensor(6.4718e+08, device='cuda:0')
c= tensor(6.4761e+08, device='cuda:0')
c= tensor(6.4812e+08, device='cuda:0')
c= tensor(6.4947e+08, device='cuda:0')
c= tensor(6.5679e+08, device='cuda:0')
c= tensor(6.5953e+08, device='cuda:0')
c= tensor(6.6055e+08, device='cuda:0')
c= tensor(6.6076e+08, device='cuda:0')
c= tensor(6.6076e+08, device='cuda:0')
c= tensor(6.7003e+08, device='cuda:0')
c= tensor(6.7033e+08, device='cuda:0')
c= tensor(6.7033e+08, device='cuda:0')
c= tensor(6.7054e+08, device='cuda:0')
c= tensor(6.7246e+08, device='cuda:0')
c= tensor(6.7793e+08, device='cuda:0')
c= tensor(6.7980e+08, device='cuda:0')
c= tensor(6.7981e+08, device='cuda:0')
c= tensor(6.8022e+08, device='cuda:0')
c= tensor(6.8024e+08, device='cuda:0')
c= tensor(6.8150e+08, device='cuda:0')
c= tensor(6.8619e+08, device='cuda:0')
c= tensor(6.8635e+08, device='cuda:0')
c= tensor(6.8950e+08, device='cuda:0')
c= tensor(6.8950e+08, device='cuda:0')
c= tensor(6.8952e+08, device='cuda:0')
c= tensor(6.9138e+08, device='cuda:0')
c= tensor(6.9233e+08, device='cuda:0')
c= tensor(6.9449e+08, device='cuda:0')
c= tensor(6.9450e+08, device='cuda:0')
c= tensor(7.1938e+08, device='cuda:0')
c= tensor(7.1950e+08, device='cuda:0')
c= tensor(7.1985e+08, device='cuda:0')
c= tensor(7.2140e+08, device='cuda:0')
c= tensor(7.2141e+08, device='cuda:0')
c= tensor(7.2539e+08, device='cuda:0')
c= tensor(7.4131e+08, device='cuda:0')
c= tensor(7.7465e+08, device='cuda:0')
c= tensor(7.7589e+08, device='cuda:0')
c= tensor(7.7630e+08, device='cuda:0')
c= tensor(7.7634e+08, device='cuda:0')
c= tensor(7.7636e+08, device='cuda:0')
c= tensor(7.8304e+08, device='cuda:0')
c= tensor(7.8310e+08, device='cuda:0')
c= tensor(7.8370e+08, device='cuda:0')
c= tensor(7.9036e+08, device='cuda:0')
c= tensor(7.9086e+08, device='cuda:0')
c= tensor(7.9109e+08, device='cuda:0')
c= tensor(7.9110e+08, device='cuda:0')
c= tensor(7.9619e+08, device='cuda:0')
c= tensor(8.3392e+08, device='cuda:0')
c= tensor(8.3551e+08, device='cuda:0')
c= tensor(8.3553e+08, device='cuda:0')
c= tensor(8.3745e+08, device='cuda:0')
c= tensor(8.3755e+08, device='cuda:0')
c= tensor(8.5139e+08, device='cuda:0')
c= tensor(8.5153e+08, device='cuda:0')
c= tensor(8.5452e+08, device='cuda:0')
c= tensor(8.5468e+08, device='cuda:0')
c= tensor(8.5878e+08, device='cuda:0')
c= tensor(8.5979e+08, device='cuda:0')
c= tensor(8.6014e+08, device='cuda:0')
c= tensor(8.6438e+08, device='cuda:0')
c= tensor(8.7670e+08, device='cuda:0')
c= tensor(8.7681e+08, device='cuda:0')
c= tensor(8.8847e+08, device='cuda:0')
c= tensor(8.9496e+08, device='cuda:0')
c= tensor(9.1748e+08, device='cuda:0')
c= tensor(9.1781e+08, device='cuda:0')
c= tensor(9.1782e+08, device='cuda:0')
c= tensor(9.1789e+08, device='cuda:0')
c= tensor(9.2005e+08, device='cuda:0')
c= tensor(9.2042e+08, device='cuda:0')
c= tensor(9.2271e+08, device='cuda:0')
c= tensor(9.2271e+08, device='cuda:0')
c= tensor(9.2413e+08, device='cuda:0')
c= tensor(9.2840e+08, device='cuda:0')
c= tensor(9.2900e+08, device='cuda:0')
c= tensor(9.2903e+08, device='cuda:0')
c= tensor(9.3096e+08, device='cuda:0')
c= tensor(9.3113e+08, device='cuda:0')
c= tensor(9.3116e+08, device='cuda:0')
c= tensor(9.3124e+08, device='cuda:0')
c= tensor(9.3126e+08, device='cuda:0')
c= tensor(9.3278e+08, device='cuda:0')
c= tensor(9.3315e+08, device='cuda:0')
c= tensor(9.3334e+08, device='cuda:0')
c= tensor(9.3396e+08, device='cuda:0')
c= tensor(9.3398e+08, device='cuda:0')
c= tensor(1.5268e+09, device='cuda:0')
c= tensor(1.5268e+09, device='cuda:0')
c= tensor(1.5312e+09, device='cuda:0')
c= tensor(1.5312e+09, device='cuda:0')
c= tensor(1.5312e+09, device='cuda:0')
c= tensor(1.5313e+09, device='cuda:0')
c= tensor(1.5314e+09, device='cuda:0')
c= tensor(1.5314e+09, device='cuda:0')
c= tensor(1.5346e+09, device='cuda:0')
c= tensor(1.5346e+09, device='cuda:0')
c= tensor(1.5346e+09, device='cuda:0')
c= tensor(1.5469e+09, device='cuda:0')
c= tensor(1.5480e+09, device='cuda:0')
c= tensor(1.5484e+09, device='cuda:0')
c= tensor(1.5518e+09, device='cuda:0')
c= tensor(1.5644e+09, device='cuda:0')
c= tensor(1.5644e+09, device='cuda:0')
c= tensor(1.5644e+09, device='cuda:0')
c= tensor(1.5647e+09, device='cuda:0')
c= tensor(1.5647e+09, device='cuda:0')
c= tensor(1.5647e+09, device='cuda:0')
c= tensor(1.5648e+09, device='cuda:0')
c= tensor(1.5648e+09, device='cuda:0')
c= tensor(1.5649e+09, device='cuda:0')
c= tensor(1.5649e+09, device='cuda:0')
c= tensor(1.5649e+09, device='cuda:0')
c= tensor(1.5796e+09, device='cuda:0')
c= tensor(1.5797e+09, device='cuda:0')
c= tensor(1.5813e+09, device='cuda:0')
c= tensor(1.5813e+09, device='cuda:0')
c= tensor(1.5813e+09, device='cuda:0')
c= tensor(1.5844e+09, device='cuda:0')
c= tensor(1.7331e+09, device='cuda:0')
c= tensor(1.7780e+09, device='cuda:0')
c= tensor(1.7781e+09, device='cuda:0')
c= tensor(1.7790e+09, device='cuda:0')
c= tensor(1.7790e+09, device='cuda:0')
c= tensor(1.7791e+09, device='cuda:0')
c= tensor(1.8476e+09, device='cuda:0')
c= tensor(1.8478e+09, device='cuda:0')
c= tensor(1.8478e+09, device='cuda:0')
c= tensor(1.8493e+09, device='cuda:0')
c= tensor(1.9168e+09, device='cuda:0')
c= tensor(1.9172e+09, device='cuda:0')
c= tensor(1.9173e+09, device='cuda:0')
c= tensor(1.9174e+09, device='cuda:0')
c= tensor(1.9174e+09, device='cuda:0')
c= tensor(1.9175e+09, device='cuda:0')
c= tensor(1.9252e+09, device='cuda:0')
c= tensor(1.9254e+09, device='cuda:0')
c= tensor(1.9254e+09, device='cuda:0')
c= tensor(1.9274e+09, device='cuda:0')
c= tensor(1.9275e+09, device='cuda:0')
c= tensor(1.9275e+09, device='cuda:0')
c= tensor(1.9280e+09, device='cuda:0')
c= tensor(1.9308e+09, device='cuda:0')
c= tensor(1.9453e+09, device='cuda:0')
c= tensor(1.9571e+09, device='cuda:0')
c= tensor(1.9668e+09, device='cuda:0')
c= tensor(1.9670e+09, device='cuda:0')
c= tensor(1.9683e+09, device='cuda:0')
c= tensor(1.9723e+09, device='cuda:0')
c= tensor(1.9826e+09, device='cuda:0')
c= tensor(1.9827e+09, device='cuda:0')
c= tensor(2.2261e+09, device='cuda:0')
c= tensor(2.7950e+09, device='cuda:0')
c= tensor(2.8096e+09, device='cuda:0')
c= tensor(2.8145e+09, device='cuda:0')
c= tensor(2.8166e+09, device='cuda:0')
c= tensor(2.8166e+09, device='cuda:0')
c= tensor(2.8167e+09, device='cuda:0')
c= tensor(2.8167e+09, device='cuda:0')
c= tensor(2.8199e+09, device='cuda:0')
c= tensor(2.8240e+09, device='cuda:0')
c= tensor(2.8477e+09, device='cuda:0')
c= tensor(2.8508e+09, device='cuda:0')
c= tensor(2.8522e+09, device='cuda:0')
c= tensor(2.8524e+09, device='cuda:0')
c= tensor(2.8566e+09, device='cuda:0')
c= tensor(2.8566e+09, device='cuda:0')
c= tensor(2.8567e+09, device='cuda:0')
c= tensor(2.8600e+09, device='cuda:0')
c= tensor(2.8609e+09, device='cuda:0')
c= tensor(2.8609e+09, device='cuda:0')
c= tensor(2.8610e+09, device='cuda:0')
c= tensor(2.9820e+09, device='cuda:0')
c= tensor(2.9821e+09, device='cuda:0')
c= tensor(2.9856e+09, device='cuda:0')
c= tensor(2.9856e+09, device='cuda:0')
c= tensor(2.9857e+09, device='cuda:0')
c= tensor(2.9857e+09, device='cuda:0')
c= tensor(2.9857e+09, device='cuda:0')
c= tensor(2.9858e+09, device='cuda:0')
c= tensor(2.9865e+09, device='cuda:0')
c= tensor(2.9867e+09, device='cuda:0')
c= tensor(2.9970e+09, device='cuda:0')
c= tensor(2.9971e+09, device='cuda:0')
c= tensor(3.0010e+09, device='cuda:0')
c= tensor(3.0011e+09, device='cuda:0')
c= tensor(3.0037e+09, device='cuda:0')
c= tensor(3.0037e+09, device='cuda:0')
c= tensor(3.0038e+09, device='cuda:0')
c= tensor(3.0041e+09, device='cuda:0')
c= tensor(3.0044e+09, device='cuda:0')
c= tensor(3.0057e+09, device='cuda:0')
c= tensor(3.2190e+09, device='cuda:0')
c= tensor(3.2191e+09, device='cuda:0')
c= tensor(3.2191e+09, device='cuda:0')
c= tensor(3.2195e+09, device='cuda:0')
c= tensor(3.2197e+09, device='cuda:0')
c= tensor(3.2561e+09, device='cuda:0')
c= tensor(3.2561e+09, device='cuda:0')
c= tensor(3.2586e+09, device='cuda:0')
c= tensor(3.2675e+09, device='cuda:0')
c= tensor(3.2675e+09, device='cuda:0')
c= tensor(3.3089e+09, device='cuda:0')
c= tensor(3.3094e+09, device='cuda:0')
c= tensor(3.3617e+09, device='cuda:0')
c= tensor(3.3617e+09, device='cuda:0')
c= tensor(3.3622e+09, device='cuda:0')
c= tensor(3.3623e+09, device='cuda:0')
c= tensor(3.3623e+09, device='cuda:0')
c= tensor(3.3623e+09, device='cuda:0')
c= tensor(3.3633e+09, device='cuda:0')
c= tensor(3.3633e+09, device='cuda:0')
c= tensor(3.3672e+09, device='cuda:0')
c= tensor(3.3672e+09, device='cuda:0')
c= tensor(3.3673e+09, device='cuda:0')
c= tensor(3.3674e+09, device='cuda:0')
c= tensor(3.3754e+09, device='cuda:0')
c= tensor(3.3815e+09, device='cuda:0')
c= tensor(3.3964e+09, device='cuda:0')
c= tensor(3.3966e+09, device='cuda:0')
c= tensor(3.3966e+09, device='cuda:0')
c= tensor(3.3966e+09, device='cuda:0')
c= tensor(3.3967e+09, device='cuda:0')
c= tensor(3.4413e+09, device='cuda:0')
c= tensor(3.4413e+09, device='cuda:0')
c= tensor(3.4415e+09, device='cuda:0')
c= tensor(3.4427e+09, device='cuda:0')
c= tensor(3.4433e+09, device='cuda:0')
c= tensor(3.4438e+09, device='cuda:0')
c= tensor(3.4438e+09, device='cuda:0')
c= tensor(3.4539e+09, device='cuda:0')
c= tensor(3.4548e+09, device='cuda:0')
c= tensor(3.4548e+09, device='cuda:0')
c= tensor(3.4552e+09, device='cuda:0')
c= tensor(3.4590e+09, device='cuda:0')
c= tensor(3.4618e+09, device='cuda:0')
c= tensor(3.5074e+09, device='cuda:0')
c= tensor(3.5120e+09, device='cuda:0')
c= tensor(3.5120e+09, device='cuda:0')
c= tensor(3.5125e+09, device='cuda:0')
c= tensor(3.5138e+09, device='cuda:0')
c= tensor(3.5138e+09, device='cuda:0')
c= tensor(3.5139e+09, device='cuda:0')
c= tensor(3.5139e+09, device='cuda:0')
c= tensor(3.5156e+09, device='cuda:0')
c= tensor(3.5162e+09, device='cuda:0')
c= tensor(3.5162e+09, device='cuda:0')
c= tensor(3.5163e+09, device='cuda:0')
c= tensor(3.5163e+09, device='cuda:0')
c= tensor(3.5167e+09, device='cuda:0')
c= tensor(3.5167e+09, device='cuda:0')
c= tensor(3.5169e+09, device='cuda:0')
c= tensor(3.5183e+09, device='cuda:0')
c= tensor(3.5184e+09, device='cuda:0')
c= tensor(3.5184e+09, device='cuda:0')
c= tensor(3.5184e+09, device='cuda:0')
c= tensor(3.5187e+09, device='cuda:0')
c= tensor(3.5272e+09, device='cuda:0')
c= tensor(3.5272e+09, device='cuda:0')
c= tensor(3.5272e+09, device='cuda:0')
c= tensor(3.5272e+09, device='cuda:0')
c= tensor(3.5301e+09, device='cuda:0')
c= tensor(3.5741e+09, device='cuda:0')
c= tensor(3.5765e+09, device='cuda:0')
c= tensor(3.5765e+09, device='cuda:0')
c= tensor(3.5884e+09, device='cuda:0')
c= tensor(3.5926e+09, device='cuda:0')
c= tensor(3.5927e+09, device='cuda:0')
c= tensor(3.5927e+09, device='cuda:0')
c= tensor(3.5931e+09, device='cuda:0')
c= tensor(3.6009e+09, device='cuda:0')
c= tensor(3.6248e+09, device='cuda:0')
c= tensor(3.6309e+09, device='cuda:0')
c= tensor(3.6318e+09, device='cuda:0')
c= tensor(3.6319e+09, device='cuda:0')
c= tensor(3.6319e+09, device='cuda:0')
c= tensor(3.6326e+09, device='cuda:0')
c= tensor(3.6327e+09, device='cuda:0')
c= tensor(3.6330e+09, device='cuda:0')
c= tensor(3.6364e+09, device='cuda:0')
c= tensor(3.6776e+09, device='cuda:0')
c= tensor(3.6777e+09, device='cuda:0')
c= tensor(3.6777e+09, device='cuda:0')
c= tensor(3.6781e+09, device='cuda:0')
c= tensor(3.6810e+09, device='cuda:0')
c= tensor(3.6832e+09, device='cuda:0')
c= tensor(3.6837e+09, device='cuda:0')
c= tensor(3.6838e+09, device='cuda:0')
c= tensor(3.6843e+09, device='cuda:0')
c= tensor(3.6843e+09, device='cuda:0')
c= tensor(3.6852e+09, device='cuda:0')
c= tensor(3.6852e+09, device='cuda:0')
c= tensor(3.6853e+09, device='cuda:0')
c= tensor(3.6853e+09, device='cuda:0')
c= tensor(3.6853e+09, device='cuda:0')
c= tensor(3.6854e+09, device='cuda:0')
c= tensor(3.6903e+09, device='cuda:0')
c= tensor(3.7021e+09, device='cuda:0')
c= tensor(3.7050e+09, device='cuda:0')
c= tensor(3.7144e+09, device='cuda:0')
c= tensor(3.7146e+09, device='cuda:0')
c= tensor(3.7147e+09, device='cuda:0')
c= tensor(3.7148e+09, device='cuda:0')
c= tensor(3.7175e+09, device='cuda:0')
c= tensor(3.7181e+09, device='cuda:0')
c= tensor(3.7186e+09, device='cuda:0')
c= tensor(3.7186e+09, device='cuda:0')
c= tensor(3.9095e+09, device='cuda:0')
c= tensor(3.9097e+09, device='cuda:0')
c= tensor(3.9107e+09, device='cuda:0')
c= tensor(3.9187e+09, device='cuda:0')
c= tensor(3.9198e+09, device='cuda:0')
c= tensor(3.9201e+09, device='cuda:0')
c= tensor(4.0522e+09, device='cuda:0')
c= tensor(4.0597e+09, device='cuda:0')
c= tensor(4.0615e+09, device='cuda:0')
c= tensor(4.0618e+09, device='cuda:0')
c= tensor(4.0623e+09, device='cuda:0')
c= tensor(4.0624e+09, device='cuda:0')
c= tensor(4.0768e+09, device='cuda:0')
c= tensor(4.2464e+09, device='cuda:0')
c= tensor(4.2509e+09, device='cuda:0')
c= tensor(4.2563e+09, device='cuda:0')
c= tensor(4.2620e+09, device='cuda:0')
c= tensor(4.2623e+09, device='cuda:0')
c= tensor(4.2630e+09, device='cuda:0')
c= tensor(4.2631e+09, device='cuda:0')
c= tensor(4.2668e+09, device='cuda:0')
c= tensor(4.2869e+09, device='cuda:0')
c= tensor(4.2892e+09, device='cuda:0')
c= tensor(4.4065e+09, device='cuda:0')
c= tensor(4.4138e+09, device='cuda:0')
c= tensor(4.4144e+09, device='cuda:0')
c= tensor(4.4145e+09, device='cuda:0')
c= tensor(4.4159e+09, device='cuda:0')
c= tensor(4.4199e+09, device='cuda:0')
c= tensor(4.4200e+09, device='cuda:0')
c= tensor(4.4567e+09, device='cuda:0')
c= tensor(4.4576e+09, device='cuda:0')
c= tensor(4.4580e+09, device='cuda:0')
c= tensor(4.4582e+09, device='cuda:0')
c= tensor(4.4582e+09, device='cuda:0')
c= tensor(4.4582e+09, device='cuda:0')
c= tensor(4.4582e+09, device='cuda:0')
c= tensor(4.4584e+09, device='cuda:0')
c= tensor(4.4594e+09, device='cuda:0')
c= tensor(5.9340e+09, device='cuda:0')
c= tensor(5.9343e+09, device='cuda:0')
c= tensor(5.9400e+09, device='cuda:0')
c= tensor(5.9400e+09, device='cuda:0')
c= tensor(5.9402e+09, device='cuda:0')
c= tensor(5.9403e+09, device='cuda:0')
c= tensor(5.9502e+09, device='cuda:0')
c= tensor(5.9513e+09, device='cuda:0')
c= tensor(6.0313e+09, device='cuda:0')
c= tensor(6.0313e+09, device='cuda:0')
c= tensor(6.0370e+09, device='cuda:0')
c= tensor(6.0372e+09, device='cuda:0')
c= tensor(6.0419e+09, device='cuda:0')
c= tensor(6.0702e+09, device='cuda:0')
c= tensor(6.0705e+09, device='cuda:0')
c= tensor(6.0705e+09, device='cuda:0')
c= tensor(6.0722e+09, device='cuda:0')
c= tensor(6.0724e+09, device='cuda:0')
c= tensor(6.0728e+09, device='cuda:0')
c= tensor(6.0808e+09, device='cuda:0')
c= tensor(6.0810e+09, device='cuda:0')
c= tensor(6.0826e+09, device='cuda:0')
c= tensor(6.0831e+09, device='cuda:0')
c= tensor(6.0864e+09, device='cuda:0')
c= tensor(6.0963e+09, device='cuda:0')
c= tensor(6.0971e+09, device='cuda:0')
c= tensor(6.0973e+09, device='cuda:0')
c= tensor(6.1023e+09, device='cuda:0')
c= tensor(6.1045e+09, device='cuda:0')
c= tensor(6.1192e+09, device='cuda:0')
c= tensor(6.1199e+09, device='cuda:0')
c= tensor(6.1207e+09, device='cuda:0')
c= tensor(6.1211e+09, device='cuda:0')
c= tensor(6.1425e+09, device='cuda:0')
c= tensor(6.1467e+09, device='cuda:0')
c= tensor(6.1468e+09, device='cuda:0')
c= tensor(6.1468e+09, device='cuda:0')
c= tensor(6.1469e+09, device='cuda:0')
c= tensor(6.1484e+09, device='cuda:0')
c= tensor(6.1528e+09, device='cuda:0')
c= tensor(6.1542e+09, device='cuda:0')
c= tensor(6.1542e+09, device='cuda:0')
c= tensor(6.1545e+09, device='cuda:0')
c= tensor(6.1548e+09, device='cuda:0')
c= tensor(6.1560e+09, device='cuda:0')
c= tensor(6.1563e+09, device='cuda:0')
c= tensor(6.1573e+09, device='cuda:0')
c= tensor(6.1578e+09, device='cuda:0')
c= tensor(6.1584e+09, device='cuda:0')
c= tensor(6.1584e+09, device='cuda:0')
c= tensor(6.1584e+09, device='cuda:0')
c= tensor(6.1794e+09, device='cuda:0')
c= tensor(6.1796e+09, device='cuda:0')
c= tensor(6.1848e+09, device='cuda:0')
c= tensor(6.1848e+09, device='cuda:0')
c= tensor(6.1849e+09, device='cuda:0')
c= tensor(6.1856e+09, device='cuda:0')
c= tensor(6.1865e+09, device='cuda:0')
c= tensor(6.1869e+09, device='cuda:0')
c= tensor(6.1869e+09, device='cuda:0')
c= tensor(6.1869e+09, device='cuda:0')
c= tensor(6.1876e+09, device='cuda:0')
c= tensor(6.1878e+09, device='cuda:0')
c= tensor(6.1920e+09, device='cuda:0')
c= tensor(6.1920e+09, device='cuda:0')
c= tensor(6.1923e+09, device='cuda:0')
c= tensor(6.1936e+09, device='cuda:0')
c= tensor(6.1937e+09, device='cuda:0')
c= tensor(6.2020e+09, device='cuda:0')
c= tensor(6.2028e+09, device='cuda:0')
c= tensor(6.2046e+09, device='cuda:0')
c= tensor(6.2074e+09, device='cuda:0')
c= tensor(6.2106e+09, device='cuda:0')
c= tensor(6.2141e+09, device='cuda:0')
c= tensor(6.2168e+09, device='cuda:0')
c= tensor(6.2168e+09, device='cuda:0')
c= tensor(6.2175e+09, device='cuda:0')
c= tensor(6.2178e+09, device='cuda:0')
c= tensor(6.2188e+09, device='cuda:0')
c= tensor(6.2250e+09, device='cuda:0')
c= tensor(6.2255e+09, device='cuda:0')
c= tensor(6.2267e+09, device='cuda:0')
c= tensor(6.2269e+09, device='cuda:0')
c= tensor(6.2301e+09, device='cuda:0')
c= tensor(6.2301e+09, device='cuda:0')
c= tensor(6.2302e+09, device='cuda:0')
c= tensor(6.2399e+09, device='cuda:0')
c= tensor(6.2642e+09, device='cuda:0')
c= tensor(6.2643e+09, device='cuda:0')
c= tensor(6.2644e+09, device='cuda:0')
c= tensor(6.2646e+09, device='cuda:0')
c= tensor(6.2672e+09, device='cuda:0')
c= tensor(6.2679e+09, device='cuda:0')
c= tensor(6.2679e+09, device='cuda:0')
c= tensor(6.2684e+09, device='cuda:0')
c= tensor(6.2737e+09, device='cuda:0')
c= tensor(6.2737e+09, device='cuda:0')
c= tensor(6.2747e+09, device='cuda:0')
c= tensor(6.2750e+09, device='cuda:0')
time to make c is 10.591195106506348
time for making loss is 10.59132981300354
p0 True
it  0 : 2045681664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5139701760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
5139902464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2121615100.0
relative error loss 0.3381037
shape of L is 
torch.Size([])
memory (bytes)
5166329856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5166489600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2107355100.0
relative error loss 0.33583122
shape of L is 
torch.Size([])
memory (bytes)
5170135040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
5170274304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2076151000.0
relative error loss 0.3308585
shape of L is 
torch.Size([])
memory (bytes)
5173460992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
5173460992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2060469500.0
relative error loss 0.32835945
shape of L is 
torch.Size([])
memory (bytes)
5176598528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5176598528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2047762400.0
relative error loss 0.32633445
shape of L is 
torch.Size([])
memory (bytes)
5179854848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5179854848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2030785000.0
relative error loss 0.3236289
shape of L is 
torch.Size([])
memory (bytes)
5183012864
| ID | GPU | MEM |
------------------
|  0 |  8% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5183012864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2020295700.0
relative error loss 0.3219573
shape of L is 
torch.Size([])
memory (bytes)
5186281472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5186281472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2014288100.0
relative error loss 0.32099992
shape of L is 
torch.Size([])
memory (bytes)
5189287936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5189500928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2001072900.0
relative error loss 0.31889394
shape of L is 
torch.Size([])
memory (bytes)
5192699904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5192699904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1996801800.0
relative error loss 0.31821328
time to take a step is 274.4648907184601
it  1 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5195874304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5195874304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1996801800.0
relative error loss 0.31821328
shape of L is 
torch.Size([])
memory (bytes)
5199122432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5199122432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1992848900.0
relative error loss 0.31758335
shape of L is 
torch.Size([])
memory (bytes)
5202321408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5202321408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1983309800.0
relative error loss 0.31606317
shape of L is 
torch.Size([])
memory (bytes)
5205544960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
5205544960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1985598000.0
relative error loss 0.31642783
shape of L is 
torch.Size([])
memory (bytes)
5208612864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5208612864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1980328000.0
relative error loss 0.31558797
shape of L is 
torch.Size([])
memory (bytes)
5211963392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5211963392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1977671200.0
relative error loss 0.3151646
shape of L is 
torch.Size([])
memory (bytes)
5215109120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5215109120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1972991500.0
relative error loss 0.31441882
shape of L is 
torch.Size([])
memory (bytes)
5218377728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5218377728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1970649600.0
relative error loss 0.31404564
shape of L is 
torch.Size([])
memory (bytes)
5221462016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5221462016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1968640000.0
relative error loss 0.31372538
shape of L is 
torch.Size([])
memory (bytes)
5224808448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5224808448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 10% |
error is  1965746700.0
relative error loss 0.31326428
time to take a step is 276.56077790260315
it  2 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5227888640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5228019712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1965746700.0
relative error loss 0.31326428
shape of L is 
torch.Size([])
memory (bytes)
5231243264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5231243264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 10% |
error is  1963840500.0
relative error loss 0.31296054
shape of L is 
torch.Size([])
memory (bytes)
5234429952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5234429952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1962422800.0
relative error loss 0.3127346
shape of L is 
torch.Size([])
memory (bytes)
5237649408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
5237649408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1959556600.0
relative error loss 0.31227782
shape of L is 
torch.Size([])
memory (bytes)
5240856576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5240856576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1958499800.0
relative error loss 0.3121094
shape of L is 
torch.Size([])
memory (bytes)
5243949056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5244071936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1956826600.0
relative error loss 0.31184277
shape of L is 
torch.Size([])
memory (bytes)
5247283200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
5247283200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1955116500.0
relative error loss 0.31157026
shape of L is 
torch.Size([])
memory (bytes)
5250424832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5250498560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1953435100.0
relative error loss 0.3113023
shape of L is 
torch.Size([])
memory (bytes)
5253705728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
5253705728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1952342500.0
relative error loss 0.3111282
shape of L is 
torch.Size([])
memory (bytes)
5256859648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
5256859648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1951053300.0
relative error loss 0.31092274
time to take a step is 270.2110300064087
it  3 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5260132352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5260132352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1951053300.0
relative error loss 0.31092274
shape of L is 
torch.Size([])
memory (bytes)
5263278080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5263278080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1950085600.0
relative error loss 0.3107685
shape of L is 
torch.Size([])
memory (bytes)
5266538496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5266538496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1948640300.0
relative error loss 0.31053817
shape of L is 
torch.Size([])
memory (bytes)
5269745664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5269745664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1947908600.0
relative error loss 0.3104216
shape of L is 
torch.Size([])
memory (bytes)
5272813568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 10% |
memory (bytes)
5272952832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1947184100.0
relative error loss 0.31030613
shape of L is 
torch.Size([])
memory (bytes)
5276168192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5276168192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 10% |
error is  1945866200.0
relative error loss 0.3100961
shape of L is 
torch.Size([])
memory (bytes)
5279363072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5279363072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1945032700.0
relative error loss 0.3099633
shape of L is 
torch.Size([])
memory (bytes)
5282476032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5282476032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1943984100.0
relative error loss 0.30979618
shape of L is 
torch.Size([])
memory (bytes)
5285785600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5285785600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  1943381500.0
relative error loss 0.30970013
shape of L is 
torch.Size([])
memory (bytes)
5288861696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5288992768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 10% |
error is  1942367200.0
relative error loss 0.3095385
time to take a step is 295.471248626709
it  4 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5292191744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5292191744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1942367200.0
relative error loss 0.3095385
shape of L is 
torch.Size([])
memory (bytes)
5295403008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5295403008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1942024200.0
relative error loss 0.30948383
shape of L is 
torch.Size([])
memory (bytes)
5298622464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5298622464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1940991000.0
relative error loss 0.3093192
shape of L is 
torch.Size([])
memory (bytes)
5301833728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5301833728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1940390400.0
relative error loss 0.30922347
shape of L is 
torch.Size([])
memory (bytes)
5305036800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5305036800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1939796000.0
relative error loss 0.30912876
shape of L is 
torch.Size([])
memory (bytes)
5308248064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5308248064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1938785800.0
relative error loss 0.30896777
shape of L is 
torch.Size([])
memory (bytes)
5311328256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5311459328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1938707500.0
relative error loss 0.30895528
shape of L is 
torch.Size([])
memory (bytes)
5314662400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5314662400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1938222600.0
relative error loss 0.308878
shape of L is 
torch.Size([])
memory (bytes)
5317881856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5317881856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 10% |
error is  1937440300.0
relative error loss 0.30875334
shape of L is 
torch.Size([])
memory (bytes)
5320953856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5321089024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1936963600.0
relative error loss 0.30867738
time to take a step is 297.97748589515686
it  5 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5324304384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5324304384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1936963600.0
relative error loss 0.30867738
shape of L is 
torch.Size([])
memory (bytes)
5327421440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5327421440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1936007200.0
relative error loss 0.30852497
shape of L is 
torch.Size([])
memory (bytes)
5330731008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
5330731008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1935800300.0
relative error loss 0.308492
shape of L is 
torch.Size([])
memory (bytes)
5333811200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5333811200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  1935163400.0
relative error loss 0.3083905
shape of L is 
torch.Size([])
memory (bytes)
5337055232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5337157632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1934855200.0
relative error loss 0.30834138
shape of L is 
torch.Size([])
memory (bytes)
5340368896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5340368896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1934346200.0
relative error loss 0.30826026
shape of L is 
torch.Size([])
memory (bytes)
5343391744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5343576064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1933802500.0
relative error loss 0.30817363
shape of L is 
torch.Size([])
memory (bytes)
5346799616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5346799616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1933259800.0
relative error loss 0.30808714
shape of L is 
torch.Size([])
memory (bytes)
5349908480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5349908480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1932910600.0
relative error loss 0.30803147
shape of L is 
torch.Size([])
memory (bytes)
5353238528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5353238528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1932421600.0
relative error loss 0.30795357
time to take a step is 270.76042127609253
it  6 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5356437504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5356437504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1932421600.0
relative error loss 0.30795357
shape of L is 
torch.Size([])
memory (bytes)
5359656960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5359656960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1931993100.0
relative error loss 0.30788526
shape of L is 
torch.Size([])
memory (bytes)
5362860032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5362860032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1931541500.0
relative error loss 0.30781332
shape of L is 
torch.Size([])
memory (bytes)
5366067200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5366067200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1931375100.0
relative error loss 0.3077868
shape of L is 
torch.Size([])
memory (bytes)
5369147392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5369147392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1931176400.0
relative error loss 0.30775514
shape of L is 
torch.Size([])
memory (bytes)
5372493824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
5372493824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1930842100.0
relative error loss 0.30770186
shape of L is 
torch.Size([])
memory (bytes)
5375696896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
5375696896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1930757100.0
relative error loss 0.3076883
shape of L is 
torch.Size([])
memory (bytes)
5378920448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5378920448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1930129900.0
relative error loss 0.30758834
shape of L is 
torch.Size([])
memory (bytes)
5381988352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5382123520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1929851400.0
relative error loss 0.30754396
shape of L is 
torch.Size([])
memory (bytes)
5385338880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5385338880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1929494500.0
relative error loss 0.3074871
time to take a step is 270.21097707748413
it  7 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5388349440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5388546048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1929494500.0
relative error loss 0.3074871
shape of L is 
torch.Size([])
memory (bytes)
5391753216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5391753216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1929040900.0
relative error loss 0.3074148
shape of L is 
torch.Size([])
memory (bytes)
5394964480
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5394964480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1928866800.0
relative error loss 0.30738705
shape of L is 
torch.Size([])
memory (bytes)
5398188032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5398188032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1928158200.0
relative error loss 0.30727413
shape of L is 
torch.Size([])
memory (bytes)
5401391104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5401391104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1927959600.0
relative error loss 0.30724248
shape of L is 
torch.Size([])
memory (bytes)
5404459008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5404590080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1927491600.0
relative error loss 0.30716792
shape of L is 
torch.Size([])
memory (bytes)
5407797248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5407797248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1927273000.0
relative error loss 0.30713305
shape of L is 
torch.Size([])
memory (bytes)
5411008512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5411008512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1926873100.0
relative error loss 0.30706933
shape of L is 
torch.Size([])
memory (bytes)
5414092800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5414092800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1926583300.0
relative error loss 0.30702317
shape of L is 
torch.Size([])
memory (bytes)
5417435136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5417435136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1926345200.0
relative error loss 0.30698523
time to take a step is 348.64214420318604
it  8 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5420638208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5420638208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1926345200.0
relative error loss 0.30698523
shape of L is 
torch.Size([])
memory (bytes)
5423845376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5423845376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  1926080500.0
relative error loss 0.30694303
shape of L is 
torch.Size([])
memory (bytes)
5427060736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5427060736
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 97% | 11% |
error is  1925683700.0
relative error loss 0.3068798
shape of L is 
torch.Size([])
memory (bytes)
5430263808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5430263808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1925426700.0
relative error loss 0.30683884
shape of L is 
torch.Size([])
memory (bytes)
5433475072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5433475072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1925174300.0
relative error loss 0.3067986
shape of L is 
torch.Size([])
memory (bytes)
5436694528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5436694528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1925121000.0
relative error loss 0.3067901
shape of L is 
torch.Size([])
memory (bytes)
5439852544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5439852544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1924691000.0
relative error loss 0.3067216
shape of L is 
torch.Size([])
memory (bytes)
5443108864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5443108864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1924515300.0
relative error loss 0.3066936
shape of L is 
torch.Size([])
memory (bytes)
5446320128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5446320128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1924297700.0
relative error loss 0.30665892
shape of L is 
torch.Size([])
memory (bytes)
5449465856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5449535488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1923994100.0
relative error loss 0.30661055
time to take a step is 362.85794281959534
it  9 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5452742656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
5452742656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1923994100.0
relative error loss 0.30661055
shape of L is 
torch.Size([])
memory (bytes)
5455958016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5455958016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1923984900.0
relative error loss 0.30660906
shape of L is 
torch.Size([])
memory (bytes)
5459161088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5459161088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1923774500.0
relative error loss 0.30657554
shape of L is 
torch.Size([])
memory (bytes)
5462368256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5462368256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1923496000.0
relative error loss 0.30653116
shape of L is 
torch.Size([])
memory (bytes)
5465444352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
5465583616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1923307500.0
relative error loss 0.30650112
shape of L is 
torch.Size([])
memory (bytes)
5468786688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5468786688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1922938900.0
relative error loss 0.30644238
shape of L is 
torch.Size([])
memory (bytes)
5471993856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
5471993856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1922766300.0
relative error loss 0.30641487
shape of L is 
torch.Size([])
memory (bytes)
5475069952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5475205120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1922563100.0
relative error loss 0.30638248
shape of L is 
torch.Size([])
memory (bytes)
5478404096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5478404096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1922411000.0
relative error loss 0.30635825
shape of L is 
torch.Size([])
memory (bytes)
5481615360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5481615360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1922212900.0
relative error loss 0.3063267
time to take a step is 353.3197145462036
it  10 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5484703744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5484703744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1922212900.0
relative error loss 0.3063267
shape of L is 
torch.Size([])
memory (bytes)
5488029696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5488029696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1921882100.0
relative error loss 0.30627397
shape of L is 
torch.Size([])
memory (bytes)
5491249152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5491249152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1921899500.0
relative error loss 0.30627674
shape of L is 
torch.Size([])
memory (bytes)
5494444032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
5494444032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1921727000.0
relative error loss 0.30624926
shape of L is 
torch.Size([])
memory (bytes)
5497659392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5497659392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1921555000.0
relative error loss 0.30622184
shape of L is 
torch.Size([])
memory (bytes)
5500735488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
5500735488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1921376300.0
relative error loss 0.30619335
shape of L is 
torch.Size([])
memory (bytes)
5504077824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5504077824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1921133600.0
relative error loss 0.30615467
shape of L is 
torch.Size([])
memory (bytes)
5507289088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5507289088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 11% |
error is  1920918500.0
relative error loss 0.30612043
shape of L is 
torch.Size([])
memory (bytes)
5510402048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5510402048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1920745000.0
relative error loss 0.30609274
shape of L is 
torch.Size([])
memory (bytes)
5513715712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5513715712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1920597500.0
relative error loss 0.30606925
time to take a step is 371.46836280822754
it  11 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5516922880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
5516922880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1920597500.0
relative error loss 0.30606925
shape of L is 
torch.Size([])
memory (bytes)
5520125952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5520125952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1920484400.0
relative error loss 0.30605122
shape of L is 
torch.Size([])
memory (bytes)
5523349504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5523353600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1920490000.0
relative error loss 0.30605212
shape of L is 
torch.Size([])
memory (bytes)
5526564864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5526564864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1920403000.0
relative error loss 0.30603826
shape of L is 
torch.Size([])
memory (bytes)
5529784320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5529784320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1920313300.0
relative error loss 0.30602396
shape of L is 
torch.Size([])
memory (bytes)
5532991488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5532991488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 11% |
error is  1920229900.0
relative error loss 0.30601066
shape of L is 
torch.Size([])
memory (bytes)
5536206848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5536206848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1920146400.0
relative error loss 0.30599737
shape of L is 
torch.Size([])
memory (bytes)
5539307520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5539307520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1920063500.0
relative error loss 0.30598414
shape of L is 
torch.Size([])
memory (bytes)
5542629376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5542629376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1920001000.0
relative error loss 0.3059742
shape of L is 
torch.Size([])
memory (bytes)
5545844736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5545844736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1919907300.0
relative error loss 0.30595925
time to take a step is 362.82239174842834
it  12 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5549056000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5549056000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1919907300.0
relative error loss 0.30595925
shape of L is 
torch.Size([])
memory (bytes)
5552287744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5552287744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1919886300.0
relative error loss 0.30595592
shape of L is 
torch.Size([])
memory (bytes)
5555494912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5555494912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1919715300.0
relative error loss 0.30592868
shape of L is 
torch.Size([])
memory (bytes)
5558714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5558714368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1919638000.0
relative error loss 0.30591634
shape of L is 
torch.Size([])
memory (bytes)
5561917440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5561917440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1919555600.0
relative error loss 0.3059032
shape of L is 
torch.Size([])
memory (bytes)
5565128704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5565128704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  1919413800.0
relative error loss 0.3058806
shape of L is 
torch.Size([])
memory (bytes)
5568299008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5568299008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1919192000.0
relative error loss 0.3058453
shape of L is 
torch.Size([])
memory (bytes)
5571563520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5571563520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1919029200.0
relative error loss 0.30581933
shape of L is 
torch.Size([])
memory (bytes)
5574717440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5574717440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1918844400.0
relative error loss 0.3057899
shape of L is 
torch.Size([])
memory (bytes)
5577981952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5577981952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1918570500.0
relative error loss 0.30574623
time to take a step is 295.60196805000305
it  13 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5581082624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5581082624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1918570500.0
relative error loss 0.30574623
shape of L is 
torch.Size([])
memory (bytes)
5584388096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5584388096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1918347800.0
relative error loss 0.30571073
shape of L is 
torch.Size([])
memory (bytes)
5587464192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5587464192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1918073900.0
relative error loss 0.30566707
shape of L is 
torch.Size([])
memory (bytes)
5590827008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5590827008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1917945900.0
relative error loss 0.3056467
shape of L is 
torch.Size([])
memory (bytes)
5593890816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5593890816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1917823000.0
relative error loss 0.3056271
shape of L is 
torch.Size([])
memory (bytes)
5597241344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5597241344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1917709800.0
relative error loss 0.30560908
shape of L is 
torch.Size([])
memory (bytes)
5600407552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5600407552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1917633500.0
relative error loss 0.30559692
shape of L is 
torch.Size([])
memory (bytes)
5603659776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5603659776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  1917521900.0
relative error loss 0.30557913
shape of L is 
torch.Size([])
memory (bytes)
5606875136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
5606875136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1917398000.0
relative error loss 0.30555937
shape of L is 
torch.Size([])
memory (bytes)
5610065920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5610065920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1917338100.0
relative error loss 0.30554983
time to take a step is 272.34097266197205
it  14 : 2566688768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5613293568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
5613293568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1917338100.0
relative error loss 0.30554983
shape of L is 
torch.Size([])
memory (bytes)
5616377856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5616500736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1917207600.0
relative error loss 0.30552903
shape of L is 
torch.Size([])
memory (bytes)
5619716096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5619716096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1917100500.0
relative error loss 0.30551198
shape of L is 
torch.Size([])
memory (bytes)
5622927360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5622927360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1916941300.0
relative error loss 0.3054866
shape of L is 
torch.Size([])
memory (bytes)
5626142720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5626142720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 11% |
error is  1916811800.0
relative error loss 0.30546597
shape of L is 
torch.Size([])
memory (bytes)
5629284352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5629349888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1916706800.0
relative error loss 0.30544922
shape of L is 
torch.Size([])
memory (bytes)
5632552960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
5632552960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1916458000.0
relative error loss 0.30540958
shape of L is 
torch.Size([])
memory (bytes)
5635776512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
5635776512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1916491300.0
relative error loss 0.3054149
shape of L is 
torch.Size([])
memory (bytes)
5638983680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5638983680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1916336600.0
relative error loss 0.30539024
shape of L is 
torch.Size([])
memory (bytes)
5642027008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5642194944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1916220900.0
relative error loss 0.3053718
time to take a step is 271.54828667640686
sum tnnu_Z after tensor(7558994.5000, device='cuda:0')
shape of features
(5132,)
shape of features
(5132,)
number of orig particles 20528
number of new particles after remove low mass 19852
tnuZ shape should be parts x labs
torch.Size([20528, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  2121524100.0
relative error without small mass is  0.3380892
nnu_Z shape should be number of particles by maxV
(20528, 702)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
shape of features
(20528,)
Wed Feb 1 04:18:25 EST 2023
