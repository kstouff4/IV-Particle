Wed Feb 1 16:28:07 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 18110440
numbers of Z: 18917
shape of features
(18917,)
shape of features
(18917,)
ZX	Vol	Parts	Cubes	Eps
Z	0.023937630052444786	18917	18.917	0.10816239513321126
X	0.020584857319425463	344	0.344	0.3911378157485003
X	0.02269231682700702	7651	7.651	0.14367687678885188
X	0.023832465411725706	4136	4.136	0.17927902450178804
X	0.02123170514846435	792	0.792	0.2992861118963313
X	0.021848440121630442	25659	25.659	0.09478222576882134
X	0.021790197600172805	16508	16.508	0.10969552292477792
X	0.02190342758871431	32559	32.559	0.08762210343538145
X	0.021838986914436306	21375	21.375	0.1007183930934513
X	0.021626644904409298	3586	3.586	0.18202300846371675
X	0.021805930877873417	15220	15.22	0.11273358711715081
X	0.021829079288196482	3031	3.031	0.19311521590187097
X	0.02161401822227226	44750	44.75	0.07845985319773964
X	0.02154393166647122	3039	3.039	0.19210179411978637
X	0.021857586276748053	110743	110.743	0.058223097402004625
X	0.021882115685945537	13441	13.441	0.11763933916570833
X	0.02182954346382114	20209	20.209	0.1026045475760937
X	0.021894048875202295	33579	33.579	0.08671337867061532
X	0.021855319225950735	37995	37.995	0.08316557001110728
X	0.02180876527822517	96823	96.823	0.06084396593177511
X	0.022111561090427025	86904	86.904	0.06336652329675745
X	0.021691050566492984	2601	2.601	0.2027900978919496
X	0.022070921078064177	138753	138.753	0.05418253979705842
X	0.02184281770673549	10009	10.009	0.12970976047344115
X	0.021865141460656115	5175	5.175	0.16166424527267498
X	0.020687340404209945	39453	39.453	0.08063833328182084
X	0.021858071932896347	45183	45.183	0.07850167604179563
X	0.021805679005580128	38063	38.063	0.08305303992997354
X	0.02170163750167286	11291	11.291	0.12433308199146374
X	0.02183051606089454	21744	21.744	0.10013245273657857
X	0.02196530968314717	891680	891.68	0.02909663514695115
X	0.021517075813962682	2643	2.643	0.20116945019848959
X	0.023373689422451	138949	138.949	0.055202313754651726
X	0.02185773088151355	5650	5.65	0.1569828498282788
X	0.021815036971252196	3778	3.778	0.1794036742302283
X	0.021831767357941737	2652	2.652	0.20191649166503897
X	0.021828024374839553	21852	21.852	0.09996341387660605
X	0.02192842793689241	41341	41.341	0.08094850644152665
X	0.02181366153444423	1185	1.185	0.26404021231668345
X	0.02179490689951343	3951	3.951	0.17669163772872523
X	0.021002211780618868	1198	1.198	0.2597782210670781
X	0.021608841896913096	3373	3.373	0.18572557728254277
X	0.021561444880324986	461	0.461	0.36029561652025194
X	0.02044046691323738	784	0.784	0.296522907434211
X	0.021473848509927555	1655	1.655	0.23498346074223392
X	0.020802666740606875	323	0.323	0.4008410241910176
X	0.021256622520895527	369	0.369	0.38620921455652996
X	0.021332376147093504	2645	2.645	0.2005416199650961
X	0.021527117333388104	2591	2.591	0.20253783110469004
X	0.020941359086123188	602	0.602	0.32643956529069795
X	0.02183792483142084	2154	2.154	0.21643220760712392
X	0.02140497572158183	6711	6.711	0.14720045991532274
X	0.02104465786539473	1718	1.718	0.2305187397890385
X	0.021458548007660918	1277	1.277	0.2561355214676553
X	0.021583686979977618	1116	1.116	0.2684234659550225
X	0.021442046798984975	5405	5.405	0.15830337732583066
X	0.021196215585139067	1457	1.457	0.24411788131860862
X	0.021706171410146188	1641	1.641	0.2364965896121774
X	0.021142506708907386	1743	1.743	0.22976631724916974
X	0.021747409940317385	1125	1.125	0.26838094197335166
X	0.02154592575520204	1010	1.01	0.2773413307476443
X	0.02187374810883551	3938	3.938	0.17709888691090764
X	0.021644901122438116	724	0.724	0.3103647190823365
X	0.022108789002423156	6041	6.041	0.15410570137973156
X	0.020677375432832722	2063	2.063	0.21560826015720802
X	0.021456450986365366	740	0.74	0.3072147451727524
X	0.021843042096313436	1440	1.44	0.247542749387731
X	0.021637029639782646	871	0.871	0.2917828945918448
X	0.021423788824683813	1847	1.847	0.22636526910895607
X	0.021708643917469437	1420	1.42	0.2481883833934153
X	0.020928121509018204	756	0.756	0.3025075030673321
X	0.02125914749277911	1511	1.511	0.24141289284851797
X	0.021773522491178407	1703	1.703	0.23383239727303984
X	0.021292837284925408	919	0.919	0.28508388900692694
X	0.02146807530827912	597	0.597	0.3300701569218762
X	0.020860596948108816	1112	1.112	0.26570964241745365
X	0.021524497260963147	2790	2.79	0.19759514569801276
X	0.021482726371548622	934	0.934	0.2843899032745152
X	0.021333848135672476	575	0.575	0.33352914092919095
X	0.020199789907076677	1726	1.726	0.22703981297823397
X	0.022404787560951163	5860	5.86	0.15636794839905976
X	0.02111249939477319	1637	1.637	0.2345112366793477
X	0.01933565234653	266	0.266	0.41734229257906896
X	0.021808818718849734	5425	5.425	0.15900500689661984
X	0.020268788841922493	344	0.344	0.38912558377932865
X	0.021286532610040702	926	0.926	0.28433564845562387
X	0.02139774455433449	1051	1.051	0.2730581189976966
X	0.02179341828289124	951	0.951	0.2840413790213772
X	0.020497598395792467	330	0.33	0.39603145375756466
X	0.02134138198862102	2363	2.363	0.20825060537490742
X	0.02177685276037042	2629	2.629	0.20233367802501437
X	0.021364663818146806	686	0.686	0.31462313141189013
X	0.020992673620838995	636	0.636	0.32077723975694233
X	0.021126128000780918	379	0.379	0.3819972463436499
X	0.020996594928201006	2285	2.285	0.2094534678043274
X	0.021043637000026087	1104	1.104	0.2671266653817693
X	0.02167616770323709	2677	2.677	0.20080662814684097
X	0.02192985095772093	3418	3.418	0.18581803959475576
X	0.021340967077068555	1688	1.688	0.23295956740829513
X	0.021383892134522565	1511	1.511	0.24188416061916893
X	0.021696794799903087	1637	1.637	0.23665497246961387
X	0.02154511002297798	6707	6.707	0.1475503135142803
X	0.02062641408784147	811	0.811	0.2940814465864362
X	0.021315372933442264	1730	1.73	0.23096647387994873
X	0.020757215649076982	998	0.998	0.2750085430982496
X	0.01983361884647147	1469	1.469	0.23811863758581084
X	0.021621221722267686	863	0.863	0.29261043621497873
X	0.021717393496104347	2564	2.564	0.20384335556258382
X	0.021013757970361542	328	0.328	0.40013815077102743
X	0.021815614510021317	1056	1.056	0.27438972486862234
X	0.021799514427349065	755	0.755	0.30678441237787674
X	0.020541738115710535	1046	1.046	0.26979578961848893
X	0.021472191378859473	690	0.69	0.31453991097122525
X	0.02139081520336611	799	0.799	0.2991531077323695
X	0.021878974679977823	1883	1.883	0.22649513701524362
X	0.021307289793494253	1913	1.913	0.22332520819347282
X	0.021400501827216773	407	0.407	0.3746362946414991
X	0.02144864709836833	1010	1.01	0.276923307434944
X	0.02157345311110175	458	0.458	0.36114759661853807
X	0.021882627941891372	4312	4.312	0.17184632198221345
X	0.019752579029742652	501	0.501	0.3403523033588561
X	0.021780510865034904	5876	5.876	0.15476120110116534
X	0.021491949405176528	1130	1.13	0.2669310667214056
X	0.02172154188180316	806	0.806	0.29981397290571465
X	0.02180024357634837	1836	1.836	0.22813726096200032
X	0.02166686598016318	829	0.829	0.29676580350686166
X	0.02129983581642129	692	0.692	0.31339354988689516
X	0.02174803191332799	1087	1.087	0.2714751858892479
X	0.021190325976491745	725	0.725	0.30803486642746236
X	0.02184674495032397	14560	14.56	0.11448320729256715
X	0.02153231663164239	1641	1.641	0.23586349276872012
X	0.021777877412889137	3387	3.387	0.18595168471344653
X	0.02170171073744112	622	0.622	0.32676424442779417
X	0.02146850873510154	1523	1.523	0.24156498964517792
X	0.02107774470843474	416	0.416	0.37003568087632793
X	0.02163967072227213	1294	1.294	0.25572437386302155
X	0.021561532028228712	808	0.808	0.29882900701836956
X	0.021273225492764787	400	0.4	0.3760605562359684
X	0.021009842668165985	1004	1.004	0.2755685784010529
X	0.021579494758699705	824	0.824	0.2969646034423079
X	0.021755273556483854	1316	1.316	0.2547433680322608
X	0.021314830448067916	2066	2.066	0.21769603631036255
X	0.020666499236785714	352	0.352	0.38866438928355024
X	0.021638950166403454	5271	5.271	0.16012078976461175
X	0.02143530982628307	4013	4.013	0.17480487297825908
X	0.021483890747924932	5750	5.75	0.15517259754803456
X	0.020697967320642344	862	0.862	0.2884962309469044
X	0.021640441009695064	1276	1.276	0.2569242776696367
X	0.02153397278785469	1701	1.701	0.2330629729964237
X	0.021397177419785817	737	0.737	0.3073474870915784
X	0.020649658007343565	994	0.994	0.2749004767373463
X	0.020609416097797942	1555	1.555	0.236652864015832
X	0.022494114140914782	4742	4.742	0.16802314714279123
X	0.023345292863980495	2258	2.258	0.21785045079976553
X	0.02167423156489597	1317	1.317	0.2543622422309027
X	0.02081504423321392	1224	1.224	0.25715717140665406
X	0.021778979727018657	6828	6.828	0.14720232607600828
X	0.021015599278096996	1221	1.221	0.2581915580593614
X	0.02173540879133881	960	0.96	0.2828995086786565
X	0.021310969515711705	1441	1.441	0.24545944185114138
X	0.02124937058251647	414	0.414	0.3716338317051562
X	0.021988811985823677	1778	1.778	0.23125439915691423
X	0.02015950748381387	515	0.515	0.3395399623872977
X	0.021575190645131023	2250	2.25	0.2124503126831991
X	0.020603011775731723	491	0.491	0.34749594376641624
X	0.02034556136272283	1178	1.178	0.2584886403924487
X	0.02117326882557701	838	0.838	0.2934368358413324
X	0.021604179163695388	1323	1.323	0.25370321767989595
X	0.02039544371320336	967	0.967	0.27629237185072797
X	0.021664711478840557	1782	1.782	0.22994028747549444
X	0.02089143787504852	267	0.267	0.4277129077288218
X	0.02010656779634072	692	0.692	0.30742837491807895
X	0.020396795425200776	697	0.697	0.3081600864766272
X	0.02169670444613349	3240	3.24	0.1884876591624113
X	0.021177136170161287	611	0.611	0.3260433446717256
X	0.021732760902982526	2338	2.338	0.21026006697266683
X	0.021775934114700517	1683	1.683	0.23476367136188805
X	0.021230430008922753	2194	2.194	0.2130953906318481
X	0.021950688079197846	1890	1.89	0.22646205542343645
X	0.021006351939364166	1405	1.405	0.24635314126987826
X	0.01967372216411037	1137	1.137	0.25864774103451016
X	0.021846409752087046	1841	1.841	0.22809132190877832
X	0.02075613990492872	962	0.962	0.27839227013085066
X	0.021548488960116696	953	0.953	0.2827751926896661
X	0.020900107696025787	1148	1.148	0.26306851826819955
X	0.021761362217489706	4347	4.347	0.1710667006332411
X	0.02186298244496223	3039	3.039	0.19304544942992308
X	0.02171461532649007	1398	1.398	0.24950638060855077
X	0.021567482239519587	1362	1.362	0.25111584325513964
X	0.02183036731547899	6304	6.304	0.1512916454257431
X	0.021761116104363352	6634	6.634	0.14858280960015438
X	0.021069618681895223	669	0.669	0.3157985895694524
X	0.021193308543397325	319	0.319	0.40501336926409415
X	0.02148213613314496	984	0.984	0.2794864701372102
X	0.02112017082415707	512	0.512	0.3455220930536216
X	0.021496154362888983	797	0.797	0.29989379745076566
X	0.021616937417779186	1864	1.864	0.22635117454679107
X	0.021794262515556544	763	0.763	0.30568388664823387
X	0.020668418927112343	410	0.41	0.3694092786480022
X	0.021413659353425582	1026	1.026	0.27532639158920713
X	0.021215316170982074	602	0.602	0.3278569082255736
X	0.021661569671102502	3964	3.964	0.1761376073873445
X	0.021491240917872332	684	0.684	0.31555020281335006
X	0.02183403014230497	3630	3.63	0.18186219428585126
X	0.022005565978236906	3869	3.869	0.17850262390722957
X	0.021671191385015377	1596	1.596	0.2385704410825651
X	0.018603617371624446	252	0.252	0.41950030985239595
X	0.02348819435382856	5249	5.249	0.16478753336621638
X	0.02099131459864335	1754	1.754	0.22873713799142403
X	0.021761145018041952	861	0.861	0.29346717441177145
X	0.023498529106380464	6408	6.408	0.15420769954131228
X	0.021251829537285553	3968	3.968	0.17496112132064467
X	0.021342675406467417	604	0.604	0.3281486638598897
X	0.02147231949419949	713	0.713	0.3111213468664494
X	0.019761018319537246	491	0.491	0.34269619009866287
X	0.021569267177947243	456	0.456	0.3616514264995221
X	0.020662321699742425	366	0.366	0.3836183248121945
X	0.02105503753086135	366	0.366	0.38603348557282224
X	0.020625802320281453	251	0.251	0.4347561256638263
X	0.021567706822422008	1920	1.92	0.22395851589261648
X	0.021437578961834024	1549	1.549	0.24009038779836955
X	0.020359866020971697	456	0.456	0.35476167034001926
X	0.021897798754237228	688	0.688	0.3169110195638719
X	0.02109669863650366	1412	1.412	0.24629746058039287
X	0.021083044707489486	636	0.636	0.3212368840650634
X	0.020895743134998878	2870	2.87	0.1938170480629868
X	0.020936443656990798	2004	2.004	0.21860918165684748
X	0.021397349810689442	1054	1.054	0.2727971268558965
X	0.020257435527989087	224	0.224	0.4488612856083524
X	0.021847786540128075	3178	3.178	0.19014482803000363
X	0.020277161772106946	408	0.408	0.36766221641326396
X	0.021471612622372	574	0.574	0.3344395191144628
X	0.021743756724899366	865	0.865	0.292936052127124
X	0.02088866633216662	1132	1.132	0.2642539140170157
X	0.02183280672880133	6256	6.256	0.1516832446515052
X	0.021502103189392944	1130	1.13	0.26697309693591564
X	0.021544752359649925	456	0.456	0.3615143614197011
X	0.02171010653207704	1063	1.063	0.2733440112869765
X	0.022313006229825124	4545	4.545	0.16995791698577614
X	0.02072273916080496	420	0.42	0.36677463201803057
X	0.02159395484529544	2607	2.607	0.20233160157548186
X	0.021902384037319988	11323	11.323	0.12459768140752032
X	0.021926894499919136	3432	3.432	0.18555668960321217
X	0.02186102227321977	2658	2.658	0.2018545283529211
X	0.020724556964557005	713	0.713	0.3074670458351859
X	0.021735520101815266	3302	3.302	0.18741211245557593
X	0.021929934943807534	18148	18.148	0.10651305899991088
X	0.021731587627128703	41770	41.77	0.08042831929438438
X	0.021703065899189977	1722	1.722	0.23271764532313363
X	0.021721331371862898	19833	19.833	0.10307800623995937
X	0.021604535601845463	44814	44.814	0.07841101491546734
X	0.0215937115079599	6552	6.552	0.14881651685497974
X	0.0234630075687836	239754	239.754	0.04608284393035368
X	0.020753296503270322	494	0.494	0.34763224038365975
X	0.021295067503602282	3611	3.611	0.18066908464432324
X	0.021711226857870997	37685	37.685	0.08320931533463831
X	0.0218668926299279	218109	218.109	0.046455573796625745
X	0.021527210115606685	1642	1.642	0.2357969585415542
X	0.021845884127334143	15988	15.988	0.11096648481893456
X	0.021956692287930867	18743	18.743	0.10541666439499855
X	0.021797816649832043	69931	69.931	0.06780281924135688
X	0.021453837422890053	33782	33.782	0.08595540861479475
X	0.021546044536652816	23275	23.275	0.09745989844915728
X	0.02183296247873665	9420	9.42	0.13233881916837353
X	0.0218838784945115	5375	5.375	0.15967929582313134
X	0.021513915443641295	11378	11.378	0.12365675417172081
X	0.0219466626803991	31826	31.826	0.08834774657203451
X	0.02083365601303829	1483	1.483	0.2412911793187239
X	0.021696213793054413	1300	1.3	0.2555525387821145
X	0.021487522104595085	36455	36.455	0.08384485370558
X	0.02152221763421866	12221	12.221	0.12076101599414145
X	0.021891202804343833	136253	136.253	0.05436355519056361
X	0.021872374868601285	47404	47.404	0.0772728595023352
X	0.02094478794026412	328	0.328	0.3996999015617241
X	0.021733433450556422	6176	6.176	0.15210390062007142
X	0.02167097678981552	3093	3.093	0.19135193384713559
X	0.021901042523652617	6634	6.634	0.1489005974812452
X	0.021924937185399347	36260	36.26	0.08456101476074937
X	0.021747302896475557	4275	4.275	0.1719846795083578
X	0.021708446314013113	21099	21.099	0.10095371144685186
X	0.020912699113479705	477	0.477	0.3526121206805964
X	0.021740170135671727	2917	2.917	0.1953329473512901
X	0.021854620709637486	12307	12.307	0.12109615608084259
X	0.02181529884313248	33763	33.763	0.08645166846346097
X	0.021239548722112013	12524	12.524	0.11925247703655714
X	0.02122034870396148	1684	1.684	0.23270380057025025
X	0.02192623666669519	140836	140.836	0.05379601903118326
X	0.021869195402172654	4237	4.237	0.17281898376769195
X	0.02180127944500958	11805	11.805	0.12268891621156526
X	0.021947586792536587	52095	52.095	0.07496586864003614
X	0.021129430487977975	1290	1.29	0.25396041439356287
X	0.02184458634008358	21371	21.371	0.10073328423362099
X	0.023030544001129347	66284	66.284	0.07030159244690957
X	0.021773879908963265	94597	94.597	0.06128480659722835
X	0.02187232993656398	6050	6.05	0.15347815080921323
X	0.021614140934016052	10807	10.807	0.12599237883259445
X	0.021544980727792148	3477	3.477	0.18367376980881084
X	0.021762044171280106	2315	2.315	0.21104880496870074
X	0.02190218050245718	13511	13.511	0.11747170889423034
X	0.02179250926495217	6067	6.067	0.1531479124886199
X	0.021907916546961833	15225	15.225	0.1128967029378508
X	0.02195070387963704	46549	46.549	0.07783579492518505
X	0.021905916739660664	13577	13.577	0.11728771845174141
X	0.021829192254098084	3784	3.784	0.1793475765837197
X	0.02189596992133852	6710	6.71	0.14832484023320805
X	0.02194152931751634	26856	26.856	0.09348498343410125
X	0.021885741573628777	6529	6.529	0.14965969852970762
X	0.021601351566344122	1465	1.465	0.24521541445593012
X	0.022973147737827923	9779	9.779	0.13293582029989243
X	0.021710293887907465	278697	278.697	0.042708187056191775
X	0.021287514576686824	4050	4.05	0.17386946256684355
X	0.02160751703754131	49359	49.359	0.07592990162925202
X	0.021622377714006623	2017	2.017	0.22049522240597882
X	0.022510002582464186	9584	9.584	0.13292589095777968
X	0.021892563165239902	9009	9.009	0.13444356133498345
X	0.021885319699326285	15972	15.972	0.11107027944916564
X	0.02179131098270722	24300	24.3	0.09643300382898629
X	0.021711054717157348	2294	2.294	0.21152538917990107
X	0.02187503037321541	59929	59.929	0.07146672452882315
X	0.02193864820306281	51741	51.741	0.07512624495722328
X	0.021873864025866864	2172	2.172	0.2159510047688558
X	0.021784197876461908	25214	25.214	0.09524304570320334
X	0.021952862578888066	57987	57.987	0.07234138835137834
X	0.023430156191755837	161474	161.474	0.05254816158472724
X	0.021856489780956022	16188	16.188	0.11052547910487456
X	0.021410913433425677	4600	4.6	0.16696513308169877
X	0.02133513296176289	3156	3.156	0.18908313156396597
X	0.021918195911810778	7271	7.271	0.1444564329331151
X	0.021555769577254347	26055	26.055	0.09387662874130238
X	0.02153769491849576	2856	2.856	0.19610123974056629
X	0.021572529281805666	1172	1.172	0.26403276180473756
X	0.021912323551776334	41048	41.048	0.08112078477518471
X	0.02184245694004232	58825	58.825	0.07187532985435491
X	0.020903068342902044	50527	50.527	0.0745124498361161
X	0.021314977286727124	2209	2.209	0.21289381906651325
X	0.021759187442447574	8582	8.582	0.13635928679711484
X	0.022949427877846876	13630	13.63	0.11896667832134716
X	0.02164638903980623	4928	4.928	0.16377147234727688
X	0.021751519745071595	4309	4.309	0.1715422243778352
X	0.0216823857170303	2358	2.358	0.2095017982564286
X	0.02325306157569625	101281	101.281	0.061232790795746186
X	0.0218967769910937	21414	21.414	0.10074591965333615
X	0.021904016255833447	5830	5.83	0.15545990552602384
X	0.02181564959080127	5081	5.081	0.16253228889327082
X	0.021013769633065633	1947	1.947	0.22099341773691714
X	0.02216230329695492	210355	210.355	0.047230297903375774
X	0.021804899827067157	3187	3.187	0.18984129049846887
X	0.021758309609207275	53417	53.417	0.07412793230012486
X	0.021685818577250257	873	0.873	0.291778883972601
X	0.02093198050993856	1361	1.361	0.24868565367872067
X	0.02189632041699432	2095	2.095	0.21863970738319344
X	0.021917954777650607	10498	10.498	0.1278098973469147
X	0.02180613212100254	9066	9.066	0.13398442232404426
X	0.02176339405491471	3820	3.82	0.17860259070892323
X	0.021099104131907097	2690	2.69	0.19868751521381062
X	0.021310544782663254	2283	2.283	0.21055370129275355
X	0.02188715588182362	146811	146.811	0.053024540217173656
X	0.02171842414483274	16689	16.689	0.10917737928897452
X	0.023288258467277827	9126	9.126	0.13665289898993707
X	0.02181681608116185	29433	29.433	0.09050077119305014
X	0.023008146084542734	29171	29.171	0.09239392914198388
X	0.021343436597092043	1762	1.762	0.22966041648709024
X	0.021649069375330406	2170	2.17	0.21527478596054425
X	0.02165916837469664	2956	2.956	0.19422827328108844
X	0.02079855052064812	258	0.258	0.43198744123179994
X	0.02159631340346508	1669	1.669	0.234769186383294
X	0.021663997360097224	11691	11.691	0.12282750908034264
X	0.02097534480472223	1716	1.716	0.23035480467608122
X	0.021211448063244064	1436	1.436	0.24536086422751205
X	0.020888708381480034	2127	2.127	0.21414833347021942
X	0.0217131869468897	2955	2.955	0.19441153476862094
X	0.02174275047578224	284542	284.542	0.04243485286577702
X	0.021522309076135105	5832	5.832	0.15453390631720032
X	0.021600787903790036	20566	20.566	0.10164981370914233
X	0.02191219163528217	13699	13.699	0.11694966548466447
X	0.021675146912165454	2081	2.081	0.2183886128872857
X	0.021762864903124512	5802	5.802	0.15537439379240367
X	0.021937523236619332	291943	291.943	0.042198450972623466
X	0.02196006783176301	322228	322.228	0.04084668352379898
X	0.021899869226617408	21376	21.376	0.10081032779532968
X	0.021827866586798583	11632	11.632	0.12334426623035055
X	0.021122887669737845	825	0.825	0.29473595143348275
X	0.02180900453059283	41336	41.336	0.08080454665700011
X	0.02173099963248709	21866	21.866	0.09979377548932415
X	0.02249676714193742	96603	96.603	0.06152380209535379
X	0.02177587886265895	3784	3.784	0.17920145081879457
X	0.021731528373092883	62858	62.858	0.07018482761124312
X	0.022357639383684895	79523	79.523	0.06551052258127797
X	0.021480751164871935	25895	25.895	0.0939603262978749
X	0.02079229835627488	2649	2.649	0.1987346129650727
X	0.0236409968442741	11932	11.932	0.12559842115729347
X	0.021738995105362344	3424	3.424	0.1851691229663417
X	0.021726910370241923	1502	1.502	0.24365538458055003
X	0.02230592537678522	52890	52.89	0.07499210760059104
X	0.021270544656646826	10530	10.53	0.12641057685147372
X	0.021247714462305757	574	0.574	0.33327298374773956
X	0.02186990891763402	9485	9.485	0.1321102597227586
X	0.021880422422142855	7786	7.786	0.14111735711441262
X	0.021510449777634156	451	0.451	0.36265275678400305
X	0.021857363746228084	28196	28.196	0.09186218248731948
X	0.021938364217927716	52955	52.955	0.07454738664675004
X	0.02187264184205224	11679	11.679	0.12326275238511647
X	0.021773957732566903	37381	37.381	0.0835145515524864
X	0.021722801999579607	32928	32.928	0.08705295081058018
X	0.021112518373018682	3358	3.358	0.18456660183478557
X	0.02184394380035785	13550	13.55	0.11725478781286539
X	0.021887483903321004	33343	33.343	0.08690879379959746
X	0.02184938798051053	71037	71.037	0.06750224570318925
X	0.021729582931769215	2572	2.572	0.2036698806173907
X	0.021940794185475773	40801	40.801	0.08131934003731264
X	0.022076686104935295	70115	70.115	0.0680311242738054
X	0.021892154260647317	13549	13.549	0.11734387319537652
X	0.021953964179991365	10962	10.962	0.1260494779065456
X	0.021532418353150253	4187	4.187	0.17260893888957915
X	0.02168556193733642	5381	5.381	0.15913629018232203
X	0.021208354836976515	2097	2.097	0.2162566727659163
X	0.02173095242838807	16901	16.901	0.10873986423664286
X	0.021892833570782408	81028	81.028	0.0646480781439224
X	0.021928256099409557	47651	47.651	0.07720475074261834
X	0.0218462194006587	261214	261.214	0.043731389883618146
X	0.021694286528091654	25935	25.935	0.09422215648767492
X	0.021843101333816593	49929	49.929	0.07591375563552195
X	0.021768555963016818	8862	8.862	0.13492713278572227
X	0.021912288647316206	57766	57.766	0.07238887291196175
X	0.021436468073018015	856	0.856	0.2925677750328841
X	0.021118826016072255	6032	6.032	0.15184581019823856
X	0.021808675341342742	97808	97.808	0.06063894492869796
X	0.021625658828365937	7800	7.8	0.14048336936019745
X	0.02122718403219863	1418	1.418	0.24645561553791182
X	0.021855773468918843	6078	6.078	0.15320343223809135
X	0.021649556372223704	15011	15.011	0.11298300605904696
X	0.02117539326950287	14472	14.472	0.1135274745554044
X	0.02209054478199424	37609	37.609	0.08374744123788333
X	0.02177084413190101	11340	11.34	0.12428557845751002
X	0.020975552356360595	15049	15.049	0.11170396474921777
X	0.021314061243855088	1299	1.299	0.2541083986945016
X	0.02184645725101925	4295	4.295	0.17197789030182822
X	0.021855747098981138	12864	12.864	0.11932456919510098
X	0.02174895922792159	17791	17.791	0.10692502812030483
X	0.021320389574627834	883	0.883	0.2890312564493169
X	0.02193425681495856	67051	67.051	0.06890317298347118
X	0.021074296771697375	728	0.728	0.30704868192557966
X	0.022321191948629977	8833	8.833	0.13620813627979567
X	0.021731380841769204	1645	1.645	0.23639619042980098
X	0.02181020195587736	20532	20.532	0.10203350615653897
X	0.0211655551285679	1610	1.61	0.23601227752171286
X	0.021718537684185624	15045	15.045	0.11301761275281387
X	0.02169156651530511	6147	6.147	0.15224483502808536
X	0.021692246653371258	10233	10.233	0.12845978375882466
X	0.02173075923428936	24465	24.465	0.09612652144187231
X	0.021842402676393622	112941	112.941	0.0578295207931246
X	0.021868081311484742	1948	1.948	0.2239102161364213
X	0.021578626778656157	8113	8.113	0.13855241792397888
X	0.02188119812247485	58995	58.995	0.0718486530811194
X	0.021364992379497814	3154	3.154	0.18921127741190397
X	0.022698955945994474	227785	227.785	0.04636179618809234
X	0.020973449710427573	811	0.811	0.29572157117275255
X	0.022047388553154138	56061	56.061	0.07326537416535116
X	0.022460258660103195	137310	137.31	0.054689530548102355
X	0.021404421647870837	1395	1.395	0.24849048477137317
X	0.021589963980197	97733	97.733	0.060451010173357723
X	0.021791744573421038	6926	6.926	0.1465333586924678
X	0.02282079364123509	85197	85.197	0.06446175825648282
X	0.02174343090953074	3531	3.531	0.18329197537793235
X	0.021986678992082346	36480	36.48	0.08446982647483624
X	0.021549884952744985	496	0.496	0.35155053227074545
X	0.02172078746952953	2818	2.818	0.19753536497315574
X	0.021341586042988227	1800	1.8	0.22802618896456303
X	0.021911461431945282	16927	16.927	0.10898426095682978
X	0.021632587967954905	8972	8.972	0.13409309181108703
X	0.021917783802143397	63594	63.594	0.07011218450005458
X	0.02183438923082591	7002	7.002	0.1460964446356114
X	0.021441078575978573	1259	1.259	0.2572805562899542
X	0.021497259173981268	2262	2.262	0.21181830931254358
X	0.022063194938895456	15323	15.323	0.11292103936023494
X	0.021532040400903087	1840	1.84	0.22703306956782116
X	0.02192954475796351	69172	69.172	0.06818671389503372
X	0.021825445213582605	51148	51.148	0.07528552530180861
X	0.02180800065275187	5021	5.021	0.16315806397937696
X	0.0215754906157627	1233	1.233	0.2596166457408432
X	0.020634860417970465	7822	7.822	0.13817450198691295
X	0.022509086841972337	493399	493.399	0.035731660910046606
X	0.021888986639538265	4717	4.717	0.16679636941765413
X	0.02171346452306254	7164	7.164	0.14471865428216857
X	0.021624628104269884	53450	53.45	0.0739605807390623
X	0.0214335122489912	14866	14.866	0.11297085223110709
X	0.021788384887387928	1294	1.294	0.2563088413277468
X	0.02027383234465819	1517	1.517	0.23731039196816447
X	0.02295425625757956	93863	93.863	0.06253496795638265
X	0.02190689865163946	11882	11.882	0.12262069792107948
X	0.021249376298156087	27712	27.712	0.09152915205959967
X	0.02188185860136215	27319	27.319	0.09286950621005913
X	0.021791058661016466	6998	6.998	0.1460275494598073
X	0.02179832350710094	20308	20.308	0.10238868837928541
X	0.021723046825214285	72085	72.085	0.06704379845569458
X	0.021921267922600803	41850	41.85	0.08061021309909261
X	0.021703195624437992	2642	2.642	0.20177326606985954
X	0.021895675048282468	25803	25.803	0.09467370622140756
X	0.021851828216726633	2988	2.988	0.19410455554527714
X	0.02160676408874678	5282	5.282	0.15993018677201265
X	0.02181725821773114	12357	12.357	0.12086365079267637
X	0.021857470200807545	2778	2.778	0.19889439804612583
X	0.021762463839651452	39303	39.303	0.0821159371635648
X	0.02155306643732298	7618	7.618	0.14143471937687144
X	0.020792383676303646	1213	1.213	0.2578385359648481
X	0.021606953256601627	8933	8.933	0.13423488635901645
X	0.021870865582695787	4404	4.404	0.17061068253179093
X	0.021763311149512216	40317	40.317	0.0814227164801998
X	0.021212528907797866	52348	52.348	0.07399984064091991
X	0.02184633364089844	4637	4.637	0.16764109492139567
X	0.020302437457709634	3854	3.854	0.17399849438315498
X	0.021843815310519453	11442	11.442	0.12405345116156122
X	0.021534812133018826	1426	1.426	0.24717650507729869
X	0.02183548171728746	2424	2.424	0.20807028113847967
X	0.021899861931578855	13876	13.876	0.11642842661513766
X	0.021828935310309373	87562	87.562	0.06293694049161708
X	0.02100340948026752	2115	2.115	0.214944568811428
X	0.021320715748083377	745	0.745	0.30587818353157
X	0.021659063814480885	1147	1.147	0.2662923892667127
X	0.021833009998594187	95766	95.766	0.0610896197503376
X	0.022498662175430822	174171	174.171	0.05055067391844932
X	0.021776623380292674	15091	15.091	0.11300322561719
X	0.021808657187063738	3195	3.195	0.18969360398969098
X	0.02184748522057604	18749	18.749	0.1052303742716239
X	0.023563474043978638	44699	44.699	0.08078185797871162
X	0.021533994414976866	1069	1.069	0.27209191064066013
X	0.0216657599294998	6867	6.867	0.1466680866286952
X	0.021846167993326535	11919	11.919	0.12238038536061566
X	0.022825408769250266	37490	37.49	0.08475546439269839
X	0.02177266688262792	18713	18.713	0.10517747411074461
X	0.021893753942295667	56505	56.505	0.07290284070437122
X	0.02148562411828859	5773	5.773	0.15497041833263475
X	0.021844475051541808	2516	2.516	0.20553083750849968
X	0.02154354408965421	1434	1.434	0.2467493398876348
X	0.021233800622913565	15954	15.954	0.10999831921740633
X	0.021878829232292576	7847	7.847	0.14074732255241976
X	0.021641398281176918	25679	25.679	0.09445734554689796
X	0.02177739940019777	25434	25.434	0.09495775747404059
X	0.021865008735028644	137249	137.249	0.054210093388044306
X	0.020907555864552292	5432	5.432	0.15671641977538078
X	0.020983898987795825	3033	3.033	0.19054808663853282
X	0.021853700112161152	16238	16.238	0.11040722152658786
X	0.02191819169130026	339752	339.752	0.0401064509804209
X	0.021790172927043116	15702	15.702	0.1115411799991522
X	0.02145168138679683	3811	3.811	0.17788560139763554
X	0.020471859297138607	631	0.631	0.3189402347758541
X	0.02176936652989946	22021	22.021	0.09961764062861912
X	0.02157124845637357	905	0.905	0.2877899125859883
X	0.0213560862218344	6989	6.989	0.14511162596418586
X	0.021694287678290552	2006	2.006	0.22114213020341397
X	0.021671321590032867	6617	6.617	0.1485051178424574
X	0.021168561382239166	5452	5.452	0.15717320133461746
X	0.021306714021050763	1839	1.839	0.22627934652086448
X	0.02151464845390683	776	0.776	0.3026617305706647
X	0.022071825509762354	105656	105.656	0.05933551230561175
X	0.021858058469714853	147796	147.796	0.052883026391664124
X	0.0219380967595717	54996	54.996	0.07361324254306438
X	0.021599393937525413	5423	5.423	0.15851388869557023
X	0.02188799373454916	13159	13.159	0.11848435888462858
X	0.02115590447010023	3792	3.792	0.1773594865637463
X	0.0212991539234291	4650	4.65	0.1660745686336352
X	0.022149430706073024	33661	33.661	0.08697848753777943
X	0.023862322023338576	23766	23.766	0.10013491568715142
X	0.02188502476335434	55193	55.193	0.07346621755770447
X	0.021894570717757224	3615	3.615	0.18228154919351303
X	0.0227123268288004	147335	147.335	0.05361898235893981
X	0.02181331217711723	12067	12.067	0.12181686932300786
X	0.02265461027757505	17318	17.318	0.1093669968277185
X	0.022466858186572396	88398	88.398	0.06334319544674907
X	0.022044994374795915	27197	27.197	0.09323872591521155
X	0.022152062299604855	102220	102.22	0.06006563820078553
X	0.02240729344204246	91753	91.753	0.06250619217641924
X	0.021879552526661177	27450	27.45	0.09271827910215436
X	0.021834312544472302	12862	12.862	0.11929173062028364
X	0.021571795737771172	10888	10.888	0.12559703751124873
X	0.021878348536383655	32864	32.864	0.08731684564091283
X	0.021911542685508042	3515	3.515	0.18404154705155482
X	0.02185835751612468	63339	63.339	0.07014263998087888
X	0.02324782876461168	376676	376.676	0.0395188971312388
X	0.02271521447445487	47435	47.435	0.07823588127005164
X	0.021637081623714062	29411	29.411	0.0902740577462913
X	0.02187125818636632	13915	13.915	0.11626888800754641
X	0.021900000120591206	3935	3.935	0.17721472025571772
X	0.023426534587588935	20838	20.838	0.103980205629751
X	0.021844349829376917	8540	8.54	0.13676041734236005
X	0.021906967283923925	37029	37.029	0.08394859370960184
X	0.023252027800174463	108293	108.293	0.059880692777810385
X	0.02113778439391455	1288	1.288	0.254125278101284
X	0.02191289672244764	43586	43.586	0.0795153410060281
X	0.0219793578089481	18221	18.221	0.10645047336161342
X	0.021726365354430812	15383	15.383	0.112197198455431
X	0.02162770310798126	1255	1.255	0.25829870241550207
X	0.021828861704880544	42462	42.462	0.08010818298023721
X	0.02186546128325528	55336	55.336	0.07338100021060119
X	0.021297410425868076	1567	1.567	0.23864514378511947
X	0.02205928192880264	312121	312.121	0.04134498073010857
X	0.02192617464668817	18679	18.679	0.10548800580792428
X	0.02169910324275524	9603	9.603	0.13122350060403645
X	0.02191175632854925	3236	3.236	0.18918624568247544
X	0.021628771514861152	2741	2.741	0.19908610001050867
X	0.021056718983952092	1202	1.202	0.2597140299511751
X	0.021360793866047466	3856	3.856	0.17694031167802424
X	0.021529456596396452	4616	4.616	0.16707921369891068
X	0.021635016048817686	41182	41.182	0.08068942038355414
X	0.0234913267011641	298711	298.711	0.04284354939694695
X	0.02186307255450153	5381	5.381	0.15956932257768974
X	0.021845754134066236	25939	25.939	0.09443607696582901
X	0.02153615724159521	1990	1.99	0.2211931698222536
X	0.021734242295278403	7587	7.587	0.14202282272946198
X	0.02156988547550299	4310	4.31	0.1710501750715028
X	0.021837221889142583	28802	28.802	0.09118531729670301
X	0.02180965269952705	6715	6.715	0.14809290224777283
X	0.0220693973602871	507610	507.61	0.03516306863358769
X	0.021846511583884766	2360	2.36	0.20996973436728517
X	0.022242088235224923	43252	43.252	0.08011671276871091
X	0.02188365308462141	6533	6.533	0.1496243882385205
X	0.021620073483342266	45672	45.672	0.07793556925345256
X	0.021751760623734327	144754	144.754	0.05316444452095687
X	0.021515590647838975	10365	10.365	0.12756395886069188
X	0.021757669605021217	2580	2.58	0.20354677410266783
X	0.021810365697028326	39752	39.752	0.0818655758913127
X	0.021834855091973172	9793	9.793	0.13064060606876834
X	0.021195984032261292	14701	14.701	0.11297150177135984
X	0.02175380875328465	56832	56.832	0.07260738385353828
X	0.02192201174582532	12311	12.311	0.12120736918350984
X	0.023937806783616268	40501	40.501	0.0839214289856721
X	0.021263954105798403	10333	10.333	0.12719573003879056
X	0.023209470663922576	32687	32.687	0.08921339312205055
X	0.021815569766116802	106448	106.448	0.05895803736521422
X	0.021860776180491755	10899	10.899	0.12611293752100505
X	0.021826053269123593	2526	2.526	0.20520154273093102
X	0.021573603063091824	61227	61.227	0.07063065511821016
X	0.021519986440313926	21359	21.359	0.10025061046033296
X	0.022772824712661274	32153	32.153	0.0891384615459703
X	0.021450880876576633	14968	14.968	0.11274408998338036
X	0.021440351679625196	26708	26.708	0.09293875399380022
X	0.021606625692091753	11800	11.8	0.12233995423290553
X	0.021486797706465135	35963	35.963	0.08422452958478036
X	0.02178978111867844	11895	11.895	0.12235718413039356
X	0.02170199687416413	9514	9.514	0.1316372648077707
X	0.021027882391941845	1492	1.492	0.241551036509098
X	0.023201114419290066	10398	10.398	0.1306731451703382
X	0.02135218776182765	10127	10.127	0.12822927592595323
X	0.021651614016321048	32826	32.826	0.08704772735986968
X	0.021919818413747544	54663	54.663	0.0737419294031292
X	0.021836469022035527	3108	3.108	0.1915286736916382
X	0.02187046040981817	12670	12.67	0.11995741643225796
X	0.021762452946757896	29215	29.215	0.09064989766594996
X	0.021924367402354112	124846	124.846	0.05599949960553981
X	0.021845565876701844	29464	29.464	0.09050874250947197
X	0.021941019219609257	32406	32.406	0.08780996217254229
X	0.021708475055966098	24772	24.772	0.09569503710709153
X	0.02194573417412364	13402	13.402	0.11786734479078466
X	0.021812159718923484	824	0.824	0.2980280577310386
X	0.021679733874938507	6387	6.387	0.1502861677491293
X	0.021860078374537562	52910	52.91	0.07447971127085178
X	0.02173689140144793	26033	26.033	0.09416533860794027
X	0.021886623011711237	30028	30.028	0.08999480703206438
X	0.021738369557850003	837	0.837	0.2961423396689225
X	0.021832327838255037	3444	3.444	0.18507409474574404
X	0.021653923120743594	89934	89.934	0.06221153160708154
X	0.021804348954915828	30128	30.128	0.08978234416341217
X	0.021814454403238096	15856	15.856	0.1112201746598632
X	0.02170364026791084	1284	1.284	0.2566389203537975
X	0.021251315267938452	794	0.794	0.29912664772766945
X	0.02182989074736677	17098	17.098	0.10848476431594546
X	0.021760043079922197	25759	25.759	0.09453158010758916
X	0.02178205894490784	33694	33.694	0.08646668036116237
X	0.021886797395758363	1724	1.724	0.23328222671883653
X	0.021365808435346224	48386	48.386	0.07614940887214368
X	0.021766452418089038	7078	7.078	0.14542052152428228
X	0.021571007257103685	4870	4.87	0.16422798131162233
X	0.022323714289659513	180688	180.688	0.04980569999239565
X	0.02178816698524743	7337	7.337	0.1437366080489405
X	0.021854306437140647	42021	42.021	0.08041866914842727
X	0.02188624063607116	63325	63.325	0.07017762350037796
X	0.021835189638053892	29478	29.478	0.09048008179622902
X	0.02211587432955378	33519	33.519	0.08705713152143285
X	0.022165326299237773	70173	70.173	0.06810327991790438
X	0.01934872988562661	1119	1.119	0.25858745888999835
X	0.02183370051717331	18587	18.587	0.10551301102713911
X	0.021822441744307895	9196	9.196	0.1333833006423315
X	0.02178752639138212	80768	80.768	0.06461344035720104
X	0.021925048454988903	70882	70.882	0.06762929595034621
X	0.021506721603044582	17880	17.88	0.10634952788695624
X	0.021912738542725792	26401	26.401	0.09397783300450718
X	0.02152663032678238	6524	6.524	0.1488746317187577
X	0.021116594535692743	2711	2.711	0.19822790350438393
X	0.021866289733844118	6507	6.507	0.14978377222382985
X	0.02134341680032354	6292	6.292	0.15025366604752668
X	0.023181517249055213	104851	104.851	0.060467650407899004
X	0.02286126447131394	59725	59.725	0.07260747987452265
X	0.02161135612198853	2131	2.131	0.21645428619299203
X	0.02193656465711791	7519	7.519	0.14289033153996825
X	0.021847746281641944	5795	5.795	0.15563875011415168
X	0.021706894014270037	211780	211.78	0.046799108558858854
X	0.021722457649614484	6469	6.469	0.1497467030422522
X	0.021600621000903035	8753	8.753	0.13513559852406415
X	0.02185632992627883	40040	40.04	0.08172615299578033
X	0.02194299355595341	39707	39.707	0.08206215795295721
X	0.02150760310265971	7015	7.015	0.1452740729464534
X	0.02182214800910758	17026	17.026	0.10862462479249851
X	0.021826193311253808	6406	6.406	0.1504747698262368
time for making epsilon is 0.9780421257019043
epsilons are
[0.3911378157485003, 0.14367687678885188, 0.17927902450178804, 0.2992861118963313, 0.09478222576882134, 0.10969552292477792, 0.08762210343538145, 0.1007183930934513, 0.18202300846371675, 0.11273358711715081, 0.19311521590187097, 0.07845985319773964, 0.19210179411978637, 0.058223097402004625, 0.11763933916570833, 0.1026045475760937, 0.08671337867061532, 0.08316557001110728, 0.06084396593177511, 0.06336652329675745, 0.2027900978919496, 0.05418253979705842, 0.12970976047344115, 0.16166424527267498, 0.08063833328182084, 0.07850167604179563, 0.08305303992997354, 0.12433308199146374, 0.10013245273657857, 0.02909663514695115, 0.20116945019848959, 0.055202313754651726, 0.1569828498282788, 0.1794036742302283, 0.20191649166503897, 0.09996341387660605, 0.08094850644152665, 0.26404021231668345, 0.17669163772872523, 0.2597782210670781, 0.18572557728254277, 0.36029561652025194, 0.296522907434211, 0.23498346074223392, 0.4008410241910176, 0.38620921455652996, 0.2005416199650961, 0.20253783110469004, 0.32643956529069795, 0.21643220760712392, 0.14720045991532274, 0.2305187397890385, 0.2561355214676553, 0.2684234659550225, 0.15830337732583066, 0.24411788131860862, 0.2364965896121774, 0.22976631724916974, 0.26838094197335166, 0.2773413307476443, 0.17709888691090764, 0.3103647190823365, 0.15410570137973156, 0.21560826015720802, 0.3072147451727524, 0.247542749387731, 0.2917828945918448, 0.22636526910895607, 0.2481883833934153, 0.3025075030673321, 0.24141289284851797, 0.23383239727303984, 0.28508388900692694, 0.3300701569218762, 0.26570964241745365, 0.19759514569801276, 0.2843899032745152, 0.33352914092919095, 0.22703981297823397, 0.15636794839905976, 0.2345112366793477, 0.41734229257906896, 0.15900500689661984, 0.38912558377932865, 0.28433564845562387, 0.2730581189976966, 0.2840413790213772, 0.39603145375756466, 0.20825060537490742, 0.20233367802501437, 0.31462313141189013, 0.32077723975694233, 0.3819972463436499, 0.2094534678043274, 0.2671266653817693, 0.20080662814684097, 0.18581803959475576, 0.23295956740829513, 0.24188416061916893, 0.23665497246961387, 0.1475503135142803, 0.2940814465864362, 0.23096647387994873, 0.2750085430982496, 0.23811863758581084, 0.29261043621497873, 0.20384335556258382, 0.40013815077102743, 0.27438972486862234, 0.30678441237787674, 0.26979578961848893, 0.31453991097122525, 0.2991531077323695, 0.22649513701524362, 0.22332520819347282, 0.3746362946414991, 0.276923307434944, 0.36114759661853807, 0.17184632198221345, 0.3403523033588561, 0.15476120110116534, 0.2669310667214056, 0.29981397290571465, 0.22813726096200032, 0.29676580350686166, 0.31339354988689516, 0.2714751858892479, 0.30803486642746236, 0.11448320729256715, 0.23586349276872012, 0.18595168471344653, 0.32676424442779417, 0.24156498964517792, 0.37003568087632793, 0.25572437386302155, 0.29882900701836956, 0.3760605562359684, 0.2755685784010529, 0.2969646034423079, 0.2547433680322608, 0.21769603631036255, 0.38866438928355024, 0.16012078976461175, 0.17480487297825908, 0.15517259754803456, 0.2884962309469044, 0.2569242776696367, 0.2330629729964237, 0.3073474870915784, 0.2749004767373463, 0.236652864015832, 0.16802314714279123, 0.21785045079976553, 0.2543622422309027, 0.25715717140665406, 0.14720232607600828, 0.2581915580593614, 0.2828995086786565, 0.24545944185114138, 0.3716338317051562, 0.23125439915691423, 0.3395399623872977, 0.2124503126831991, 0.34749594376641624, 0.2584886403924487, 0.2934368358413324, 0.25370321767989595, 0.27629237185072797, 0.22994028747549444, 0.4277129077288218, 0.30742837491807895, 0.3081600864766272, 0.1884876591624113, 0.3260433446717256, 0.21026006697266683, 0.23476367136188805, 0.2130953906318481, 0.22646205542343645, 0.24635314126987826, 0.25864774103451016, 0.22809132190877832, 0.27839227013085066, 0.2827751926896661, 0.26306851826819955, 0.1710667006332411, 0.19304544942992308, 0.24950638060855077, 0.25111584325513964, 0.1512916454257431, 0.14858280960015438, 0.3157985895694524, 0.40501336926409415, 0.2794864701372102, 0.3455220930536216, 0.29989379745076566, 0.22635117454679107, 0.30568388664823387, 0.3694092786480022, 0.27532639158920713, 0.3278569082255736, 0.1761376073873445, 0.31555020281335006, 0.18186219428585126, 0.17850262390722957, 0.2385704410825651, 0.41950030985239595, 0.16478753336621638, 0.22873713799142403, 0.29346717441177145, 0.15420769954131228, 0.17496112132064467, 0.3281486638598897, 0.3111213468664494, 0.34269619009866287, 0.3616514264995221, 0.3836183248121945, 0.38603348557282224, 0.4347561256638263, 0.22395851589261648, 0.24009038779836955, 0.35476167034001926, 0.3169110195638719, 0.24629746058039287, 0.3212368840650634, 0.1938170480629868, 0.21860918165684748, 0.2727971268558965, 0.4488612856083524, 0.19014482803000363, 0.36766221641326396, 0.3344395191144628, 0.292936052127124, 0.2642539140170157, 0.1516832446515052, 0.26697309693591564, 0.3615143614197011, 0.2733440112869765, 0.16995791698577614, 0.36677463201803057, 0.20233160157548186, 0.12459768140752032, 0.18555668960321217, 0.2018545283529211, 0.3074670458351859, 0.18741211245557593, 0.10651305899991088, 0.08042831929438438, 0.23271764532313363, 0.10307800623995937, 0.07841101491546734, 0.14881651685497974, 0.04608284393035368, 0.34763224038365975, 0.18066908464432324, 0.08320931533463831, 0.046455573796625745, 0.2357969585415542, 0.11096648481893456, 0.10541666439499855, 0.06780281924135688, 0.08595540861479475, 0.09745989844915728, 0.13233881916837353, 0.15967929582313134, 0.12365675417172081, 0.08834774657203451, 0.2412911793187239, 0.2555525387821145, 0.08384485370558, 0.12076101599414145, 0.05436355519056361, 0.0772728595023352, 0.3996999015617241, 0.15210390062007142, 0.19135193384713559, 0.1489005974812452, 0.08456101476074937, 0.1719846795083578, 0.10095371144685186, 0.3526121206805964, 0.1953329473512901, 0.12109615608084259, 0.08645166846346097, 0.11925247703655714, 0.23270380057025025, 0.05379601903118326, 0.17281898376769195, 0.12268891621156526, 0.07496586864003614, 0.25396041439356287, 0.10073328423362099, 0.07030159244690957, 0.06128480659722835, 0.15347815080921323, 0.12599237883259445, 0.18367376980881084, 0.21104880496870074, 0.11747170889423034, 0.1531479124886199, 0.1128967029378508, 0.07783579492518505, 0.11728771845174141, 0.1793475765837197, 0.14832484023320805, 0.09348498343410125, 0.14965969852970762, 0.24521541445593012, 0.13293582029989243, 0.042708187056191775, 0.17386946256684355, 0.07592990162925202, 0.22049522240597882, 0.13292589095777968, 0.13444356133498345, 0.11107027944916564, 0.09643300382898629, 0.21152538917990107, 0.07146672452882315, 0.07512624495722328, 0.2159510047688558, 0.09524304570320334, 0.07234138835137834, 0.05254816158472724, 0.11052547910487456, 0.16696513308169877, 0.18908313156396597, 0.1444564329331151, 0.09387662874130238, 0.19610123974056629, 0.26403276180473756, 0.08112078477518471, 0.07187532985435491, 0.0745124498361161, 0.21289381906651325, 0.13635928679711484, 0.11896667832134716, 0.16377147234727688, 0.1715422243778352, 0.2095017982564286, 0.061232790795746186, 0.10074591965333615, 0.15545990552602384, 0.16253228889327082, 0.22099341773691714, 0.047230297903375774, 0.18984129049846887, 0.07412793230012486, 0.291778883972601, 0.24868565367872067, 0.21863970738319344, 0.1278098973469147, 0.13398442232404426, 0.17860259070892323, 0.19868751521381062, 0.21055370129275355, 0.053024540217173656, 0.10917737928897452, 0.13665289898993707, 0.09050077119305014, 0.09239392914198388, 0.22966041648709024, 0.21527478596054425, 0.19422827328108844, 0.43198744123179994, 0.234769186383294, 0.12282750908034264, 0.23035480467608122, 0.24536086422751205, 0.21414833347021942, 0.19441153476862094, 0.04243485286577702, 0.15453390631720032, 0.10164981370914233, 0.11694966548466447, 0.2183886128872857, 0.15537439379240367, 0.042198450972623466, 0.04084668352379898, 0.10081032779532968, 0.12334426623035055, 0.29473595143348275, 0.08080454665700011, 0.09979377548932415, 0.06152380209535379, 0.17920145081879457, 0.07018482761124312, 0.06551052258127797, 0.0939603262978749, 0.1987346129650727, 0.12559842115729347, 0.1851691229663417, 0.24365538458055003, 0.07499210760059104, 0.12641057685147372, 0.33327298374773956, 0.1321102597227586, 0.14111735711441262, 0.36265275678400305, 0.09186218248731948, 0.07454738664675004, 0.12326275238511647, 0.0835145515524864, 0.08705295081058018, 0.18456660183478557, 0.11725478781286539, 0.08690879379959746, 0.06750224570318925, 0.2036698806173907, 0.08131934003731264, 0.0680311242738054, 0.11734387319537652, 0.1260494779065456, 0.17260893888957915, 0.15913629018232203, 0.2162566727659163, 0.10873986423664286, 0.0646480781439224, 0.07720475074261834, 0.043731389883618146, 0.09422215648767492, 0.07591375563552195, 0.13492713278572227, 0.07238887291196175, 0.2925677750328841, 0.15184581019823856, 0.06063894492869796, 0.14048336936019745, 0.24645561553791182, 0.15320343223809135, 0.11298300605904696, 0.1135274745554044, 0.08374744123788333, 0.12428557845751002, 0.11170396474921777, 0.2541083986945016, 0.17197789030182822, 0.11932456919510098, 0.10692502812030483, 0.2890312564493169, 0.06890317298347118, 0.30704868192557966, 0.13620813627979567, 0.23639619042980098, 0.10203350615653897, 0.23601227752171286, 0.11301761275281387, 0.15224483502808536, 0.12845978375882466, 0.09612652144187231, 0.0578295207931246, 0.2239102161364213, 0.13855241792397888, 0.0718486530811194, 0.18921127741190397, 0.04636179618809234, 0.29572157117275255, 0.07326537416535116, 0.054689530548102355, 0.24849048477137317, 0.060451010173357723, 0.1465333586924678, 0.06446175825648282, 0.18329197537793235, 0.08446982647483624, 0.35155053227074545, 0.19753536497315574, 0.22802618896456303, 0.10898426095682978, 0.13409309181108703, 0.07011218450005458, 0.1460964446356114, 0.2572805562899542, 0.21181830931254358, 0.11292103936023494, 0.22703306956782116, 0.06818671389503372, 0.07528552530180861, 0.16315806397937696, 0.2596166457408432, 0.13817450198691295, 0.035731660910046606, 0.16679636941765413, 0.14471865428216857, 0.0739605807390623, 0.11297085223110709, 0.2563088413277468, 0.23731039196816447, 0.06253496795638265, 0.12262069792107948, 0.09152915205959967, 0.09286950621005913, 0.1460275494598073, 0.10238868837928541, 0.06704379845569458, 0.08061021309909261, 0.20177326606985954, 0.09467370622140756, 0.19410455554527714, 0.15993018677201265, 0.12086365079267637, 0.19889439804612583, 0.0821159371635648, 0.14143471937687144, 0.2578385359648481, 0.13423488635901645, 0.17061068253179093, 0.0814227164801998, 0.07399984064091991, 0.16764109492139567, 0.17399849438315498, 0.12405345116156122, 0.24717650507729869, 0.20807028113847967, 0.11642842661513766, 0.06293694049161708, 0.214944568811428, 0.30587818353157, 0.2662923892667127, 0.0610896197503376, 0.05055067391844932, 0.11300322561719, 0.18969360398969098, 0.1052303742716239, 0.08078185797871162, 0.27209191064066013, 0.1466680866286952, 0.12238038536061566, 0.08475546439269839, 0.10517747411074461, 0.07290284070437122, 0.15497041833263475, 0.20553083750849968, 0.2467493398876348, 0.10999831921740633, 0.14074732255241976, 0.09445734554689796, 0.09495775747404059, 0.054210093388044306, 0.15671641977538078, 0.19054808663853282, 0.11040722152658786, 0.0401064509804209, 0.1115411799991522, 0.17788560139763554, 0.3189402347758541, 0.09961764062861912, 0.2877899125859883, 0.14511162596418586, 0.22114213020341397, 0.1485051178424574, 0.15717320133461746, 0.22627934652086448, 0.3026617305706647, 0.05933551230561175, 0.052883026391664124, 0.07361324254306438, 0.15851388869557023, 0.11848435888462858, 0.1773594865637463, 0.1660745686336352, 0.08697848753777943, 0.10013491568715142, 0.07346621755770447, 0.18228154919351303, 0.05361898235893981, 0.12181686932300786, 0.1093669968277185, 0.06334319544674907, 0.09323872591521155, 0.06006563820078553, 0.06250619217641924, 0.09271827910215436, 0.11929173062028364, 0.12559703751124873, 0.08731684564091283, 0.18404154705155482, 0.07014263998087888, 0.0395188971312388, 0.07823588127005164, 0.0902740577462913, 0.11626888800754641, 0.17721472025571772, 0.103980205629751, 0.13676041734236005, 0.08394859370960184, 0.059880692777810385, 0.254125278101284, 0.0795153410060281, 0.10645047336161342, 0.112197198455431, 0.25829870241550207, 0.08010818298023721, 0.07338100021060119, 0.23864514378511947, 0.04134498073010857, 0.10548800580792428, 0.13122350060403645, 0.18918624568247544, 0.19908610001050867, 0.2597140299511751, 0.17694031167802424, 0.16707921369891068, 0.08068942038355414, 0.04284354939694695, 0.15956932257768974, 0.09443607696582901, 0.2211931698222536, 0.14202282272946198, 0.1710501750715028, 0.09118531729670301, 0.14809290224777283, 0.03516306863358769, 0.20996973436728517, 0.08011671276871091, 0.1496243882385205, 0.07793556925345256, 0.05316444452095687, 0.12756395886069188, 0.20354677410266783, 0.0818655758913127, 0.13064060606876834, 0.11297150177135984, 0.07260738385353828, 0.12120736918350984, 0.0839214289856721, 0.12719573003879056, 0.08921339312205055, 0.05895803736521422, 0.12611293752100505, 0.20520154273093102, 0.07063065511821016, 0.10025061046033296, 0.0891384615459703, 0.11274408998338036, 0.09293875399380022, 0.12233995423290553, 0.08422452958478036, 0.12235718413039356, 0.1316372648077707, 0.241551036509098, 0.1306731451703382, 0.12822927592595323, 0.08704772735986968, 0.0737419294031292, 0.1915286736916382, 0.11995741643225796, 0.09064989766594996, 0.05599949960553981, 0.09050874250947197, 0.08780996217254229, 0.09569503710709153, 0.11786734479078466, 0.2980280577310386, 0.1502861677491293, 0.07447971127085178, 0.09416533860794027, 0.08999480703206438, 0.2961423396689225, 0.18507409474574404, 0.06221153160708154, 0.08978234416341217, 0.1112201746598632, 0.2566389203537975, 0.29912664772766945, 0.10848476431594546, 0.09453158010758916, 0.08646668036116237, 0.23328222671883653, 0.07614940887214368, 0.14542052152428228, 0.16422798131162233, 0.04980569999239565, 0.1437366080489405, 0.08041866914842727, 0.07017762350037796, 0.09048008179622902, 0.08705713152143285, 0.06810327991790438, 0.25858745888999835, 0.10551301102713911, 0.1333833006423315, 0.06461344035720104, 0.06762929595034621, 0.10634952788695624, 0.09397783300450718, 0.1488746317187577, 0.19822790350438393, 0.14978377222382985, 0.15025366604752668, 0.060467650407899004, 0.07260747987452265, 0.21645428619299203, 0.14289033153996825, 0.15563875011415168, 0.046799108558858854, 0.1497467030422522, 0.13513559852406415, 0.08172615299578033, 0.08206215795295721, 0.1452740729464534, 0.10862462479249851, 0.1504747698262368]
0.10816239513321126
Making ranges
torch.Size([21567, 2])
We keep 6.78e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([833, 2])
We keep 1.58e+04/1.18e+05 = 13% of the original kernel matrix.

torch.Size([5609, 2])
We keep 4.32e+05/6.51e+06 =  6% of the original kernel matrix.

torch.Size([10374, 2])
We keep 2.44e+06/5.85e+07 =  4% of the original kernel matrix.

torch.Size([14771, 2])
We keep 3.66e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([6280, 2])
We keep 8.15e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([11856, 2])
We keep 2.31e+06/7.82e+07 =  2% of the original kernel matrix.

torch.Size([1611, 2])
We keep 5.68e+04/6.27e+05 =  9% of the original kernel matrix.

torch.Size([7033, 2])
We keep 7.25e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([25925, 2])
We keep 2.09e+07/6.58e+08 =  3% of the original kernel matrix.

torch.Size([24145, 2])
We keep 9.53e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([18877, 2])
We keep 7.44e+06/2.73e+08 =  2% of the original kernel matrix.

torch.Size([20670, 2])
We keep 6.59e+06/3.12e+08 =  2% of the original kernel matrix.

torch.Size([35156, 2])
We keep 1.98e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([28372, 2])
We keep 1.13e+07/6.16e+08 =  1% of the original kernel matrix.

torch.Size([23651, 2])
We keep 8.92e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([23319, 2])
We keep 7.95e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([5703, 2])
We keep 1.17e+06/1.29e+07 =  9% of the original kernel matrix.

torch.Size([11416, 2])
We keep 1.97e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([13644, 2])
We keep 2.03e+07/2.32e+08 =  8% of the original kernel matrix.

torch.Size([17087, 2])
We keep 5.92e+06/2.88e+08 =  2% of the original kernel matrix.

torch.Size([4857, 2])
We keep 5.14e+05/9.19e+06 =  5% of the original kernel matrix.

torch.Size([10602, 2])
We keep 1.80e+06/5.73e+07 =  3% of the original kernel matrix.

torch.Size([45771, 2])
We keep 4.16e+07/2.00e+09 =  2% of the original kernel matrix.

torch.Size([31826, 2])
We keep 1.44e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([5147, 2])
We keep 4.72e+05/9.24e+06 =  5% of the original kernel matrix.

torch.Size([10825, 2])
We keep 1.80e+06/5.75e+07 =  3% of the original kernel matrix.

torch.Size([116434, 2])
We keep 1.90e+08/1.23e+10 =  1% of the original kernel matrix.

torch.Size([51261, 2])
We keep 3.18e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([15878, 2])
We keep 4.86e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([18629, 2])
We keep 5.44e+06/2.54e+08 =  2% of the original kernel matrix.

torch.Size([22045, 2])
We keep 9.40e+06/4.08e+08 =  2% of the original kernel matrix.

torch.Size([22402, 2])
We keep 7.67e+06/3.82e+08 =  2% of the original kernel matrix.

torch.Size([35529, 2])
We keep 2.01e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([28667, 2])
We keep 1.14e+07/6.35e+08 =  1% of the original kernel matrix.

torch.Size([39825, 2])
We keep 3.07e+07/1.44e+09 =  2% of the original kernel matrix.

torch.Size([29811, 2])
We keep 1.29e+07/7.19e+08 =  1% of the original kernel matrix.

torch.Size([104928, 2])
We keep 1.52e+08/9.37e+09 =  1% of the original kernel matrix.

torch.Size([48303, 2])
We keep 2.84e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([94151, 2])
We keep 1.30e+08/7.55e+09 =  1% of the original kernel matrix.

torch.Size([45488, 2])
We keep 2.60e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([3813, 2])
We keep 7.07e+05/6.77e+06 = 10% of the original kernel matrix.

torch.Size([9420, 2])
We keep 1.62e+06/4.92e+07 =  3% of the original kernel matrix.

torch.Size([146994, 2])
We keep 2.24e+08/1.93e+10 =  1% of the original kernel matrix.

torch.Size([57785, 2])
We keep 3.75e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([13219, 2])
We keep 3.48e+06/1.00e+08 =  3% of the original kernel matrix.

torch.Size([16803, 2])
We keep 4.44e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([7669, 2])
We keep 1.09e+06/2.68e+07 =  4% of the original kernel matrix.

torch.Size([12790, 2])
We keep 2.67e+06/9.79e+07 =  2% of the original kernel matrix.

torch.Size([29284, 2])
We keep 6.15e+07/1.56e+09 =  3% of the original kernel matrix.

torch.Size([23070, 2])
We keep 1.35e+07/7.46e+08 =  1% of the original kernel matrix.

torch.Size([47351, 2])
We keep 3.14e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([32670, 2])
We keep 1.47e+07/8.55e+08 =  1% of the original kernel matrix.

torch.Size([34923, 2])
We keep 4.15e+07/1.45e+09 =  2% of the original kernel matrix.

torch.Size([27374, 2])
We keep 1.29e+07/7.20e+08 =  1% of the original kernel matrix.

torch.Size([13275, 2])
We keep 9.37e+06/1.27e+08 =  7% of the original kernel matrix.

torch.Size([16866, 2])
We keep 5.06e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([23291, 2])
We keep 1.67e+07/4.73e+08 =  3% of the original kernel matrix.

torch.Size([22627, 2])
We keep 8.26e+06/4.11e+08 =  2% of the original kernel matrix.

torch.Size([1058738, 2])
We keep 5.38e+09/7.95e+11 =  0% of the original kernel matrix.

torch.Size([165450, 2])
We keep 2.06e+08/1.69e+10 =  1% of the original kernel matrix.

torch.Size([4237, 2])
We keep 5.51e+05/6.99e+06 =  7% of the original kernel matrix.

torch.Size([9939, 2])
We keep 1.63e+06/5.00e+07 =  3% of the original kernel matrix.

torch.Size([126520, 2])
We keep 5.23e+08/1.93e+10 =  2% of the original kernel matrix.

torch.Size([54226, 2])
We keep 3.84e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([7939, 2])
We keep 1.81e+06/3.19e+07 =  5% of the original kernel matrix.

torch.Size([13120, 2])
We keep 2.90e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([5888, 2])
We keep 6.53e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([11424, 2])
We keep 2.10e+06/7.15e+07 =  2% of the original kernel matrix.

torch.Size([4169, 2])
We keep 5.35e+05/7.03e+06 =  7% of the original kernel matrix.

torch.Size([9812, 2])
We keep 1.63e+06/5.02e+07 =  3% of the original kernel matrix.

torch.Size([21559, 2])
We keep 2.86e+07/4.78e+08 =  5% of the original kernel matrix.

torch.Size([21948, 2])
We keep 8.06e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([43604, 2])
We keep 3.09e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([31559, 2])
We keep 1.36e+07/7.82e+08 =  1% of the original kernel matrix.

torch.Size([2088, 2])
We keep 1.15e+05/1.40e+06 =  8% of the original kernel matrix.

torch.Size([7656, 2])
We keep 9.75e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([6240, 2])
We keep 7.70e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([11710, 2])
We keep 2.19e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([2024, 2])
We keep 1.28e+05/1.44e+06 =  8% of the original kernel matrix.

torch.Size([7461, 2])
We keep 9.73e+05/2.27e+07 =  4% of the original kernel matrix.

torch.Size([5268, 2])
We keep 6.11e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([10875, 2])
We keep 1.97e+06/6.38e+07 =  3% of the original kernel matrix.

torch.Size([709, 2])
We keep 3.18e+04/2.13e+05 = 14% of the original kernel matrix.

torch.Size([4863, 2])
We keep 4.98e+05/8.72e+06 =  5% of the original kernel matrix.

torch.Size([1283, 2])
We keep 1.15e+05/6.15e+05 = 18% of the original kernel matrix.

torch.Size([6107, 2])
We keep 6.80e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([2912, 2])
We keep 1.93e+05/2.74e+06 =  7% of the original kernel matrix.

torch.Size([8664, 2])
We keep 1.19e+06/3.13e+07 =  3% of the original kernel matrix.

torch.Size([851, 2])
We keep 1.37e+04/1.04e+05 = 13% of the original kernel matrix.

torch.Size([5637, 2])
We keep 4.02e+05/6.11e+06 =  6% of the original kernel matrix.

torch.Size([841, 2])
We keep 1.79e+04/1.36e+05 = 13% of the original kernel matrix.

torch.Size([5554, 2])
We keep 4.50e+05/6.98e+06 =  6% of the original kernel matrix.

torch.Size([4209, 2])
We keep 4.12e+05/7.00e+06 =  5% of the original kernel matrix.

torch.Size([9904, 2])
We keep 1.65e+06/5.00e+07 =  3% of the original kernel matrix.

torch.Size([4233, 2])
We keep 4.49e+05/6.71e+06 =  6% of the original kernel matrix.

torch.Size([9933, 2])
We keep 1.66e+06/4.90e+07 =  3% of the original kernel matrix.

torch.Size([1282, 2])
We keep 4.00e+04/3.62e+05 = 11% of the original kernel matrix.

torch.Size([6377, 2])
We keep 6.08e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([3500, 2])
We keep 3.79e+05/4.64e+06 =  8% of the original kernel matrix.

torch.Size([9163, 2])
We keep 1.40e+06/4.07e+07 =  3% of the original kernel matrix.

torch.Size([9715, 2])
We keep 1.82e+06/4.50e+07 =  4% of the original kernel matrix.

torch.Size([14288, 2])
We keep 3.26e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([2982, 2])
We keep 2.01e+05/2.95e+06 =  6% of the original kernel matrix.

torch.Size([8755, 2])
We keep 1.21e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([2443, 2])
We keep 1.40e+05/1.63e+06 =  8% of the original kernel matrix.

torch.Size([8139, 2])
We keep 9.73e+05/2.42e+07 =  4% of the original kernel matrix.

torch.Size([2197, 2])
We keep 1.09e+05/1.25e+06 =  8% of the original kernel matrix.

torch.Size([7766, 2])
We keep 8.95e+05/2.11e+07 =  4% of the original kernel matrix.

torch.Size([8227, 2])
We keep 1.39e+06/2.92e+07 =  4% of the original kernel matrix.

torch.Size([13075, 2])
We keep 2.81e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([2681, 2])
We keep 1.89e+05/2.12e+06 =  8% of the original kernel matrix.

torch.Size([8377, 2])
We keep 1.06e+06/2.76e+07 =  3% of the original kernel matrix.

torch.Size([2795, 2])
We keep 1.83e+05/2.69e+06 =  6% of the original kernel matrix.

torch.Size([8579, 2])
We keep 1.18e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([3080, 2])
We keep 1.97e+05/3.04e+06 =  6% of the original kernel matrix.

torch.Size([8807, 2])
We keep 1.22e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([2190, 2])
We keep 9.98e+04/1.27e+06 =  7% of the original kernel matrix.

torch.Size([7804, 2])
We keep 9.19e+05/2.13e+07 =  4% of the original kernel matrix.

torch.Size([1853, 2])
We keep 9.23e+04/1.02e+06 =  9% of the original kernel matrix.

torch.Size([7299, 2])
We keep 8.51e+05/1.91e+07 =  4% of the original kernel matrix.

torch.Size([6198, 2])
We keep 7.25e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([11781, 2])
We keep 2.18e+06/7.45e+07 =  2% of the original kernel matrix.

torch.Size([1443, 2])
We keep 5.35e+04/5.24e+05 = 10% of the original kernel matrix.

torch.Size([6701, 2])
We keep 6.92e+05/1.37e+07 =  5% of the original kernel matrix.

torch.Size([8984, 2])
We keep 1.55e+06/3.65e+07 =  4% of the original kernel matrix.

torch.Size([13743, 2])
We keep 3.02e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([3655, 2])
We keep 2.88e+05/4.26e+06 =  6% of the original kernel matrix.

torch.Size([9315, 2])
We keep 1.34e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([1510, 2])
We keep 5.41e+04/5.48e+05 =  9% of the original kernel matrix.

torch.Size([6786, 2])
We keep 6.98e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([2548, 2])
We keep 1.47e+05/2.07e+06 =  7% of the original kernel matrix.

torch.Size([8255, 2])
We keep 1.09e+06/2.72e+07 =  4% of the original kernel matrix.

torch.Size([1636, 2])
We keep 7.12e+04/7.59e+05 =  9% of the original kernel matrix.

torch.Size([6904, 2])
We keep 7.83e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([3164, 2])
We keep 2.26e+05/3.41e+06 =  6% of the original kernel matrix.

torch.Size([8881, 2])
We keep 1.28e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([2565, 2])
We keep 1.32e+05/2.02e+06 =  6% of the original kernel matrix.

torch.Size([8369, 2])
We keep 1.06e+06/2.69e+07 =  3% of the original kernel matrix.

torch.Size([1615, 2])
We keep 5.67e+04/5.72e+05 =  9% of the original kernel matrix.

torch.Size([6923, 2])
We keep 7.04e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([2518, 2])
We keep 1.82e+05/2.28e+06 =  7% of the original kernel matrix.

torch.Size([8027, 2])
We keep 1.10e+06/2.86e+07 =  3% of the original kernel matrix.

torch.Size([3146, 2])
We keep 1.97e+05/2.90e+06 =  6% of the original kernel matrix.

torch.Size([9014, 2])
We keep 1.21e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([1875, 2])
We keep 7.12e+04/8.45e+05 =  8% of the original kernel matrix.

torch.Size([7395, 2])
We keep 8.04e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([1266, 2])
We keep 3.50e+04/3.56e+05 =  9% of the original kernel matrix.

torch.Size([6415, 2])
We keep 5.79e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([2055, 2])
We keep 9.89e+04/1.24e+06 =  7% of the original kernel matrix.

torch.Size([7651, 2])
We keep 9.14e+05/2.10e+07 =  4% of the original kernel matrix.

torch.Size([4630, 2])
We keep 4.43e+05/7.78e+06 =  5% of the original kernel matrix.

torch.Size([10312, 2])
We keep 1.71e+06/5.28e+07 =  3% of the original kernel matrix.

torch.Size([1823, 2])
We keep 8.10e+04/8.72e+05 =  9% of the original kernel matrix.

torch.Size([7287, 2])
We keep 8.13e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([1083, 2])
We keep 4.16e+04/3.31e+05 = 12% of the original kernel matrix.

torch.Size([6013, 2])
We keep 6.06e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([2982, 2])
We keep 2.44e+05/2.98e+06 =  8% of the original kernel matrix.

torch.Size([8611, 2])
We keep 1.22e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([8693, 2])
We keep 1.39e+06/3.43e+07 =  4% of the original kernel matrix.

torch.Size([13702, 2])
We keep 2.97e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([3085, 2])
We keep 1.83e+05/2.68e+06 =  6% of the original kernel matrix.

torch.Size([8864, 2])
We keep 1.19e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([499, 2])
We keep 1.23e+04/7.08e+04 = 17% of the original kernel matrix.

torch.Size([4377, 2])
We keep 3.56e+05/5.03e+06 =  7% of the original kernel matrix.

torch.Size([8280, 2])
We keep 1.32e+06/2.94e+07 =  4% of the original kernel matrix.

torch.Size([13177, 2])
We keep 2.76e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([723, 2])
We keep 1.58e+04/1.18e+05 = 13% of the original kernel matrix.

torch.Size([5287, 2])
We keep 4.21e+05/6.51e+06 =  6% of the original kernel matrix.

torch.Size([1839, 2])
We keep 7.73e+04/8.57e+05 =  9% of the original kernel matrix.

torch.Size([7366, 2])
We keep 8.12e+05/1.75e+07 =  4% of the original kernel matrix.

torch.Size([2000, 2])
We keep 8.54e+04/1.10e+06 =  7% of the original kernel matrix.

torch.Size([7597, 2])
We keep 8.75e+05/1.99e+07 =  4% of the original kernel matrix.

torch.Size([1875, 2])
We keep 8.65e+04/9.04e+05 =  9% of the original kernel matrix.

torch.Size([7352, 2])
We keep 8.34e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([734, 2])
We keep 1.61e+04/1.09e+05 = 14% of the original kernel matrix.

torch.Size([5271, 2])
We keep 4.22e+05/6.24e+06 =  6% of the original kernel matrix.

torch.Size([3959, 2])
We keep 3.33e+05/5.58e+06 =  5% of the original kernel matrix.

torch.Size([9723, 2])
We keep 1.53e+06/4.47e+07 =  3% of the original kernel matrix.

torch.Size([4298, 2])
We keep 4.27e+05/6.91e+06 =  6% of the original kernel matrix.

torch.Size([9998, 2])
We keep 1.66e+06/4.97e+07 =  3% of the original kernel matrix.

torch.Size([1244, 2])
We keep 4.76e+04/4.71e+05 = 10% of the original kernel matrix.

torch.Size([6366, 2])
We keep 6.93e+05/1.30e+07 =  5% of the original kernel matrix.

torch.Size([1422, 2])
We keep 4.27e+04/4.04e+05 = 10% of the original kernel matrix.

torch.Size([6751, 2])
We keep 6.41e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([867, 2])
We keep 1.75e+04/1.44e+05 = 12% of the original kernel matrix.

torch.Size([5609, 2])
We keep 4.45e+05/7.17e+06 =  6% of the original kernel matrix.

torch.Size([3834, 2])
We keep 3.22e+05/5.22e+06 =  6% of the original kernel matrix.

torch.Size([9562, 2])
We keep 1.47e+06/4.32e+07 =  3% of the original kernel matrix.

torch.Size([2050, 2])
We keep 9.58e+04/1.22e+06 =  7% of the original kernel matrix.

torch.Size([7559, 2])
We keep 8.91e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([4196, 2])
We keep 4.42e+05/7.17e+06 =  6% of the original kernel matrix.

torch.Size([9789, 2])
We keep 1.68e+06/5.06e+07 =  3% of the original kernel matrix.

torch.Size([5351, 2])
We keep 5.79e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([10939, 2])
We keep 1.98e+06/6.47e+07 =  3% of the original kernel matrix.

torch.Size([3057, 2])
We keep 2.33e+05/2.85e+06 =  8% of the original kernel matrix.

torch.Size([8859, 2])
We keep 1.19e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([2764, 2])
We keep 1.71e+05/2.28e+06 =  7% of the original kernel matrix.

torch.Size([8467, 2])
We keep 1.12e+06/2.86e+07 =  3% of the original kernel matrix.

torch.Size([2791, 2])
We keep 1.81e+05/2.68e+06 =  6% of the original kernel matrix.

torch.Size([8521, 2])
We keep 1.19e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([9331, 2])
We keep 1.82e+06/4.50e+07 =  4% of the original kernel matrix.

torch.Size([13923, 2])
We keep 3.28e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([1472, 2])
We keep 6.10e+04/6.58e+05 =  9% of the original kernel matrix.

torch.Size([6721, 2])
We keep 7.41e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([3134, 2])
We keep 1.92e+05/2.99e+06 =  6% of the original kernel matrix.

torch.Size([8995, 2])
We keep 1.23e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([1917, 2])
We keep 8.49e+04/9.96e+05 =  8% of the original kernel matrix.

torch.Size([7376, 2])
We keep 8.44e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([2514, 2])
We keep 2.02e+05/2.16e+06 =  9% of the original kernel matrix.

torch.Size([7987, 2])
We keep 1.04e+06/2.78e+07 =  3% of the original kernel matrix.

torch.Size([1557, 2])
We keep 7.03e+04/7.45e+05 =  9% of the original kernel matrix.

torch.Size([6803, 2])
We keep 7.76e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([4334, 2])
We keep 3.70e+05/6.57e+06 =  5% of the original kernel matrix.

torch.Size([10200, 2])
We keep 1.63e+06/4.85e+07 =  3% of the original kernel matrix.

torch.Size([861, 2])
We keep 1.45e+04/1.08e+05 = 13% of the original kernel matrix.

torch.Size([5678, 2])
We keep 3.94e+05/6.20e+06 =  6% of the original kernel matrix.

torch.Size([1918, 2])
We keep 9.84e+04/1.12e+06 =  8% of the original kernel matrix.

torch.Size([7369, 2])
We keep 8.87e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([1487, 2])
We keep 6.23e+04/5.70e+05 = 10% of the original kernel matrix.

torch.Size([6748, 2])
We keep 6.93e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([1939, 2])
We keep 9.19e+04/1.09e+06 =  8% of the original kernel matrix.

torch.Size([7394, 2])
We keep 8.59e+05/1.98e+07 =  4% of the original kernel matrix.

torch.Size([1274, 2])
We keep 5.03e+04/4.76e+05 = 10% of the original kernel matrix.

torch.Size([6378, 2])
We keep 6.69e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([1603, 2])
We keep 6.31e+04/6.38e+05 =  9% of the original kernel matrix.

torch.Size([6879, 2])
We keep 7.21e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([3282, 2])
We keep 2.13e+05/3.55e+06 =  6% of the original kernel matrix.

torch.Size([9180, 2])
We keep 1.29e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([3320, 2])
We keep 2.60e+05/3.66e+06 =  7% of the original kernel matrix.

torch.Size([9040, 2])
We keep 1.33e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([947, 2])
We keep 2.18e+04/1.66e+05 = 13% of the original kernel matrix.

torch.Size([5697, 2])
We keep 4.74e+05/7.70e+06 =  6% of the original kernel matrix.

torch.Size([1904, 2])
We keep 7.87e+04/1.02e+06 =  7% of the original kernel matrix.

torch.Size([7463, 2])
We keep 8.42e+05/1.91e+07 =  4% of the original kernel matrix.

torch.Size([1000, 2])
We keep 2.61e+04/2.10e+05 = 12% of the original kernel matrix.

torch.Size([5958, 2])
We keep 5.34e+05/8.66e+06 =  6% of the original kernel matrix.

torch.Size([6767, 2])
We keep 8.74e+05/1.86e+07 =  4% of the original kernel matrix.

torch.Size([12171, 2])
We keep 2.34e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([1111, 2])
We keep 2.87e+04/2.51e+05 = 11% of the original kernel matrix.

torch.Size([6154, 2])
We keep 5.29e+05/9.48e+06 =  5% of the original kernel matrix.

torch.Size([8534, 2])
We keep 1.44e+06/3.45e+07 =  4% of the original kernel matrix.

torch.Size([13353, 2])
We keep 2.97e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([2082, 2])
We keep 1.08e+05/1.28e+06 =  8% of the original kernel matrix.

torch.Size([7610, 2])
We keep 9.32e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([1616, 2])
We keep 5.92e+04/6.50e+05 =  9% of the original kernel matrix.

torch.Size([7023, 2])
We keep 7.43e+05/1.52e+07 =  4% of the original kernel matrix.

torch.Size([3029, 2])
We keep 2.25e+05/3.37e+06 =  6% of the original kernel matrix.

torch.Size([8783, 2])
We keep 1.28e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([1515, 2])
We keep 6.23e+04/6.87e+05 =  9% of the original kernel matrix.

torch.Size([6806, 2])
We keep 7.56e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([1288, 2])
We keep 4.27e+04/4.79e+05 =  8% of the original kernel matrix.

torch.Size([6410, 2])
We keep 6.63e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([2092, 2])
We keep 1.04e+05/1.18e+06 =  8% of the original kernel matrix.

torch.Size([7710, 2])
We keep 9.21e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([1549, 2])
We keep 4.82e+04/5.26e+05 =  9% of the original kernel matrix.

torch.Size([6853, 2])
We keep 6.88e+05/1.37e+07 =  5% of the original kernel matrix.

torch.Size([17494, 2])
We keep 5.30e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([19750, 2])
We keep 5.92e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([2865, 2])
We keep 1.98e+05/2.69e+06 =  7% of the original kernel matrix.

torch.Size([8585, 2])
We keep 1.20e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([5314, 2])
We keep 6.19e+05/1.15e+07 =  5% of the original kernel matrix.

torch.Size([10966, 2])
We keep 1.97e+06/6.41e+07 =  3% of the original kernel matrix.

torch.Size([1375, 2])
We keep 4.23e+04/3.87e+05 = 10% of the original kernel matrix.

torch.Size([6629, 2])
We keep 6.08e+05/1.18e+07 =  5% of the original kernel matrix.

torch.Size([2764, 2])
We keep 1.71e+05/2.32e+06 =  7% of the original kernel matrix.

torch.Size([8418, 2])
We keep 1.13e+06/2.88e+07 =  3% of the original kernel matrix.

torch.Size([918, 2])
We keep 2.21e+04/1.73e+05 = 12% of the original kernel matrix.

torch.Size([5712, 2])
We keep 4.78e+05/7.87e+06 =  6% of the original kernel matrix.

torch.Size([2503, 2])
We keep 1.24e+05/1.67e+06 =  7% of the original kernel matrix.

torch.Size([8234, 2])
We keep 1.02e+06/2.45e+07 =  4% of the original kernel matrix.

torch.Size([1626, 2])
We keep 6.19e+04/6.53e+05 =  9% of the original kernel matrix.

torch.Size([7015, 2])
We keep 7.51e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([975, 2])
We keep 2.00e+04/1.60e+05 = 12% of the original kernel matrix.

torch.Size([5844, 2])
We keep 4.68e+05/7.57e+06 =  6% of the original kernel matrix.

torch.Size([1866, 2])
We keep 8.87e+04/1.01e+06 =  8% of the original kernel matrix.

torch.Size([7287, 2])
We keep 8.52e+05/1.90e+07 =  4% of the original kernel matrix.

torch.Size([1538, 2])
We keep 6.41e+04/6.79e+05 =  9% of the original kernel matrix.

torch.Size([6740, 2])
We keep 7.65e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([2327, 2])
We keep 1.47e+05/1.73e+06 =  8% of the original kernel matrix.

torch.Size([7969, 2])
We keep 1.04e+06/2.49e+07 =  4% of the original kernel matrix.

torch.Size([3393, 2])
We keep 2.78e+05/4.27e+06 =  6% of the original kernel matrix.

torch.Size([9007, 2])
We keep 1.40e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([809, 2])
We keep 1.54e+04/1.24e+05 = 12% of the original kernel matrix.

torch.Size([5479, 2])
We keep 4.22e+05/6.66e+06 =  6% of the original kernel matrix.

torch.Size([7877, 2])
We keep 1.27e+06/2.78e+07 =  4% of the original kernel matrix.

torch.Size([12930, 2])
We keep 2.74e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([6358, 2])
We keep 8.37e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([11783, 2])
We keep 2.22e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([8774, 2])
We keep 1.38e+06/3.31e+07 =  4% of the original kernel matrix.

torch.Size([13670, 2])
We keep 2.91e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([1795, 2])
We keep 6.72e+04/7.43e+05 =  9% of the original kernel matrix.

torch.Size([7247, 2])
We keep 7.70e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([2514, 2])
We keep 1.19e+05/1.63e+06 =  7% of the original kernel matrix.

torch.Size([8310, 2])
We keep 9.97e+05/2.41e+07 =  4% of the original kernel matrix.

torch.Size([2974, 2])
We keep 2.88e+05/2.89e+06 =  9% of the original kernel matrix.

torch.Size([8751, 2])
We keep 1.18e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([1578, 2])
We keep 5.20e+04/5.43e+05 =  9% of the original kernel matrix.

torch.Size([6926, 2])
We keep 7.03e+05/1.39e+07 =  5% of the original kernel matrix.

torch.Size([1883, 2])
We keep 8.26e+04/9.88e+05 =  8% of the original kernel matrix.

torch.Size([7375, 2])
We keep 8.40e+05/1.88e+07 =  4% of the original kernel matrix.

torch.Size([2575, 2])
We keep 1.99e+05/2.42e+06 =  8% of the original kernel matrix.

torch.Size([8032, 2])
We keep 1.17e+06/2.94e+07 =  3% of the original kernel matrix.

torch.Size([7188, 2])
We keep 1.04e+06/2.25e+07 =  4% of the original kernel matrix.

torch.Size([12463, 2])
We keep 2.53e+06/8.97e+07 =  2% of the original kernel matrix.

torch.Size([3571, 2])
We keep 2.96e+05/5.10e+06 =  5% of the original kernel matrix.

torch.Size([9385, 2])
We keep 1.48e+06/4.27e+07 =  3% of the original kernel matrix.

torch.Size([2290, 2])
We keep 1.98e+05/1.73e+06 = 11% of the original kernel matrix.

torch.Size([7694, 2])
We keep 9.98e+05/2.49e+07 =  4% of the original kernel matrix.

torch.Size([2434, 2])
We keep 1.11e+05/1.50e+06 =  7% of the original kernel matrix.

torch.Size([8076, 2])
We keep 9.58e+05/2.32e+07 =  4% of the original kernel matrix.

torch.Size([9914, 2])
We keep 2.01e+06/4.66e+07 =  4% of the original kernel matrix.

torch.Size([14343, 2])
We keep 3.26e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([2521, 2])
We keep 1.16e+05/1.49e+06 =  7% of the original kernel matrix.

torch.Size([8223, 2])
We keep 9.61e+05/2.31e+07 =  4% of the original kernel matrix.

torch.Size([1789, 2])
We keep 7.79e+04/9.22e+05 =  8% of the original kernel matrix.

torch.Size([7238, 2])
We keep 8.27e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([2658, 2])
We keep 1.65e+05/2.08e+06 =  7% of the original kernel matrix.

torch.Size([8384, 2])
We keep 1.11e+06/2.73e+07 =  4% of the original kernel matrix.

torch.Size([973, 2])
We keep 2.43e+04/1.71e+05 = 14% of the original kernel matrix.

torch.Size([5796, 2])
We keep 4.70e+05/7.83e+06 =  6% of the original kernel matrix.

torch.Size([3118, 2])
We keep 2.22e+05/3.16e+06 =  7% of the original kernel matrix.

torch.Size([8919, 2])
We keep 1.24e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([1097, 2])
We keep 2.92e+04/2.65e+05 = 10% of the original kernel matrix.

torch.Size([6050, 2])
We keep 5.47e+05/9.74e+06 =  5% of the original kernel matrix.

torch.Size([3854, 2])
We keep 3.23e+05/5.06e+06 =  6% of the original kernel matrix.

torch.Size([9626, 2])
We keep 1.47e+06/4.26e+07 =  3% of the original kernel matrix.

torch.Size([1224, 2])
We keep 2.59e+04/2.41e+05 = 10% of the original kernel matrix.

torch.Size([6377, 2])
We keep 5.31e+05/9.29e+06 =  5% of the original kernel matrix.

torch.Size([2112, 2])
We keep 1.29e+05/1.39e+06 =  9% of the original kernel matrix.

torch.Size([7498, 2])
We keep 9.35e+05/2.23e+07 =  4% of the original kernel matrix.

torch.Size([1472, 2])
We keep 6.22e+04/7.02e+05 =  8% of the original kernel matrix.

torch.Size([6714, 2])
We keep 7.57e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([2479, 2])
We keep 1.24e+05/1.75e+06 =  7% of the original kernel matrix.

torch.Size([8253, 2])
We keep 9.96e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([1768, 2])
We keep 7.53e+04/9.35e+05 =  8% of the original kernel matrix.

torch.Size([7227, 2])
We keep 8.17e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([3136, 2])
We keep 1.99e+05/3.18e+06 =  6% of the original kernel matrix.

torch.Size([8903, 2])
We keep 1.25e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([674, 2])
We keep 1.29e+04/7.13e+04 = 18% of the original kernel matrix.

torch.Size([5166, 2])
We keep 3.74e+05/5.05e+06 =  7% of the original kernel matrix.

torch.Size([1522, 2])
We keep 5.40e+04/4.79e+05 = 11% of the original kernel matrix.

torch.Size([6673, 2])
We keep 6.69e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([1322, 2])
We keep 5.40e+04/4.86e+05 = 11% of the original kernel matrix.

torch.Size([6326, 2])
We keep 6.54e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([5454, 2])
We keep 5.51e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([11085, 2])
We keep 1.90e+06/6.13e+07 =  3% of the original kernel matrix.

torch.Size([1414, 2])
We keep 3.77e+04/3.73e+05 = 10% of the original kernel matrix.

torch.Size([6801, 2])
We keep 6.03e+05/1.16e+07 =  5% of the original kernel matrix.

torch.Size([3959, 2])
We keep 3.16e+05/5.47e+06 =  5% of the original kernel matrix.

torch.Size([9785, 2])
We keep 1.49e+06/4.42e+07 =  3% of the original kernel matrix.

torch.Size([3064, 2])
We keep 1.78e+05/2.83e+06 =  6% of the original kernel matrix.

torch.Size([8927, 2])
We keep 1.19e+06/3.18e+07 =  3% of the original kernel matrix.

torch.Size([3817, 2])
We keep 3.01e+05/4.81e+06 =  6% of the original kernel matrix.

torch.Size([9667, 2])
We keep 1.46e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([3069, 2])
We keep 2.64e+05/3.57e+06 =  7% of the original kernel matrix.

torch.Size([8807, 2])
We keep 1.34e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([2523, 2])
We keep 1.60e+05/1.97e+06 =  8% of the original kernel matrix.

torch.Size([8111, 2])
We keep 1.04e+06/2.66e+07 =  3% of the original kernel matrix.

torch.Size([2083, 2])
We keep 1.23e+05/1.29e+06 =  9% of the original kernel matrix.

torch.Size([7369, 2])
We keep 8.82e+05/2.15e+07 =  4% of the original kernel matrix.

torch.Size([3091, 2])
We keep 2.26e+05/3.39e+06 =  6% of the original kernel matrix.

torch.Size([8828, 2])
We keep 1.28e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([1774, 2])
We keep 7.28e+04/9.25e+05 =  7% of the original kernel matrix.

torch.Size([7236, 2])
We keep 8.22e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([1725, 2])
We keep 9.12e+04/9.08e+05 = 10% of the original kernel matrix.

torch.Size([7026, 2])
We keep 7.94e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([2022, 2])
We keep 1.24e+05/1.32e+06 =  9% of the original kernel matrix.

torch.Size([7538, 2])
We keep 9.02e+05/2.17e+07 =  4% of the original kernel matrix.

torch.Size([6792, 2])
We keep 8.49e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([12133, 2])
We keep 2.36e+06/8.22e+07 =  2% of the original kernel matrix.

torch.Size([4807, 2])
We keep 5.07e+05/9.24e+06 =  5% of the original kernel matrix.

torch.Size([10475, 2])
We keep 1.85e+06/5.75e+07 =  3% of the original kernel matrix.

torch.Size([2444, 2])
We keep 1.37e+05/1.95e+06 =  6% of the original kernel matrix.

torch.Size([8097, 2])
We keep 1.06e+06/2.64e+07 =  4% of the original kernel matrix.

torch.Size([2332, 2])
We keep 1.60e+05/1.86e+06 =  8% of the original kernel matrix.

torch.Size([7886, 2])
We keep 1.05e+06/2.58e+07 =  4% of the original kernel matrix.

torch.Size([9300, 2])
We keep 1.65e+06/3.97e+07 =  4% of the original kernel matrix.

torch.Size([14009, 2])
We keep 3.11e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([9576, 2])
We keep 1.95e+06/4.40e+07 =  4% of the original kernel matrix.

torch.Size([14169, 2])
We keep 3.26e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([1279, 2])
We keep 4.35e+04/4.48e+05 =  9% of the original kernel matrix.

torch.Size([6382, 2])
We keep 6.56e+05/1.27e+07 =  5% of the original kernel matrix.

torch.Size([837, 2])
We keep 1.47e+04/1.02e+05 = 14% of the original kernel matrix.

torch.Size([5536, 2])
We keep 4.11e+05/6.03e+06 =  6% of the original kernel matrix.

torch.Size([1829, 2])
We keep 8.74e+04/9.68e+05 =  9% of the original kernel matrix.

torch.Size([7269, 2])
We keep 8.61e+05/1.86e+07 =  4% of the original kernel matrix.

torch.Size([1222, 2])
We keep 2.73e+04/2.62e+05 = 10% of the original kernel matrix.

torch.Size([6342, 2])
We keep 5.26e+05/9.69e+06 =  5% of the original kernel matrix.

torch.Size([1486, 2])
We keep 7.41e+04/6.35e+05 = 11% of the original kernel matrix.

torch.Size([6645, 2])
We keep 7.03e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([3156, 2])
We keep 2.27e+05/3.47e+06 =  6% of the original kernel matrix.

torch.Size([9024, 2])
We keep 1.30e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([1506, 2])
We keep 5.53e+04/5.82e+05 =  9% of the original kernel matrix.

torch.Size([6761, 2])
We keep 7.26e+05/1.44e+07 =  5% of the original kernel matrix.

torch.Size([988, 2])
We keep 1.97e+04/1.68e+05 = 11% of the original kernel matrix.

torch.Size([5896, 2])
We keep 4.70e+05/7.76e+06 =  6% of the original kernel matrix.

torch.Size([1920, 2])
We keep 8.44e+04/1.05e+06 =  8% of the original kernel matrix.

torch.Size([7431, 2])
We keep 8.66e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([1316, 2])
We keep 3.68e+04/3.62e+05 = 10% of the original kernel matrix.

torch.Size([6540, 2])
We keep 6.02e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([6510, 2])
We keep 7.34e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([12004, 2])
We keep 2.20e+06/7.50e+07 =  2% of the original kernel matrix.

torch.Size([1322, 2])
We keep 4.61e+04/4.68e+05 =  9% of the original kernel matrix.

torch.Size([6544, 2])
We keep 6.71e+05/1.29e+07 =  5% of the original kernel matrix.

torch.Size([5891, 2])
We keep 6.53e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([11436, 2])
We keep 2.07e+06/6.87e+07 =  3% of the original kernel matrix.

torch.Size([6103, 2])
We keep 7.68e+05/1.50e+07 =  5% of the original kernel matrix.

torch.Size([11667, 2])
We keep 2.16e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([2716, 2])
We keep 2.06e+05/2.55e+06 =  8% of the original kernel matrix.

torch.Size([8391, 2])
We keep 1.15e+06/3.02e+07 =  3% of the original kernel matrix.

torch.Size([618, 2])
We keep 1.11e+04/6.35e+04 = 17% of the original kernel matrix.

torch.Size([4971, 2])
We keep 3.48e+05/4.77e+06 =  7% of the original kernel matrix.

torch.Size([7790, 2])
We keep 1.24e+06/2.76e+07 =  4% of the original kernel matrix.

torch.Size([13003, 2])
We keep 2.74e+06/9.93e+07 =  2% of the original kernel matrix.

torch.Size([3098, 2])
We keep 2.13e+05/3.08e+06 =  6% of the original kernel matrix.

torch.Size([8845, 2])
We keep 1.25e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([1607, 2])
We keep 6.73e+04/7.41e+05 =  9% of the original kernel matrix.

torch.Size([6940, 2])
We keep 7.69e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([9188, 2])
We keep 1.66e+06/4.11e+07 =  4% of the original kernel matrix.

torch.Size([13988, 2])
We keep 3.18e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([6311, 2])
We keep 1.01e+06/1.57e+07 =  6% of the original kernel matrix.

torch.Size([11665, 2])
We keep 2.21e+06/7.51e+07 =  2% of the original kernel matrix.

torch.Size([1290, 2])
We keep 4.43e+04/3.65e+05 = 12% of the original kernel matrix.

torch.Size([6368, 2])
We keep 5.94e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([1401, 2])
We keep 5.10e+04/5.08e+05 = 10% of the original kernel matrix.

torch.Size([6671, 2])
We keep 6.99e+05/1.35e+07 =  5% of the original kernel matrix.

torch.Size([1204, 2])
We keep 2.66e+04/2.41e+05 = 11% of the original kernel matrix.

torch.Size([6354, 2])
We keep 5.24e+05/9.29e+06 =  5% of the original kernel matrix.

torch.Size([1028, 2])
We keep 2.26e+04/2.08e+05 = 10% of the original kernel matrix.

torch.Size([6051, 2])
We keep 4.94e+05/8.63e+06 =  5% of the original kernel matrix.

torch.Size([762, 2])
We keep 1.78e+04/1.34e+05 = 13% of the original kernel matrix.

torch.Size([5333, 2])
We keep 4.35e+05/6.92e+06 =  6% of the original kernel matrix.

torch.Size([758, 2])
We keep 1.70e+04/1.34e+05 = 12% of the original kernel matrix.

torch.Size([5292, 2])
We keep 4.32e+05/6.92e+06 =  6% of the original kernel matrix.

torch.Size([706, 2])
We keep 8.82e+03/6.30e+04 = 14% of the original kernel matrix.

torch.Size([5355, 2])
We keep 3.41e+05/4.75e+06 =  7% of the original kernel matrix.

torch.Size([3413, 2])
We keep 2.55e+05/3.69e+06 =  6% of the original kernel matrix.

torch.Size([9253, 2])
We keep 1.29e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([2758, 2])
We keep 2.03e+05/2.40e+06 =  8% of the original kernel matrix.

torch.Size([8491, 2])
We keep 1.12e+06/2.93e+07 =  3% of the original kernel matrix.

torch.Size([953, 2])
We keep 2.47e+04/2.08e+05 = 11% of the original kernel matrix.

torch.Size([5605, 2])
We keep 4.94e+05/8.63e+06 =  5% of the original kernel matrix.

torch.Size([1407, 2])
We keep 4.69e+04/4.73e+05 =  9% of the original kernel matrix.

torch.Size([6690, 2])
We keep 6.64e+05/1.30e+07 =  5% of the original kernel matrix.

torch.Size([2556, 2])
We keep 1.51e+05/1.99e+06 =  7% of the original kernel matrix.

torch.Size([8181, 2])
We keep 1.07e+06/2.67e+07 =  4% of the original kernel matrix.

torch.Size([1342, 2])
We keep 4.23e+04/4.04e+05 = 10% of the original kernel matrix.

torch.Size([6551, 2])
We keep 6.29e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([4686, 2])
We keep 4.78e+05/8.24e+06 =  5% of the original kernel matrix.

torch.Size([10265, 2])
We keep 1.74e+06/5.43e+07 =  3% of the original kernel matrix.

torch.Size([3357, 2])
We keep 2.49e+05/4.02e+06 =  6% of the original kernel matrix.

torch.Size([9072, 2])
We keep 1.35e+06/3.79e+07 =  3% of the original kernel matrix.

torch.Size([1981, 2])
We keep 9.78e+04/1.11e+06 =  8% of the original kernel matrix.

torch.Size([7392, 2])
We keep 9.00e+05/1.99e+07 =  4% of the original kernel matrix.

torch.Size([561, 2])
We keep 8.48e+03/5.02e+04 = 16% of the original kernel matrix.

torch.Size([4795, 2])
We keep 3.28e+05/4.24e+06 =  7% of the original kernel matrix.

torch.Size([4960, 2])
We keep 5.86e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([10548, 2])
We keep 1.90e+06/6.01e+07 =  3% of the original kernel matrix.

torch.Size([928, 2])
We keep 2.83e+04/1.66e+05 = 16% of the original kernel matrix.

torch.Size([5688, 2])
We keep 4.69e+05/7.72e+06 =  6% of the original kernel matrix.

torch.Size([1154, 2])
We keep 3.42e+04/3.29e+05 = 10% of the original kernel matrix.

torch.Size([6183, 2])
We keep 5.85e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([1574, 2])
We keep 7.33e+04/7.48e+05 =  9% of the original kernel matrix.

torch.Size([6884, 2])
We keep 7.88e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([2110, 2])
We keep 1.11e+05/1.28e+06 =  8% of the original kernel matrix.

torch.Size([7628, 2])
We keep 9.31e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([9110, 2])
We keep 1.69e+06/3.91e+07 =  4% of the original kernel matrix.

torch.Size([13924, 2])
We keep 3.12e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([2116, 2])
We keep 1.06e+05/1.28e+06 =  8% of the original kernel matrix.

torch.Size([7740, 2])
We keep 9.29e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([921, 2])
We keep 2.64e+04/2.08e+05 = 12% of the original kernel matrix.

torch.Size([5655, 2])
We keep 5.03e+05/8.63e+06 =  5% of the original kernel matrix.

torch.Size([2003, 2])
We keep 9.17e+04/1.13e+06 =  8% of the original kernel matrix.

torch.Size([7515, 2])
We keep 8.89e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([7002, 2])
We keep 8.89e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([12357, 2])
We keep 2.40e+06/8.60e+07 =  2% of the original kernel matrix.

torch.Size([909, 2])
We keep 2.16e+04/1.76e+05 = 12% of the original kernel matrix.

torch.Size([5629, 2])
We keep 4.76e+05/7.95e+06 =  5% of the original kernel matrix.

torch.Size([4195, 2])
We keep 3.74e+05/6.80e+06 =  5% of the original kernel matrix.

torch.Size([10016, 2])
We keep 1.61e+06/4.93e+07 =  3% of the original kernel matrix.

torch.Size([11802, 2])
We keep 1.10e+07/1.28e+08 =  8% of the original kernel matrix.

torch.Size([15814, 2])
We keep 4.59e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([5196, 2])
We keep 6.88e+05/1.18e+07 =  5% of the original kernel matrix.

torch.Size([11020, 2])
We keep 1.80e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([4398, 2])
We keep 4.17e+05/7.06e+06 =  5% of the original kernel matrix.

torch.Size([10172, 2])
We keep 1.68e+06/5.03e+07 =  3% of the original kernel matrix.

torch.Size([1392, 2])
We keep 7.49e+04/5.08e+05 = 14% of the original kernel matrix.

torch.Size([6395, 2])
We keep 6.86e+05/1.35e+07 =  5% of the original kernel matrix.

torch.Size([5293, 2])
We keep 6.11e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([10963, 2])
We keep 1.97e+06/6.25e+07 =  3% of the original kernel matrix.

torch.Size([20695, 2])
We keep 1.37e+07/3.29e+08 =  4% of the original kernel matrix.

torch.Size([21552, 2])
We keep 7.29e+06/3.43e+08 =  2% of the original kernel matrix.

torch.Size([37905, 2])
We keep 1.28e+08/1.74e+09 =  7% of the original kernel matrix.

torch.Size([28652, 2])
We keep 1.36e+07/7.90e+08 =  1% of the original kernel matrix.

torch.Size([3190, 2])
We keep 1.90e+05/2.97e+06 =  6% of the original kernel matrix.

torch.Size([9063, 2])
We keep 1.22e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([20981, 2])
We keep 1.50e+07/3.93e+08 =  3% of the original kernel matrix.

torch.Size([21633, 2])
We keep 7.44e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([44766, 2])
We keep 4.21e+07/2.01e+09 =  2% of the original kernel matrix.

torch.Size([31130, 2])
We keep 1.44e+07/8.48e+08 =  1% of the original kernel matrix.

torch.Size([9119, 2])
We keep 2.18e+06/4.29e+07 =  5% of the original kernel matrix.

torch.Size([13876, 2])
We keep 3.26e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([204164, 2])
We keep 1.28e+09/5.75e+10 =  2% of the original kernel matrix.

torch.Size([69499, 2])
We keep 6.46e+07/4.54e+09 =  1% of the original kernel matrix.

torch.Size([1115, 2])
We keep 2.94e+04/2.44e+05 = 12% of the original kernel matrix.

torch.Size([6114, 2])
We keep 5.26e+05/9.34e+06 =  5% of the original kernel matrix.

torch.Size([5258, 2])
We keep 1.13e+06/1.30e+07 =  8% of the original kernel matrix.

torch.Size([10816, 2])
We keep 2.15e+06/6.83e+07 =  3% of the original kernel matrix.

torch.Size([34024, 2])
We keep 4.21e+07/1.42e+09 =  2% of the original kernel matrix.

torch.Size([27095, 2])
We keep 1.26e+07/7.13e+08 =  1% of the original kernel matrix.

torch.Size([225293, 2])
We keep 5.71e+08/4.76e+10 =  1% of the original kernel matrix.

torch.Size([73859, 2])
We keep 5.68e+07/4.13e+09 =  1% of the original kernel matrix.

torch.Size([2919, 2])
We keep 1.87e+05/2.70e+06 =  6% of the original kernel matrix.

torch.Size([8708, 2])
We keep 1.16e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([18716, 2])
We keep 7.50e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([20343, 2])
We keep 6.37e+06/3.02e+08 =  2% of the original kernel matrix.

torch.Size([19460, 2])
We keep 2.22e+07/3.51e+08 =  6% of the original kernel matrix.

torch.Size([20557, 2])
We keep 7.44e+06/3.55e+08 =  2% of the original kernel matrix.

torch.Size([63615, 2])
We keep 1.72e+08/4.89e+09 =  3% of the original kernel matrix.

torch.Size([37054, 2])
We keep 2.21e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([32479, 2])
We keep 5.75e+07/1.14e+09 =  5% of the original kernel matrix.

torch.Size([26424, 2])
We keep 1.19e+07/6.39e+08 =  1% of the original kernel matrix.

torch.Size([21788, 2])
We keep 2.24e+07/5.42e+08 =  4% of the original kernel matrix.

torch.Size([21671, 2])
We keep 8.80e+06/4.40e+08 =  1% of the original kernel matrix.

torch.Size([11811, 2])
We keep 4.73e+06/8.87e+07 =  5% of the original kernel matrix.

torch.Size([16113, 2])
We keep 4.02e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([7970, 2])
We keep 1.25e+06/2.89e+07 =  4% of the original kernel matrix.

torch.Size([13128, 2])
We keep 2.75e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([9363, 2])
We keep 2.22e+07/1.29e+08 = 17% of the original kernel matrix.

torch.Size([13655, 2])
We keep 5.09e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([27129, 2])
We keep 5.22e+07/1.01e+09 =  5% of the original kernel matrix.

torch.Size([24514, 2])
We keep 1.02e+07/6.02e+08 =  1% of the original kernel matrix.

torch.Size([2515, 2])
We keep 2.64e+05/2.20e+06 = 11% of the original kernel matrix.

torch.Size([8023, 2])
We keep 1.12e+06/2.81e+07 =  4% of the original kernel matrix.

torch.Size([2159, 2])
We keep 1.39e+05/1.69e+06 =  8% of the original kernel matrix.

torch.Size([7582, 2])
We keep 1.02e+06/2.46e+07 =  4% of the original kernel matrix.

torch.Size([36778, 2])
We keep 3.25e+07/1.33e+09 =  2% of the original kernel matrix.

torch.Size([28825, 2])
We keep 1.26e+07/6.90e+08 =  1% of the original kernel matrix.

torch.Size([13733, 2])
We keep 5.01e+06/1.49e+08 =  3% of the original kernel matrix.

torch.Size([16904, 2])
We keep 5.17e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([138224, 2])
We keep 2.91e+08/1.86e+10 =  1% of the original kernel matrix.

torch.Size([56658, 2])
We keep 3.84e+07/2.58e+09 =  1% of the original kernel matrix.

torch.Size([49762, 2])
We keep 3.43e+07/2.25e+09 =  1% of the original kernel matrix.

torch.Size([33450, 2])
We keep 1.53e+07/8.97e+08 =  1% of the original kernel matrix.

torch.Size([817, 2])
We keep 1.55e+04/1.08e+05 = 14% of the original kernel matrix.

torch.Size([5618, 2])
We keep 4.29e+05/6.20e+06 =  6% of the original kernel matrix.

torch.Size([8816, 2])
We keep 1.59e+06/3.81e+07 =  4% of the original kernel matrix.

torch.Size([13677, 2])
We keep 3.02e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([4684, 2])
We keep 7.35e+05/9.57e+06 =  7% of the original kernel matrix.

torch.Size([10257, 2])
We keep 1.84e+06/5.85e+07 =  3% of the original kernel matrix.

torch.Size([8620, 2])
We keep 2.07e+06/4.40e+07 =  4% of the original kernel matrix.

torch.Size([13749, 2])
We keep 3.19e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([39330, 2])
We keep 2.16e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([29903, 2])
We keep 1.21e+07/6.86e+08 =  1% of the original kernel matrix.

torch.Size([5824, 2])
We keep 2.67e+06/1.83e+07 = 14% of the original kernel matrix.

torch.Size([11302, 2])
We keep 2.31e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([22393, 2])
We keep 1.02e+07/4.45e+08 =  2% of the original kernel matrix.

torch.Size([22463, 2])
We keep 7.85e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([1030, 2])
We keep 3.36e+04/2.28e+05 = 14% of the original kernel matrix.

torch.Size([5775, 2])
We keep 5.13e+05/9.02e+06 =  5% of the original kernel matrix.

torch.Size([4956, 2])
We keep 4.48e+05/8.51e+06 =  5% of the original kernel matrix.

torch.Size([10729, 2])
We keep 1.77e+06/5.52e+07 =  3% of the original kernel matrix.

torch.Size([13404, 2])
We keep 1.30e+07/1.51e+08 =  8% of the original kernel matrix.

torch.Size([17048, 2])
We keep 5.01e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([32490, 2])
We keep 3.03e+07/1.14e+09 =  2% of the original kernel matrix.

torch.Size([26601, 2])
We keep 1.19e+07/6.39e+08 =  1% of the original kernel matrix.

torch.Size([14612, 2])
We keep 6.21e+06/1.57e+08 =  3% of the original kernel matrix.

torch.Size([17574, 2])
We keep 5.16e+06/2.37e+08 =  2% of the original kernel matrix.

torch.Size([2947, 2])
We keep 2.23e+05/2.84e+06 =  7% of the original kernel matrix.

torch.Size([8535, 2])
We keep 1.21e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([135036, 2])
We keep 2.22e+08/1.98e+10 =  1% of the original kernel matrix.

torch.Size([55978, 2])
We keep 3.89e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([6503, 2])
We keep 7.92e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([11896, 2])
We keep 2.28e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([14568, 2])
We keep 3.96e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([17788, 2])
We keep 5.02e+06/2.23e+08 =  2% of the original kernel matrix.

torch.Size([52873, 2])
We keep 4.43e+07/2.71e+09 =  1% of the original kernel matrix.

torch.Size([34337, 2])
We keep 1.65e+07/9.85e+08 =  1% of the original kernel matrix.

torch.Size([2414, 2])
We keep 1.47e+05/1.66e+06 =  8% of the original kernel matrix.

torch.Size([8043, 2])
We keep 1.03e+06/2.44e+07 =  4% of the original kernel matrix.

torch.Size([22740, 2])
We keep 1.12e+07/4.57e+08 =  2% of the original kernel matrix.

torch.Size([22451, 2])
We keep 8.05e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([63881, 2])
We keep 1.30e+08/4.39e+09 =  2% of the original kernel matrix.

torch.Size([37334, 2])
We keep 2.02e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([90505, 2])
We keep 2.31e+08/8.95e+09 =  2% of the original kernel matrix.

torch.Size([44538, 2])
We keep 2.80e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([8799, 2])
We keep 1.63e+06/3.66e+07 =  4% of the original kernel matrix.

torch.Size([13679, 2])
We keep 2.99e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([13572, 2])
We keep 4.13e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([16829, 2])
We keep 4.74e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([5081, 2])
We keep 8.43e+05/1.21e+07 =  6% of the original kernel matrix.

torch.Size([10561, 2])
We keep 2.08e+06/6.58e+07 =  3% of the original kernel matrix.

torch.Size([4017, 2])
We keep 3.05e+05/5.36e+06 =  5% of the original kernel matrix.

torch.Size([9797, 2])
We keep 1.50e+06/4.38e+07 =  3% of the original kernel matrix.

torch.Size([15240, 2])
We keep 7.55e+06/1.83e+08 =  4% of the original kernel matrix.

torch.Size([18193, 2])
We keep 5.41e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([8990, 2])
We keep 1.56e+06/3.68e+07 =  4% of the original kernel matrix.

torch.Size([13789, 2])
We keep 3.02e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([17157, 2])
We keep 8.48e+06/2.32e+08 =  3% of the original kernel matrix.

torch.Size([19434, 2])
We keep 6.24e+06/2.88e+08 =  2% of the original kernel matrix.

torch.Size([42181, 2])
We keep 9.06e+07/2.17e+09 =  4% of the original kernel matrix.

torch.Size([30612, 2])
We keep 1.36e+07/8.81e+08 =  1% of the original kernel matrix.

torch.Size([15277, 2])
We keep 1.13e+07/1.84e+08 =  6% of the original kernel matrix.

torch.Size([18330, 2])
We keep 5.42e+06/2.57e+08 =  2% of the original kernel matrix.

torch.Size([4749, 2])
We keep 1.25e+06/1.43e+07 =  8% of the original kernel matrix.

torch.Size([10415, 2])
We keep 2.10e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([9777, 2])
We keep 1.69e+06/4.50e+07 =  3% of the original kernel matrix.

torch.Size([14288, 2])
We keep 3.21e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([14621, 2])
We keep 3.28e+07/7.21e+08 =  4% of the original kernel matrix.

torch.Size([16913, 2])
We keep 8.90e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([8745, 2])
We keep 1.83e+06/4.26e+07 =  4% of the original kernel matrix.

torch.Size([13766, 2])
We keep 3.14e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([2677, 2])
We keep 1.60e+05/2.15e+06 =  7% of the original kernel matrix.

torch.Size([8396, 2])
We keep 1.10e+06/2.77e+07 =  3% of the original kernel matrix.

torch.Size([12518, 2])
We keep 3.49e+06/9.56e+07 =  3% of the original kernel matrix.

torch.Size([16351, 2])
We keep 4.41e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([261334, 2])
We keep 1.15e+09/7.77e+10 =  1% of the original kernel matrix.

torch.Size([77605, 2])
We keep 7.10e+07/5.27e+09 =  1% of the original kernel matrix.

torch.Size([5957, 2])
We keep 1.14e+06/1.64e+07 =  6% of the original kernel matrix.

torch.Size([11390, 2])
We keep 2.23e+06/7.66e+07 =  2% of the original kernel matrix.

torch.Size([41159, 2])
We keep 1.01e+08/2.44e+09 =  4% of the original kernel matrix.

torch.Size([29280, 2])
We keep 1.56e+07/9.34e+08 =  1% of the original kernel matrix.

torch.Size([3202, 2])
We keep 3.42e+05/4.07e+06 =  8% of the original kernel matrix.

torch.Size([8906, 2])
We keep 1.37e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([7808, 2])
We keep 6.32e+06/9.19e+07 =  6% of the original kernel matrix.

torch.Size([12156, 2])
We keep 4.31e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([11320, 2])
We keep 3.26e+06/8.12e+07 =  4% of the original kernel matrix.

torch.Size([15766, 2])
We keep 3.85e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([11416, 2])
We keep 4.78e+07/2.55e+08 = 18% of the original kernel matrix.

torch.Size([15359, 2])
We keep 5.90e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([25234, 2])
We keep 1.16e+07/5.90e+08 =  1% of the original kernel matrix.

torch.Size([23575, 2])
We keep 8.79e+06/4.60e+08 =  1% of the original kernel matrix.

torch.Size([3965, 2])
We keep 3.29e+05/5.26e+06 =  6% of the original kernel matrix.

torch.Size([9826, 2])
We keep 1.53e+06/4.34e+07 =  3% of the original kernel matrix.

torch.Size([66458, 2])
We keep 5.25e+07/3.59e+09 =  1% of the original kernel matrix.

torch.Size([38342, 2])
We keep 1.86e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([54968, 2])
We keep 4.17e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([35054, 2])
We keep 1.64e+07/9.79e+08 =  1% of the original kernel matrix.

torch.Size([3418, 2])
We keep 2.84e+05/4.72e+06 =  6% of the original kernel matrix.

torch.Size([9291, 2])
We keep 1.39e+06/4.11e+07 =  3% of the original kernel matrix.

torch.Size([26065, 2])
We keep 1.39e+07/6.36e+08 =  2% of the original kernel matrix.

torch.Size([24636, 2])
We keep 9.05e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([63711, 2])
We keep 4.75e+07/3.36e+09 =  1% of the original kernel matrix.

torch.Size([37767, 2])
We keep 1.81e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([162577, 2])
We keep 3.11e+08/2.61e+10 =  1% of the original kernel matrix.

torch.Size([61926, 2])
We keep 4.43e+07/3.05e+09 =  1% of the original kernel matrix.

torch.Size([17556, 2])
We keep 6.56e+07/2.62e+08 = 25% of the original kernel matrix.

torch.Size([19703, 2])
We keep 6.23e+06/3.06e+08 =  2% of the original kernel matrix.

torch.Size([4818, 2])
We keep 3.10e+06/2.12e+07 = 14% of the original kernel matrix.

torch.Size([10069, 2])
We keep 2.61e+06/8.70e+07 =  3% of the original kernel matrix.

torch.Size([5152, 2])
We keep 5.29e+05/9.96e+06 =  5% of the original kernel matrix.

torch.Size([10724, 2])
We keep 1.88e+06/5.97e+07 =  3% of the original kernel matrix.

torch.Size([8251, 2])
We keep 2.63e+06/5.29e+07 =  4% of the original kernel matrix.

torch.Size([13508, 2])
We keep 3.31e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([27627, 2])
We keep 2.63e+07/6.79e+08 =  3% of the original kernel matrix.

torch.Size([24958, 2])
We keep 9.61e+06/4.93e+08 =  1% of the original kernel matrix.

torch.Size([4768, 2])
We keep 4.63e+05/8.16e+06 =  5% of the original kernel matrix.

torch.Size([10596, 2])
We keep 1.74e+06/5.40e+07 =  3% of the original kernel matrix.

torch.Size([2107, 2])
We keep 1.10e+05/1.37e+06 =  8% of the original kernel matrix.

torch.Size([7614, 2])
We keep 9.71e+05/2.22e+07 =  4% of the original kernel matrix.

torch.Size([42477, 2])
We keep 3.62e+07/1.68e+09 =  2% of the original kernel matrix.

torch.Size([31200, 2])
We keep 1.36e+07/7.77e+08 =  1% of the original kernel matrix.

torch.Size([61570, 2])
We keep 5.77e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([36525, 2])
We keep 1.85e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([35686, 2])
We keep 8.21e+07/2.55e+09 =  3% of the original kernel matrix.

torch.Size([24924, 2])
We keep 1.66e+07/9.56e+08 =  1% of the original kernel matrix.

torch.Size([3791, 2])
We keep 3.24e+05/4.88e+06 =  6% of the original kernel matrix.

torch.Size([9616, 2])
We keep 1.48e+06/4.18e+07 =  3% of the original kernel matrix.

torch.Size([10223, 2])
We keep 3.40e+06/7.37e+07 =  4% of the original kernel matrix.

torch.Size([14628, 2])
We keep 3.90e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([16207, 2])
We keep 5.98e+06/1.86e+08 =  3% of the original kernel matrix.

torch.Size([18841, 2])
We keep 5.75e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([7538, 2])
We keep 1.30e+06/2.43e+07 =  5% of the original kernel matrix.

torch.Size([12697, 2])
We keep 2.53e+06/9.32e+07 =  2% of the original kernel matrix.

torch.Size([6562, 2])
We keep 8.68e+05/1.86e+07 =  4% of the original kernel matrix.

torch.Size([12031, 2])
We keep 2.36e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([3749, 2])
We keep 4.00e+05/5.56e+06 =  7% of the original kernel matrix.

torch.Size([9473, 2])
We keep 1.52e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([77839, 2])
We keep 2.64e+08/1.03e+10 =  2% of the original kernel matrix.

torch.Size([40162, 2])
We keep 3.02e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([23791, 2])
We keep 9.18e+06/4.59e+08 =  2% of the original kernel matrix.

torch.Size([23298, 2])
We keep 7.95e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([8542, 2])
We keep 1.40e+06/3.40e+07 =  4% of the original kernel matrix.

torch.Size([13557, 2])
We keep 2.88e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([7610, 2])
We keep 1.71e+06/2.58e+07 =  6% of the original kernel matrix.

torch.Size([12804, 2])
We keep 2.56e+06/9.61e+07 =  2% of the original kernel matrix.

torch.Size([3099, 2])
We keep 3.90e+05/3.79e+06 = 10% of the original kernel matrix.

torch.Size([8743, 2])
We keep 1.29e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([214310, 2])
We keep 5.43e+08/4.42e+10 =  1% of the original kernel matrix.

torch.Size([71969, 2])
We keep 5.49e+07/3.98e+09 =  1% of the original kernel matrix.

torch.Size([5097, 2])
We keep 5.63e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([10633, 2])
We keep 1.89e+06/6.03e+07 =  3% of the original kernel matrix.

torch.Size([56773, 2])
We keep 5.73e+07/2.85e+09 =  2% of the original kernel matrix.

torch.Size([35363, 2])
We keep 1.70e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([1490, 2])
We keep 6.95e+04/7.62e+05 =  9% of the original kernel matrix.

torch.Size([6628, 2])
We keep 7.41e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([2437, 2])
We keep 1.42e+05/1.85e+06 =  7% of the original kernel matrix.

torch.Size([8093, 2])
We keep 1.04e+06/2.57e+07 =  4% of the original kernel matrix.

torch.Size([3572, 2])
We keep 2.69e+05/4.39e+06 =  6% of the original kernel matrix.

torch.Size([9343, 2])
We keep 1.41e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([13419, 2])
We keep 4.62e+06/1.10e+08 =  4% of the original kernel matrix.

torch.Size([17064, 2])
We keep 4.37e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([10693, 2])
We keep 6.70e+06/8.22e+07 =  8% of the original kernel matrix.

torch.Size([15137, 2])
We keep 4.27e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([5796, 2])
We keep 8.02e+05/1.46e+07 =  5% of the original kernel matrix.

torch.Size([11364, 2])
We keep 2.13e+06/7.23e+07 =  2% of the original kernel matrix.

torch.Size([4471, 2])
We keep 4.13e+05/7.24e+06 =  5% of the original kernel matrix.

torch.Size([10186, 2])
We keep 1.67e+06/5.09e+07 =  3% of the original kernel matrix.

torch.Size([3909, 2])
We keep 3.39e+05/5.21e+06 =  6% of the original kernel matrix.

torch.Size([9603, 2])
We keep 1.51e+06/4.32e+07 =  3% of the original kernel matrix.

torch.Size([152555, 2])
We keep 2.93e+08/2.16e+10 =  1% of the original kernel matrix.

torch.Size([59598, 2])
We keep 4.02e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([19295, 2])
We keep 8.36e+06/2.79e+08 =  3% of the original kernel matrix.

torch.Size([20767, 2])
We keep 6.61e+06/3.16e+08 =  2% of the original kernel matrix.

torch.Size([11649, 2])
We keep 4.26e+06/8.33e+07 =  5% of the original kernel matrix.

torch.Size([15866, 2])
We keep 4.11e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([31463, 2])
We keep 2.34e+07/8.66e+08 =  2% of the original kernel matrix.

torch.Size([26965, 2])
We keep 1.04e+07/5.57e+08 =  1% of the original kernel matrix.

torch.Size([18944, 2])
We keep 4.72e+07/8.51e+08 =  5% of the original kernel matrix.

torch.Size([19819, 2])
We keep 1.02e+07/5.52e+08 =  1% of the original kernel matrix.

torch.Size([3126, 2])
We keep 1.99e+05/3.10e+06 =  6% of the original kernel matrix.

torch.Size([8896, 2])
We keep 1.24e+06/3.33e+07 =  3% of the original kernel matrix.

torch.Size([3689, 2])
We keep 3.02e+05/4.71e+06 =  6% of the original kernel matrix.

torch.Size([9394, 2])
We keep 1.43e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([4275, 2])
We keep 7.82e+05/8.74e+06 =  8% of the original kernel matrix.

torch.Size([9968, 2])
We keep 1.73e+06/5.59e+07 =  3% of the original kernel matrix.

torch.Size([668, 2])
We keep 9.95e+03/6.66e+04 = 14% of the original kernel matrix.

torch.Size([5173, 2])
We keep 3.48e+05/4.88e+06 =  7% of the original kernel matrix.

torch.Size([3031, 2])
We keep 1.86e+05/2.79e+06 =  6% of the original kernel matrix.

torch.Size([8774, 2])
We keep 1.20e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([12665, 2])
We keep 5.57e+06/1.37e+08 =  4% of the original kernel matrix.

torch.Size([16390, 2])
We keep 5.03e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([2994, 2])
We keep 2.69e+05/2.94e+06 =  9% of the original kernel matrix.

torch.Size([8577, 2])
We keep 1.23e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([2680, 2])
We keep 1.52e+05/2.06e+06 =  7% of the original kernel matrix.

torch.Size([8408, 2])
We keep 1.09e+06/2.72e+07 =  4% of the original kernel matrix.

torch.Size([3638, 2])
We keep 3.39e+05/4.52e+06 =  7% of the original kernel matrix.

torch.Size([9410, 2])
We keep 1.45e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([4849, 2])
We keep 4.93e+05/8.73e+06 =  5% of the original kernel matrix.

torch.Size([10572, 2])
We keep 1.77e+06/5.59e+07 =  3% of the original kernel matrix.

torch.Size([232437, 2])
We keep 1.40e+09/8.10e+10 =  1% of the original kernel matrix.

torch.Size([71871, 2])
We keep 7.38e+07/5.38e+09 =  1% of the original kernel matrix.

torch.Size([7875, 2])
We keep 1.84e+06/3.40e+07 =  5% of the original kernel matrix.

torch.Size([12946, 2])
We keep 2.94e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([22481, 2])
We keep 1.11e+07/4.23e+08 =  2% of the original kernel matrix.

torch.Size([22530, 2])
We keep 7.74e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([13596, 2])
We keep 9.01e+06/1.88e+08 =  4% of the original kernel matrix.

torch.Size([16996, 2])
We keep 5.78e+06/2.59e+08 =  2% of the original kernel matrix.

torch.Size([3384, 2])
We keep 2.96e+05/4.33e+06 =  6% of the original kernel matrix.

torch.Size([9093, 2])
We keep 1.41e+06/3.94e+07 =  3% of the original kernel matrix.

torch.Size([7990, 2])
We keep 2.07e+06/3.37e+07 =  6% of the original kernel matrix.

torch.Size([13081, 2])
We keep 2.93e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([260526, 2])
We keep 2.05e+09/8.52e+10 =  2% of the original kernel matrix.

torch.Size([77868, 2])
We keep 7.17e+07/5.52e+09 =  1% of the original kernel matrix.

torch.Size([348400, 2])
We keep 9.18e+08/1.04e+11 =  0% of the original kernel matrix.

torch.Size([92284, 2])
We keep 8.13e+07/6.10e+09 =  1% of the original kernel matrix.

torch.Size([23426, 2])
We keep 1.24e+07/4.57e+08 =  2% of the original kernel matrix.

torch.Size([22987, 2])
We keep 7.99e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([14721, 2])
We keep 4.84e+06/1.35e+08 =  3% of the original kernel matrix.

torch.Size([17908, 2])
We keep 4.94e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([1485, 2])
We keep 6.64e+04/6.81e+05 =  9% of the original kernel matrix.

torch.Size([6805, 2])
We keep 7.70e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([31083, 2])
We keep 6.14e+07/1.71e+09 =  3% of the original kernel matrix.

torch.Size([24779, 2])
We keep 1.40e+07/7.82e+08 =  1% of the original kernel matrix.

torch.Size([22982, 2])
We keep 1.88e+07/4.78e+08 =  3% of the original kernel matrix.

torch.Size([22680, 2])
We keep 7.86e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([89988, 2])
We keep 2.10e+08/9.33e+09 =  2% of the original kernel matrix.

torch.Size([44656, 2])
We keep 2.90e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([5555, 2])
We keep 7.73e+05/1.43e+07 =  5% of the original kernel matrix.

torch.Size([10968, 2])
We keep 2.18e+06/7.16e+07 =  3% of the original kernel matrix.

torch.Size([65921, 2])
We keep 7.35e+07/3.95e+09 =  1% of the original kernel matrix.

torch.Size([37889, 2])
We keep 1.96e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([81259, 2])
We keep 1.24e+08/6.32e+09 =  1% of the original kernel matrix.

torch.Size([41881, 2])
We keep 2.33e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([26127, 2])
We keep 1.60e+07/6.71e+08 =  2% of the original kernel matrix.

torch.Size([23886, 2])
We keep 9.27e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([3891, 2])
We keep 5.25e+05/7.02e+06 =  7% of the original kernel matrix.

torch.Size([9446, 2])
We keep 1.64e+06/5.01e+07 =  3% of the original kernel matrix.

torch.Size([15181, 2])
We keep 3.91e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([18276, 2])
We keep 5.05e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([5351, 2])
We keep 5.91e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([11044, 2])
We keep 1.99e+06/6.48e+07 =  3% of the original kernel matrix.

torch.Size([2731, 2])
We keep 1.57e+05/2.26e+06 =  6% of the original kernel matrix.

torch.Size([8529, 2])
We keep 1.10e+06/2.84e+07 =  3% of the original kernel matrix.

torch.Size([52148, 2])
We keep 5.98e+07/2.80e+09 =  2% of the original kernel matrix.

torch.Size([34056, 2])
We keep 1.68e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([11173, 2])
We keep 4.76e+06/1.11e+08 =  4% of the original kernel matrix.

torch.Size([14967, 2])
We keep 4.61e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([1130, 2])
We keep 3.73e+04/3.29e+05 = 11% of the original kernel matrix.

torch.Size([6050, 2])
We keep 6.09e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([12485, 2])
We keep 3.07e+06/9.00e+07 =  3% of the original kernel matrix.

torch.Size([16333, 2])
We keep 4.16e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([11008, 2])
We keep 2.33e+06/6.06e+07 =  3% of the original kernel matrix.

torch.Size([15256, 2])
We keep 3.64e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([1013, 2])
We keep 2.46e+04/2.03e+05 = 12% of the original kernel matrix.

torch.Size([5975, 2])
We keep 5.10e+05/8.53e+06 =  5% of the original kernel matrix.

torch.Size([26988, 2])
We keep 4.34e+07/7.95e+08 =  5% of the original kernel matrix.

torch.Size([24659, 2])
We keep 9.72e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([58101, 2])
We keep 4.59e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([36055, 2])
We keep 1.65e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([14972, 2])
We keep 5.67e+06/1.36e+08 =  4% of the original kernel matrix.

torch.Size([18077, 2])
We keep 5.02e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([37990, 2])
We keep 5.03e+07/1.40e+09 =  3% of the original kernel matrix.

torch.Size([29157, 2])
We keep 1.26e+07/7.07e+08 =  1% of the original kernel matrix.

torch.Size([33279, 2])
We keep 3.36e+07/1.08e+09 =  3% of the original kernel matrix.

torch.Size([27349, 2])
We keep 1.14e+07/6.23e+08 =  1% of the original kernel matrix.

torch.Size([4572, 2])
We keep 9.26e+05/1.13e+07 =  8% of the original kernel matrix.

torch.Size([10038, 2])
We keep 1.90e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([16019, 2])
We keep 6.46e+06/1.84e+08 =  3% of the original kernel matrix.

torch.Size([18617, 2])
We keep 5.63e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([34342, 2])
We keep 1.81e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([28328, 2])
We keep 1.14e+07/6.31e+08 =  1% of the original kernel matrix.

torch.Size([80221, 2])
We keep 7.89e+07/5.05e+09 =  1% of the original kernel matrix.

torch.Size([41697, 2])
We keep 2.16e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([4487, 2])
We keep 3.69e+05/6.62e+06 =  5% of the original kernel matrix.

torch.Size([10330, 2])
We keep 1.61e+06/4.87e+07 =  3% of the original kernel matrix.

torch.Size([37315, 2])
We keep 7.39e+07/1.66e+09 =  4% of the original kernel matrix.

torch.Size([28898, 2])
We keep 1.24e+07/7.72e+08 =  1% of the original kernel matrix.

torch.Size([61146, 2])
We keep 1.60e+08/4.92e+09 =  3% of the original kernel matrix.

torch.Size([36639, 2])
We keep 1.97e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([16249, 2])
We keep 5.43e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([18987, 2])
We keep 5.38e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([10114, 2])
We keep 7.02e+06/1.20e+08 =  5% of the original kernel matrix.

torch.Size([14778, 2])
We keep 4.16e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([4950, 2])
We keep 5.57e+06/1.75e+07 = 31% of the original kernel matrix.

torch.Size([10379, 2])
We keep 1.97e+06/7.92e+07 =  2% of the original kernel matrix.

torch.Size([8051, 2])
We keep 1.29e+06/2.90e+07 =  4% of the original kernel matrix.

torch.Size([13183, 2])
We keep 2.76e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([3595, 2])
We keep 2.71e+05/4.40e+06 =  6% of the original kernel matrix.

torch.Size([9306, 2])
We keep 1.38e+06/3.97e+07 =  3% of the original kernel matrix.

torch.Size([14157, 2])
We keep 1.96e+07/2.86e+08 =  6% of the original kernel matrix.

torch.Size([16889, 2])
We keep 6.80e+06/3.20e+08 =  2% of the original kernel matrix.

torch.Size([90672, 2])
We keep 9.16e+07/6.57e+09 =  1% of the original kernel matrix.

torch.Size([44188, 2])
We keep 2.41e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([50318, 2])
We keep 3.59e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([33630, 2])
We keep 1.54e+07/9.01e+08 =  1% of the original kernel matrix.

torch.Size([254691, 2])
We keep 1.04e+09/6.82e+10 =  1% of the original kernel matrix.

torch.Size([77242, 2])
We keep 6.65e+07/4.94e+09 =  1% of the original kernel matrix.

torch.Size([26425, 2])
We keep 3.98e+07/6.73e+08 =  5% of the original kernel matrix.

torch.Size([24328, 2])
We keep 9.16e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([50291, 2])
We keep 5.71e+07/2.49e+09 =  2% of the original kernel matrix.

torch.Size([33255, 2])
We keep 1.64e+07/9.45e+08 =  1% of the original kernel matrix.

torch.Size([10170, 2])
We keep 2.53e+06/7.85e+07 =  3% of the original kernel matrix.

torch.Size([14906, 2])
We keep 3.96e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([63012, 2])
We keep 6.10e+07/3.34e+09 =  1% of the original kernel matrix.

torch.Size([37353, 2])
We keep 1.81e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([1662, 2])
We keep 7.28e+04/7.33e+05 =  9% of the original kernel matrix.

torch.Size([7054, 2])
We keep 7.53e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([7596, 2])
We keep 2.00e+06/3.64e+07 =  5% of the original kernel matrix.

torch.Size([12545, 2])
We keep 3.05e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([104246, 2])
We keep 1.66e+08/9.57e+09 =  1% of the original kernel matrix.

torch.Size([48191, 2])
We keep 2.84e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([10749, 2])
We keep 3.06e+06/6.08e+07 =  5% of the original kernel matrix.

torch.Size([15056, 2])
We keep 3.56e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([2531, 2])
We keep 1.52e+05/2.01e+06 =  7% of the original kernel matrix.

torch.Size([8171, 2])
We keep 1.06e+06/2.68e+07 =  3% of the original kernel matrix.

torch.Size([7617, 2])
We keep 2.34e+06/3.69e+07 =  6% of the original kernel matrix.

torch.Size([12952, 2])
We keep 3.06e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([16765, 2])
We keep 1.15e+07/2.25e+08 =  5% of the original kernel matrix.

torch.Size([19118, 2])
We keep 5.83e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([15296, 2])
We keep 9.03e+06/2.09e+08 =  4% of the original kernel matrix.

torch.Size([18015, 2])
We keep 6.00e+06/2.74e+08 =  2% of the original kernel matrix.

torch.Size([37900, 2])
We keep 3.21e+07/1.41e+09 =  2% of the original kernel matrix.

torch.Size([29431, 2])
We keep 1.28e+07/7.11e+08 =  1% of the original kernel matrix.

torch.Size([10167, 2])
We keep 9.79e+06/1.29e+08 =  7% of the original kernel matrix.

torch.Size([14370, 2])
We keep 5.03e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([14652, 2])
We keep 1.19e+07/2.26e+08 =  5% of the original kernel matrix.

torch.Size([17381, 2])
We keep 6.11e+06/2.85e+08 =  2% of the original kernel matrix.

torch.Size([2453, 2])
We keep 1.32e+05/1.69e+06 =  7% of the original kernel matrix.

torch.Size([8149, 2])
We keep 9.95e+05/2.46e+07 =  4% of the original kernel matrix.

torch.Size([6588, 2])
We keep 1.11e+06/1.84e+07 =  6% of the original kernel matrix.

torch.Size([12117, 2])
We keep 2.31e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([15422, 2])
We keep 5.30e+06/1.65e+08 =  3% of the original kernel matrix.

torch.Size([18267, 2])
We keep 5.38e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([16423, 2])
We keep 3.47e+07/3.17e+08 = 10% of the original kernel matrix.

torch.Size([18711, 2])
We keep 6.85e+06/3.37e+08 =  2% of the original kernel matrix.

torch.Size([1790, 2])
We keep 6.51e+04/7.80e+05 =  8% of the original kernel matrix.

torch.Size([7308, 2])
We keep 7.81e+05/1.67e+07 =  4% of the original kernel matrix.

torch.Size([71634, 2])
We keep 8.16e+07/4.50e+09 =  1% of the original kernel matrix.

torch.Size([39624, 2])
We keep 2.03e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([1482, 2])
We keep 5.33e+04/5.30e+05 = 10% of the original kernel matrix.

torch.Size([6797, 2])
We keep 6.95e+05/1.38e+07 =  5% of the original kernel matrix.

torch.Size([11926, 2])
We keep 3.10e+06/7.80e+07 =  3% of the original kernel matrix.

torch.Size([15876, 2])
We keep 4.08e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([2638, 2])
We keep 2.10e+05/2.71e+06 =  7% of the original kernel matrix.

torch.Size([8316, 2])
We keep 1.17e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([23007, 2])
We keep 8.68e+06/4.22e+08 =  2% of the original kernel matrix.

torch.Size([23136, 2])
We keep 7.76e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([2884, 2])
We keep 1.71e+05/2.59e+06 =  6% of the original kernel matrix.

torch.Size([8603, 2])
We keep 1.16e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([16338, 2])
We keep 1.67e+07/2.26e+08 =  7% of the original kernel matrix.

torch.Size([18888, 2])
We keep 6.04e+06/2.85e+08 =  2% of the original kernel matrix.

torch.Size([8523, 2])
We keep 1.83e+06/3.78e+07 =  4% of the original kernel matrix.

torch.Size([13494, 2])
We keep 3.10e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([13100, 2])
We keep 3.83e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([16757, 2])
We keep 4.55e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([27046, 2])
We keep 1.93e+07/5.99e+08 =  3% of the original kernel matrix.

torch.Size([24658, 2])
We keep 8.72e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([112413, 2])
We keep 2.25e+08/1.28e+10 =  1% of the original kernel matrix.

torch.Size([50399, 2])
We keep 3.18e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([3249, 2])
We keep 2.33e+05/3.79e+06 =  6% of the original kernel matrix.

torch.Size([9016, 2])
We keep 1.34e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([7390, 2])
We keep 6.20e+06/6.58e+07 =  9% of the original kernel matrix.

torch.Size([11984, 2])
We keep 3.89e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([52816, 2])
We keep 8.73e+07/3.48e+09 =  2% of the original kernel matrix.

torch.Size([33328, 2])
We keep 1.87e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([5004, 2])
We keep 6.53e+05/9.95e+06 =  6% of the original kernel matrix.

torch.Size([10642, 2])
We keep 1.87e+06/5.97e+07 =  3% of the original kernel matrix.

torch.Size([230060, 2])
We keep 5.44e+08/5.19e+10 =  1% of the original kernel matrix.

torch.Size([74922, 2])
We keep 5.97e+07/4.31e+09 =  1% of the original kernel matrix.

torch.Size([1405, 2])
We keep 7.13e+04/6.58e+05 = 10% of the original kernel matrix.

torch.Size([6478, 2])
We keep 7.55e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([57639, 2])
We keep 5.86e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([35211, 2])
We keep 1.78e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([141107, 2])
We keep 2.18e+08/1.89e+10 =  1% of the original kernel matrix.

torch.Size([57247, 2])
We keep 3.84e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([2477, 2])
We keep 1.48e+05/1.95e+06 =  7% of the original kernel matrix.

torch.Size([8091, 2])
We keep 1.07e+06/2.64e+07 =  4% of the original kernel matrix.

torch.Size([88413, 2])
We keep 1.83e+08/9.55e+09 =  1% of the original kernel matrix.

torch.Size([43763, 2])
We keep 2.80e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([6176, 2])
We keep 3.42e+06/4.80e+07 =  7% of the original kernel matrix.

torch.Size([11301, 2])
We keep 3.05e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([79761, 2])
We keep 1.89e+08/7.26e+09 =  2% of the original kernel matrix.

torch.Size([41401, 2])
We keep 2.52e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([5628, 2])
We keep 6.72e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([11215, 2])
We keep 2.06e+06/6.68e+07 =  3% of the original kernel matrix.

torch.Size([30348, 2])
We keep 3.92e+07/1.33e+09 =  2% of the original kernel matrix.

torch.Size([25196, 2])
We keep 1.27e+07/6.90e+08 =  1% of the original kernel matrix.

torch.Size([1133, 2])
We keep 3.02e+04/2.46e+05 = 12% of the original kernel matrix.

torch.Size([6185, 2])
We keep 5.26e+05/9.38e+06 =  5% of the original kernel matrix.

torch.Size([4657, 2])
We keep 4.47e+05/7.94e+06 =  5% of the original kernel matrix.

torch.Size([10391, 2])
We keep 1.73e+06/5.33e+07 =  3% of the original kernel matrix.

torch.Size([2990, 2])
We keep 2.51e+05/3.24e+06 =  7% of the original kernel matrix.

torch.Size([8655, 2])
We keep 1.23e+06/3.41e+07 =  3% of the original kernel matrix.

torch.Size([19092, 2])
We keep 1.01e+07/2.87e+08 =  3% of the original kernel matrix.

torch.Size([20562, 2])
We keep 6.52e+06/3.20e+08 =  2% of the original kernel matrix.

torch.Size([10109, 2])
We keep 3.73e+06/8.05e+07 =  4% of the original kernel matrix.

torch.Size([14524, 2])
We keep 4.09e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([66211, 2])
We keep 1.25e+08/4.04e+09 =  3% of the original kernel matrix.

torch.Size([38083, 2])
We keep 1.96e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([9822, 2])
We keep 1.94e+06/4.90e+07 =  3% of the original kernel matrix.

torch.Size([14367, 2])
We keep 3.38e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([2418, 2])
We keep 1.29e+05/1.59e+06 =  8% of the original kernel matrix.

torch.Size([8013, 2])
We keep 1.01e+06/2.38e+07 =  4% of the original kernel matrix.

torch.Size([3758, 2])
We keep 3.97e+05/5.12e+06 =  7% of the original kernel matrix.

torch.Size([9515, 2])
We keep 1.49e+06/4.28e+07 =  3% of the original kernel matrix.

torch.Size([17066, 2])
We keep 3.02e+07/2.35e+08 = 12% of the original kernel matrix.

torch.Size([19398, 2])
We keep 5.98e+06/2.90e+08 =  2% of the original kernel matrix.

torch.Size([2917, 2])
We keep 4.55e+05/3.39e+06 = 13% of the original kernel matrix.

torch.Size([8565, 2])
We keep 1.27e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([77592, 2])
We keep 6.58e+07/4.78e+09 =  1% of the original kernel matrix.

torch.Size([40957, 2])
We keep 2.09e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([49580, 2])
We keep 7.15e+07/2.62e+09 =  2% of the original kernel matrix.

torch.Size([32986, 2])
We keep 1.68e+07/9.68e+08 =  1% of the original kernel matrix.

torch.Size([6557, 2])
We keep 1.84e+06/2.52e+07 =  7% of the original kernel matrix.

torch.Size([12034, 2])
We keep 2.53e+06/9.50e+07 =  2% of the original kernel matrix.

torch.Size([2212, 2])
We keep 1.21e+05/1.52e+06 =  7% of the original kernel matrix.

torch.Size([7773, 2])
We keep 9.82e+05/2.33e+07 =  4% of the original kernel matrix.

torch.Size([7886, 2])
We keep 1.13e+07/6.12e+07 = 18% of the original kernel matrix.

torch.Size([12864, 2])
We keep 3.69e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([536325, 2])
We keep 2.77e+09/2.43e+11 =  1% of the original kernel matrix.

torch.Size([115215, 2])
We keep 1.21e+08/9.33e+09 =  1% of the original kernel matrix.

torch.Size([7121, 2])
We keep 1.04e+06/2.23e+07 =  4% of the original kernel matrix.

torch.Size([12495, 2])
We keep 2.50e+06/8.92e+07 =  2% of the original kernel matrix.

torch.Size([9703, 2])
We keep 2.64e+06/5.13e+07 =  5% of the original kernel matrix.

torch.Size([14374, 2])
We keep 3.45e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([43343, 2])
We keep 2.08e+08/2.86e+09 =  7% of the original kernel matrix.

torch.Size([29531, 2])
We keep 1.73e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([13112, 2])
We keep 1.04e+07/2.21e+08 =  4% of the original kernel matrix.

torch.Size([16272, 2])
We keep 6.05e+06/2.81e+08 =  2% of the original kernel matrix.

torch.Size([2394, 2])
We keep 1.54e+05/1.67e+06 =  9% of the original kernel matrix.

torch.Size([8036, 2])
We keep 1.03e+06/2.45e+07 =  4% of the original kernel matrix.

torch.Size([2586, 2])
We keep 1.87e+05/2.30e+06 =  8% of the original kernel matrix.

torch.Size([8158, 2])
We keep 1.12e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([79118, 2])
We keep 3.26e+08/8.81e+09 =  3% of the original kernel matrix.

torch.Size([41193, 2])
We keep 2.70e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([14933, 2])
We keep 4.68e+06/1.41e+08 =  3% of the original kernel matrix.

torch.Size([18095, 2])
We keep 5.06e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([24659, 2])
We keep 2.53e+07/7.68e+08 =  3% of the original kernel matrix.

torch.Size([22690, 2])
We keep 9.80e+06/5.24e+08 =  1% of the original kernel matrix.

torch.Size([28815, 2])
We keep 2.07e+07/7.46e+08 =  2% of the original kernel matrix.

torch.Size([25798, 2])
We keep 9.77e+06/5.17e+08 =  1% of the original kernel matrix.

torch.Size([9873, 2])
We keep 2.56e+06/4.90e+07 =  5% of the original kernel matrix.

torch.Size([14538, 2])
We keep 3.31e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([22213, 2])
We keep 1.23e+07/4.12e+08 =  2% of the original kernel matrix.

torch.Size([22420, 2])
We keep 7.57e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([73179, 2])
We keep 1.11e+08/5.20e+09 =  2% of the original kernel matrix.

torch.Size([39639, 2])
We keep 2.14e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([45310, 2])
We keep 3.03e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([32174, 2])
We keep 1.39e+07/7.92e+08 =  1% of the original kernel matrix.

torch.Size([4173, 2])
We keep 4.43e+05/6.98e+06 =  6% of the original kernel matrix.

torch.Size([10020, 2])
We keep 1.61e+06/5.00e+07 =  3% of the original kernel matrix.

torch.Size([27771, 2])
We keep 1.21e+07/6.66e+08 =  1% of the original kernel matrix.

torch.Size([25138, 2])
We keep 9.21e+06/4.88e+08 =  1% of the original kernel matrix.

torch.Size([4907, 2])
We keep 5.29e+05/8.93e+06 =  5% of the original kernel matrix.

torch.Size([10590, 2])
We keep 1.75e+06/5.65e+07 =  3% of the original kernel matrix.

torch.Size([7588, 2])
We keep 1.52e+06/2.79e+07 =  5% of the original kernel matrix.

torch.Size([12836, 2])
We keep 2.70e+06/9.99e+07 =  2% of the original kernel matrix.

torch.Size([14663, 2])
We keep 5.34e+06/1.53e+08 =  3% of the original kernel matrix.

torch.Size([17913, 2])
We keep 5.27e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([4349, 2])
We keep 4.20e+05/7.72e+06 =  5% of the original kernel matrix.

torch.Size([10163, 2])
We keep 1.67e+06/5.26e+07 =  3% of the original kernel matrix.

torch.Size([28778, 2])
We keep 6.08e+07/1.54e+09 =  3% of the original kernel matrix.

torch.Size([24239, 2])
We keep 1.32e+07/7.43e+08 =  1% of the original kernel matrix.

torch.Size([10185, 2])
We keep 2.27e+06/5.80e+07 =  3% of the original kernel matrix.

torch.Size([14655, 2])
We keep 3.62e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([2155, 2])
We keep 1.50e+05/1.47e+06 = 10% of the original kernel matrix.

torch.Size([7511, 2])
We keep 9.91e+05/2.29e+07 =  4% of the original kernel matrix.

torch.Size([11501, 2])
We keep 3.11e+06/7.98e+07 =  3% of the original kernel matrix.

torch.Size([15537, 2])
We keep 4.11e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([3437, 2])
We keep 2.38e+06/1.94e+07 = 12% of the original kernel matrix.

torch.Size([8613, 2])
We keep 2.02e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([31625, 2])
We keep 7.46e+07/1.63e+09 =  4% of the original kernel matrix.

torch.Size([25289, 2])
We keep 1.34e+07/7.63e+08 =  1% of the original kernel matrix.

torch.Size([36797, 2])
We keep 1.79e+08/2.74e+09 =  6% of the original kernel matrix.

torch.Size([26391, 2])
We keep 1.74e+07/9.90e+08 =  1% of the original kernel matrix.

torch.Size([5435, 2])
We keep 1.22e+06/2.15e+07 =  5% of the original kernel matrix.

torch.Size([10439, 2])
We keep 2.44e+06/8.77e+07 =  2% of the original kernel matrix.

torch.Size([4552, 2])
We keep 1.49e+06/1.49e+07 = 10% of the original kernel matrix.

torch.Size([9746, 2])
We keep 2.14e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([13918, 2])
We keep 6.27e+06/1.31e+08 =  4% of the original kernel matrix.

torch.Size([17450, 2])
We keep 4.86e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([2453, 2])
We keep 1.55e+05/2.03e+06 =  7% of the original kernel matrix.

torch.Size([8161, 2])
We keep 1.09e+06/2.70e+07 =  4% of the original kernel matrix.

torch.Size([3866, 2])
We keep 3.44e+05/5.88e+06 =  5% of the original kernel matrix.

torch.Size([9725, 2])
We keep 1.56e+06/4.59e+07 =  3% of the original kernel matrix.

torch.Size([16868, 2])
We keep 5.65e+06/1.93e+08 =  2% of the original kernel matrix.

torch.Size([19336, 2])
We keep 5.64e+06/2.62e+08 =  2% of the original kernel matrix.

torch.Size([90610, 2])
We keep 1.61e+08/7.67e+09 =  2% of the original kernel matrix.

torch.Size([44857, 2])
We keep 2.62e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([3683, 2])
We keep 3.02e+05/4.47e+06 =  6% of the original kernel matrix.

torch.Size([9414, 2])
We keep 1.41e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([1570, 2])
We keep 6.30e+04/5.55e+05 = 11% of the original kernel matrix.

torch.Size([6868, 2])
We keep 7.12e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([2051, 2])
We keep 1.09e+05/1.32e+06 =  8% of the original kernel matrix.

torch.Size([7630, 2])
We keep 9.23e+05/2.17e+07 =  4% of the original kernel matrix.

torch.Size([90086, 2])
We keep 2.24e+08/9.17e+09 =  2% of the original kernel matrix.

torch.Size([44380, 2])
We keep 2.79e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([175911, 2])
We keep 5.58e+08/3.03e+10 =  1% of the original kernel matrix.

torch.Size([64617, 2])
We keep 4.59e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([15820, 2])
We keep 9.17e+06/2.28e+08 =  4% of the original kernel matrix.

torch.Size([18344, 2])
We keep 6.14e+06/2.85e+08 =  2% of the original kernel matrix.

torch.Size([5203, 2])
We keep 5.70e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([10830, 2])
We keep 1.90e+06/6.04e+07 =  3% of the original kernel matrix.

torch.Size([18394, 2])
We keep 1.24e+07/3.52e+08 =  3% of the original kernel matrix.

torch.Size([20129, 2])
We keep 7.07e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([45275, 2])
We keep 3.18e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([32321, 2])
We keep 1.47e+07/8.46e+08 =  1% of the original kernel matrix.

torch.Size([2147, 2])
We keep 9.14e+04/1.14e+06 =  7% of the original kernel matrix.

torch.Size([7785, 2])
We keep 9.01e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([8992, 2])
We keep 2.38e+06/4.72e+07 =  5% of the original kernel matrix.

torch.Size([14014, 2])
We keep 3.29e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([14875, 2])
We keep 3.98e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([17961, 2])
We keep 5.05e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([25065, 2])
We keep 1.17e+08/1.41e+09 =  8% of the original kernel matrix.

torch.Size([22433, 2])
We keep 1.33e+07/7.09e+08 =  1% of the original kernel matrix.

torch.Size([19423, 2])
We keep 1.46e+07/3.50e+08 =  4% of the original kernel matrix.

torch.Size([20465, 2])
We keep 7.28e+06/3.54e+08 =  2% of the original kernel matrix.

torch.Size([60226, 2])
We keep 5.45e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([36413, 2])
We keep 1.77e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([8626, 2])
We keep 1.43e+06/3.33e+07 =  4% of the original kernel matrix.

torch.Size([13479, 2])
We keep 2.88e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([3914, 2])
We keep 7.35e+05/6.33e+06 = 11% of the original kernel matrix.

torch.Size([9688, 2])
We keep 1.64e+06/4.76e+07 =  3% of the original kernel matrix.

torch.Size([2733, 2])
We keep 1.38e+05/2.06e+06 =  6% of the original kernel matrix.

torch.Size([8518, 2])
We keep 1.08e+06/2.71e+07 =  3% of the original kernel matrix.

torch.Size([17359, 2])
We keep 1.02e+07/2.55e+08 =  4% of the original kernel matrix.

torch.Size([19570, 2])
We keep 6.45e+06/3.02e+08 =  2% of the original kernel matrix.

torch.Size([10921, 2])
We keep 2.44e+06/6.16e+07 =  3% of the original kernel matrix.

torch.Size([15307, 2])
We keep 3.67e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([26138, 2])
We keep 1.82e+07/6.59e+08 =  2% of the original kernel matrix.

torch.Size([24080, 2])
We keep 9.36e+06/4.86e+08 =  1% of the original kernel matrix.

torch.Size([26747, 2])
We keep 5.69e+07/6.47e+08 =  8% of the original kernel matrix.

torch.Size([24403, 2])
We keep 9.21e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([145651, 2])
We keep 2.24e+08/1.88e+10 =  1% of the original kernel matrix.

torch.Size([57344, 2])
We keep 3.73e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([7768, 2])
We keep 1.55e+06/2.95e+07 =  5% of the original kernel matrix.

torch.Size([12869, 2])
We keep 2.83e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([4227, 2])
We keep 1.07e+06/9.20e+06 = 11% of the original kernel matrix.

torch.Size([9795, 2])
We keep 1.89e+06/5.74e+07 =  3% of the original kernel matrix.

torch.Size([18444, 2])
We keep 1.57e+07/2.64e+08 =  5% of the original kernel matrix.

torch.Size([20202, 2])
We keep 6.63e+06/3.07e+08 =  2% of the original kernel matrix.

torch.Size([282985, 2])
We keep 4.32e+09/1.15e+11 =  3% of the original kernel matrix.

torch.Size([82587, 2])
We keep 8.14e+07/6.43e+09 =  1% of the original kernel matrix.

torch.Size([18325, 2])
We keep 6.03e+06/2.47e+08 =  2% of the original kernel matrix.

torch.Size([20079, 2])
We keep 6.21e+06/2.97e+08 =  2% of the original kernel matrix.

torch.Size([5486, 2])
We keep 9.50e+05/1.45e+07 =  6% of the original kernel matrix.

torch.Size([11044, 2])
We keep 2.21e+06/7.21e+07 =  3% of the original kernel matrix.

torch.Size([1313, 2])
We keep 4.21e+04/3.98e+05 = 10% of the original kernel matrix.

torch.Size([6373, 2])
We keep 6.07e+05/1.19e+07 =  5% of the original kernel matrix.

torch.Size([24092, 2])
We keep 1.11e+07/4.85e+08 =  2% of the original kernel matrix.

torch.Size([23490, 2])
We keep 8.19e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([1775, 2])
We keep 7.24e+04/8.19e+05 =  8% of the original kernel matrix.

torch.Size([7262, 2])
We keep 7.87e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([7984, 2])
We keep 3.68e+06/4.88e+07 =  7% of the original kernel matrix.

torch.Size([12890, 2])
We keep 3.25e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([3401, 2])
We keep 2.94e+05/4.02e+06 =  7% of the original kernel matrix.

torch.Size([9121, 2])
We keep 1.38e+06/3.79e+07 =  3% of the original kernel matrix.

torch.Size([9775, 2])
We keep 1.58e+06/4.38e+07 =  3% of the original kernel matrix.

torch.Size([14341, 2])
We keep 3.16e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([7560, 2])
We keep 1.74e+06/2.97e+07 =  5% of the original kernel matrix.

torch.Size([12704, 2])
We keep 2.76e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([3241, 2])
We keep 2.19e+05/3.38e+06 =  6% of the original kernel matrix.

torch.Size([9050, 2])
We keep 1.29e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([1655, 2])
We keep 5.65e+04/6.02e+05 =  9% of the original kernel matrix.

torch.Size([7031, 2])
We keep 7.31e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([113459, 2])
We keep 1.45e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([50648, 2])
We keep 3.05e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([152748, 2])
We keep 3.21e+08/2.18e+10 =  1% of the original kernel matrix.

torch.Size([59527, 2])
We keep 4.05e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([58690, 2])
We keep 6.23e+07/3.02e+09 =  2% of the original kernel matrix.

torch.Size([36166, 2])
We keep 1.76e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([7299, 2])
We keep 3.23e+06/2.94e+07 = 10% of the original kernel matrix.

torch.Size([12546, 2])
We keep 2.61e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([16189, 2])
We keep 4.51e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([18855, 2])
We keep 5.41e+06/2.49e+08 =  2% of the original kernel matrix.

torch.Size([5239, 2])
We keep 1.00e+06/1.44e+07 =  6% of the original kernel matrix.

torch.Size([10683, 2])
We keep 2.13e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([7082, 2])
We keep 1.09e+06/2.16e+07 =  5% of the original kernel matrix.

torch.Size([12318, 2])
We keep 2.50e+06/8.80e+07 =  2% of the original kernel matrix.

torch.Size([34426, 2])
We keep 1.91e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([28499, 2])
We keep 1.16e+07/6.37e+08 =  1% of the original kernel matrix.

torch.Size([26354, 2])
We keep 1.88e+07/5.65e+08 =  3% of the original kernel matrix.

torch.Size([25042, 2])
We keep 8.65e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([52019, 2])
We keep 1.25e+08/3.05e+09 =  4% of the original kernel matrix.

torch.Size([33755, 2])
We keep 1.69e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([5607, 2])
We keep 7.64e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([11114, 2])
We keep 2.07e+06/6.84e+07 =  3% of the original kernel matrix.

torch.Size([133366, 2])
We keep 6.07e+08/2.17e+10 =  2% of the original kernel matrix.

torch.Size([55558, 2])
We keep 4.01e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([15012, 2])
We keep 5.84e+06/1.46e+08 =  4% of the original kernel matrix.

torch.Size([17975, 2])
We keep 5.16e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([19798, 2])
We keep 8.16e+06/3.00e+08 =  2% of the original kernel matrix.

torch.Size([21317, 2])
We keep 6.78e+06/3.28e+08 =  2% of the original kernel matrix.

torch.Size([91818, 2])
We keep 2.09e+08/7.81e+09 =  2% of the original kernel matrix.

torch.Size([45326, 2])
We keep 2.63e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([28839, 2])
We keep 1.47e+07/7.40e+08 =  1% of the original kernel matrix.

torch.Size([25883, 2])
We keep 9.77e+06/5.14e+08 =  1% of the original kernel matrix.

torch.Size([87600, 2])
We keep 3.48e+08/1.04e+10 =  3% of the original kernel matrix.

torch.Size([43734, 2])
We keep 2.90e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([95834, 2])
We keep 2.15e+08/8.42e+09 =  2% of the original kernel matrix.

torch.Size([45952, 2])
We keep 2.62e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([29997, 2])
We keep 1.37e+07/7.54e+08 =  1% of the original kernel matrix.

torch.Size([26079, 2])
We keep 9.70e+06/5.19e+08 =  1% of the original kernel matrix.

torch.Size([15752, 2])
We keep 4.82e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([18598, 2])
We keep 5.34e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([12825, 2])
We keep 5.20e+06/1.19e+08 =  4% of the original kernel matrix.

torch.Size([16420, 2])
We keep 4.82e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([34717, 2])
We keep 2.22e+07/1.08e+09 =  2% of the original kernel matrix.

torch.Size([28329, 2])
We keep 1.13e+07/6.22e+08 =  1% of the original kernel matrix.

torch.Size([5587, 2])
We keep 6.13e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([11390, 2])
We keep 1.98e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([69539, 2])
We keep 8.81e+07/4.01e+09 =  2% of the original kernel matrix.

torch.Size([38850, 2])
We keep 1.95e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([391731, 2])
We keep 1.57e+09/1.42e+11 =  1% of the original kernel matrix.

torch.Size([96548, 2])
We keep 9.45e+07/7.13e+09 =  1% of the original kernel matrix.

torch.Size([46639, 2])
We keep 1.27e+08/2.25e+09 =  5% of the original kernel matrix.

torch.Size([32183, 2])
We keep 1.55e+07/8.97e+08 =  1% of the original kernel matrix.

torch.Size([22311, 2])
We keep 5.39e+07/8.65e+08 =  6% of the original kernel matrix.

torch.Size([21496, 2])
We keep 1.02e+07/5.56e+08 =  1% of the original kernel matrix.

torch.Size([14676, 2])
We keep 8.34e+06/1.94e+08 =  4% of the original kernel matrix.

torch.Size([17723, 2])
We keep 5.83e+06/2.63e+08 =  2% of the original kernel matrix.

torch.Size([5106, 2])
We keep 1.18e+06/1.55e+07 =  7% of the original kernel matrix.

torch.Size([10798, 2])
We keep 2.08e+06/7.44e+07 =  2% of the original kernel matrix.

torch.Size([22864, 2])
We keep 9.20e+06/4.34e+08 =  2% of the original kernel matrix.

torch.Size([23212, 2])
We keep 7.92e+06/3.94e+08 =  2% of the original kernel matrix.

torch.Size([9938, 2])
We keep 3.56e+06/7.29e+07 =  4% of the original kernel matrix.

torch.Size([14531, 2])
We keep 3.99e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([34763, 2])
We keep 4.98e+07/1.37e+09 =  3% of the original kernel matrix.

torch.Size([27989, 2])
We keep 1.24e+07/7.00e+08 =  1% of the original kernel matrix.

torch.Size([109533, 2])
We keep 1.49e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([49672, 2])
We keep 3.13e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([2158, 2])
We keep 1.71e+05/1.66e+06 = 10% of the original kernel matrix.

torch.Size([7501, 2])
We keep 9.89e+05/2.44e+07 =  4% of the original kernel matrix.

torch.Size([36837, 2])
We keep 1.21e+08/1.90e+09 =  6% of the original kernel matrix.

torch.Size([28090, 2])
We keep 1.36e+07/8.25e+08 =  1% of the original kernel matrix.

torch.Size([20479, 2])
We keep 1.53e+07/3.32e+08 =  4% of the original kernel matrix.

torch.Size([21493, 2])
We keep 6.90e+06/3.45e+08 =  2% of the original kernel matrix.

torch.Size([17864, 2])
We keep 6.35e+06/2.37e+08 =  2% of the original kernel matrix.

torch.Size([19794, 2])
We keep 6.18e+06/2.91e+08 =  2% of the original kernel matrix.

torch.Size([2474, 2])
We keep 1.18e+05/1.58e+06 =  7% of the original kernel matrix.

torch.Size([8247, 2])
We keep 9.90e+05/2.37e+07 =  4% of the original kernel matrix.

torch.Size([44779, 2])
We keep 3.48e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([31864, 2])
We keep 1.41e+07/8.03e+08 =  1% of the original kernel matrix.

torch.Size([61267, 2])
We keep 4.91e+07/3.06e+09 =  1% of the original kernel matrix.

torch.Size([36848, 2])
We keep 1.75e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([2652, 2])
We keep 1.84e+05/2.46e+06 =  7% of the original kernel matrix.

torch.Size([8316, 2])
We keep 1.16e+06/2.96e+07 =  3% of the original kernel matrix.

torch.Size([336457, 2])
We keep 8.54e+08/9.74e+10 =  0% of the original kernel matrix.

torch.Size([90523, 2])
We keep 7.89e+07/5.90e+09 =  1% of the original kernel matrix.

torch.Size([20273, 2])
We keep 1.09e+07/3.49e+08 =  3% of the original kernel matrix.

torch.Size([21274, 2])
We keep 7.08e+06/3.53e+08 =  2% of the original kernel matrix.

torch.Size([12785, 2])
We keep 3.61e+06/9.22e+07 =  3% of the original kernel matrix.

torch.Size([16454, 2])
We keep 4.26e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([5352, 2])
We keep 5.37e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([11111, 2])
We keep 1.81e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([4410, 2])
We keep 4.50e+05/7.51e+06 =  5% of the original kernel matrix.

torch.Size([10179, 2])
We keep 1.73e+06/5.19e+07 =  3% of the original kernel matrix.

torch.Size([2086, 2])
We keep 1.56e+05/1.44e+06 = 10% of the original kernel matrix.

torch.Size([7483, 2])
We keep 9.78e+05/2.27e+07 =  4% of the original kernel matrix.

torch.Size([5506, 2])
We keep 1.27e+06/1.49e+07 =  8% of the original kernel matrix.

torch.Size([11035, 2])
We keep 2.21e+06/7.29e+07 =  3% of the original kernel matrix.

torch.Size([6897, 2])
We keep 1.10e+06/2.13e+07 =  5% of the original kernel matrix.

torch.Size([12350, 2])
We keep 2.42e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([42426, 2])
We keep 3.77e+07/1.70e+09 =  2% of the original kernel matrix.

torch.Size([30456, 2])
We keep 1.38e+07/7.79e+08 =  1% of the original kernel matrix.

torch.Size([163671, 2])
We keep 4.35e+09/8.92e+10 =  4% of the original kernel matrix.

torch.Size([56300, 2])
We keep 7.20e+07/5.65e+09 =  1% of the original kernel matrix.

torch.Size([7481, 2])
We keep 2.56e+06/2.90e+07 =  8% of the original kernel matrix.

torch.Size([12696, 2])
We keep 2.82e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([27795, 2])
We keep 1.46e+07/6.73e+08 =  2% of the original kernel matrix.

torch.Size([25271, 2])
We keep 9.42e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([3483, 2])
We keep 2.61e+05/3.96e+06 =  6% of the original kernel matrix.

torch.Size([9300, 2])
We keep 1.35e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([10641, 2])
We keep 2.13e+06/5.76e+07 =  3% of the original kernel matrix.

torch.Size([14836, 2])
We keep 3.57e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([6410, 2])
We keep 1.01e+06/1.86e+07 =  5% of the original kernel matrix.

torch.Size([11813, 2])
We keep 2.37e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([21512, 2])
We keep 1.02e+08/8.30e+08 = 12% of the original kernel matrix.

torch.Size([20878, 2])
We keep 9.65e+06/5.45e+08 =  1% of the original kernel matrix.

torch.Size([8495, 2])
We keep 3.93e+06/4.51e+07 =  8% of the original kernel matrix.

torch.Size([13508, 2])
We keep 3.33e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([567397, 2])
We keep 2.19e+09/2.58e+11 =  0% of the original kernel matrix.

torch.Size([119041, 2])
We keep 1.23e+08/9.60e+09 =  1% of the original kernel matrix.

torch.Size([3798, 2])
We keep 3.23e+05/5.57e+06 =  5% of the original kernel matrix.

torch.Size([9545, 2])
We keep 1.50e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([45157, 2])
We keep 4.31e+07/1.87e+09 =  2% of the original kernel matrix.

torch.Size([32184, 2])
We keep 1.39e+07/8.18e+08 =  1% of the original kernel matrix.

torch.Size([9104, 2])
We keep 3.69e+06/4.27e+07 =  8% of the original kernel matrix.

torch.Size([14035, 2])
We keep 3.07e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([48317, 2])
We keep 4.55e+07/2.09e+09 =  2% of the original kernel matrix.

torch.Size([32680, 2])
We keep 1.49e+07/8.64e+08 =  1% of the original kernel matrix.

torch.Size([145772, 2])
We keep 2.77e+08/2.10e+10 =  1% of the original kernel matrix.

torch.Size([58128, 2])
We keep 3.95e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([13507, 2])
We keep 3.69e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([16975, 2])
We keep 4.53e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([4321, 2])
We keep 4.26e+05/6.66e+06 =  6% of the original kernel matrix.

torch.Size([10118, 2])
We keep 1.65e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([43322, 2])
We keep 2.74e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([31335, 2])
We keep 1.33e+07/7.52e+08 =  1% of the original kernel matrix.

torch.Size([11830, 2])
We keep 4.84e+06/9.59e+07 =  5% of the original kernel matrix.

torch.Size([15879, 2])
We keep 4.47e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([13567, 2])
We keep 1.17e+07/2.16e+08 =  5% of the original kernel matrix.

torch.Size([16646, 2])
We keep 5.93e+06/2.78e+08 =  2% of the original kernel matrix.

torch.Size([62216, 2])
We keep 5.33e+07/3.23e+09 =  1% of the original kernel matrix.

torch.Size([36916, 2])
We keep 1.79e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([11136, 2])
We keep 8.66e+06/1.52e+08 =  5% of the original kernel matrix.

torch.Size([15575, 2])
We keep 4.79e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([41620, 2])
We keep 2.63e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([31738, 2])
We keep 1.35e+07/7.66e+08 =  1% of the original kernel matrix.

torch.Size([13239, 2])
We keep 3.64e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([16765, 2])
We keep 4.49e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([32844, 2])
We keep 3.69e+07/1.07e+09 =  3% of the original kernel matrix.

torch.Size([27461, 2])
We keep 1.14e+07/6.18e+08 =  1% of the original kernel matrix.

torch.Size([95851, 2])
We keep 2.97e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([45609, 2])
We keep 2.98e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([13811, 2])
We keep 4.57e+06/1.19e+08 =  3% of the original kernel matrix.

torch.Size([17224, 2])
We keep 4.72e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([4273, 2])
We keep 3.65e+05/6.38e+06 =  5% of the original kernel matrix.

torch.Size([10030, 2])
We keep 1.59e+06/4.78e+07 =  3% of the original kernel matrix.

torch.Size([54301, 2])
We keep 1.14e+08/3.75e+09 =  3% of the original kernel matrix.

torch.Size([33641, 2])
We keep 1.90e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([22344, 2])
We keep 1.14e+07/4.56e+08 =  2% of the original kernel matrix.

torch.Size([22090, 2])
We keep 8.07e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([31787, 2])
We keep 3.85e+07/1.03e+09 =  3% of the original kernel matrix.

torch.Size([26902, 2])
We keep 1.12e+07/6.08e+08 =  1% of the original kernel matrix.

torch.Size([15768, 2])
We keep 2.80e+07/2.24e+08 = 12% of the original kernel matrix.

torch.Size([18348, 2])
We keep 6.14e+06/2.83e+08 =  2% of the original kernel matrix.

torch.Size([25722, 2])
We keep 2.41e+07/7.13e+08 =  3% of the original kernel matrix.

torch.Size([23441, 2])
We keep 9.61e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([13149, 2])
We keep 6.61e+06/1.39e+08 =  4% of the original kernel matrix.

torch.Size([16715, 2])
We keep 5.02e+06/2.23e+08 =  2% of the original kernel matrix.

torch.Size([28501, 2])
We keep 4.91e+07/1.29e+09 =  3% of the original kernel matrix.

torch.Size([24033, 2])
We keep 1.22e+07/6.80e+08 =  1% of the original kernel matrix.

torch.Size([14125, 2])
We keep 5.76e+06/1.41e+08 =  4% of the original kernel matrix.

torch.Size([17423, 2])
We keep 5.07e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([12722, 2])
We keep 3.63e+06/9.05e+07 =  4% of the original kernel matrix.

torch.Size([16434, 2])
We keep 4.25e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([2734, 2])
We keep 1.62e+05/2.23e+06 =  7% of the original kernel matrix.

torch.Size([8437, 2])
We keep 1.12e+06/2.82e+07 =  3% of the original kernel matrix.

torch.Size([12859, 2])
We keep 3.86e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([16627, 2])
We keep 4.63e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([11996, 2])
We keep 8.67e+06/1.03e+08 =  8% of the original kernel matrix.

torch.Size([15877, 2])
We keep 4.24e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([33276, 2])
We keep 3.33e+07/1.08e+09 =  3% of the original kernel matrix.

torch.Size([27266, 2])
We keep 1.11e+07/6.21e+08 =  1% of the original kernel matrix.

torch.Size([58568, 2])
We keep 6.43e+07/2.99e+09 =  2% of the original kernel matrix.

torch.Size([36112, 2])
We keep 1.71e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([4890, 2])
We keep 5.78e+05/9.66e+06 =  5% of the original kernel matrix.

torch.Size([10584, 2])
We keep 1.88e+06/5.88e+07 =  3% of the original kernel matrix.

torch.Size([15309, 2])
We keep 4.36e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([18349, 2])
We keep 5.27e+06/2.40e+08 =  2% of the original kernel matrix.

torch.Size([28918, 2])
We keep 2.37e+07/8.54e+08 =  2% of the original kernel matrix.

torch.Size([25410, 2])
We keep 1.05e+07/5.53e+08 =  1% of the original kernel matrix.

torch.Size([130379, 2])
We keep 4.49e+08/1.56e+10 =  2% of the original kernel matrix.

torch.Size([54734, 2])
We keep 3.49e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([30953, 2])
We keep 1.82e+07/8.68e+08 =  2% of the original kernel matrix.

torch.Size([26728, 2])
We keep 1.04e+07/5.57e+08 =  1% of the original kernel matrix.

torch.Size([35369, 2])
We keep 1.80e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([28671, 2])
We keep 1.10e+07/6.13e+08 =  1% of the original kernel matrix.

torch.Size([21984, 2])
We keep 2.64e+07/6.14e+08 =  4% of the original kernel matrix.

torch.Size([21665, 2])
We keep 9.24e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([16153, 2])
We keep 5.08e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([18925, 2])
We keep 5.42e+06/2.54e+08 =  2% of the original kernel matrix.

torch.Size([1624, 2])
We keep 5.95e+04/6.79e+05 =  8% of the original kernel matrix.

torch.Size([7036, 2])
We keep 7.40e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([9234, 2])
We keep 1.81e+06/4.08e+07 =  4% of the original kernel matrix.

torch.Size([13959, 2])
We keep 3.09e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([58120, 2])
We keep 4.32e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([35983, 2])
We keep 1.67e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([27014, 2])
We keep 1.95e+07/6.78e+08 =  2% of the original kernel matrix.

torch.Size([24462, 2])
We keep 9.47e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([31034, 2])
We keep 1.72e+07/9.02e+08 =  1% of the original kernel matrix.

torch.Size([26928, 2])
We keep 1.06e+07/5.68e+08 =  1% of the original kernel matrix.

torch.Size([1582, 2])
We keep 5.94e+04/7.01e+05 =  8% of the original kernel matrix.

torch.Size([6985, 2])
We keep 7.59e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([5539, 2])
We keep 6.40e+05/1.19e+07 =  5% of the original kernel matrix.

torch.Size([11185, 2])
We keep 2.00e+06/6.52e+07 =  3% of the original kernel matrix.

torch.Size([88434, 2])
We keep 1.84e+08/8.09e+09 =  2% of the original kernel matrix.

torch.Size([43918, 2])
We keep 2.70e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([31102, 2])
We keep 2.08e+07/9.08e+08 =  2% of the original kernel matrix.

torch.Size([26434, 2])
We keep 1.05e+07/5.70e+08 =  1% of the original kernel matrix.

torch.Size([18609, 2])
We keep 7.12e+06/2.51e+08 =  2% of the original kernel matrix.

torch.Size([20446, 2])
We keep 6.39e+06/3.00e+08 =  2% of the original kernel matrix.

torch.Size([2445, 2])
We keep 1.18e+05/1.65e+06 =  7% of the original kernel matrix.

torch.Size([8201, 2])
We keep 1.00e+06/2.43e+07 =  4% of the original kernel matrix.

torch.Size([1623, 2])
We keep 5.70e+04/6.30e+05 =  9% of the original kernel matrix.

torch.Size([6983, 2])
We keep 7.29e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([19430, 2])
We keep 8.54e+06/2.92e+08 =  2% of the original kernel matrix.

torch.Size([20244, 2])
We keep 6.59e+06/3.23e+08 =  2% of the original kernel matrix.

torch.Size([27273, 2])
We keep 1.69e+07/6.64e+08 =  2% of the original kernel matrix.

torch.Size([24825, 2])
We keep 9.30e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([31824, 2])
We keep 3.69e+07/1.14e+09 =  3% of the original kernel matrix.

torch.Size([26577, 2])
We keep 1.18e+07/6.37e+08 =  1% of the original kernel matrix.

torch.Size([2909, 2])
We keep 2.03e+05/2.97e+06 =  6% of the original kernel matrix.

torch.Size([8701, 2])
We keep 1.18e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([39781, 2])
We keep 1.22e+08/2.34e+09 =  5% of the original kernel matrix.

torch.Size([28905, 2])
We keep 1.61e+07/9.15e+08 =  1% of the original kernel matrix.

torch.Size([8729, 2])
We keep 2.87e+06/5.01e+07 =  5% of the original kernel matrix.

torch.Size([13528, 2])
We keep 3.46e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([6392, 2])
We keep 1.39e+06/2.37e+07 =  5% of the original kernel matrix.

torch.Size([11546, 2])
We keep 2.59e+06/9.21e+07 =  2% of the original kernel matrix.

torch.Size([178414, 2])
We keep 5.25e+08/3.26e+10 =  1% of the original kernel matrix.

torch.Size([65025, 2])
We keep 4.83e+07/3.42e+09 =  1% of the original kernel matrix.

torch.Size([8699, 2])
We keep 3.82e+06/5.38e+07 =  7% of the original kernel matrix.

torch.Size([13727, 2])
We keep 3.44e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([42986, 2])
We keep 4.52e+07/1.77e+09 =  2% of the original kernel matrix.

torch.Size([30788, 2])
We keep 1.37e+07/7.95e+08 =  1% of the original kernel matrix.

torch.Size([70691, 2])
We keep 1.05e+08/4.01e+09 =  2% of the original kernel matrix.

torch.Size([39307, 2])
We keep 1.94e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([31307, 2])
We keep 1.76e+07/8.69e+08 =  2% of the original kernel matrix.

torch.Size([27002, 2])
We keep 1.04e+07/5.58e+08 =  1% of the original kernel matrix.

torch.Size([34964, 2])
We keep 2.28e+07/1.12e+09 =  2% of the original kernel matrix.

torch.Size([28567, 2])
We keep 1.14e+07/6.34e+08 =  1% of the original kernel matrix.

torch.Size([76657, 2])
We keep 8.27e+07/4.92e+09 =  1% of the original kernel matrix.

torch.Size([40780, 2])
We keep 2.10e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([2136, 2])
We keep 1.08e+05/1.25e+06 =  8% of the original kernel matrix.

torch.Size([7506, 2])
We keep 9.16e+05/2.12e+07 =  4% of the original kernel matrix.

torch.Size([20223, 2])
We keep 9.79e+06/3.45e+08 =  2% of the original kernel matrix.

torch.Size([21463, 2])
We keep 7.30e+06/3.52e+08 =  2% of the original kernel matrix.

torch.Size([12486, 2])
We keep 3.12e+06/8.46e+07 =  3% of the original kernel matrix.

torch.Size([16265, 2])
We keep 4.18e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([90133, 2])
We keep 1.08e+08/6.52e+09 =  1% of the original kernel matrix.

torch.Size([44015, 2])
We keep 2.39e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([75436, 2])
We keep 8.27e+07/5.02e+09 =  1% of the original kernel matrix.

torch.Size([40809, 2])
We keep 2.14e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([16773, 2])
We keep 1.12e+07/3.20e+08 =  3% of the original kernel matrix.

torch.Size([18794, 2])
We keep 7.01e+06/3.38e+08 =  2% of the original kernel matrix.

torch.Size([28621, 2])
We keep 1.43e+07/6.97e+08 =  2% of the original kernel matrix.

torch.Size([25844, 2])
We keep 9.50e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([9055, 2])
We keep 1.69e+06/4.26e+07 =  3% of the original kernel matrix.

torch.Size([13996, 2])
We keep 3.20e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([2576, 2])
We keep 1.09e+06/7.35e+06 = 14% of the original kernel matrix.

torch.Size([7608, 2])
We keep 1.55e+06/5.13e+07 =  3% of the original kernel matrix.

torch.Size([8822, 2])
We keep 3.38e+06/4.23e+07 =  7% of the original kernel matrix.

torch.Size([13852, 2])
We keep 3.12e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([8630, 2])
We keep 1.82e+06/3.96e+07 =  4% of the original kernel matrix.

torch.Size([13428, 2])
We keep 3.15e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([104125, 2])
We keep 1.75e+08/1.10e+10 =  1% of the original kernel matrix.

torch.Size([48178, 2])
We keep 3.05e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([60784, 2])
We keep 1.55e+08/3.57e+09 =  4% of the original kernel matrix.

torch.Size([36552, 2])
We keep 1.82e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([3339, 2])
We keep 4.53e+05/4.54e+06 =  9% of the original kernel matrix.

torch.Size([8994, 2])
We keep 1.43e+06/4.03e+07 =  3% of the original kernel matrix.

torch.Size([10481, 2])
We keep 2.14e+06/5.65e+07 =  3% of the original kernel matrix.

torch.Size([15016, 2])
We keep 3.45e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([8753, 2])
We keep 1.28e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([13740, 2])
We keep 2.87e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([206362, 2])
We keep 6.12e+08/4.49e+10 =  1% of the original kernel matrix.

torch.Size([70051, 2])
We keep 5.67e+07/4.01e+09 =  1% of the original kernel matrix.

torch.Size([9289, 2])
We keep 1.81e+06/4.18e+07 =  4% of the original kernel matrix.

torch.Size([14028, 2])
We keep 3.15e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([11553, 2])
We keep 2.92e+06/7.66e+07 =  3% of the original kernel matrix.

torch.Size([15582, 2])
We keep 4.03e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([42900, 2])
We keep 3.36e+07/1.60e+09 =  2% of the original kernel matrix.

torch.Size([31329, 2])
We keep 1.36e+07/7.57e+08 =  1% of the original kernel matrix.

torch.Size([42963, 2])
We keep 2.65e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([31386, 2])
We keep 1.32e+07/7.51e+08 =  1% of the original kernel matrix.

torch.Size([9602, 2])
We keep 2.21e+06/4.92e+07 =  4% of the original kernel matrix.

torch.Size([14193, 2])
We keep 3.41e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([19571, 2])
We keep 8.95e+06/2.90e+08 =  3% of the original kernel matrix.

torch.Size([20568, 2])
We keep 6.64e+06/3.22e+08 =  2% of the original kernel matrix.

torch.Size([8708, 2])
We keep 2.27e+06/4.10e+07 =  5% of the original kernel matrix.

torch.Size([13714, 2])
We keep 3.16e+06/1.21e+08 =  2% of the original kernel matrix.

time for making ranges is 2.0449600219726562
Sorting X and nu_X
time for sorting X is 0.06659889221191406
Sorting Z and nu_Z
time for sorting Z is 0.00026869773864746094
Starting Optim
sum tnu_Z before tensor(20036258., device='cuda:0')
c= tensor(393.4017, device='cuda:0')
c= tensor(26397.1309, device='cuda:0')
c= tensor(35081.7422, device='cuda:0')
c= tensor(36045.1875, device='cuda:0')
c= tensor(353298.0312, device='cuda:0')
c= tensor(452490.2500, device='cuda:0')
c= tensor(784566.8125, device='cuda:0')
c= tensor(908450.7500, device='cuda:0')
c= tensor(928052., device='cuda:0')
c= tensor(3015288.2500, device='cuda:0')
c= tensor(3021353.2500, device='cuda:0')
c= tensor(4108308., device='cuda:0')
c= tensor(4113491.2500, device='cuda:0')
c= tensor(8414913., device='cuda:0')
c= tensor(8521921., device='cuda:0')
c= tensor(8644752., device='cuda:0')
c= tensor(9014529., device='cuda:0')
c= tensor(9717212., device='cuda:0')
c= tensor(12769170., device='cuda:0')
c= tensor(15202230., device='cuda:0')
c= tensor(15210311., device='cuda:0')
c= tensor(22567056., device='cuda:0')
c= tensor(22604594., device='cuda:0')
c= tensor(22619794., device='cuda:0')
c= tensor(24290130., device='cuda:0')
c= tensor(24914440., device='cuda:0')
c= tensor(25689340., device='cuda:0')
c= tensor(25852434., device='cuda:0')
c= tensor(26378626., device='cuda:0')
c= tensor(2.0445e+08, device='cuda:0')
c= tensor(2.0446e+08, device='cuda:0')
c= tensor(2.1948e+08, device='cuda:0')
c= tensor(2.1951e+08, device='cuda:0')
c= tensor(2.1952e+08, device='cuda:0')
c= tensor(2.1952e+08, device='cuda:0')
c= tensor(2.1999e+08, device='cuda:0')
c= tensor(2.2055e+08, device='cuda:0')
c= tensor(2.2055e+08, device='cuda:0')
c= tensor(2.2056e+08, device='cuda:0')
c= tensor(2.2056e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2058e+08, device='cuda:0')
c= tensor(2.2058e+08, device='cuda:0')
c= tensor(2.2058e+08, device='cuda:0')
c= tensor(2.2059e+08, device='cuda:0')
c= tensor(2.2061e+08, device='cuda:0')
c= tensor(2.2061e+08, device='cuda:0')
c= tensor(2.2061e+08, device='cuda:0')
c= tensor(2.2061e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2064e+08, device='cuda:0')
c= tensor(2.2064e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2066e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2069e+08, device='cuda:0')
c= tensor(2.2069e+08, device='cuda:0')
c= tensor(2.2069e+08, device='cuda:0')
c= tensor(2.2069e+08, device='cuda:0')
c= tensor(2.2071e+08, device='cuda:0')
c= tensor(2.2071e+08, device='cuda:0')
c= tensor(2.2071e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2075e+08, device='cuda:0')
c= tensor(2.2075e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2081e+08, device='cuda:0')
c= tensor(2.2081e+08, device='cuda:0')
c= tensor(2.2081e+08, device='cuda:0')
c= tensor(2.2081e+08, device='cuda:0')
c= tensor(2.2081e+08, device='cuda:0')
c= tensor(2.2082e+08, device='cuda:0')
c= tensor(2.2082e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2085e+08, device='cuda:0')
c= tensor(2.2085e+08, device='cuda:0')
c= tensor(2.2091e+08, device='cuda:0')
c= tensor(2.2091e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2095e+08, device='cuda:0')
c= tensor(2.2096e+08, device='cuda:0')
c= tensor(2.2097e+08, device='cuda:0')
c= tensor(2.2097e+08, device='cuda:0')
c= tensor(2.2097e+08, device='cuda:0')
c= tensor(2.2098e+08, device='cuda:0')
c= tensor(2.2098e+08, device='cuda:0')
c= tensor(2.2098e+08, device='cuda:0')
c= tensor(2.2098e+08, device='cuda:0')
c= tensor(2.2099e+08, device='cuda:0')
c= tensor(2.2099e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2105e+08, device='cuda:0')
c= tensor(2.2105e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2107e+08, device='cuda:0')
c= tensor(2.2107e+08, device='cuda:0')
c= tensor(2.2107e+08, device='cuda:0')
c= tensor(2.2107e+08, device='cuda:0')
c= tensor(2.2107e+08, device='cuda:0')
c= tensor(2.2108e+08, device='cuda:0')
c= tensor(2.2108e+08, device='cuda:0')
c= tensor(2.2109e+08, device='cuda:0')
c= tensor(2.2109e+08, device='cuda:0')
c= tensor(2.2109e+08, device='cuda:0')
c= tensor(2.2111e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2115e+08, device='cuda:0')
c= tensor(2.2115e+08, device='cuda:0')
c= tensor(2.2116e+08, device='cuda:0')
c= tensor(2.2116e+08, device='cuda:0')
c= tensor(2.2117e+08, device='cuda:0')
c= tensor(2.2117e+08, device='cuda:0')
c= tensor(2.2118e+08, device='cuda:0')
c= tensor(2.2118e+08, device='cuda:0')
c= tensor(2.2118e+08, device='cuda:0')
c= tensor(2.2120e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2123e+08, device='cuda:0')
c= tensor(2.2123e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2125e+08, device='cuda:0')
c= tensor(2.2126e+08, device='cuda:0')
c= tensor(2.2126e+08, device='cuda:0')
c= tensor(2.2126e+08, device='cuda:0')
c= tensor(2.2127e+08, device='cuda:0')
c= tensor(2.2128e+08, device='cuda:0')
c= tensor(2.2128e+08, device='cuda:0')
c= tensor(2.2129e+08, device='cuda:0')
c= tensor(2.2150e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2154e+08, device='cuda:0')
c= tensor(2.2154e+08, device='cuda:0')
c= tensor(2.2155e+08, device='cuda:0')
c= tensor(2.2202e+08, device='cuda:0')
c= tensor(2.2523e+08, device='cuda:0')
c= tensor(2.2523e+08, device='cuda:0')
c= tensor(2.2549e+08, device='cuda:0')
c= tensor(2.2639e+08, device='cuda:0')
c= tensor(2.2642e+08, device='cuda:0')
c= tensor(2.6804e+08, device='cuda:0')
c= tensor(2.6804e+08, device='cuda:0')
c= tensor(2.6806e+08, device='cuda:0')
c= tensor(2.6876e+08, device='cuda:0')
c= tensor(2.8757e+08, device='cuda:0')
c= tensor(2.8757e+08, device='cuda:0')
c= tensor(2.8769e+08, device='cuda:0')
c= tensor(2.8988e+08, device='cuda:0')
c= tensor(2.9586e+08, device='cuda:0')
c= tensor(2.9718e+08, device='cuda:0')
c= tensor(2.9763e+08, device='cuda:0')
c= tensor(2.9776e+08, device='cuda:0')
c= tensor(2.9777e+08, device='cuda:0')
c= tensor(2.9825e+08, device='cuda:0')
c= tensor(3.0085e+08, device='cuda:0')
c= tensor(3.0085e+08, device='cuda:0')
c= tensor(3.0085e+08, device='cuda:0')
c= tensor(3.0140e+08, device='cuda:0')
c= tensor(3.0148e+08, device='cuda:0')
c= tensor(3.0909e+08, device='cuda:0')
c= tensor(3.0971e+08, device='cuda:0')
c= tensor(3.0971e+08, device='cuda:0')
c= tensor(3.0973e+08, device='cuda:0')
c= tensor(3.0974e+08, device='cuda:0')
c= tensor(3.0979e+08, device='cuda:0')
c= tensor(3.1053e+08, device='cuda:0')
c= tensor(3.1085e+08, device='cuda:0')
c= tensor(3.1103e+08, device='cuda:0')
c= tensor(3.1103e+08, device='cuda:0')
c= tensor(3.1104e+08, device='cuda:0')
c= tensor(3.1124e+08, device='cuda:0')
c= tensor(3.1188e+08, device='cuda:0')
c= tensor(3.1197e+08, device='cuda:0')
c= tensor(3.1198e+08, device='cuda:0')
c= tensor(3.2427e+08, device='cuda:0')
c= tensor(3.2429e+08, device='cuda:0')
c= tensor(3.2436e+08, device='cuda:0')
c= tensor(3.2532e+08, device='cuda:0')
c= tensor(3.2532e+08, device='cuda:0')
c= tensor(3.2549e+08, device='cuda:0')
c= tensor(3.2833e+08, device='cuda:0')
c= tensor(3.3551e+08, device='cuda:0')
c= tensor(3.3552e+08, device='cuda:0')
c= tensor(3.3558e+08, device='cuda:0')
c= tensor(3.3559e+08, device='cuda:0')
c= tensor(3.3559e+08, device='cuda:0')
c= tensor(3.3579e+08, device='cuda:0')
c= tensor(3.3581e+08, device='cuda:0')
c= tensor(3.3603e+08, device='cuda:0')
c= tensor(3.4433e+08, device='cuda:0')
c= tensor(3.4455e+08, device='cuda:0')
c= tensor(3.4458e+08, device='cuda:0')
c= tensor(3.4460e+08, device='cuda:0')
c= tensor(3.4648e+08, device='cuda:0')
c= tensor(3.4653e+08, device='cuda:0')
c= tensor(3.4653e+08, device='cuda:0')
c= tensor(3.4657e+08, device='cuda:0')
c= tensor(3.8234e+08, device='cuda:0')
c= tensor(3.8236e+08, device='cuda:0')
c= tensor(3.8421e+08, device='cuda:0')
c= tensor(3.8422e+08, device='cuda:0')
c= tensor(3.8460e+08, device='cuda:0')
c= tensor(3.8478e+08, device='cuda:0')
c= tensor(3.8599e+08, device='cuda:0')
c= tensor(3.8647e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8789e+08, device='cuda:0')
c= tensor(3.8881e+08, device='cuda:0')
c= tensor(3.8882e+08, device='cuda:0')
c= tensor(3.8902e+08, device='cuda:0')
c= tensor(3.9071e+08, device='cuda:0')
c= tensor(3.9786e+08, device='cuda:0')
c= tensor(3.9918e+08, device='cuda:0')
c= tensor(3.9921e+08, device='cuda:0')
c= tensor(3.9922e+08, device='cuda:0')
c= tensor(3.9936e+08, device='cuda:0')
c= tensor(3.9988e+08, device='cuda:0')
c= tensor(3.9989e+08, device='cuda:0')
c= tensor(3.9989e+08, device='cuda:0')
c= tensor(4.0059e+08, device='cuda:0')
c= tensor(4.0161e+08, device='cuda:0')
c= tensor(4.0399e+08, device='cuda:0')
c= tensor(4.0399e+08, device='cuda:0')
c= tensor(4.0405e+08, device='cuda:0')
c= tensor(4.0413e+08, device='cuda:0')
c= tensor(4.0414e+08, device='cuda:0')
c= tensor(4.0415e+08, device='cuda:0')
c= tensor(4.0416e+08, device='cuda:0')
c= tensor(4.1291e+08, device='cuda:0')
c= tensor(4.1306e+08, device='cuda:0')
c= tensor(4.1309e+08, device='cuda:0')
c= tensor(4.1312e+08, device='cuda:0')
c= tensor(4.1312e+08, device='cuda:0')
c= tensor(4.3245e+08, device='cuda:0')
c= tensor(4.3246e+08, device='cuda:0')
c= tensor(4.3358e+08, device='cuda:0')
c= tensor(4.3358e+08, device='cuda:0')
c= tensor(4.3358e+08, device='cuda:0')
c= tensor(4.3358e+08, device='cuda:0')
c= tensor(4.3377e+08, device='cuda:0')
c= tensor(4.3389e+08, device='cuda:0')
c= tensor(4.3391e+08, device='cuda:0')
c= tensor(4.3392e+08, device='cuda:0')
c= tensor(4.3392e+08, device='cuda:0')
c= tensor(4.4065e+08, device='cuda:0')
c= tensor(4.4084e+08, device='cuda:0')
c= tensor(4.4092e+08, device='cuda:0')
c= tensor(4.4146e+08, device='cuda:0')
c= tensor(4.4214e+08, device='cuda:0')
c= tensor(4.4214e+08, device='cuda:0')
c= tensor(4.4215e+08, device='cuda:0')
c= tensor(4.4216e+08, device='cuda:0')
c= tensor(4.4216e+08, device='cuda:0')
c= tensor(4.4216e+08, device='cuda:0')
c= tensor(4.4223e+08, device='cuda:0')
c= tensor(4.4224e+08, device='cuda:0')
c= tensor(4.4224e+08, device='cuda:0')
c= tensor(4.4224e+08, device='cuda:0')
c= tensor(4.4225e+08, device='cuda:0')
c= tensor(4.8558e+08, device='cuda:0')
c= tensor(4.8564e+08, device='cuda:0')
c= tensor(4.8586e+08, device='cuda:0')
c= tensor(4.8603e+08, device='cuda:0')
c= tensor(4.8604e+08, device='cuda:0')
c= tensor(4.8606e+08, device='cuda:0')
c= tensor(6.0105e+08, device='cuda:0')
c= tensor(6.3137e+08, device='cuda:0')
c= tensor(6.3169e+08, device='cuda:0')
c= tensor(6.3176e+08, device='cuda:0')
c= tensor(6.3176e+08, device='cuda:0')
c= tensor(6.3377e+08, device='cuda:0')
c= tensor(6.3419e+08, device='cuda:0')
c= tensor(6.3980e+08, device='cuda:0')
c= tensor(6.3981e+08, device='cuda:0')
c= tensor(6.4181e+08, device='cuda:0')
c= tensor(6.4483e+08, device='cuda:0')
c= tensor(6.4510e+08, device='cuda:0')
c= tensor(6.4511e+08, device='cuda:0')
c= tensor(6.4515e+08, device='cuda:0')
c= tensor(6.4516e+08, device='cuda:0')
c= tensor(6.4517e+08, device='cuda:0')
c= tensor(6.4643e+08, device='cuda:0')
c= tensor(6.4651e+08, device='cuda:0')
c= tensor(6.4651e+08, device='cuda:0')
c= tensor(6.4655e+08, device='cuda:0')
c= tensor(6.4657e+08, device='cuda:0')
c= tensor(6.4658e+08, device='cuda:0')
c= tensor(6.4733e+08, device='cuda:0')
c= tensor(6.4838e+08, device='cuda:0')
c= tensor(6.4846e+08, device='cuda:0')
c= tensor(6.4931e+08, device='cuda:0')
c= tensor(6.5000e+08, device='cuda:0')
c= tensor(6.5001e+08, device='cuda:0')
c= tensor(6.5011e+08, device='cuda:0')
c= tensor(6.5041e+08, device='cuda:0')
c= tensor(6.5202e+08, device='cuda:0')
c= tensor(6.5203e+08, device='cuda:0')
c= tensor(6.5881e+08, device='cuda:0')
c= tensor(6.7059e+08, device='cuda:0')
c= tensor(6.7080e+08, device='cuda:0')
c= tensor(6.7124e+08, device='cuda:0')
c= tensor(6.7148e+08, device='cuda:0')
c= tensor(6.7150e+08, device='cuda:0')
c= tensor(6.7150e+08, device='cuda:0')
c= tensor(6.7185e+08, device='cuda:0')
c= tensor(6.7344e+08, device='cuda:0')
c= tensor(6.7433e+08, device='cuda:0')
c= tensor(7.0658e+08, device='cuda:0')
c= tensor(7.0740e+08, device='cuda:0')
c= tensor(7.0847e+08, device='cuda:0')
c= tensor(7.0861e+08, device='cuda:0')
c= tensor(7.0968e+08, device='cuda:0')
c= tensor(7.0968e+08, device='cuda:0')
c= tensor(7.0970e+08, device='cuda:0')
c= tensor(7.1349e+08, device='cuda:0')
c= tensor(7.1353e+08, device='cuda:0')
c= tensor(7.1353e+08, device='cuda:0')
c= tensor(7.1361e+08, device='cuda:0')
c= tensor(7.1385e+08, device='cuda:0')
c= tensor(7.1398e+08, device='cuda:0')
c= tensor(7.1454e+08, device='cuda:0')
c= tensor(7.1468e+08, device='cuda:0')
c= tensor(7.1489e+08, device='cuda:0')
c= tensor(7.1490e+08, device='cuda:0')
c= tensor(7.1493e+08, device='cuda:0')
c= tensor(7.1500e+08, device='cuda:0')
c= tensor(7.1564e+08, device='cuda:0')
c= tensor(7.1564e+08, device='cuda:0')
c= tensor(7.1738e+08, device='cuda:0')
c= tensor(7.1738e+08, device='cuda:0')
c= tensor(7.1742e+08, device='cuda:0')
c= tensor(7.1743e+08, device='cuda:0')
c= tensor(7.1754e+08, device='cuda:0')
c= tensor(7.1755e+08, device='cuda:0')
c= tensor(7.1785e+08, device='cuda:0')
c= tensor(7.1788e+08, device='cuda:0')
c= tensor(7.1793e+08, device='cuda:0')
c= tensor(7.1847e+08, device='cuda:0')
c= tensor(7.2593e+08, device='cuda:0')
c= tensor(7.2593e+08, device='cuda:0')
c= tensor(7.2606e+08, device='cuda:0')
c= tensor(7.2817e+08, device='cuda:0')
c= tensor(7.2818e+08, device='cuda:0')
c= tensor(7.4472e+08, device='cuda:0')
c= tensor(7.4472e+08, device='cuda:0')
c= tensor(7.4583e+08, device='cuda:0')
c= tensor(7.5025e+08, device='cuda:0')
c= tensor(7.5025e+08, device='cuda:0')
c= tensor(7.5608e+08, device='cuda:0')
c= tensor(7.5626e+08, device='cuda:0')
c= tensor(7.6031e+08, device='cuda:0')
c= tensor(7.6031e+08, device='cuda:0')
c= tensor(7.6106e+08, device='cuda:0')
c= tensor(7.6106e+08, device='cuda:0')
c= tensor(7.6106e+08, device='cuda:0')
c= tensor(7.6107e+08, device='cuda:0')
c= tensor(7.6144e+08, device='cuda:0')
c= tensor(7.6150e+08, device='cuda:0')
c= tensor(7.6386e+08, device='cuda:0')
c= tensor(7.6388e+08, device='cuda:0')
c= tensor(7.6388e+08, device='cuda:0')
c= tensor(7.6388e+08, device='cuda:0')
c= tensor(7.6445e+08, device='cuda:0')
c= tensor(7.6446e+08, device='cuda:0')
c= tensor(7.6564e+08, device='cuda:0')
c= tensor(7.6697e+08, device='cuda:0')
c= tensor(7.6699e+08, device='cuda:0')
c= tensor(7.6700e+08, device='cuda:0')
c= tensor(7.6718e+08, device='cuda:0')
c= tensor(8.5680e+08, device='cuda:0')
c= tensor(8.5681e+08, device='cuda:0')
c= tensor(8.5686e+08, device='cuda:0')
c= tensor(8.6133e+08, device='cuda:0')
c= tensor(8.6173e+08, device='cuda:0')
c= tensor(8.6173e+08, device='cuda:0')
c= tensor(8.6173e+08, device='cuda:0')
c= tensor(8.7113e+08, device='cuda:0')
c= tensor(8.7120e+08, device='cuda:0')
c= tensor(8.7190e+08, device='cuda:0')
c= tensor(8.7221e+08, device='cuda:0')
c= tensor(8.7225e+08, device='cuda:0')
c= tensor(8.7248e+08, device='cuda:0')
c= tensor(8.7507e+08, device='cuda:0')
c= tensor(8.7562e+08, device='cuda:0')
c= tensor(8.7562e+08, device='cuda:0')
c= tensor(8.7592e+08, device='cuda:0')
c= tensor(8.7593e+08, device='cuda:0')
c= tensor(8.7595e+08, device='cuda:0')
c= tensor(8.7603e+08, device='cuda:0')
c= tensor(8.7604e+08, device='cuda:0')
c= tensor(8.7775e+08, device='cuda:0')
c= tensor(8.7778e+08, device='cuda:0')
c= tensor(8.7778e+08, device='cuda:0')
c= tensor(8.7781e+08, device='cuda:0')
c= tensor(8.7795e+08, device='cuda:0')
c= tensor(8.7972e+08, device='cuda:0')
c= tensor(8.8402e+08, device='cuda:0')
c= tensor(8.8405e+08, device='cuda:0')
c= tensor(8.8406e+08, device='cuda:0')
c= tensor(8.8416e+08, device='cuda:0')
c= tensor(8.8416e+08, device='cuda:0')
c= tensor(8.8416e+08, device='cuda:0')
c= tensor(8.8425e+08, device='cuda:0')
c= tensor(8.9042e+08, device='cuda:0')
c= tensor(8.9042e+08, device='cuda:0')
c= tensor(8.9042e+08, device='cuda:0')
c= tensor(8.9042e+08, device='cuda:0')
c= tensor(8.9587e+08, device='cuda:0')
c= tensor(9.1070e+08, device='cuda:0')
c= tensor(9.1093e+08, device='cuda:0')
c= tensor(9.1093e+08, device='cuda:0')
c= tensor(9.1113e+08, device='cuda:0')
c= tensor(9.1162e+08, device='cuda:0')
c= tensor(9.1162e+08, device='cuda:0')
c= tensor(9.1169e+08, device='cuda:0')
c= tensor(9.1174e+08, device='cuda:0')
c= tensor(9.1776e+08, device='cuda:0')
c= tensor(9.1824e+08, device='cuda:0')
c= tensor(9.1928e+08, device='cuda:0')
c= tensor(9.1929e+08, device='cuda:0')
c= tensor(9.1932e+08, device='cuda:0')
c= tensor(9.1932e+08, device='cuda:0')
c= tensor(9.1951e+08, device='cuda:0')
c= tensor(9.1954e+08, device='cuda:0')
c= tensor(9.2007e+08, device='cuda:0')
c= tensor(9.2121e+08, device='cuda:0')
c= tensor(9.2611e+08, device='cuda:0')
c= tensor(9.2613e+08, device='cuda:0')
c= tensor(9.2615e+08, device='cuda:0')
c= tensor(9.2642e+08, device='cuda:0')
c= tensor(1.0752e+09, device='cuda:0')
c= tensor(1.0753e+09, device='cuda:0')
c= tensor(1.0753e+09, device='cuda:0')
c= tensor(1.0753e+09, device='cuda:0')
c= tensor(1.0755e+09, device='cuda:0')
c= tensor(1.0755e+09, device='cuda:0')
c= tensor(1.0755e+09, device='cuda:0')
c= tensor(1.0755e+09, device='cuda:0')
c= tensor(1.0756e+09, device='cuda:0')
c= tensor(1.0756e+09, device='cuda:0')
c= tensor(1.0756e+09, device='cuda:0')
c= tensor(1.0756e+09, device='cuda:0')
c= tensor(1.0788e+09, device='cuda:0')
c= tensor(1.0874e+09, device='cuda:0')
c= tensor(1.0886e+09, device='cuda:0')
c= tensor(1.0887e+09, device='cuda:0')
c= tensor(1.0888e+09, device='cuda:0')
c= tensor(1.0888e+09, device='cuda:0')
c= tensor(1.0888e+09, device='cuda:0')
c= tensor(1.0891e+09, device='cuda:0')
c= tensor(1.0894e+09, device='cuda:0')
c= tensor(1.0919e+09, device='cuda:0')
c= tensor(1.0919e+09, device='cuda:0')
c= tensor(1.1082e+09, device='cuda:0')
c= tensor(1.1083e+09, device='cuda:0')
c= tensor(1.1084e+09, device='cuda:0')
c= tensor(1.1155e+09, device='cuda:0')
c= tensor(1.1158e+09, device='cuda:0')
c= tensor(1.1233e+09, device='cuda:0')
c= tensor(1.1284e+09, device='cuda:0')
c= tensor(1.1288e+09, device='cuda:0')
c= tensor(1.1289e+09, device='cuda:0')
c= tensor(1.1289e+09, device='cuda:0')
c= tensor(1.1293e+09, device='cuda:0')
c= tensor(1.1293e+09, device='cuda:0')
c= tensor(1.1309e+09, device='cuda:0')
c= tensor(1.1701e+09, device='cuda:0')
c= tensor(1.1732e+09, device='cuda:0')
c= tensor(1.1741e+09, device='cuda:0')
c= tensor(1.1742e+09, device='cuda:0')
c= tensor(1.1742e+09, device='cuda:0')
c= tensor(1.1744e+09, device='cuda:0')
c= tensor(1.1744e+09, device='cuda:0')
c= tensor(1.1753e+09, device='cuda:0')
c= tensor(1.1786e+09, device='cuda:0')
c= tensor(1.1786e+09, device='cuda:0')
c= tensor(1.1821e+09, device='cuda:0')
c= tensor(1.1826e+09, device='cuda:0')
c= tensor(1.1827e+09, device='cuda:0')
c= tensor(1.1827e+09, device='cuda:0')
c= tensor(1.1834e+09, device='cuda:0')
c= tensor(1.1843e+09, device='cuda:0')
c= tensor(1.1843e+09, device='cuda:0')
c= tensor(1.2053e+09, device='cuda:0')
c= tensor(1.2056e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2065e+09, device='cuda:0')
c= tensor(1.3553e+09, device='cuda:0')
c= tensor(1.3553e+09, device='cuda:0')
c= tensor(1.3555e+09, device='cuda:0')
c= tensor(1.3555e+09, device='cuda:0')
c= tensor(1.3556e+09, device='cuda:0')
c= tensor(1.3556e+09, device='cuda:0')
c= tensor(1.3586e+09, device='cuda:0')
c= tensor(1.3587e+09, device='cuda:0')
c= tensor(1.4243e+09, device='cuda:0')
c= tensor(1.4243e+09, device='cuda:0')
c= tensor(1.4253e+09, device='cuda:0')
c= tensor(1.4254e+09, device='cuda:0')
c= tensor(1.4263e+09, device='cuda:0')
c= tensor(1.4355e+09, device='cuda:0')
c= tensor(1.4355e+09, device='cuda:0')
c= tensor(1.4355e+09, device='cuda:0')
c= tensor(1.4360e+09, device='cuda:0')
c= tensor(1.4361e+09, device='cuda:0')
c= tensor(1.4363e+09, device='cuda:0')
c= tensor(1.4373e+09, device='cuda:0')
c= tensor(1.4378e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4383e+09, device='cuda:0')
c= tensor(1.4390e+09, device='cuda:0')
c= tensor(1.4455e+09, device='cuda:0')
c= tensor(1.4455e+09, device='cuda:0')
c= tensor(1.4455e+09, device='cuda:0')
c= tensor(1.4486e+09, device='cuda:0')
c= tensor(1.4488e+09, device='cuda:0')
c= tensor(1.4494e+09, device='cuda:0')
c= tensor(1.4499e+09, device='cuda:0')
c= tensor(1.4504e+09, device='cuda:0')
c= tensor(1.4505e+09, device='cuda:0')
c= tensor(1.4525e+09, device='cuda:0')
c= tensor(1.4526e+09, device='cuda:0')
c= tensor(1.4526e+09, device='cuda:0')
c= tensor(1.4526e+09, device='cuda:0')
c= tensor(1.4527e+09, device='cuda:0')
c= tensor(1.4529e+09, device='cuda:0')
c= tensor(1.4536e+09, device='cuda:0')
c= tensor(1.4548e+09, device='cuda:0')
c= tensor(1.4548e+09, device='cuda:0')
c= tensor(1.4549e+09, device='cuda:0')
c= tensor(1.4553e+09, device='cuda:0')
c= tensor(1.4654e+09, device='cuda:0')
c= tensor(1.4657e+09, device='cuda:0')
c= tensor(1.4661e+09, device='cuda:0')
c= tensor(1.4677e+09, device='cuda:0')
c= tensor(1.4678e+09, device='cuda:0')
c= tensor(1.4678e+09, device='cuda:0')
c= tensor(1.4678e+09, device='cuda:0')
c= tensor(1.4687e+09, device='cuda:0')
c= tensor(1.4691e+09, device='cuda:0')
c= tensor(1.4694e+09, device='cuda:0')
c= tensor(1.4694e+09, device='cuda:0')
c= tensor(1.4694e+09, device='cuda:0')
c= tensor(1.4732e+09, device='cuda:0')
c= tensor(1.4736e+09, device='cuda:0')
c= tensor(1.4737e+09, device='cuda:0')
c= tensor(1.4737e+09, device='cuda:0')
c= tensor(1.4737e+09, device='cuda:0')
c= tensor(1.4738e+09, device='cuda:0')
c= tensor(1.4741e+09, device='cuda:0')
c= tensor(1.4750e+09, device='cuda:0')
c= tensor(1.4750e+09, device='cuda:0')
c= tensor(1.4825e+09, device='cuda:0')
c= tensor(1.4825e+09, device='cuda:0')
c= tensor(1.4826e+09, device='cuda:0')
c= tensor(1.4999e+09, device='cuda:0')
c= tensor(1.5001e+09, device='cuda:0')
c= tensor(1.5010e+09, device='cuda:0')
c= tensor(1.5030e+09, device='cuda:0')
c= tensor(1.5033e+09, device='cuda:0')
c= tensor(1.5037e+09, device='cuda:0')
c= tensor(1.5054e+09, device='cuda:0')
c= tensor(1.5054e+09, device='cuda:0')
c= tensor(1.5055e+09, device='cuda:0')
c= tensor(1.5055e+09, device='cuda:0')
c= tensor(1.5073e+09, device='cuda:0')
c= tensor(1.5094e+09, device='cuda:0')
c= tensor(1.5095e+09, device='cuda:0')
c= tensor(1.5099e+09, device='cuda:0')
c= tensor(1.5099e+09, device='cuda:0')
c= tensor(1.5100e+09, device='cuda:0')
c= tensor(1.5101e+09, device='cuda:0')
c= tensor(1.5101e+09, device='cuda:0')
c= tensor(1.5138e+09, device='cuda:0')
c= tensor(1.5172e+09, device='cuda:0')
c= tensor(1.5172e+09, device='cuda:0')
c= tensor(1.5173e+09, device='cuda:0')
c= tensor(1.5173e+09, device='cuda:0')
c= tensor(1.5338e+09, device='cuda:0')
c= tensor(1.5338e+09, device='cuda:0')
c= tensor(1.5338e+09, device='cuda:0')
c= tensor(1.5344e+09, device='cuda:0')
c= tensor(1.5348e+09, device='cuda:0')
c= tensor(1.5349e+09, device='cuda:0')
c= tensor(1.5350e+09, device='cuda:0')
c= tensor(1.5350e+09, device='cuda:0')
memory (bytes)
3848011776
time for making loss 2 is 14.931521654129028
p0 True
it  0 : 939197952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 16% |
shape of L is 
torch.Size([])
memory (bytes)
3848216576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3848871936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  22720432000.0
relative error loss 14.801232
shape of L is 
torch.Size([])
memory (bytes)
4100382720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  6% |
memory (bytes)
4100554752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  22720290000.0
relative error loss 14.80114
shape of L is 
torch.Size([])
memory (bytes)
4103286784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4103294976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  22719638000.0
relative error loss 14.8007145
shape of L is 
torch.Size([])
memory (bytes)
4105330688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
4105342976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  22717456000.0
relative error loss 14.7992935
shape of L is 
torch.Size([])
memory (bytes)
4107436032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
4107436032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  22704069000.0
relative error loss 14.790572
shape of L is 
torch.Size([])
memory (bytes)
4109598720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4109598720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  22557407000.0
relative error loss 14.695029
shape of L is 
torch.Size([])
memory (bytes)
4111691776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  6% |
memory (bytes)
4111712256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  21835543000.0
relative error loss 14.224771
shape of L is 
torch.Size([])
memory (bytes)
4113702912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
4113702912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  15145874000.0
relative error loss 9.866784
shape of L is 
torch.Size([])
memory (bytes)
4115881984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4115939328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  4724309500.0
relative error loss 3.0776527
shape of L is 
torch.Size([])
memory (bytes)
4118061056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4118061056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  2805291500.0
relative error loss 1.827508
time to take a step is 258.1533930301666
it  1 : 1311033344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4120010752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4120211456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  2805291500.0
relative error loss 1.827508
shape of L is 
torch.Size([])
memory (bytes)
4122329088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4122337280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1935672700.0
relative error loss 1.2609946
shape of L is 
torch.Size([])
memory (bytes)
4124319744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4124319744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1718093300.0
relative error loss 1.1192524
shape of L is 
torch.Size([])
memory (bytes)
4126609408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4126609408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1582311800.0
relative error loss 1.0307975
shape of L is 
torch.Size([])
memory (bytes)
4128759808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4128768000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  1531976200.0
relative error loss 0.99800634
shape of L is 
torch.Size([])
memory (bytes)
4130701312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4130893824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1491199500.0
relative error loss 0.97144234
shape of L is 
torch.Size([])
memory (bytes)
4132929536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4132929536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1670239500.0
relative error loss 1.088078
shape of L is 
torch.Size([])
memory (bytes)
4135174144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4135182336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1444743700.0
relative error loss 0.9411787
shape of L is 
torch.Size([])
memory (bytes)
4137115648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4137115648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1447422300.0
relative error loss 0.9429237
shape of L is 
torch.Size([])
memory (bytes)
4139339776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4139339776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1398991000.0
relative error loss 0.9113731
time to take a step is 245.5917329788208
it  2 : 1417272832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4141510656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4141518848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1398991000.0
relative error loss 0.9113731
shape of L is 
torch.Size([])
memory (bytes)
4143534080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4143534080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1325964800.0
relative error loss 0.86380017
shape of L is 
torch.Size([])
memory (bytes)
4145524736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4145524736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1277562500.0
relative error loss 0.8322685
shape of L is 
torch.Size([])
memory (bytes)
4147867648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4147867648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1226482200.0
relative error loss 0.79899216
shape of L is 
torch.Size([])
memory (bytes)
4149829632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4149829632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1171600400.0
relative error loss 0.7632394
shape of L is 
torch.Size([])
memory (bytes)
4152020992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4152020992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  1126068700.0
relative error loss 0.7335778
shape of L is 
torch.Size([])
memory (bytes)
4154236928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4154249216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1105894000.0
relative error loss 0.72043496
shape of L is 
torch.Size([])
memory (bytes)
4156313600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4156313600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1065481340.0
relative error loss 0.6941081
shape of L is 
torch.Size([])
memory (bytes)
4158275584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4158275584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  992409400.0
relative error loss 0.6465054
shape of L is 
torch.Size([])
memory (bytes)
4160626688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4160634880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  913701700.0
relative error loss 0.59523124
time to take a step is 237.1985056400299
it  3 : 1417272320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4162719744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4162719744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  913701700.0
relative error loss 0.59523124
shape of L is 
torch.Size([])
memory (bytes)
4164694016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4164853760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  828734660.0
relative error loss 0.53987944
shape of L is 
torch.Size([])
memory (bytes)
4166967296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4166967296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  758174200.0
relative error loss 0.49391282
shape of L is 
torch.Size([])
memory (bytes)
4169187328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4169187328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  716547900.0
relative error loss 0.46679533
shape of L is 
torch.Size([])
memory (bytes)
4171141120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4171358208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  678765630.0
relative error loss 0.44218206
shape of L is 
torch.Size([])
memory (bytes)
4173365248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4173365248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  613399040.0
relative error loss 0.399599
shape of L is 
torch.Size([])
memory (bytes)
4175634432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4175646720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  558055500.0
relative error loss 0.36354542
shape of L is 
torch.Size([])
memory (bytes)
4177731584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4177731584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  519231500.0
relative error loss 0.3382535
shape of L is 
torch.Size([])
memory (bytes)
4179947520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4179947520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  465463550.0
relative error loss 0.30322635
shape of L is 
torch.Size([])
memory (bytes)
4182093824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4182093824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  430840450.0
relative error loss 0.28067112
time to take a step is 237.69531917572021
c= tensor(393.4017, device='cuda:0')
c= tensor(26397.1309, device='cuda:0')
c= tensor(35081.7422, device='cuda:0')
c= tensor(36045.1875, device='cuda:0')
c= tensor(353298.0312, device='cuda:0')
c= tensor(452490.2500, device='cuda:0')
c= tensor(784566.8125, device='cuda:0')
c= tensor(908450.7500, device='cuda:0')
c= tensor(928052., device='cuda:0')
c= tensor(3015288.2500, device='cuda:0')
c= tensor(3021353.2500, device='cuda:0')
c= tensor(4108308., device='cuda:0')
c= tensor(4113491.2500, device='cuda:0')
c= tensor(8414913., device='cuda:0')
c= tensor(8521921., device='cuda:0')
c= tensor(8644752., device='cuda:0')
c= tensor(9014529., device='cuda:0')
c= tensor(9717212., device='cuda:0')
c= tensor(12769170., device='cuda:0')
c= tensor(15202230., device='cuda:0')
c= tensor(15210311., device='cuda:0')
c= tensor(22567056., device='cuda:0')
c= tensor(22604594., device='cuda:0')
c= tensor(22619794., device='cuda:0')
c= tensor(24290130., device='cuda:0')
c= tensor(24914440., device='cuda:0')
c= tensor(25689340., device='cuda:0')
c= tensor(25852434., device='cuda:0')
c= tensor(26378626., device='cuda:0')
c= tensor(2.0445e+08, device='cuda:0')
c= tensor(2.0446e+08, device='cuda:0')
c= tensor(2.1948e+08, device='cuda:0')
c= tensor(2.1951e+08, device='cuda:0')
c= tensor(2.1952e+08, device='cuda:0')
c= tensor(2.1952e+08, device='cuda:0')
c= tensor(2.1999e+08, device='cuda:0')
c= tensor(2.2055e+08, device='cuda:0')
c= tensor(2.2055e+08, device='cuda:0')
c= tensor(2.2056e+08, device='cuda:0')
c= tensor(2.2056e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2057e+08, device='cuda:0')
c= tensor(2.2058e+08, device='cuda:0')
c= tensor(2.2058e+08, device='cuda:0')
c= tensor(2.2058e+08, device='cuda:0')
c= tensor(2.2059e+08, device='cuda:0')
c= tensor(2.2061e+08, device='cuda:0')
c= tensor(2.2061e+08, device='cuda:0')
c= tensor(2.2061e+08, device='cuda:0')
c= tensor(2.2061e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2064e+08, device='cuda:0')
c= tensor(2.2064e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2066e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2068e+08, device='cuda:0')
c= tensor(2.2069e+08, device='cuda:0')
c= tensor(2.2069e+08, device='cuda:0')
c= tensor(2.2069e+08, device='cuda:0')
c= tensor(2.2069e+08, device='cuda:0')
c= tensor(2.2071e+08, device='cuda:0')
c= tensor(2.2071e+08, device='cuda:0')
c= tensor(2.2071e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2075e+08, device='cuda:0')
c= tensor(2.2075e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2080e+08, device='cuda:0')
c= tensor(2.2081e+08, device='cuda:0')
c= tensor(2.2081e+08, device='cuda:0')
c= tensor(2.2081e+08, device='cuda:0')
c= tensor(2.2081e+08, device='cuda:0')
c= tensor(2.2081e+08, device='cuda:0')
c= tensor(2.2082e+08, device='cuda:0')
c= tensor(2.2082e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2085e+08, device='cuda:0')
c= tensor(2.2085e+08, device='cuda:0')
c= tensor(2.2091e+08, device='cuda:0')
c= tensor(2.2091e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2093e+08, device='cuda:0')
c= tensor(2.2095e+08, device='cuda:0')
c= tensor(2.2096e+08, device='cuda:0')
c= tensor(2.2097e+08, device='cuda:0')
c= tensor(2.2097e+08, device='cuda:0')
c= tensor(2.2097e+08, device='cuda:0')
c= tensor(2.2098e+08, device='cuda:0')
c= tensor(2.2098e+08, device='cuda:0')
c= tensor(2.2098e+08, device='cuda:0')
c= tensor(2.2098e+08, device='cuda:0')
c= tensor(2.2099e+08, device='cuda:0')
c= tensor(2.2099e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2105e+08, device='cuda:0')
c= tensor(2.2105e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2107e+08, device='cuda:0')
c= tensor(2.2107e+08, device='cuda:0')
c= tensor(2.2107e+08, device='cuda:0')
c= tensor(2.2107e+08, device='cuda:0')
c= tensor(2.2107e+08, device='cuda:0')
c= tensor(2.2108e+08, device='cuda:0')
c= tensor(2.2108e+08, device='cuda:0')
c= tensor(2.2109e+08, device='cuda:0')
c= tensor(2.2109e+08, device='cuda:0')
c= tensor(2.2109e+08, device='cuda:0')
c= tensor(2.2111e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2115e+08, device='cuda:0')
c= tensor(2.2115e+08, device='cuda:0')
c= tensor(2.2116e+08, device='cuda:0')
c= tensor(2.2116e+08, device='cuda:0')
c= tensor(2.2117e+08, device='cuda:0')
c= tensor(2.2117e+08, device='cuda:0')
c= tensor(2.2118e+08, device='cuda:0')
c= tensor(2.2118e+08, device='cuda:0')
c= tensor(2.2118e+08, device='cuda:0')
c= tensor(2.2120e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2121e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2123e+08, device='cuda:0')
c= tensor(2.2123e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2125e+08, device='cuda:0')
c= tensor(2.2126e+08, device='cuda:0')
c= tensor(2.2126e+08, device='cuda:0')
c= tensor(2.2126e+08, device='cuda:0')
c= tensor(2.2127e+08, device='cuda:0')
c= tensor(2.2128e+08, device='cuda:0')
c= tensor(2.2128e+08, device='cuda:0')
c= tensor(2.2129e+08, device='cuda:0')
c= tensor(2.2150e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2154e+08, device='cuda:0')
c= tensor(2.2154e+08, device='cuda:0')
c= tensor(2.2155e+08, device='cuda:0')
c= tensor(2.2202e+08, device='cuda:0')
c= tensor(2.2523e+08, device='cuda:0')
c= tensor(2.2523e+08, device='cuda:0')
c= tensor(2.2549e+08, device='cuda:0')
c= tensor(2.2639e+08, device='cuda:0')
c= tensor(2.2642e+08, device='cuda:0')
c= tensor(2.6804e+08, device='cuda:0')
c= tensor(2.6804e+08, device='cuda:0')
c= tensor(2.6806e+08, device='cuda:0')
c= tensor(2.6876e+08, device='cuda:0')
c= tensor(2.8757e+08, device='cuda:0')
c= tensor(2.8757e+08, device='cuda:0')
c= tensor(2.8769e+08, device='cuda:0')
c= tensor(2.8988e+08, device='cuda:0')
c= tensor(2.9586e+08, device='cuda:0')
c= tensor(2.9718e+08, device='cuda:0')
c= tensor(2.9763e+08, device='cuda:0')
c= tensor(2.9776e+08, device='cuda:0')
c= tensor(2.9777e+08, device='cuda:0')
c= tensor(2.9825e+08, device='cuda:0')
c= tensor(3.0085e+08, device='cuda:0')
c= tensor(3.0085e+08, device='cuda:0')
c= tensor(3.0085e+08, device='cuda:0')
c= tensor(3.0140e+08, device='cuda:0')
c= tensor(3.0148e+08, device='cuda:0')
c= tensor(3.0909e+08, device='cuda:0')
c= tensor(3.0971e+08, device='cuda:0')
c= tensor(3.0971e+08, device='cuda:0')
c= tensor(3.0973e+08, device='cuda:0')
c= tensor(3.0974e+08, device='cuda:0')
c= tensor(3.0979e+08, device='cuda:0')
c= tensor(3.1053e+08, device='cuda:0')
c= tensor(3.1085e+08, device='cuda:0')
c= tensor(3.1103e+08, device='cuda:0')
c= tensor(3.1103e+08, device='cuda:0')
c= tensor(3.1104e+08, device='cuda:0')
c= tensor(3.1124e+08, device='cuda:0')
c= tensor(3.1188e+08, device='cuda:0')
c= tensor(3.1197e+08, device='cuda:0')
c= tensor(3.1198e+08, device='cuda:0')
c= tensor(3.2427e+08, device='cuda:0')
c= tensor(3.2429e+08, device='cuda:0')
c= tensor(3.2436e+08, device='cuda:0')
c= tensor(3.2532e+08, device='cuda:0')
c= tensor(3.2532e+08, device='cuda:0')
c= tensor(3.2549e+08, device='cuda:0')
c= tensor(3.2833e+08, device='cuda:0')
c= tensor(3.3551e+08, device='cuda:0')
c= tensor(3.3552e+08, device='cuda:0')
c= tensor(3.3558e+08, device='cuda:0')
c= tensor(3.3559e+08, device='cuda:0')
c= tensor(3.3559e+08, device='cuda:0')
c= tensor(3.3579e+08, device='cuda:0')
c= tensor(3.3581e+08, device='cuda:0')
c= tensor(3.3603e+08, device='cuda:0')
c= tensor(3.4433e+08, device='cuda:0')
c= tensor(3.4455e+08, device='cuda:0')
c= tensor(3.4458e+08, device='cuda:0')
c= tensor(3.4460e+08, device='cuda:0')
c= tensor(3.4648e+08, device='cuda:0')
c= tensor(3.4653e+08, device='cuda:0')
c= tensor(3.4653e+08, device='cuda:0')
c= tensor(3.4657e+08, device='cuda:0')
c= tensor(3.8234e+08, device='cuda:0')
c= tensor(3.8236e+08, device='cuda:0')
c= tensor(3.8421e+08, device='cuda:0')
c= tensor(3.8422e+08, device='cuda:0')
c= tensor(3.8460e+08, device='cuda:0')
c= tensor(3.8478e+08, device='cuda:0')
c= tensor(3.8599e+08, device='cuda:0')
c= tensor(3.8647e+08, device='cuda:0')
c= tensor(3.8648e+08, device='cuda:0')
c= tensor(3.8789e+08, device='cuda:0')
c= tensor(3.8881e+08, device='cuda:0')
c= tensor(3.8882e+08, device='cuda:0')
c= tensor(3.8902e+08, device='cuda:0')
c= tensor(3.9071e+08, device='cuda:0')
c= tensor(3.9786e+08, device='cuda:0')
c= tensor(3.9918e+08, device='cuda:0')
c= tensor(3.9921e+08, device='cuda:0')
c= tensor(3.9922e+08, device='cuda:0')
c= tensor(3.9936e+08, device='cuda:0')
c= tensor(3.9988e+08, device='cuda:0')
c= tensor(3.9989e+08, device='cuda:0')
c= tensor(3.9989e+08, device='cuda:0')
c= tensor(4.0059e+08, device='cuda:0')
c= tensor(4.0161e+08, device='cuda:0')
c= tensor(4.0399e+08, device='cuda:0')
c= tensor(4.0399e+08, device='cuda:0')
c= tensor(4.0405e+08, device='cuda:0')
c= tensor(4.0413e+08, device='cuda:0')
c= tensor(4.0414e+08, device='cuda:0')
c= tensor(4.0415e+08, device='cuda:0')
c= tensor(4.0416e+08, device='cuda:0')
c= tensor(4.1291e+08, device='cuda:0')
c= tensor(4.1306e+08, device='cuda:0')
c= tensor(4.1309e+08, device='cuda:0')
c= tensor(4.1312e+08, device='cuda:0')
c= tensor(4.1312e+08, device='cuda:0')
c= tensor(4.3245e+08, device='cuda:0')
c= tensor(4.3246e+08, device='cuda:0')
c= tensor(4.3358e+08, device='cuda:0')
c= tensor(4.3358e+08, device='cuda:0')
c= tensor(4.3358e+08, device='cuda:0')
c= tensor(4.3358e+08, device='cuda:0')
c= tensor(4.3377e+08, device='cuda:0')
c= tensor(4.3389e+08, device='cuda:0')
c= tensor(4.3391e+08, device='cuda:0')
c= tensor(4.3392e+08, device='cuda:0')
c= tensor(4.3392e+08, device='cuda:0')
c= tensor(4.4065e+08, device='cuda:0')
c= tensor(4.4084e+08, device='cuda:0')
c= tensor(4.4092e+08, device='cuda:0')
c= tensor(4.4146e+08, device='cuda:0')
c= tensor(4.4214e+08, device='cuda:0')
c= tensor(4.4214e+08, device='cuda:0')
c= tensor(4.4215e+08, device='cuda:0')
c= tensor(4.4216e+08, device='cuda:0')
c= tensor(4.4216e+08, device='cuda:0')
c= tensor(4.4216e+08, device='cuda:0')
c= tensor(4.4223e+08, device='cuda:0')
c= tensor(4.4224e+08, device='cuda:0')
c= tensor(4.4224e+08, device='cuda:0')
c= tensor(4.4224e+08, device='cuda:0')
c= tensor(4.4225e+08, device='cuda:0')
c= tensor(4.8558e+08, device='cuda:0')
c= tensor(4.8564e+08, device='cuda:0')
c= tensor(4.8586e+08, device='cuda:0')
c= tensor(4.8603e+08, device='cuda:0')
c= tensor(4.8604e+08, device='cuda:0')
c= tensor(4.8606e+08, device='cuda:0')
c= tensor(6.0105e+08, device='cuda:0')
c= tensor(6.3137e+08, device='cuda:0')
c= tensor(6.3169e+08, device='cuda:0')
c= tensor(6.3176e+08, device='cuda:0')
c= tensor(6.3176e+08, device='cuda:0')
c= tensor(6.3377e+08, device='cuda:0')
c= tensor(6.3419e+08, device='cuda:0')
c= tensor(6.3980e+08, device='cuda:0')
c= tensor(6.3981e+08, device='cuda:0')
c= tensor(6.4181e+08, device='cuda:0')
c= tensor(6.4483e+08, device='cuda:0')
c= tensor(6.4510e+08, device='cuda:0')
c= tensor(6.4511e+08, device='cuda:0')
c= tensor(6.4515e+08, device='cuda:0')
c= tensor(6.4516e+08, device='cuda:0')
c= tensor(6.4517e+08, device='cuda:0')
c= tensor(6.4643e+08, device='cuda:0')
c= tensor(6.4651e+08, device='cuda:0')
c= tensor(6.4651e+08, device='cuda:0')
c= tensor(6.4655e+08, device='cuda:0')
c= tensor(6.4657e+08, device='cuda:0')
c= tensor(6.4658e+08, device='cuda:0')
c= tensor(6.4733e+08, device='cuda:0')
c= tensor(6.4838e+08, device='cuda:0')
c= tensor(6.4846e+08, device='cuda:0')
c= tensor(6.4931e+08, device='cuda:0')
c= tensor(6.5000e+08, device='cuda:0')
c= tensor(6.5001e+08, device='cuda:0')
c= tensor(6.5011e+08, device='cuda:0')
c= tensor(6.5041e+08, device='cuda:0')
c= tensor(6.5202e+08, device='cuda:0')
c= tensor(6.5203e+08, device='cuda:0')
c= tensor(6.5881e+08, device='cuda:0')
c= tensor(6.7059e+08, device='cuda:0')
c= tensor(6.7080e+08, device='cuda:0')
c= tensor(6.7124e+08, device='cuda:0')
c= tensor(6.7148e+08, device='cuda:0')
c= tensor(6.7150e+08, device='cuda:0')
c= tensor(6.7150e+08, device='cuda:0')
c= tensor(6.7185e+08, device='cuda:0')
c= tensor(6.7344e+08, device='cuda:0')
c= tensor(6.7433e+08, device='cuda:0')
c= tensor(7.0658e+08, device='cuda:0')
c= tensor(7.0740e+08, device='cuda:0')
c= tensor(7.0847e+08, device='cuda:0')
c= tensor(7.0861e+08, device='cuda:0')
c= tensor(7.0968e+08, device='cuda:0')
c= tensor(7.0968e+08, device='cuda:0')
c= tensor(7.0970e+08, device='cuda:0')
c= tensor(7.1349e+08, device='cuda:0')
c= tensor(7.1353e+08, device='cuda:0')
c= tensor(7.1353e+08, device='cuda:0')
c= tensor(7.1361e+08, device='cuda:0')
c= tensor(7.1385e+08, device='cuda:0')
c= tensor(7.1398e+08, device='cuda:0')
c= tensor(7.1454e+08, device='cuda:0')
c= tensor(7.1468e+08, device='cuda:0')
c= tensor(7.1489e+08, device='cuda:0')
c= tensor(7.1490e+08, device='cuda:0')
c= tensor(7.1493e+08, device='cuda:0')
c= tensor(7.1500e+08, device='cuda:0')
c= tensor(7.1564e+08, device='cuda:0')
c= tensor(7.1564e+08, device='cuda:0')
c= tensor(7.1738e+08, device='cuda:0')
c= tensor(7.1738e+08, device='cuda:0')
c= tensor(7.1742e+08, device='cuda:0')
c= tensor(7.1743e+08, device='cuda:0')
c= tensor(7.1754e+08, device='cuda:0')
c= tensor(7.1755e+08, device='cuda:0')
c= tensor(7.1785e+08, device='cuda:0')
c= tensor(7.1788e+08, device='cuda:0')
c= tensor(7.1793e+08, device='cuda:0')
c= tensor(7.1847e+08, device='cuda:0')
c= tensor(7.2593e+08, device='cuda:0')
c= tensor(7.2593e+08, device='cuda:0')
c= tensor(7.2606e+08, device='cuda:0')
c= tensor(7.2817e+08, device='cuda:0')
c= tensor(7.2818e+08, device='cuda:0')
c= tensor(7.4472e+08, device='cuda:0')
c= tensor(7.4472e+08, device='cuda:0')
c= tensor(7.4583e+08, device='cuda:0')
c= tensor(7.5025e+08, device='cuda:0')
c= tensor(7.5025e+08, device='cuda:0')
c= tensor(7.5608e+08, device='cuda:0')
c= tensor(7.5626e+08, device='cuda:0')
c= tensor(7.6031e+08, device='cuda:0')
c= tensor(7.6031e+08, device='cuda:0')
c= tensor(7.6106e+08, device='cuda:0')
c= tensor(7.6106e+08, device='cuda:0')
c= tensor(7.6106e+08, device='cuda:0')
c= tensor(7.6107e+08, device='cuda:0')
c= tensor(7.6144e+08, device='cuda:0')
c= tensor(7.6150e+08, device='cuda:0')
c= tensor(7.6386e+08, device='cuda:0')
c= tensor(7.6388e+08, device='cuda:0')
c= tensor(7.6388e+08, device='cuda:0')
c= tensor(7.6388e+08, device='cuda:0')
c= tensor(7.6445e+08, device='cuda:0')
c= tensor(7.6446e+08, device='cuda:0')
c= tensor(7.6564e+08, device='cuda:0')
c= tensor(7.6697e+08, device='cuda:0')
c= tensor(7.6699e+08, device='cuda:0')
c= tensor(7.6700e+08, device='cuda:0')
c= tensor(7.6718e+08, device='cuda:0')
c= tensor(8.5680e+08, device='cuda:0')
c= tensor(8.5681e+08, device='cuda:0')
c= tensor(8.5686e+08, device='cuda:0')
c= tensor(8.6133e+08, device='cuda:0')
c= tensor(8.6173e+08, device='cuda:0')
c= tensor(8.6173e+08, device='cuda:0')
c= tensor(8.6173e+08, device='cuda:0')
c= tensor(8.7113e+08, device='cuda:0')
c= tensor(8.7120e+08, device='cuda:0')
c= tensor(8.7190e+08, device='cuda:0')
c= tensor(8.7221e+08, device='cuda:0')
c= tensor(8.7225e+08, device='cuda:0')
c= tensor(8.7248e+08, device='cuda:0')
c= tensor(8.7507e+08, device='cuda:0')
c= tensor(8.7562e+08, device='cuda:0')
c= tensor(8.7562e+08, device='cuda:0')
c= tensor(8.7592e+08, device='cuda:0')
c= tensor(8.7593e+08, device='cuda:0')
c= tensor(8.7595e+08, device='cuda:0')
c= tensor(8.7603e+08, device='cuda:0')
c= tensor(8.7604e+08, device='cuda:0')
c= tensor(8.7775e+08, device='cuda:0')
c= tensor(8.7778e+08, device='cuda:0')
c= tensor(8.7778e+08, device='cuda:0')
c= tensor(8.7781e+08, device='cuda:0')
c= tensor(8.7795e+08, device='cuda:0')
c= tensor(8.7972e+08, device='cuda:0')
c= tensor(8.8402e+08, device='cuda:0')
c= tensor(8.8405e+08, device='cuda:0')
c= tensor(8.8406e+08, device='cuda:0')
c= tensor(8.8416e+08, device='cuda:0')
c= tensor(8.8416e+08, device='cuda:0')
c= tensor(8.8416e+08, device='cuda:0')
c= tensor(8.8425e+08, device='cuda:0')
c= tensor(8.9042e+08, device='cuda:0')
c= tensor(8.9042e+08, device='cuda:0')
c= tensor(8.9042e+08, device='cuda:0')
c= tensor(8.9042e+08, device='cuda:0')
c= tensor(8.9587e+08, device='cuda:0')
c= tensor(9.1070e+08, device='cuda:0')
c= tensor(9.1093e+08, device='cuda:0')
c= tensor(9.1093e+08, device='cuda:0')
c= tensor(9.1113e+08, device='cuda:0')
c= tensor(9.1162e+08, device='cuda:0')
c= tensor(9.1162e+08, device='cuda:0')
c= tensor(9.1169e+08, device='cuda:0')
c= tensor(9.1174e+08, device='cuda:0')
c= tensor(9.1776e+08, device='cuda:0')
c= tensor(9.1824e+08, device='cuda:0')
c= tensor(9.1928e+08, device='cuda:0')
c= tensor(9.1929e+08, device='cuda:0')
c= tensor(9.1932e+08, device='cuda:0')
c= tensor(9.1932e+08, device='cuda:0')
c= tensor(9.1951e+08, device='cuda:0')
c= tensor(9.1954e+08, device='cuda:0')
c= tensor(9.2007e+08, device='cuda:0')
c= tensor(9.2121e+08, device='cuda:0')
c= tensor(9.2611e+08, device='cuda:0')
c= tensor(9.2613e+08, device='cuda:0')
c= tensor(9.2615e+08, device='cuda:0')
c= tensor(9.2642e+08, device='cuda:0')
c= tensor(1.0752e+09, device='cuda:0')
c= tensor(1.0753e+09, device='cuda:0')
c= tensor(1.0753e+09, device='cuda:0')
c= tensor(1.0753e+09, device='cuda:0')
c= tensor(1.0755e+09, device='cuda:0')
c= tensor(1.0755e+09, device='cuda:0')
c= tensor(1.0755e+09, device='cuda:0')
c= tensor(1.0755e+09, device='cuda:0')
c= tensor(1.0756e+09, device='cuda:0')
c= tensor(1.0756e+09, device='cuda:0')
c= tensor(1.0756e+09, device='cuda:0')
c= tensor(1.0756e+09, device='cuda:0')
c= tensor(1.0788e+09, device='cuda:0')
c= tensor(1.0874e+09, device='cuda:0')
c= tensor(1.0886e+09, device='cuda:0')
c= tensor(1.0887e+09, device='cuda:0')
c= tensor(1.0888e+09, device='cuda:0')
c= tensor(1.0888e+09, device='cuda:0')
c= tensor(1.0888e+09, device='cuda:0')
c= tensor(1.0891e+09, device='cuda:0')
c= tensor(1.0894e+09, device='cuda:0')
c= tensor(1.0919e+09, device='cuda:0')
c= tensor(1.0919e+09, device='cuda:0')
c= tensor(1.1082e+09, device='cuda:0')
c= tensor(1.1083e+09, device='cuda:0')
c= tensor(1.1084e+09, device='cuda:0')
c= tensor(1.1155e+09, device='cuda:0')
c= tensor(1.1158e+09, device='cuda:0')
c= tensor(1.1233e+09, device='cuda:0')
c= tensor(1.1284e+09, device='cuda:0')
c= tensor(1.1288e+09, device='cuda:0')
c= tensor(1.1289e+09, device='cuda:0')
c= tensor(1.1289e+09, device='cuda:0')
c= tensor(1.1293e+09, device='cuda:0')
c= tensor(1.1293e+09, device='cuda:0')
c= tensor(1.1309e+09, device='cuda:0')
c= tensor(1.1701e+09, device='cuda:0')
c= tensor(1.1732e+09, device='cuda:0')
c= tensor(1.1741e+09, device='cuda:0')
c= tensor(1.1742e+09, device='cuda:0')
c= tensor(1.1742e+09, device='cuda:0')
c= tensor(1.1744e+09, device='cuda:0')
c= tensor(1.1744e+09, device='cuda:0')
c= tensor(1.1753e+09, device='cuda:0')
c= tensor(1.1786e+09, device='cuda:0')
c= tensor(1.1786e+09, device='cuda:0')
c= tensor(1.1821e+09, device='cuda:0')
c= tensor(1.1826e+09, device='cuda:0')
c= tensor(1.1827e+09, device='cuda:0')
c= tensor(1.1827e+09, device='cuda:0')
c= tensor(1.1834e+09, device='cuda:0')
c= tensor(1.1843e+09, device='cuda:0')
c= tensor(1.1843e+09, device='cuda:0')
c= tensor(1.2053e+09, device='cuda:0')
c= tensor(1.2056e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2065e+09, device='cuda:0')
c= tensor(1.3553e+09, device='cuda:0')
c= tensor(1.3553e+09, device='cuda:0')
c= tensor(1.3555e+09, device='cuda:0')
c= tensor(1.3555e+09, device='cuda:0')
c= tensor(1.3556e+09, device='cuda:0')
c= tensor(1.3556e+09, device='cuda:0')
c= tensor(1.3586e+09, device='cuda:0')
c= tensor(1.3587e+09, device='cuda:0')
c= tensor(1.4243e+09, device='cuda:0')
c= tensor(1.4243e+09, device='cuda:0')
c= tensor(1.4253e+09, device='cuda:0')
c= tensor(1.4254e+09, device='cuda:0')
c= tensor(1.4263e+09, device='cuda:0')
c= tensor(1.4355e+09, device='cuda:0')
c= tensor(1.4355e+09, device='cuda:0')
c= tensor(1.4355e+09, device='cuda:0')
c= tensor(1.4360e+09, device='cuda:0')
c= tensor(1.4361e+09, device='cuda:0')
c= tensor(1.4363e+09, device='cuda:0')
c= tensor(1.4373e+09, device='cuda:0')
c= tensor(1.4378e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4383e+09, device='cuda:0')
c= tensor(1.4390e+09, device='cuda:0')
c= tensor(1.4455e+09, device='cuda:0')
c= tensor(1.4455e+09, device='cuda:0')
c= tensor(1.4455e+09, device='cuda:0')
c= tensor(1.4486e+09, device='cuda:0')
c= tensor(1.4488e+09, device='cuda:0')
c= tensor(1.4494e+09, device='cuda:0')
c= tensor(1.4499e+09, device='cuda:0')
c= tensor(1.4504e+09, device='cuda:0')
c= tensor(1.4505e+09, device='cuda:0')
c= tensor(1.4525e+09, device='cuda:0')
c= tensor(1.4526e+09, device='cuda:0')
c= tensor(1.4526e+09, device='cuda:0')
c= tensor(1.4526e+09, device='cuda:0')
c= tensor(1.4527e+09, device='cuda:0')
c= tensor(1.4529e+09, device='cuda:0')
c= tensor(1.4536e+09, device='cuda:0')
c= tensor(1.4548e+09, device='cuda:0')
c= tensor(1.4548e+09, device='cuda:0')
c= tensor(1.4549e+09, device='cuda:0')
c= tensor(1.4553e+09, device='cuda:0')
c= tensor(1.4654e+09, device='cuda:0')
c= tensor(1.4657e+09, device='cuda:0')
c= tensor(1.4661e+09, device='cuda:0')
c= tensor(1.4677e+09, device='cuda:0')
c= tensor(1.4678e+09, device='cuda:0')
c= tensor(1.4678e+09, device='cuda:0')
c= tensor(1.4678e+09, device='cuda:0')
c= tensor(1.4687e+09, device='cuda:0')
c= tensor(1.4691e+09, device='cuda:0')
c= tensor(1.4694e+09, device='cuda:0')
c= tensor(1.4694e+09, device='cuda:0')
c= tensor(1.4694e+09, device='cuda:0')
c= tensor(1.4732e+09, device='cuda:0')
c= tensor(1.4736e+09, device='cuda:0')
c= tensor(1.4737e+09, device='cuda:0')
c= tensor(1.4737e+09, device='cuda:0')
c= tensor(1.4737e+09, device='cuda:0')
c= tensor(1.4738e+09, device='cuda:0')
c= tensor(1.4741e+09, device='cuda:0')
c= tensor(1.4750e+09, device='cuda:0')
c= tensor(1.4750e+09, device='cuda:0')
c= tensor(1.4825e+09, device='cuda:0')
c= tensor(1.4825e+09, device='cuda:0')
c= tensor(1.4826e+09, device='cuda:0')
c= tensor(1.4999e+09, device='cuda:0')
c= tensor(1.5001e+09, device='cuda:0')
c= tensor(1.5010e+09, device='cuda:0')
c= tensor(1.5030e+09, device='cuda:0')
c= tensor(1.5033e+09, device='cuda:0')
c= tensor(1.5037e+09, device='cuda:0')
c= tensor(1.5054e+09, device='cuda:0')
c= tensor(1.5054e+09, device='cuda:0')
c= tensor(1.5055e+09, device='cuda:0')
c= tensor(1.5055e+09, device='cuda:0')
c= tensor(1.5073e+09, device='cuda:0')
c= tensor(1.5094e+09, device='cuda:0')
c= tensor(1.5095e+09, device='cuda:0')
c= tensor(1.5099e+09, device='cuda:0')
c= tensor(1.5099e+09, device='cuda:0')
c= tensor(1.5100e+09, device='cuda:0')
c= tensor(1.5101e+09, device='cuda:0')
c= tensor(1.5101e+09, device='cuda:0')
c= tensor(1.5138e+09, device='cuda:0')
c= tensor(1.5172e+09, device='cuda:0')
c= tensor(1.5172e+09, device='cuda:0')
c= tensor(1.5173e+09, device='cuda:0')
c= tensor(1.5173e+09, device='cuda:0')
c= tensor(1.5338e+09, device='cuda:0')
c= tensor(1.5338e+09, device='cuda:0')
c= tensor(1.5338e+09, device='cuda:0')
c= tensor(1.5344e+09, device='cuda:0')
c= tensor(1.5348e+09, device='cuda:0')
c= tensor(1.5349e+09, device='cuda:0')
c= tensor(1.5350e+09, device='cuda:0')
c= tensor(1.5350e+09, device='cuda:0')
time to make c is 11.27050495147705
time for making loss is 11.270539283752441
p0 True
it  0 : 939425792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
4184207360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
4184420352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  430840450.0
relative error loss 0.28067112
shape of L is 
torch.Size([])
memory (bytes)
4211417088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4211417088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  426933380.0
relative error loss 0.27812588
shape of L is 
torch.Size([])
memory (bytes)
4214947840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4215033856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  409938300.0
relative error loss 0.26705444
shape of L is 
torch.Size([])
memory (bytes)
4218245120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4218245120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  403905150.0
relative error loss 0.26312414
shape of L is 
torch.Size([])
memory (bytes)
4221423616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4221452288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  399030000.0
relative error loss 0.25994822
shape of L is 
torch.Size([])
memory (bytes)
4224507904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4224507904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  393704830.0
relative error loss 0.2564791
shape of L is 
torch.Size([])
memory (bytes)
4227858432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4227887104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  390662800.0
relative error loss 0.25449738
shape of L is 
torch.Size([])
memory (bytes)
4231110656
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4231110656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  387517950.0
relative error loss 0.25244868
shape of L is 
torch.Size([])
memory (bytes)
4234301440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4234330112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  385561730.0
relative error loss 0.2511743
shape of L is 
torch.Size([])
memory (bytes)
4237516800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4237545472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  383689340.0
relative error loss 0.24995454
time to take a step is 305.0860273838043
it  1 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4240748544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4240764928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  383689340.0
relative error loss 0.24995454
shape of L is 
torch.Size([])
memory (bytes)
4243972096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4243976192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  382167300.0
relative error loss 0.248963
shape of L is 
torch.Size([])
memory (bytes)
4247183360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4247212032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  380586750.0
relative error loss 0.24793334
shape of L is 
torch.Size([])
memory (bytes)
4250345472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4250345472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  379639040.0
relative error loss 0.24731596
shape of L is 
torch.Size([])
memory (bytes)
4253622272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4253650944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  378721400.0
relative error loss 0.24671817
shape of L is 
torch.Size([])
memory (bytes)
4256743424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4256743424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  377800450.0
relative error loss 0.24611822
shape of L is 
torch.Size([])
memory (bytes)
4260073472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4260102144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  376573700.0
relative error loss 0.24531904
shape of L is 
torch.Size([])
memory (bytes)
4263333888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  7% |
memory (bytes)
4263333888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  376005120.0
relative error loss 0.24494864
shape of L is 
torch.Size([])
memory (bytes)
4266536960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4266536960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  375345400.0
relative error loss 0.24451888
shape of L is 
torch.Size([])
memory (bytes)
4269785088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  7% |
memory (bytes)
4269785088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  374626180.0
relative error loss 0.24405032
time to take a step is 299.61830592155457
it  2 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4272787456
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4273004544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  7% |
error is  374626180.0
relative error loss 0.24405032
shape of L is 
torch.Size([])
memory (bytes)
4276219904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4276244480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  373829000.0
relative error loss 0.243531
shape of L is 
torch.Size([])
memory (bytes)
4279336960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4279472128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  373415800.0
relative error loss 0.24326183
shape of L is 
torch.Size([])
memory (bytes)
4282695680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4282695680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  372847870.0
relative error loss 0.24289185
shape of L is 
torch.Size([])
memory (bytes)
4285886464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4285886464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  372669200.0
relative error loss 0.24277544
shape of L is 
torch.Size([])
memory (bytes)
4289110016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4289110016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  371957760.0
relative error loss 0.24231198
shape of L is 
torch.Size([])
memory (bytes)
4292362240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4292362240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  371670140.0
relative error loss 0.24212462
shape of L is 
torch.Size([])
memory (bytes)
4295475200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4295585792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  371385200.0
relative error loss 0.24193901
shape of L is 
torch.Size([])
memory (bytes)
4298772480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4298801152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  370908160.0
relative error loss 0.24162823
shape of L is 
torch.Size([])
memory (bytes)
4302016512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4302016512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  370917500.0
relative error loss 0.24163431
shape of L is 
torch.Size([])
memory (bytes)
4305223680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4305252352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  370616320.0
relative error loss 0.2414381
time to take a step is 324.6913056373596
it  3 : 1419546624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4308451328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4308475904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  370616320.0
relative error loss 0.2414381
shape of L is 
torch.Size([])
memory (bytes)
4311560192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4311674880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  370272000.0
relative error loss 0.2412138
shape of L is 
torch.Size([])
memory (bytes)
4314877952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4314906624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  369969400.0
relative error loss 0.24101667
shape of L is 
torch.Size([])
memory (bytes)
4318007296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4318126080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  369587840.0
relative error loss 0.2407681
shape of L is 
torch.Size([])
memory (bytes)
4321341440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4321341440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  369289200.0
relative error loss 0.24057357
shape of L is 
torch.Size([])
memory (bytes)
4324438016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4324560896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  369088640.0
relative error loss 0.2404429
shape of L is 
torch.Size([])
memory (bytes)
4327677952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4327780352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  368866560.0
relative error loss 0.24029823
shape of L is 
torch.Size([])
memory (bytes)
4331003904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4331003904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  368634240.0
relative error loss 0.24014688
shape of L is 
torch.Size([])
memory (bytes)
4334120960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4334231552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  368393600.0
relative error loss 0.23999012
shape of L is 
torch.Size([])
memory (bytes)
4337446912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4337446912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  368216960.0
relative error loss 0.23987505
time to take a step is 292.5459680557251
it  4 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4340568064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4340678656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  368216960.0
relative error loss 0.23987505
shape of L is 
torch.Size([])
memory (bytes)
4343889920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4343889920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  368015500.0
relative error loss 0.2397438
shape of L is 
torch.Size([])
memory (bytes)
4347080704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4347080704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  367870850.0
relative error loss 0.23964956
shape of L is 
torch.Size([])
memory (bytes)
4350345216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4350345216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  367738370.0
relative error loss 0.23956327
shape of L is 
torch.Size([])
memory (bytes)
4353564672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4353564672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  367563140.0
relative error loss 0.23944911
shape of L is 
torch.Size([])
memory (bytes)
4356694016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4356780032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  367411460.0
relative error loss 0.2393503
shape of L is 
torch.Size([])
memory (bytes)
4360007680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4360007680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  367317000.0
relative error loss 0.23928876
shape of L is 
torch.Size([])
memory (bytes)
4363194368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4363223040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  8% |
error is  367105800.0
relative error loss 0.23915118
shape of L is 
torch.Size([])
memory (bytes)
4366413824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4366438400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  366968200.0
relative error loss 0.23906153
shape of L is 
torch.Size([])
memory (bytes)
4369494016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4369657856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  366861820.0
relative error loss 0.23899224
time to take a step is 293.74322056770325
it  5 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4372881408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4372881408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  366861820.0
relative error loss 0.23899224
shape of L is 
torch.Size([])
memory (bytes)
4375941120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4376096768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  366562560.0
relative error loss 0.23879729
shape of L is 
torch.Size([])
memory (bytes)
4379172864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4379328512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  366403330.0
relative error loss 0.23869355
shape of L is 
torch.Size([])
memory (bytes)
4382531584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4382560256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  366218750.0
relative error loss 0.23857331
shape of L is 
torch.Size([])
memory (bytes)
4385660928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4385771520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  366013060.0
relative error loss 0.2384393
shape of L is 
torch.Size([])
memory (bytes)
4388974592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4389003264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  365916160.0
relative error loss 0.23837619
shape of L is 
torch.Size([])
memory (bytes)
4392116224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  7% |
memory (bytes)
4392116224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  365780860.0
relative error loss 0.23828804
shape of L is 
torch.Size([])
memory (bytes)
4395446272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4395446272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  365683330.0
relative error loss 0.2382245
shape of L is 
torch.Size([])
memory (bytes)
4398555136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4398555136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  365564670.0
relative error loss 0.23814721
shape of L is 
torch.Size([])
memory (bytes)
4401893376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4401893376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  365701760.0
relative error loss 0.23823652
shape of L is 
torch.Size([])
memory (bytes)
4404994048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4405125120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  365484160.0
relative error loss 0.23809476
time to take a step is 321.7380747795105
it  6 : 1419546624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4408291328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4408291328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  365484160.0
relative error loss 0.23809476
shape of L is 
torch.Size([])
memory (bytes)
4411518976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4411551744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  365371400.0
relative error loss 0.2380213
shape of L is 
torch.Size([])
memory (bytes)
4414623744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4414783488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  365277440.0
relative error loss 0.2379601
shape of L is 
torch.Size([])
memory (bytes)
4417970176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4417998848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  365159800.0
relative error loss 0.23788346
shape of L is 
torch.Size([])
memory (bytes)
4421038080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4421210112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  365046140.0
relative error loss 0.23780942
shape of L is 
torch.Size([])
memory (bytes)
4424417280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4424445952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364935420.0
relative error loss 0.23773728
shape of L is 
torch.Size([])
memory (bytes)
4427526144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4427669504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364807550.0
relative error loss 0.23765399
shape of L is 
torch.Size([])
memory (bytes)
4430761984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4430880768
| ID | GPU | MEM |
------------------
|  0 |  4% |  0% |
|  1 | 91% |  8% |
error is  364719230.0
relative error loss 0.23759645
shape of L is 
torch.Size([])
memory (bytes)
4434075648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4434104320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  8% |
error is  364614270.0
relative error loss 0.23752807
shape of L is 
torch.Size([])
memory (bytes)
4437291008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4437291008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364533250.0
relative error loss 0.23747529
time to take a step is 293.28660798072815
it  7 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4440563712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4440563712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  364533250.0
relative error loss 0.23747529
shape of L is 
torch.Size([])
memory (bytes)
4443742208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4443742208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364461820.0
relative error loss 0.23742875
shape of L is 
torch.Size([])
memory (bytes)
4446887936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4446998528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  8% |
error is  364405900.0
relative error loss 0.23739232
shape of L is 
torch.Size([])
memory (bytes)
4450189312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4450217984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364358270.0
relative error loss 0.2373613
shape of L is 
torch.Size([])
memory (bytes)
4453310464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4453416960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364282750.0
relative error loss 0.23731211
shape of L is 
torch.Size([])
memory (bytes)
4456660992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4456660992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364242940.0
relative error loss 0.23728617
shape of L is 
torch.Size([])
memory (bytes)
4459819008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4459819008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364181900.0
relative error loss 0.2372464
shape of L is 
torch.Size([])
memory (bytes)
4462952448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4463099904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364185860.0
relative error loss 0.23724899
shape of L is 
torch.Size([])
memory (bytes)
4466307072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4466331648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364142600.0
relative error loss 0.2372208
shape of L is 
torch.Size([])
memory (bytes)
4469415936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4469555200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  364078080.0
relative error loss 0.23717877
time to take a step is 292.822114944458
it  8 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4472778752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4472778752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  364078080.0
relative error loss 0.23717877
shape of L is 
torch.Size([])
memory (bytes)
4475887616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4475990016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363980030.0
relative error loss 0.23711489
shape of L is 
torch.Size([])
memory (bytes)
4479160320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4479225856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363904900.0
relative error loss 0.23706596
shape of L is 
torch.Size([])
memory (bytes)
4482437120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4482437120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  363815680.0
relative error loss 0.23700783
shape of L is 
torch.Size([])
memory (bytes)
4485570560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4485570560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363715460.0
relative error loss 0.23694254
shape of L is 
torch.Size([])
memory (bytes)
4488896512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4488896512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363645570.0
relative error loss 0.236897
shape of L is 
torch.Size([])
memory (bytes)
4492070912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4492070912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  363561100.0
relative error loss 0.23684198
shape of L is 
torch.Size([])
memory (bytes)
4495159296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4495335424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363509900.0
relative error loss 0.23680863
shape of L is 
torch.Size([])
memory (bytes)
4498542592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4498571264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  8% |
error is  363455500.0
relative error loss 0.23677318
shape of L is 
torch.Size([])
memory (bytes)
4501753856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4501753856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363394430.0
relative error loss 0.2367334
time to take a step is 293.509272813797
it  9 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4505014272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4505014272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  363394430.0
relative error loss 0.2367334
shape of L is 
torch.Size([])
memory (bytes)
4508221440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4508221440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363326600.0
relative error loss 0.23668921
shape of L is 
torch.Size([])
memory (bytes)
4511395840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4511449088
| ID | GPU  | MEM |
-------------------
|  0 |   5% |  0% |
|  1 | 100% |  8% |
error is  363278980.0
relative error loss 0.2366582
shape of L is 
torch.Size([])
memory (bytes)
4514672640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4514676736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363235200.0
relative error loss 0.23662968
shape of L is 
torch.Size([])
memory (bytes)
4517756928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4517756928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363211780.0
relative error loss 0.23661442
shape of L is 
torch.Size([])
memory (bytes)
4521127936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4521127936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  363095680.0
relative error loss 0.23653878
shape of L is 
torch.Size([])
memory (bytes)
4524322816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4524322816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363058700.0
relative error loss 0.23651469
shape of L is 
torch.Size([])
memory (bytes)
4527529984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4527529984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363000060.0
relative error loss 0.2364765
shape of L is 
torch.Size([])
memory (bytes)
4530790400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4530790400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362920960.0
relative error loss 0.23642497
shape of L is 
torch.Size([])
memory (bytes)
4533977088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4533977088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362856450.0
relative error loss 0.23638293
time to take a step is 293.90959572792053
it  10 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4537225216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4537225216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  362856450.0
relative error loss 0.23638293
shape of L is 
torch.Size([])
memory (bytes)
4540362752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4540362752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362809730.0
relative error loss 0.2363525
shape of L is 
torch.Size([])
memory (bytes)
4543639552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4543639552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362764930.0
relative error loss 0.23632331
shape of L is 
torch.Size([])
memory (bytes)
4546899968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4546899968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362670600.0
relative error loss 0.23626186
shape of L is 
torch.Size([])
memory (bytes)
4550070272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4550070272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362638340.0
relative error loss 0.23624085
shape of L is 
torch.Size([])
memory (bytes)
4553347072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4553347072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362554500.0
relative error loss 0.23618624
shape of L is 
torch.Size([])
memory (bytes)
4556468224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4556468224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  8% |
error is  362510340.0
relative error loss 0.23615746
shape of L is 
torch.Size([])
memory (bytes)
4559765504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4559765504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362464640.0
relative error loss 0.23612769
shape of L is 
torch.Size([])
memory (bytes)
4562984960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4563013632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362401540.0
relative error loss 0.23608659
shape of L is 
torch.Size([])
memory (bytes)
4566163456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4566163456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362337020.0
relative error loss 0.23604456
time to take a step is 294.49046516418457
it  11 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4569464832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4569464832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  362337020.0
relative error loss 0.23604456
shape of L is 
torch.Size([])
memory (bytes)
4572565504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4572565504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362303360.0
relative error loss 0.23602262
shape of L is 
torch.Size([])
memory (bytes)
4575911936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4575911936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362250750.0
relative error loss 0.23598836
shape of L is 
torch.Size([])
memory (bytes)
4579106816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  8% |
memory (bytes)
4579106816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362212480.0
relative error loss 0.23596342
shape of L is 
torch.Size([])
memory (bytes)
4582273024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4582273024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362160260.0
relative error loss 0.2359294
shape of L is 
torch.Size([])
memory (bytes)
4585566208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4585570304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362127870.0
relative error loss 0.2359083
shape of L is 
torch.Size([])
memory (bytes)
4588687360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4588687360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  362103300.0
relative error loss 0.2358923
shape of L is 
torch.Size([])
memory (bytes)
4592013312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4592013312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362045300.0
relative error loss 0.23585452
shape of L is 
torch.Size([])
memory (bytes)
4595183616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4595183616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361991420.0
relative error loss 0.23581941
shape of L is 
torch.Size([])
memory (bytes)
4598456320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4598456320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361960060.0
relative error loss 0.23579898
time to take a step is 302.4709484577179
it  12 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4601675776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4601675776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  361960060.0
relative error loss 0.23579898
shape of L is 
torch.Size([])
memory (bytes)
4604784640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4604895232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361919230.0
relative error loss 0.23577239
shape of L is 
torch.Size([])
memory (bytes)
4608114688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4608122880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361857660.0
relative error loss 0.23573229
shape of L is 
torch.Size([])
memory (bytes)
4611227648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4611227648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  361912580.0
relative error loss 0.23576805
shape of L is 
torch.Size([])
memory (bytes)
4614561792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4614561792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361818370.0
relative error loss 0.23570669
shape of L is 
torch.Size([])
memory (bytes)
4617756672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4617756672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361767680.0
relative error loss 0.23567367
shape of L is 
torch.Size([])
memory (bytes)
4620992512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4620992512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361722750.0
relative error loss 0.23564439
shape of L is 
torch.Size([])
memory (bytes)
4624105472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4624105472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  8% |
error is  361683840.0
relative error loss 0.23561904
shape of L is 
torch.Size([])
memory (bytes)
4627349504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4627431424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361639040.0
relative error loss 0.23558986
shape of L is 
torch.Size([])
memory (bytes)
4630654976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4630654976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361608700.0
relative error loss 0.23557009
time to take a step is 308.2781777381897
it  13 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4633767936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4633767936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  361608700.0
relative error loss 0.23557009
shape of L is 
torch.Size([])
memory (bytes)
4637077504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4637077504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361576320.0
relative error loss 0.235549
shape of L is 
torch.Size([])
memory (bytes)
4640260096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4640260096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361555460.0
relative error loss 0.23553541
shape of L is 
torch.Size([])
memory (bytes)
4643532800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4643532800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361512450.0
relative error loss 0.23550738
shape of L is 
torch.Size([])
memory (bytes)
4646699008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4646699008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361492860.0
relative error loss 0.23549463
shape of L is 
torch.Size([])
memory (bytes)
4649979904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4649979904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361469700.0
relative error loss 0.23547953
shape of L is 
torch.Size([])
memory (bytes)
4653170688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4653195264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361441920.0
relative error loss 0.23546144
shape of L is 
torch.Size([])
memory (bytes)
4656390144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4656418816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  8% |
error is  361400700.0
relative error loss 0.23543459
shape of L is 
torch.Size([])
memory (bytes)
4659625984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4659625984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361377660.0
relative error loss 0.23541959
shape of L is 
torch.Size([])
memory (bytes)
4662771712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4662771712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361352580.0
relative error loss 0.23540324
time to take a step is 304.2073881626129
it  14 : 1419546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4666040320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4666068992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  361352580.0
relative error loss 0.23540324
shape of L is 
torch.Size([])
memory (bytes)
4669227008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4669227008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361302800.0
relative error loss 0.2353708
shape of L is 
torch.Size([])
memory (bytes)
4672512000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4672512000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  8% |
error is  361254000.0
relative error loss 0.23533903
shape of L is 
torch.Size([])
memory (bytes)
4675706880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4675735552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  361220350.0
relative error loss 0.2353171
shape of L is 
torch.Size([])
memory (bytes)
4678926336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4678963200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  361193100.0
relative error loss 0.23529935
shape of L is 
torch.Size([])
memory (bytes)
4682162176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4682190848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361120900.0
relative error loss 0.2352523
shape of L is 
torch.Size([])
memory (bytes)
4685258752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4685258752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361129200.0
relative error loss 0.23525773
shape of L is 
torch.Size([])
memory (bytes)
4688588800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4688617472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361088500.0
relative error loss 0.23523122
shape of L is 
torch.Size([])
memory (bytes)
4691726336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4691726336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361060100.0
relative error loss 0.2352127
shape of L is 
torch.Size([])
memory (bytes)
4695068672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4695068672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361008900.0
relative error loss 0.23517935
time to take a step is 294.0237364768982
sum tnnu_Z after tensor(6174227., device='cuda:0')
shape of features
(4729,)
shape of features
(4729,)
number of orig particles 18917
number of new particles after remove low mass 18917
tnuZ shape should be parts x labs
torch.Size([18917, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  430556900.0
relative error without small mass is  0.2804864
nnu_Z shape should be number of particles by maxV
(18917, 702)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
shape of features
(18917,)
Wed Feb 1 18:01:15 EST 2023
