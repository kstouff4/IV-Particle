Tue Jan 31 03:42:31 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 37944506
numbers of Z: 22006
shape of features
(22006,)
shape of features
(22006,)
ZX	Vol	Parts	Cubes	Eps
Z	0.016005996479555657	22006	22.006	0.08993192082749403
X	0.014281793941542128	1183	1.183	0.2294031301126105
X	0.01463654144241118	22290	22.29	0.0869179482704705
X	0.01526911955571625	2120	2.12	0.1931194628932878
X	0.014508985388475648	16054	16.054	0.09668325463280267
X	0.01578743498032627	30201	30.201	0.08055578804551222
X	0.014538385693602435	29708	29.708	0.07880387806164936
X	0.014527714323557029	51883	51.883	0.06542203821188965
X	0.014741461391434344	37971	37.971	0.07295070637895608
X	0.014749832163567263	13957	13.957	0.10185874791226145
X	0.01506456712880074	20040	20.04	0.09092562883129353
X	0.014760821180761842	8688	8.688	0.11932466824810482
X	0.01453934362740064	72364	72.364	0.05856984553240925
X	0.014481613887983304	6222	6.222	0.1325243407324569
X	0.015802698777521607	349769	349.769	0.03561639688611518
X	0.01452297249014416	23234	23.234	0.08550217778232952
X	0.014534167326312974	31070	31.07	0.0776276252876568
X	0.014531571165207096	64630	64.63	0.060807825382646244
X	0.014473032834675943	25661	25.661	0.08262192770700487
X	0.014752247736083935	117223	117.223	0.050112775293745526
X	0.014550106109549597	97637	97.637	0.05301725578736728
X	0.014331417675005316	23670	23.67	0.08459868876978324
X	0.015132745083223448	167804	167.804	0.04484408176428974
X	0.014485108650896129	10550	10.55	0.11114495794020543
X	0.014499603351039983	53249	53.249	0.06481589991260303
X	0.014716763424098604	2177	2.177	0.18908248143797476
X	0.014545650571152622	59640	59.64	0.06247867899067865
X	0.015690799922792313	68001	68.001	0.06133522015163798
X	0.014485815488587968	9777	9.777	0.11400199322102404
X	0.014801209661026613	81661	81.661	0.05659275532231723
X	0.015249440516027208	943119	943.119	0.025286912474661718
X	0.01442871038589927	9232	9.232	0.11604966919927547
X	0.01563370538283237	987213	987.213	0.025112137804149934
X	0.014482973857909651	16013	16.013	0.09670783949808492
X	0.014464255273044721	8158	8.158	0.12103309127343898
X	0.014436947380123587	9309	9.309	0.11575083015344749
X	0.015015035908978137	217634	217.634	0.04101410229807051
X	0.015264069897112029	90591	90.591	0.05523255039190301
X	0.014184213048028999	1235	1.235	0.22562095304493845
X	0.014407396249533962	3889	3.889	0.15473284800958306
X	0.014435654742572884	3785	3.785	0.15623926991535947
X	0.014433567397555584	2863	2.863	0.1714684066170309
X	0.014117353617996889	1739	1.739	0.20097925734058253
X	0.013920145173696441	556	0.556	0.2925429642561147
X	0.014231741560339198	2859	2.859	0.1707449880155561
X	0.014466564647242426	819	0.819	0.26043169455905035
X	0.014390509561702593	939	0.939	0.24839157430334605
X	0.01444329553175801	2470	2.47	0.18015920143159397
X	0.01446467795070547	4016	4.016	0.1532868930058845
X	0.014333679170232805	3309	3.309	0.16301227335659155
X	0.014451915412499178	11914	11.914	0.10664879145892273
X	0.014497012856858735	19682	19.682	0.09031012267210367
X	0.01484894253225218	1524	1.524	0.21358472331935294
X	0.014468611926849643	5464	5.464	0.13834773686085935
X	0.014336865724536255	2137	2.137	0.18860356542804174
X	0.014522171193772184	4966	4.966	0.14300183563452631
X	0.014465349127733574	9011	9.011	0.11708968656221512
X	0.01435824131406774	1644	1.644	0.20593641348996644
X	0.01438478796619768	2995	2.995	0.168720898428321
X	0.014453156650991814	2913	2.913	0.17055877772089711
X	0.014311646915601307	3119	3.119	0.16617208896872399
X	0.015720301177707296	9117	9.117	0.11991392979057687
X	0.014506276466959657	1657	1.657	0.20609991559455734
X	0.014438634335431742	7004	7.004	0.12727018052890543
X	0.014461538396544016	8344	8.344	0.12011946794143132
X	0.01445484860242244	2709	2.709	0.17474369473859447
X	0.014441444546761729	1803	1.803	0.20008059492946847
X	0.014469552895690484	2340	2.34	0.18354663129027443
X	0.01437021505081952	3378	3.378	0.16203215474334262
X	0.014471520878319996	3309	3.309	0.16353315019224046
X	0.014419892614624771	4262	4.262	0.15012379239644338
X	0.01445019139172968	4632	4.632	0.1461173148984895
X	0.014513863964177807	3628	3.628	0.15874690125530994
X	0.014286942281108009	1832	1.832	0.19830695239105028
X	0.014350160626262614	2608	2.608	0.17654268406008541
X	0.015199215451451227	4145	4.145	0.15420515023613668
X	0.014497636155308951	12918	12.918	0.10392035906610841
X	0.014429673475055211	832	0.832	0.25884774891299156
X	0.014343690701871933	769	0.769	0.2652028523017715
X	0.014401380174464743	2181	2.181	0.18760719200062292
X	0.014164596336868005	7025	7.025	0.12633372198399487
X	0.014398734270796444	2419	2.419	0.18122974644354722
X	0.014082053914335223	1481	1.481	0.21185415273498015
X	0.014486326368576096	3262	3.262	0.16437083810066933
X	0.014318640229743189	1979	1.979	0.1934128514241811
X	0.014426360718824362	1832	1.832	0.19894992172202677
X	0.0144350313909945	1040	1.04	0.2403227017599803
X	0.014597177050419649	4178	4.178	0.15174002338861747
X	0.014463022550291636	1094	1.094	0.23645431682479515
X	0.014362658971818097	2418	2.418	0.18110322498212167
X	0.014408062028940965	3808	3.808	0.1558246678847338
X	0.01447296895437707	3338	3.338	0.16306362750581815
X	0.014326923414345865	1169	1.169	0.23055761771809888
X	0.014365950745311832	1170	1.17	0.23070101421345557
X	0.014480939823499392	7336	7.336	0.1254428200480441
X	0.014308025622256165	3602	3.602	0.1583720968780342
X	0.015106336899817894	3238	3.238	0.16709396857484915
X	0.014449517754591672	4146	4.146	0.151614714249141
X	0.01440424088297657	10113	10.113	0.11251319466484078
X	0.014415179792685456	7387	7.387	0.12496373239296714
X	0.014454694140986362	2946	2.946	0.16992556449620677
X	0.01444708248749808	13543	13.543	0.10217745737691339
X	0.01447403533353939	6014	6.014	0.13401149997855347
X	0.014451301977459875	3514	3.514	0.1602144708275995
X	0.01443385574020818	1573	1.573	0.20935558250730188
X	0.014344875988855883	4790	4.79	0.1441408553554732
X	0.01442637652022542	1903	1.903	0.19644432696544398
X	0.014467788997726088	4532	4.532	0.14724393721014484
X	0.014427750396202284	737	0.737	0.2695113404970534
X	0.014379266088334971	2651	2.651	0.17570157891149324
X	0.014361364901456522	1213	1.213	0.22791783214046044
X	0.014166048145313763	1659	1.659	0.20439367035774358
X	0.01438513322980596	2428	2.428	0.1809485339138065
X	0.01431334329795628	2090	2.09	0.18990290159876036
X	0.014239981605634026	2381	2.381	0.1815167377088311
X	0.015003173215259971	5140	5.14	0.14291352613001512
X	0.014120410409187614	2562	2.562	0.17664013911422466
X	0.014347112476087543	2022	2.022	0.19215917135344301
X	0.013930066030742937	1016	1.016	0.23934249962695137
X	0.015154676871703866	8950	8.95	0.1191903829546101
X	0.014035643987951324	990	0.99	0.24202805818178297
X	0.014458615451699856	7697	7.697	0.12338671641364046
X	0.01427948158751044	1768	1.768	0.20063655398890323
X	0.014377647507892624	1923	1.923	0.19554025741636025
X	0.014396778585732935	2452	2.452	0.1804048822454512
X	0.014444428556799657	2175	2.175	0.18796646242178627
X	0.014446636158729214	2951	2.951	0.16979797577229844
X	0.014283505827023863	1286	1.286	0.22311629140938932
X	0.014409711450636216	2092	2.092	0.19026746620213142
X	0.015064840534112252	8739	8.739	0.1199041643014163
X	0.014490785062669022	2649	2.649	0.1761989511227762
X	0.014476556266445331	3449	3.449	0.16130853839336978
X	0.014438997122486847	1659	1.659	0.2056980689304263
X	0.014484306454649603	3857	3.857	0.15543519248926657
X	0.014215782719981938	1937	1.937	0.19433322237160405
X	0.01426018774075667	2258	2.258	0.18484190476636264
X	0.014394094619896342	1583	1.583	0.2087218006497554
X	0.014376750532423934	1981	1.981	0.1936089458920989
X	0.014322813741461063	1470	1.47	0.21358479709624897
X	0.014734299861042556	1475	1.475	0.2153670013594012
X	0.014368618040429645	1315	1.315	0.22190295337676638
X	0.014358265725423876	3124	3.124	0.16626352623435153
X	0.013942485650269584	1218	1.218	0.2253707963581745
X	0.01445625008332077	6292	6.292	0.13195392498508338
X	0.014512483924418361	14507	14.507	0.10001259905230551
X	0.014364568968419214	4319	4.319	0.1492690601170501
X	0.014336799563826028	1432	1.432	0.2155276855273078
X	0.015131534452979777	1410	1.41	0.22057415261259217
X	0.015117401629353303	1507	1.507	0.21566912002515107
X	0.01442282043221821	1399	1.399	0.21764272790614453
X	0.014298783860261628	2211	2.211	0.186310266169151
X	0.014301717395252876	2579	2.579	0.1770023164122951
X	0.014450598402607788	5152	5.152	0.14102734366366287
X	0.01443693384640837	3419	3.419	0.1616312339664397
X	0.014346168996738783	13045	13.045	0.10322002335898314
X	0.014133558324990848	3609	3.609	0.15762370969294295
X	0.015148897153536867	20413	20.413	0.09053671385930187
X	0.014490336807339802	2445	2.445	0.18096720223787677
X	0.014503799188736406	4032	4.032	0.15322174998937332
X	0.014755426881906874	3398	3.398	0.16314588893892387
X	0.014327221354622654	481	0.481	0.30998377640585184
X	0.015645763773449863	8131	8.131	0.12438010981320334
X	0.01407733880530746	1333	1.333	0.21939672371004457
X	0.01444766296360949	5696	5.696	0.1363774420549424
X	0.01422747205032292	957	0.957	0.24588871712404553
X	0.014340399740737605	3588	3.588	0.15869732549781415
X	0.014395672758155578	2030	2.03	0.19212268284880576
X	0.014495013323534927	2423	2.423	0.18153278407298554
X	0.014481108958167876	2676	2.676	0.17556525463461764
X	0.014456302832785354	3583	3.583	0.15919770759322932
X	0.0142462465789212	570	0.57	0.2923760988889869
X	0.014297650313069269	818	0.818	0.2595197748713433
X	0.014176102084402195	1634	1.634	0.20547938628554918
X	0.01448084534017669	4537	4.537	0.14723409073198002
X	0.01442750747824702	1448	1.448	0.2151828137435751
X	0.014532056168098759	5367	5.367	0.1393793679699065
X	0.014514997517278938	1911	1.911	0.19657068886297563
X	0.014396466386659822	3396	3.396	0.16184380315469338
X	0.014470355712604589	3474	3.474	0.16089768580568584
X	0.014427229351168223	6043	6.043	0.13365240576728296
X	0.014275292307627094	2039	2.039	0.1913033585724576
X	0.014461263102326689	6576	6.576	0.13004135148213386
X	0.014410354350511249	2456	2.456	0.18036354635298832
X	0.014177289323321901	5201	5.201	0.13969111467566295
X	0.014382809119061889	1235	1.235	0.22666906493536687
X	0.014376485060649802	3370	3.37	0.162183849926712
X	0.015108771307584628	6211	6.211	0.134489756140818
X	0.014116246941168002	1756	1.756	0.20032335194141498
X	0.0145313057005085	6766	6.766	0.12902015182338633
X	0.01531695397836352	4728	4.728	0.1479670378864532
X	0.014582892732301062	11056	11.056	0.10966849503302523
X	0.014364625152839684	1528	1.528	0.21105240937098796
X	0.013761382570838072	645	0.645	0.2773537983930962
X	0.01444653761942504	2048	2.048	0.19178351474239907
X	0.014470221713456861	1879	1.879	0.1974768245584183
X	0.014373079868347358	5776	5.776	0.13551089639057562
X	0.014381931212303101	1867	1.867	0.19749569337169853
X	0.014450294570547535	1582	1.582	0.20903711699855224
X	0.014375893510980077	1630	1.63	0.2066089263545275
X	0.014483763356929007	4480	4.48	0.14786582208636173
X	0.014455549670226872	1089	1.089	0.23677485365150608
X	0.015796051757933447	7882	7.882	0.12607743727899248
X	0.014410537567755708	2062	2.062	0.19118941323177654
X	0.015066719242314522	10868	10.868	0.11150380524021511
X	0.014424502862508938	3105	3.105	0.1668577628869958
X	0.014485608811014022	3141	3.141	0.16645222523659706
X	0.014263697833945868	919	0.919	0.24944350841287213
X	0.014376619149363834	3887	3.887	0.1546491043480897
X	0.014202339041657314	3341	3.341	0.16199233548493613
X	0.014376319594220976	2873	2.873	0.17104249814102565
X	0.014854950936213734	5013	5.013	0.1436341931195041
X	0.014318287405821847	5722	5.722	0.13576288992552407
X	0.01423465797798657	2591	2.591	0.1764519821107788
X	0.014356327784279624	1774	1.774	0.20076896649682227
X	0.014398571331374014	1954	1.954	0.19459497183765645
X	0.014308046742750213	657	0.657	0.27925759504252756
X	0.014134720683994776	933	0.933	0.24743921490087345
X	0.014328044417875729	565	0.565	0.2937961884015448
X	0.014273949053449607	1390	1.39	0.21735840942508786
X	0.014040606334336634	9872	9.872	0.11245883289919584
X	0.014389554632778676	1553	1.553	0.21003514240066434
X	0.014064706272854508	3347	3.347	0.16137081217983626
X	0.01430878174947768	1370	1.37	0.21858852730089143
X	0.01443310972417916	3863	3.863	0.15517142020836436
X	0.014319880117905595	2803	2.803	0.17222866218464347
X	0.014161129389227277	2935	2.935	0.1689779077432226
X	0.014350055565811825	2175	2.175	0.18755620664608888
X	0.014659096235078106	1155	1.155	0.23326078736287081
X	0.01419578021210025	1324	1.324	0.22050770058766628
X	0.014390633725549086	3357	3.357	0.162446188580657
X	0.014064916616186892	875	0.875	0.25237308483033644
X	0.01435382387109292	1146	1.146	0.2322349959851398
X	0.01448022339939062	2344	2.344	0.1834872471160334
X	0.014318906282313205	1991	1.991	0.1930246893573819
X	0.014681404706947532	4833	4.833	0.14482724583056117
X	0.014288983711131503	2081	2.081	0.19006832770764057
X	0.014318235997171266	2108	2.108	0.1893824089925959
X	0.015345559521493253	9065	9.065	0.11918043691454784
X	0.01470735657375512	5465	5.465	0.1390960568187009
X	0.014140070946498344	924	0.924	0.2482713118877857
X	0.01451778081030983	22054	22.054	0.08699032531507692
X	0.014536934400030871	76073	76.073	0.05759888756427847
X	0.0145535411917623	2373	2.373	0.18304462491904455
X	0.014521179470175977	7532	7.532	0.12446020936563224
X	0.014759092708406672	615	0.615	0.28844400363816025
X	0.014752214916927246	3981	3.981	0.15474680262071047
X	0.014495189566321785	7820	7.82	0.12283978531789513
X	0.014526868144182768	186560	186.56	0.04270228884318009
X	0.014486331948558952	1976	1.976	0.1942631817041184
X	0.01492272080083831	113826	113.826	0.05080060054537119
X	0.01445658191927542	25418	25.418	0.08285297019486086
X	0.014414531674761635	14430	14.43	0.09996425535769532
X	0.014483964590488537	20453	20.453	0.08913401687383331
X	0.014391365711577137	1438	1.438	0.21550021502739236
X	0.01443665087830092	5257	5.257	0.14003700881469414
X	0.01447933847673046	208802	208.802	0.041083870861580954
X	0.014735661780841354	270217	270.217	0.03792151320568247
X	0.014307294266244507	4471	4.471	0.14736158358597493
X	0.014515213072422177	41820	41.82	0.07027698624363408
X	0.01467869242683019	24696	24.696	0.08407888002082166
X	0.015123925311135171	25743	25.743	0.08375327323558979
X	0.015187697274110934	78273	78.273	0.05789307940548865
X	0.014943293109556593	35966	35.966	0.07461933798881978
X	0.015449458525581182	27423	27.423	0.08259100796199269
X	0.01453682701495002	21932	21.932	0.08718942128811422
X	0.014176092317952466	1307	1.307	0.221357197821394
X	0.014725215524118679	208968	208.968	0.04130417555725376
X	0.014323832615245924	5525	5.525	0.1373754047458879
X	0.014440888355768672	2632	2.632	0.1763745864620907
X	0.01447708302463171	15984	15.984	0.0967531688858534
X	0.014745294040601415	49120	49.12	0.06695736583126531
X	0.01585404798880073	294846	294.846	0.03774403353134436
X	0.014705618119446066	59985	59.985	0.06258643393107621
X	0.014119289096412801	1614	1.614	0.20604864682464957
X	0.014906193261275957	23422	23.422	0.08601633293051496
X	0.014895570898937052	10821	10.821	0.11124058189554921
X	0.014531782572010458	27254	27.254	0.08108917461612408
X	0.014548789669905068	45241	45.241	0.06851172301487783
X	0.01513127805558561	7543	7.543	0.12611802703254685
X	0.014470036400495795	29995	29.995	0.07842844164822825
X	0.01363965242568595	463	0.463	0.30884513941225894
X	0.01448349832547421	4491	4.491	0.14774409749756706
X	0.014504678673312467	81465	81.465	0.056257317083269476
X	0.014491344559750008	39035	39.035	0.07187063792650672
X	0.014476795392083873	56640	56.64	0.06346238613824816
X	0.014466121708347861	4056	4.056	0.15278640870207136
X	0.014744066786223775	231573	231.573	0.03993096347229672
X	0.014504939251054543	10120	10.12	0.1127487668368021
X	0.014482355212026753	25699	25.699	0.08259891140086176
X	0.014548772757799533	94075	94.075	0.05367648205354185
X	0.014451928140210972	2145	2.145	0.18887138526746713
X	0.01448517929903585	64723	64.723	0.060713939290866666
X	0.015408910461106082	200822	200.822	0.0424932760412673
X	0.0151662746406367	313396	313.396	0.036441408594471286
X	0.015532922642162791	23588	23.588	0.08700028729275848
X	0.014409812347159465	14822	14.822	0.09906429931306338
X	0.014876649475352683	7610	7.61	0.1250374643034691
X	0.014415889736964827	4638	4.638	0.14593862020699186
X	0.014537944467360548	87769	87.769	0.05491874680157302
X	0.014866221884592898	6396	6.396	0.1324638535671719
X	0.014494902365709526	18518	18.518	0.0921595626012178
X	0.01514800017209233	33788	33.788	0.07653573759350073
X	0.014542154305790814	22207	22.207	0.08683862775522448
X	0.014455203089281856	17809	17.809	0.09328136400589991
X	0.015418217086385539	4060	4.06	0.1560160844604087
X	0.015032020940377332	192118	192.118	0.04277103618464992
X	0.014548136441287678	10798	10.798	0.11044719248312147
X	0.015025856941335222	18593	18.593	0.09314579636525057
X	0.015401366776382024	17499	17.499	0.09583305746186849
X	0.01449207811199409	75113	75.113	0.057783678575954754
X	0.0149582366290595	10972	10.972	0.11088297427665413
X	0.014538085376976454	137723	137.723	0.04726071055705501
X	0.01443717025749598	5508	5.508	0.13787834089375703
X	0.014272212777103742	36127	36.127	0.07337584870256249
X	0.01451789963249505	7889	7.889	0.12254453396105258
X	0.015349234824065559	202058	202.058	0.04235163945620816
X	0.014483690906957558	32235	32.235	0.07659207895662574
X	0.014465513191407016	5980	5.98	0.1342386431891027
X	0.015092000288249386	112311	112.311	0.051220227553717065
X	0.014537607381023178	135230	135.23	0.047548843959546255
X	0.014480826771090097	3624	3.624	0.1586847004686742
X	0.014536611987406273	107617	107.617	0.05130906560205043
X	0.015824241445517567	165849	165.849	0.04569514023680692
X	0.015742176441207522	335053	335.053	0.03608420058287053
X	0.014543522305618607	30213	30.213	0.07837157659973457
X	0.014447729372289213	2478	2.478	0.17998352999947115
X	0.014537338279128337	8675	8.675	0.11877867354195325
X	0.014655416852629203	6683	6.683	0.12991986827634222
X	0.014463420457018207	14746	14.746	0.09935710464965115
X	0.014509756131076584	18662	18.662	0.09195329780913371
X	0.014334369600420487	1730	1.73	0.2023535517866163
X	0.014522182932980054	57176	57.176	0.0633294978857635
X	0.014533734229924582	99513	99.513	0.05266221620407332
X	0.014454506409217685	4081	4.081	0.1524329626017934
X	0.014369160777516354	8277	8.277	0.12018570526915283
X	0.01436624152291972	23610	23.61	0.0847388161029949
X	0.015256808507439587	9862	9.862	0.11565548452498525
X	0.014491970804031352	8834	8.834	0.11793879471742723
X	0.014492746356868178	12785	12.785	0.10426774625699091
X	0.014469802646697012	3315	3.315	0.16342795952224307
X	0.0142081564587657	15717	15.717	0.09669173636145685
X	0.014547148148182611	35036	35.036	0.07460268323318507
X	0.014636515766927902	22246	22.246	0.08697516420391631
X	0.015146367976731457	46463	46.463	0.06882302414635795
X	0.014368892960208862	9238	9.238	0.11586398209221146
X	0.014845308310828517	384526	384.526	0.033797753729084085
X	0.014533772428216324	8829	8.829	0.11807436345464528
X	0.014480918144887844	67073	67.073	0.05999054245631345
X	0.014411767628335792	1656	1.656	0.20569274449211988
X	0.014466144330515716	3527	3.527	0.16007214994851082
X	0.014373348134965814	3449	3.449	0.1609242832375594
X	0.01451569490534204	11218	11.218	0.10897012906497527
X	0.014430382767603996	2571	2.571	0.17771547967422288
X	0.014479157586356645	94387	94.387	0.05353161806634105
X	0.014290237484953488	3101	3.101	0.16640992569267873
X	0.01430049401104125	3277	3.277	0.16341485503509556
X	0.01519549431321901	175578	175.578	0.04423318837337595
X	0.01537556199864128	52155	52.155	0.06655464169371599
X	0.014808200498693448	38779	38.779	0.07254963415664618
X	0.015423392974009827	115349	115.349	0.05113544387130282
X	0.015489031078512336	273878	273.878	0.038384483500283825
X	0.01493805551629837	7131	7.131	0.1279522353690017
X	0.014379110268123213	7990	7.99	0.12163589217574333
X	0.014824542449662059	14566	14.566	0.10058819092886084
X	0.013834086254974187	1175	1.175	0.22749448397697114
X	0.014453864640752257	2921	2.921	0.1704057095953195
X	0.014506348472374928	18497	18.497	0.09221868741583064
X	0.01446471259051441	3603	3.603	0.15893340293956368
X	0.01450279983160228	3003	3.003	0.16903066765281202
X	0.014411615535668273	3272	3.272	0.16392044494945043
X	0.014521465284560598	5176	5.176	0.14103883904419912
X	0.014557849366996525	225938	225.938	0.0400899930002455
X	0.01427989163652405	17560	17.56	0.0933397858346098
X	0.01582679798128438	83373	83.373	0.057471942567688804
X	0.01528232097069773	3503	3.503	0.1633991249769678
X	0.0143844109319885	4468	4.468	0.14765890347461522
X	0.014613378471694242	38615	38.615	0.07233217610909069
X	0.01577604007166086	240011	240.011	0.04035731687270376
X	0.015092209474274396	336329	336.329	0.03553553318768279
X	0.015036653007999298	10846	10.846	0.1115048760476746
X	0.014545751430198189	38144	38.144	0.07251646792483593
X	0.014457465378742413	2693	2.693	0.17509964645098
X	0.014459197867856335	3576	3.576	0.15931214935093074
X	0.014506365906459507	316349	316.349	0.035792941226277415
X	0.014594492049542131	18813	18.813	0.09188479334925796
X	0.014444936517560368	7186	7.186	0.12620487806463815
X	0.014465492329231189	39918	39.918	0.07129430096828132
X	0.014609918548722635	584381	584.381	0.029240439935294755
X	0.014471051486138113	20568	20.568	0.08894113658297992
X	0.014965713894310145	7410	7.41	0.12640368747669659
X	0.014395181722224263	10661	10.661	0.11052819288589649
X	0.014483638840779892	22036	22.036	0.08694573991661173
X	0.014501114261507835	3984	3.984	0.1538251503618341
X	0.01567838093275029	124238	124.238	0.050159005853791856
X	0.015134700669506	17116	17.116	0.09598216005508815
X	0.014287369531353709	935	0.935	0.24814958633204717
X	0.015156511863932031	52055	52.055	0.06627945637303378
X	0.015584933258114284	20138	20.138	0.09181131407890804
X	0.014036385275471734	1225	1.225	0.225444544612155
X	0.014535417311344266	90876	90.876	0.054282449124234015
X	0.01455054569693535	72368	72.368	0.05858380428701426
X	0.01519338502514104	240394	240.394	0.0398330572005905
X	0.015133215169042483	174501	174.501	0.044263365214932265
X	0.014778184947465052	140594	140.594	0.04719378069772943
X	0.014445828459165201	21718	21.718	0.08729195260779855
X	0.014489868051922093	14026	14.026	0.10109046604266231
X	0.014545522883651301	78690	78.69	0.05696436548260062
X	0.014772893811968078	102970	102.97	0.05235019307901133
X	0.014467633459669501	5547	5.547	0.13765112879611338
X	0.014562565704404508	43425	43.425	0.06947566145511477
X	0.014575196025103838	115328	115.328	0.05018337679533811
X	0.014876240602770957	122062	122.062	0.049579739137130915
X	0.015867253359373713	87910	87.91	0.056513797627717306
X	0.014475527006766145	51872	51.872	0.06534822518483145
X	0.014498546029669022	9303	9.303	0.1159401372740436
X	0.014395228159194364	3917	3.917	0.15431980305275558
X	0.014355028490487679	7081	7.081	0.12656194918733674
X	0.014537861798296685	97621	97.621	0.053005275315014984
X	0.015330999233607843	91860	91.86	0.05505727005085875
X	0.014823022831764452	267648	267.648	0.0381174867812346
X	0.015607008367009319	132548	132.548	0.0490134750120926
X	0.014484942452089707	53354	53.354	0.06475151436913495
X	0.014469852077847197	13242	13.242	0.10299990780425046
X	0.015284961659209744	151673	151.673	0.0465356044058557
X	0.014266093994233663	2619	2.619	0.17595023892025108
X	0.014421288532982501	7700	7.7	0.12326443241547971
X	0.01546555082219838	143542	143.542	0.047584129978615296
X	0.015087942331519385	15666	15.666	0.09875459169923213
X	0.014299108008789847	3786	3.786	0.15573137292007241
X	0.014619958142471334	17058	17.058	0.0949888594507263
X	0.015495541846497407	370827	370.827	0.03470125165479235
X	0.014476588552900497	16904	16.904	0.09496398171737643
X	0.01463471334012549	110388	110.388	0.05099028644817589
X	0.014350480856050441	1971	1.971	0.1938176289919028
X	0.014442432065044256	4442	4.442	0.1481450891692957
X	0.014496808392034604	3903	3.903	0.15486666610220887
X	0.014443818640325493	4813	4.813	0.14424099902116128
X	0.015104022189457047	23065	23.065	0.08683864775071505
X	0.014441454465017323	54733	54.733	0.06413876018766498
X	0.014876515200379433	2758	2.758	0.17537552447607616
X	0.014551073802342607	93022	93.022	0.053881100434142404
X	0.014426338191344267	3199	3.199	0.1652141687501629
X	0.01513070148864801	39590	39.59	0.07257012210612723
X	0.014427982066554115	6880	6.88	0.12799874707127876
X	0.014496612240657463	79974	79.974	0.05659428376489907
X	0.014432054604011639	8184	8.184	0.12081499702928801
X	0.014510781331353181	17506	17.506	0.09393655403363808
X	0.015084732645468803	22672	22.672	0.08730033860037992
X	0.01446399008635588	26880	26.88	0.0813366504639189
X	0.014742599232279524	68691	68.691	0.05987213433923131
X	0.015647569829710387	286837	286.837	0.03792601759923918
X	0.015134408858590345	5929	5.929	0.1366662686917744
X	0.014636043081416467	4201	4.201	0.151596904199358
X	0.014542570718716692	52012	52.012	0.0653901816045458
X	0.014424212276524644	12848	12.848	0.1039326958630656
X	0.015857595980190126	478449	478.449	0.03212185119065785
X	0.014491760722867664	1555	1.555	0.21044095004482363
X	0.014537457149343012	95480	95.48	0.05339804361720812
X	0.015273019590626262	226824	226.824	0.040682906070734416
X	0.014455935846610103	3706	3.706	0.15741529674896448
X	0.014542266326576934	116566	116.566	0.04996739786436206
X	0.014617191404432529	17471	17.471	0.0942284520425687
X	0.015602886541797138	654802	654.802	0.028776081251966037
X	0.01449049170937257	9303	9.303	0.11591866404096764
X	0.01432985888039131	12764	12.764	0.10393257946009896
X	0.014357580803476273	7632	7.632	0.12344712192634107
X	0.014442523368869748	3173	3.173	0.16572613509393597
X	0.01446510709581939	12668	12.668	0.1045212350686133
X	0.014538343904316164	21041	21.041	0.08840622652901117
X	0.01443852725057234	8628	8.628	0.11872374482367255
X	0.014515423186255347	79688	79.688	0.05668640625633179
X	0.01449887175885616	14978	14.978	0.09892213039184324
X	0.014456414998851863	2363	2.363	0.1828937909046558
X	0.014443064138632805	13867	13.867	0.10136599197464445
X	0.014726228309855953	236137	236.137	0.039656023442339344
X	0.015308678056070741	82204	82.204	0.05710597879932375
X	0.015556261601830365	297948	297.948	0.03737560698899517
X	0.014436745917313257	9647	9.647	0.11438234909943892
X	0.014512607719761931	6292	6.292	0.13212517659592002
X	0.014500365180759941	4396	4.396	0.14885853892726567
X	0.014375680606685192	6091	6.091	0.1331414467828516
X	0.015189778900153836	146380	146.38	0.04699192505260624
X	0.014494128415809042	9131	9.131	0.11665174881064086
X	0.014967686466128364	14684	14.684	0.10063987773189312
X	0.014461553824448463	30911	30.911	0.07763078320779068
X	0.014439584208872291	27733	27.733	0.0804487459670733
X	0.014471316266686192	3859	3.859	0.15536186244109462
X	0.014465870264360494	7086	7.086	0.1268570104937359
X	0.015121211539740031	221163	221.163	0.04089071509667809
X	0.014537644792118757	23721	23.721	0.08494158355832093
X	0.014407543123563167	6290	6.29	0.13181953062652185
X	0.014542874792884145	12524	12.524	0.10510799104163544
X	0.014876633180068379	132508	132.508	0.048241505413281906
X	0.01453486315274267	86482	86.482	0.055185935309835774
X	0.015299853724825061	284318	284.318	0.037753826391830496
X	0.014793637155795474	85547	85.547	0.05571302327387772
X	0.014542169151482822	4560	4.56	0.1471933252025191
X	0.014534450874566372	29254	29.254	0.07920229944190592
X	0.014657323731926391	25239	25.239	0.08343101212260046
X	0.014527100463830413	6732	6.732	0.12922452492320172
X	0.014442547966872625	9958	9.958	0.11319412817908289
X	0.014522843978247432	3446	3.446	0.16152712611446302
X	0.014513518648964403	25108	25.108	0.08330163618618434
X	0.01430587902381427	18155	18.155	0.09236471580538863
X	0.014264461592118107	2102	2.102	0.18932482210927454
X	0.014499413574125625	22015	22.015	0.08700494081918617
X	0.014551624960851337	7222	7.222	0.12630426124777916
X	0.014502696320754432	26883	26.883	0.0814061108639679
X	0.014921650956814582	9097	9.097	0.11793414442449128
X	0.01452547414567604	21243	21.243	0.08809910359955159
X	0.014503343162408134	1727	1.727	0.2032631235506754
X	0.014517207838451047	12469	12.469	0.10520034317406707
X	0.01427897752866923	3988	3.988	0.1529844608036359
X	0.014421180349498923	9293	9.293	0.11577504460515044
X	0.014547351612822085	18951	18.951	0.09156242116649248
X	0.014530164860173903	116153	116.153	0.05001266956886294
X	0.014659820624542219	5272	5.272	0.14062129235661816
X	0.014293851224331954	1252	1.252	0.22517230637973354
X	0.014393322227840174	4935	4.935	0.14287556595662543
X	0.014510031234351796	90076	90.076	0.054410963746406096
X	0.015863899511517867	380639	380.639	0.03467093513533767
X	0.014401101694693512	33694	33.694	0.07532648726117513
X	0.014384249871646725	6480	6.48	0.13044799377833335
X	0.01448110401652748	113493	113.493	0.05034359198981459
X	0.015176313892899753	110238	110.238	0.051635096345047544
X	0.014249825888106915	1675	1.675	0.2041416319032301
X	0.014491935055900176	9045	9.045	0.11701438901318369
X	0.01454630173708126	22219	22.219	0.08683124492571276
X	0.01517271989091639	96793	96.793	0.05391875180032497
X	0.01564016364419078	141577	141.577	0.047982498948040596
X	0.014531730762583024	156045	156.045	0.04532688023826334
X	0.014512009480998093	9252	9.252	0.1161887224478026
X	0.01442482472686074	3352	3.352	0.16265553443692557
X	0.014424409857615847	1399	1.399	0.2176507225091987
X	0.01445386060772823	32584	32.584	0.07626521314120346
X	0.014541259130822144	9703	9.703	0.11443670049248868
X	0.014467629659399768	10782	10.782	0.11029758882045497
X	0.014494436972402432	105901	105.901	0.05153478038922279
X	0.015696387961057358	412249	412.249	0.03364186299706443
X	0.014401387300175374	8118	8.118	0.12105565847839193
X	0.014374009153487225	2454	2.454	0.18026072753956715
X	0.015868525349823442	10627	10.627	0.11429887692329498
X	0.015014875256570673	30075	30.075	0.07933022708620176
X	0.014425873384145358	22884	22.884	0.08574393141691751
X	0.014346388104319647	923	0.923	0.2495630539003978
X	0.014428598184916902	3450	3.45	0.16111464350397575
X	0.014506762703095089	18071	18.071	0.09293859592245653
X	0.014475767494141175	2208	2.208	0.18716049133660603
X	0.015427053889188618	81620	81.62	0.05738901913337753
X	0.014455833673893177	3710	3.71	0.15735833234146684
X	0.014496722079675355	16561	16.561	0.09565942189648441
X	0.014516758971804856	12912	12.912	0.10398213163760409
X	0.014447972056828495	4774	4.774	0.14464657658016553
X	0.014467566879412225	3589	3.589	0.15915025791892334
X	0.01454719124418508	129786	129.786	0.04821517716174027
X	0.014726094960301528	348224	348.224	0.03483985179737305
X	0.014690158432267439	106056	106.056	0.051740475085134366
X	0.01509401406268256	53962	53.962	0.06539917014392034
X	0.014739007596256957	15363	15.363	0.09862735902502222
X	0.014451068646589784	11344	11.344	0.10840381891844127
X	0.015162364896376488	16935	16.935	0.09638155179183257
X	0.01526749084690655	109721	109.721	0.05181943065723618
X	0.014969304945598708	30899	30.899	0.07853906687515741
X	0.014556968950075163	15116	15.116	0.09875172503682561
X	0.01450079375457085	15905	15.905	0.09696597471134422
X	0.015791147378087293	1001163	1001.163	0.025078581330844403
X	0.014483888077512505	9859	9.859	0.11368000862298586
X	0.01494107807357984	49448	49.448	0.06710338082894429
X	0.015208499011599463	162563	162.563	0.04539641416158109
X	0.014899158796911878	40023	40.023	0.07193673181777592
X	0.014543951321548253	22954	22.954	0.08588975125784305
X	0.01511182304774052	387845	387.845	0.033901549946556246
X	0.015184250660897694	70377	70.377	0.059977386582777896
X	0.015269664720723239	45474	45.474	0.06950597096563747
X	0.014539595772292307	4291	4.291	0.1501981577884406
X	0.014542481237255938	26554	26.554	0.08181561269980818
X	0.014504012298867969	4532	4.532	0.1473667206874803
X	0.015285821061476341	136074	136.074	0.048250802402835954
X	0.015614716340953636	1001356	1001.356	0.02498322685995115
X	0.014870573282495535	108764	108.764	0.051516638050996245
X	0.015321578942761468	145066	145.066	0.04726931357249146
X	0.014783253765617137	39172	39.172	0.07226556935934476
X	0.01448264257165302	41473	41.473	0.0704196918436766
X	0.015686861596742322	25704	25.704	0.0848226397821709
X	0.014390031334332344	8049	8.049	0.12136867314600082
X	0.0157395439456232	168823	168.823	0.04534403638064158
X	0.014768378474470531	305629	305.629	0.03642331063657697
X	0.014439254930057452	48962	48.962	0.06656233443047169
X	0.014485933397230559	519020	519.02	0.030333379383191302
X	0.0156823422710394	196321	196.321	0.043067340787955134
X	0.01449779186862299	29482	29.482	0.07893112744642711
X	0.014252062731245194	2836	2.836	0.17128677515772345
X	0.015645085912400592	49533	49.533	0.06810219450906503
X	0.01527374795523353	82069	82.069	0.057093783783456004
X	0.014501863441327416	4809	4.809	0.14447399365911837
X	0.015845983732435023	452820	452.82	0.03270879426322885
X	0.014553119526951526	15954	15.954	0.09698297515147566
X	0.014501920413685476	27287	27.287	0.08100091202806026
X	0.014513818524026355	4078	4.078	0.15267859532230066
X	0.014429403762426999	7122	7.122	0.12653639933779048
X	0.014341511767168049	2007	2.007	0.19261163613712276
X	0.014339690153203178	4846	4.846	0.1435661715139151
X	0.014474920259631324	23952	23.952	0.08454568859156064
X	0.01480490446177406	15717	15.717	0.09802691126963725
X	0.01591695876267574	3180237	3180.237	0.017105411842319593
X	0.015040203362392864	8962	8.962	0.11883642810685906
X	0.01454828022944969	69771	69.771	0.059298754290867964
X	0.014925888792619175	6369	6.369	0.1328280053175669
X	0.014464939720163808	13019	13.019	0.10357294703572315
X	0.014315150860492688	8923	8.923	0.11706534393367979
X	0.015159734866094916	163767	163.767	0.045236438384709664
X	0.014438822171299713	32580	32.58	0.07624187399729937
X	0.01552834108348271	680453	680.453	0.02836453334395976
X	0.014477508063122842	5389	5.389	0.13901506826285698
X	0.014626607521189255	149360	149.36	0.04609315483926171
X	0.015064901147143544	29325	29.325	0.0800895933366503
X	0.014803712419744706	111871	111.871	0.050958625642515694
X	0.015013827804946029	173851	173.851	0.044201609061188224
X	0.014469550295323713	12200	12.2	0.10585183740354379
X	0.014391496684913912	8010	8.01	0.12156945842113079
X	0.015707676182421474	88039	88.039	0.05629618254073962
X	0.015219334476906418	19761	19.761	0.0916633195879177
X	0.01445403440066396	30429	30.429	0.07802500429916964
X	0.01450408871262707	106035	106.035	0.051524494013286466
X	0.014754815588334493	9009	9.009	0.11787428361693526
X	0.01562279979777811	79482	79.482	0.058142900568560904
X	0.014497744133972446	16330	16.33	0.09611062807552265
X	0.015570962633912333	136814	136.814	0.048461290390660584
X	0.014522721747902173	300482	300.482	0.03642586315813814
X	0.014470592621425884	23362	23.362	0.08524300790337601
X	0.014957482176296263	7420	7.42	0.12632370736117463
X	0.014501940701488369	96289	96.289	0.053204677637767143
X	0.015097262491715573	45976	45.976	0.06899045470380126
X	0.014899146063854153	201260	201.26	0.04198891344623426
X	0.014414034723182114	27569	27.569	0.08056038199371385
X	0.014497244675715778	32843	32.843	0.07614023556953356
X	0.015218071802688764	40919	40.919	0.07191368427812349
X	0.01442433142809953	65372	65.372	0.060427501831011005
X	0.014507587892187456	111086	111.086	0.05073550184144088
X	0.014515073071094477	18606	18.606	0.09205670017128302
X	0.014204372672915475	5269	5.269	0.1391760863321887
X	0.014360887347670066	5633	5.633	0.13660937615262994
X	0.015005762670876251	43831	43.831	0.06995609595154972
X	0.014451891912442928	52175	52.175	0.0651859624819176
X	0.014548211137197477	54944	54.944	0.06421400742819894
X	0.014462558473320272	10374	10.374	0.11171194879413093
X	0.01516378703061298	19910	19.91	0.09132271309820177
X	0.014448570468503777	31465	31.465	0.07714936955096152
X	0.015585924440234317	34437	34.437	0.0767778156139437
X	0.015511092551992938	28123	28.123	0.08200876041238875
X	0.014532047395786223	62317	62.317	0.061551705248710495
X	0.014543286683484832	11750	11.75	0.10736800894393336
X	0.014524387976099151	28640	28.64	0.07974588385863543
X	0.014431340529867755	1660	1.66	0.20562039804888838
X	0.015718427004898856	6881	6.881	0.1317000202022995
X	0.01518701407951872	77560	77.56	0.05806906924722697
X	0.014508061141966943	17553	17.553	0.09384677252792241
X	0.014587542872329893	51767	51.767	0.06556061959534586
X	0.014425379771761265	1882	1.882	0.19716774924144967
X	0.014500734334011898	5969	5.969	0.1344299815494127
X	0.014954538868404282	23442	23.442	0.0860847299510406
X	0.014474848348811176	43525	43.525	0.06928274414705833
X	0.014525241610371829	20750	20.75	0.08879089248780346
X	0.014483415592664476	3301	3.301	0.1637099802515116
X	0.01438129757025015	6806	6.806	0.12832227265816012
X	0.01444917575624712	33034	33.034	0.07590912227345069
X	0.014474797311029058	25252	25.252	0.08306898490671508
X	0.014532895083056314	88253	88.253	0.054811819678667405
X	0.014510347122497493	3819	3.819	0.15604243348355193
X	0.014589628595377576	7891	7.891	0.12273565113883052
X	0.0144540452907489	19827	19.827	0.09000033684403316
X	0.014428734246445857	8947	8.947	0.11726910187587222
X	0.014566940954772343	193150	193.15	0.04224978946237598
X	0.014484028962062082	4537	4.537	0.14724487977354883
X	0.014581064022190771	40833	40.833	0.07094568332634055
X	0.01525572577703329	126590	126.59	0.04939441253560208
X	0.014481689652000812	22680	22.68	0.08611101902615996
X	0.015143501127376339	125933	125.933	0.049358533554484406
X	0.014620200244562963	103028	103.028	0.052159411638669356
X	0.014459945804495727	1884	1.884	0.19725525818934203
X	0.015223747266698588	32209	32.209	0.07789593143103528
X	0.01454533390441176	10978	10.978	0.1098331627382306
X	0.015738652677410717	73591	73.591	0.05980180359781476
X	0.014491148108765105	80351	80.351	0.05649853278191419
X	0.014437112963903313	18088	18.088	0.09276054005197056
X	0.014541545917073507	41048	41.048	0.070757561760269
X	0.014498152862506282	14927	14.927	0.09903302551942943
X	0.01444543724773412	23415	23.415	0.08512925707185046
X	0.014461775494479225	3794	3.794	0.1562097352103446
X	0.01492346186724957	9329	9.329	0.11695303479020834
X	0.014600587172279305	116119	116.119	0.05009822568211102
X	0.01584697137408014	366388	366.388	0.03510225450962632
X	0.014470743707363216	19863	19.863	0.08998055544638225
X	0.014553299098894716	15522	15.522	0.09787488361713109
X	0.01484351391302485	23709	23.709	0.08554759979611504
X	0.01450355025099708	77864	77.864	0.05711004966978841
X	0.014475025512030937	22554	22.554	0.08625784169949095
X	0.014436140987928099	5586	5.586	0.13723031529734905
X	0.01537947418898933	33922	33.922	0.07682218922917708
X	0.014533982542919264	119280	119.28	0.04957609719552741
X	0.014419046378189327	5736	5.736	0.1359698020707523
X	0.014506550035106626	43261	43.261	0.06947403589669952
X	0.014511261751857072	4578	4.578	0.14689594108219153
time for making epsilon is 1.9971966743469238
epsilons are
[0.2294031301126105, 0.0869179482704705, 0.1931194628932878, 0.09668325463280267, 0.08055578804551222, 0.07880387806164936, 0.06542203821188965, 0.07295070637895608, 0.10185874791226145, 0.09092562883129353, 0.11932466824810482, 0.05856984553240925, 0.1325243407324569, 0.03561639688611518, 0.08550217778232952, 0.0776276252876568, 0.060807825382646244, 0.08262192770700487, 0.050112775293745526, 0.05301725578736728, 0.08459868876978324, 0.04484408176428974, 0.11114495794020543, 0.06481589991260303, 0.18908248143797476, 0.06247867899067865, 0.06133522015163798, 0.11400199322102404, 0.05659275532231723, 0.025286912474661718, 0.11604966919927547, 0.025112137804149934, 0.09670783949808492, 0.12103309127343898, 0.11575083015344749, 0.04101410229807051, 0.05523255039190301, 0.22562095304493845, 0.15473284800958306, 0.15623926991535947, 0.1714684066170309, 0.20097925734058253, 0.2925429642561147, 0.1707449880155561, 0.26043169455905035, 0.24839157430334605, 0.18015920143159397, 0.1532868930058845, 0.16301227335659155, 0.10664879145892273, 0.09031012267210367, 0.21358472331935294, 0.13834773686085935, 0.18860356542804174, 0.14300183563452631, 0.11708968656221512, 0.20593641348996644, 0.168720898428321, 0.17055877772089711, 0.16617208896872399, 0.11991392979057687, 0.20609991559455734, 0.12727018052890543, 0.12011946794143132, 0.17474369473859447, 0.20008059492946847, 0.18354663129027443, 0.16203215474334262, 0.16353315019224046, 0.15012379239644338, 0.1461173148984895, 0.15874690125530994, 0.19830695239105028, 0.17654268406008541, 0.15420515023613668, 0.10392035906610841, 0.25884774891299156, 0.2652028523017715, 0.18760719200062292, 0.12633372198399487, 0.18122974644354722, 0.21185415273498015, 0.16437083810066933, 0.1934128514241811, 0.19894992172202677, 0.2403227017599803, 0.15174002338861747, 0.23645431682479515, 0.18110322498212167, 0.1558246678847338, 0.16306362750581815, 0.23055761771809888, 0.23070101421345557, 0.1254428200480441, 0.1583720968780342, 0.16709396857484915, 0.151614714249141, 0.11251319466484078, 0.12496373239296714, 0.16992556449620677, 0.10217745737691339, 0.13401149997855347, 0.1602144708275995, 0.20935558250730188, 0.1441408553554732, 0.19644432696544398, 0.14724393721014484, 0.2695113404970534, 0.17570157891149324, 0.22791783214046044, 0.20439367035774358, 0.1809485339138065, 0.18990290159876036, 0.1815167377088311, 0.14291352613001512, 0.17664013911422466, 0.19215917135344301, 0.23934249962695137, 0.1191903829546101, 0.24202805818178297, 0.12338671641364046, 0.20063655398890323, 0.19554025741636025, 0.1804048822454512, 0.18796646242178627, 0.16979797577229844, 0.22311629140938932, 0.19026746620213142, 0.1199041643014163, 0.1761989511227762, 0.16130853839336978, 0.2056980689304263, 0.15543519248926657, 0.19433322237160405, 0.18484190476636264, 0.2087218006497554, 0.1936089458920989, 0.21358479709624897, 0.2153670013594012, 0.22190295337676638, 0.16626352623435153, 0.2253707963581745, 0.13195392498508338, 0.10001259905230551, 0.1492690601170501, 0.2155276855273078, 0.22057415261259217, 0.21566912002515107, 0.21764272790614453, 0.186310266169151, 0.1770023164122951, 0.14102734366366287, 0.1616312339664397, 0.10322002335898314, 0.15762370969294295, 0.09053671385930187, 0.18096720223787677, 0.15322174998937332, 0.16314588893892387, 0.30998377640585184, 0.12438010981320334, 0.21939672371004457, 0.1363774420549424, 0.24588871712404553, 0.15869732549781415, 0.19212268284880576, 0.18153278407298554, 0.17556525463461764, 0.15919770759322932, 0.2923760988889869, 0.2595197748713433, 0.20547938628554918, 0.14723409073198002, 0.2151828137435751, 0.1393793679699065, 0.19657068886297563, 0.16184380315469338, 0.16089768580568584, 0.13365240576728296, 0.1913033585724576, 0.13004135148213386, 0.18036354635298832, 0.13969111467566295, 0.22666906493536687, 0.162183849926712, 0.134489756140818, 0.20032335194141498, 0.12902015182338633, 0.1479670378864532, 0.10966849503302523, 0.21105240937098796, 0.2773537983930962, 0.19178351474239907, 0.1974768245584183, 0.13551089639057562, 0.19749569337169853, 0.20903711699855224, 0.2066089263545275, 0.14786582208636173, 0.23677485365150608, 0.12607743727899248, 0.19118941323177654, 0.11150380524021511, 0.1668577628869958, 0.16645222523659706, 0.24944350841287213, 0.1546491043480897, 0.16199233548493613, 0.17104249814102565, 0.1436341931195041, 0.13576288992552407, 0.1764519821107788, 0.20076896649682227, 0.19459497183765645, 0.27925759504252756, 0.24743921490087345, 0.2937961884015448, 0.21735840942508786, 0.11245883289919584, 0.21003514240066434, 0.16137081217983626, 0.21858852730089143, 0.15517142020836436, 0.17222866218464347, 0.1689779077432226, 0.18755620664608888, 0.23326078736287081, 0.22050770058766628, 0.162446188580657, 0.25237308483033644, 0.2322349959851398, 0.1834872471160334, 0.1930246893573819, 0.14482724583056117, 0.19006832770764057, 0.1893824089925959, 0.11918043691454784, 0.1390960568187009, 0.2482713118877857, 0.08699032531507692, 0.05759888756427847, 0.18304462491904455, 0.12446020936563224, 0.28844400363816025, 0.15474680262071047, 0.12283978531789513, 0.04270228884318009, 0.1942631817041184, 0.05080060054537119, 0.08285297019486086, 0.09996425535769532, 0.08913401687383331, 0.21550021502739236, 0.14003700881469414, 0.041083870861580954, 0.03792151320568247, 0.14736158358597493, 0.07027698624363408, 0.08407888002082166, 0.08375327323558979, 0.05789307940548865, 0.07461933798881978, 0.08259100796199269, 0.08718942128811422, 0.221357197821394, 0.04130417555725376, 0.1373754047458879, 0.1763745864620907, 0.0967531688858534, 0.06695736583126531, 0.03774403353134436, 0.06258643393107621, 0.20604864682464957, 0.08601633293051496, 0.11124058189554921, 0.08108917461612408, 0.06851172301487783, 0.12611802703254685, 0.07842844164822825, 0.30884513941225894, 0.14774409749756706, 0.056257317083269476, 0.07187063792650672, 0.06346238613824816, 0.15278640870207136, 0.03993096347229672, 0.1127487668368021, 0.08259891140086176, 0.05367648205354185, 0.18887138526746713, 0.060713939290866666, 0.0424932760412673, 0.036441408594471286, 0.08700028729275848, 0.09906429931306338, 0.1250374643034691, 0.14593862020699186, 0.05491874680157302, 0.1324638535671719, 0.0921595626012178, 0.07653573759350073, 0.08683862775522448, 0.09328136400589991, 0.1560160844604087, 0.04277103618464992, 0.11044719248312147, 0.09314579636525057, 0.09583305746186849, 0.057783678575954754, 0.11088297427665413, 0.04726071055705501, 0.13787834089375703, 0.07337584870256249, 0.12254453396105258, 0.04235163945620816, 0.07659207895662574, 0.1342386431891027, 0.051220227553717065, 0.047548843959546255, 0.1586847004686742, 0.05130906560205043, 0.04569514023680692, 0.03608420058287053, 0.07837157659973457, 0.17998352999947115, 0.11877867354195325, 0.12991986827634222, 0.09935710464965115, 0.09195329780913371, 0.2023535517866163, 0.0633294978857635, 0.05266221620407332, 0.1524329626017934, 0.12018570526915283, 0.0847388161029949, 0.11565548452498525, 0.11793879471742723, 0.10426774625699091, 0.16342795952224307, 0.09669173636145685, 0.07460268323318507, 0.08697516420391631, 0.06882302414635795, 0.11586398209221146, 0.033797753729084085, 0.11807436345464528, 0.05999054245631345, 0.20569274449211988, 0.16007214994851082, 0.1609242832375594, 0.10897012906497527, 0.17771547967422288, 0.05353161806634105, 0.16640992569267873, 0.16341485503509556, 0.04423318837337595, 0.06655464169371599, 0.07254963415664618, 0.05113544387130282, 0.038384483500283825, 0.1279522353690017, 0.12163589217574333, 0.10058819092886084, 0.22749448397697114, 0.1704057095953195, 0.09221868741583064, 0.15893340293956368, 0.16903066765281202, 0.16392044494945043, 0.14103883904419912, 0.0400899930002455, 0.0933397858346098, 0.057471942567688804, 0.1633991249769678, 0.14765890347461522, 0.07233217610909069, 0.04035731687270376, 0.03553553318768279, 0.1115048760476746, 0.07251646792483593, 0.17509964645098, 0.15931214935093074, 0.035792941226277415, 0.09188479334925796, 0.12620487806463815, 0.07129430096828132, 0.029240439935294755, 0.08894113658297992, 0.12640368747669659, 0.11052819288589649, 0.08694573991661173, 0.1538251503618341, 0.050159005853791856, 0.09598216005508815, 0.24814958633204717, 0.06627945637303378, 0.09181131407890804, 0.225444544612155, 0.054282449124234015, 0.05858380428701426, 0.0398330572005905, 0.044263365214932265, 0.04719378069772943, 0.08729195260779855, 0.10109046604266231, 0.05696436548260062, 0.05235019307901133, 0.13765112879611338, 0.06947566145511477, 0.05018337679533811, 0.049579739137130915, 0.056513797627717306, 0.06534822518483145, 0.1159401372740436, 0.15431980305275558, 0.12656194918733674, 0.053005275315014984, 0.05505727005085875, 0.0381174867812346, 0.0490134750120926, 0.06475151436913495, 0.10299990780425046, 0.0465356044058557, 0.17595023892025108, 0.12326443241547971, 0.047584129978615296, 0.09875459169923213, 0.15573137292007241, 0.0949888594507263, 0.03470125165479235, 0.09496398171737643, 0.05099028644817589, 0.1938176289919028, 0.1481450891692957, 0.15486666610220887, 0.14424099902116128, 0.08683864775071505, 0.06413876018766498, 0.17537552447607616, 0.053881100434142404, 0.1652141687501629, 0.07257012210612723, 0.12799874707127876, 0.05659428376489907, 0.12081499702928801, 0.09393655403363808, 0.08730033860037992, 0.0813366504639189, 0.05987213433923131, 0.03792601759923918, 0.1366662686917744, 0.151596904199358, 0.0653901816045458, 0.1039326958630656, 0.03212185119065785, 0.21044095004482363, 0.05339804361720812, 0.040682906070734416, 0.15741529674896448, 0.04996739786436206, 0.0942284520425687, 0.028776081251966037, 0.11591866404096764, 0.10393257946009896, 0.12344712192634107, 0.16572613509393597, 0.1045212350686133, 0.08840622652901117, 0.11872374482367255, 0.05668640625633179, 0.09892213039184324, 0.1828937909046558, 0.10136599197464445, 0.039656023442339344, 0.05710597879932375, 0.03737560698899517, 0.11438234909943892, 0.13212517659592002, 0.14885853892726567, 0.1331414467828516, 0.04699192505260624, 0.11665174881064086, 0.10063987773189312, 0.07763078320779068, 0.0804487459670733, 0.15536186244109462, 0.1268570104937359, 0.04089071509667809, 0.08494158355832093, 0.13181953062652185, 0.10510799104163544, 0.048241505413281906, 0.055185935309835774, 0.037753826391830496, 0.05571302327387772, 0.1471933252025191, 0.07920229944190592, 0.08343101212260046, 0.12922452492320172, 0.11319412817908289, 0.16152712611446302, 0.08330163618618434, 0.09236471580538863, 0.18932482210927454, 0.08700494081918617, 0.12630426124777916, 0.0814061108639679, 0.11793414442449128, 0.08809910359955159, 0.2032631235506754, 0.10520034317406707, 0.1529844608036359, 0.11577504460515044, 0.09156242116649248, 0.05001266956886294, 0.14062129235661816, 0.22517230637973354, 0.14287556595662543, 0.054410963746406096, 0.03467093513533767, 0.07532648726117513, 0.13044799377833335, 0.05034359198981459, 0.051635096345047544, 0.2041416319032301, 0.11701438901318369, 0.08683124492571276, 0.05391875180032497, 0.047982498948040596, 0.04532688023826334, 0.1161887224478026, 0.16265553443692557, 0.2176507225091987, 0.07626521314120346, 0.11443670049248868, 0.11029758882045497, 0.05153478038922279, 0.03364186299706443, 0.12105565847839193, 0.18026072753956715, 0.11429887692329498, 0.07933022708620176, 0.08574393141691751, 0.2495630539003978, 0.16111464350397575, 0.09293859592245653, 0.18716049133660603, 0.05738901913337753, 0.15735833234146684, 0.09565942189648441, 0.10398213163760409, 0.14464657658016553, 0.15915025791892334, 0.04821517716174027, 0.03483985179737305, 0.051740475085134366, 0.06539917014392034, 0.09862735902502222, 0.10840381891844127, 0.09638155179183257, 0.05181943065723618, 0.07853906687515741, 0.09875172503682561, 0.09696597471134422, 0.025078581330844403, 0.11368000862298586, 0.06710338082894429, 0.04539641416158109, 0.07193673181777592, 0.08588975125784305, 0.033901549946556246, 0.059977386582777896, 0.06950597096563747, 0.1501981577884406, 0.08181561269980818, 0.1473667206874803, 0.048250802402835954, 0.02498322685995115, 0.051516638050996245, 0.04726931357249146, 0.07226556935934476, 0.0704196918436766, 0.0848226397821709, 0.12136867314600082, 0.04534403638064158, 0.03642331063657697, 0.06656233443047169, 0.030333379383191302, 0.043067340787955134, 0.07893112744642711, 0.17128677515772345, 0.06810219450906503, 0.057093783783456004, 0.14447399365911837, 0.03270879426322885, 0.09698297515147566, 0.08100091202806026, 0.15267859532230066, 0.12653639933779048, 0.19261163613712276, 0.1435661715139151, 0.08454568859156064, 0.09802691126963725, 0.017105411842319593, 0.11883642810685906, 0.059298754290867964, 0.1328280053175669, 0.10357294703572315, 0.11706534393367979, 0.045236438384709664, 0.07624187399729937, 0.02836453334395976, 0.13901506826285698, 0.04609315483926171, 0.0800895933366503, 0.050958625642515694, 0.044201609061188224, 0.10585183740354379, 0.12156945842113079, 0.05629618254073962, 0.0916633195879177, 0.07802500429916964, 0.051524494013286466, 0.11787428361693526, 0.058142900568560904, 0.09611062807552265, 0.048461290390660584, 0.03642586315813814, 0.08524300790337601, 0.12632370736117463, 0.053204677637767143, 0.06899045470380126, 0.04198891344623426, 0.08056038199371385, 0.07614023556953356, 0.07191368427812349, 0.060427501831011005, 0.05073550184144088, 0.09205670017128302, 0.1391760863321887, 0.13660937615262994, 0.06995609595154972, 0.0651859624819176, 0.06421400742819894, 0.11171194879413093, 0.09132271309820177, 0.07714936955096152, 0.0767778156139437, 0.08200876041238875, 0.061551705248710495, 0.10736800894393336, 0.07974588385863543, 0.20562039804888838, 0.1317000202022995, 0.05806906924722697, 0.09384677252792241, 0.06556061959534586, 0.19716774924144967, 0.1344299815494127, 0.0860847299510406, 0.06928274414705833, 0.08879089248780346, 0.1637099802515116, 0.12832227265816012, 0.07590912227345069, 0.08306898490671508, 0.054811819678667405, 0.15604243348355193, 0.12273565113883052, 0.09000033684403316, 0.11726910187587222, 0.04224978946237598, 0.14724487977354883, 0.07094568332634055, 0.04939441253560208, 0.08611101902615996, 0.049358533554484406, 0.052159411638669356, 0.19725525818934203, 0.07789593143103528, 0.1098331627382306, 0.05980180359781476, 0.05649853278191419, 0.09276054005197056, 0.070757561760269, 0.09903302551942943, 0.08512925707185046, 0.1562097352103446, 0.11695303479020834, 0.05009822568211102, 0.03510225450962632, 0.08998055544638225, 0.09787488361713109, 0.08554759979611504, 0.05711004966978841, 0.08625784169949095, 0.13723031529734905, 0.07682218922917708, 0.04957609719552741, 0.1359698020707523, 0.06947403589669952, 0.14689594108219153]
0.08993192082749403
Making ranges
torch.Size([41524, 2])
We keep 6.27e+06/4.84e+08 =  1% of the original kernel matrix.

torch.Size([3410, 2])
We keep 8.93e+04/1.40e+06 =  6% of the original kernel matrix.

torch.Size([13377, 2])
We keep 7.96e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([41232, 2])
We keep 8.13e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([43046, 2])
We keep 7.00e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([5524, 2])
We keep 2.06e+05/4.49e+06 =  4% of the original kernel matrix.

torch.Size([16290, 2])
We keep 1.19e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([26358, 2])
We keep 9.61e+06/2.58e+08 =  3% of the original kernel matrix.

torch.Size([33711, 2])
We keep 5.32e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([53936, 2])
We keep 1.25e+07/9.12e+08 =  1% of the original kernel matrix.

torch.Size([49031, 2])
We keep 8.93e+06/6.65e+08 =  1% of the original kernel matrix.

torch.Size([57059, 2])
We keep 1.19e+07/8.83e+08 =  1% of the original kernel matrix.

torch.Size([49945, 2])
We keep 8.71e+06/6.54e+08 =  1% of the original kernel matrix.

torch.Size([103165, 2])
We keep 3.33e+07/2.69e+09 =  1% of the original kernel matrix.

torch.Size([65786, 2])
We keep 1.38e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([73889, 2])
We keep 1.81e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([56150, 2])
We keep 1.07e+07/8.36e+08 =  1% of the original kernel matrix.

torch.Size([25587, 2])
We keep 1.25e+07/1.95e+08 =  6% of the original kernel matrix.

torch.Size([33472, 2])
We keep 4.68e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([30179, 2])
We keep 2.34e+07/4.02e+08 =  5% of the original kernel matrix.

torch.Size([35955, 2])
We keep 6.17e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([17717, 2])
We keep 2.00e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([27611, 2])
We keep 3.28e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([123285, 2])
We keep 1.14e+08/5.24e+09 =  2% of the original kernel matrix.

torch.Size([71988, 2])
We keep 1.87e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([14856, 2])
We keep 1.05e+06/3.87e+07 =  2% of the original kernel matrix.

torch.Size([25103, 2])
We keep 2.56e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([671028, 2])
We keep 9.76e+08/1.22e+11 =  0% of the original kernel matrix.

torch.Size([176467, 2])
We keep 7.45e+07/7.70e+09 =  0% of the original kernel matrix.

torch.Size([44280, 2])
We keep 7.71e+06/5.40e+08 =  1% of the original kernel matrix.

torch.Size([44225, 2])
We keep 7.08e+06/5.11e+08 =  1% of the original kernel matrix.

torch.Size([60752, 2])
We keep 1.41e+07/9.65e+08 =  1% of the original kernel matrix.

torch.Size([51565, 2])
We keep 9.00e+06/6.84e+08 =  1% of the original kernel matrix.

torch.Size([126142, 2])
We keep 5.41e+07/4.18e+09 =  1% of the original kernel matrix.

torch.Size([73391, 2])
We keep 1.67e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([47583, 2])
We keep 1.05e+07/6.58e+08 =  1% of the original kernel matrix.

torch.Size([45266, 2])
We keep 7.65e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([217506, 2])
We keep 1.66e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([98727, 2])
We keep 2.80e+07/2.58e+09 =  1% of the original kernel matrix.

torch.Size([187371, 2])
We keep 1.30e+08/9.53e+09 =  1% of the original kernel matrix.

torch.Size([91061, 2])
We keep 2.36e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([42017, 2])
We keep 1.13e+07/5.60e+08 =  2% of the original kernel matrix.

torch.Size([42792, 2])
We keep 7.34e+06/5.21e+08 =  1% of the original kernel matrix.

torch.Size([311849, 2])
We keep 2.33e+08/2.82e+10 =  0% of the original kernel matrix.

torch.Size([121677, 2])
We keep 3.77e+07/3.69e+09 =  1% of the original kernel matrix.

torch.Size([22616, 2])
We keep 2.35e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([31640, 2])
We keep 3.78e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([94652, 2])
We keep 6.60e+07/2.84e+09 =  2% of the original kernel matrix.

torch.Size([63397, 2])
We keep 1.43e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([5262, 2])
We keep 2.55e+05/4.74e+06 =  5% of the original kernel matrix.

torch.Size([15617, 2])
We keep 1.21e+06/4.79e+07 =  2% of the original kernel matrix.

torch.Size([119246, 2])
We keep 3.88e+07/3.56e+09 =  1% of the original kernel matrix.

torch.Size([71243, 2])
We keep 1.55e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([122766, 2])
We keep 9.50e+07/4.62e+09 =  2% of the original kernel matrix.

torch.Size([72562, 2])
We keep 1.76e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([21127, 2])
We keep 2.20e+06/9.56e+07 =  2% of the original kernel matrix.

torch.Size([30422, 2])
We keep 3.57e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([105551, 2])
We keep 2.14e+08/6.67e+09 =  3% of the original kernel matrix.

torch.Size([66572, 2])
We keep 2.05e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([2049645, 2])
We keep 4.83e+09/8.89e+11 =  0% of the original kernel matrix.

torch.Size([321466, 2])
We keep 1.83e+08/2.08e+10 =  0% of the original kernel matrix.

torch.Size([18851, 2])
We keep 2.15e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([28581, 2])
We keep 3.45e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([1994641, 2])
We keep 5.90e+09/9.75e+11 =  0% of the original kernel matrix.

torch.Size([315514, 2])
We keep 1.92e+08/2.17e+10 =  0% of the original kernel matrix.

torch.Size([28729, 2])
We keep 8.19e+06/2.56e+08 =  3% of the original kernel matrix.

torch.Size([35410, 2])
We keep 5.33e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([18579, 2])
We keep 1.94e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([28376, 2])
We keep 3.11e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([19542, 2])
We keep 2.09e+06/8.67e+07 =  2% of the original kernel matrix.

torch.Size([28985, 2])
We keep 3.46e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([398219, 2])
We keep 5.65e+08/4.74e+10 =  1% of the original kernel matrix.

torch.Size([134666, 2])
We keep 4.80e+07/4.79e+09 =  1% of the original kernel matrix.

torch.Size([168080, 2])
We keep 9.95e+07/8.21e+09 =  1% of the original kernel matrix.

torch.Size([85892, 2])
We keep 2.24e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([3708, 2])
We keep 8.59e+04/1.53e+06 =  5% of the original kernel matrix.

torch.Size([13908, 2])
We keep 8.12e+05/2.72e+07 =  2% of the original kernel matrix.

torch.Size([9891, 2])
We keep 5.76e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([20672, 2])
We keep 1.80e+06/8.56e+07 =  2% of the original kernel matrix.

torch.Size([9149, 2])
We keep 5.42e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([19725, 2])
We keep 1.78e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([7776, 2])
We keep 3.47e+05/8.20e+06 =  4% of the original kernel matrix.

torch.Size([18704, 2])
We keep 1.45e+06/6.30e+07 =  2% of the original kernel matrix.

torch.Size([3718, 2])
We keep 1.96e+05/3.02e+06 =  6% of the original kernel matrix.

torch.Size([12873, 2])
We keep 1.02e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([1806, 2])
We keep 2.54e+04/3.09e+05 =  8% of the original kernel matrix.

torch.Size([10612, 2])
We keep 4.79e+05/1.22e+07 =  3% of the original kernel matrix.

torch.Size([7247, 2])
We keep 3.61e+05/8.17e+06 =  4% of the original kernel matrix.

torch.Size([17848, 2])
We keep 1.48e+06/6.29e+07 =  2% of the original kernel matrix.

torch.Size([2291, 2])
We keep 4.75e+04/6.71e+05 =  7% of the original kernel matrix.

torch.Size([11581, 2])
We keep 6.19e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([2788, 2])
We keep 6.28e+04/8.82e+05 =  7% of the original kernel matrix.

torch.Size([12452, 2])
We keep 6.71e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([6374, 2])
We keep 3.13e+05/6.10e+06 =  5% of the original kernel matrix.

torch.Size([17209, 2])
We keep 1.31e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([10203, 2])
We keep 5.59e+05/1.61e+07 =  3% of the original kernel matrix.

torch.Size([21000, 2])
We keep 1.85e+06/8.84e+07 =  2% of the original kernel matrix.

torch.Size([7911, 2])
We keep 4.52e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([18589, 2])
We keep 1.61e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([22754, 2])
We keep 3.35e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([31403, 2])
We keep 4.19e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([37099, 2])
We keep 6.25e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([38709, 2])
We keep 5.91e+06/4.33e+08 =  1% of the original kernel matrix.

torch.Size([4101, 2])
We keep 1.26e+05/2.32e+06 =  5% of the original kernel matrix.

torch.Size([14377, 2])
We keep 9.54e+05/3.35e+07 =  2% of the original kernel matrix.

torch.Size([12715, 2])
We keep 8.99e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([23255, 2])
We keep 2.31e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([5641, 2])
We keep 2.17e+05/4.57e+06 =  4% of the original kernel matrix.

torch.Size([16273, 2])
We keep 1.20e+06/4.70e+07 =  2% of the original kernel matrix.

torch.Size([12663, 2])
We keep 7.88e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([23279, 2])
We keep 2.16e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([18038, 2])
We keep 2.11e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([27737, 2])
We keep 3.38e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([4540, 2])
We keep 1.35e+05/2.70e+06 =  4% of the original kernel matrix.

torch.Size([15053, 2])
We keep 9.86e+05/3.62e+07 =  2% of the original kernel matrix.

torch.Size([7955, 2])
We keep 3.43e+05/8.97e+06 =  3% of the original kernel matrix.

torch.Size([18752, 2])
We keep 1.50e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([7198, 2])
We keep 3.63e+05/8.49e+06 =  4% of the original kernel matrix.

torch.Size([17841, 2])
We keep 1.47e+06/6.41e+07 =  2% of the original kernel matrix.

torch.Size([7705, 2])
We keep 3.94e+05/9.73e+06 =  4% of the original kernel matrix.

torch.Size([18178, 2])
We keep 1.55e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([18525, 2])
We keep 1.95e+06/8.31e+07 =  2% of the original kernel matrix.

torch.Size([28503, 2])
We keep 3.47e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([4595, 2])
We keep 1.39e+05/2.75e+06 =  5% of the original kernel matrix.

torch.Size([15059, 2])
We keep 9.84e+05/3.65e+07 =  2% of the original kernel matrix.

torch.Size([15347, 2])
We keep 1.35e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([25532, 2])
We keep 2.80e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([18628, 2])
We keep 1.61e+06/6.96e+07 =  2% of the original kernel matrix.

torch.Size([28463, 2])
We keep 3.18e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([7025, 2])
We keep 2.98e+05/7.34e+06 =  4% of the original kernel matrix.

torch.Size([17890, 2])
We keep 1.39e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([4859, 2])
We keep 1.56e+05/3.25e+06 =  4% of the original kernel matrix.

torch.Size([15327, 2])
We keep 1.04e+06/3.97e+07 =  2% of the original kernel matrix.

torch.Size([6358, 2])
We keep 2.39e+05/5.48e+06 =  4% of the original kernel matrix.

torch.Size([17191, 2])
We keep 1.26e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([8498, 2])
We keep 4.45e+05/1.14e+07 =  3% of the original kernel matrix.

torch.Size([19315, 2])
We keep 1.65e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([8460, 2])
We keep 4.17e+05/1.09e+07 =  3% of the original kernel matrix.

torch.Size([19386, 2])
We keep 1.60e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([10616, 2])
We keep 6.43e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([21452, 2])
We keep 1.94e+06/9.38e+07 =  2% of the original kernel matrix.

torch.Size([9623, 2])
We keep 1.49e+06/2.15e+07 =  6% of the original kernel matrix.

torch.Size([19947, 2])
We keep 2.12e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([8623, 2])
We keep 6.16e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([19298, 2])
We keep 1.73e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([5245, 2])
We keep 1.63e+05/3.36e+06 =  4% of the original kernel matrix.

torch.Size([15905, 2])
We keep 1.06e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([6566, 2])
We keep 2.90e+05/6.80e+06 =  4% of the original kernel matrix.

torch.Size([17048, 2])
We keep 1.36e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([9700, 2])
We keep 6.12e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([20510, 2])
We keep 1.93e+06/9.12e+07 =  2% of the original kernel matrix.

torch.Size([25477, 2])
We keep 3.36e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([33631, 2])
We keep 4.48e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([2494, 2])
We keep 5.37e+04/6.92e+05 =  7% of the original kernel matrix.

torch.Size([11810, 2])
We keep 6.30e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([2046, 2])
We keep 4.54e+04/5.91e+05 =  7% of the original kernel matrix.

torch.Size([10921, 2])
We keep 5.94e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([5908, 2])
We keep 2.04e+05/4.76e+06 =  4% of the original kernel matrix.

torch.Size([16615, 2])
We keep 1.18e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([15766, 2])
We keep 1.37e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([25593, 2])
We keep 2.82e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([6542, 2])
We keep 2.63e+05/5.85e+06 =  4% of the original kernel matrix.

torch.Size([17482, 2])
We keep 1.31e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([3953, 2])
We keep 1.30e+05/2.19e+06 =  5% of the original kernel matrix.

torch.Size([13949, 2])
We keep 9.21e+05/3.26e+07 =  2% of the original kernel matrix.

torch.Size([7495, 2])
We keep 4.83e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([17993, 2])
We keep 1.61e+06/7.18e+07 =  2% of the original kernel matrix.

torch.Size([5362, 2])
We keep 1.92e+05/3.92e+06 =  4% of the original kernel matrix.

torch.Size([15948, 2])
We keep 1.13e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([4969, 2])
We keep 1.69e+05/3.36e+06 =  5% of the original kernel matrix.

torch.Size([15498, 2])
We keep 1.07e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([2958, 2])
We keep 6.51e+04/1.08e+06 =  6% of the original kernel matrix.

torch.Size([12654, 2])
We keep 7.25e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([9723, 2])
We keep 6.77e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([20351, 2])
We keep 1.92e+06/9.19e+07 =  2% of the original kernel matrix.

torch.Size([3171, 2])
We keep 7.06e+04/1.20e+06 =  5% of the original kernel matrix.

torch.Size([13088, 2])
We keep 7.41e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([6781, 2])
We keep 2.51e+05/5.85e+06 =  4% of the original kernel matrix.

torch.Size([17696, 2])
We keep 1.29e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([9700, 2])
We keep 5.49e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([20522, 2])
We keep 1.77e+06/8.38e+07 =  2% of the original kernel matrix.

torch.Size([8634, 2])
We keep 4.03e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([19410, 2])
We keep 1.61e+06/7.35e+07 =  2% of the original kernel matrix.

torch.Size([3411, 2])
We keep 7.64e+04/1.37e+06 =  5% of the original kernel matrix.

torch.Size([13514, 2])
We keep 7.79e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([3346, 2])
We keep 8.33e+04/1.37e+06 =  6% of the original kernel matrix.

torch.Size([13410, 2])
We keep 7.87e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([15901, 2])
We keep 1.68e+06/5.38e+07 =  3% of the original kernel matrix.

torch.Size([25931, 2])
We keep 2.90e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([8831, 2])
We keep 5.04e+05/1.30e+07 =  3% of the original kernel matrix.

torch.Size([19575, 2])
We keep 1.72e+06/7.93e+07 =  2% of the original kernel matrix.

torch.Size([8059, 2])
We keep 4.03e+05/1.05e+07 =  3% of the original kernel matrix.

torch.Size([19188, 2])
We keep 1.60e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([10320, 2])
We keep 5.96e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([21079, 2])
We keep 1.91e+06/9.12e+07 =  2% of the original kernel matrix.

torch.Size([17805, 2])
We keep 2.69e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([27196, 2])
We keep 3.71e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([16632, 2])
We keep 1.41e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([26682, 2])
We keep 2.91e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([7822, 2])
We keep 3.41e+05/8.68e+06 =  3% of the original kernel matrix.

torch.Size([18794, 2])
We keep 1.46e+06/6.48e+07 =  2% of the original kernel matrix.

torch.Size([26413, 2])
We keep 3.44e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([34065, 2])
We keep 4.62e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([14685, 2])
We keep 1.11e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([24957, 2])
We keep 2.49e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([9344, 2])
We keep 4.94e+05/1.23e+07 =  3% of the original kernel matrix.

torch.Size([20425, 2])
We keep 1.68e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([4445, 2])
We keep 1.33e+05/2.47e+06 =  5% of the original kernel matrix.

torch.Size([14848, 2])
We keep 9.68e+05/3.46e+07 =  2% of the original kernel matrix.

torch.Size([11138, 2])
We keep 7.97e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([21645, 2])
We keep 2.09e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([5156, 2])
We keep 1.70e+05/3.62e+06 =  4% of the original kernel matrix.

torch.Size([15771, 2])
We keep 1.09e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([11378, 2])
We keep 6.44e+05/2.05e+07 =  3% of the original kernel matrix.

torch.Size([22090, 2])
We keep 2.01e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([2028, 2])
We keep 4.47e+04/5.43e+05 =  8% of the original kernel matrix.

torch.Size([10886, 2])
We keep 5.85e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([6934, 2])
We keep 3.00e+05/7.03e+06 =  4% of the original kernel matrix.

torch.Size([17688, 2])
We keep 1.36e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([3239, 2])
We keep 8.98e+04/1.47e+06 =  6% of the original kernel matrix.

torch.Size([13034, 2])
We keep 7.99e+05/2.67e+07 =  2% of the original kernel matrix.

torch.Size([4593, 2])
We keep 1.51e+05/2.75e+06 =  5% of the original kernel matrix.

torch.Size([14957, 2])
We keep 9.87e+05/3.65e+07 =  2% of the original kernel matrix.

torch.Size([5842, 2])
We keep 3.55e+05/5.90e+06 =  6% of the original kernel matrix.

torch.Size([16116, 2])
We keep 1.32e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([5252, 2])
We keep 2.26e+05/4.37e+06 =  5% of the original kernel matrix.

torch.Size([15547, 2])
We keep 1.18e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([6001, 2])
We keep 2.88e+05/5.67e+06 =  5% of the original kernel matrix.

torch.Size([16576, 2])
We keep 1.25e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([11345, 2])
We keep 9.97e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([21962, 2])
We keep 2.24e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([6197, 2])
We keep 2.88e+05/6.56e+06 =  4% of the original kernel matrix.

torch.Size([16557, 2])
We keep 1.33e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([5628, 2])
We keep 1.94e+05/4.09e+06 =  4% of the original kernel matrix.

torch.Size([16444, 2])
We keep 1.13e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([3141, 2])
We keep 6.46e+04/1.03e+06 =  6% of the original kernel matrix.

torch.Size([12995, 2])
We keep 7.16e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([18101, 2])
We keep 1.96e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([27932, 2])
We keep 3.39e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([2877, 2])
We keep 5.81e+04/9.80e+05 =  5% of the original kernel matrix.

torch.Size([12541, 2])
We keep 6.98e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([17219, 2])
We keep 1.43e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([27210, 2])
We keep 2.98e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([4642, 2])
We keep 1.65e+05/3.13e+06 =  5% of the original kernel matrix.

torch.Size([14907, 2])
We keep 1.03e+06/3.89e+07 =  2% of the original kernel matrix.

torch.Size([4944, 2])
We keep 1.94e+05/3.70e+06 =  5% of the original kernel matrix.

torch.Size([15267, 2])
We keep 1.10e+06/4.23e+07 =  2% of the original kernel matrix.

torch.Size([6548, 2])
We keep 2.72e+05/6.01e+06 =  4% of the original kernel matrix.

torch.Size([17530, 2])
We keep 1.31e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([5650, 2])
We keep 2.69e+05/4.73e+06 =  5% of the original kernel matrix.

torch.Size([16215, 2])
We keep 1.20e+06/4.79e+07 =  2% of the original kernel matrix.

torch.Size([7483, 2])
We keep 3.34e+05/8.71e+06 =  3% of the original kernel matrix.

torch.Size([18297, 2])
We keep 1.48e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([3923, 2])
We keep 9.17e+04/1.65e+06 =  5% of the original kernel matrix.

torch.Size([14330, 2])
We keep 8.31e+05/2.83e+07 =  2% of the original kernel matrix.

torch.Size([5927, 2])
We keep 1.93e+05/4.38e+06 =  4% of the original kernel matrix.

torch.Size([16754, 2])
We keep 1.16e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([18443, 2])
We keep 1.82e+06/7.64e+07 =  2% of the original kernel matrix.

torch.Size([28330, 2])
We keep 3.34e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([7262, 2])
We keep 2.87e+05/7.02e+06 =  4% of the original kernel matrix.

torch.Size([18128, 2])
We keep 1.37e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([8773, 2])
We keep 4.40e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([19593, 2])
We keep 1.65e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([4339, 2])
We keep 1.54e+05/2.75e+06 =  5% of the original kernel matrix.

torch.Size([14741, 2])
We keep 9.85e+05/3.65e+07 =  2% of the original kernel matrix.

torch.Size([9481, 2])
We keep 5.69e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([20371, 2])
We keep 1.79e+06/8.49e+07 =  2% of the original kernel matrix.

torch.Size([4657, 2])
We keep 1.92e+05/3.75e+06 =  5% of the original kernel matrix.

torch.Size([14678, 2])
We keep 1.10e+06/4.26e+07 =  2% of the original kernel matrix.

torch.Size([6083, 2])
We keep 2.25e+05/5.10e+06 =  4% of the original kernel matrix.

torch.Size([16835, 2])
We keep 1.24e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([4518, 2])
We keep 1.26e+05/2.51e+06 =  5% of the original kernel matrix.

torch.Size([15038, 2])
We keep 9.50e+05/3.48e+07 =  2% of the original kernel matrix.

torch.Size([5108, 2])
We keep 1.95e+05/3.92e+06 =  4% of the original kernel matrix.

torch.Size([15651, 2])
We keep 1.11e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([3920, 2])
We keep 1.12e+05/2.16e+06 =  5% of the original kernel matrix.

torch.Size([14134, 2])
We keep 9.07e+05/3.23e+07 =  2% of the original kernel matrix.

torch.Size([3999, 2])
We keep 1.14e+05/2.18e+06 =  5% of the original kernel matrix.

torch.Size([14279, 2])
We keep 9.22e+05/3.25e+07 =  2% of the original kernel matrix.

torch.Size([3806, 2])
We keep 1.00e+05/1.73e+06 =  5% of the original kernel matrix.

torch.Size([14110, 2])
We keep 8.39e+05/2.89e+07 =  2% of the original kernel matrix.

torch.Size([7653, 2])
We keep 4.07e+05/9.76e+06 =  4% of the original kernel matrix.

torch.Size([18286, 2])
We keep 1.55e+06/6.87e+07 =  2% of the original kernel matrix.

torch.Size([3414, 2])
We keep 9.68e+04/1.48e+06 =  6% of the original kernel matrix.

torch.Size([13298, 2])
We keep 8.04e+05/2.68e+07 =  2% of the original kernel matrix.

torch.Size([13305, 2])
We keep 1.18e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([23609, 2])
We keep 2.59e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([28345, 2])
We keep 3.85e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([35587, 2])
We keep 4.88e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([10601, 2])
We keep 6.72e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([21322, 2])
We keep 1.96e+06/9.50e+07 =  2% of the original kernel matrix.

torch.Size([4015, 2])
We keep 1.08e+05/2.05e+06 =  5% of the original kernel matrix.

torch.Size([14356, 2])
We keep 9.06e+05/3.15e+07 =  2% of the original kernel matrix.

torch.Size([3981, 2])
We keep 1.19e+05/1.99e+06 =  5% of the original kernel matrix.

torch.Size([14370, 2])
We keep 8.98e+05/3.10e+07 =  2% of the original kernel matrix.

torch.Size([3804, 2])
We keep 1.28e+05/2.27e+06 =  5% of the original kernel matrix.

torch.Size([13908, 2])
We keep 9.32e+05/3.32e+07 =  2% of the original kernel matrix.

torch.Size([3604, 2])
We keep 1.09e+05/1.96e+06 =  5% of the original kernel matrix.

torch.Size([13542, 2])
We keep 8.84e+05/3.08e+07 =  2% of the original kernel matrix.

torch.Size([5280, 2])
We keep 2.42e+05/4.89e+06 =  4% of the original kernel matrix.

torch.Size([15577, 2])
We keep 1.19e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([6112, 2])
We keep 3.18e+05/6.65e+06 =  4% of the original kernel matrix.

torch.Size([16609, 2])
We keep 1.36e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([12503, 2])
We keep 8.34e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([23060, 2])
We keep 2.22e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([8737, 2])
We keep 4.64e+05/1.17e+07 =  3% of the original kernel matrix.

torch.Size([19629, 2])
We keep 1.65e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([22375, 2])
We keep 4.01e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([31052, 2])
We keep 4.48e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([8490, 2])
We keep 5.05e+05/1.30e+07 =  3% of the original kernel matrix.

torch.Size([18959, 2])
We keep 1.72e+06/7.94e+07 =  2% of the original kernel matrix.

torch.Size([33992, 2])
We keep 7.86e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([37161, 2])
We keep 6.27e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([6709, 2])
We keep 2.58e+05/5.98e+06 =  4% of the original kernel matrix.

torch.Size([17680, 2])
We keep 1.30e+06/5.38e+07 =  2% of the original kernel matrix.

torch.Size([9939, 2])
We keep 6.75e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([20851, 2])
We keep 1.86e+06/8.87e+07 =  2% of the original kernel matrix.

torch.Size([8423, 2])
We keep 4.35e+05/1.15e+07 =  3% of the original kernel matrix.

torch.Size([19260, 2])
We keep 1.65e+06/7.48e+07 =  2% of the original kernel matrix.

torch.Size([1561, 2])
We keep 1.99e+04/2.31e+05 =  8% of the original kernel matrix.

torch.Size([10120, 2])
We keep 4.42e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([16247, 2])
We keep 1.87e+06/6.61e+07 =  2% of the original kernel matrix.

torch.Size([26509, 2])
We keep 3.18e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([3672, 2])
We keep 1.00e+05/1.78e+06 =  5% of the original kernel matrix.

torch.Size([13524, 2])
We keep 8.49e+05/2.93e+07 =  2% of the original kernel matrix.

torch.Size([12326, 2])
We keep 1.71e+06/3.24e+07 =  5% of the original kernel matrix.

torch.Size([22976, 2])
We keep 2.41e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([2553, 2])
We keep 6.59e+04/9.16e+05 =  7% of the original kernel matrix.

torch.Size([11766, 2])
We keep 6.93e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([7958, 2])
We keep 5.03e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([18481, 2])
We keep 1.69e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([5537, 2])
We keep 2.00e+05/4.12e+06 =  4% of the original kernel matrix.

torch.Size([16245, 2])
We keep 1.14e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([6562, 2])
We keep 2.63e+05/5.87e+06 =  4% of the original kernel matrix.

torch.Size([17529, 2])
We keep 1.29e+06/5.33e+07 =  2% of the original kernel matrix.

torch.Size([7148, 2])
We keep 3.09e+05/7.16e+06 =  4% of the original kernel matrix.

torch.Size([17993, 2])
We keep 1.39e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([9222, 2])
We keep 4.67e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([20057, 2])
We keep 1.71e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([1824, 2])
We keep 2.56e+04/3.25e+05 =  7% of the original kernel matrix.

torch.Size([10622, 2])
We keep 4.96e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([2329, 2])
We keep 4.92e+04/6.69e+05 =  7% of the original kernel matrix.

torch.Size([11474, 2])
We keep 6.24e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([4277, 2])
We keep 1.70e+05/2.67e+06 =  6% of the original kernel matrix.

torch.Size([14487, 2])
We keep 9.88e+05/3.60e+07 =  2% of the original kernel matrix.

torch.Size([11227, 2])
We keep 8.05e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([22096, 2])
We keep 2.01e+06/9.98e+07 =  2% of the original kernel matrix.

torch.Size([4061, 2])
We keep 1.18e+05/2.10e+06 =  5% of the original kernel matrix.

torch.Size([14442, 2])
We keep 8.96e+05/3.19e+07 =  2% of the original kernel matrix.

torch.Size([12481, 2])
We keep 9.12e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([23058, 2])
We keep 2.29e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([5040, 2])
We keep 1.68e+05/3.65e+06 =  4% of the original kernel matrix.

torch.Size([15536, 2])
We keep 1.08e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([8437, 2])
We keep 4.67e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([19281, 2])
We keep 1.63e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([8962, 2])
We keep 4.45e+05/1.21e+07 =  3% of the original kernel matrix.

torch.Size([19941, 2])
We keep 1.67e+06/7.64e+07 =  2% of the original kernel matrix.

torch.Size([14036, 2])
We keep 1.18e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([24514, 2])
We keep 2.48e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([5660, 2])
We keep 1.87e+05/4.16e+06 =  4% of the original kernel matrix.

torch.Size([16286, 2])
We keep 1.14e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([15247, 2])
We keep 1.21e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([25484, 2])
We keep 2.66e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([6326, 2])
We keep 2.89e+05/6.03e+06 =  4% of the original kernel matrix.

torch.Size([17096, 2])
We keep 1.33e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([11595, 2])
We keep 9.44e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([22114, 2])
We keep 2.23e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([3617, 2])
We keep 8.26e+04/1.53e+06 =  5% of the original kernel matrix.

torch.Size([13837, 2])
We keep 8.12e+05/2.72e+07 =  2% of the original kernel matrix.

torch.Size([8563, 2])
We keep 4.46e+05/1.14e+07 =  3% of the original kernel matrix.

torch.Size([19338, 2])
We keep 1.62e+06/7.42e+07 =  2% of the original kernel matrix.

torch.Size([14383, 2])
We keep 1.07e+06/3.86e+07 =  2% of the original kernel matrix.

torch.Size([24824, 2])
We keep 2.56e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([4925, 2])
We keep 1.60e+05/3.08e+06 =  5% of the original kernel matrix.

torch.Size([15393, 2])
We keep 1.04e+06/3.86e+07 =  2% of the original kernel matrix.

torch.Size([13714, 2])
We keep 1.45e+06/4.58e+07 =  3% of the original kernel matrix.

torch.Size([23943, 2])
We keep 2.74e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([11209, 2])
We keep 7.88e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([21940, 2])
We keep 2.12e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([21493, 2])
We keep 2.64e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([30575, 2])
We keep 3.94e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([4335, 2])
We keep 1.19e+05/2.33e+06 =  5% of the original kernel matrix.

torch.Size([14658, 2])
We keep 9.36e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([2025, 2])
We keep 3.10e+04/4.16e+05 =  7% of the original kernel matrix.

torch.Size([11065, 2])
We keep 5.26e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([5712, 2])
We keep 1.81e+05/4.19e+06 =  4% of the original kernel matrix.

torch.Size([16519, 2])
We keep 1.13e+06/4.51e+07 =  2% of the original kernel matrix.

torch.Size([4954, 2])
We keep 1.74e+05/3.53e+06 =  4% of the original kernel matrix.

torch.Size([15317, 2])
We keep 1.08e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([12496, 2])
We keep 1.10e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([22935, 2])
We keep 2.40e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([4831, 2])
We keep 1.80e+05/3.49e+06 =  5% of the original kernel matrix.

torch.Size([15164, 2])
We keep 1.08e+06/4.11e+07 =  2% of the original kernel matrix.

torch.Size([4526, 2])
We keep 1.47e+05/2.50e+06 =  5% of the original kernel matrix.

torch.Size([15133, 2])
We keep 9.59e+05/3.48e+07 =  2% of the original kernel matrix.

torch.Size([4405, 2])
We keep 1.47e+05/2.66e+06 =  5% of the original kernel matrix.

torch.Size([14863, 2])
We keep 9.70e+05/3.59e+07 =  2% of the original kernel matrix.

torch.Size([11008, 2])
We keep 9.84e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([21787, 2])
We keep 2.03e+06/9.86e+07 =  2% of the original kernel matrix.

torch.Size([3238, 2])
We keep 7.34e+04/1.19e+06 =  6% of the original kernel matrix.

torch.Size([13263, 2])
We keep 7.54e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([17265, 2])
We keep 1.54e+06/6.21e+07 =  2% of the original kernel matrix.

torch.Size([27302, 2])
We keep 3.11e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([5698, 2])
We keep 1.96e+05/4.25e+06 =  4% of the original kernel matrix.

torch.Size([16481, 2])
We keep 1.15e+06/4.54e+07 =  2% of the original kernel matrix.

torch.Size([22756, 2])
We keep 2.30e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([31771, 2])
We keep 3.93e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([8224, 2])
We keep 3.76e+05/9.64e+06 =  3% of the original kernel matrix.

torch.Size([19126, 2])
We keep 1.54e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([8168, 2])
We keep 3.92e+05/9.87e+06 =  3% of the original kernel matrix.

torch.Size([19124, 2])
We keep 1.55e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([2795, 2])
We keep 5.40e+04/8.45e+05 =  6% of the original kernel matrix.

torch.Size([12470, 2])
We keep 6.67e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([9111, 2])
We keep 6.16e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([19767, 2])
We keep 1.81e+06/8.55e+07 =  2% of the original kernel matrix.

torch.Size([8944, 2])
We keep 4.17e+05/1.12e+07 =  3% of the original kernel matrix.

torch.Size([19770, 2])
We keep 1.63e+06/7.35e+07 =  2% of the original kernel matrix.

torch.Size([7361, 2])
We keep 3.47e+05/8.25e+06 =  4% of the original kernel matrix.

torch.Size([18286, 2])
We keep 1.45e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([12230, 2])
We keep 7.71e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([22915, 2])
We keep 2.20e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([12513, 2])
We keep 1.04e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([22917, 2])
We keep 2.39e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([6649, 2])
We keep 3.19e+05/6.71e+06 =  4% of the original kernel matrix.

torch.Size([17476, 2])
We keep 1.35e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([5064, 2])
We keep 1.57e+05/3.15e+06 =  4% of the original kernel matrix.

torch.Size([15718, 2])
We keep 1.04e+06/3.90e+07 =  2% of the original kernel matrix.

torch.Size([5168, 2])
We keep 1.94e+05/3.82e+06 =  5% of the original kernel matrix.

torch.Size([15530, 2])
We keep 1.11e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([1844, 2])
We keep 3.23e+04/4.32e+05 =  7% of the original kernel matrix.

torch.Size([10547, 2])
We keep 5.32e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([2444, 2])
We keep 6.50e+04/8.70e+05 =  7% of the original kernel matrix.

torch.Size([11494, 2])
We keep 6.78e+05/2.05e+07 =  3% of the original kernel matrix.

torch.Size([1769, 2])
We keep 2.59e+04/3.19e+05 =  8% of the original kernel matrix.

torch.Size([10611, 2])
We keep 4.85e+05/1.24e+07 =  3% of the original kernel matrix.

torch.Size([3771, 2])
We keep 1.11e+05/1.93e+06 =  5% of the original kernel matrix.

torch.Size([13950, 2])
We keep 8.81e+05/3.06e+07 =  2% of the original kernel matrix.

torch.Size([18908, 2])
We keep 2.93e+06/9.75e+07 =  3% of the original kernel matrix.

torch.Size([28107, 2])
We keep 3.64e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([4590, 2])
We keep 1.21e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([15021, 2])
We keep 9.40e+05/3.42e+07 =  2% of the original kernel matrix.

torch.Size([7426, 2])
We keep 5.32e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([17815, 2])
We keep 1.62e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([3655, 2])
We keep 1.05e+05/1.88e+06 =  5% of the original kernel matrix.

torch.Size([13572, 2])
We keep 8.69e+05/3.01e+07 =  2% of the original kernel matrix.

torch.Size([9482, 2])
We keep 5.45e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([20182, 2])
We keep 1.81e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([6476, 2])
We keep 3.75e+05/7.86e+06 =  4% of the original kernel matrix.

torch.Size([16833, 2])
We keep 1.43e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([7373, 2])
We keep 3.49e+05/8.61e+06 =  4% of the original kernel matrix.

torch.Size([17913, 2])
We keep 1.46e+06/6.46e+07 =  2% of the original kernel matrix.

torch.Size([5664, 2])
We keep 2.55e+05/4.73e+06 =  5% of the original kernel matrix.

torch.Size([16212, 2])
We keep 1.20e+06/4.79e+07 =  2% of the original kernel matrix.

torch.Size([3325, 2])
We keep 7.69e+04/1.33e+06 =  5% of the original kernel matrix.

torch.Size([13286, 2])
We keep 7.88e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([3796, 2])
We keep 9.62e+04/1.75e+06 =  5% of the original kernel matrix.

torch.Size([13972, 2])
We keep 8.48e+05/2.91e+07 =  2% of the original kernel matrix.

torch.Size([8239, 2])
We keep 4.30e+05/1.13e+07 =  3% of the original kernel matrix.

torch.Size([18862, 2])
We keep 1.63e+06/7.39e+07 =  2% of the original kernel matrix.

torch.Size([2551, 2])
We keep 5.38e+04/7.66e+05 =  7% of the original kernel matrix.

torch.Size([11907, 2])
We keep 6.50e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([2986, 2])
We keep 9.03e+04/1.31e+06 =  6% of the original kernel matrix.

torch.Size([12503, 2])
We keep 7.69e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([5933, 2])
We keep 2.69e+05/5.49e+06 =  4% of the original kernel matrix.

torch.Size([16471, 2])
We keep 1.25e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([5334, 2])
We keep 1.86e+05/3.96e+06 =  4% of the original kernel matrix.

torch.Size([16081, 2])
We keep 1.13e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([11385, 2])
We keep 8.97e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([22327, 2])
We keep 2.15e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([5714, 2])
We keep 2.10e+05/4.33e+06 =  4% of the original kernel matrix.

torch.Size([16378, 2])
We keep 1.16e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([5321, 2])
We keep 2.29e+05/4.44e+06 =  5% of the original kernel matrix.

torch.Size([15701, 2])
We keep 1.16e+06/4.64e+07 =  2% of the original kernel matrix.

torch.Size([19124, 2])
We keep 1.87e+06/8.22e+07 =  2% of the original kernel matrix.

torch.Size([29078, 2])
We keep 3.41e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([13173, 2])
We keep 9.58e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([23820, 2])
We keep 2.32e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([2753, 2])
We keep 5.37e+04/8.54e+05 =  6% of the original kernel matrix.

torch.Size([12434, 2])
We keep 6.67e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([39194, 2])
We keep 1.13e+07/4.86e+08 =  2% of the original kernel matrix.

torch.Size([41534, 2])
We keep 6.95e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([105142, 2])
We keep 1.13e+08/5.79e+09 =  1% of the original kernel matrix.

torch.Size([65108, 2])
We keep 1.93e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([6350, 2])
We keep 3.27e+05/5.63e+06 =  5% of the original kernel matrix.

torch.Size([17367, 2])
We keep 1.24e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([16640, 2])
We keep 1.68e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([26883, 2])
We keep 2.97e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([1504, 2])
We keep 4.48e+04/3.78e+05 = 11% of the original kernel matrix.

torch.Size([9592, 2])
We keep 5.29e+05/1.35e+07 =  3% of the original kernel matrix.

torch.Size([10030, 2])
We keep 5.80e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([20899, 2])
We keep 1.85e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([17265, 2])
We keep 1.74e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([27251, 2])
We keep 3.05e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([285247, 2])
We keep 5.76e+08/3.48e+10 =  1% of the original kernel matrix.

torch.Size([111097, 2])
We keep 4.13e+07/4.11e+09 =  1% of the original kernel matrix.

torch.Size([5417, 2])
We keep 1.80e+05/3.90e+06 =  4% of the original kernel matrix.

torch.Size([16215, 2])
We keep 1.12e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([200136, 2])
We keep 1.40e+08/1.30e+10 =  1% of the original kernel matrix.

torch.Size([93566, 2])
We keep 2.72e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([43937, 2])
We keep 2.40e+07/6.46e+08 =  3% of the original kernel matrix.

torch.Size([43321, 2])
We keep 7.65e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([25829, 2])
We keep 5.66e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([33256, 2])
We keep 4.88e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([38379, 2])
We keep 8.63e+06/4.18e+08 =  2% of the original kernel matrix.

torch.Size([40776, 2])
We keep 6.31e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([4209, 2])
We keep 1.05e+05/2.07e+06 =  5% of the original kernel matrix.

torch.Size([14752, 2])
We keep 8.98e+05/3.16e+07 =  2% of the original kernel matrix.

torch.Size([12528, 2])
We keep 9.27e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([23021, 2])
We keep 2.27e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([399392, 2])
We keep 4.69e+08/4.36e+10 =  1% of the original kernel matrix.

torch.Size([134262, 2])
We keep 4.62e+07/4.59e+09 =  1% of the original kernel matrix.

torch.Size([555357, 2])
We keep 7.54e+08/7.30e+10 =  1% of the original kernel matrix.

torch.Size([159323, 2])
We keep 5.89e+07/5.95e+09 =  0% of the original kernel matrix.

torch.Size([10725, 2])
We keep 1.01e+06/2.00e+07 =  5% of the original kernel matrix.

torch.Size([21662, 2])
We keep 2.00e+06/9.84e+07 =  2% of the original kernel matrix.

torch.Size([82605, 2])
We keep 2.87e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([58942, 2])
We keep 1.16e+07/9.20e+08 =  1% of the original kernel matrix.

torch.Size([41518, 2])
We keep 2.90e+07/6.10e+08 =  4% of the original kernel matrix.

torch.Size([42355, 2])
We keep 7.52e+06/5.43e+08 =  1% of the original kernel matrix.

torch.Size([44744, 2])
We keep 2.16e+07/6.63e+08 =  3% of the original kernel matrix.

torch.Size([44210, 2])
We keep 7.84e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([117985, 2])
We keep 9.86e+07/6.13e+09 =  1% of the original kernel matrix.

torch.Size([70026, 2])
We keep 1.99e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([43265, 2])
We keep 7.00e+07/1.29e+09 =  5% of the original kernel matrix.

torch.Size([40186, 2])
We keep 1.04e+07/7.91e+08 =  1% of the original kernel matrix.

torch.Size([47588, 2])
We keep 1.60e+07/7.52e+08 =  2% of the original kernel matrix.

torch.Size([45744, 2])
We keep 8.22e+06/6.03e+08 =  1% of the original kernel matrix.

torch.Size([40583, 2])
We keep 1.10e+07/4.81e+08 =  2% of the original kernel matrix.

torch.Size([42101, 2])
We keep 6.75e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([3953, 2])
We keep 9.18e+04/1.71e+06 =  5% of the original kernel matrix.

torch.Size([14353, 2])
We keep 8.35e+05/2.88e+07 =  2% of the original kernel matrix.

torch.Size([307377, 2])
We keep 7.20e+08/4.37e+10 =  1% of the original kernel matrix.

torch.Size([114425, 2])
We keep 4.58e+07/4.60e+09 =  0% of the original kernel matrix.

torch.Size([10758, 2])
We keep 3.70e+06/3.05e+07 = 12% of the original kernel matrix.

torch.Size([21494, 2])
We keep 2.33e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([7095, 2])
We keep 2.75e+05/6.93e+06 =  3% of the original kernel matrix.

torch.Size([17934, 2])
We keep 1.36e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([30467, 2])
We keep 5.48e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([36925, 2])
We keep 5.27e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([80110, 2])
We keep 5.57e+07/2.41e+09 =  2% of the original kernel matrix.

torch.Size([56168, 2])
We keep 1.34e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([533905, 2])
We keep 1.11e+09/8.69e+10 =  1% of the original kernel matrix.

torch.Size([156903, 2])
We keep 6.34e+07/6.49e+09 =  0% of the original kernel matrix.

torch.Size([117403, 2])
We keep 4.45e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([70579, 2])
We keep 1.58e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([4427, 2])
We keep 1.32e+05/2.60e+06 =  5% of the original kernel matrix.

torch.Size([14882, 2])
We keep 9.69e+05/3.55e+07 =  2% of the original kernel matrix.

torch.Size([40840, 2])
We keep 1.18e+07/5.49e+08 =  2% of the original kernel matrix.

torch.Size([42313, 2])
We keep 7.24e+06/5.15e+08 =  1% of the original kernel matrix.

torch.Size([20914, 2])
We keep 2.88e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([30225, 2])
We keep 3.94e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([46558, 2])
We keep 1.37e+07/7.43e+08 =  1% of the original kernel matrix.

torch.Size([44614, 2])
We keep 8.04e+06/6.00e+08 =  1% of the original kernel matrix.

torch.Size([86477, 2])
We keep 2.40e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([60942, 2])
We keep 1.22e+07/9.96e+08 =  1% of the original kernel matrix.

torch.Size([15129, 2])
We keep 2.92e+06/5.69e+07 =  5% of the original kernel matrix.

torch.Size([25453, 2])
We keep 3.02e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([55703, 2])
We keep 1.82e+07/9.00e+08 =  2% of the original kernel matrix.

torch.Size([48830, 2])
We keep 8.88e+06/6.60e+08 =  1% of the original kernel matrix.

torch.Size([1607, 2])
We keep 1.88e+04/2.14e+05 =  8% of the original kernel matrix.

torch.Size([10318, 2])
We keep 4.28e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([11413, 2])
We keep 6.70e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([22147, 2])
We keep 1.99e+06/9.88e+07 =  2% of the original kernel matrix.

torch.Size([146387, 2])
We keep 7.83e+07/6.64e+09 =  1% of the original kernel matrix.

torch.Size([78959, 2])
We keep 2.03e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([62361, 2])
We keep 3.12e+07/1.52e+09 =  2% of the original kernel matrix.

torch.Size([50529, 2])
We keep 1.10e+07/8.59e+08 =  1% of the original kernel matrix.

torch.Size([95347, 2])
We keep 5.13e+07/3.21e+09 =  1% of the original kernel matrix.

torch.Size([61536, 2])
We keep 1.50e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([9455, 2])
We keep 6.38e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([20206, 2])
We keep 1.85e+06/8.93e+07 =  2% of the original kernel matrix.

torch.Size([372613, 2])
We keep 3.87e+08/5.36e+10 =  0% of the original kernel matrix.

torch.Size([131094, 2])
We keep 5.02e+07/5.10e+09 =  0% of the original kernel matrix.

torch.Size([21798, 2])
We keep 2.12e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([31010, 2])
We keep 3.67e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([49047, 2])
We keep 1.09e+07/6.60e+08 =  1% of the original kernel matrix.

torch.Size([46378, 2])
We keep 7.72e+06/5.66e+08 =  1% of the original kernel matrix.

torch.Size([163821, 2])
We keep 1.09e+08/8.85e+09 =  1% of the original kernel matrix.

torch.Size([84130, 2])
We keep 2.30e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([5548, 2])
We keep 2.15e+05/4.60e+06 =  4% of the original kernel matrix.

torch.Size([16161, 2])
We keep 1.18e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([125824, 2])
We keep 6.06e+07/4.19e+09 =  1% of the original kernel matrix.

torch.Size([73113, 2])
We keep 1.67e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([324412, 2])
We keep 5.67e+08/4.03e+10 =  1% of the original kernel matrix.

torch.Size([121278, 2])
We keep 4.55e+07/4.42e+09 =  1% of the original kernel matrix.

torch.Size([582155, 2])
We keep 9.26e+08/9.82e+10 =  0% of the original kernel matrix.

torch.Size([163191, 2])
We keep 6.69e+07/6.90e+09 =  0% of the original kernel matrix.

torch.Size([39434, 2])
We keep 8.98e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([41796, 2])
We keep 7.40e+06/5.19e+08 =  1% of the original kernel matrix.

torch.Size([28197, 2])
We keep 4.55e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([35170, 2])
We keep 5.02e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([14431, 2])
We keep 3.02e+06/5.79e+07 =  5% of the original kernel matrix.

torch.Size([24548, 2])
We keep 3.00e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([11599, 2])
We keep 6.60e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([22255, 2])
We keep 2.04e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([132013, 2])
We keep 1.42e+08/7.70e+09 =  1% of the original kernel matrix.

torch.Size([74343, 2])
We keep 2.17e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([13941, 2])
We keep 1.23e+06/4.09e+07 =  3% of the original kernel matrix.

torch.Size([24541, 2])
We keep 2.58e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([33715, 2])
We keep 8.96e+06/3.43e+08 =  2% of the original kernel matrix.

torch.Size([38472, 2])
We keep 6.00e+06/4.08e+08 =  1% of the original kernel matrix.

torch.Size([59173, 2])
We keep 3.21e+07/1.14e+09 =  2% of the original kernel matrix.

torch.Size([50496, 2])
We keep 8.53e+06/7.44e+08 =  1% of the original kernel matrix.

torch.Size([40151, 2])
We keep 3.00e+07/4.93e+08 =  6% of the original kernel matrix.

torch.Size([41975, 2])
We keep 6.57e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([30254, 2])
We keep 8.73e+06/3.17e+08 =  2% of the original kernel matrix.

torch.Size([35953, 2])
We keep 5.89e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([10126, 2])
We keep 5.66e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([21034, 2])
We keep 1.87e+06/8.93e+07 =  2% of the original kernel matrix.

torch.Size([197509, 2])
We keep 5.35e+08/3.69e+10 =  1% of the original kernel matrix.

torch.Size([86624, 2])
We keep 4.29e+07/4.23e+09 =  1% of the original kernel matrix.

torch.Size([20338, 2])
We keep 4.54e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([29687, 2])
We keep 3.94e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([28970, 2])
We keep 2.40e+07/3.46e+08 =  6% of the original kernel matrix.

torch.Size([35254, 2])
We keep 6.09e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([31800, 2])
We keep 5.54e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([37862, 2])
We keep 5.77e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([103181, 2])
We keep 1.70e+08/5.64e+09 =  3% of the original kernel matrix.

torch.Size([65478, 2])
We keep 1.83e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([22356, 2])
We keep 2.67e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([31425, 2])
We keep 3.97e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([205631, 2])
We keep 3.37e+08/1.90e+10 =  1% of the original kernel matrix.

torch.Size([93955, 2])
We keep 3.25e+07/3.03e+09 =  1% of the original kernel matrix.

torch.Size([11482, 2])
We keep 1.54e+06/3.03e+07 =  5% of the original kernel matrix.

torch.Size([22375, 2])
We keep 2.29e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([53425, 2])
We keep 2.54e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([45829, 2])
We keep 1.02e+07/7.95e+08 =  1% of the original kernel matrix.

torch.Size([17017, 2])
We keep 2.13e+06/6.22e+07 =  3% of the original kernel matrix.

torch.Size([27181, 2])
We keep 2.96e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([291339, 2])
We keep 6.77e+08/4.08e+10 =  1% of the original kernel matrix.

torch.Size([109960, 2])
We keep 4.48e+07/4.45e+09 =  1% of the original kernel matrix.

torch.Size([50670, 2])
We keep 1.37e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([46513, 2])
We keep 9.18e+06/7.09e+08 =  1% of the original kernel matrix.

torch.Size([14590, 2])
We keep 9.94e+05/3.58e+07 =  2% of the original kernel matrix.

torch.Size([25022, 2])
We keep 2.46e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([209342, 2])
We keep 1.06e+08/1.26e+10 =  0% of the original kernel matrix.

torch.Size([96986, 2])
We keep 2.69e+07/2.47e+09 =  1% of the original kernel matrix.

torch.Size([261301, 2])
We keep 1.47e+08/1.83e+10 =  0% of the original kernel matrix.

torch.Size([109664, 2])
We keep 3.14e+07/2.98e+09 =  1% of the original kernel matrix.

torch.Size([9002, 2])
We keep 5.27e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([19913, 2])
We keep 1.71e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([196800, 2])
We keep 2.42e+08/1.16e+10 =  2% of the original kernel matrix.

torch.Size([93409, 2])
We keep 2.57e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([291864, 2])
We keep 2.17e+08/2.75e+10 =  0% of the original kernel matrix.

torch.Size([116016, 2])
We keep 3.79e+07/3.65e+09 =  1% of the original kernel matrix.

torch.Size([645759, 2])
We keep 8.35e+08/1.12e+11 =  0% of the original kernel matrix.

torch.Size([172646, 2])
We keep 7.06e+07/7.37e+09 =  0% of the original kernel matrix.

torch.Size([43588, 2])
We keep 4.26e+07/9.13e+08 =  4% of the original kernel matrix.

torch.Size([42689, 2])
We keep 9.06e+06/6.65e+08 =  1% of the original kernel matrix.

torch.Size([6103, 2])
We keep 3.01e+05/6.14e+06 =  4% of the original kernel matrix.

torch.Size([16856, 2])
We keep 1.32e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([19466, 2])
We keep 1.66e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([29013, 2])
We keep 3.25e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([12804, 2])
We keep 1.59e+06/4.47e+07 =  3% of the original kernel matrix.

torch.Size([23590, 2])
We keep 2.56e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([29115, 2])
We keep 4.49e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([35913, 2])
We keep 4.99e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([35759, 2])
We keep 1.24e+07/3.48e+08 =  3% of the original kernel matrix.

torch.Size([40168, 2])
We keep 6.09e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([4833, 2])
We keep 1.46e+05/2.99e+06 =  4% of the original kernel matrix.

torch.Size([15339, 2])
We keep 1.03e+06/3.81e+07 =  2% of the original kernel matrix.

torch.Size([112724, 2])
We keep 5.69e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([69005, 2])
We keep 1.51e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([190645, 2])
We keep 9.78e+07/9.90e+09 =  0% of the original kernel matrix.

torch.Size([91839, 2])
We keep 2.42e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([6122, 2])
We keep 3.33e+06/1.67e+07 = 20% of the original kernel matrix.

torch.Size([16165, 2])
We keep 1.82e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([17550, 2])
We keep 2.20e+06/6.85e+07 =  3% of the original kernel matrix.

torch.Size([27442, 2])
We keep 3.16e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([44405, 2])
We keep 2.54e+07/5.57e+08 =  4% of the original kernel matrix.

torch.Size([43962, 2])
We keep 7.12e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([20140, 2])
We keep 2.28e+06/9.73e+07 =  2% of the original kernel matrix.

torch.Size([29719, 2])
We keep 3.65e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([17536, 2])
We keep 3.64e+06/7.80e+07 =  4% of the original kernel matrix.

torch.Size([27440, 2])
We keep 3.35e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([25920, 2])
We keep 3.08e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([34042, 2])
We keep 4.41e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([8612, 2])
We keep 4.08e+05/1.10e+07 =  3% of the original kernel matrix.

torch.Size([19546, 2])
We keep 1.59e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([28080, 2])
We keep 9.78e+06/2.47e+08 =  3% of the original kernel matrix.

torch.Size([34490, 2])
We keep 5.21e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([69487, 2])
We keep 1.82e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([54771, 2])
We keep 9.87e+06/7.71e+08 =  1% of the original kernel matrix.

torch.Size([41710, 2])
We keep 9.49e+06/4.95e+08 =  1% of the original kernel matrix.

torch.Size([43160, 2])
We keep 6.90e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([73562, 2])
We keep 4.82e+07/2.16e+09 =  2% of the original kernel matrix.

torch.Size([55093, 2])
We keep 1.26e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([18274, 2])
We keep 2.27e+06/8.53e+07 =  2% of the original kernel matrix.

torch.Size([27857, 2])
We keep 3.44e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([728872, 2])
We keep 1.33e+09/1.48e+11 =  0% of the original kernel matrix.

torch.Size([185953, 2])
We keep 8.15e+07/8.46e+09 =  0% of the original kernel matrix.

torch.Size([19481, 2])
We keep 1.92e+06/7.80e+07 =  2% of the original kernel matrix.

torch.Size([29012, 2])
We keep 3.32e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([127945, 2])
We keep 5.27e+07/4.50e+09 =  1% of the original kernel matrix.

torch.Size([73983, 2])
We keep 1.71e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([4351, 2])
We keep 1.38e+05/2.74e+06 =  5% of the original kernel matrix.

torch.Size([14723, 2])
We keep 9.84e+05/3.64e+07 =  2% of the original kernel matrix.

torch.Size([9457, 2])
We keep 4.57e+05/1.24e+07 =  3% of the original kernel matrix.

torch.Size([20494, 2])
We keep 1.69e+06/7.76e+07 =  2% of the original kernel matrix.

torch.Size([8972, 2])
We keep 4.74e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([19832, 2])
We keep 1.66e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([23620, 2])
We keep 3.58e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([32299, 2])
We keep 3.91e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([6794, 2])
We keep 2.89e+05/6.61e+06 =  4% of the original kernel matrix.

torch.Size([17579, 2])
We keep 1.36e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([154734, 2])
We keep 1.92e+08/8.91e+09 =  2% of the original kernel matrix.

torch.Size([81811, 2])
We keep 2.32e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([8250, 2])
We keep 3.77e+05/9.62e+06 =  3% of the original kernel matrix.

torch.Size([19008, 2])
We keep 1.55e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([8064, 2])
We keep 4.15e+05/1.07e+07 =  3% of the original kernel matrix.

torch.Size([18879, 2])
We keep 1.59e+06/7.21e+07 =  2% of the original kernel matrix.

torch.Size([307341, 2])
We keep 3.10e+08/3.08e+10 =  1% of the original kernel matrix.

torch.Size([119281, 2])
We keep 4.00e+07/3.86e+09 =  1% of the original kernel matrix.

torch.Size([92898, 2])
We keep 4.32e+07/2.72e+09 =  1% of the original kernel matrix.

torch.Size([62573, 2])
We keep 1.40e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([71671, 2])
We keep 2.35e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([55035, 2])
We keep 1.07e+07/8.53e+08 =  1% of the original kernel matrix.

torch.Size([202124, 2])
We keep 1.43e+08/1.33e+10 =  1% of the original kernel matrix.

torch.Size([94790, 2])
We keep 2.74e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([535013, 2])
We keep 5.68e+08/7.50e+10 =  0% of the original kernel matrix.

torch.Size([156038, 2])
We keep 5.94e+07/6.03e+09 =  0% of the original kernel matrix.

torch.Size([15658, 2])
We keep 1.36e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([25813, 2])
We keep 2.87e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([18471, 2])
We keep 1.51e+06/6.38e+07 =  2% of the original kernel matrix.

torch.Size([28172, 2])
We keep 3.08e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([23732, 2])
We keep 6.25e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([31813, 2])
We keep 4.93e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([3274, 2])
We keep 8.15e+04/1.38e+06 =  5% of the original kernel matrix.

torch.Size([13003, 2])
We keep 7.80e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([7390, 2])
We keep 3.39e+05/8.53e+06 =  3% of the original kernel matrix.

torch.Size([18122, 2])
We keep 1.48e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([34431, 2])
We keep 8.28e+06/3.42e+08 =  2% of the original kernel matrix.

torch.Size([38877, 2])
We keep 5.86e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([8545, 2])
We keep 9.64e+05/1.30e+07 =  7% of the original kernel matrix.

torch.Size([19462, 2])
We keep 1.69e+06/7.93e+07 =  2% of the original kernel matrix.

torch.Size([8075, 2])
We keep 3.49e+05/9.02e+06 =  3% of the original kernel matrix.

torch.Size([19001, 2])
We keep 1.50e+06/6.61e+07 =  2% of the original kernel matrix.

torch.Size([8407, 2])
We keep 4.26e+05/1.07e+07 =  3% of the original kernel matrix.

torch.Size([19143, 2])
We keep 1.60e+06/7.20e+07 =  2% of the original kernel matrix.

torch.Size([11482, 2])
We keep 1.41e+06/2.68e+07 =  5% of the original kernel matrix.

torch.Size([22182, 2])
We keep 2.22e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([417024, 2])
We keep 5.59e+08/5.10e+10 =  1% of the original kernel matrix.

torch.Size([137572, 2])
We keep 4.95e+07/4.97e+09 =  0% of the original kernel matrix.

torch.Size([26318, 2])
We keep 6.59e+06/3.08e+08 =  2% of the original kernel matrix.

torch.Size([33052, 2])
We keep 5.82e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([137412, 2])
We keep 1.29e+08/6.95e+09 =  1% of the original kernel matrix.

torch.Size([76652, 2])
We keep 2.09e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([7658, 2])
We keep 7.34e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([18493, 2])
We keep 1.71e+06/7.71e+07 =  2% of the original kernel matrix.

torch.Size([10201, 2])
We keep 7.32e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([20848, 2])
We keep 2.01e+06/9.83e+07 =  2% of the original kernel matrix.

torch.Size([72345, 2])
We keep 2.24e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([54980, 2])
We keep 1.08e+07/8.50e+08 =  1% of the original kernel matrix.

torch.Size([386599, 2])
We keep 1.29e+09/5.76e+10 =  2% of the original kernel matrix.

torch.Size([132379, 2])
We keep 5.06e+07/5.28e+09 =  0% of the original kernel matrix.

torch.Size([670810, 2])
We keep 7.44e+08/1.13e+11 =  0% of the original kernel matrix.

torch.Size([177277, 2])
We keep 7.05e+07/7.40e+09 =  0% of the original kernel matrix.

torch.Size([22613, 2])
We keep 2.34e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([31578, 2])
We keep 3.86e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([75358, 2])
We keep 2.40e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([56734, 2])
We keep 1.06e+07/8.39e+08 =  1% of the original kernel matrix.

torch.Size([7294, 2])
We keep 3.01e+05/7.25e+06 =  4% of the original kernel matrix.

torch.Size([18222, 2])
We keep 1.39e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([8681, 2])
We keep 5.95e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([19648, 2])
We keep 1.69e+06/7.87e+07 =  2% of the original kernel matrix.

torch.Size([424833, 2])
We keep 2.75e+09/1.00e+11 =  2% of the original kernel matrix.

torch.Size([134732, 2])
We keep 6.75e+07/6.96e+09 =  0% of the original kernel matrix.

torch.Size([36083, 2])
We keep 8.32e+06/3.54e+08 =  2% of the original kernel matrix.

torch.Size([40575, 2])
We keep 6.05e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([16176, 2])
We keep 1.36e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([26274, 2])
We keep 2.84e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([68908, 2])
We keep 2.74e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([54191, 2])
We keep 1.10e+07/8.78e+08 =  1% of the original kernel matrix.

torch.Size([1110893, 2])
We keep 2.56e+09/3.42e+11 =  0% of the original kernel matrix.

torch.Size([227505, 2])
We keep 1.18e+08/1.29e+10 =  0% of the original kernel matrix.

torch.Size([36450, 2])
We keep 7.25e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([39642, 2])
We keep 6.51e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([14746, 2])
We keep 1.76e+06/5.49e+07 =  3% of the original kernel matrix.

torch.Size([24755, 2])
We keep 2.92e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([19897, 2])
We keep 3.80e+06/1.14e+08 =  3% of the original kernel matrix.

torch.Size([29269, 2])
We keep 3.79e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([42373, 2])
We keep 7.08e+06/4.86e+08 =  1% of the original kernel matrix.

torch.Size([43693, 2])
We keep 6.89e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([9521, 2])
We keep 1.38e+06/1.59e+07 =  8% of the original kernel matrix.

torch.Size([20626, 2])
We keep 1.79e+06/8.77e+07 =  2% of the original kernel matrix.

torch.Size([214961, 2])
We keep 1.94e+08/1.54e+10 =  1% of the original kernel matrix.

torch.Size([98213, 2])
We keep 2.95e+07/2.73e+09 =  1% of the original kernel matrix.

torch.Size([29444, 2])
We keep 8.12e+06/2.93e+08 =  2% of the original kernel matrix.

torch.Size([35815, 2])
We keep 5.68e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([2780, 2])
We keep 5.58e+04/8.74e+05 =  6% of the original kernel matrix.

torch.Size([12479, 2])
We keep 6.76e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([103295, 2])
We keep 4.62e+07/2.71e+09 =  1% of the original kernel matrix.

torch.Size([65801, 2])
We keep 1.35e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([36718, 2])
We keep 6.18e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([41646, 2])
We keep 6.55e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([3467, 2])
We keep 9.68e+04/1.50e+06 =  6% of the original kernel matrix.

torch.Size([13290, 2])
We keep 8.19e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([170020, 2])
We keep 1.42e+08/8.26e+09 =  1% of the original kernel matrix.

torch.Size([86364, 2])
We keep 2.19e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([136888, 2])
We keep 7.60e+07/5.24e+09 =  1% of the original kernel matrix.

torch.Size([76896, 2])
We keep 1.85e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([438288, 2])
We keep 6.77e+08/5.78e+10 =  1% of the original kernel matrix.

torch.Size([141286, 2])
We keep 5.32e+07/5.29e+09 =  1% of the original kernel matrix.

torch.Size([291597, 2])
We keep 4.26e+08/3.05e+10 =  1% of the original kernel matrix.

torch.Size([115396, 2])
We keep 4.00e+07/3.84e+09 =  1% of the original kernel matrix.

torch.Size([230357, 2])
We keep 2.67e+08/1.98e+10 =  1% of the original kernel matrix.

torch.Size([100550, 2])
We keep 3.29e+07/3.09e+09 =  1% of the original kernel matrix.

torch.Size([36366, 2])
We keep 9.85e+06/4.72e+08 =  2% of the original kernel matrix.

torch.Size([39395, 2])
We keep 6.91e+06/4.78e+08 =  1% of the original kernel matrix.

torch.Size([25360, 2])
We keep 5.79e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([33171, 2])
We keep 4.74e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([153991, 2])
We keep 6.22e+07/6.19e+09 =  1% of the original kernel matrix.

torch.Size([82215, 2])
We keep 1.97e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([196820, 2])
We keep 1.12e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([93631, 2])
We keep 2.45e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([12836, 2])
We keep 9.95e+05/3.08e+07 =  3% of the original kernel matrix.

torch.Size([23537, 2])
We keep 2.34e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([66569, 2])
We keep 5.60e+07/1.89e+09 =  2% of the original kernel matrix.

torch.Size([53584, 2])
We keep 1.09e+07/9.56e+08 =  1% of the original kernel matrix.

torch.Size([177520, 2])
We keep 1.72e+08/1.33e+10 =  1% of the original kernel matrix.

torch.Size([87984, 2])
We keep 2.70e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([207124, 2])
We keep 2.20e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([96300, 2])
We keep 2.89e+07/2.69e+09 =  1% of the original kernel matrix.

torch.Size([148886, 2])
We keep 1.10e+08/7.73e+09 =  1% of the original kernel matrix.

torch.Size([80522, 2])
We keep 2.20e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([76465, 2])
We keep 1.03e+08/2.69e+09 =  3% of the original kernel matrix.

torch.Size([55110, 2])
We keep 1.37e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([19065, 2])
We keep 2.42e+06/8.65e+07 =  2% of the original kernel matrix.

torch.Size([28661, 2])
We keep 3.48e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([10164, 2])
We keep 5.12e+05/1.53e+07 =  3% of the original kernel matrix.

torch.Size([21052, 2])
We keep 1.81e+06/8.62e+07 =  2% of the original kernel matrix.

torch.Size([13805, 2])
We keep 2.70e+06/5.01e+07 =  5% of the original kernel matrix.

torch.Size([23908, 2])
We keep 2.81e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([189247, 2])
We keep 8.64e+07/9.53e+09 =  0% of the original kernel matrix.

torch.Size([91649, 2])
We keep 2.37e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([167824, 2])
We keep 7.97e+07/8.44e+09 =  0% of the original kernel matrix.

torch.Size([85896, 2])
We keep 2.24e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([444799, 2])
We keep 7.03e+08/7.16e+10 =  0% of the original kernel matrix.

torch.Size([140554, 2])
We keep 5.84e+07/5.89e+09 =  0% of the original kernel matrix.

torch.Size([216985, 2])
We keep 2.89e+08/1.76e+10 =  1% of the original kernel matrix.

torch.Size([98468, 2])
We keep 3.09e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([99514, 2])
We keep 4.02e+07/2.85e+09 =  1% of the original kernel matrix.

torch.Size([65025, 2])
We keep 1.41e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([21052, 2])
We keep 3.38e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([29867, 2])
We keep 4.53e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([277061, 2])
We keep 2.61e+08/2.30e+10 =  1% of the original kernel matrix.

torch.Size([113166, 2])
We keep 3.51e+07/3.34e+09 =  1% of the original kernel matrix.

torch.Size([6648, 2])
We keep 3.58e+05/6.86e+06 =  5% of the original kernel matrix.

torch.Size([17190, 2])
We keep 1.37e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([15847, 2])
We keep 1.78e+06/5.93e+07 =  3% of the original kernel matrix.

torch.Size([25953, 2])
We keep 3.01e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([258933, 2])
We keep 2.37e+08/2.06e+10 =  1% of the original kernel matrix.

torch.Size([108808, 2])
We keep 3.31e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([29539, 2])
We keep 5.44e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([36592, 2])
We keep 5.28e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([9208, 2])
We keep 5.86e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([20028, 2])
We keep 1.78e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([32887, 2])
We keep 4.86e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([37843, 2])
We keep 5.45e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([571344, 2])
We keep 2.16e+09/1.38e+11 =  1% of the original kernel matrix.

torch.Size([159738, 2])
We keep 7.85e+07/8.16e+09 =  0% of the original kernel matrix.

torch.Size([31670, 2])
We keep 5.65e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([37141, 2])
We keep 5.50e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([199502, 2])
We keep 1.60e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([93925, 2])
We keep 2.64e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([5416, 2])
We keep 1.92e+05/3.88e+06 =  4% of the original kernel matrix.

torch.Size([16140, 2])
We keep 1.13e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([10837, 2])
We keep 6.94e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([21571, 2])
We keep 2.00e+06/9.78e+07 =  2% of the original kernel matrix.

torch.Size([9455, 2])
We keep 5.74e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([20185, 2])
We keep 1.83e+06/8.59e+07 =  2% of the original kernel matrix.

torch.Size([10994, 2])
We keep 1.04e+06/2.32e+07 =  4% of the original kernel matrix.

torch.Size([21488, 2])
We keep 2.08e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([42504, 2])
We keep 1.04e+07/5.32e+08 =  1% of the original kernel matrix.

torch.Size([43870, 2])
We keep 7.15e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([105994, 2])
We keep 5.33e+07/3.00e+09 =  1% of the original kernel matrix.

torch.Size([65914, 2])
We keep 1.44e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([7014, 2])
We keep 3.17e+05/7.61e+06 =  4% of the original kernel matrix.

torch.Size([17945, 2])
We keep 1.43e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([166555, 2])
We keep 1.69e+08/8.65e+09 =  1% of the original kernel matrix.

torch.Size([85347, 2])
We keep 2.32e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([7568, 2])
We keep 5.52e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([18420, 2])
We keep 1.55e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([75334, 2])
We keep 2.46e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([56816, 2])
We keep 1.11e+07/8.71e+08 =  1% of the original kernel matrix.

torch.Size([14145, 2])
We keep 1.75e+06/4.73e+07 =  3% of the original kernel matrix.

torch.Size([24493, 2])
We keep 2.80e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([155577, 2])
We keep 6.60e+07/6.40e+09 =  1% of the original kernel matrix.

torch.Size([82648, 2])
We keep 1.98e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([16469, 2])
We keep 2.75e+06/6.70e+07 =  4% of the original kernel matrix.

torch.Size([26429, 2])
We keep 3.15e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([33514, 2])
We keep 8.84e+06/3.06e+08 =  2% of the original kernel matrix.

torch.Size([38856, 2])
We keep 5.59e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([41959, 2])
We keep 8.15e+06/5.14e+08 =  1% of the original kernel matrix.

torch.Size([43433, 2])
We keep 7.06e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([49118, 2])
We keep 1.66e+07/7.23e+08 =  2% of the original kernel matrix.

torch.Size([46069, 2])
We keep 8.13e+06/5.92e+08 =  1% of the original kernel matrix.

torch.Size([125812, 2])
We keep 5.73e+07/4.72e+09 =  1% of the original kernel matrix.

torch.Size([73600, 2])
We keep 1.77e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([444434, 2])
We keep 1.62e+09/8.23e+10 =  1% of the original kernel matrix.

torch.Size([141397, 2])
We keep 6.32e+07/6.31e+09 =  1% of the original kernel matrix.

torch.Size([13948, 2])
We keep 1.02e+06/3.52e+07 =  2% of the original kernel matrix.

torch.Size([24347, 2])
We keep 2.48e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([8670, 2])
We keep 1.13e+06/1.76e+07 =  6% of the original kernel matrix.

torch.Size([19374, 2])
We keep 1.91e+06/9.24e+07 =  2% of the original kernel matrix.

torch.Size([98079, 2])
We keep 5.95e+07/2.71e+09 =  2% of the original kernel matrix.

torch.Size([64618, 2])
We keep 1.39e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([25585, 2])
We keep 3.42e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([33628, 2])
We keep 4.48e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([909564, 2])
We keep 1.38e+09/2.29e+11 =  0% of the original kernel matrix.

torch.Size([210608, 2])
We keep 9.75e+07/1.05e+10 =  0% of the original kernel matrix.

torch.Size([4346, 2])
We keep 1.41e+05/2.42e+06 =  5% of the original kernel matrix.

torch.Size([14749, 2])
We keep 9.42e+05/3.42e+07 =  2% of the original kernel matrix.

torch.Size([176744, 2])
We keep 9.33e+07/9.12e+09 =  1% of the original kernel matrix.

torch.Size([87987, 2])
We keep 2.33e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([441622, 2])
We keep 3.54e+08/5.14e+10 =  0% of the original kernel matrix.

torch.Size([142325, 2])
We keep 4.96e+07/4.99e+09 =  0% of the original kernel matrix.

torch.Size([9362, 2])
We keep 4.87e+05/1.37e+07 =  3% of the original kernel matrix.

torch.Size([20185, 2])
We keep 1.75e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([157967, 2])
We keep 3.11e+08/1.36e+10 =  2% of the original kernel matrix.

torch.Size([80926, 2])
We keep 2.83e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([24623, 2])
We keep 7.51e+06/3.05e+08 =  2% of the original kernel matrix.

torch.Size([31927, 2])
We keep 5.69e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([1333583, 2])
We keep 3.06e+09/4.29e+11 =  0% of the original kernel matrix.

torch.Size([252831, 2])
We keep 1.31e+08/1.44e+10 =  0% of the original kernel matrix.

torch.Size([19907, 2])
We keep 2.54e+06/8.65e+07 =  2% of the original kernel matrix.

torch.Size([29353, 2])
We keep 3.45e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([25165, 2])
We keep 3.56e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([33131, 2])
We keep 4.44e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([13241, 2])
We keep 2.30e+06/5.82e+07 =  3% of the original kernel matrix.

torch.Size([23379, 2])
We keep 3.02e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([8232, 2])
We keep 4.05e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([18995, 2])
We keep 1.56e+06/6.98e+07 =  2% of the original kernel matrix.

torch.Size([22743, 2])
We keep 3.55e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([31508, 2])
We keep 4.40e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([37612, 2])
We keep 1.05e+07/4.43e+08 =  2% of the original kernel matrix.

torch.Size([40926, 2])
We keep 6.36e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([18141, 2])
We keep 8.96e+06/7.44e+07 = 12% of the original kernel matrix.

torch.Size([27943, 2])
We keep 3.24e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([148609, 2])
We keep 7.47e+07/6.35e+09 =  1% of the original kernel matrix.

torch.Size([80218, 2])
We keep 1.99e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([28395, 2])
We keep 3.89e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([35716, 2])
We keep 5.00e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([6371, 2])
We keep 2.71e+05/5.58e+06 =  4% of the original kernel matrix.

torch.Size([17090, 2])
We keep 1.28e+06/5.20e+07 =  2% of the original kernel matrix.

torch.Size([26130, 2])
We keep 4.03e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([33927, 2])
We keep 4.78e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([391622, 2])
We keep 5.49e+08/5.58e+10 =  0% of the original kernel matrix.

torch.Size([130229, 2])
We keep 5.19e+07/5.20e+09 =  0% of the original kernel matrix.

torch.Size([129316, 2])
We keep 1.02e+08/6.76e+09 =  1% of the original kernel matrix.

torch.Size([73915, 2])
We keep 2.08e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([576570, 2])
We keep 7.85e+08/8.88e+10 =  0% of the original kernel matrix.

torch.Size([162356, 2])
We keep 6.40e+07/6.56e+09 =  0% of the original kernel matrix.

torch.Size([20936, 2])
We keep 2.33e+06/9.31e+07 =  2% of the original kernel matrix.

torch.Size([30233, 2])
We keep 3.50e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([12366, 2])
We keep 1.22e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([23208, 2])
We keep 2.57e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([11394, 2])
We keep 6.48e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([22250, 2])
We keep 1.95e+06/9.67e+07 =  2% of the original kernel matrix.

torch.Size([11776, 2])
We keep 2.31e+06/3.71e+07 =  6% of the original kernel matrix.

torch.Size([22505, 2])
We keep 2.47e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([254041, 2])
We keep 4.40e+08/2.14e+10 =  2% of the original kernel matrix.

torch.Size([108105, 2])
We keep 3.31e+07/3.22e+09 =  1% of the original kernel matrix.

torch.Size([19307, 2])
We keep 1.97e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([28930, 2])
We keep 3.42e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([27797, 2])
We keep 4.26e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([35324, 2])
We keep 4.94e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([56146, 2])
We keep 2.54e+07/9.55e+08 =  2% of the original kernel matrix.

torch.Size([48815, 2])
We keep 8.80e+06/6.80e+08 =  1% of the original kernel matrix.

torch.Size([42228, 2])
We keep 1.59e+07/7.69e+08 =  2% of the original kernel matrix.

torch.Size([41636, 2])
We keep 8.15e+06/6.10e+08 =  1% of the original kernel matrix.

torch.Size([9368, 2])
We keep 6.80e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([20348, 2])
We keep 1.80e+06/8.49e+07 =  2% of the original kernel matrix.

torch.Size([14963, 2])
We keep 1.41e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([25050, 2])
We keep 2.83e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([376946, 2])
We keep 5.53e+08/4.89e+10 =  1% of the original kernel matrix.

torch.Size([130026, 2])
We keep 4.88e+07/4.87e+09 =  1% of the original kernel matrix.

torch.Size([43813, 2])
We keep 1.05e+07/5.63e+08 =  1% of the original kernel matrix.

torch.Size([44181, 2])
We keep 7.29e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([12932, 2])
We keep 1.27e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([23429, 2])
We keep 2.53e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([25135, 2])
We keep 3.60e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([33353, 2])
We keep 4.29e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([201162, 2])
We keep 8.65e+08/1.76e+10 =  4% of the original kernel matrix.

torch.Size([93643, 2])
We keep 3.16e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([161976, 2])
We keep 9.61e+07/7.48e+09 =  1% of the original kernel matrix.

torch.Size([84075, 2])
We keep 2.12e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([455309, 2])
We keep 8.73e+08/8.08e+10 =  1% of the original kernel matrix.

torch.Size([138919, 2])
We keep 6.17e+07/6.26e+09 =  0% of the original kernel matrix.

torch.Size([162921, 2])
We keep 9.32e+07/7.32e+09 =  1% of the original kernel matrix.

torch.Size([84529, 2])
We keep 2.12e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([10829, 2])
We keep 8.90e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([21803, 2])
We keep 2.00e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([55865, 2])
We keep 1.20e+07/8.56e+08 =  1% of the original kernel matrix.

torch.Size([49313, 2])
We keep 8.49e+06/6.44e+08 =  1% of the original kernel matrix.

torch.Size([36987, 2])
We keep 2.45e+07/6.37e+08 =  3% of the original kernel matrix.

torch.Size([38867, 2])
We keep 7.78e+06/5.55e+08 =  1% of the original kernel matrix.

torch.Size([15710, 2])
We keep 1.41e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([25980, 2])
We keep 2.70e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([18344, 2])
We keep 2.31e+06/9.92e+07 =  2% of the original kernel matrix.

torch.Size([27969, 2])
We keep 3.62e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([8985, 2])
We keep 4.26e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([19991, 2])
We keep 1.65e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([46247, 2])
We keep 2.48e+07/6.30e+08 =  3% of the original kernel matrix.

torch.Size([44749, 2])
We keep 7.59e+06/5.53e+08 =  1% of the original kernel matrix.

torch.Size([32809, 2])
We keep 7.17e+06/3.30e+08 =  2% of the original kernel matrix.

torch.Size([37904, 2])
We keep 5.93e+06/4.00e+08 =  1% of the original kernel matrix.

torch.Size([4903, 2])
We keep 3.20e+05/4.42e+06 =  7% of the original kernel matrix.

torch.Size([14896, 2])
We keep 1.19e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([41572, 2])
We keep 7.90e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([43144, 2])
We keep 6.86e+06/4.84e+08 =  1% of the original kernel matrix.

torch.Size([14558, 2])
We keep 1.87e+06/5.22e+07 =  3% of the original kernel matrix.

torch.Size([24863, 2])
We keep 2.81e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([36600, 2])
We keep 2.31e+07/7.23e+08 =  3% of the original kernel matrix.

torch.Size([38636, 2])
We keep 7.92e+06/5.92e+08 =  1% of the original kernel matrix.

torch.Size([17427, 2])
We keep 2.27e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([27357, 2])
We keep 3.44e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([39480, 2])
We keep 9.25e+06/4.51e+08 =  2% of the original kernel matrix.

torch.Size([42108, 2])
We keep 6.77e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([4367, 2])
We keep 2.59e+05/2.98e+06 =  8% of the original kernel matrix.

torch.Size([14594, 2])
We keep 1.01e+06/3.80e+07 =  2% of the original kernel matrix.

torch.Size([24276, 2])
We keep 3.98e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([32297, 2])
We keep 4.31e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([9559, 2])
We keep 6.07e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([20265, 2])
We keep 1.85e+06/8.78e+07 =  2% of the original kernel matrix.

torch.Size([20048, 2])
We keep 1.92e+06/8.64e+07 =  2% of the original kernel matrix.

torch.Size([29515, 2])
We keep 3.46e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([37045, 2])
We keep 5.74e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([41785, 2])
We keep 6.10e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([213760, 2])
We keep 1.75e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([97825, 2])
We keep 2.77e+07/2.56e+09 =  1% of the original kernel matrix.

torch.Size([11988, 2])
We keep 1.00e+06/2.78e+07 =  3% of the original kernel matrix.

torch.Size([22587, 2])
We keep 2.23e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([3608, 2])
We keep 8.82e+04/1.57e+06 =  5% of the original kernel matrix.

torch.Size([13623, 2])
We keep 8.17e+05/2.76e+07 =  2% of the original kernel matrix.

torch.Size([10916, 2])
We keep 9.40e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([21432, 2])
We keep 2.16e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([158057, 2])
We keep 1.41e+08/8.11e+09 =  1% of the original kernel matrix.

torch.Size([83241, 2])
We keep 2.16e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([691582, 2])
We keep 2.02e+09/1.45e+11 =  1% of the original kernel matrix.

torch.Size([180612, 2])
We keep 7.99e+07/8.38e+09 =  0% of the original kernel matrix.

torch.Size([58953, 2])
We keep 1.80e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([49250, 2])
We keep 9.66e+06/7.41e+08 =  1% of the original kernel matrix.

torch.Size([14571, 2])
We keep 1.19e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([24786, 2])
We keep 2.62e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([184122, 2])
We keep 1.68e+08/1.29e+10 =  1% of the original kernel matrix.

torch.Size([88997, 2])
We keep 2.72e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([205824, 2])
We keep 1.12e+08/1.22e+10 =  0% of the original kernel matrix.

torch.Size([95520, 2])
We keep 2.66e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([4567, 2])
We keep 1.44e+05/2.81e+06 =  5% of the original kernel matrix.

torch.Size([14981, 2])
We keep 1.01e+06/3.69e+07 =  2% of the original kernel matrix.

torch.Size([17264, 2])
We keep 2.77e+06/8.18e+07 =  3% of the original kernel matrix.

torch.Size([27343, 2])
We keep 3.36e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([43185, 2])
We keep 6.67e+06/4.94e+08 =  1% of the original kernel matrix.

torch.Size([44419, 2])
We keep 6.82e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([98049, 2])
We keep 2.84e+08/9.37e+09 =  3% of the original kernel matrix.

torch.Size([62350, 2])
We keep 2.33e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([140513, 2])
We keep 7.96e+08/2.00e+10 =  3% of the original kernel matrix.

torch.Size([75482, 2])
We keep 3.31e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([291049, 2])
We keep 2.06e+08/2.44e+10 =  0% of the original kernel matrix.

torch.Size([114919, 2])
We keep 3.55e+07/3.43e+09 =  1% of the original kernel matrix.

torch.Size([20175, 2])
We keep 2.01e+06/8.56e+07 =  2% of the original kernel matrix.

torch.Size([29690, 2])
We keep 3.43e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([8609, 2])
We keep 4.25e+05/1.12e+07 =  3% of the original kernel matrix.

torch.Size([19581, 2])
We keep 1.62e+06/7.38e+07 =  2% of the original kernel matrix.

torch.Size([3878, 2])
We keep 1.10e+05/1.96e+06 =  5% of the original kernel matrix.

torch.Size([14130, 2])
We keep 8.88e+05/3.08e+07 =  2% of the original kernel matrix.

torch.Size([57586, 2])
We keep 2.53e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([49395, 2])
We keep 9.55e+06/7.17e+08 =  1% of the original kernel matrix.

torch.Size([21384, 2])
We keep 2.03e+06/9.41e+07 =  2% of the original kernel matrix.

torch.Size([30640, 2])
We keep 3.56e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([18131, 2])
We keep 2.86e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([27534, 2])
We keep 3.76e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([195355, 2])
We keep 1.46e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([92753, 2])
We keep 2.57e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([794298, 2])
We keep 1.14e+09/1.70e+11 =  0% of the original kernel matrix.

torch.Size([195709, 2])
We keep 8.61e+07/9.07e+09 =  0% of the original kernel matrix.

torch.Size([17176, 2])
We keep 1.93e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([27006, 2])
We keep 3.17e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([6701, 2])
We keep 2.68e+05/6.02e+06 =  4% of the original kernel matrix.

torch.Size([17632, 2])
We keep 1.31e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([21088, 2])
We keep 2.38e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([30649, 2])
We keep 3.87e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([57323, 2])
We keep 1.38e+07/9.05e+08 =  1% of the original kernel matrix.

torch.Size([50107, 2])
We keep 8.80e+06/6.62e+08 =  1% of the original kernel matrix.

torch.Size([43850, 2])
We keep 1.12e+07/5.24e+08 =  2% of the original kernel matrix.

torch.Size([43788, 2])
We keep 7.11e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([2684, 2])
We keep 5.49e+04/8.52e+05 =  6% of the original kernel matrix.

torch.Size([12380, 2])
We keep 6.70e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([8662, 2])
We keep 4.98e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([19466, 2])
We keep 1.66e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([34662, 2])
We keep 8.55e+06/3.27e+08 =  2% of the original kernel matrix.

torch.Size([38934, 2])
We keep 5.79e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([5808, 2])
We keep 2.37e+05/4.88e+06 =  4% of the original kernel matrix.

torch.Size([16507, 2])
We keep 1.20e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([112232, 2])
We keep 1.55e+08/6.66e+09 =  2% of the original kernel matrix.

torch.Size([67282, 2])
We keep 2.06e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([9192, 2])
We keep 5.65e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([19978, 2])
We keep 1.76e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([32436, 2])
We keep 4.35e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([38468, 2])
We keep 5.42e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([25235, 2])
We keep 4.05e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([33284, 2])
We keep 4.50e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([12189, 2])
We keep 7.19e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([22927, 2])
We keep 2.10e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([9088, 2])
We keep 4.97e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([19899, 2])
We keep 1.73e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([248218, 2])
We keep 1.57e+08/1.68e+10 =  0% of the original kernel matrix.

torch.Size([106388, 2])
We keep 3.03e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([714317, 2])
We keep 9.76e+08/1.21e+11 =  0% of the original kernel matrix.

torch.Size([183193, 2])
We keep 7.35e+07/7.66e+09 =  0% of the original kernel matrix.

torch.Size([181617, 2])
We keep 2.10e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([89728, 2])
We keep 2.55e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([85855, 2])
We keep 7.22e+07/2.91e+09 =  2% of the original kernel matrix.

torch.Size([59369, 2])
We keep 1.46e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([30584, 2])
We keep 4.10e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([37120, 2])
We keep 5.10e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([23334, 2])
We keep 3.31e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([31856, 2])
We keep 3.99e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([30652, 2])
We keep 5.47e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([36779, 2])
We keep 5.58e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([203841, 2])
We keep 1.21e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([94997, 2])
We keep 2.64e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([51432, 2])
We keep 2.48e+07/9.55e+08 =  2% of the original kernel matrix.

torch.Size([46775, 2])
We keep 8.94e+06/6.80e+08 =  1% of the original kernel matrix.

torch.Size([28531, 2])
We keep 5.51e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([35537, 2])
We keep 5.05e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([30500, 2])
We keep 4.55e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([36948, 2])
We keep 5.25e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([2026059, 2])
We keep 5.84e+09/1.00e+12 =  0% of the original kernel matrix.

torch.Size([318810, 2])
We keep 1.96e+08/2.20e+10 =  0% of the original kernel matrix.

torch.Size([20887, 2])
We keep 2.34e+06/9.72e+07 =  2% of the original kernel matrix.

torch.Size([30289, 2])
We keep 3.66e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([89658, 2])
We keep 3.48e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([60741, 2])
We keep 1.34e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([275694, 2])
We keep 3.76e+08/2.64e+10 =  1% of the original kernel matrix.

torch.Size([111481, 2])
We keep 3.65e+07/3.58e+09 =  1% of the original kernel matrix.

torch.Size([78672, 2])
We keep 2.56e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([58003, 2])
We keep 1.10e+07/8.81e+08 =  1% of the original kernel matrix.

torch.Size([42439, 2])
We keep 1.07e+07/5.27e+08 =  2% of the original kernel matrix.

torch.Size([43227, 2])
We keep 7.12e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([706107, 2])
We keep 1.68e+09/1.50e+11 =  1% of the original kernel matrix.

torch.Size([181946, 2])
We keep 8.21e+07/8.53e+09 =  0% of the original kernel matrix.

torch.Size([129803, 2])
We keep 5.16e+07/4.95e+09 =  1% of the original kernel matrix.

torch.Size([74831, 2])
We keep 1.79e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([85964, 2])
We keep 3.65e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([60170, 2])
We keep 1.25e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([10274, 2])
We keep 8.54e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([21186, 2])
We keep 1.94e+06/9.44e+07 =  2% of the original kernel matrix.

torch.Size([46898, 2])
We keep 1.58e+07/7.05e+08 =  2% of the original kernel matrix.

torch.Size([45130, 2])
We keep 7.97e+06/5.84e+08 =  1% of the original kernel matrix.

torch.Size([10601, 2])
We keep 9.48e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([21608, 2])
We keep 2.01e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([250822, 2])
We keep 3.53e+08/1.85e+10 =  1% of the original kernel matrix.

torch.Size([107099, 2])
We keep 3.22e+07/2.99e+09 =  1% of the original kernel matrix.

torch.Size([2099756, 2])
We keep 6.28e+09/1.00e+12 =  0% of the original kernel matrix.

torch.Size([326671, 2])
We keep 1.96e+08/2.20e+10 =  0% of the original kernel matrix.

torch.Size([186670, 2])
We keep 3.17e+08/1.18e+10 =  2% of the original kernel matrix.

torch.Size([90618, 2])
We keep 2.63e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([250431, 2])
We keep 1.92e+08/2.10e+10 =  0% of the original kernel matrix.

torch.Size([106757, 2])
We keep 3.39e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([69261, 2])
We keep 8.02e+07/1.53e+09 =  5% of the original kernel matrix.

torch.Size([54170, 2])
We keep 1.11e+07/8.62e+08 =  1% of the original kernel matrix.

torch.Size([63634, 2])
We keep 4.35e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([50699, 2])
We keep 1.16e+07/9.13e+08 =  1% of the original kernel matrix.

torch.Size([43442, 2])
We keep 1.23e+07/6.61e+08 =  1% of the original kernel matrix.

torch.Size([43553, 2])
We keep 7.84e+06/5.66e+08 =  1% of the original kernel matrix.

torch.Size([15704, 2])
We keep 2.19e+06/6.48e+07 =  3% of the original kernel matrix.

torch.Size([25660, 2])
We keep 3.11e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([181367, 2])
We keep 1.01e+09/2.85e+10 =  3% of the original kernel matrix.

torch.Size([86297, 2])
We keep 3.80e+07/3.72e+09 =  1% of the original kernel matrix.

torch.Size([623637, 2])
We keep 6.08e+08/9.34e+10 =  0% of the original kernel matrix.

torch.Size([169977, 2])
We keep 6.48e+07/6.73e+09 =  0% of the original kernel matrix.

torch.Size([75460, 2])
We keep 4.14e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([53151, 2])
We keep 1.33e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([787609, 2])
We keep 5.05e+09/2.69e+11 =  1% of the original kernel matrix.

torch.Size([186449, 2])
We keep 1.06e+08/1.14e+10 =  0% of the original kernel matrix.

torch.Size([280952, 2])
We keep 6.21e+08/3.85e+10 =  1% of the original kernel matrix.

torch.Size([110639, 2])
We keep 4.44e+07/4.32e+09 =  1% of the original kernel matrix.

torch.Size([55757, 2])
We keep 1.21e+07/8.69e+08 =  1% of the original kernel matrix.

torch.Size([49208, 2])
We keep 8.57e+06/6.49e+08 =  1% of the original kernel matrix.

torch.Size([7023, 2])
We keep 3.84e+05/8.04e+06 =  4% of the original kernel matrix.

torch.Size([17706, 2])
We keep 1.45e+06/6.24e+07 =  2% of the original kernel matrix.

torch.Size([87357, 2])
We keep 5.29e+07/2.45e+09 =  2% of the original kernel matrix.

torch.Size([61077, 2])
We keep 1.35e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([153973, 2])
We keep 6.71e+07/6.74e+09 =  0% of the original kernel matrix.

torch.Size([82056, 2])
We keep 2.06e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([10700, 2])
We keep 8.77e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([21212, 2])
We keep 2.10e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([846060, 2])
We keep 1.29e+09/2.05e+11 =  0% of the original kernel matrix.

torch.Size([204459, 2])
We keep 9.35e+07/9.96e+09 =  0% of the original kernel matrix.

torch.Size([28693, 2])
We keep 9.95e+06/2.55e+08 =  3% of the original kernel matrix.

torch.Size([35418, 2])
We keep 4.82e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([47479, 2])
We keep 1.46e+07/7.45e+08 =  1% of the original kernel matrix.

torch.Size([44963, 2])
We keep 8.15e+06/6.00e+08 =  1% of the original kernel matrix.

torch.Size([9850, 2])
We keep 5.93e+05/1.66e+07 =  3% of the original kernel matrix.

torch.Size([20819, 2])
We keep 1.79e+06/8.97e+07 =  1% of the original kernel matrix.

torch.Size([15866, 2])
We keep 1.31e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([25947, 2])
We keep 2.81e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([5579, 2])
We keep 1.95e+05/4.03e+06 =  4% of the original kernel matrix.

torch.Size([16231, 2])
We keep 1.13e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([10492, 2])
We keep 1.59e+06/2.35e+07 =  6% of the original kernel matrix.

torch.Size([21153, 2])
We keep 2.11e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([44995, 2])
We keep 1.48e+07/5.74e+08 =  2% of the original kernel matrix.

torch.Size([44424, 2])
We keep 7.39e+06/5.27e+08 =  1% of the original kernel matrix.

torch.Size([28933, 2])
We keep 5.54e+06/2.47e+08 =  2% of the original kernel matrix.

torch.Size([35834, 2])
We keep 5.25e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([6728368, 2])
We keep 5.78e+10/1.01e+13 =  0% of the original kernel matrix.

torch.Size([579091, 2])
We keep 5.78e+08/7.00e+10 =  0% of the original kernel matrix.

torch.Size([16256, 2])
We keep 2.62e+06/8.03e+07 =  3% of the original kernel matrix.

torch.Size([26138, 2])
We keep 3.41e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([139215, 2])
We keep 4.75e+07/4.87e+09 =  0% of the original kernel matrix.

torch.Size([77290, 2])
We keep 1.75e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([14215, 2])
We keep 1.20e+06/4.06e+07 =  2% of the original kernel matrix.

torch.Size([24667, 2])
We keep 2.61e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([25821, 2])
We keep 3.66e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([33856, 2])
We keep 4.56e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([17822, 2])
We keep 2.24e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([27381, 2])
We keep 3.35e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([246239, 2])
We keep 3.52e+08/2.68e+10 =  1% of the original kernel matrix.

torch.Size([104064, 2])
We keep 3.73e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([44274, 2])
We keep 6.76e+07/1.06e+09 =  6% of the original kernel matrix.

torch.Size([42565, 2])
We keep 9.37e+06/7.17e+08 =  1% of the original kernel matrix.

torch.Size([1420550, 2])
We keep 2.75e+09/4.63e+11 =  0% of the original kernel matrix.

torch.Size([262298, 2])
We keep 1.36e+08/1.50e+10 =  0% of the original kernel matrix.

torch.Size([12841, 2])
We keep 9.75e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([23410, 2])
We keep 2.27e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([280250, 2])
We keep 2.19e+08/2.23e+10 =  0% of the original kernel matrix.

torch.Size([113570, 2])
We keep 3.42e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([46137, 2])
We keep 2.68e+07/8.60e+08 =  3% of the original kernel matrix.

torch.Size([44202, 2])
We keep 8.63e+06/6.45e+08 =  1% of the original kernel matrix.

torch.Size([198652, 2])
We keep 1.31e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([93713, 2])
We keep 2.68e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([252075, 2])
We keep 4.57e+08/3.02e+10 =  1% of the original kernel matrix.

torch.Size([105920, 2])
We keep 4.01e+07/3.83e+09 =  1% of the original kernel matrix.

torch.Size([24815, 2])
We keep 4.29e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([32948, 2])
We keep 4.31e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([17282, 2])
We keep 1.76e+06/6.42e+07 =  2% of the original kernel matrix.

torch.Size([27308, 2])
We keep 3.10e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([160773, 2])
We keep 7.87e+07/7.75e+09 =  1% of the original kernel matrix.

torch.Size([84199, 2])
We keep 2.19e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([35263, 2])
We keep 2.84e+07/3.90e+08 =  7% of the original kernel matrix.

torch.Size([39902, 2])
We keep 6.34e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([52178, 2])
We keep 3.86e+07/9.26e+08 =  4% of the original kernel matrix.

torch.Size([46755, 2])
We keep 8.85e+06/6.70e+08 =  1% of the original kernel matrix.

torch.Size([195470, 2])
We keep 2.28e+08/1.12e+10 =  2% of the original kernel matrix.

torch.Size([92728, 2])
We keep 2.57e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([16707, 2])
We keep 2.36e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([26700, 2])
We keep 3.37e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([130717, 2])
We keep 1.34e+08/6.32e+09 =  2% of the original kernel matrix.

torch.Size([74499, 2])
We keep 2.03e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([27951, 2])
We keep 9.63e+06/2.67e+08 =  3% of the original kernel matrix.

torch.Size([34632, 2])
We keep 5.52e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([229174, 2])
We keep 2.57e+08/1.87e+10 =  1% of the original kernel matrix.

torch.Size([101215, 2])
We keep 3.17e+07/3.01e+09 =  1% of the original kernel matrix.

torch.Size([571792, 2])
We keep 9.04e+08/9.03e+10 =  1% of the original kernel matrix.

torch.Size([160255, 2])
We keep 6.39e+07/6.61e+09 =  0% of the original kernel matrix.

torch.Size([44497, 2])
We keep 9.31e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([44524, 2])
We keep 7.18e+06/5.14e+08 =  1% of the original kernel matrix.

torch.Size([15567, 2])
We keep 1.59e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([25768, 2])
We keep 2.95e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([157129, 2])
We keep 1.89e+08/9.27e+09 =  2% of the original kernel matrix.

torch.Size([82016, 2])
We keep 2.36e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([78293, 2])
We keep 3.79e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([56539, 2])
We keep 1.27e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([218687, 2])
We keep 1.53e+09/4.05e+10 =  3% of the original kernel matrix.

torch.Size([96056, 2])
We keep 4.56e+07/4.43e+09 =  1% of the original kernel matrix.

torch.Size([48767, 2])
We keep 2.45e+07/7.60e+08 =  3% of the original kernel matrix.

torch.Size([45609, 2])
We keep 8.24e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([53988, 2])
We keep 2.47e+07/1.08e+09 =  2% of the original kernel matrix.

torch.Size([47279, 2])
We keep 9.45e+06/7.23e+08 =  1% of the original kernel matrix.

torch.Size([68991, 2])
We keep 4.72e+07/1.67e+09 =  2% of the original kernel matrix.

torch.Size([54056, 2])
We keep 1.12e+07/9.00e+08 =  1% of the original kernel matrix.

torch.Size([92222, 2])
We keep 9.90e+07/4.27e+09 =  2% of the original kernel matrix.

torch.Size([60049, 2])
We keep 1.71e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([178747, 2])
We keep 2.36e+08/1.23e+10 =  1% of the original kernel matrix.

torch.Size([88023, 2])
We keep 2.67e+07/2.44e+09 =  1% of the original kernel matrix.

torch.Size([36025, 2])
We keep 5.89e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([40829, 2])
We keep 6.08e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([12609, 2])
We keep 9.07e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([23138, 2])
We keep 2.25e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([13065, 2])
We keep 1.02e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([23489, 2])
We keep 2.39e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([64251, 2])
We keep 4.82e+07/1.92e+09 =  2% of the original kernel matrix.

torch.Size([50792, 2])
We keep 1.21e+07/9.65e+08 =  1% of the original kernel matrix.

torch.Size([93125, 2])
We keep 8.10e+07/2.72e+09 =  2% of the original kernel matrix.

torch.Size([62112, 2])
We keep 1.38e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([110892, 2])
We keep 4.14e+07/3.02e+09 =  1% of the original kernel matrix.

torch.Size([68442, 2])
We keep 1.45e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([21744, 2])
We keep 2.32e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([30919, 2])
We keep 3.78e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([36253, 2])
We keep 5.98e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([40297, 2])
We keep 6.28e+06/4.38e+08 =  1% of the original kernel matrix.

torch.Size([59087, 2])
We keep 2.49e+07/9.90e+08 =  2% of the original kernel matrix.

torch.Size([50384, 2])
We keep 9.12e+06/6.92e+08 =  1% of the original kernel matrix.

torch.Size([63800, 2])
We keep 1.75e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([52776, 2])
We keep 9.85e+06/7.58e+08 =  1% of the original kernel matrix.

torch.Size([48473, 2])
We keep 1.94e+07/7.91e+08 =  2% of the original kernel matrix.

torch.Size([46006, 2])
We keep 8.37e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([124616, 2])
We keep 4.51e+07/3.88e+09 =  1% of the original kernel matrix.

torch.Size([72690, 2])
We keep 1.62e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([19737, 2])
We keep 5.05e+06/1.38e+08 =  3% of the original kernel matrix.

torch.Size([28852, 2])
We keep 4.14e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([55436, 2])
We keep 1.26e+07/8.20e+08 =  1% of the original kernel matrix.

torch.Size([49200, 2])
We keep 8.38e+06/6.30e+08 =  1% of the original kernel matrix.

torch.Size([4361, 2])
We keep 1.54e+05/2.76e+06 =  5% of the original kernel matrix.

torch.Size([14658, 2])
We keep 9.90e+05/3.65e+07 =  2% of the original kernel matrix.

torch.Size([14464, 2])
We keep 1.33e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([24847, 2])
We keep 2.83e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([147918, 2])
We keep 5.89e+07/6.02e+09 =  0% of the original kernel matrix.

torch.Size([80352, 2])
We keep 1.94e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([30395, 2])
We keep 7.15e+06/3.08e+08 =  2% of the original kernel matrix.

torch.Size([36069, 2])
We keep 5.70e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([105552, 2])
We keep 2.98e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([66610, 2])
We keep 1.37e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([5336, 2])
We keep 1.65e+05/3.54e+06 =  4% of the original kernel matrix.

torch.Size([16099, 2])
We keep 1.07e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([13553, 2])
We keep 1.88e+06/3.56e+07 =  5% of the original kernel matrix.

torch.Size([24200, 2])
We keep 2.51e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([42950, 2])
We keep 1.35e+07/5.50e+08 =  2% of the original kernel matrix.

torch.Size([43779, 2])
We keep 7.13e+06/5.16e+08 =  1% of the original kernel matrix.

torch.Size([85321, 2])
We keep 2.64e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([59748, 2])
We keep 1.19e+07/9.58e+08 =  1% of the original kernel matrix.

torch.Size([39392, 2])
We keep 7.06e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([42489, 2])
We keep 6.56e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([8274, 2])
We keep 4.41e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([19077, 2])
We keep 1.62e+06/7.26e+07 =  2% of the original kernel matrix.

torch.Size([13444, 2])
We keep 1.48e+06/4.63e+07 =  3% of the original kernel matrix.

torch.Size([23669, 2])
We keep 2.74e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([63426, 2])
We keep 2.16e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([52234, 2])
We keep 9.46e+06/7.27e+08 =  1% of the original kernel matrix.

torch.Size([48899, 2])
We keep 1.56e+07/6.38e+08 =  2% of the original kernel matrix.

torch.Size([46348, 2])
We keep 7.57e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([164227, 2])
We keep 1.29e+08/7.79e+09 =  1% of the original kernel matrix.

torch.Size([84789, 2])
We keep 2.20e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([9175, 2])
We keep 5.49e+05/1.46e+07 =  3% of the original kernel matrix.

torch.Size([20172, 2])
We keep 1.77e+06/8.40e+07 =  2% of the original kernel matrix.

torch.Size([15618, 2])
We keep 1.94e+06/6.23e+07 =  3% of the original kernel matrix.

torch.Size([25421, 2])
We keep 3.08e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([31432, 2])
We keep 8.92e+06/3.93e+08 =  2% of the original kernel matrix.

torch.Size([34603, 2])
We keep 5.92e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([18464, 2])
We keep 2.26e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([28125, 2])
We keep 3.31e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([342874, 2])
We keep 4.66e+08/3.73e+10 =  1% of the original kernel matrix.

torch.Size([125291, 2])
We keep 4.29e+07/4.25e+09 =  1% of the original kernel matrix.

torch.Size([11214, 2])
We keep 6.33e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([22136, 2])
We keep 1.99e+06/9.98e+07 =  1% of the original kernel matrix.

torch.Size([71438, 2])
We keep 3.20e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([54787, 2])
We keep 1.14e+07/8.99e+08 =  1% of the original kernel matrix.

torch.Size([235045, 2])
We keep 1.77e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([103099, 2])
We keep 3.01e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([44215, 2])
We keep 9.64e+06/5.14e+08 =  1% of the original kernel matrix.

torch.Size([44719, 2])
We keep 6.98e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([221872, 2])
We keep 1.92e+08/1.59e+10 =  1% of the original kernel matrix.

torch.Size([100126, 2])
We keep 3.01e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([201837, 2])
We keep 1.09e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([94871, 2])
We keep 2.49e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([5175, 2])
We keep 1.69e+05/3.55e+06 =  4% of the original kernel matrix.

torch.Size([15778, 2])
We keep 1.07e+06/4.15e+07 =  2% of the original kernel matrix.

torch.Size([60907, 2])
We keep 1.47e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([51554, 2])
We keep 9.39e+06/7.09e+08 =  1% of the original kernel matrix.

torch.Size([23379, 2])
We keep 2.31e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([32124, 2])
We keep 3.88e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([132362, 2])
We keep 1.13e+08/5.42e+09 =  2% of the original kernel matrix.

torch.Size([75527, 2])
We keep 1.89e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([135707, 2])
We keep 1.16e+08/6.46e+09 =  1% of the original kernel matrix.

torch.Size([76633, 2])
We keep 2.04e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([31988, 2])
We keep 8.72e+06/3.27e+08 =  2% of the original kernel matrix.

torch.Size([37287, 2])
We keep 5.88e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([81994, 2])
We keep 2.19e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([59110, 2])
We keep 1.13e+07/9.03e+08 =  1% of the original kernel matrix.

torch.Size([27854, 2])
We keep 1.21e+07/2.23e+08 =  5% of the original kernel matrix.

torch.Size([34832, 2])
We keep 4.98e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([29630, 2])
We keep 1.57e+07/5.48e+08 =  2% of the original kernel matrix.

torch.Size([34322, 2])
We keep 7.14e+06/5.15e+08 =  1% of the original kernel matrix.

torch.Size([9749, 2])
We keep 5.09e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([20753, 2])
We keep 1.77e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([18923, 2])
We keep 2.25e+06/8.70e+07 =  2% of the original kernel matrix.

torch.Size([28632, 2])
We keep 3.50e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([220449, 2])
We keep 2.62e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([99560, 2])
We keep 2.79e+07/2.56e+09 =  1% of the original kernel matrix.

torch.Size([660236, 2])
We keep 1.22e+09/1.34e+11 =  0% of the original kernel matrix.

torch.Size([174847, 2])
We keep 7.73e+07/8.06e+09 =  0% of the original kernel matrix.

torch.Size([33266, 2])
We keep 7.66e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([35428, 2])
We keep 5.92e+06/4.37e+08 =  1% of the original kernel matrix.

torch.Size([28910, 2])
We keep 4.36e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([35994, 2])
We keep 5.09e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([43690, 2])
We keep 1.01e+07/5.62e+08 =  1% of the original kernel matrix.

torch.Size([44098, 2])
We keep 7.34e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([134456, 2])
We keep 9.86e+07/6.06e+09 =  1% of the original kernel matrix.

torch.Size([76233, 2])
We keep 1.94e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([43928, 2])
We keep 8.19e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([44696, 2])
We keep 7.07e+06/4.96e+08 =  1% of the original kernel matrix.

torch.Size([12957, 2])
We keep 9.09e+05/3.12e+07 =  2% of the original kernel matrix.

torch.Size([23716, 2])
We keep 2.35e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([61505, 2])
We keep 2.10e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([51486, 2])
We keep 9.74e+06/7.46e+08 =  1% of the original kernel matrix.

torch.Size([232935, 2])
We keep 1.44e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([102504, 2])
We keep 2.82e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([11679, 2])
We keep 1.43e+06/3.29e+07 =  4% of the original kernel matrix.

torch.Size([22161, 2])
We keep 2.40e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([80596, 2])
We keep 4.67e+07/1.87e+09 =  2% of the original kernel matrix.

torch.Size([58299, 2])
We keep 1.19e+07/9.52e+08 =  1% of the original kernel matrix.

torch.Size([11646, 2])
We keep 8.09e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([22581, 2])
We keep 1.98e+06/1.01e+08 =  1% of the original kernel matrix.

time for making ranges is 5.623283624649048
Sorting X and nu_X
time for sorting X is 0.08770298957824707
Sorting Z and nu_Z
time for sorting Z is 0.0002646446228027344
Starting Optim
sum tnu_Z before tensor(30034172., device='cuda:0')
c= tensor(1531.3965, device='cuda:0')
c= tensor(140575.8281, device='cuda:0')
c= tensor(143714.4688, device='cuda:0')
c= tensor(284626.3438, device='cuda:0')
c= tensor(501241., device='cuda:0')
c= tensor(708740.3125, device='cuda:0')
c= tensor(1352724.5000, device='cuda:0')
c= tensor(1680921.5000, device='cuda:0')
c= tensor(2056028.5000, device='cuda:0')
c= tensor(5840883.5000, device='cuda:0')
c= tensor(5870270., device='cuda:0')
c= tensor(9377203., device='cuda:0')
c= tensor(9391449., device='cuda:0')
c= tensor(38370248., device='cuda:0')
c= tensor(38567940., device='cuda:0')
c= tensor(38794696., device='cuda:0')
c= tensor(39996708., device='cuda:0')
c= tensor(40397040., device='cuda:0')
c= tensor(45426164., device='cuda:0')
c= tensor(48855416., device='cuda:0')
c= tensor(49041904., device='cuda:0')
c= tensor(57322780., device='cuda:0')
c= tensor(57355876., device='cuda:0')
c= tensor(58837672., device='cuda:0')
c= tensor(58846644., device='cuda:0')
c= tensor(59773380., device='cuda:0')
c= tensor(63826832., device='cuda:0')
c= tensor(63858664., device='cuda:0')
c= tensor(69202248., device='cuda:0')
c= tensor(2.4602e+08, device='cuda:0')
c= tensor(2.4606e+08, device='cuda:0')
c= tensor(4.8447e+08, device='cuda:0')
c= tensor(4.8463e+08, device='cuda:0')
c= tensor(4.8466e+08, device='cuda:0')
c= tensor(4.8470e+08, device='cuda:0')
c= tensor(4.9962e+08, device='cuda:0')
c= tensor(5.0231e+08, device='cuda:0')
c= tensor(5.0231e+08, device='cuda:0')
c= tensor(5.0232e+08, device='cuda:0')
c= tensor(5.0233e+08, device='cuda:0')
c= tensor(5.0233e+08, device='cuda:0')
c= tensor(5.0234e+08, device='cuda:0')
c= tensor(5.0234e+08, device='cuda:0')
c= tensor(5.0234e+08, device='cuda:0')
c= tensor(5.0234e+08, device='cuda:0')
c= tensor(5.0235e+08, device='cuda:0')
c= tensor(5.0235e+08, device='cuda:0')
c= tensor(5.0236e+08, device='cuda:0')
c= tensor(5.0236e+08, device='cuda:0')
c= tensor(5.0242e+08, device='cuda:0')
c= tensor(5.0253e+08, device='cuda:0')
c= tensor(5.0253e+08, device='cuda:0')
c= tensor(5.0254e+08, device='cuda:0')
c= tensor(5.0255e+08, device='cuda:0')
c= tensor(5.0256e+08, device='cuda:0')
c= tensor(5.0259e+08, device='cuda:0')
c= tensor(5.0259e+08, device='cuda:0')
c= tensor(5.0260e+08, device='cuda:0')
c= tensor(5.0260e+08, device='cuda:0')
c= tensor(5.0261e+08, device='cuda:0')
c= tensor(5.0264e+08, device='cuda:0')
c= tensor(5.0264e+08, device='cuda:0')
c= tensor(5.0266e+08, device='cuda:0')
c= tensor(5.0268e+08, device='cuda:0')
c= tensor(5.0268e+08, device='cuda:0')
c= tensor(5.0269e+08, device='cuda:0')
c= tensor(5.0269e+08, device='cuda:0')
c= tensor(5.0270e+08, device='cuda:0')
c= tensor(5.0270e+08, device='cuda:0')
c= tensor(5.0271e+08, device='cuda:0')
c= tensor(5.0273e+08, device='cuda:0')
c= tensor(5.0274e+08, device='cuda:0')
c= tensor(5.0274e+08, device='cuda:0')
c= tensor(5.0275e+08, device='cuda:0')
c= tensor(5.0276e+08, device='cuda:0')
c= tensor(5.0281e+08, device='cuda:0')
c= tensor(5.0281e+08, device='cuda:0')
c= tensor(5.0281e+08, device='cuda:0')
c= tensor(5.0281e+08, device='cuda:0')
c= tensor(5.0283e+08, device='cuda:0')
c= tensor(5.0284e+08, device='cuda:0')
c= tensor(5.0284e+08, device='cuda:0')
c= tensor(5.0285e+08, device='cuda:0')
c= tensor(5.0285e+08, device='cuda:0')
c= tensor(5.0285e+08, device='cuda:0')
c= tensor(5.0285e+08, device='cuda:0')
c= tensor(5.0286e+08, device='cuda:0')
c= tensor(5.0286e+08, device='cuda:0')
c= tensor(5.0287e+08, device='cuda:0')
c= tensor(5.0287e+08, device='cuda:0')
c= tensor(5.0288e+08, device='cuda:0')
c= tensor(5.0288e+08, device='cuda:0')
c= tensor(5.0288e+08, device='cuda:0')
c= tensor(5.0290e+08, device='cuda:0')
c= tensor(5.0291e+08, device='cuda:0')
c= tensor(5.0292e+08, device='cuda:0')
c= tensor(5.0293e+08, device='cuda:0')
c= tensor(5.0297e+08, device='cuda:0')
c= tensor(5.0299e+08, device='cuda:0')
c= tensor(5.0299e+08, device='cuda:0')
c= tensor(5.0305e+08, device='cuda:0')
c= tensor(5.0307e+08, device='cuda:0')
c= tensor(5.0308e+08, device='cuda:0')
c= tensor(5.0308e+08, device='cuda:0')
c= tensor(5.0309e+08, device='cuda:0')
c= tensor(5.0309e+08, device='cuda:0')
c= tensor(5.0310e+08, device='cuda:0')
c= tensor(5.0310e+08, device='cuda:0')
c= tensor(5.0311e+08, device='cuda:0')
c= tensor(5.0311e+08, device='cuda:0')
c= tensor(5.0311e+08, device='cuda:0')
c= tensor(5.0312e+08, device='cuda:0')
c= tensor(5.0312e+08, device='cuda:0')
c= tensor(5.0312e+08, device='cuda:0')
c= tensor(5.0314e+08, device='cuda:0')
c= tensor(5.0314e+08, device='cuda:0')
c= tensor(5.0315e+08, device='cuda:0')
c= tensor(5.0315e+08, device='cuda:0')
c= tensor(5.0317e+08, device='cuda:0')
c= tensor(5.0318e+08, device='cuda:0')
c= tensor(5.0320e+08, device='cuda:0')
c= tensor(5.0320e+08, device='cuda:0')
c= tensor(5.0320e+08, device='cuda:0')
c= tensor(5.0320e+08, device='cuda:0')
c= tensor(5.0321e+08, device='cuda:0')
c= tensor(5.0321e+08, device='cuda:0')
c= tensor(5.0321e+08, device='cuda:0')
c= tensor(5.0322e+08, device='cuda:0')
c= tensor(5.0324e+08, device='cuda:0')
c= tensor(5.0325e+08, device='cuda:0')
c= tensor(5.0325e+08, device='cuda:0')
c= tensor(5.0326e+08, device='cuda:0')
c= tensor(5.0326e+08, device='cuda:0')
c= tensor(5.0327e+08, device='cuda:0')
c= tensor(5.0327e+08, device='cuda:0')
c= tensor(5.0327e+08, device='cuda:0')
c= tensor(5.0328e+08, device='cuda:0')
c= tensor(5.0328e+08, device='cuda:0')
c= tensor(5.0328e+08, device='cuda:0')
c= tensor(5.0328e+08, device='cuda:0')
c= tensor(5.0329e+08, device='cuda:0')
c= tensor(5.0329e+08, device='cuda:0')
c= tensor(5.0331e+08, device='cuda:0')
c= tensor(5.0337e+08, device='cuda:0')
c= tensor(5.0338e+08, device='cuda:0')
c= tensor(5.0338e+08, device='cuda:0')
c= tensor(5.0338e+08, device='cuda:0')
c= tensor(5.0338e+08, device='cuda:0')
c= tensor(5.0339e+08, device='cuda:0')
c= tensor(5.0339e+08, device='cuda:0')
c= tensor(5.0339e+08, device='cuda:0')
c= tensor(5.0341e+08, device='cuda:0')
c= tensor(5.0341e+08, device='cuda:0')
c= tensor(5.0348e+08, device='cuda:0')
c= tensor(5.0349e+08, device='cuda:0')
c= tensor(5.0362e+08, device='cuda:0')
c= tensor(5.0363e+08, device='cuda:0')
c= tensor(5.0363e+08, device='cuda:0')
c= tensor(5.0364e+08, device='cuda:0')
c= tensor(5.0364e+08, device='cuda:0')
c= tensor(5.0367e+08, device='cuda:0')
c= tensor(5.0367e+08, device='cuda:0')
c= tensor(5.0369e+08, device='cuda:0')
c= tensor(5.0369e+08, device='cuda:0')
c= tensor(5.0371e+08, device='cuda:0')
c= tensor(5.0371e+08, device='cuda:0')
c= tensor(5.0371e+08, device='cuda:0')
c= tensor(5.0372e+08, device='cuda:0')
c= tensor(5.0372e+08, device='cuda:0')
c= tensor(5.0372e+08, device='cuda:0')
c= tensor(5.0373e+08, device='cuda:0')
c= tensor(5.0373e+08, device='cuda:0')
c= tensor(5.0374e+08, device='cuda:0')
c= tensor(5.0374e+08, device='cuda:0')
c= tensor(5.0375e+08, device='cuda:0')
c= tensor(5.0376e+08, device='cuda:0')
c= tensor(5.0376e+08, device='cuda:0')
c= tensor(5.0377e+08, device='cuda:0')
c= tensor(5.0379e+08, device='cuda:0')
c= tensor(5.0379e+08, device='cuda:0')
c= tensor(5.0381e+08, device='cuda:0')
c= tensor(5.0381e+08, device='cuda:0')
c= tensor(5.0382e+08, device='cuda:0')
c= tensor(5.0382e+08, device='cuda:0')
c= tensor(5.0383e+08, device='cuda:0')
c= tensor(5.0384e+08, device='cuda:0')
c= tensor(5.0385e+08, device='cuda:0')
c= tensor(5.0387e+08, device='cuda:0')
c= tensor(5.0388e+08, device='cuda:0')
c= tensor(5.0392e+08, device='cuda:0')
c= tensor(5.0392e+08, device='cuda:0')
c= tensor(5.0392e+08, device='cuda:0')
c= tensor(5.0393e+08, device='cuda:0')
c= tensor(5.0393e+08, device='cuda:0')
c= tensor(5.0394e+08, device='cuda:0')
c= tensor(5.0395e+08, device='cuda:0')
c= tensor(5.0395e+08, device='cuda:0')
c= tensor(5.0395e+08, device='cuda:0')
c= tensor(5.0396e+08, device='cuda:0')
c= tensor(5.0396e+08, device='cuda:0')
c= tensor(5.0399e+08, device='cuda:0')
c= tensor(5.0399e+08, device='cuda:0')
c= tensor(5.0402e+08, device='cuda:0')
c= tensor(5.0403e+08, device='cuda:0')
c= tensor(5.0403e+08, device='cuda:0')
c= tensor(5.0403e+08, device='cuda:0')
c= tensor(5.0404e+08, device='cuda:0')
c= tensor(5.0405e+08, device='cuda:0')
c= tensor(5.0405e+08, device='cuda:0')
c= tensor(5.0406e+08, device='cuda:0')
c= tensor(5.0408e+08, device='cuda:0')
c= tensor(5.0408e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0414e+08, device='cuda:0')
c= tensor(5.0414e+08, device='cuda:0')
c= tensor(5.0415e+08, device='cuda:0')
c= tensor(5.0415e+08, device='cuda:0')
c= tensor(5.0416e+08, device='cuda:0')
c= tensor(5.0416e+08, device='cuda:0')
c= tensor(5.0417e+08, device='cuda:0')
c= tensor(5.0417e+08, device='cuda:0')
c= tensor(5.0417e+08, device='cuda:0')
c= tensor(5.0417e+08, device='cuda:0')
c= tensor(5.0418e+08, device='cuda:0')
c= tensor(5.0418e+08, device='cuda:0')
c= tensor(5.0418e+08, device='cuda:0')
c= tensor(5.0419e+08, device='cuda:0')
c= tensor(5.0419e+08, device='cuda:0')
c= tensor(5.0420e+08, device='cuda:0')
c= tensor(5.0421e+08, device='cuda:0')
c= tensor(5.0421e+08, device='cuda:0')
c= tensor(5.0425e+08, device='cuda:0')
c= tensor(5.0426e+08, device='cuda:0')
c= tensor(5.0426e+08, device='cuda:0')
c= tensor(5.0450e+08, device='cuda:0')
c= tensor(5.0773e+08, device='cuda:0')
c= tensor(5.0774e+08, device='cuda:0')
c= tensor(5.0776e+08, device='cuda:0')
c= tensor(5.0776e+08, device='cuda:0')
c= tensor(5.0777e+08, device='cuda:0')
c= tensor(5.0781e+08, device='cuda:0')
c= tensor(5.2614e+08, device='cuda:0')
c= tensor(5.2614e+08, device='cuda:0')
c= tensor(5.2954e+08, device='cuda:0')
c= tensor(5.3036e+08, device='cuda:0')
c= tensor(5.3046e+08, device='cuda:0')
c= tensor(5.3122e+08, device='cuda:0')
c= tensor(5.3122e+08, device='cuda:0')
c= tensor(5.3124e+08, device='cuda:0')
c= tensor(5.4356e+08, device='cuda:0')
c= tensor(5.6829e+08, device='cuda:0')
c= tensor(5.6830e+08, device='cuda:0')
c= tensor(5.6874e+08, device='cuda:0')
c= tensor(5.6970e+08, device='cuda:0')
c= tensor(5.7019e+08, device='cuda:0')
c= tensor(5.7262e+08, device='cuda:0')
c= tensor(5.7402e+08, device='cuda:0')
c= tensor(5.7440e+08, device='cuda:0')
c= tensor(5.7473e+08, device='cuda:0')
c= tensor(5.7474e+08, device='cuda:0')
c= tensor(5.9928e+08, device='cuda:0')
c= tensor(5.9938e+08, device='cuda:0')
c= tensor(5.9939e+08, device='cuda:0')
c= tensor(5.9948e+08, device='cuda:0')
c= tensor(6.0047e+08, device='cuda:0')
c= tensor(6.4047e+08, device='cuda:0')
c= tensor(6.4143e+08, device='cuda:0')
c= tensor(6.4143e+08, device='cuda:0')
c= tensor(6.4166e+08, device='cuda:0')
c= tensor(6.4170e+08, device='cuda:0')
c= tensor(6.4212e+08, device='cuda:0')
c= tensor(6.4294e+08, device='cuda:0')
c= tensor(6.4325e+08, device='cuda:0')
c= tensor(6.4367e+08, device='cuda:0')
c= tensor(6.4367e+08, device='cuda:0')
c= tensor(6.4368e+08, device='cuda:0')
c= tensor(6.4548e+08, device='cuda:0')
c= tensor(6.4620e+08, device='cuda:0')
c= tensor(6.4724e+08, device='cuda:0')
c= tensor(6.4725e+08, device='cuda:0')
c= tensor(6.7279e+08, device='cuda:0')
c= tensor(6.7283e+08, device='cuda:0')
c= tensor(6.7312e+08, device='cuda:0')
c= tensor(6.7610e+08, device='cuda:0')
c= tensor(6.7611e+08, device='cuda:0')
c= tensor(6.7726e+08, device='cuda:0')
c= tensor(6.9324e+08, device='cuda:0')
c= tensor(7.3834e+08, device='cuda:0')
c= tensor(7.3850e+08, device='cuda:0')
c= tensor(7.3859e+08, device='cuda:0')
c= tensor(7.3863e+08, device='cuda:0')
c= tensor(7.3864e+08, device='cuda:0')
c= tensor(7.4318e+08, device='cuda:0')
c= tensor(7.4321e+08, device='cuda:0')
c= tensor(7.4354e+08, device='cuda:0')
c= tensor(7.4655e+08, device='cuda:0')
c= tensor(7.4735e+08, device='cuda:0')
c= tensor(7.4760e+08, device='cuda:0')
c= tensor(7.4760e+08, device='cuda:0')
c= tensor(7.6673e+08, device='cuda:0')
c= tensor(7.6686e+08, device='cuda:0')
c= tensor(7.6753e+08, device='cuda:0')
c= tensor(7.6762e+08, device='cuda:0')
c= tensor(7.7275e+08, device='cuda:0')
c= tensor(7.7279e+08, device='cuda:0')
c= tensor(7.8138e+08, device='cuda:0')
c= tensor(7.8144e+08, device='cuda:0')
c= tensor(7.8374e+08, device='cuda:0')
c= tensor(7.8387e+08, device='cuda:0')
c= tensor(8.0444e+08, device='cuda:0')
c= tensor(8.0527e+08, device='cuda:0')
c= tensor(8.0529e+08, device='cuda:0')
c= tensor(8.0928e+08, device='cuda:0')
c= tensor(8.1309e+08, device='cuda:0')
c= tensor(8.1310e+08, device='cuda:0')
c= tensor(8.1814e+08, device='cuda:0')
c= tensor(8.2811e+08, device='cuda:0')
c= tensor(8.5542e+08, device='cuda:0')
c= tensor(8.5651e+08, device='cuda:0')
c= tensor(8.5652e+08, device='cuda:0')
c= tensor(8.5654e+08, device='cuda:0')
c= tensor(8.5665e+08, device='cuda:0')
c= tensor(8.5676e+08, device='cuda:0')
c= tensor(8.5696e+08, device='cuda:0')
c= tensor(8.5696e+08, device='cuda:0')
c= tensor(8.5825e+08, device='cuda:0')
c= tensor(8.6040e+08, device='cuda:0')
c= tensor(8.6050e+08, device='cuda:0')
c= tensor(8.6055e+08, device='cuda:0')
c= tensor(8.6168e+08, device='cuda:0')
c= tensor(8.6171e+08, device='cuda:0')
c= tensor(8.6177e+08, device='cuda:0')
c= tensor(8.6182e+08, device='cuda:0')
c= tensor(8.6183e+08, device='cuda:0')
c= tensor(8.6323e+08, device='cuda:0')
c= tensor(8.6379e+08, device='cuda:0')
c= tensor(8.6402e+08, device='cuda:0')
c= tensor(8.6541e+08, device='cuda:0')
c= tensor(8.6545e+08, device='cuda:0')
c= tensor(9.1582e+08, device='cuda:0')
c= tensor(9.1584e+08, device='cuda:0')
c= tensor(9.1797e+08, device='cuda:0')
c= tensor(9.1797e+08, device='cuda:0')
c= tensor(9.1798e+08, device='cuda:0')
c= tensor(9.1798e+08, device='cuda:0')
c= tensor(9.1806e+08, device='cuda:0')
c= tensor(9.1806e+08, device='cuda:0')
c= tensor(9.2309e+08, device='cuda:0')
c= tensor(9.2309e+08, device='cuda:0')
c= tensor(9.2310e+08, device='cuda:0')
c= tensor(9.3108e+08, device='cuda:0')
c= tensor(9.3213e+08, device='cuda:0')
c= tensor(9.3284e+08, device='cuda:0')
c= tensor(9.3664e+08, device='cuda:0')
c= tensor(9.5237e+08, device='cuda:0')
c= tensor(9.5239e+08, device='cuda:0')
c= tensor(9.5241e+08, device='cuda:0')
c= tensor(9.5252e+08, device='cuda:0')
c= tensor(9.5252e+08, device='cuda:0')
c= tensor(9.5252e+08, device='cuda:0')
c= tensor(9.5272e+08, device='cuda:0')
c= tensor(9.5274e+08, device='cuda:0')
c= tensor(9.5274e+08, device='cuda:0')
c= tensor(9.5275e+08, device='cuda:0')
c= tensor(9.5277e+08, device='cuda:0')
c= tensor(9.7402e+08, device='cuda:0')
c= tensor(9.7414e+08, device='cuda:0')
c= tensor(9.7759e+08, device='cuda:0')
c= tensor(9.7763e+08, device='cuda:0')
c= tensor(9.7764e+08, device='cuda:0')
c= tensor(9.7803e+08, device='cuda:0')
c= tensor(1.0299e+09, device='cuda:0')
c= tensor(1.0578e+09, device='cuda:0')
c= tensor(1.0579e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.1415e+09, device='cuda:0')
c= tensor(1.1417e+09, device='cuda:0')
c= tensor(1.1417e+09, device='cuda:0')
c= tensor(1.1429e+09, device='cuda:0')
c= tensor(1.2331e+09, device='cuda:0')
c= tensor(1.2332e+09, device='cuda:0')
c= tensor(1.2333e+09, device='cuda:0')
c= tensor(1.2333e+09, device='cuda:0')
c= tensor(1.2335e+09, device='cuda:0')
c= tensor(1.2335e+09, device='cuda:0')
c= tensor(1.2387e+09, device='cuda:0')
c= tensor(1.2389e+09, device='cuda:0')
c= tensor(1.2389e+09, device='cuda:0')
c= tensor(1.2396e+09, device='cuda:0')
c= tensor(1.2398e+09, device='cuda:0')
c= tensor(1.2398e+09, device='cuda:0')
c= tensor(1.2429e+09, device='cuda:0')
c= tensor(1.2449e+09, device='cuda:0')
c= tensor(1.2649e+09, device='cuda:0')
c= tensor(1.2766e+09, device='cuda:0')
c= tensor(1.2836e+09, device='cuda:0')
c= tensor(1.2838e+09, device='cuda:0')
c= tensor(1.2839e+09, device='cuda:0')
c= tensor(1.2852e+09, device='cuda:0')
c= tensor(1.2879e+09, device='cuda:0')
c= tensor(1.2879e+09, device='cuda:0')
c= tensor(1.2943e+09, device='cuda:0')
c= tensor(1.3007e+09, device='cuda:0')
c= tensor(1.3073e+09, device='cuda:0')
c= tensor(1.3096e+09, device='cuda:0')
c= tensor(1.3130e+09, device='cuda:0')
c= tensor(1.3130e+09, device='cuda:0')
c= tensor(1.3130e+09, device='cuda:0')
c= tensor(1.3131e+09, device='cuda:0')
c= tensor(1.3150e+09, device='cuda:0')
c= tensor(1.3179e+09, device='cuda:0')
c= tensor(1.3490e+09, device='cuda:0')
c= tensor(1.3577e+09, device='cuda:0')
c= tensor(1.3591e+09, device='cuda:0')
c= tensor(1.3594e+09, device='cuda:0')
c= tensor(1.3659e+09, device='cuda:0')
c= tensor(1.3659e+09, device='cuda:0')
c= tensor(1.3659e+09, device='cuda:0')
c= tensor(1.3730e+09, device='cuda:0')
c= tensor(1.3730e+09, device='cuda:0')
c= tensor(1.3730e+09, device='cuda:0')
c= tensor(1.3732e+09, device='cuda:0')
c= tensor(1.4476e+09, device='cuda:0')
c= tensor(1.4478e+09, device='cuda:0')
c= tensor(1.4516e+09, device='cuda:0')
c= tensor(1.4516e+09, device='cuda:0')
c= tensor(1.4516e+09, device='cuda:0')
c= tensor(1.4516e+09, device='cuda:0')
c= tensor(1.4517e+09, device='cuda:0')
c= tensor(1.4519e+09, device='cuda:0')
c= tensor(1.4530e+09, device='cuda:0')
c= tensor(1.4530e+09, device='cuda:0')
c= tensor(1.4567e+09, device='cuda:0')
c= tensor(1.4568e+09, device='cuda:0')
c= tensor(1.4573e+09, device='cuda:0')
c= tensor(1.4574e+09, device='cuda:0')
c= tensor(1.4587e+09, device='cuda:0')
c= tensor(1.4587e+09, device='cuda:0')
c= tensor(1.4589e+09, device='cuda:0')
c= tensor(1.4590e+09, device='cuda:0')
c= tensor(1.4594e+09, device='cuda:0')
c= tensor(1.4613e+09, device='cuda:0')
c= tensor(1.5186e+09, device='cuda:0')
c= tensor(1.5186e+09, device='cuda:0')
c= tensor(1.5187e+09, device='cuda:0')
c= tensor(1.5203e+09, device='cuda:0')
c= tensor(1.5204e+09, device='cuda:0')
c= tensor(1.5760e+09, device='cuda:0')
c= tensor(1.5761e+09, device='cuda:0')
c= tensor(1.5784e+09, device='cuda:0')
c= tensor(1.5873e+09, device='cuda:0')
c= tensor(1.5874e+09, device='cuda:0')
c= tensor(1.5969e+09, device='cuda:0')
c= tensor(1.5974e+09, device='cuda:0')
c= tensor(1.6970e+09, device='cuda:0')
c= tensor(1.6971e+09, device='cuda:0')
c= tensor(1.6972e+09, device='cuda:0')
c= tensor(1.6972e+09, device='cuda:0')
c= tensor(1.6972e+09, device='cuda:0')
c= tensor(1.6973e+09, device='cuda:0')
c= tensor(1.6977e+09, device='cuda:0')
c= tensor(1.6979e+09, device='cuda:0')
c= tensor(1.6998e+09, device='cuda:0')
c= tensor(1.6999e+09, device='cuda:0')
c= tensor(1.6999e+09, device='cuda:0')
c= tensor(1.7000e+09, device='cuda:0')
c= tensor(1.7151e+09, device='cuda:0')
c= tensor(1.7173e+09, device='cuda:0')
c= tensor(1.7435e+09, device='cuda:0')
c= tensor(1.7436e+09, device='cuda:0')
c= tensor(1.7436e+09, device='cuda:0')
c= tensor(1.7436e+09, device='cuda:0')
c= tensor(1.7437e+09, device='cuda:0')
c= tensor(1.7564e+09, device='cuda:0')
c= tensor(1.7564e+09, device='cuda:0')
c= tensor(1.7565e+09, device='cuda:0')
c= tensor(1.7580e+09, device='cuda:0')
c= tensor(1.7591e+09, device='cuda:0')
c= tensor(1.7591e+09, device='cuda:0')
c= tensor(1.7591e+09, device='cuda:0')
c= tensor(1.7844e+09, device='cuda:0')
c= tensor(1.7846e+09, device='cuda:0')
c= tensor(1.7846e+09, device='cuda:0')
c= tensor(1.7847e+09, device='cuda:0')
c= tensor(1.8118e+09, device='cuda:0')
c= tensor(1.8143e+09, device='cuda:0')
c= tensor(1.8389e+09, device='cuda:0')
c= tensor(1.8411e+09, device='cuda:0')
c= tensor(1.8412e+09, device='cuda:0')
c= tensor(1.8415e+09, device='cuda:0')
c= tensor(1.8420e+09, device='cuda:0')
c= tensor(1.8420e+09, device='cuda:0')
c= tensor(1.8421e+09, device='cuda:0')
c= tensor(1.8421e+09, device='cuda:0')
c= tensor(1.8432e+09, device='cuda:0')
c= tensor(1.8433e+09, device='cuda:0')
c= tensor(1.8433e+09, device='cuda:0')
c= tensor(1.8434e+09, device='cuda:0')
c= tensor(1.8435e+09, device='cuda:0')
c= tensor(1.8443e+09, device='cuda:0')
c= tensor(1.8444e+09, device='cuda:0')
c= tensor(1.8446e+09, device='cuda:0')
c= tensor(1.8446e+09, device='cuda:0')
c= tensor(1.8447e+09, device='cuda:0')
c= tensor(1.8447e+09, device='cuda:0')
c= tensor(1.8447e+09, device='cuda:0')
c= tensor(1.8448e+09, device='cuda:0')
c= tensor(1.8516e+09, device='cuda:0')
c= tensor(1.8516e+09, device='cuda:0')
c= tensor(1.8516e+09, device='cuda:0')
c= tensor(1.8517e+09, device='cuda:0')
c= tensor(1.8564e+09, device='cuda:0')
c= tensor(1.9183e+09, device='cuda:0')
c= tensor(1.9187e+09, device='cuda:0')
c= tensor(1.9187e+09, device='cuda:0')
c= tensor(1.9227e+09, device='cuda:0')
c= tensor(1.9250e+09, device='cuda:0')
c= tensor(1.9250e+09, device='cuda:0')
c= tensor(1.9251e+09, device='cuda:0')
c= tensor(1.9252e+09, device='cuda:0')
c= tensor(1.9381e+09, device='cuda:0')
c= tensor(1.9632e+09, device='cuda:0')
c= tensor(1.9689e+09, device='cuda:0')
c= tensor(1.9689e+09, device='cuda:0')
c= tensor(1.9689e+09, device='cuda:0')
c= tensor(1.9689e+09, device='cuda:0')
c= tensor(1.9694e+09, device='cuda:0')
c= tensor(1.9695e+09, device='cuda:0')
c= tensor(1.9696e+09, device='cuda:0')
c= tensor(1.9733e+09, device='cuda:0')
c= tensor(2.0068e+09, device='cuda:0')
c= tensor(2.0068e+09, device='cuda:0')
c= tensor(2.0068e+09, device='cuda:0')
c= tensor(2.0069e+09, device='cuda:0')
c= tensor(2.0073e+09, device='cuda:0')
c= tensor(2.0075e+09, device='cuda:0')
c= tensor(2.0075e+09, device='cuda:0')
c= tensor(2.0075e+09, device='cuda:0')
c= tensor(2.0077e+09, device='cuda:0')
c= tensor(2.0077e+09, device='cuda:0')
c= tensor(2.0121e+09, device='cuda:0')
c= tensor(2.0121e+09, device='cuda:0')
c= tensor(2.0122e+09, device='cuda:0')
c= tensor(2.0123e+09, device='cuda:0')
c= tensor(2.0123e+09, device='cuda:0')
c= tensor(2.0123e+09, device='cuda:0')
c= tensor(2.0171e+09, device='cuda:0')
c= tensor(2.0501e+09, device='cuda:0')
c= tensor(2.0558e+09, device='cuda:0')
c= tensor(2.0577e+09, device='cuda:0')
c= tensor(2.0578e+09, device='cuda:0')
c= tensor(2.0579e+09, device='cuda:0')
c= tensor(2.0580e+09, device='cuda:0')
c= tensor(2.0604e+09, device='cuda:0')
c= tensor(2.0609e+09, device='cuda:0')
c= tensor(2.0610e+09, device='cuda:0')
c= tensor(2.0611e+09, device='cuda:0')
c= tensor(2.2840e+09, device='cuda:0')
c= tensor(2.2841e+09, device='cuda:0')
c= tensor(2.2848e+09, device='cuda:0')
c= tensor(2.2990e+09, device='cuda:0')
c= tensor(2.2996e+09, device='cuda:0')
c= tensor(2.2999e+09, device='cuda:0')
c= tensor(2.3516e+09, device='cuda:0')
c= tensor(2.3536e+09, device='cuda:0')
c= tensor(2.3542e+09, device='cuda:0')
c= tensor(2.3542e+09, device='cuda:0')
c= tensor(2.3545e+09, device='cuda:0')
c= tensor(2.3545e+09, device='cuda:0')
c= tensor(2.3628e+09, device='cuda:0')
c= tensor(2.5801e+09, device='cuda:0')
c= tensor(2.5902e+09, device='cuda:0')
c= tensor(2.5952e+09, device='cuda:0')
c= tensor(2.5966e+09, device='cuda:0')
c= tensor(2.5975e+09, device='cuda:0')
c= tensor(2.5978e+09, device='cuda:0')
c= tensor(2.5979e+09, device='cuda:0')
c= tensor(2.6246e+09, device='cuda:0')
c= tensor(2.6452e+09, device='cuda:0')
c= tensor(2.6459e+09, device='cuda:0')
c= tensor(2.8254e+09, device='cuda:0')
c= tensor(2.8485e+09, device='cuda:0')
c= tensor(2.8488e+09, device='cuda:0')
c= tensor(2.8488e+09, device='cuda:0')
c= tensor(2.8500e+09, device='cuda:0')
c= tensor(2.8515e+09, device='cuda:0')
c= tensor(2.8515e+09, device='cuda:0')
c= tensor(2.8920e+09, device='cuda:0')
c= tensor(2.8926e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8933e+09, device='cuda:0')
c= tensor(2.8934e+09, device='cuda:0')
c= tensor(5.4428e+09, device='cuda:0')
c= tensor(5.4431e+09, device='cuda:0')
c= tensor(5.4440e+09, device='cuda:0')
c= tensor(5.4440e+09, device='cuda:0')
c= tensor(5.4441e+09, device='cuda:0')
c= tensor(5.4441e+09, device='cuda:0')
c= tensor(5.4557e+09, device='cuda:0')
c= tensor(5.4570e+09, device='cuda:0')
c= tensor(5.5712e+09, device='cuda:0')
c= tensor(5.5712e+09, device='cuda:0')
c= tensor(5.5774e+09, device='cuda:0')
c= tensor(5.5781e+09, device='cuda:0')
c= tensor(5.5818e+09, device='cuda:0')
c= tensor(5.5980e+09, device='cuda:0')
c= tensor(5.5981e+09, device='cuda:0')
c= tensor(5.5981e+09, device='cuda:0')
c= tensor(5.6001e+09, device='cuda:0')
c= tensor(5.6014e+09, device='cuda:0')
c= tensor(5.6026e+09, device='cuda:0')
c= tensor(5.6079e+09, device='cuda:0')
c= tensor(5.6081e+09, device='cuda:0')
c= tensor(5.6117e+09, device='cuda:0')
c= tensor(5.6119e+09, device='cuda:0')
c= tensor(5.6187e+09, device='cuda:0')
c= tensor(5.6448e+09, device='cuda:0')
c= tensor(5.6450e+09, device='cuda:0')
c= tensor(5.6450e+09, device='cuda:0')
c= tensor(5.6502e+09, device='cuda:0')
c= tensor(5.6508e+09, device='cuda:0')
c= tensor(5.6886e+09, device='cuda:0')
c= tensor(5.6891e+09, device='cuda:0')
c= tensor(5.6897e+09, device='cuda:0')
c= tensor(5.6907e+09, device='cuda:0')
c= tensor(5.6955e+09, device='cuda:0')
c= tensor(5.7015e+09, device='cuda:0')
c= tensor(5.7016e+09, device='cuda:0')
c= tensor(5.7016e+09, device='cuda:0')
c= tensor(5.7016e+09, device='cuda:0')
c= tensor(5.7028e+09, device='cuda:0')
c= tensor(5.7045e+09, device='cuda:0')
c= tensor(5.7053e+09, device='cuda:0')
c= tensor(5.7053e+09, device='cuda:0')
c= tensor(5.7055e+09, device='cuda:0')
c= tensor(5.7062e+09, device='cuda:0')
c= tensor(5.7068e+09, device='cuda:0')
c= tensor(5.7072e+09, device='cuda:0')
c= tensor(5.7087e+09, device='cuda:0')
c= tensor(5.7094e+09, device='cuda:0')
c= tensor(5.7097e+09, device='cuda:0')
c= tensor(5.7097e+09, device='cuda:0')
c= tensor(5.7097e+09, device='cuda:0')
c= tensor(5.7113e+09, device='cuda:0')
c= tensor(5.7116e+09, device='cuda:0')
c= tensor(5.7122e+09, device='cuda:0')
c= tensor(5.7122e+09, device='cuda:0')
c= tensor(5.7122e+09, device='cuda:0')
c= tensor(5.7125e+09, device='cuda:0')
c= tensor(5.7130e+09, device='cuda:0')
c= tensor(5.7131e+09, device='cuda:0')
c= tensor(5.7131e+09, device='cuda:0')
c= tensor(5.7132e+09, device='cuda:0')
c= tensor(5.7135e+09, device='cuda:0')
c= tensor(5.7140e+09, device='cuda:0')
c= tensor(5.7175e+09, device='cuda:0')
c= tensor(5.7175e+09, device='cuda:0')
c= tensor(5.7176e+09, device='cuda:0')
c= tensor(5.7177e+09, device='cuda:0')
c= tensor(5.7178e+09, device='cuda:0')
c= tensor(5.7333e+09, device='cuda:0')
c= tensor(5.7333e+09, device='cuda:0')
c= tensor(5.7342e+09, device='cuda:0')
c= tensor(5.7385e+09, device='cuda:0')
c= tensor(5.7387e+09, device='cuda:0')
c= tensor(5.7432e+09, device='cuda:0')
c= tensor(5.7459e+09, device='cuda:0')
c= tensor(5.7459e+09, device='cuda:0')
c= tensor(5.7462e+09, device='cuda:0')
c= tensor(5.7463e+09, device='cuda:0')
c= tensor(5.7484e+09, device='cuda:0')
c= tensor(5.7516e+09, device='cuda:0')
c= tensor(5.7517e+09, device='cuda:0')
c= tensor(5.7523e+09, device='cuda:0')
c= tensor(5.7527e+09, device='cuda:0')
c= tensor(5.7545e+09, device='cuda:0')
c= tensor(5.7545e+09, device='cuda:0')
c= tensor(5.7545e+09, device='cuda:0')
c= tensor(5.7614e+09, device='cuda:0')
c= tensor(5.7998e+09, device='cuda:0')
c= tensor(5.7999e+09, device='cuda:0')
c= tensor(5.8000e+09, device='cuda:0')
c= tensor(5.8002e+09, device='cuda:0')
c= tensor(5.8031e+09, device='cuda:0')
c= tensor(5.8032e+09, device='cuda:0')
c= tensor(5.8032e+09, device='cuda:0')
c= tensor(5.8036e+09, device='cuda:0')
c= tensor(5.8070e+09, device='cuda:0')
c= tensor(5.8070e+09, device='cuda:0')
c= tensor(5.8080e+09, device='cuda:0')
c= tensor(5.8080e+09, device='cuda:0')
memory (bytes)
4891754496
time for making loss 2 is 12.881442785263062
p0 True
it  0 : 2477538816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 87% |
shape of L is 
torch.Size([])
memory (bytes)
4892098560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 56% | 10% |
memory (bytes)
4892704768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  46388490000.0
relative error loss 7.986973
shape of L is 
torch.Size([])
memory (bytes)
5071101952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% | 11% |
memory (bytes)
5071273984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  46388285000.0
relative error loss 7.9869375
shape of L is 
torch.Size([])
memory (bytes)
5073129472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
5073248256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  46387642000.0
relative error loss 7.986827
shape of L is 
torch.Size([])
memory (bytes)
5074153472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5074153472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  46383813000.0
relative error loss 7.9861674
shape of L is 
torch.Size([])
memory (bytes)
5075931136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 11% |
memory (bytes)
5075931136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  46355116000.0
relative error loss 7.981227
shape of L is 
torch.Size([])
memory (bytes)
5078167552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5078200320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  46205020000.0
relative error loss 7.9553833
shape of L is 
torch.Size([])
memory (bytes)
5080113152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5080113152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  45386433000.0
relative error loss 7.814443
shape of L is 
torch.Size([])
memory (bytes)
5082439680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5082439680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  41109733000.0
relative error loss 7.0780993
shape of L is 
torch.Size([])
memory (bytes)
5084520448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5084520448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  13085243000.0
relative error loss 2.2529616
shape of L is 
torch.Size([])
memory (bytes)
5086662656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5086695424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  8497804300.0
relative error loss 1.4631158
time to take a step is 178.3102376461029
it  1 : 2910094336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5088821248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5088821248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  8497804300.0
relative error loss 1.4631158
shape of L is 
torch.Size([])
memory (bytes)
5090979840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5090979840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  8569608000.0
relative error loss 1.4754786
shape of L is 
torch.Size([])
memory (bytes)
5092917248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5092917248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  7006743000.0
relative error loss 1.2063912
shape of L is 
torch.Size([])
memory (bytes)
5095161856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5095161856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  6125367300.0
relative error loss 1.0546397
shape of L is 
torch.Size([])
memory (bytes)
5097390080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5097390080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  5806376400.0
relative error loss 0.9997172
shape of L is 
torch.Size([])
memory (bytes)
5099524096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5099524096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  7319722500.0
relative error loss 1.2602787
shape of L is 
torch.Size([])
memory (bytes)
5101641728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
5101641728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  5446870500.0
relative error loss 0.937819
shape of L is 
torch.Size([])
memory (bytes)
5103759360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5103767552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  5281550000.0
relative error loss 0.9093548
shape of L is 
torch.Size([])
memory (bytes)
5105709056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5105709056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  4810511000.0
relative error loss 0.82825327
shape of L is 
torch.Size([])
memory (bytes)
5107634176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5107634176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4727724000.0
relative error loss 0.8139994
shape of L is 
torch.Size([])
memory (bytes)
5110059008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5110083584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  4527556600.0
relative error loss 0.7795354
time to take a step is 223.88206243515015
it  2 : 3033682432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5112020992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5112020992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4527556600.0
relative error loss 0.7795354
shape of L is 
torch.Size([])
memory (bytes)
5114040320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5114040320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  4146317800.0
relative error loss 0.7138954
shape of L is 
torch.Size([])
memory (bytes)
5116432384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5116432384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3734015500.0
relative error loss 0.6429069
shape of L is 
torch.Size([])
memory (bytes)
5118554112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5118554112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3523324400.0
relative error loss 0.60663104
shape of L is 
torch.Size([])
memory (bytes)
5120606208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5120606208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3198279000.0
relative error loss 0.55066603
shape of L is 
torch.Size([])
memory (bytes)
5122768896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5122793472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2958030300.0
relative error loss 0.50930107
shape of L is 
torch.Size([])
memory (bytes)
5124808704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5124808704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3002633000.0
relative error loss 0.5169806
shape of L is 
torch.Size([])
memory (bytes)
5127012352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5127012352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2764552200.0
relative error loss 0.47598884
shape of L is 
torch.Size([])
memory (bytes)
5129216000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5129216000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2577615000.0
relative error loss 0.4438028
shape of L is 
torch.Size([])
memory (bytes)
5131325440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5131325440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2444758000.0
relative error loss 0.42092803
time to take a step is 203.48499965667725
it  3 : 3033681920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5133209600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5133209600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2444758000.0
relative error loss 0.42092803
shape of L is 
torch.Size([])
memory (bytes)
5135646720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5135646720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2210692900.0
relative error loss 0.3806277
shape of L is 
torch.Size([])
memory (bytes)
5137760256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5137776640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1979356700.0
relative error loss 0.34079722
shape of L is 
torch.Size([])
memory (bytes)
5139771392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
5139771392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% | 11% |
error is  1851634700.0
relative error loss 0.3188066
shape of L is 
torch.Size([])
memory (bytes)
5141876736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5142056960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1765853400.0
relative error loss 0.30403712
shape of L is 
torch.Size([])
memory (bytes)
5144186880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5144186880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1675056900.0
relative error loss 0.28840417
shape of L is 
torch.Size([])
memory (bytes)
5146324992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5146324992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1587315000.0
relative error loss 0.27329713
shape of L is 
torch.Size([])
memory (bytes)
5148426240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5148459008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1681689100.0
relative error loss 0.28954607
shape of L is 
torch.Size([])
memory (bytes)
5150568448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5150597120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 11% |
error is  1438506000.0
relative error loss 0.24767584
shape of L is 
torch.Size([])
memory (bytes)
5152632832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
5152632832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1344229400.0
relative error loss 0.2314437
time to take a step is 203.39272117614746
c= tensor(1531.3965, device='cuda:0')
c= tensor(140575.8281, device='cuda:0')
c= tensor(143714.4688, device='cuda:0')
c= tensor(284626.3438, device='cuda:0')
c= tensor(501241., device='cuda:0')
c= tensor(708740.3125, device='cuda:0')
c= tensor(1352724.5000, device='cuda:0')
c= tensor(1680921.5000, device='cuda:0')
c= tensor(2056028.5000, device='cuda:0')
c= tensor(5840883.5000, device='cuda:0')
c= tensor(5870270., device='cuda:0')
c= tensor(9377203., device='cuda:0')
c= tensor(9391449., device='cuda:0')
c= tensor(38370248., device='cuda:0')
c= tensor(38567940., device='cuda:0')
c= tensor(38794696., device='cuda:0')
c= tensor(39996708., device='cuda:0')
c= tensor(40397040., device='cuda:0')
c= tensor(45426164., device='cuda:0')
c= tensor(48855416., device='cuda:0')
c= tensor(49041904., device='cuda:0')
c= tensor(57322780., device='cuda:0')
c= tensor(57355876., device='cuda:0')
c= tensor(58837672., device='cuda:0')
c= tensor(58846644., device='cuda:0')
c= tensor(59773380., device='cuda:0')
c= tensor(63826832., device='cuda:0')
c= tensor(63858664., device='cuda:0')
c= tensor(69202248., device='cuda:0')
c= tensor(2.4602e+08, device='cuda:0')
c= tensor(2.4606e+08, device='cuda:0')
c= tensor(4.8447e+08, device='cuda:0')
c= tensor(4.8463e+08, device='cuda:0')
c= tensor(4.8466e+08, device='cuda:0')
c= tensor(4.8470e+08, device='cuda:0')
c= tensor(4.9962e+08, device='cuda:0')
c= tensor(5.0231e+08, device='cuda:0')
c= tensor(5.0231e+08, device='cuda:0')
c= tensor(5.0232e+08, device='cuda:0')
c= tensor(5.0233e+08, device='cuda:0')
c= tensor(5.0233e+08, device='cuda:0')
c= tensor(5.0234e+08, device='cuda:0')
c= tensor(5.0234e+08, device='cuda:0')
c= tensor(5.0234e+08, device='cuda:0')
c= tensor(5.0234e+08, device='cuda:0')
c= tensor(5.0235e+08, device='cuda:0')
c= tensor(5.0235e+08, device='cuda:0')
c= tensor(5.0236e+08, device='cuda:0')
c= tensor(5.0236e+08, device='cuda:0')
c= tensor(5.0242e+08, device='cuda:0')
c= tensor(5.0253e+08, device='cuda:0')
c= tensor(5.0253e+08, device='cuda:0')
c= tensor(5.0254e+08, device='cuda:0')
c= tensor(5.0255e+08, device='cuda:0')
c= tensor(5.0256e+08, device='cuda:0')
c= tensor(5.0259e+08, device='cuda:0')
c= tensor(5.0259e+08, device='cuda:0')
c= tensor(5.0260e+08, device='cuda:0')
c= tensor(5.0260e+08, device='cuda:0')
c= tensor(5.0261e+08, device='cuda:0')
c= tensor(5.0264e+08, device='cuda:0')
c= tensor(5.0264e+08, device='cuda:0')
c= tensor(5.0266e+08, device='cuda:0')
c= tensor(5.0268e+08, device='cuda:0')
c= tensor(5.0268e+08, device='cuda:0')
c= tensor(5.0269e+08, device='cuda:0')
c= tensor(5.0269e+08, device='cuda:0')
c= tensor(5.0270e+08, device='cuda:0')
c= tensor(5.0270e+08, device='cuda:0')
c= tensor(5.0271e+08, device='cuda:0')
c= tensor(5.0273e+08, device='cuda:0')
c= tensor(5.0274e+08, device='cuda:0')
c= tensor(5.0274e+08, device='cuda:0')
c= tensor(5.0275e+08, device='cuda:0')
c= tensor(5.0276e+08, device='cuda:0')
c= tensor(5.0281e+08, device='cuda:0')
c= tensor(5.0281e+08, device='cuda:0')
c= tensor(5.0281e+08, device='cuda:0')
c= tensor(5.0281e+08, device='cuda:0')
c= tensor(5.0283e+08, device='cuda:0')
c= tensor(5.0284e+08, device='cuda:0')
c= tensor(5.0284e+08, device='cuda:0')
c= tensor(5.0285e+08, device='cuda:0')
c= tensor(5.0285e+08, device='cuda:0')
c= tensor(5.0285e+08, device='cuda:0')
c= tensor(5.0285e+08, device='cuda:0')
c= tensor(5.0286e+08, device='cuda:0')
c= tensor(5.0286e+08, device='cuda:0')
c= tensor(5.0287e+08, device='cuda:0')
c= tensor(5.0287e+08, device='cuda:0')
c= tensor(5.0288e+08, device='cuda:0')
c= tensor(5.0288e+08, device='cuda:0')
c= tensor(5.0288e+08, device='cuda:0')
c= tensor(5.0290e+08, device='cuda:0')
c= tensor(5.0291e+08, device='cuda:0')
c= tensor(5.0292e+08, device='cuda:0')
c= tensor(5.0293e+08, device='cuda:0')
c= tensor(5.0297e+08, device='cuda:0')
c= tensor(5.0299e+08, device='cuda:0')
c= tensor(5.0299e+08, device='cuda:0')
c= tensor(5.0305e+08, device='cuda:0')
c= tensor(5.0307e+08, device='cuda:0')
c= tensor(5.0308e+08, device='cuda:0')
c= tensor(5.0308e+08, device='cuda:0')
c= tensor(5.0309e+08, device='cuda:0')
c= tensor(5.0309e+08, device='cuda:0')
c= tensor(5.0310e+08, device='cuda:0')
c= tensor(5.0310e+08, device='cuda:0')
c= tensor(5.0311e+08, device='cuda:0')
c= tensor(5.0311e+08, device='cuda:0')
c= tensor(5.0311e+08, device='cuda:0')
c= tensor(5.0312e+08, device='cuda:0')
c= tensor(5.0312e+08, device='cuda:0')
c= tensor(5.0312e+08, device='cuda:0')
c= tensor(5.0314e+08, device='cuda:0')
c= tensor(5.0314e+08, device='cuda:0')
c= tensor(5.0315e+08, device='cuda:0')
c= tensor(5.0315e+08, device='cuda:0')
c= tensor(5.0317e+08, device='cuda:0')
c= tensor(5.0318e+08, device='cuda:0')
c= tensor(5.0320e+08, device='cuda:0')
c= tensor(5.0320e+08, device='cuda:0')
c= tensor(5.0320e+08, device='cuda:0')
c= tensor(5.0320e+08, device='cuda:0')
c= tensor(5.0321e+08, device='cuda:0')
c= tensor(5.0321e+08, device='cuda:0')
c= tensor(5.0321e+08, device='cuda:0')
c= tensor(5.0322e+08, device='cuda:0')
c= tensor(5.0324e+08, device='cuda:0')
c= tensor(5.0325e+08, device='cuda:0')
c= tensor(5.0325e+08, device='cuda:0')
c= tensor(5.0326e+08, device='cuda:0')
c= tensor(5.0326e+08, device='cuda:0')
c= tensor(5.0327e+08, device='cuda:0')
c= tensor(5.0327e+08, device='cuda:0')
c= tensor(5.0327e+08, device='cuda:0')
c= tensor(5.0328e+08, device='cuda:0')
c= tensor(5.0328e+08, device='cuda:0')
c= tensor(5.0328e+08, device='cuda:0')
c= tensor(5.0328e+08, device='cuda:0')
c= tensor(5.0329e+08, device='cuda:0')
c= tensor(5.0329e+08, device='cuda:0')
c= tensor(5.0331e+08, device='cuda:0')
c= tensor(5.0337e+08, device='cuda:0')
c= tensor(5.0338e+08, device='cuda:0')
c= tensor(5.0338e+08, device='cuda:0')
c= tensor(5.0338e+08, device='cuda:0')
c= tensor(5.0338e+08, device='cuda:0')
c= tensor(5.0339e+08, device='cuda:0')
c= tensor(5.0339e+08, device='cuda:0')
c= tensor(5.0339e+08, device='cuda:0')
c= tensor(5.0341e+08, device='cuda:0')
c= tensor(5.0341e+08, device='cuda:0')
c= tensor(5.0348e+08, device='cuda:0')
c= tensor(5.0349e+08, device='cuda:0')
c= tensor(5.0362e+08, device='cuda:0')
c= tensor(5.0363e+08, device='cuda:0')
c= tensor(5.0363e+08, device='cuda:0')
c= tensor(5.0364e+08, device='cuda:0')
c= tensor(5.0364e+08, device='cuda:0')
c= tensor(5.0367e+08, device='cuda:0')
c= tensor(5.0367e+08, device='cuda:0')
c= tensor(5.0369e+08, device='cuda:0')
c= tensor(5.0369e+08, device='cuda:0')
c= tensor(5.0371e+08, device='cuda:0')
c= tensor(5.0371e+08, device='cuda:0')
c= tensor(5.0371e+08, device='cuda:0')
c= tensor(5.0372e+08, device='cuda:0')
c= tensor(5.0372e+08, device='cuda:0')
c= tensor(5.0372e+08, device='cuda:0')
c= tensor(5.0373e+08, device='cuda:0')
c= tensor(5.0373e+08, device='cuda:0')
c= tensor(5.0374e+08, device='cuda:0')
c= tensor(5.0374e+08, device='cuda:0')
c= tensor(5.0375e+08, device='cuda:0')
c= tensor(5.0376e+08, device='cuda:0')
c= tensor(5.0376e+08, device='cuda:0')
c= tensor(5.0377e+08, device='cuda:0')
c= tensor(5.0379e+08, device='cuda:0')
c= tensor(5.0379e+08, device='cuda:0')
c= tensor(5.0381e+08, device='cuda:0')
c= tensor(5.0381e+08, device='cuda:0')
c= tensor(5.0382e+08, device='cuda:0')
c= tensor(5.0382e+08, device='cuda:0')
c= tensor(5.0383e+08, device='cuda:0')
c= tensor(5.0384e+08, device='cuda:0')
c= tensor(5.0385e+08, device='cuda:0')
c= tensor(5.0387e+08, device='cuda:0')
c= tensor(5.0388e+08, device='cuda:0')
c= tensor(5.0392e+08, device='cuda:0')
c= tensor(5.0392e+08, device='cuda:0')
c= tensor(5.0392e+08, device='cuda:0')
c= tensor(5.0393e+08, device='cuda:0')
c= tensor(5.0393e+08, device='cuda:0')
c= tensor(5.0394e+08, device='cuda:0')
c= tensor(5.0395e+08, device='cuda:0')
c= tensor(5.0395e+08, device='cuda:0')
c= tensor(5.0395e+08, device='cuda:0')
c= tensor(5.0396e+08, device='cuda:0')
c= tensor(5.0396e+08, device='cuda:0')
c= tensor(5.0399e+08, device='cuda:0')
c= tensor(5.0399e+08, device='cuda:0')
c= tensor(5.0402e+08, device='cuda:0')
c= tensor(5.0403e+08, device='cuda:0')
c= tensor(5.0403e+08, device='cuda:0')
c= tensor(5.0403e+08, device='cuda:0')
c= tensor(5.0404e+08, device='cuda:0')
c= tensor(5.0405e+08, device='cuda:0')
c= tensor(5.0405e+08, device='cuda:0')
c= tensor(5.0406e+08, device='cuda:0')
c= tensor(5.0408e+08, device='cuda:0')
c= tensor(5.0408e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0409e+08, device='cuda:0')
c= tensor(5.0414e+08, device='cuda:0')
c= tensor(5.0414e+08, device='cuda:0')
c= tensor(5.0415e+08, device='cuda:0')
c= tensor(5.0415e+08, device='cuda:0')
c= tensor(5.0416e+08, device='cuda:0')
c= tensor(5.0416e+08, device='cuda:0')
c= tensor(5.0417e+08, device='cuda:0')
c= tensor(5.0417e+08, device='cuda:0')
c= tensor(5.0417e+08, device='cuda:0')
c= tensor(5.0417e+08, device='cuda:0')
c= tensor(5.0418e+08, device='cuda:0')
c= tensor(5.0418e+08, device='cuda:0')
c= tensor(5.0418e+08, device='cuda:0')
c= tensor(5.0419e+08, device='cuda:0')
c= tensor(5.0419e+08, device='cuda:0')
c= tensor(5.0420e+08, device='cuda:0')
c= tensor(5.0421e+08, device='cuda:0')
c= tensor(5.0421e+08, device='cuda:0')
c= tensor(5.0425e+08, device='cuda:0')
c= tensor(5.0426e+08, device='cuda:0')
c= tensor(5.0426e+08, device='cuda:0')
c= tensor(5.0450e+08, device='cuda:0')
c= tensor(5.0773e+08, device='cuda:0')
c= tensor(5.0774e+08, device='cuda:0')
c= tensor(5.0776e+08, device='cuda:0')
c= tensor(5.0776e+08, device='cuda:0')
c= tensor(5.0777e+08, device='cuda:0')
c= tensor(5.0781e+08, device='cuda:0')
c= tensor(5.2614e+08, device='cuda:0')
c= tensor(5.2614e+08, device='cuda:0')
c= tensor(5.2954e+08, device='cuda:0')
c= tensor(5.3036e+08, device='cuda:0')
c= tensor(5.3046e+08, device='cuda:0')
c= tensor(5.3122e+08, device='cuda:0')
c= tensor(5.3122e+08, device='cuda:0')
c= tensor(5.3124e+08, device='cuda:0')
c= tensor(5.4356e+08, device='cuda:0')
c= tensor(5.6829e+08, device='cuda:0')
c= tensor(5.6830e+08, device='cuda:0')
c= tensor(5.6874e+08, device='cuda:0')
c= tensor(5.6970e+08, device='cuda:0')
c= tensor(5.7019e+08, device='cuda:0')
c= tensor(5.7262e+08, device='cuda:0')
c= tensor(5.7402e+08, device='cuda:0')
c= tensor(5.7440e+08, device='cuda:0')
c= tensor(5.7473e+08, device='cuda:0')
c= tensor(5.7474e+08, device='cuda:0')
c= tensor(5.9928e+08, device='cuda:0')
c= tensor(5.9938e+08, device='cuda:0')
c= tensor(5.9939e+08, device='cuda:0')
c= tensor(5.9948e+08, device='cuda:0')
c= tensor(6.0047e+08, device='cuda:0')
c= tensor(6.4047e+08, device='cuda:0')
c= tensor(6.4143e+08, device='cuda:0')
c= tensor(6.4143e+08, device='cuda:0')
c= tensor(6.4166e+08, device='cuda:0')
c= tensor(6.4170e+08, device='cuda:0')
c= tensor(6.4212e+08, device='cuda:0')
c= tensor(6.4294e+08, device='cuda:0')
c= tensor(6.4325e+08, device='cuda:0')
c= tensor(6.4367e+08, device='cuda:0')
c= tensor(6.4367e+08, device='cuda:0')
c= tensor(6.4368e+08, device='cuda:0')
c= tensor(6.4548e+08, device='cuda:0')
c= tensor(6.4620e+08, device='cuda:0')
c= tensor(6.4724e+08, device='cuda:0')
c= tensor(6.4725e+08, device='cuda:0')
c= tensor(6.7279e+08, device='cuda:0')
c= tensor(6.7283e+08, device='cuda:0')
c= tensor(6.7312e+08, device='cuda:0')
c= tensor(6.7610e+08, device='cuda:0')
c= tensor(6.7611e+08, device='cuda:0')
c= tensor(6.7726e+08, device='cuda:0')
c= tensor(6.9324e+08, device='cuda:0')
c= tensor(7.3834e+08, device='cuda:0')
c= tensor(7.3850e+08, device='cuda:0')
c= tensor(7.3859e+08, device='cuda:0')
c= tensor(7.3863e+08, device='cuda:0')
c= tensor(7.3864e+08, device='cuda:0')
c= tensor(7.4318e+08, device='cuda:0')
c= tensor(7.4321e+08, device='cuda:0')
c= tensor(7.4354e+08, device='cuda:0')
c= tensor(7.4655e+08, device='cuda:0')
c= tensor(7.4735e+08, device='cuda:0')
c= tensor(7.4760e+08, device='cuda:0')
c= tensor(7.4760e+08, device='cuda:0')
c= tensor(7.6673e+08, device='cuda:0')
c= tensor(7.6686e+08, device='cuda:0')
c= tensor(7.6753e+08, device='cuda:0')
c= tensor(7.6762e+08, device='cuda:0')
c= tensor(7.7275e+08, device='cuda:0')
c= tensor(7.7279e+08, device='cuda:0')
c= tensor(7.8138e+08, device='cuda:0')
c= tensor(7.8144e+08, device='cuda:0')
c= tensor(7.8374e+08, device='cuda:0')
c= tensor(7.8387e+08, device='cuda:0')
c= tensor(8.0444e+08, device='cuda:0')
c= tensor(8.0527e+08, device='cuda:0')
c= tensor(8.0529e+08, device='cuda:0')
c= tensor(8.0928e+08, device='cuda:0')
c= tensor(8.1309e+08, device='cuda:0')
c= tensor(8.1310e+08, device='cuda:0')
c= tensor(8.1814e+08, device='cuda:0')
c= tensor(8.2811e+08, device='cuda:0')
c= tensor(8.5542e+08, device='cuda:0')
c= tensor(8.5651e+08, device='cuda:0')
c= tensor(8.5652e+08, device='cuda:0')
c= tensor(8.5654e+08, device='cuda:0')
c= tensor(8.5665e+08, device='cuda:0')
c= tensor(8.5676e+08, device='cuda:0')
c= tensor(8.5696e+08, device='cuda:0')
c= tensor(8.5696e+08, device='cuda:0')
c= tensor(8.5825e+08, device='cuda:0')
c= tensor(8.6040e+08, device='cuda:0')
c= tensor(8.6050e+08, device='cuda:0')
c= tensor(8.6055e+08, device='cuda:0')
c= tensor(8.6168e+08, device='cuda:0')
c= tensor(8.6171e+08, device='cuda:0')
c= tensor(8.6177e+08, device='cuda:0')
c= tensor(8.6182e+08, device='cuda:0')
c= tensor(8.6183e+08, device='cuda:0')
c= tensor(8.6323e+08, device='cuda:0')
c= tensor(8.6379e+08, device='cuda:0')
c= tensor(8.6402e+08, device='cuda:0')
c= tensor(8.6541e+08, device='cuda:0')
c= tensor(8.6545e+08, device='cuda:0')
c= tensor(9.1582e+08, device='cuda:0')
c= tensor(9.1584e+08, device='cuda:0')
c= tensor(9.1797e+08, device='cuda:0')
c= tensor(9.1797e+08, device='cuda:0')
c= tensor(9.1798e+08, device='cuda:0')
c= tensor(9.1798e+08, device='cuda:0')
c= tensor(9.1806e+08, device='cuda:0')
c= tensor(9.1806e+08, device='cuda:0')
c= tensor(9.2309e+08, device='cuda:0')
c= tensor(9.2309e+08, device='cuda:0')
c= tensor(9.2310e+08, device='cuda:0')
c= tensor(9.3108e+08, device='cuda:0')
c= tensor(9.3213e+08, device='cuda:0')
c= tensor(9.3284e+08, device='cuda:0')
c= tensor(9.3664e+08, device='cuda:0')
c= tensor(9.5237e+08, device='cuda:0')
c= tensor(9.5239e+08, device='cuda:0')
c= tensor(9.5241e+08, device='cuda:0')
c= tensor(9.5252e+08, device='cuda:0')
c= tensor(9.5252e+08, device='cuda:0')
c= tensor(9.5252e+08, device='cuda:0')
c= tensor(9.5272e+08, device='cuda:0')
c= tensor(9.5274e+08, device='cuda:0')
c= tensor(9.5274e+08, device='cuda:0')
c= tensor(9.5275e+08, device='cuda:0')
c= tensor(9.5277e+08, device='cuda:0')
c= tensor(9.7402e+08, device='cuda:0')
c= tensor(9.7414e+08, device='cuda:0')
c= tensor(9.7759e+08, device='cuda:0')
c= tensor(9.7763e+08, device='cuda:0')
c= tensor(9.7764e+08, device='cuda:0')
c= tensor(9.7803e+08, device='cuda:0')
c= tensor(1.0299e+09, device='cuda:0')
c= tensor(1.0578e+09, device='cuda:0')
c= tensor(1.0579e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.1415e+09, device='cuda:0')
c= tensor(1.1417e+09, device='cuda:0')
c= tensor(1.1417e+09, device='cuda:0')
c= tensor(1.1429e+09, device='cuda:0')
c= tensor(1.2331e+09, device='cuda:0')
c= tensor(1.2332e+09, device='cuda:0')
c= tensor(1.2333e+09, device='cuda:0')
c= tensor(1.2333e+09, device='cuda:0')
c= tensor(1.2335e+09, device='cuda:0')
c= tensor(1.2335e+09, device='cuda:0')
c= tensor(1.2387e+09, device='cuda:0')
c= tensor(1.2389e+09, device='cuda:0')
c= tensor(1.2389e+09, device='cuda:0')
c= tensor(1.2396e+09, device='cuda:0')
c= tensor(1.2398e+09, device='cuda:0')
c= tensor(1.2398e+09, device='cuda:0')
c= tensor(1.2429e+09, device='cuda:0')
c= tensor(1.2449e+09, device='cuda:0')
c= tensor(1.2649e+09, device='cuda:0')
c= tensor(1.2766e+09, device='cuda:0')
c= tensor(1.2836e+09, device='cuda:0')
c= tensor(1.2838e+09, device='cuda:0')
c= tensor(1.2839e+09, device='cuda:0')
c= tensor(1.2852e+09, device='cuda:0')
c= tensor(1.2879e+09, device='cuda:0')
c= tensor(1.2879e+09, device='cuda:0')
c= tensor(1.2943e+09, device='cuda:0')
c= tensor(1.3007e+09, device='cuda:0')
c= tensor(1.3073e+09, device='cuda:0')
c= tensor(1.3096e+09, device='cuda:0')
c= tensor(1.3130e+09, device='cuda:0')
c= tensor(1.3130e+09, device='cuda:0')
c= tensor(1.3130e+09, device='cuda:0')
c= tensor(1.3131e+09, device='cuda:0')
c= tensor(1.3150e+09, device='cuda:0')
c= tensor(1.3179e+09, device='cuda:0')
c= tensor(1.3490e+09, device='cuda:0')
c= tensor(1.3577e+09, device='cuda:0')
c= tensor(1.3591e+09, device='cuda:0')
c= tensor(1.3594e+09, device='cuda:0')
c= tensor(1.3659e+09, device='cuda:0')
c= tensor(1.3659e+09, device='cuda:0')
c= tensor(1.3659e+09, device='cuda:0')
c= tensor(1.3730e+09, device='cuda:0')
c= tensor(1.3730e+09, device='cuda:0')
c= tensor(1.3730e+09, device='cuda:0')
c= tensor(1.3732e+09, device='cuda:0')
c= tensor(1.4476e+09, device='cuda:0')
c= tensor(1.4478e+09, device='cuda:0')
c= tensor(1.4516e+09, device='cuda:0')
c= tensor(1.4516e+09, device='cuda:0')
c= tensor(1.4516e+09, device='cuda:0')
c= tensor(1.4516e+09, device='cuda:0')
c= tensor(1.4517e+09, device='cuda:0')
c= tensor(1.4519e+09, device='cuda:0')
c= tensor(1.4530e+09, device='cuda:0')
c= tensor(1.4530e+09, device='cuda:0')
c= tensor(1.4567e+09, device='cuda:0')
c= tensor(1.4568e+09, device='cuda:0')
c= tensor(1.4573e+09, device='cuda:0')
c= tensor(1.4574e+09, device='cuda:0')
c= tensor(1.4587e+09, device='cuda:0')
c= tensor(1.4587e+09, device='cuda:0')
c= tensor(1.4589e+09, device='cuda:0')
c= tensor(1.4590e+09, device='cuda:0')
c= tensor(1.4594e+09, device='cuda:0')
c= tensor(1.4613e+09, device='cuda:0')
c= tensor(1.5186e+09, device='cuda:0')
c= tensor(1.5186e+09, device='cuda:0')
c= tensor(1.5187e+09, device='cuda:0')
c= tensor(1.5203e+09, device='cuda:0')
c= tensor(1.5204e+09, device='cuda:0')
c= tensor(1.5760e+09, device='cuda:0')
c= tensor(1.5761e+09, device='cuda:0')
c= tensor(1.5784e+09, device='cuda:0')
c= tensor(1.5873e+09, device='cuda:0')
c= tensor(1.5874e+09, device='cuda:0')
c= tensor(1.5969e+09, device='cuda:0')
c= tensor(1.5974e+09, device='cuda:0')
c= tensor(1.6970e+09, device='cuda:0')
c= tensor(1.6971e+09, device='cuda:0')
c= tensor(1.6972e+09, device='cuda:0')
c= tensor(1.6972e+09, device='cuda:0')
c= tensor(1.6972e+09, device='cuda:0')
c= tensor(1.6973e+09, device='cuda:0')
c= tensor(1.6977e+09, device='cuda:0')
c= tensor(1.6979e+09, device='cuda:0')
c= tensor(1.6998e+09, device='cuda:0')
c= tensor(1.6999e+09, device='cuda:0')
c= tensor(1.6999e+09, device='cuda:0')
c= tensor(1.7000e+09, device='cuda:0')
c= tensor(1.7151e+09, device='cuda:0')
c= tensor(1.7173e+09, device='cuda:0')
c= tensor(1.7435e+09, device='cuda:0')
c= tensor(1.7436e+09, device='cuda:0')
c= tensor(1.7436e+09, device='cuda:0')
c= tensor(1.7436e+09, device='cuda:0')
c= tensor(1.7437e+09, device='cuda:0')
c= tensor(1.7564e+09, device='cuda:0')
c= tensor(1.7564e+09, device='cuda:0')
c= tensor(1.7565e+09, device='cuda:0')
c= tensor(1.7580e+09, device='cuda:0')
c= tensor(1.7591e+09, device='cuda:0')
c= tensor(1.7591e+09, device='cuda:0')
c= tensor(1.7591e+09, device='cuda:0')
c= tensor(1.7844e+09, device='cuda:0')
c= tensor(1.7846e+09, device='cuda:0')
c= tensor(1.7846e+09, device='cuda:0')
c= tensor(1.7847e+09, device='cuda:0')
c= tensor(1.8118e+09, device='cuda:0')
c= tensor(1.8143e+09, device='cuda:0')
c= tensor(1.8389e+09, device='cuda:0')
c= tensor(1.8411e+09, device='cuda:0')
c= tensor(1.8412e+09, device='cuda:0')
c= tensor(1.8415e+09, device='cuda:0')
c= tensor(1.8420e+09, device='cuda:0')
c= tensor(1.8420e+09, device='cuda:0')
c= tensor(1.8421e+09, device='cuda:0')
c= tensor(1.8421e+09, device='cuda:0')
c= tensor(1.8432e+09, device='cuda:0')
c= tensor(1.8433e+09, device='cuda:0')
c= tensor(1.8433e+09, device='cuda:0')
c= tensor(1.8434e+09, device='cuda:0')
c= tensor(1.8435e+09, device='cuda:0')
c= tensor(1.8443e+09, device='cuda:0')
c= tensor(1.8444e+09, device='cuda:0')
c= tensor(1.8446e+09, device='cuda:0')
c= tensor(1.8446e+09, device='cuda:0')
c= tensor(1.8447e+09, device='cuda:0')
c= tensor(1.8447e+09, device='cuda:0')
c= tensor(1.8447e+09, device='cuda:0')
c= tensor(1.8448e+09, device='cuda:0')
c= tensor(1.8516e+09, device='cuda:0')
c= tensor(1.8516e+09, device='cuda:0')
c= tensor(1.8516e+09, device='cuda:0')
c= tensor(1.8517e+09, device='cuda:0')
c= tensor(1.8564e+09, device='cuda:0')
c= tensor(1.9183e+09, device='cuda:0')
c= tensor(1.9187e+09, device='cuda:0')
c= tensor(1.9187e+09, device='cuda:0')
c= tensor(1.9227e+09, device='cuda:0')
c= tensor(1.9250e+09, device='cuda:0')
c= tensor(1.9250e+09, device='cuda:0')
c= tensor(1.9251e+09, device='cuda:0')
c= tensor(1.9252e+09, device='cuda:0')
c= tensor(1.9381e+09, device='cuda:0')
c= tensor(1.9632e+09, device='cuda:0')
c= tensor(1.9689e+09, device='cuda:0')
c= tensor(1.9689e+09, device='cuda:0')
c= tensor(1.9689e+09, device='cuda:0')
c= tensor(1.9689e+09, device='cuda:0')
c= tensor(1.9694e+09, device='cuda:0')
c= tensor(1.9695e+09, device='cuda:0')
c= tensor(1.9696e+09, device='cuda:0')
c= tensor(1.9733e+09, device='cuda:0')
c= tensor(2.0068e+09, device='cuda:0')
c= tensor(2.0068e+09, device='cuda:0')
c= tensor(2.0068e+09, device='cuda:0')
c= tensor(2.0069e+09, device='cuda:0')
c= tensor(2.0073e+09, device='cuda:0')
c= tensor(2.0075e+09, device='cuda:0')
c= tensor(2.0075e+09, device='cuda:0')
c= tensor(2.0075e+09, device='cuda:0')
c= tensor(2.0077e+09, device='cuda:0')
c= tensor(2.0077e+09, device='cuda:0')
c= tensor(2.0121e+09, device='cuda:0')
c= tensor(2.0121e+09, device='cuda:0')
c= tensor(2.0122e+09, device='cuda:0')
c= tensor(2.0123e+09, device='cuda:0')
c= tensor(2.0123e+09, device='cuda:0')
c= tensor(2.0123e+09, device='cuda:0')
c= tensor(2.0171e+09, device='cuda:0')
c= tensor(2.0501e+09, device='cuda:0')
c= tensor(2.0558e+09, device='cuda:0')
c= tensor(2.0577e+09, device='cuda:0')
c= tensor(2.0578e+09, device='cuda:0')
c= tensor(2.0579e+09, device='cuda:0')
c= tensor(2.0580e+09, device='cuda:0')
c= tensor(2.0604e+09, device='cuda:0')
c= tensor(2.0609e+09, device='cuda:0')
c= tensor(2.0610e+09, device='cuda:0')
c= tensor(2.0611e+09, device='cuda:0')
c= tensor(2.2840e+09, device='cuda:0')
c= tensor(2.2841e+09, device='cuda:0')
c= tensor(2.2848e+09, device='cuda:0')
c= tensor(2.2990e+09, device='cuda:0')
c= tensor(2.2996e+09, device='cuda:0')
c= tensor(2.2999e+09, device='cuda:0')
c= tensor(2.3516e+09, device='cuda:0')
c= tensor(2.3536e+09, device='cuda:0')
c= tensor(2.3542e+09, device='cuda:0')
c= tensor(2.3542e+09, device='cuda:0')
c= tensor(2.3545e+09, device='cuda:0')
c= tensor(2.3545e+09, device='cuda:0')
c= tensor(2.3628e+09, device='cuda:0')
c= tensor(2.5801e+09, device='cuda:0')
c= tensor(2.5902e+09, device='cuda:0')
c= tensor(2.5952e+09, device='cuda:0')
c= tensor(2.5966e+09, device='cuda:0')
c= tensor(2.5975e+09, device='cuda:0')
c= tensor(2.5978e+09, device='cuda:0')
c= tensor(2.5979e+09, device='cuda:0')
c= tensor(2.6246e+09, device='cuda:0')
c= tensor(2.6452e+09, device='cuda:0')
c= tensor(2.6459e+09, device='cuda:0')
c= tensor(2.8254e+09, device='cuda:0')
c= tensor(2.8485e+09, device='cuda:0')
c= tensor(2.8488e+09, device='cuda:0')
c= tensor(2.8488e+09, device='cuda:0')
c= tensor(2.8500e+09, device='cuda:0')
c= tensor(2.8515e+09, device='cuda:0')
c= tensor(2.8515e+09, device='cuda:0')
c= tensor(2.8920e+09, device='cuda:0')
c= tensor(2.8926e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8929e+09, device='cuda:0')
c= tensor(2.8933e+09, device='cuda:0')
c= tensor(2.8934e+09, device='cuda:0')
c= tensor(5.4428e+09, device='cuda:0')
c= tensor(5.4431e+09, device='cuda:0')
c= tensor(5.4440e+09, device='cuda:0')
c= tensor(5.4440e+09, device='cuda:0')
c= tensor(5.4441e+09, device='cuda:0')
c= tensor(5.4441e+09, device='cuda:0')
c= tensor(5.4557e+09, device='cuda:0')
c= tensor(5.4570e+09, device='cuda:0')
c= tensor(5.5712e+09, device='cuda:0')
c= tensor(5.5712e+09, device='cuda:0')
c= tensor(5.5774e+09, device='cuda:0')
c= tensor(5.5781e+09, device='cuda:0')
c= tensor(5.5818e+09, device='cuda:0')
c= tensor(5.5980e+09, device='cuda:0')
c= tensor(5.5981e+09, device='cuda:0')
c= tensor(5.5981e+09, device='cuda:0')
c= tensor(5.6001e+09, device='cuda:0')
c= tensor(5.6014e+09, device='cuda:0')
c= tensor(5.6026e+09, device='cuda:0')
c= tensor(5.6079e+09, device='cuda:0')
c= tensor(5.6081e+09, device='cuda:0')
c= tensor(5.6117e+09, device='cuda:0')
c= tensor(5.6119e+09, device='cuda:0')
c= tensor(5.6187e+09, device='cuda:0')
c= tensor(5.6448e+09, device='cuda:0')
c= tensor(5.6450e+09, device='cuda:0')
c= tensor(5.6450e+09, device='cuda:0')
c= tensor(5.6502e+09, device='cuda:0')
c= tensor(5.6508e+09, device='cuda:0')
c= tensor(5.6886e+09, device='cuda:0')
c= tensor(5.6891e+09, device='cuda:0')
c= tensor(5.6897e+09, device='cuda:0')
c= tensor(5.6907e+09, device='cuda:0')
c= tensor(5.6955e+09, device='cuda:0')
c= tensor(5.7015e+09, device='cuda:0')
c= tensor(5.7016e+09, device='cuda:0')
c= tensor(5.7016e+09, device='cuda:0')
c= tensor(5.7016e+09, device='cuda:0')
c= tensor(5.7028e+09, device='cuda:0')
c= tensor(5.7045e+09, device='cuda:0')
c= tensor(5.7053e+09, device='cuda:0')
c= tensor(5.7053e+09, device='cuda:0')
c= tensor(5.7055e+09, device='cuda:0')
c= tensor(5.7062e+09, device='cuda:0')
c= tensor(5.7068e+09, device='cuda:0')
c= tensor(5.7072e+09, device='cuda:0')
c= tensor(5.7087e+09, device='cuda:0')
c= tensor(5.7094e+09, device='cuda:0')
c= tensor(5.7097e+09, device='cuda:0')
c= tensor(5.7097e+09, device='cuda:0')
c= tensor(5.7097e+09, device='cuda:0')
c= tensor(5.7113e+09, device='cuda:0')
c= tensor(5.7116e+09, device='cuda:0')
c= tensor(5.7122e+09, device='cuda:0')
c= tensor(5.7122e+09, device='cuda:0')
c= tensor(5.7122e+09, device='cuda:0')
c= tensor(5.7125e+09, device='cuda:0')
c= tensor(5.7130e+09, device='cuda:0')
c= tensor(5.7131e+09, device='cuda:0')
c= tensor(5.7131e+09, device='cuda:0')
c= tensor(5.7132e+09, device='cuda:0')
c= tensor(5.7135e+09, device='cuda:0')
c= tensor(5.7140e+09, device='cuda:0')
c= tensor(5.7175e+09, device='cuda:0')
c= tensor(5.7175e+09, device='cuda:0')
c= tensor(5.7176e+09, device='cuda:0')
c= tensor(5.7177e+09, device='cuda:0')
c= tensor(5.7178e+09, device='cuda:0')
c= tensor(5.7333e+09, device='cuda:0')
c= tensor(5.7333e+09, device='cuda:0')
c= tensor(5.7342e+09, device='cuda:0')
c= tensor(5.7385e+09, device='cuda:0')
c= tensor(5.7387e+09, device='cuda:0')
c= tensor(5.7432e+09, device='cuda:0')
c= tensor(5.7459e+09, device='cuda:0')
c= tensor(5.7459e+09, device='cuda:0')
c= tensor(5.7462e+09, device='cuda:0')
c= tensor(5.7463e+09, device='cuda:0')
c= tensor(5.7484e+09, device='cuda:0')
c= tensor(5.7516e+09, device='cuda:0')
c= tensor(5.7517e+09, device='cuda:0')
c= tensor(5.7523e+09, device='cuda:0')
c= tensor(5.7527e+09, device='cuda:0')
c= tensor(5.7545e+09, device='cuda:0')
c= tensor(5.7545e+09, device='cuda:0')
c= tensor(5.7545e+09, device='cuda:0')
c= tensor(5.7614e+09, device='cuda:0')
c= tensor(5.7998e+09, device='cuda:0')
c= tensor(5.7999e+09, device='cuda:0')
c= tensor(5.8000e+09, device='cuda:0')
c= tensor(5.8002e+09, device='cuda:0')
c= tensor(5.8031e+09, device='cuda:0')
c= tensor(5.8032e+09, device='cuda:0')
c= tensor(5.8032e+09, device='cuda:0')
c= tensor(5.8036e+09, device='cuda:0')
c= tensor(5.8070e+09, device='cuda:0')
c= tensor(5.8070e+09, device='cuda:0')
c= tensor(5.8080e+09, device='cuda:0')
c= tensor(5.8080e+09, device='cuda:0')
time to make c is 9.727454900741577
time for making loss is 9.7274751663208
p0 True
it  0 : 2477803008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5154824192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5155151872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1344229400.0
relative error loss 0.2314437
shape of L is 
torch.Size([])
memory (bytes)
5181407232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5181407232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1336927200.0
relative error loss 0.23018645
shape of L is 
torch.Size([])
memory (bytes)
5185060864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5185060864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1291306000.0
relative error loss 0.22233157
shape of L is 
torch.Size([])
memory (bytes)
5188141056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5188141056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  1252300300.0
relative error loss 0.21561573
shape of L is 
torch.Size([])
memory (bytes)
5191483392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5191483392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1231744000.0
relative error loss 0.21207644
shape of L is 
torch.Size([])
memory (bytes)
5194690560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5194690560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  1218316300.0
relative error loss 0.20976451
shape of L is 
torch.Size([])
memory (bytes)
5197746176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5197889536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1203912200.0
relative error loss 0.20728448
shape of L is 
torch.Size([])
memory (bytes)
5201096704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 11% |
memory (bytes)
5201096704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1196522500.0
relative error loss 0.20601216
shape of L is 
torch.Size([])
memory (bytes)
5204074496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5204307968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1189444600.0
relative error loss 0.20479351
shape of L is 
torch.Size([])
memory (bytes)
5207511040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5207511040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1181935100.0
relative error loss 0.20350055
time to take a step is 262.62723541259766
it  1 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5210664960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5210664960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1181935100.0
relative error loss 0.20350055
shape of L is 
torch.Size([])
memory (bytes)
5213913088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5213913088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1177413100.0
relative error loss 0.20272198
shape of L is 
torch.Size([])
memory (bytes)
5217062912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5217132544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1173362200.0
relative error loss 0.2020245
shape of L is 
torch.Size([])
memory (bytes)
5220335616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5220335616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  1168491000.0
relative error loss 0.20118581
shape of L is 
torch.Size([])
memory (bytes)
5223399424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5223399424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1165495800.0
relative error loss 0.20067011
shape of L is 
torch.Size([])
memory (bytes)
5226741760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
5226741760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1161989100.0
relative error loss 0.20006634
shape of L is 
torch.Size([])
memory (bytes)
5229801472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5229801472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1158899700.0
relative error loss 0.19953442
shape of L is 
torch.Size([])
memory (bytes)
5233205248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5233205248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1156190700.0
relative error loss 0.199068
shape of L is 
torch.Size([])
memory (bytes)
5236408320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5236408320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1154260500.0
relative error loss 0.19873565
shape of L is 
torch.Size([])
memory (bytes)
5239447552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5239607296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1152102900.0
relative error loss 0.19836418
time to take a step is 267.69169545173645
it  2 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5242814464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5242814464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1152102900.0
relative error loss 0.19836418
shape of L is 
torch.Size([])
memory (bytes)
5245894656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5246017536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1150298600.0
relative error loss 0.19805352
shape of L is 
torch.Size([])
memory (bytes)
5249224704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5249224704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1149048800.0
relative error loss 0.19783834
shape of L is 
torch.Size([])
memory (bytes)
5252345856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5252345856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1147931600.0
relative error loss 0.19764599
shape of L is 
torch.Size([])
memory (bytes)
5255630848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5255630848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1146986000.0
relative error loss 0.19748317
shape of L is 
torch.Size([])
memory (bytes)
5258727424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5258727424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1145806300.0
relative error loss 0.19728006
shape of L is 
torch.Size([])
memory (bytes)
5262036992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5262036992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1145262100.0
relative error loss 0.19718635
shape of L is 
torch.Size([])
memory (bytes)
5265186816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
5265186816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1144453600.0
relative error loss 0.19704716
shape of L is 
torch.Size([])
memory (bytes)
5268389888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5268389888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1143555100.0
relative error loss 0.19689246
shape of L is 
torch.Size([])
memory (bytes)
5271666688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5271666688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1142596600.0
relative error loss 0.19672742
time to take a step is 276.2896556854248
it  3 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5274730496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5274730496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1142596600.0
relative error loss 0.19672742
shape of L is 
torch.Size([])
memory (bytes)
5278089216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5278089216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1141988900.0
relative error loss 0.19662279
shape of L is 
torch.Size([])
memory (bytes)
5281267712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
5281267712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1139806200.0
relative error loss 0.19624698
shape of L is 
torch.Size([])
memory (bytes)
5284503552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5284503552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1138678800.0
relative error loss 0.19605286
shape of L is 
torch.Size([])
memory (bytes)
5287542784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5287718912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1137500700.0
relative error loss 0.19585003
shape of L is 
torch.Size([])
memory (bytes)
5290774528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5290774528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1135722500.0
relative error loss 0.19554387
shape of L is 
torch.Size([])
memory (bytes)
5294120960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5294120960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1135902700.0
relative error loss 0.1955749
shape of L is 
torch.Size([])
memory (bytes)
5297274880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5297274880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1134929400.0
relative error loss 0.19540732
shape of L is 
torch.Size([])
memory (bytes)
5300531200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5300531200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1134130700.0
relative error loss 0.1952698
shape of L is 
torch.Size([])
memory (bytes)
5303607296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5303607296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1133477900.0
relative error loss 0.19515741
time to take a step is 277.01331210136414
it  4 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5306937344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5306937344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1133477900.0
relative error loss 0.19515741
shape of L is 
torch.Size([])
memory (bytes)
5310087168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5310087168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 11% |
error is  1132516400.0
relative error loss 0.19499184
shape of L is 
torch.Size([])
memory (bytes)
5313179648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5313359872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1131820000.0
relative error loss 0.19487196
shape of L is 
torch.Size([])
memory (bytes)
5316571136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5316571136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1131370000.0
relative error loss 0.19479448
shape of L is 
torch.Size([])
memory (bytes)
5319700480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5319700480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  1130833400.0
relative error loss 0.19470209
shape of L is 
torch.Size([])
memory (bytes)
5322985472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5322985472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1130292700.0
relative error loss 0.194609
shape of L is 
torch.Size([])
memory (bytes)
5326147584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5326147584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1130034700.0
relative error loss 0.19456457
shape of L is 
torch.Size([])
memory (bytes)
5329395712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5329395712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1129339400.0
relative error loss 0.19444485
shape of L is 
torch.Size([])
memory (bytes)
5332611072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5332611072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1129043500.0
relative error loss 0.1943939
shape of L is 
torch.Size([])
memory (bytes)
5335633920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5335801856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1128601600.0
relative error loss 0.19431782
time to take a step is 277.9888744354248
it  5 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5339021312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5339021312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1128601600.0
relative error loss 0.19431782
shape of L is 
torch.Size([])
memory (bytes)
5342228480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
5342228480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1128297500.0
relative error loss 0.19426546
shape of L is 
torch.Size([])
memory (bytes)
5345435648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5345435648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1127518200.0
relative error loss 0.19413128
shape of L is 
torch.Size([])
memory (bytes)
5348622336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5348622336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1127045100.0
relative error loss 0.19404984
shape of L is 
torch.Size([])
memory (bytes)
5351849984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 11% |
memory (bytes)
5351849984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1126564900.0
relative error loss 0.19396715
shape of L is 
torch.Size([])
memory (bytes)
5354991616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5354991616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1125937700.0
relative error loss 0.19385916
shape of L is 
torch.Size([])
memory (bytes)
5358272512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5358272512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1125304800.0
relative error loss 0.1937502
shape of L is 
torch.Size([])
memory (bytes)
5361467392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5361467392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1124894200.0
relative error loss 0.1936795
shape of L is 
torch.Size([])
memory (bytes)
5364629504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5364629504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1124364300.0
relative error loss 0.19358826
shape of L is 
torch.Size([])
memory (bytes)
5367877632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5367877632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1123999200.0
relative error loss 0.1935254
time to take a step is 268.8611400127411
it  6 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  5% |  0% |
|  1 | 13% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5370945536
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5370945536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1123999200.0
relative error loss 0.1935254
shape of L is 
torch.Size([])
memory (bytes)
5374279680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5374279680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1123403300.0
relative error loss 0.1934228
shape of L is 
torch.Size([])
memory (bytes)
5377486848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5377486848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1123088900.0
relative error loss 0.19336867
shape of L is 
torch.Size([])
memory (bytes)
5380702208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5380702208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1122821600.0
relative error loss 0.19332266
shape of L is 
torch.Size([])
memory (bytes)
5383905280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5383905280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1122240500.0
relative error loss 0.1932226
shape of L is 
torch.Size([])
memory (bytes)
5386964992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5387108352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1122256400.0
relative error loss 0.19322532
shape of L is 
torch.Size([])
memory (bytes)
5390323712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5390323712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1121932300.0
relative error loss 0.19316953
shape of L is 
torch.Size([])
memory (bytes)
5393498112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5393498112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1121566200.0
relative error loss 0.1931065
shape of L is 
torch.Size([])
memory (bytes)
5396742144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5396742144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1121188900.0
relative error loss 0.19304153
shape of L is 
torch.Size([])
memory (bytes)
5399801856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5399801856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1120769500.0
relative error loss 0.19296934
time to take a step is 261.0370087623596
it  7 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5403156480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5403156480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1120769500.0
relative error loss 0.19296934
shape of L is 
torch.Size([])
memory (bytes)
5406248960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5406248960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  1120410600.0
relative error loss 0.19290754
shape of L is 
torch.Size([])
memory (bytes)
5409587200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5409587200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1120198100.0
relative error loss 0.19287094
shape of L is 
torch.Size([])
memory (bytes)
5412794368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5412794368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1119927300.0
relative error loss 0.19282432
shape of L is 
torch.Size([])
memory (bytes)
5415927808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5415993344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1119657500.0
relative error loss 0.19277786
shape of L is 
torch.Size([])
memory (bytes)
5419216896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 11% |
memory (bytes)
5419216896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  1119340500.0
relative error loss 0.19272329
shape of L is 
torch.Size([])
memory (bytes)
5422379008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 11% |
memory (bytes)
5422379008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1119152600.0
relative error loss 0.19269094
shape of L is 
torch.Size([])
memory (bytes)
5425651712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5425651712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1118895100.0
relative error loss 0.1926466
shape of L is 
torch.Size([])
memory (bytes)
5428850688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 11% |
memory (bytes)
5428850688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1118658600.0
relative error loss 0.19260587
shape of L is 
torch.Size([])
memory (bytes)
5432070144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5432070144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1118491100.0
relative error loss 0.19257705
time to take a step is 262.33454394340515
it  8 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5435211776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5435211776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1118491100.0
relative error loss 0.19257705
shape of L is 
torch.Size([])
memory (bytes)
5438476288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5438476288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1118267400.0
relative error loss 0.19253853
shape of L is 
torch.Size([])
memory (bytes)
5441515520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5441691648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1118123500.0
relative error loss 0.19251375
shape of L is 
torch.Size([])
memory (bytes)
5444898816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5444898816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1117965300.0
relative error loss 0.19248651
shape of L is 
torch.Size([])
memory (bytes)
5448105984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5448105984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1117659600.0
relative error loss 0.19243388
shape of L is 
torch.Size([])
memory (bytes)
5451304960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5451313152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1117492700.0
relative error loss 0.19240515
shape of L is 
torch.Size([])
memory (bytes)
5454524416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5454524416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1117310500.0
relative error loss 0.19237377
shape of L is 
torch.Size([])
memory (bytes)
5457682432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5457682432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1116848600.0
relative error loss 0.19229425
shape of L is 
torch.Size([])
memory (bytes)
5460942848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 11% |
memory (bytes)
5460942848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1116751400.0
relative error loss 0.1922775
shape of L is 
torch.Size([])
memory (bytes)
5464010752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5464010752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1116488700.0
relative error loss 0.19223228
time to take a step is 261.83053731918335
it  9 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5467353088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5467353088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1116488700.0
relative error loss 0.19223228
shape of L is 
torch.Size([])
memory (bytes)
5470523392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5470523392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1116242400.0
relative error loss 0.19218987
shape of L is 
torch.Size([])
memory (bytes)
5473779712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5473779712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1116039700.0
relative error loss 0.19215496
shape of L is 
torch.Size([])
memory (bytes)
5476986880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5476986880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1115673100.0
relative error loss 0.19209185
shape of L is 
torch.Size([])
memory (bytes)
5480103936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 11% |
memory (bytes)
5480194048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 11% |
error is  1115542000.0
relative error loss 0.19206928
shape of L is 
torch.Size([])
memory (bytes)
5483417600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
5483417600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  1115278800.0
relative error loss 0.19202396
shape of L is 
torch.Size([])
memory (bytes)
5486571520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5486571520
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 11% |
error is  1115169800.0
relative error loss 0.19200519
shape of L is 
torch.Size([])
memory (bytes)
5489831936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5489831936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1114863600.0
relative error loss 0.19195248
shape of L is 
torch.Size([])
memory (bytes)
5493035008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5493035008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1114732000.0
relative error loss 0.19192982
shape of L is 
torch.Size([])
memory (bytes)
5496254464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5496254464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1114449900.0
relative error loss 0.19188125
time to take a step is 262.3993728160858
it  10 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5499396096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5499396096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1114449900.0
relative error loss 0.19188125
shape of L is 
torch.Size([])
memory (bytes)
5502685184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5502685184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1114244100.0
relative error loss 0.1918458
shape of L is 
torch.Size([])
memory (bytes)
5505896448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5505896448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1114094100.0
relative error loss 0.19181998
shape of L is 
torch.Size([])
memory (bytes)
5509120000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5509120000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1113931800.0
relative error loss 0.19179204
shape of L is 
torch.Size([])
memory (bytes)
5512146944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5512331264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1113699800.0
relative error loss 0.1917521
shape of L is 
torch.Size([])
memory (bytes)
5515538432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5515538432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1113546800.0
relative error loss 0.19172575
shape of L is 
torch.Size([])
memory (bytes)
5518737408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5518737408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1113382400.0
relative error loss 0.19169745
shape of L is 
torch.Size([])
memory (bytes)
5521944576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5521944576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1113467900.0
relative error loss 0.19171217
shape of L is 
torch.Size([])
memory (bytes)
5525090304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5525155840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1113231400.0
relative error loss 0.19167145
shape of L is 
torch.Size([])
memory (bytes)
5528358912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5528358912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1112994800.0
relative error loss 0.19163072
time to take a step is 290.1347713470459
it  11 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5531475968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5531570176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1112994800.0
relative error loss 0.19163072
shape of L is 
torch.Size([])
memory (bytes)
5534785536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5534785536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  1112786400.0
relative error loss 0.19159484
shape of L is 
torch.Size([])
memory (bytes)
5538000896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5538004992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1112593900.0
relative error loss 0.19156168
shape of L is 
torch.Size([])
memory (bytes)
5541220352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5541220352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1112417800.0
relative error loss 0.19153136
shape of L is 
torch.Size([])
memory (bytes)
5544357888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5544427520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1112249300.0
relative error loss 0.19150236
shape of L is 
torch.Size([])
memory (bytes)
5547638784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5547638784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1112066000.0
relative error loss 0.1914708
shape of L is 
torch.Size([])
memory (bytes)
5550690304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5550850048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1111948800.0
relative error loss 0.19145061
shape of L is 
torch.Size([])
memory (bytes)
5554053120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5554053120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1111801300.0
relative error loss 0.19142523
shape of L is 
torch.Size([])
memory (bytes)
5557231616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5557231616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1111687700.0
relative error loss 0.19140565
shape of L is 
torch.Size([])
memory (bytes)
5560475648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
5560475648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1111644700.0
relative error loss 0.19139825
time to take a step is 268.8683819770813
it  12 : 3036319232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5563678720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5563678720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1111644700.0
relative error loss 0.19139825
shape of L is 
torch.Size([])
memory (bytes)
5566808064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5566808064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1111553500.0
relative error loss 0.19138256
shape of L is 
torch.Size([])
memory (bytes)
5570113536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5570113536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1111505400.0
relative error loss 0.19137427
shape of L is 
torch.Size([])
memory (bytes)
5573218304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5573218304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1111418400.0
relative error loss 0.19135928
shape of L is 
torch.Size([])
memory (bytes)
5576540160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 11% |
memory (bytes)
5576540160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1111378400.0
relative error loss 0.19135241
shape of L is 
torch.Size([])
memory (bytes)
5579747328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5579747328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1111208400.0
relative error loss 0.19132315
shape of L is 
torch.Size([])
memory (bytes)
5582798848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5582962688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1111068200.0
relative error loss 0.19129899
shape of L is 
torch.Size([])
memory (bytes)
5586165760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5586165760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1110934500.0
relative error loss 0.19127598
shape of L is 
torch.Size([])
memory (bytes)
5589184512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5589184512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1110739000.0
relative error loss 0.1912423
shape of L is 
torch.Size([])
memory (bytes)
5592588288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5592588288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1110994400.0
relative error loss 0.1912863
shape of L is 
torch.Size([])
memory (bytes)
5595803648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5595803648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1110622200.0
relative error loss 0.1912222
time to take a step is 287.57091999053955
it  13 : 3036319744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5599014912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5599014912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1110622200.0
relative error loss 0.1912222
shape of L is 
torch.Size([])
memory (bytes)
5602222080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5602222080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1110432300.0
relative error loss 0.1911895
shape of L is 
torch.Size([])
memory (bytes)
5605253120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
5605253120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1110331400.0
relative error loss 0.19117214
shape of L is 
torch.Size([])
memory (bytes)
5608644608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5608644608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1110207500.0
relative error loss 0.1911508
shape of L is 
torch.Size([])
memory (bytes)
5611732992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5611732992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 11% |
error is  1110094800.0
relative error loss 0.19113141
shape of L is 
torch.Size([])
memory (bytes)
5614874624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5615058944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1110045700.0
relative error loss 0.19112295
shape of L is 
torch.Size([])
memory (bytes)
5618278400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5618278400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 11% |
error is  1109961700.0
relative error loss 0.1911085
shape of L is 
torch.Size([])
memory (bytes)
5621301248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5621489664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1109986300.0
relative error loss 0.19111273
shape of L is 
torch.Size([])
memory (bytes)
5624696832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5624696832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1109912000.0
relative error loss 0.19109994
shape of L is 
torch.Size([])
memory (bytes)
5627699200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5627699200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1109853200.0
relative error loss 0.1910898
time to take a step is 261.32009100914
it  14 : 3036582912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5631115264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5631115264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1109853200.0
relative error loss 0.1910898
shape of L is 
torch.Size([])
memory (bytes)
5634334720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5634334720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1109792300.0
relative error loss 0.1910793
shape of L is 
torch.Size([])
memory (bytes)
5637492736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5637492736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1109755400.0
relative error loss 0.19107297
shape of L is 
torch.Size([])
memory (bytes)
5640765440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5640765440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1109688800.0
relative error loss 0.1910615
shape of L is 
torch.Size([])
memory (bytes)
5643829248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5643829248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1109656000.0
relative error loss 0.19105586
shape of L is 
torch.Size([])
memory (bytes)
5647179776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5647179776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1109508600.0
relative error loss 0.19103047
shape of L is 
torch.Size([])
memory (bytes)
5650288640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5650399232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1109453800.0
relative error loss 0.19102104
shape of L is 
torch.Size([])
memory (bytes)
5653610496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5653610496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1109275600.0
relative error loss 0.19099036
shape of L is 
torch.Size([])
memory (bytes)
5656809472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
5656809472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  1109127200.0
relative error loss 0.1909648
shape of L is 
torch.Size([])
memory (bytes)
5659963392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5659963392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1109025300.0
relative error loss 0.19094725
time to take a step is 265.3412137031555
sum tnnu_Z after tensor(13981151., device='cuda:0')
shape of features
(5502,)
shape of features
(5502,)
number of orig particles 22006
number of new particles after remove low mass 21136
tnuZ shape should be parts x labs
torch.Size([22006, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1344093400.0
relative error without small mass is  0.2314203
nnu_Z shape should be number of particles by maxV
(22006, 702)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
shape of features
(22006,)
Tue Jan 31 05:05:57 EST 2023
